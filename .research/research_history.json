{
  "research_topic": "3D点群処理のメモリ効率に関して改善したい",
  "queries": [
    "memory-efficient point cloud",
    "point cloud compression",
    "point cloud quantization",
    "octree memory optimization",
    "kd-tree memory efficiency"
  ],
  "research_study_list": [
    {
      "title": "MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models",
      "abstract": "We present a novel compression algorithm for reducing the storage of LiDAR\nsensor data streams. Our model exploits spatio-temporal relationships across\nmultiple LiDAR sweeps to reduce the bitrate of both geometry and intensity\nvalues. Towards this goal, we propose a novel conditional entropy model that\nmodels the probabilities of the octree symbols by considering both coarse level\ngeometry and previous sweeps' geometric and intensity information. We then use\nthe learned probability to encode the full data stream into a compact one. Our\nexperiments demonstrate that our method significantly reduces the joint\ngeometry and intensity bitrate over prior state-of-the-art LiDAR compression\nmethods, with a reduction of 7-17% and 15-35% on the UrbanCity and\nSemanticKITTI datasets respectively.",
      "full_text": "MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models Sourav Biswas1,2 Jerry Liu1 Kelvin Wong1,3 Shenlong Wang1,3 Raquel Urtasun1,3 1Uber Advanced Technologies Group 2University of Waterloo 3University of Toronto {souravb,jerryl,kelvin.wong,slwang,urtasun}@uber.com Abstract We present a novel compression algorithm for reducing the storage of LiDAR sensor data streams. Our model exploits spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values. Towards this goal, we propose a novel conditional entropy model that models the probabilities of the octree symbols by considering both coarse level geometry and previous sweeps’ geometric and intensity information. We then use the learned probability to encode the full data stream into a compact one. Our experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduction of 7–17% and 6–19% on the UrbanCity and SemanticKITTI datasets respectively. 1 Introduction The past decade has witnessed numerous innovations in intelligent systems, thanks to an explosion of progress in sensing and AI algorithms. In particular, LiDAR sensors are extensively used in various applications such as indoor rovers, unmanned aerial vehicles, and self-driving cars to accurately capture the 3D geometry of the scene. Yet the rapid adoption of LiDAR has brought about a key challenge—dealing with the mounting storage costs associated with the massive inﬂux of LiDAR data. For instance, a 64-line Velodyne LiDAR continuously scanning a given scene produces over3 billion pointsin a single hour. Hence, developing efﬁcient and effective compression algorithms to store such 3D point cloud data streams is crucial to reduce the storage and communication bandwidth. Unlike its well-studied image and video counterparts, point cloud stream compression is a challenging yet under-explored problem. Many prior approaches have focused on encoding a point cloud stream as independent sweeps, where each sweep captures a rough 360-degree rotation of the sensor. Early approaches exploit a variety of compact data structures to represent the point cloud in a memory- efﬁcient manner, such as octrees [1], KD-trees [2], and spherical images [3]. More recent works along this direction utilize powerful machine learning models to encode redundant geometric correlations within these data structures for better compression [4, 5, 6]. In general, most of these aforementioned approaches do not make effective use of temporal correlations within point clouds. Moreover, these prior approaches have largely focused on compressing the geometric structure of the point cloud (the spatial coordinates); yet there has been little attention paid towards compression of other attributes, e.g. LiDAR intensity, which are crucial for many downstream tasks. Compressing such attributes along with geometric structure can make a signiﬁcant impact on reducing storage. In this paper, we present a novel, learning-based compression algorithm that comprehensively reduces the storage of LiDAR sensor data streams. Our method extends the recent success of octree-structured deep entropy models [6] for single LiDAR sweep compression to intensity-valued LiDAR streaming data. Speciﬁcally, we propose a novel deep conditional entropy modelthat models the probabilities of the octree symbols and associated intensity values by exploiting spatio-temporal correlations within the data: taking both coarse level information at the current sweep, as well as relevant neighboring 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2011.07590v2  [eess.IV]  8 Jan 2021(a) Top-down Pass(b) Bottom-up Pass Sweep t-1 Sweep t (2) Prior Octree Dependence(1) Ancestral Node Dependence (3) Spatio-Temporal Aggregation z x y  Octree Occupancy Entropy Model (Sec. 2.3)Intensity Entropy Model (Sec. 2.4) Sweept -1 Sweep t Geometry Bitstream Probability Estimation Intensity Bitstream Continuous Convolution Continuous Convolution z x y  Sweep t-1 Sweep t Input Point Cloud àOctree (Sec 2.1) Probability Estimation Figure 1: Comprehensive overview of our method. Our point cloud stream is serialized into an octree repre- sentation (Sec 2.1). We apply a spatio-temporal entropy model to the octree occupancy bytestream (Sec. 2.3), modeling ancestral dependence, prior octree dependence, and octree alignment. We also apply a deep entropy model to model the intensity stream (Sec. 2.4). nodes information from the previous sweep. Unlike prior approaches, our method models the joint entropy across an entire point cloud sequence, while unifying geometry and attribute compression into the same framework. We validate the performance of our approach on two large datasets, namely UrbanCity [ 7] and SemanticKITTI [8]. The experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduc- tion of 7–17% on UrbanCity and 6–19% on SemanticKITTI. We also conduct extensive experiments showcasing superior performance against prior works on numerous downstream perception tasks. 2 Multi-Sweep LiDAR Compression In this work, we propose a comprehensive framework for thelossy compression of LiDAR point cloud streams, by exploiting the spatio-temporal redundancies through a learned entropy model. We aim to maximize the reconstruction quality of these point clouds while reducing their joint bitrate. Every point in a LiDAR point cloud contains both a spatial 3D location (x,y,z ), as well as an intensity value r, and we jointly compress both. Our method is shown in Fig. 1. We ﬁrst quantize and encode all point spatial coordinates in the stream into an octree representation, where leaves represent the quantized points and intermediate nodes contain 8-bit symbols representing child occupancies (Sec. 2.1). We then present a novel deep entropy model (Sec. 2.2): a probability model that utilizes spatio-temporal contextto predict occupancy symbols for each node (Sec. 2.3), as well as intensity values for each point for intensity compression (Sec. 2.4). The outputs of these entropy models are ﬁnally fed into a lossless entropy coding algorithm, such as range coding, to produce the ﬁnal bitstream (Sec. 2.5). 2.1 Octree Representation Octree Structure and Bit Representation:LiDAR point clouds are intrinsically challenging to process due to their sparsity and inherently unstructured nature. A tool to counteract these challenges is to use a tree-based data structure, such as an octree or KD-tree, to efﬁciently partition the space. Inspired by [1, 6], we quantize and represent every point cloud in our stream as an octree with an associated depth value D, corresponding to the quantized precision of the point cloud. Speciﬁcally, an octree can be constructed from a 3D point cloud by ﬁrst partitioning the spatial region into 8 octants, and recursively partitioning each octant until each node contains at most one point, or until Dis reached. The resulting octree contains both intermediate nodes and leaf nodes. Each intermediate node can be represented by an 8-bit occupancy symbol x, representing the occupancies of its children; each node also has an implied spatial position. Each leaf node contains one point of the point cloud, and stores the offset between the point and its corner position, as well as the point intensity. We determine the intensity value of each point in the quantized point cloud by taking that of its nearest neighbor in the original point cloud. The number of bits allocated to each leaf node is level-dependent; an octree with D= kwill store k−ibits for a leaf node at level i,i ≤k. Hence, 2the octree is memory-efﬁcient—shared bits are encoded with intermediate nodes and residual bits with leaves. Serialization: We serialize the octree into two (uncompressed) bytestreams by traversing the octree in breadth-ﬁrst order. The ﬁrst bytestream contains the intermediate node occupancy symbols in breadth-ﬁrst order, and the second bytestream contains the leaf node offsets/intensities encountered during traversal. Our entropy model focuses primarily on the node occupancies/intensities—we demonstrate in our supplementary materials that leaf offsets do not contain meaningful patterns we can exploit. Hence for subsequent sections we denote P(t) = (X(t),R(t)), where X(t) = {x(t) 1 ,..., x(t) mt } is the set of occupancy symbols, and R(t) = {r(t) 1 ,..., r(t) nt }is the set of intensities. The serialization is lossless; the only loss comes from D-dependent octree quantization. This gives a guarantee on reconstruction quality and allows compression efforts to solely focus on bitrate reduction. 2.2 Octree-Based Conditional Entropy Module The octree sequence is now fed into ourentropy model. Our entropy model is a probability model that approximates the unknown joint distribution of point clouds pdata with our own distribution p(·; w). Since we convert our point clouds to octree representations, the probability model is equivalent to modeling p(P(1),..., P(n); w). According to the classic Shannon’s source coding theorem [9], the expected bitrate for the point cloud stream is tightly approximated by the cross-entropy between the real point cloud stream distribution and our parametrized model: Epdata [−log p(P(1),..., P(n); w)]. We then assume that the joint probability factorizes as follows: log p(P(1),..., P(n); w) = ∑ t log p(P(t)|P(t−1); w) (1) = ∑ t {log p(X(t)|P(t−1); w) + logp(R(t)|X(t),P(t−1); w)} (2) We make a 1st-order Markov assumption: a given octree P(t) only depends on the sweep preced- ing it, P(t−1). We then factor the octree into two entropy models: the node occupancy model p(X(t)|P(t−1); w), and the intensity model p(R(t)|X(t),P(t−1); w) conditioned on occupancies. The dependence only on past sweeps makes the model applicable to an online LiDAR stream setting. 2.3 Occupancy Entropy Model We obtain our node occupancy model by continuing to factorize the occupancy probabilities: p(X(t)|P(t−1); w) = ∏ i p(x(t) i |X(t) ans(i),P(t−1); w) (3) Here, X(t) ans(i) = {x(t) pa(i),x(t) pa(pa(i)),..., x(t) pa(...(pa(i)))}represents the set of ancestor nodes of x(t) i and P(t−1) represents the point cloud from previous sweep. As seen above, we simplify the autoregressive dependency on ancestors nodes on the octree for the given timestamp, as well as all the nodes at the previous timestamp. We model p(·|X(t) ans(i),P(t−1); w) with a deep neural network. The architecture has two backbones, namely the ancestral node dependencemodule which encodes recurrent dependencies on the ancestor nodes X(t) ans(i) from the current sweep’s octree as well as a prior octree dependencemodule which models information passed from the previous sweep. Fig. 1 depicts the architecture of such network. Ancestral Node Dependence: Our ancestral node dependence module is a recurrent network deﬁned over an ancestral, top-down octree path. Inspired by [ 6], we feed a context feature ci for every node xi through a multi-layer perceptron (MLP) to extract an initial hidden embed- ding h(t) i,0 = σ0(ci; w), where σ0(·; w) denotes a MLP with learnable parameter w. Context 3features include the current octree level, octant spatial location, and parent occupancy; they are known beforehand per node xi and computed to facilitate representation learning. We then per- form Kans rounds of aggregation between every node’s embedding and its parental embedding: h(t) i,k = σk([h(t) i,k−1,h(t) pa(i),k−1]; w). As shorthand, we denote this entire tree-structured recurrent backbone branch as h(t) i = fans(x(t) i ,X(t) ans(i)). Temporal Octree Dependence: We also incorporate the previous octree P(t−1) into the current entropy model at time t through a temporal octree dependencemodule. We thus ﬁrst align the previous octree into the sensor coordinate frame of the current octree. Unlike the current octree where we only have access to parental information, we can construct features that make use of all information within the previous octree, containing both top-down ancestral information as well as bottom-up child information. We exploit this fact by designing a two-stream feature backbone to compute embeddings for every octree node at time t−1, inspired by tree-structured message passing algorithms [10, 11]. The forward pass stream is the same as the ancestral dependence module above, generating top-down features from ancestors: h(t−1) j = fans(x(t−1) j ,X(t−1) ans(j) ). After the top-down pass, we design a bottom-up aggregation pass, a recurrent network that produces aggregated features from descendants to the current node. Unlike the ancestral module in which each node only has one parent, the number of children per node can vary, and we desire that the output is invariant to the ordering of the children. Hence, we resolve this by designing the following function inspired by deep sets [12]: g(t−1) j = fagg,1(h(t−1) j + ∑ c∈child(j) fagg,2(g(t−1) c )), which produces the ﬁnal aggregated embedding feature containing both top-down and bottom-up context. Spatio-Temporal Aggregation: The ﬁnal step incorporates the set of aggregated features in the previous octree {g(t−1) j }, with ancestral features in the current octree {h(t) i }to help with occupancy prediction in the current octree. A key observation is that only a subset of spatially proximal nodes in the previous sweep can contribute to better prediction for a given node at timet; moreover, the relative location of each neighbor should deﬁne its relative importance. Inspired by this fact, we employ continuous convolutions[13] to process previous octree features at the current node. A continuous conv. layer aggregates features from neighboring points to a given node in the following manner: hi = ∑ j∈N(i) σ(pj −pi)hj where N(i) is the i-th node’sk-nearest neighboring nodes in 3D space from the (t−1) sweep at the same level as i, pi is the 3D position of each node, and σdenotes a learned MLP. We use a separate MLP with a continuous conv. layer per octree level to process the aggregated features in the previous octree {g(t−1) j }j∈N(i) and produce an embedding feature g(t) i,st. Entropy Header: Finally, the warped feature g(t) i,st and ancestral features h(t) i are aggre- gated through a ﬁnal MLP to output a 256-dimensional softmax of probability values p(x(t) i |X(t) ans(i),P(t−1); w), corresponding to the predicted 8-bit occupancy for node i, time t. 2.4 Intensity Entropy Model The goal of the intensity entropy model is to compress extraneous intensities tied to each spatial point coordinate. We assume these intensities are bounded and discrete, so compression is lossless; if they are continuous, there will be a loss incurred through discretization. The model factorizes as follows: p(R(t)|X(t),P(t−1); w) = ∏ i p(r(t) i |X(t),P(t−1); w) (4) The intent of conditioning on the occupancies X(t) is not to directly use their values per se, but to emphasize that intensity decoding occurs after the point spatial coordinates have already been reconstructed in R3. Therefore, we can directly make use of the spatial position corresponding to each intensity R(t) i in compression. We aim to leverage temporal correlations between point intensities across consecutive timestamps to better model the entropy of r(t) i . Similar to node occupancy predic- tion above, there is the challenge of how to incorporate previous intensity information when there are no direct correspondences between the two point clouds. We again employ continuous convolutions 40 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 0.00 0.05 0.10 0.15Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 70 80 90 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 0.00 0.05 0.10 0.15 0.20Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 60 70 80 90 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Figure 2: Bitrate vs. reconstruction quality curves on UrbanCity (top) and KITTI (bottom). From left-to-right: F1 with τgeo = 10cmand τint = 0(↑), point-to-point chamfer distance (↓), point-to-plane PSNR (↑). to resolve this challenge. Let RN(i) be the set of nearest neighbor intensities {r(t−1) j }j∈N(i), where nearest neighbor is deﬁned by spatial proximity of previous point jto the current point i. We apply an MLP with a continuous conv. layer that takes the past intensities r(t−1) j as input and outputs an embedding feature for each node i. This feature is then fed through a linear layer and softmax to output intensity probability values. In our setting we assume our intensity value is an 8-bit integer, so the resulting probability vector is 256-dimensional p(r(t) i |X(t),P(t−1); w). 2.5 Entropy Coding Process Encoding: We integrate our entropy model with an entropy coding algorithm (range coding [14]) to produced the ﬁnal compressed bitstream. During the encoding pass, for every octree in the stream, the entropy model is applied across the octree occupancy bytestream, as well as across the intensities per point, to predict the respective probability distributions. We note that encoding only requires one batch GPU pass per sweep for the occupancy and intensity models. The resulting distributions are then passed to range coding which compresses the occupancies and intensities into two bitstreams. Decoding: The same entropy models are used during decoding. First, the occupancy entropy model is run, given the already decoded past octree, to produce distributions that recover the occupancy serialization and spatial coordinates of the current point cloud. Then, the intensity entropy model is run, given the already decoded intensities in the past point cloud, to produce distributions that recover the current point intensities. Note that our model is well-setup for parallel computation during decoding, for both the occupancies and intensities. As mentioned in Sec. 2.3, the dependence on ancestral nodes instead of all past nodes allows us to only run at most O(D) GPU passes for the occupancy model per sweep. Moreover, the assumed independence between intensities in the current sweep, given the past, allows us to only run 1 GPU pass per sweep for the intensity entropy model. 2.6 Learning Both our occupancy and intensity entropy models are trained end-to-end with cross-entropy loss, for every node x(t) i ∈X(t) i and intensity r(t) i ∈R(t) i , for every point cloud in a stream: ℓ= EP∼pdata [ − ∑ t ∑ i log p(x(t) i,gt|X(t) ans(i),P(t−1); w) − ∑ t ∑ i log p(r(t) i,gt|X(t),P(t−1); w) ] (5) 5Oracle (UrbanCity): Bitrate 104  Ours: F1 92.4, Bitrate 9.3  Draco: F1 80.8, Bitrate 9.4  MPEG: F1 59.1, Bitrate 11.4 Oracle (UrbanCity): Bitrate 104  Ours: F1 99.2, Bitrate 13.7  Draco: F1 92.3, Bitrate 13.8  MPEG: F1 81.5, Bitrate 16.2 Oracle (KITTI): Bitrate 104  Ours: F1 90.2, Bitrate 5.6  Draco: F1 87.1, Bitrate 5.7  MPEG: F1 61.0, Bitrate 9.5 Oracle (KITTI): Bitrate 104  Ours: F1 98.6, Bitrate 10.1  Draco: F1 96.9, Bitrate 10.1  MPEG: F1 79.9, Bitrate 12.9 Figure 3: Qualitative results on UrbanCity and KITTI. Points are colored by intensity. Here, x(t) i,gt and r(t) i,gt denote the ground-truth values of the node occupancies/intensities, respectively. As mentioned above, minimizing cross-entropy loss is equivalent to our goal of reducing expected bitrate of the point cloud stream. 2.7 Discussion and Related Works Our approach belongs to a family of point cloud compression algorithms based on tree data struc- tures [15, 2, 16, 17, 18, 19, 20, 1, 20, 21, 22, 23, 24]. Tree-based algorithms are advantageous since they use spatial-partioning data structures that can efﬁciently represent sparse and non-uniformly dense 3D point clouds. Two notable examples are Google’s Draco [2] and the MPEG anchor [25], which use a KD-tree codec [ 15] and an octree codec [ 1] respectively. To exploit temporal redun- dancies, the MPEG anchor encodes successive point clouds as block-based rigid transformations of previous point clouds; this, however, narrows its usefulness to scenes with limited motion. Moreover, these prior works use simple entropy models that do not fully exploit redundant information hidden in LiDAR point clouds; e.g., repetitive local structures, objects with strong shape priors, etc. In contrast, we use a learned entropy model to directly capture these dependencies. Our approach is also related to work in deep point cloud compression [ 4, 5, 26, 27, 28, 29, 6]. In particular, both our approach and the prior state-of-the-art [6] use deep entropy models that operate on octree structures directly. However, they do not model temporal redundancies between successive point clouds and compress LiDAR geometry only. In this work, we propose a uniﬁed framework that aggregates spatio-temporal context to jointly compress both LiDAR geometry and intensity. Finally, our work is inspired by recent successes in deep image compression [30, 31, 32, 33, 34, 35, 36] and video compression [37, 38, 39, 40, 41, 42, 43], many of which use deep entropy models. 3 Experimental Evaluation We evaluate our LiDAR compression method on two large-scale datasets. Holding reconstruction quality equal, our framework for joint geometry and intensity compression achieves a 7–17% and 6– 19% bitrate reduction over OctSqueeze [6], the prior state-of-the-art in deep point cloud compression, on UrbanCity and SemanticKITTI. Holding bitrate equal, our method’s reconstructions also have a smaller realism gap on downstream tasks. 6Spatial Bitrate O T B CC D = 12 D = 14 D = 16 ✓ 2.91 8.12 14.16 ✓ ✓ 2.87 8.04 14.08 ✓ ✓ ✓ 2.72 7.90 13.95 ✓ ✓ ✓ ✓ 2.55 7.64 13.79 Table 1: Abalation study of occupancy entropy model on UrbanCity. O, T, and B stand for using past oc- cupancy bytes, top-down aggregated features, and bottom-up aggregated features. CC indicates using continuous conv. D stands for the octree’s max depth. Intensity Bitrate Encoder P D = 12 D = 14 D = 16 zlib[45] 2.42 4.79 5.23 MLP ✓ 2.31 4.62 5.01 CC ✓ 2.13 4.30 4.68 Table 2: Ablation study of intensity entropy model on SemanticKITTI. zlib is an off-the-shelf library [45]; MLP is our model without continuous conv.; and CC is our ﬁnal model. P stands for using past intensity information. D stands for the octree’s max depth. 3.1 Experimental Details We validate the performance of our approach on two datasets: UrbanCity [7] and SemanticKITTI [8]. UrbanCity: UrbanCity is a large-scale dataset collected by a ﬂeet of self-driving vehicles in several cities across North America [7]. Every sequence consists of 250 Velodyne HDL-64E LiDAR sweeps sampled at 10Hz, each containing a 3D point cloud (as 32-bit ﬂoats) and their intensity values (as 8-bit unsigned integers). The average size of each sweep is 80,156 points. We train our entropy models on 5000 sequences and evaluate on a test set of 500. Every sweep in UrbanCity is annotated with per-point semantic labels for the vehicle, pedestrian, motorbike, road, and background classes, as well as bird’s eye view bounding boxes for the ﬁrst three classes. We use these labels to perform downstream perception experiments on the same train/test split. SemanticKITTI: We also conduct compression and downstream perception experiments on Se- manticKITTI [8], which enhances the KITTI [44] dataset with dense semantic labels for each LiDAR sweep. It consists of 22 driving sequences containing a total of 43,552 Velodyne HDL-64E LiDAR sweeps sampled at 10Hz. The average size of each sweep is 120,402 points. In our experiments, we use the ofﬁcial train/test splits: sequences 00 to 10 (except for 08) for training and sequences 11 to 21 to evaluate reconstruction quality. Since semantic labels for the test split are unavailable, we evaluate downstream tasks on the validation sequence 08. Baselines: We compare against a number of state-of-the-art LiDAR compression algorithms: Huang et al.’s deep octree-based method (OctSqueeze) [6], Google’s KD-tree based method (Draco) [2], Mekuria et al.’s octree-based MPEG anchor (MPEG Anchor) [1]1, and MPEG TMC132. From discussions with the authors, “MPEG Anchor” in [ 6] is a custom implementation that uses an empirical histogram distribution to compress octree occupancy symbols; we report this baseline as Octree. As OctSqueeze and Octree do not compress LiDAR intensities, we augment them with an off-the-shelf lossless compression algorithm [45]. In particular, we ﬁrst assign an intensity to each encoded point based on the intensity of its nearest neighbour in the original point cloud. Then, we compress the resulting bytestream. For MPEG Anchor, we use the built-in PCL color coder in the authors’ implementation, which encodes the average intensity at each leaf node in the octree with range coding. Similarly, for Draco and MPEG TMC13, we use their built-in attributes coders. We also compare against a video compression based algorithm using LiDAR’s range image representation (MPEG Range). As this baseline was uncompetitive, we report its results in the supplementary. Implementation Details: In our experiments, we construct octrees over a 400m ×400m ×400m region of interest centered on the ego-vehicle. By varying the octree’s maximum depth from 11 to 16, we can control our method’s bitrate-distortion tradeoff, with spatial quantization errors ranging from 9.75cm (at depth 11) to 0.3cm (at depth 16). We train and evaluate individual entropy models at each depth from 11 to 16, which we found gave the best results. Our models use Kans = 4 rounds of aggregation and k= 5 nearest neighbors for continuous convolution. Our method is implemented in PyTorch [46] and we use Horovod [47] to distribute training over 16 GPUs. We train our models over 150,000 steps using the Adam optimizer [48] with a learning rate of 1e−4 and a batch size of 16. 1We use the authors’ implementation:https://github.com/cwi-dis/cwi-pcl-codec . 2MPEG TMC13 reference implementation: https://github.com/MPEGGroup/mpeg-pcc-tmc13 75 10 15 20 25 30 Overall Bits Per Point (BPP) 15 20 25 30 35 40Mean IOU Bitrate vs. IOU (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 40 50 60 70 80 90Mean IOU Bitrate vs. IOU (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 83.00 83.25 83.50 83.75 84.00 84.25Vehicle AP@70% IOU Bitrate vs. Vehicle AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 76 77 78Pedestrian AP@50% IOU Bitrate vs. Pedestrian AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 67 68 69 70Motorbike AP@50% IOU Bitrate vs. Motorbike AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle Figure 4: Bitrate vs. downstream task performance. Top: mean IoU for semantic segmentation on SemanticKITTI (left) and UrbanCity (right). Bottom: AP for vehicle, pedestrian, and motorbike detection on UrbanCity. Oracle IOU: 37.5, Bitrate: 104.0  Ours IOU: 36.8, Bitrate: 13.1  Oracle AP: 88.5, Bitrate: 104.0  Ours AP: 88.5, Bitrate: 9.9 Figure 5: Left: Semantic segmentation on SemanticKITTI. Right: Object detection on UrbanCity. Even at very low bitrates, our reconstructions have a minimal realism gap for downstream tasks. Metrics: We report reconstruction metrics in terms of F1 score, point-to-point (D1) Chamfer distance [ 6], and point-to-plane (D2) PSNR [ 49]. Point-to-point and point-to-plane errors are standard MPEG metrics [25]. But whereas they measure reconstruction quality in terms of geometry only, F1 measures this in terms of both geometry and intensity. Reconstruction metrics are averaged across sweeps and bitrate is the average number of bits used to store a LiDAR point. Following standard practice, we do not count the one-time transmission of network weights since it is negligible compared to the size of long LiDAR streams; e.g. 1 hour. See our supplementary materials for details. 3.2 Results Quantitative Results: In Fig. 2, we report bitratevs. reconstruction quality curves for all competing methods on UrbanCity and SemanticKITTI. The leftmost ﬁgures show the trade-off between overall bitrate vs. F1. Here, we see that our method outperforms the prior state-of-the-art and, holding reconstruction quality equal, achieves a 7–17% (resp., 6–19%) relative reduction in bitrate versus OctSqueeze on UrbanCity (resp., SemanticKITTI). Our model also outperforms MPEG TMC13— the MPEG point cloud compression standard—especially at lower bitrates. The right two ﬁgures show the trade-off between spatial bitrate vs. Chamfer distance and PSNR respectively. Although our method shares a common octree data structure with OctSqueeze ( resp., Octree), and thus have the same reconstruction quality, we achieve a 5–30% ( resp., 15–45%) reduction in spatial bitrates on UrbanCity by additionally exploiting temporal information; similar results also hold in SemanticKITTI. These results validate our uniﬁed framework for geometry and intensity compression using spatial-temporal information. Qualitative Results: In Fig. 3, we show reconstructions from our method, Draco, and MPEG Anchor on UrbanCity and SemanticKITTI. At similar bitrates, our method yields higher quality reconstructions than the competing methods in terms of both geometry and intensity. For example, from the ﬁrst and third rows of Fig. 3, we can see that our method produces faithful reconstructions even at high compression rates. In contrast, Draco and MPEG Anchor produce apparent artifacts. 8Ablation Studies: We perform two ablation studies on our occupancy and intensity entropy models. In Tab. 1, we ablate how to incorporate past information to lower the entropy of our occupancy model. We start with using the past octree’s occupancy bytes (O) and then progressively add top-down and bottom-up aggregated features (T and B respectively), and ﬁnally continuous convolutions ( CC). We see that, holding reconstruction quality equal, each aspect of our model consistently reduces bitrates. In Tab. 2, we compare three compression methods for the intensity model: the zlib library, a multi-layer perceptron entropy model ( MLP), and our ﬁnal model ( CC). Note that both MLP and CC conditions on context from neighboring points in the past sweep; zlib does not. However, whereas MLP uses context from one neighbor only, CC aggregates context from multiple neighbors via continuous convolutions. Our results show that learning to incorporate past context reduces intensity bitrates by 4–5%, and that this improvement is strengthened to 11–12% by using continuous convolutions to align information across space and time. Please see our supplementary for details. Impact on Downstream Tasks:To study the impact of compression on downstream perception tasks, we ﬁrst train segmentation and detection models on uncompressed LiDAR for SemanticKITTI and UrbanCity. Note that these models use both LiDAR geometry and intensity as input (see supplementary for details). Next, we evaluate the models on LiDAR reconstructions obtained from various compression schemes and report their performance as a function of overall bitrate. For segmentation on SemanticKITTI and UrbanCity, we report mean IOU using voxelized ground truth labels at a 10cm resolution. For detection on UrbanCity, we report AP at 50% IOU for pedestrians and motorbikes and 70% IOU for vehicles. In Fig. 4 and 5, we see that our method’s reconstructions have the smallest realism gap for downstream tasks across all bitrates. This result is especially pronounced for segmentation models, which are more sensitive to ﬁne-grained geometric and intensity details. 4 Conclusion We have presented a novel LiDAR point cloud compression algorithm using a deep entropy model which exploits spatio-temporal redundancies between successive LiDAR point clouds. We showed that we can compress point clouds at identical reconstruction quality to the state-of-the-art while lowering bitrate signiﬁcantly, as well as compress LiDAR intensity values effectively which was not as extensively explored by prior works. Furthermore, we showed our compression can be applied to downstream self-driving perception tasks without hindering performance. Looking forward, we plan to extend our method to jointly compress data streams from entire sensor suites. Broader Impact On an immediate level, our contributions are directly applicable as a data compression algorithm in a novel problem setting: the greater we can maximize the performance of such an algorithm, the more we can reduce the storage cost and space required by point clouds. We hope that this in turn unlocks a milestone towards fulﬁlling our ultimate vision: scaling up the research and deployment of intelligent robots, such as self-driving vehicles, that will revolutionize the safety, efﬁciency, and convenience of our transportation infrastructure. By capturing the 3D geometry of the scene, LiDAR sensors have proven to be crucial in effective and safe prediction/planning of these robots. Currently, LiDAR sensors are not only expensive due to the upfront cost, but also due to the recurring costs of the massive quantities of data they generate. Good point cloud and LiDAR compression algorithms will thus help to democratize the usage of LiDAR by making it more feasible for people to own and operate. Perhaps just as importantly, our responsibility as researchers in a novel problem area led us to carefully consider the downstream impacts of such a compression algorithm—if the primary usage of LiDAR currently is on perception tasks, such as detection and segmentation, then we need to demonstrate how compression bitrate affects perception performance, helping the community determine the acceptable bitrate at which compression can be used for safe vision and robotics applications. We hope that our work inspires the community to further advance sensor compression in addition to the traditional image and video settings. References [1] Rufael Mekuria, Kees Blom, and Pablo Cesar. Design, implementation and evaluation of a point cloud codec for tele-immersive video. In IEEE IEEE Transactions on Circuits and Systems for 9Video Technology, 2016. [2] Google. Draco 3d data compresison. https://github.com/google/draco, 2017. [3] Chenxi Tu, E. Takeuchi, C. Miyajima, and K. Takeda. Compressing continuous point cloud data using image compression methods. In 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), 2016. [4] Chenxi Tu, Eijiro Takeuchi, Alexander Carballo, and Kazuya Takeda. Real-time streaming point cloud compression for 3d lidar sensor using u-net. IEEE Access, 2019. [5] C. Tu, E. Takeuchi, A. Carballo, and K. Takeda. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 International Conference on Robotics and Automation (ICRA), 2019. [6] Lila Huang, Shenlong Wang, Kelvin Wong, Jerry Liu, and Raquel Urtasun. Octsqueeze: Octree- structured entropy model for lidar compression. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2020. [7] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end perception and prediction with tracking in the loop, 2020. [8] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In IEEE International Conference on Computer Vision, ICCV), 2019. [9] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 1948. [10] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009. [11] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Generalized belief propagation. In Proceedings of the 13th International Conference on Neural Information Processing Systems, NIPS’00, page 668–674, Cambridge, MA, USA, 2000. MIT Press. [12] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3391–3401. Curran Associates, Inc., 2017. [13] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep parametric continuous convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2018. [14] G. Nigel Martin. * range encoding: an algorithm for removing redundancy from a digitised message. 1979. [15] O. Devillers and P. . Gandoin. Geometric compression for interactive transmission. In Proceed- ings Visualization 2000. VIS 2000 (Cat. No.00CH37145), 2000. [16] Ruwen Schnabel and Reinhard Klein. Octree-based point-cloud compression. In Proceedings of the 3rd Eurographics / IEEE VGTC Conference on Point-Based Graphics, SPBG’06, page 111–121, Goslar, DEU, 2006. Eurographics Association. [17] Yan Huang, Jingliang Peng, C.-.C. Jay Kuo, and M. Gopi. A generic scheme for progressive point cloud coding. IEEE Transactions on Visualization and Computer Graphics, 2008. [18] Cha Zhang, Dinei Florencio, and Charles Loop. Point cloud attribute compression with graph transform. IEEE - Institute of Electrical and Electronics Engineers, October 2014. [19] Dorina Thanou, Philip Chou, and Pascal Frossard. Graph-based motion estimation and compen- sation for dynamic 3d point cloud compressio,. pages 3235–3239, 09 2015. [20] Ricardo L. de Queiroz and Philip A. Chou. Compression of 3d point clouds using a region- adaptive hierarchical transform. Trans. Img. Proc., 25(8):3947–3956, August 2016. [21] Diogo C. Garcia and Ricardo L. de Queiroz. Context-based octree coding for point-cloud video. In 2017 IEEE International Conference on Image Processing (ICIP), 2017. [22] Yiting Shao, Zhaobin Zhang, Zhu Li, Kui Fan, and Ge Li. Attribute compression of 3d point clouds using laplacian sparsity optimized graph transform. 2017 IEEE Visual Communications and Image Processing (VCIP), pages 1–4, 2017. 10[23] Diogo C. Garcia and Ricardo L. de Queiroz. Intra-frame context-based octree coding for point- cloud geometry. In 2018 25th IEEE International Conference on Image Processing (ICIP), 2018. [24] Diogo C. Garcia, Tiago A. Fonseca, Renan U. Ferreira, and Ricardo L. de Queiroz. Geom- etry coding for dynamic voxelized point clouds using octrees and multiple contexts. IEEE Transactions on Image Processing, 2020. [25] S. Schwarz, M. Preda, V . Baroncini, M. Budagavi, P. Cesar, P. A. Chou, R. A. Cohen, M. Kri- voku´ca, S. Lasserre, Z. Li, J. Llach, K. Mammou, R. Mekuria, O. Nakagami, E. Siahaan, A. Tabatabai, A. M. Tourapis, and V . Zakharchenko. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2019. [26] Maurice Quach, Giuseppe Valenzise, and Frederic Dufaux. Learning convolutional transforms for lossy point cloud geometry compression, 2019. [27] Jianqiang Wang, Hao Zhu, Z. Ma, Tong Chen, Haojie Liu, and Qiu Shen. Learned point cloud geometry compression. ArXiv, abs/1909.12037, 2019. [28] Wei Yan, Yiting Shao, Shan Liu, Thomas H. Li, Zhu Li, and Ge Li. Deep autoencoder-based lossy geometry compression for point clouds. CoRR, abs/1905.03691, 2019. [29] Tianxin Huang and Yong Liu. 3d point cloud geometry compression on deep learning. In Proceedings of the 27th ACM International Conference on Multimedia, MM ’19, page 890–898, New York, NY , USA, 2019. Association for Computing Machinery. [30] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] David Minnen, Johannes Ballé, and George Toderici. Joint autoregressive and hierarchical priors for learned image compression. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 10794–10803, 2018. [32] George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele Covell. Full resolution image compression with recurrent neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5435–5443. IEEE Computer Society, 2017. [33] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 4394–4402. IEEE Computer Society, 2018. [34] Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár. Lossy image compression with compressive autoencoders. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [35] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Practical full resolution learned lossless image compression. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10629–10638. Computer Vision Foundation / IEEE, 2019. [36] James Townsend, Thomas Bird, and David Barber. Practical lossless compression with latent variables using bits back coding. In International Conference on Learning Representations, 2019. [37] Chao-Yuan Wu, Nayan Singhal, and Philipp Krähenbühl. Video compression through image interpolation. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII, volume 11212 of Lecture Notes in Computer Science, pages 425–440. Springer, 2018. [38] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao. DVC: an end-to-end deep video compression framework. In IEEE Conference on Computer Vision and 11Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 11006–11015. Computer Vision Foundation / IEEE, 2019. [39] Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander G. Anderson, and Lubomir D. Bourdev. Learned video compression. In 2019 IEEE/CVF International Conference on Com- puter Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 3453– 3462. IEEE, 2019. [40] AmirHossein Habibian, Ties van Rozendaal, Jakub M. Tomczak, and Taco Cohen. Video compression with rate-distortion autoencoders. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 7032–7041. IEEE, 2019. [41] Abdelaziz Djelouah, Joaquim Campos, Simone Schaub-Meyer, and Christopher Schroers. Neural inter-frame compression for video coding. In The IEEE International Conference on Computer Vision (ICCV), October 2019. [42] Ren Yang, Fabian Mentzer, Luc Van Gool, and Radu Timofte. Learning for video compression with hierarchical quality and recurrent enhancement, 2020. [43] Jianping Lin, Dong Liu, Houqiang Li, and Feng Wu. M-lvc: Multiple frames prediction for learned video compression, 2020. [44] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. [45] Jean loup Gailly and Mark Adler. zlib. https://github.com/madler/zlib, 1995. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, 2019. [47] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799, 2018. [48] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [49] D. Tian, H. Ochimizu, C. Feng, R. Cohen, and A. Vetro. Geometric distortion metrics for point cloud compression. In 2017 IEEE International Conference on Image Processing (ICIP), 2017. [50] Igor Pavlov. Lzma. https://sourceforge.net/p/scoremanager/discussion/457976/ thread/c262da00/, 1998. [51] Julian Seward. bzip2. https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/ source/src/util/compress/bzip2/README, 1996. [52] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A modern library for 3D data processing. arXiv:1801.09847, 2018. [53] Chris Zhang, Wenjie Luo, and Raquel Urtasun. Efﬁcient convolutions for real-time semantic segmentation of 3d point clouds. In 2018 International Conference on 3D Vision, 3DV 2018, Verona, Italy, September 5-8, 2018, pages 399–408. IEEE Computer Society, 2018. [54] Yuxin Wu and Kaiming He. Group normalization. CoRR, abs/1803.08494, 2018. [55] Bin Yang, Wenjie Luo, and Raquel Urtasun. PIXOR: real-time 3d object detection from point clouds. CoRR, abs/1902.06326, 2019. 12A Additional Experiments A.1 Compression of Leaf Offsets We mention in Sec. 2.1 of the main paper that we do not attempt to compress the leaf offsets from the octree. The reason is that we experimented with a few compression baselines and were not able to obtain a bitrate improvement over the uncompressed leaf offsets. We experiment with the zlib [45], LZMA [50], and bzip2 [51] compression algorithms on the leaf offset stream from UrbanCity. The results are shown in Tab. 3; we surprisingly found that in all cases the compressed string was longer than the uncompressed one. Uncompressed zlib [45] LZMA [50] bzip2 [51] Avg. Bytes / Sweep 102429.31 102468.93 102493.84 103242.28 Table 3: Comparison of compression algorithms on leaf offsets from UrbanCity, in terms of average bytes per sweep. There can be room for future work in entropy modeling the leaf offsets, but our current hypothesis is that since the intermediate octree nodes already encode the shared bits between points, the leaf offsets represent residual bits that can be considered “higher-frequency” artifacts (similar to residual frames in video compression), and are therefore harder to compress. A.2 Using a Range Image Representation We mention in Sec. 3.1 of the main paper that we designed a range image-based compression baseline. Towards this goal, we ﬁrst converted point cloud streams in UrbanCity and KITTI into range image representations, which store LiDAR packet data into a 2D matrix. We consider two possible range image representations. The ﬁrst contains dimensions Hlid ×Wazm, where the height dimension represents the separate laser ID’sof the LiDAR sensor, and the width dimension represents the discretized azimuth bins between -180 ◦and 180◦. Each pixel value represents the distance returned by the laser ID at the speciﬁc azimuth angle. Such a representation requires sufﬁcient auxiliary calibration and vehicle information in order to reconstruct the points in Euclidean space— for instance, a separate transform matrix per laser and velocity information to compensate for rolling shutter effects. We use this representation for UrbanCity because we have access to most required information; unfortunately, not every log contains detailed calibration or precise velocity information, requiring us to use approximations. The second representation simply projects the spatial coordinates of the point cloud sweep into the coordinate frame of the sensor, and does not require a map between laser ID and Euclidean space. Such an image contains dimensions Hpitch ×Wazm, where the height dimension now represents discretized pitch angles; each pixel value now represents the distance of a given point from the sensor frame at a given pitch and azimuth bin. We use this representation for our KITTI point clouds, since the dataset does not provide detailed laser calibration information. We explore both geometry-only and geometry + intensity representations. Spatial positions are encoded in the 8-bit R,G channels of the png image (16 bits total). If intensity is encoded, it is encoded in the B channel. We run H.264 on the png image sequence as our compression algorithm. We evaluate on the same reconstruction metrics: point-to-point Chamfer distance and point-to-plane PSNR (geometry), and F1 score (geometry + intensity). We show here in Fig. 6, that the results were uncompetitive—the range image representation under- performs other baselines and our approach on every evaluation metric. We observe that even the “lossless” representation (the right-most point on the curves) does not yield perfect reconstruction metrics. This can be surprising for the laser ID representation in UrbanCity. But we hypothesize that the errors come from approximations of the true calibration values (which are not obtainable for every log), as well as the velocity used in rolling shutter compensation—we found that small perturbations in these calibration values yield a large variance in reconstruction quality and metrics. 130 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 6: Bitrate vs. reconstruction quality curves on UrbanCity (top) and KITTI (bottom). From left-to-right: F1 with τgeo = 10cmand τint = 0(↑), point-to-point chamfer distance (↓), point-to-plane PSNR (↑). B Additional Architecture Details In this section we provide additional architecture details of our octree occupancy and intensity entropy models (Secs. 2.3 and 2.4 in main paper). We also provide architecture details of the models used in the ablation studies of the occupancy and intensity model (Tab. 1, Tab. 2 in main paper). B.1 Occupancy Entropy Model Ancestral Node Dependence: The context feature ci consists of the octree level of the current node (1– 16), spatial location of the node’s octant(x,y,z ), octant index of the node relative to its parent (0–8), and parent occupancy byte (0–255), as well as occupancy byte in the corresponding node in the previous octree (0–255 if exists, 0 otherwise). The initial feature extractor is a 4-layer MLP with fully-connected (fc) layers and intermediate ReLU activations. The hidden layer dimension is 128. Then, every aggregation round consists of a 2-layer fc/ReLU MLP with a 256-dimensional input (concatenating with the ancestor feature), and a hidden dimension of 128. We set the number of aggregation rounds, Kans, to 4. Temporal Octree Dependence: The top-down pass to generate h(t−1) j has essentially the same architecture as the ancestral node dependence module above. The one difference is that each context feature additionally includes the “ground-truth” occupancy byte of each node, since each node in sweep t−1 has already been decoded. Moreover, each hidden dimension is 64 instead of 128. Next, recall that the bottom-up aggregation pass has the following formulation: g(t−1) j = fagg,1(h(t−1) j + ∑ c∈child(j) fagg,2(g(t−1) c )) Here, fagg,2 is a 2-layer fc/ReLU MLP taking a 64-dim input and outputting a 32-dim intermediate embedding. fagg,1 is a 2-layer fc/ReLU MLP taking a (32 + 64)-dim embedding (child embedding + top-down embedding), and outputting a 64-dim embedding for the current node j. The bottom-up pass is run starting from the lowest level D(where there are no children) back up to level 0. Spatio-Temporal Aggregation and Entropy Header: Recall that the continuous convolution layer has the formulation hi = ∑ j∈N(i) σ(pj −pi)hj where N(i) is the i-th node’sk-nearest neighbors in sweep t−1, at the same octree level as node i, and pi is the 3D position of each node. Here, σis a learned kernel function, and it is parameterized by 14an MLP, inspired by [13]. The MLP contains 3 fc/ReLU layers (no ReLU in last layer), with output dimensions 16, 32, and 64 respectively. The continuous conv layer produces the warped feature g(t) i,st. The warped feature g(t) i,st and ancestral feature h(t) i are aggregated through a ﬁnal, 4-layer fc/ReLU MLP with hidden dim 128. The prediction header outputs a softmaxed, 256-dim vector of occupancy predictions. B.2 Intensity Entropy Model The input to the intensity entropy MLP consists of the k-nearest neighbor intensities in sweep t−1: {r(t−1) j }j∈N(i). We set k = 5. In addition to the raw intensity value, we include the following features per r(t−1) j : spatial (x,y,z ) position ∈R3, delta vector to current point ∈R3, and 1-D distance value. Hence each point contains an 8-dimensional feature. Each feature per r(t−1) j is then independently given to a 4-layer MLP, consisting of fc layers and ReLU activations. The dimension of each hidden layer is 128. Then, the koutput features are input to a continuous convolution layer to produce a single 128-dimensional embedding. The kernel function σof the continuous conv. is parameterized with the same MLP as the one used in spatio-temporal aggregation in the occupancy model. The ﬁnal predictor is a fc layer and softmax with a 256-dim. output. B.3 Ablation Study Architectures We ﬁrst describe the architectures of the occupancy ablation in Tab. 1 of the main paper. • O uses the past occupancy byte to model temporal dependence. The byte is taken from the corresponding node in the previous octree if it exists; if it does not, the feature is zeroed out. This past occupancy byte is then appended to the context feature ci (along with parent occupancy byte, octree level, etc.) and fed to the ancestral dependence module. There is no temporal octree dependence module or spatio-temporal aggregation; the ﬁnal prediction header is directly attached to the ancestral feature. • O,T includes the temporal octree dependence module, but removes the bottom-up pass. Hence the ﬁnal feature produced from this module is h(t−1) j (as opposed to g(t−1) j ). There does not exist a spatio-temporal aggregation module using continuous convolutions to produce an embedding for every nodei. Instead, we use a simpler “exact matching” heuristic similar to including the occupancy bytes— h(t−1) j will only be included as a feature for node iin sweep t, if node jcorresponds to the same octant in sweep (t−1) as node iin sweep t. If there is no exact correspondence, the feature is zeroed out. • O,T,B includes the full temporal octree dependence module, including the bottom-up pass to produce g(t−1) j . As with the above, we do not include our spatio-temporal aggregation module but rather use the exact matching heuristic to include g(t−1) j in the corresponding nodes iin sweep tonly if the correspondence exists. • O,T,B,CC includes our full model, including using spatio-temporal aggregation with continuous convolutions to produce an embedding feature for every node i. We now describe the architectures of the intensity ablation in Tab. 2 of the main paper. • MLP only utilizes context from one neighbor in sweep t−1. First, the nearest neighbor to node iis obtained in sweep t−1. We take the neighbor’s corresponding intensity, the delta vector to the current position ∈R3, and 1-D distance value as inputs, and feed it through a 4-layer fc/ReLU MLP and a ﬁnal softmax predictor head to output 256-dim probabilities. • CC contains the full intensity model with continuous convolutions. For architecture details see B.2. 150 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 0) Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 5) Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 5)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 5)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 0) Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 5) Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 5)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 5)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 7: Bitrate vs. F1 curves on UrbanCity (top three rows) and KITTI (bottom three rows). We report F1 across various spatial and intensity thresholds: τgeo ∈{5cm,10cm,15cm}and τint ∈{0,5,10}. 16C Additional Experiment Details C.1 Reconstruction Metrics In Sec. 3.3 of the main text, we report reconstruction quality in terms of three metrics: F1 score, point-to-point Chamfer Distance [6], and point-to-plane PSNR [49]. In the following, we explain each metric in detail. Let P= {(pi,ri)}N i=1 be an input LiDAR point cloud, where each pi ∈R3 denotes a point’s spatial coordinates andri ∈{0,..., 255}its intensity. Furthermore, let ˆP= {(ˆpj,ˆrj)}M j=1 be its reconstruction, where ˆpj and ˆrj are similarly deﬁned. Our ﬁrst metric is an F1 score that measures reconstruction quality in terms of both geometry and intensity: F1(P, ˆP) = 2 ×# true positives 2 ×# true positives + # false positives + # false negatives (6) where a reconstructed point (ˆpj,ˆrj) ∈ ˆPis a true positive if and only if there exists a point (pi,ri) ∈P such that ∥pi −ˆpj∥2 ≤τgeo and |ri −ˆrj|≤ τint. False positivesare the reconstructed points in ˆPthat are not true positives, and false negativesare the original points in Pfor which no reconstructed point is a true positive. In our experiments, we use τgeo = 10cm and τint = 0, and we report F1 as a function of overall bitrates; i.e., the number of bits to store p and r. We further report the F1 score for τgeo ∈{5cm,10cm,15cm}and τint ∈{0,5,10}in Fig. 7. Following the MPEG standards, we also use two standard metrics that measure reconstruction quality in terms of geometry only [ 25]. We report these metrics as a function of spatial bitrates; i.e., the number of bits to store p. The ﬁrst such metric measures the point-to-point error between the original point cloud Pand the reconstructed point cloud ˆP; this metric is often called the D1 error in the MPEG standards. In our paper, we report this metric as a symmetric Chamfer distance: CDsym(P, ˆP) = max { CD(P, ˆP),CD( ˆP,P) } (7) where CD(P, ˆP) = 1 |P| ∑ pi∈P min ˆpj∈ˆP ∥pi −ˆpj∥2 (8) The second metric measures the point-to-place error between the original point cloud Pand the reconstructed point cloud ˆP; this metric is often called the D2 error in the MPEG standards. In our paper, we report this metric in terms of its peak signal-to-noise ratio (PSNR): PSNR(P, ˆP) = 10 log10 3r2 max{MSE(P, ˆP),MSE(P, ˆP)} (9) where MSE(P, ˆP) = 1 |P| ∑ i((pi −ˆpi) ·ˆni)2 is the mean squared point-to-plane distance, ˆni is the normal vector on ˆpi, ˆpi = argminˆp∈ˆP∥pi −ˆp∥2 2 is pi’s nearest neighbor point in ˆP, and r is the peak constant value. We estimate the normal ni at each point pi ∈P using the Open3D function estimate_normals with k= 12 nearest neighbors [52], and we compute the normal ˆni corresponding to each point ˆpi ∈ ˆPby taking the normal of its nearest neighbor in the original point cloud P. Following the MPEG standard, for each dataset, we compute ras the maximum nearest neighbor distance among all point clouds in the dataset: r= max P max pi∈P min j̸=i ∥pi −pj∥2 (10) For UrbanCity, we use r= 98.69 and for SemanticKITTI, we use r= 59.70. For completeness, we also report the point-to-point error in terms of its peak signal-to-noise ratio and the point-to-plane error as a symmetric Chamfer distance in Fig. 8. C.2 Downstream Experiment Details In this section, we provide additional details for our downstream perception experiments. 170 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 40 60 80 100Point-to-Point PSNR Bitrate v. D1 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Plane Chamfer Distance Bitrate v. D2 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 40 60 80 100Point-to-Point PSNR Bitrate v. D1 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.1 0.2 0.3Point-to-Plane Chamfer Distance Bitrate v. D2 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 8: Bitrate vs. reconstruction curves on UrbanCity (top two rows) and KITTI (bottom two rows). We report point-to-point (D1) and point-to-plane (D2) errors in terms of Chamfer distances (left) and PSNR (right). C.2.1 Semantic Segmentation We use a modiﬁed version of the LiDAR semantic segmentation model described in [6]. Input Representation: Our model takes as input T bird’s eye view (“BEV”) occupancy grids of the past T input LiDAR point clouds {P(t−T+1),..., P(t)}, stacked along the height dimension (i.e., the z-axis). By treating the height dimension as multi-dimensional input features, we have a compact input representation on which we can use 2D convolutions [ 53]. Each voxel in the occupancy grids store the average intensity value of the points occupying its volume, or 0 if it contains no points. We use a region of interest of 160m ×160m ×5m centered on the ego-vehicle, T = 5 past LiDAR point clouds, and a voxel resolution of 0.15625cm, yielding an input volume x of size (T ×Z) ×W ×H = 160 ×1024 ×1024. Architecture Details: Our model architecture consists of two components: (1) a backbone feature extractor; and (2) a semantic segmentation head. The backbone feature extractor CNNBEV is a feature pyramid network based on the backbone architecture of [7]: fBEV = CNNBEV(x) (11) where fBEV ∈RCBEV×W/4×H/4 and CBEV = 256. 18The semantic segmentation head CNNsem consists of four 2D convolution blocks with 128 hidden channels 3, followed by a 1 ×1 convolution layer: fsem = CNNsem(fBEV) (12) where fsem ∈R(K×Z)×W/4×H/4 and Kis the number of classes plus an additional ignore class. To extract per-point predictions, we ﬁrst reshape fsem into a K×Z×W/4 ×H/4 logits tensor, then use trilinear interpolation to extract per-point K-dimensional logits, and ﬁnally apply softmax. Training Details: We use the cross-entropy loss to train our semantic segmentation model. For SemanticKITTI, we follow [8] and reweight the loss at each point by the inverse of the frequency of its ground truth class; this helps to counteract the effects of severe class imbalance. Moreover, we use data augmentation by randomly scaling the point cloud by s∼Uniform(0.95,1.05), rotating it by θ∼Uniform(−π/4,π/4), and reﬂecting it along the xand y-axes. We use the Adam optimizer [48] with a learning rate of 4e−4 and a batch size of 12, and we train until convergence. C.2.2 Object Detection We use a modiﬁed version of the LiDAR object detection model described in [6]. It largely follows the same architecture as our semantic segmentation model, with a few modiﬁcations to adapt it for object detection. We describe these modiﬁcations below. Architecture Details: Our object detection model consists of two components: (1) a backbone feature extractor; and (2) an object detection head. The backbone feature extractor here shares an identical architecture to that of the semantic segmentation model. The object detection head consists of four 2D convolution blocks with 128 hidden channels followed by a 1 ×1 convolution layer to predict a bounding box bi,k and detection score αi,k for every BEV pixel iand class k. Each bounding box bi,k is parameterized by (∆x,∆y,log w,log h,sin θ,cos θ), where (∆x,∆y) are the position offsets to the object’s center,(w,h) are the width and height of its bounding box, and θis its heading angle. To remove duplicate bounding boxes predictions, we use non-maximum suppression. Training Details: We use a combination of classiﬁcation and regression losses to train our de- tection model. In particular, for object classiﬁcation, we use a binary cross-entropy loss with online hard negative mining, where positive and negative BEV pixels are determined based on their distance to an object center [ 55]. For bounding box regression, we use a smooth ℓ1 loss on ∆x,∆y,log w,log h,sin θ,cos θ. We use the Adam optimizer [48] with a learning rate of 4e−4 and a batch size of 12, and we train until convergence. D Additional Qualitative Results In Fig. 9 and 10, we compare the reconstruction quality of our method versus Draco [2] and MPEG anchor [1]. Then, in Figs. 11, 12, and 13, we visualize results from semantic segmentation and object detection on SemanticKITTI and UrbanCity. As shown in these ﬁgures, our compression algorithm yields the best reconstruction quality at comparable or lower bitrates than the competing methods. E Change Log ArXiv v2: We updated our reconstruction metrics to use the standard MPEG deﬁnitions [ 25]. Furthermore, we added bitrate vs. F1 curves for a number of spatial and intensity thresholds. We also updated our Draco baseline to use its built-in attributes coder. 3Each 2D convolution block consists of a 3 ×3 convolution, GroupNorm [54], and ReLU. 19Oracle (UrbanCity): Bitrate 104.0  Ours: F1 92.9 Bitrate 10.0 Draco: F1 85.1 Bitrate 10.9  MPEG: F1 53.4 Bitrate 10.4 Figure 9: Qualitative results on UrbanCity. Points are colored by intensity. Oracle (KITTI): Bitrate 104.0  Ours: F1 90.8 Bitrate 5.6 Draco: F1 89.2 Bitrate 5.8  MPEG: F1 69.2 Bitrate 11.0 Figure 10: Qualitative results on SemanticKITTI. Points are colored by intensity. 20Oracle (KITTI): IOU 31.3 Bitrate 104.0  Ours: IOU 29.5 Bitrate 6.7 Draco: IOU 29.0 Bitrate 8.4  MPEG: IOU 26.3 Bitrate 13.0 Figure 11: Semantic segmentation results on SemanticKITTI. IOU is averaged over all classes. Oracle (KITTI): IOU 97.2 Bitrate 104.0  Ours: IOU 94.3 Bitrate 19.8 Draco: IOU 88.3 Bitrate 20.1  MPEG: IOU 85.2 Bitrate 20.5 Figure 12: Semantic segmentation results on UrbanCity. IOU is averaged over all classes. 21Oracle (KITTI): AP 90.6 Bitrate 104.0  Ours: AP 90.4 Bitrate 14.5 Draco: AP 89.4 Bitrate 16.6  MPEG: AP 91.2 Bitrate 17.6 Figure 13: Object detection results on UrbanCity. AP is averaged over the vehicle, pedestrian, and motorbike classes. 22",
      "meta_data": {
        "arxiv_id": "2011.07590v2",
        "authors": [
          "Sourav Biswas",
          "Jerry Liu",
          "Kelvin Wong",
          "Shenlong Wang",
          "Raquel Urtasun"
        ],
        "published_date": "2020-11-15T17:41:14Z",
        "pdf_url": "https://arxiv.org/pdf/2011.07590v2.pdf",
        "github_url": "https://github.com/cwi-dis/cwi-pcl-codec"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper presents MuSCLE, a novel learning-based compression algorithm for LiDAR sensor data streams. It exploits spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values. The core contribution is a deep conditional entropy model that unifies geometry and attribute (intensity) compression by considering coarse-level geometry and previous sweeps' information to model octree symbol probabilities. This method significantly reduces joint geometry and intensity bitrate by 7–17% on UrbanCity and 6–19% on SemanticKITTI datasets compared to prior state-of-the-art methods, while also demonstrating superior performance in downstream perception tasks at similar bitrates.",
        "methodology": "The proposed method first quantizes and encodes LiDAR point spatial coordinates into an octree representation, serialized into occupancy symbol and leaf offset/intensity bytestreams. A deep conditional entropy model, based on a 1st-order Markov assumption for temporal dependencies, is then applied. This model factorizes into two main parts: 1) An Occupancy Entropy Model, which predicts node occupancy probabilities by incorporating ancestral node dependence (recurrent network over octree paths) and temporal octree dependence (two-stream feature backbone with top-down and bottom-up passes on the previous sweep's octree). These features are aggregated using continuous convolutions to exploit spatio-temporal proximity. 2) An Intensity Entropy Model, which compresses 8-bit intensity values by leveraging temporal correlations between point intensities across consecutive timestamps, also using continuous convolutions on k-nearest neighbor intensities from the previous sweep. Finally, a lossless entropy coding algorithm (range coding) compresses the predicted probability distributions into the final bitstream. The models are trained end-to-end with cross-entropy loss.",
        "experimental_setup": "The method was evaluated on two large-scale datasets: UrbanCity (5000 training, 500 test sequences of 250 Velodyne HDL-64E LiDAR sweeps, ~80k points/sweep) and SemanticKITTI (43,552 Velodyne HDL-64E LiDAR sweeps, ~120k points/sweep). Baselines included OctSqueeze, Google's Draco, MPEG Anchor, and MPEG TMC13. Reconstruction quality was measured using F1 score (for joint geometry and intensity, with varying thresholds like τgeo = 10cm, τint = 0), point-to-point Chamfer distance (D1 error), and point-to-plane PSNR (D2 error). Performance on downstream perception tasks was also evaluated using mean IoU for semantic segmentation and Average Precision (AP) for object detection (vehicles, pedestrians, motorbikes). Octrees were constructed over a 400m³ region, with max depth varied from 11 to 16 to control bitrate-distortion tradeoffs. Models used 4 aggregation rounds and 5 nearest neighbors for continuous convolutions. Training was done in PyTorch with Horovod over 16 GPUs for 150,000 steps using Adam optimizer with a learning rate of 1e-4 and batch size 16.",
        "limitations": "The current method experiences loss only from D-dependent octree quantization. It assumes intensity values are bounded and discrete for lossless compression; continuous intensities would incur loss through discretization. Furthermore, the paper notes that leaf offsets in the octree representation were found not to contain meaningful patterns that could be exploited for compression, with off-the-shelf compression algorithms (zlib, LZMA, bzip2) failing to reduce their bitrate. While the 1st-order Markov assumption enables online processing, it might be a simplification of potentially longer temporal dependencies. An experimental baseline using range image representation was found to be uncompetitive, with errors hypothesized to arise from approximations of true calibration values and velocity information.",
        "future_research_directions": "The authors plan to extend the proposed method to jointly compress data streams from entire sensor suites, moving beyond just LiDAR. Additionally, they suggest that future research could explore entropy modeling for the leaf offsets, which proved difficult to compress with current methods, positing that they represent 'higher-frequency' artifacts.",
        "experimental_code": "The provided file 'cmake/merge_cmake_install.py' is a build utility script for modifying CMake installation files by removing debug/release specific conditional blocks. It does not contain any code related to the quantization, octree encoding, deep conditional entropy models (occupancy or intensity), continuous convolutions, range coding, or training described in the Method section.",
        "experimental_info": "No experimental settings or code directly implementing the described LiDAR compression method (octree quantization, deep conditional entropy models, range coding, training) could be extracted from the provided file 'cmake/merge_cmake_install.py'. This file is a utility for CMake build configuration."
      }
    },
    {
      "title": "PointMamba: A Simple State Space Model for Point Cloud Analysis",
      "abstract": "Transformers have become one of the foundational architectures in point cloud\nanalysis tasks due to their excellent global modeling ability. However, the\nattention mechanism has quadratic complexity, making the design of a linear\ncomplexity method with global modeling appealing. In this paper, we propose\nPointMamba, transferring the success of Mamba, a recent representative state\nspace model (SSM), from NLP to point cloud analysis tasks. Unlike traditional\nTransformers, PointMamba employs a linear complexity algorithm, presenting\nglobal modeling capacity while significantly reducing computational costs.\nSpecifically, our method leverages space-filling curves for effective point\ntokenization and adopts an extremely simple, non-hierarchical Mamba encoder as\nthe backbone. Comprehensive evaluations demonstrate that PointMamba achieves\nsuperior performance across multiple datasets while significantly reducing GPU\nmemory usage and FLOPs. This work underscores the potential of SSMs in 3D\nvision-related tasks and presents a simple yet effective Mamba-based baseline\nfor future research. The code will be made available at\n\\url{https://github.com/LMD0311/PointMamba}.",
      "full_text": "empty PointMamba: A Simple State Space Model for Point Cloud Analysis Dingkang Liang1∗, Xin Zhou1∗, Wei Xu1, Xingkui Zhu1, Zhikang Zou2, Xiaoqing Ye2, Xiao Tan2, Xiang Bai1† 1Huazhong University of Science & Technology,2Baidu Inc. {dkliang, xzhou03, xbai}@hust.edu.cn 89.31 89.17 89.04 86.1 86.2 94.32 93.39 92.43 91.91 91.2288.21 93.3 93.2 93.6 93.29 92.77 92.60 (a) Performance comparison (d) FLOPs comparison 50.52 2.33 3.21 0 10 20 30 40 50 60 70 80GPU memory (G) 256 512 1024 2048 4096 8192 16384 32768 65536  The length of point tokens Point-MAE PointMamba (ours) 24.9 × (c) GPU usage com parison Point-MAE (ECCV 22) ACT (ICLR 23) PointMamba(Ours) PointGPT (NeurIPS 23) 163.81 0.4913 180.92 14.81 0.25 5 100FPS w/ log scale The length of point tokens Point-MAE PointMamba (ours) 30.2 × (b ) Inference speed comparison 696.02 132.88 0 150 300 450 600 750 256 512 1024 2048 4096 8192 16384 32768 FLOPs (G) The length of point tokens Point-MAE PointMamba (ours) 5.2 × OOM ShapeNet-part OBJ-BG OBJ-ONLY PB-T50-RS ModelNet40 256 512 1024 2048 4096 8192 16384 32768 Figure 1: Comprehensive comparisons between our PointMamba and its Transformer-based counter- parts [33, 6, 11]. (a) Without bells and whistles, our PointMamba achieve better performance than the representative Transformer-based methods on the various point cloud analysis datasets. (b)-(d) The Transformer presents quadratic complexity, while our PointMamba has linear complexity. For example, with the length of point tokens increasing, we significantly reduce GPU memory usage and FLOPs and have a faster inference speed compared to the most convincing Transformer-based method, i.e., PointMAE [33]. Abstract Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose PointMamba , transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our method ∗Equal contribution. † Corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2402.10739v5  [cs.CV]  25 Nov 2024leverages space-filling curves for effective point tokenization and adopts an ex- tremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. The code will be made available at https://github.com/LMD0311/PointMamba. 1 Introduction Point cloud analysis is one of the fundamental tasks in computer vision and has a wide range of real-world applications [47, 55, 8], including robotics, autonomous driving, and augmented reality. It is a challenging task due to the intrinsic irregularity and sparsity of point clouds. To address the issues, there has been rapid progress in deep learning-based methods [38, 33, 41, 44], consistently pushing the performance to the new record. Recently, Transformers have achieved remarkable progress in point cloud analysis. The key to the Transformer is the attention mechanism, which can effectively capture the relationship of a set of points. By integrating self-supervised learning paradigms with fine-tuning for downstream tasks, these Transformer-based methods have achieved superior performance [60, 33, 42]. However, the complexity of attention mechanisms is quadratic, bringing significant computational cost, which is not friendly to low-resource devices. Thus, this naturally raises a question: how to design a simple, elegant method that operates with linear complexity, thereby retaining the benefits of global modeling for point cloud analysis? We note the recent advance of the State Space Models (SSMs). As a pioneer, the Structured State Space Sequence Model [17] (S4) has emerged as a promising class of architectures for sequence modeling thanks to its strong representation ability and linear-time complexity (achieved by elimi- nating the need to store the complete context). Another pioneer, Mamba [14], adopts time-varying parameters to the SSM based on S4, proposing an efficient hardware-aware algorithm to enable highly efficient training and inference with dynamic modeling. Recently, a few concurrent methods [71, 35] successfully transfer the 1D-sequence Mamba from NLP to 2D vision tasks (e.g., image classification and segmentation), achieving similar or surpass the Transformer counterpart [12] while significantly reducing memory usage. However, regarding the more complex, unstructured data, e.g., 3D point cloud, the effectiveness of Mamba remains unclear. The lack of early exploration of Mamba’s poten- tial for point cloud-related tasks hinders further development of its capabilities across the diverse range of applications in this domain. Inspired by this, this paper aims to unlock the potential of SSM in point cloud analysis tasks, discussing whether it can be a viable alternative to Transformers in this domain. Through a series of pilot experiments, we find that directly using the pioneering SSM, Mamba [14], can not achieve ideal performance. We argue that the main inherent limitation comes from the unidirectional modeling employed by the default Mamba, as the context is obtained by compressing the historical hidden state instead of through the interaction between each element. In contrast, the self-attention of the Transformer is invariant to the permutation of the input elements. Given the three-dimensional nature (e.g., unstructured and disordered) of point clouds, using a single scanning process often struggles to concurrently capture dependency information across various directions, which makes it difficult to construct global modeling for the RNN-like modes (e.g., Mamba). Therefore, we introduce a simple yet effective Point Cloud State Space Model (denoted as Point- Mamba  ) with global modeling and linear complexity. Specifically, to enable Mamba to capture the point cloud structure causally, we first use a point tokenizer to generate two types of point tokens via a point scanning strategy, employing two space-filling curves to scan key points from different directions. As a result, the unstructured 3D point clouds can be transformed into a regular sequence. The first type of token has local modeling capabilities through sequential encoding, with the latest token holding global sequence information. Consequently, the second type of token can achieve global modeling by containing global information that comes from the first type. Besides, we propose an extremely simple order indicator to maintain the distinct spatial characteristics of different scanning when training, preserving the integrity of the spatial information. 2To make the model as simple as possible, PointMamba only employs plain and non-hierarchical Mamba as the backbone to extract features for given serialized point tokens without bells and whistles. We demonstrate that PointMamba is very flexible in the pre-training paradigm, where we customize an MAE-like pertaining strategy to provide a good prior, which chooses a random serialization strategy from a pre-defined serialization bank to perform mask modeling. It facilitates the model to exact the general local relationships from different scanning perspectives, better matching the requirement of indirection modeling of mamba. Despite no elaborate or complex designs in the model, our PointMamba achieves superior performance on various point cloud analysis datasets (Fig. 1(a)). Besides the superior performance, thanks to the linear complexity of Mamba, we show the surprisingly low computational cost 2, as shown in Fig. 1(b)-(c). These notable results underscore the potential of SSM in 3D vision-related tasks. In conclusion, the contributions of this paper are twofold. 1) We introduce the first state space model for point cloud analysis, named PointMamba  , which features global modeling with linear complexity. Despite the absence of elaborate or complex structural designs, PointMamba demon- strates its potential as an optional model for 3D vision applications. 2) Our PointMamba exhibits impressive capabilities, including structural simplicity (e.g., vanilla Mamba), low computational cost, and knowledge transferability (e.g., support for self-supervised learning). 2 Related work 2.1 Point Cloud Transformers Vision Transformer (ViT) [ 12] has become one of the mainstream architectures in point cloud analysis tasks due to its excellent global modeling ability. Specifically, Point-BERT [ 60] and Point-MAE [38] introduce a standard Transformer architecture for self-supervised learning and is applicable to different downstream tasks. Several works further introduce GPT scheme [6], multi- scale [64, 61], and multi-modal [ 11, 42, 43] to guide 3D representation learning. On the other hand, some researchers [ 20, 68, 51] focus on modifying the Transformers for point clouds. The PCT [20] conducts global attention directly on the point cloud. Point Transformer [68] applies vector attention [67] to perform local attention between each point and its adjacent points. The later Point Transformer series [56, 55] further extends the performance and efficiency of the Transformer for different tasks. OctFormer [51] leverages sorted shuffled keys of octrees to partition point clouds and significantly improve efficiency and effectiveness. Standard Transformers can be smoothly integrated into autoencoders using an encoder-decoder design [25], which makes this structure ideal for pre-training and leads to significant performance improvements in downstream point cloud analysis tasks [60, 38, 6, 62, 63]. However, the attention mechanism has a time complexity of O(n2d), where n represents the length of the input token sequence and d represents the dimension of the Transformer. This implies that as the input sequence grows, the operational efficiency of the Transformer is significantly constrained. In this work, we focus on designing a simple State Space Model (SSM) for point cloud analysis without attention while maintaining the global modeling advantages of the Transformer. 2.2 State Space Models Linear state space equations [ 15, 18], combined with deep learning, offer a compelling approach for modeling sequential data, presenting an alternative to CNNs or Transformers. The Structured State Space Sequence Model [ 17] (S4) leverages a linear state space for contextualization and shows strong performance on various sequence modeling tasks, especially with lengthy sequences. To alleviate computational burden, HTTYH [ 19], DSS [ 22], and S4D [ 16] propose employing a diagonal matrix within S4, maintaining performance without excessive computational costs. The S5 [48] proposes a parallel scan and the MIMO SSM, enabling the state space model to be efficiently utilized and widely implemented. Recently, Mamba [14] introduced the selective SSM mechanism, a breakthrough achieving linear-time inference and effective training using a hardware-aware algorithm, garnering considerable attention. In the vision domain, Vision Mamba [35] compresses the visual 2Note that we removed the tokenizer of both Point-MAE and PointMamba , directly fed with a predefined sequence, to better illustrate the structural efficiency. 3representation through bidirectional state space models. VMamba [71] introduces the Cross-Scan Module, enabling 1D selective scanning in 2D images with global receptive fields. Besides, the great potential of Mamba motivates a series of work across diverse domains, including graph [50, 3], medical segmentation [36, 34], video understanding [30, 7] and generative models [29, 31]. To the best of our knowledge, there are limited works that study SSMs for point cloud analysis. In this work, we delve into the potential of Mamba in point cloud analysis and propose PointMamba, which achieves superior performance and significantly reduces computational costs. 3 Preliminaries State Space Model. Drawing inspiration from control theory, the State Space Model (SSM) represents a continuous system that maps a state xt to yt through an implicit latent state ht ∈ RN . To integrate SSMs into deep models, S4 [17] defines the system with four parameters (A, B, C, and sampling step size ∆). The sequence-to-sequence transformation is defined as: ht = Aht−1 + Bxt, y t = Cht + Dxt, (1) where C ∈ R1×N is a project parameter, and D ∈ R1×N represents a residual connection. The parameters A, B are defined using the zero-order hold (ZOH) discretization rule: A ∈ RN×N = exp(A∆), B ∈ RN×1 = (A∆)−1 (exp(A∆) − I) · ∆B. (2) However, the parameter (A, B, C, ∆) are fixed across all time steps due to the Linear Time-Invariant (LTI) property of SSMs, which limits their capacity to handle varied input sequences. Recently, Selective SSM (S6) considers parameters B, C, ∆ as functions of the input, effectively transforming the SSM into a time-variant model. Our PointMamba adopts a hardware-aware imple- mentation [14] of S6, showing linear complexity and strong sequence modeling capability. Space-filling curve . Space-filling curves are paths that traverse every point within a higher- dimensional discrete space while maintaining spatial proximity to a certain degree. Mathematically, they can be defined as a bijective function Φ : Z → Z3 for point clouds. Our PointMamba focuses on the Hilbert space-filling curve [ 27] and its transposed variant (called Trans-Hilbert), both of which are recognized for effectively preserving locality, ensuring that data points close inZ space remain close after transformation to Z3. We note that some methods [ 55, 51] utilize space-filling curves to partition the point cloud for capturing spatial contexts, whereas our work mainly focuses on transferring the point clouds to serialization-based sequences and combine with Mamba to implement global modeling. The motivation and objective are different. 4 PointMamba This paper aims to design a simple yet solid Mamba-based Point cloud analysis method. The pipeline of our method is shown in Fig. 2. Starting with an input point cloud, we first sample the key points via Farthest Point Sampling (FPS). Then, a simple space-scanning strategy is applied to reorganize these points, resulting in serialized key points. Under a KNN and lightweight PointNet [ 40], we obtain the serialized point tokens. Finally, the entire sequence is subsequently processed by a plain, non-hierarchical encoder structure composed of several stacked Mamba blocks. Besides, to provide a good prior for PointMamba, we propose a serialization-based mask modeling paradigm, which randomly chooses a space-filling curve for serialization and mask, as shown in Fig. 4. 4.1 The structure of PointMamba In this section, we introduce the structure of our PointMamba. The goal of this paper is to provide a simple yet solid Mamba baseline for point cloud analysis tasks and explore the potential of plain and non-hierarchical Mamba. Thus, in the spirit of Occam’s razor, we make the structure as simple as possible without any complex or elaborate design. Point scanning strategy. Building on the pioneer works [60, 38], we first utilize the Farthest Point Sampling (FPS) to select the key points. Specifically, given an input point cloud P ∈ RM×3, where M is the number of points, the FPS is applied to sample n key points from the original point cloud 4Vanilla Mamba  block Selective SSM DWConv LN Mamba block 𝜎𝜎 𝜎𝜎 Hilbert Trans-Hilbert FPS … … Token embedding layer … … 𝑁𝑁 × Task head KNN Order indicator … … FPS: Farthest Point Sampling 𝜎𝜎 : SiLU ⊕ Scale ⨀ Shift 𝛾𝛾ℎ′ 𝛽𝛽ℎ′ Order indicator ⊕ Shift ⨀ Scale 𝛾𝛾ℎ 𝛽𝛽ℎ Linear Linear Linear Figure 2: The pipeline of our PointMamba. It is simple and elegant, without bells and whistles. We first utilize Farthest Point Sampling (FPS) to select the key points. Then, we propose to utilize two types of space-filling curves, including Hilbert and Trans-Hilbert, to generate the serialized key points. Based on these, the KNN is used to form point patches, which will be fed to the token embedding layer to generate the serialized point tokens. To indicate the tokens generated from which space-filling curve, the order indicator is proposed. The encoder is extremely simple, consisting of N× plain and non-hierarchical Mamba blocks. P, resulting in p ∈ Rn×3. In general, the order of the sampling key points p is random, without specific order. This is not a significant problem for the previous Transformer-based methods, as the Transformer is order-invariant when processing sequence data: in the self-attention mechanism, each element at a given position can interact with all other elements in the sequence through attention weights. However, for the selective state space model, i.e., Mamba, we argue that it is hard to model the unstructured point clouds due to the unidirectional modeling. Thus, we propose to leverage the space-filling curves to transform the unstructured point clouds into a regular sequence. Specifically, we choose two representative space-filling curves to scan the key points: the Hilbert curve [27] and its transposed variant, denoted as Trans-Hilbert. Compared with the random sequence, space-filling curves like the Hilbert curve can preserve spatial locality well, i.e., along the scanned 1D serialized point sequence, adjacent key points often have geometrically close positions in 3D space. We argue that this property ensures that the spatial relationships between points are largely maintained, which is crucial for accurate feature representation and analysis in point cloud data. As a complementary, the Trans-Hilbert performs similarly but scans from different clues, which can provide diverse perspectives on spatial locality. By applying Hilbert and Trans-Hilbert to the key points, we obtain two different point serializations, ph and ph′, which will be used to construct point tokens. Point tokenizer . After obtaining the two serialized key points ph and ph′, we then utilize the KNN algorithm to select k nearest neighbors for each key point, forming n token patches Th ∈ Rn×k×3 and Th′ ∈ Rn×k×3 with patch size k. To aggregate local information, points within each patch are normalized by subtracting the key point to obtain relative coordinates. We map the unbiased local patches to feature space using a lightweight PointNet [40] (point embedding layer), obtaining serialized point tokens Eh 0 ∈ Rn×C and Eh′ 0 ∈ Rn×C, where the former is the Hilbert-based and the latter is Trans-Hilbert-based. Order indicator. Directly fed the two type serialized point tokens Eh 0 ∈ Rn×C and Eh′ 0 ∈ Rn×C into Mamba encoder might cause confusion as Eh 0 and Eh′ 0 actually share the same center but with different order. Maintaining the distinct characteristics of these different scanning strategies is important for preserving the integrity of the spatial information. Thus, we propose an extremely simple order indicator to indicate the scanning strategy used. Specifically, the proposed order indicator performs the linear transformation to transfer features into different latent spaces. The formulation can be written as follows: Zh 0 = Eh 0 ⊙ γh + βh, Zh′ 0 = Eh′ 0 ⊙ γh′ + βh′, (3) where γh/γh′ ∈ RC and βh/βh′ ∈ RC refer to the scale and shift factors, respectively. ⊙ is the dot product. We then concat Zh 0 and Zh′ 0 , resulting in Z0 ∈ R2n×C. 5Choose Token EmbeddingOrder GuiderOr Point cloud Hilbert Trans. Hilbert Point center … … Vanilla Mamba encoder … Vanilla Mamba decoder Autoencoder pre-training … Pred GT Loss Serialization FPS KNN Choose Order indicatoror Point cloud Hilbert Trans-Hilbert Point center … … Vanilla Mamba encoder … Vanilla Mamba decoder Autoencoder pre-training … Pred GT Loss Serialization FPS KNN Token embedding layer Figure 4: The details of our proposed serialization-based mask modeling. During the pre-training, we randomly choose one space-filling curve to generate the serialized point tokens for mask modeling, and different serialized point tokens have different order indicators. Mamba encoder. After obtaining the token Z0, we will feed it into the encoder, containing N × Mamba block, to extract the feature. Specifically, for each Mamba block, layer normalization (LN), Selective SSM, depth-wise convolution [10] (DW), and residual connections are employed. A standard Mamba layer is shown in Fig. 2, and the output can be summarized as follows: Z′ l−1 = LN (Zl−1) , Z′ l = σ \u0000 DW \u0000 Linear \u0000 Z′ l−1 \u0001\u0001\u0001 Z′′ l = σ \u0000 Linear \u0000 Z′ l−1 \u0001\u0001 , Zl = Linear (SelectiveSSM (Z′ l) ⊙ Z′′ l ) + Zl−1 (4) Serialized point tokens 𝒁!\"from HilbertSerialized point tokens 𝒁!\"!from Trans-Hilbert Global information… …Modelingdirection: Figure 3: An intuitive illustration of global mod- eling from PointMamba. Zl ∈ R2n×C is the output of the l-th block, and σ indicates SiLU activation [ 26]. The SelectiveSSM is the key to the Mamba block, with a detailed description in Sec. 3. To better understand why the proposed PointMamba has global modeling capacity, we provide an intuitive visualization. As shown in Fig. 3, after model- ing the first group of point tokens (i.e., Hilbert- based), the accumulated global information can improve the serialization process for the next set of tokens (i.e., Trans-Hilbert-based). This mech- anism ensures that each serialized point in the Trans-Hilbert sequence is informed by the entire history of the previously processed Hilbert sequence, thereby enabling a more contextually rich and globally aware modeling process. More discussions can be found in Appendix A.2. In our study, we show that even a very simple Mamba block without specific designs, our Point- Mamba can surpass the various Transformer-based point cloud analysis methods. 4.2 The serialization-based mask modeling One intriguing characteristic of Transformers-based methods [ 33, 60, 6] is their improved perfor- mance using the pre-training scheme, especially mask modeling [25]. In this paper, considering the unidirectional modeling of Mamba, we customize a simple yet effective serialization-based mask modeling paradigm, as shown in Fig. 4. Specifically, after obtaining the key points, we randomly choose Hilbert or Trans-Hilbert curve to implement serialization in each iteration, resulting in serialization-based key points, i.e., ph and ph′, are obtained from Hilbert and Trans-Hilbert, respectively. Such a scheme allows the model to exact the local relationships from different scanning clues. Then, the KNN and the token embedding layer are used to generate the point tokens. To discriminate the point tokens serialized from which space-filling curves, we apply the order indicator to the point tokens, where different serialized point tokens have different order indicators, which are similar to the mentioned Eq. 3. Next, we randomly mask the serialization-based point tokens with a high ratio of 60%. Then, an asymmetric autoencoder, consisting of several vanilla Mamba blocks, is employed to extract the point feature, and the final layer of the autoencoder utilizes a simple prediction head for reconstruction. To reconstruct masked point patches in coordinate space, we employ a linear head to project the masked token to the shape 6Table 1: Object classification on the ScanObjectNN dataset [49]. We evaluate PointMamba on three variants, with PB-T50-RS being the most challenging. Overall accuracy (%) is reported. Param. denotes the number of tunable parameters during training. † indicates that using simple rotational augmentation [11] for training. Methods Reference Backbone Param. (M) ↓ FLOPs (G) ↓ OBJ-BG ↑ OBJ-ONLY ↑ PB-T50-RS ↑ Supervised Learning Only PointNet [40] CVPR 17 - 3.5 0.5 73.3 79.2 68.0 PointNet++ [41] NeurIPS 17 - 1.5 1.7 82.3 84.3 77.9 PointCNN [32] NeurIPS 18 - 0.6 0.9 86.1 85.5 78.5 DGCNN [52] TOG 19 - 1.8 2.4 82.8 86.2 78.1 PRANet [9] TIP 21 - - - - - 81.0 MVTN [23] ICCV 21 - 11.2 43.7 - - 82.8 PointNeXt [44] NeurIPS 22 - 1.4 1.6 - - 87.7 PointMLP [37] ICLR 22 - 13.2 31.4 - - 85.4 RepSurf-U [45] CVPR 22 - 1.5 0.8 - - 84.3 ADS [28] ICCV 23 - - - - - 87.5 Training from pre-training (Single-Modal) Point-BERT [60] CVPR 22 Transformer 22.1 4.8 87.43 88.12 83.07 MaskPoint [33] CVPR 22 Transformer 22.1 4.8 89.30 88.10 84.30 Point-MAE [38] ECCV 22 Transformer 22.1 4.8 90.02 88.29 85.18 Point-M2AE [64] NeurIPS 22 Transformer 15.3 3.6 91.22 88.81 86.43 PointDif [69] CVPR 24 Transformer - - 93.29 91.91 87.61 Point-MAE+IDPT [63] ICCV 23 Transformer 1.7 7.2 91.22 90.02 84.94 Point-MAE+DAPT [70] CVPR 24 Transformer 1.1 5.0 90.88 90.19 85.08 Point-MAE† [38] ECCV 22 Transformer 22.1 4.8 92.77 91.22 89.04 PointGPT-S† [6] NeurIPS 23 Transformer 29.2 5.7 93.39 92.43 89.17 PointMamba† (ours) - Mamba 12.3 3.1 94.32 92.60 89.31 Training from pre-training (Cross-Modal) ACT† [11] ICLR 23 Transformer 22.1 4.8 93.29 91.91 88.21 Joint-MAE [21] IJCAI 23 Transformer - - 90.94 88.86 86.07 I2P-MAE† [65] CVPR 23 Transformer 15.3 - 94.15 91.57 90.11 RECON† [42] ICML 23 Transformer 43.6 5.3 95.18 93.29 90.63 of the masked input points. The Chamfer Distance [ 13] is then used as the reconstruction loss to recover the coordinates of the points in each masked point patch. We demonstrate that with such a simple serialization-based mask modeling paradigm, Point- Mamba can easily achieve superior performance. 5 Experiments 5.1 Implementation details SSM is new to 3D point cloud analysis, with no existing works detailing the specific implementation. To handle different resolutions of the input point cloud, we divide them into different numbers of patches with a linear scaling (e.g.,M = 1024 input points are divided inton = 64 point patches), with each patch containing k = 32 points determined by the KNN algorithm. The PointMamba encoder has N = 12 vanilla Mamba blocks, each Mamba block featuring C = 384 hidden dimensions. For the pre-training, we utilize ShapeNetCore [5] as the dataset, following previous methods [60, 38, 6]. In addition, we utilize 4 × Mamba blocks as the decoder to reconstruct the masked point clouds. 5.2 Compared with Transformer-based counterparts This paper aims to unlock the potential of Mamba in point cloud tasks, discussing whether it can be a viable alternative to Transformers. Thus, in the following experiments, we mainly compare with the state-of-the-art vanilla Transformer-based point cloud analysis methods. Real-world object classification on ScanObjectNN. ScanObjectNN [ 49] is a challenging 3D dataset comprising about 15,000 objects across 15 categories, scanned from real-world indoor scenes with cluttered complexity backgrounds. As shown in Tab. 1, we conduct experiments on three versions of ScanObjectNN (i.e., OBJ-BG, OBJ-ONLY , and PB-T50-RS), each with increasing complexity. When compared with the most convincing Transformer-based method, i.e., Point- 7Table 2: Classification on ModelNet40 [ 57]. Overall accuracy (%) is reported. The results are obtained from 1024 points without voting. Methods Param. (M) ↓ FLOPs (G)↓ OA (%)↑ Supervised Learning Only PointNet [40] 3.5 0.5 89.2 PointNet++ [41] 1.5 1.7 90.7 PointCNN [32] 0.6 - 92.2 DGCNN [39] 1.8 2.4 92.9 PointNeXt [44] 1.4 1.6 92.9 PCT [20] 2.9 2.3 93.2 OctFormer [51] 3.98 31.3 92.7 with Self-supervised pre-training Point-BERT [60] 22.1 2.3 92.7 MaskPoint [33] 22.1 2.3 92.6 Point-M2AE [64] 12.8 4.6 93.4 Point-MAE [38] 22.1 2.4 93.2 PointGPT-S [6] 29.2 2.9 93.3 ACT [11] 22.1 2.4 93.6 PointMamba (ours) 12.3 1.5 93.6 Table 3: Few-shot learning on ModelNet40 [ 57]. Overall accuracy (%)±the standard deviation (%) without voting is reported. Methods 5-way 10-way 10-shot 20-shot 10-shot 20-shot Supervised Learning Only PointNet [40] 52.0 ±3.8 57.8±4.9 46.6±4.3 35.2±4.8 PointNet-CrossPoint [1] 90.9±1.9 93.5±4.4 84.6±4.7 90.2±2.2 DGCNN [52] 31.6 ±2.8 40.8±4.6 19.9±2.1 16.9±1.5 DGCNN-CrossPoint [1] 92.5±3.0 94.9±2.1 83.6±5.3 87.9±4.2 with Self-supervised pre-training Point-BERT [60] 94.6 ±3.1 96.3±2.7 91.0±5.4 92.7±5.1 MaskPoint [33] 95.0 ±3.7 97.2±1.7 91.4±4.0 93.4±3.5 Point-MAE [38] 96.3 ±2.5 97.8±1.8 92.6±4.1 95.0±3.0 Point-M2AE [64] 96.8 ±1.8 98.3±1.4 92.3±4.5 95.0±3.0 PointGPT-S [6] 96.8 ±2.0 98.6±1.1 92.6±4.6 95.2±3.4 ACT [11] 96.8 ±2.3 98.0±1.4 93.3±4.0 95.6±2.8 PointMamba (ours) 96.9±2.0 99.0±1.1 93.0±4.4 95.6±3.2 MAE [33], PointMamba surpasses it by 1.55%, 1.38% and 0.27% on OBJ-BG, OBJ-ONLY , and PB-T50-RS respectively while using less computational costs. Besides, we also outperform the SOTA PointGPT-S [6] by 0.93%, 0.17%, 0.14% across three variants on a comparable scale setting. Note that our method follows Occam’s Razor, without auxiliary tasks like generation during fine-tuning used in PointGPT [6]. Furthermore, compared to cross-modal learning methods [ 11, 42] that use additional training data (cross-modal information) or teacher models, which is not a fair comparison, our PointMamba still maintains highly competitive. We mainly want to introduce a new Mamba- based point cloud analysis methods paradigm. Although using some complex designs can bring improvement, they might be heuristics. More importantly, these heuristic designs will decrease the objectivity of the evaluation of our method. Synthetic object classification on ModelNet40. ModelNet40 [57] is a pristine 3D CAD dataset consisting of 12,311 clean samples across 40 categories. As shown in Tab. 2, we report the over- all accuracy without adopting the voting strategy. The proposed PointMamba achieves the best results compared with various self-supervised Transformer-based methods [33, 60, 6]. In particular, PointMamba surpasses Point-MAE [ 33] and PointGPT-S [ 6] by 0.4% and 0.3% respectively. It is worth noting that the single-modal-learned PointMamba achieves comparable results with the cross-modal-based ACT [11] while significantly reducing parameters and FLOPs about 44% and 38%, respectively. Additionally, PointMamba demonstrates competitive performance against elaborately designed Transformer models like OctFormer [51]. Table 4: Part segmentation on the ShapeNet- Part [58]. The mIoU for all classes (Cls.) and for all instances (Inst.) are reported. Methods Cls. mIoU (%) ↑ Inst. mIoU (%)↑ Supervised Learing Only PointNet [40] 80.39 83.7 PointNet++ [41] 81.85 85.1 DGCNN [52] 82.33 85.2 APES [54] 83.67 85.8 with Self-supervised pre-training Transformer [60] 83.4 85.1 OcCo [60] 83.4 85.1 MaskPoint [33] 84.6 86.0 Point-BERT [60] 84.1 85.6 Point-MAE [38] 84.2 86.1 PointGPT-S [6] 84.1 86.2 ACT [11] 84.7 86.1 PointMamba (ours) 84.4 86.2 Few-shot learning. We further conduct few- shot experiments on ModelNet40 [57] to demon- strate our few-shot transfer ability. Consistent with prior studies [ 60], we utilize the \" n-way, m-shot\" setup, where n ∈ {5, 10} denotes the category count and m ∈ {10, 20} represents the samples per category. Following standard proce- dure, we carry out 10 separate experiments for each setting and reported mean accuracy along with the standard deviation. As indicated in Tab. 3, our PointMamba shows competitive re- sults with limited data, e.g., +1.0% mean ac- curacy compared to the cross-modal method ACT [11] on the 5-way 20-shot split. Part segmentation on ShapeNetPart. Part seg- mentation on ShapeNetPart [58] is a challenging task that aims to predict a more detailed label for each point within a sample. As shown in Tab. 4, we report mean IoU (mIoU) for all classes (Cls.) 8(d) w/ Selective SSM (ours) DWConv LN 𝜎 𝜎 Linear Linear Linear (c) w/ MLP DWConv LN 𝜎 𝜎 Linear Linear Linear DWConv LN 𝜎 Linear Linear Linear (b) w/ Attention(a) w/ Identity Selective SSM DWConv LN 𝜎 𝜎 Linear Linear Linear MLPAttention Figure 5: Different variant of PointMamba. (a) Directly removing the SSM part. (b) Replacing SSM with attention. (c) Replacing SSM with MLP. (d) Ours PointMamba with Selective SSM. Table 5: The effect of each component. Hilbert Trans-Hilbert Order indicator OBJ-BG OBJ-ONLY Random - 92.60 90.18 ✓ - - 92.94 91.05 - ✓ - 93.46 91.74 ✓ ✓ - 93.80 91.91 ✓ ✓ ✓ 94.32 92.60 Table 6: The effect of different scanning curves. Scanning curve OBJ-BG OBJ-ONLY Random 92.60 90.18 Hilbert and Trans-Hilbert 94.32 92.60 Z-order and Trans-Z-zorder 93.29 90.36 Hilbert and Z-order 93.29 90.88 Trans-Hilbert and Trans-Z-order 93.29 91.91 Table 7: The effect of Selective SSM. Setting Param. OBJ-BG OBJ-ONLY w/ Identity 11.4 93.80 91.57 w/ Attention 39.8 92.77 91.22 w/ MLP 18.5 93.29 91.22 w/ Selective SSM 12.3 94.32 92.60 Table 8: The effect of Order indicator. Setting OBJ-BG OBJ-ONLY None 93.80 91.91 Using the same indicator 93.29 90.19 Using different indicator 94.32 92.60 and all instances (Inst.). Our PointMamba model demonstrates highly competitive performance compared to the Transformer-based counterparts [33, 6, 60]. These impressive results further prove the potential of SSM in the point cloud analysis tasks. 5.3 Analysis and ablation study To investigate the architecture design, we conduct ablation studies on ScanObjectNN [49] with both pre-training and fine-tuning. Default settings are marked in gray . The structural efficiency. We first discuss the efficiency of our method. To fully explore the potential of processing the long point tokens (sequence), we gradually increase the sequence length until the GPU (NVIDIA A800 80GB) memory explodes. The comprehensive efficiency comparisons are present in Fig. 1(b)-(d), where Compared with the most convincing Transformer-based method [33], our PointMamba demonstrates significantly improved inference speed and reduce the GPU usage and FLOPs, especially when facing the long sequence. For example, when the length increases to more than 32,768, we outperform PointMAE by 30.2×, 24.9×, and 5.2× in terms of inference speed, GPU memory, and FLOPs, respectively. More importantly, even presenting impressive efficiency, we still achieve impressive performance on various point cloud analysis datasets. The effect of each component. We then study the effectiveness of the proposed components of PointMamba as shown in Tab. 5. We can make the following observations: 1) Directly utilizing random serialization, PointMamba only achieves 92.26% and 90.18% overall accuracy on OBJ-BG and OBJ-ONLY , respectively. It is reasonable as Mamba is hard to model the unstructured point clouds due to its unidirectional modeling. 2) By introducing the locality-preserved Hilbert or Trans- 9Hilbert scanning, PointMamba’s ability to capture sequence information is enhanced, leading to performance improvements compared to random serialization. Further applying both Hilbert and Trans-Hilbert scanning curves, PointMamba surpasses the random serialization by 1.20% and 1.73% on two datasets, respectively. 3) By using the order indicator to maintain the distinct characteristics of the two different scanning strategies, we achieve notable improvement, resulting in 94.32% and 92.60% on OBJ-BG and OBJ-ONLY , respectively. Note that the order indicator is extremely light (only 1.5k parameters), which will not introduce additional computational costs. The effect of different scanning curves. We further explore the effect of using different scanning curves to construct serialized point tokens. Specifically, we select two widely used space-filling curves, including Hilbert and Z-order, along with their transposed variants, i.e., Trans-Hilbert and Trans-Z- order. As listed in Tab. 6, we empirically find that serializing point clouds with space-filling curves scanning can achieve better performance compared to random sequences. We argue that scanning sequences along a specific pattern of spatial locations offers a more logical sequence modeling order for SSM. We choose the combination of Hilbert and Trans-Hilbert for PointMamba due to their superior locality-preserving properties. The effect of Selective SSM. The key of S6 models or Mamba [14] is the SSM with the selective mechanism. We prove that as a unidirectional modeling method, SSM can be analogous to masked self-attention, ensuring each position can only attend to previous positions (the detailed proof can be found in the Appendix A.1). Thus, as shown in Tab. 7, we analyze the effect of selective SSM by removing it (i.e., identity setting) or replace with masked self-attention or MLP (an illustration is shown in Fig. 5). Compared with the identity setting, the selective SSM brings notable improvement, indicating the effectiveness of introducing global modeling from SSM. Note that while a very recent method, MambaOut [59], thinks the SSM of Mamba might negatively impact image classification tasks, our findings demonstrate that this is not the case for point cloud analysis tasks. Another interesting thing is that when Selective SSM is replaced with masked self-attention, the performance is even lower than that of the identity setting. We argue the main reason is that masked self-attention is hard to combine with the Gated MLP [46] used in default Mamba, leading to optimized difficulty, which might need to be explored in the future. Analysis on order indicator. This part analyzes the effect of the order indicator. PointMamba applies Hilbert and Trans-Hilbert to recognize the key points, obtaining two types of serialized point tokens Eh 0 and Eh′ 0 . The order indicator is used to indicate the scanning strategy. As shown in Tab. 8, using two different order indicators can improve 1.20% and 1.03% compared to no indicator on OBJ-BG and OBJ-ONLY , respectively. However, using the same order indicator for both types of sequences without distinguishing between different scanning strategies does not yield positive results. 5.4 Limitation Although PointMamba achieves promising results, there are some limitations: 1) We only focus on the point cloud analysis task in this paper while designing a unified Mamba-based foundation model for various 3D vision tasks (e.g., 3D object classification/detection/segmentation) is a more appealing direction. 2) We only use the point clouds as training data while combining them with 2D images or language knowledge to improve the performance, which is also worthy of exploration. We left these in our future work. 6 Conclusion In this paper, we present an elegant, simple Mamba-based method named PointMamba for point cloud analysis. PointMamba utilizes a space-filling curve-based point tokenizer and a plain, non- hierarchical Mamba architecture to achieve global modeling with linear complexity. Despite its structural simplicity, PointMamba delivers state-of-the-art performance across various datasets, significantly reducing computational costs in terms of GPU memory and FLOPs. PointMamba success highlights the potential of SSMs, particularly Mamba, in handling the complexities of point cloud data. As a newcomer to point cloud analysis, PointMamba is a promising option for constructing 3D vision foundation models, and we hope it can offer a new perspective for the field. 10References [1] Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake, Amaya Dharmasiri, Kanchana Thilakarathna, and Ranga Rodrigo. Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022. [2] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024. [3] Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space models. In Proc. of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 119–130, 2024. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proc. of Advances in Neural Information Processing Systems, 2020. [5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. [6] Guangyan Chen, Meiling Wang, Yi Yang, Kai Yu, Li Yuan, and Yufeng Yue. Pointgpt: Auto-regressively generative pre-training from point clouds. In Proc. of Advances in Neural Information Processing Systems, 2023. [7] Guo Chen, Yifei Huang, Jilan Xu, Baoqi Pei, Zhe Chen, Zhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, and Limin Wang. Video mamba suite: State space model as a versatile alternative for video understanding. arXiv preprint arXiv:2403.09626, 2024. [8] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. V oxelnext: Fully sparse voxelnet for 3d object detection and tracking. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2023. [9] Silin Cheng, Xiwu Chen, Xinwei He, Zhe Liu, and Xiang Bai. Pra-net: Point relation-aware network for 3d point cloud analysis. IEEE Transactions on Image Processing, 30:4436–4448, 2021. [10] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2017. [11] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? In Proc. of Intl. Conf. on Learning Representations, 2022. [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proc. of Intl. Conf. on Learning Representations, 2021. [13] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstruction from a single image. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2017. [14] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In Conference on Language Modeling, 2024. [15] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. In Proc. of Advances in Neural Information Processing Systems, 2020. [16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and initialization of diagonal state space models. In Proc. of Advances in Neural Information Processing Systems, 2022. [17] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proc. of Intl. Conf. on Learning Representations, 2021. [18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Proc. of Advances in Neural Information Processing Systems, 2021. [19] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your hippo: State space models with generalized orthogonal basis projections. In Proc. of Intl. Conf. on Learning Representations, 2022. [20] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud transformer. Computational Visual Media, 2021. [21] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzhi Li, and Pheng-Ann Heng. Joint-mae: 2d-3d joint masked autoencoders for 3d point cloud pre-training. In Proc. of Intl. Joint Conf. on Artificial Intelligence, 2023. [22] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Proc. of Advances in Neural Information Processing Systems, 2022. 11[23] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view transformation network for 3d shape recognition. In Porc. of IEEE Intl. Conf. on Computer Vision, 2021. [24] Xu Han, Yuan Tang, Zhaoxuan Wang, and Xianzhi Li. Mamba3d: Enhancing local features for 3d point cloud analysis via state space model. In Proc. of ACM Multimedia, 2024. [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022. [26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [27] David Hilbert. Über die stetige abbildung einer linie auf ein flächenstück. Dritter Band: Analysis · Grundlagen der Mathematik· Physik Verschiedenes: Nebst Einer Lebensgeschichte, 1935. [28] Cheng-Yao Hong, Yu-Ying Chou, and Tyng-Luh Liu. Attention discriminant sampling for point clouds. In Porc. of IEEE Intl. Conf. on Computer Vision, 2023. [29] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model. In Proc. of European Conference on Computer Vision, 2024. [30] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. In Proc. of European Conference on Computer Vision, 2024. [31] Shufan Li, Harkanwar Singh, and Aditya Grover. Mamba-nd: Selective state space modeling for multi- dimensional data. arXiv preprint arXiv:2402.05892, 2024. [32] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Proc. of Advances in Neural Information Processing Systems, 2018. [33] Haotian Liu, Mu Cai, and Yong Jae Lee. Masked discrimination for self-supervised learning on point clouds. In Proc. of European Conference on Computer Vision, 2022. [34] Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Cheng Li, Yong Liang, Guangming Shi, Yizhou Yu, Shaoting Zhang, et al. Swin-umamba: Mamba-based unet with imagenet-based pretraining. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 615–625. Springer, 2024. [35] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model. In Proc. of Advances in Neural Information Processing Systems, 2024. [36] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722, 2024. [37] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. In Proc. of Intl. Conf. on Learning Representations, 2022. [38] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In Proc. of European Conference on Computer Vision, 2022. [39] Anh Viet Phan, Minh Le Nguyen, Yen Lam Hoang Nguyen, and Lam Thu Bui. Dgcnn: A convolutional neural network over large-scale labeled graphs. Neural Networks, 2018. [40] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2017. [41] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Proc. of Advances in Neural Information Processing Systems, 2017. [42] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. In Proc. of Intl. Conf. on Machine Learning, 2023. [43] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In ECCV, 2024. [44] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In Proc. of Advances in Neural Information Processing Systems, 2022. [45] Haoxi Ran, Jun Liu, and Chengjie Wang. Surface representation for point clouds. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022. 12[46] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [47] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [48] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proc. of Intl. Conf. on Learning Representations, 2022. [49] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019. [50] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024. [51] Peng-Shuai Wang. Octformer: Octree-based transformers for 3d point clouds. ACM Transactions ON Graphics, 2023. [52] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions ON Graphics, 2019. [53] Zicheng Wang, Zhenghao Chen, Yiming Wu, Zhen Zhao, Luping Zhou, and Dong Xu. Pointramba: A hybrid transformer-mamba framework for point cloud analysis. arXiv preprint arXiv:2405.15463, 2024. [54] Chengzhi Wu, Junwei Zheng, Julius Pfrommer, and Jürgen Beyerer. Attention-based point cloud edge sampling. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2023. [55] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024. [56] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. In Proc. of Advances in Neural Information Processing Systems, 2022. [57] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2015. [58] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. ACM Transactions ON Graphics, 2016. [59] Weihao Yu and Xinchao Wang. Mambaout: Do we really need mamba for vision? arXiv preprint arXiv:2405.07992, 2024. [60] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022. [61] Yaohua Zha, Huizhen Ji, Jinmin Li, Rongsheng Li, Tao Dai, Bin Chen, Zhi Wang, and Shu-Tao Xia. Towards compact 3d representations via point feature enhancement masked autoencoders. In Proc. of the AAAI Conf. on Artificial Intelligence, 2024. [62] Yaohua Zha, Naiqi Li, Yanzi Wang, Tao Dai, Hang Guo, Bin Chen, Zhi Wang, Zhihao Ouyang, and Shu-Tao Xia. Lcm: Locally constrained compact point cloud model for masked point modeling. In Proc. of Advances in Neural Information Processing Systems, 2024. [63] Yaohua Zha, Jinpeng Wang, Tao Dai, Bin Chen, Zhi Wang, and Shu-Tao Xia. Instance-aware dynamic prompt tuning for pre-trained point cloud models. In Porc. of IEEE Intl. Conf. on Computer Vision, 2023. [64] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training. In Proc. of Advances in Neural Information Processing Systems, 2022. [65] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2023. [66] Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, and Shuicheng Yan. Point could mamba: Point cloud learning via state space model. arXiv preprint arXiv:2403.00762, 2024. [67] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2020. [68] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Porc. of IEEE Intl. Conf. on Computer Vision, 2021. 13[69] Xiao Zheng, Xiaoshui Huang, Guofeng Mei, Yuenan Hou, Zhaoyang Lyu, Bo Dai, Wanli Ouyang, and Yongshun Gong. Point cloud pre-training with diffusion models. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024. [70] Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, and Xiang Bai. Dynamic adapter meets prompt tuning: Parameter-efficient transfer learning for point cloud analysis. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024. [71] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. In Proc. of Intl. Conf. on Machine Learning, 2024. 14Appendix A Theoretically Analysis A.1 Closer look at Selective SSM As described in Sec. 3, Selective SSM [14] considers parameters B, C, ∆ in Eq. 2 as functions of the input. To be specific, given the input sequence ˆx = [x1, ··· , xt, ··· , xL] ∈ RL×C, the per-time matrices Bt, Ct, ∆t can be computed as follows: Bt = LB(xt), Ct = LC(xt), ∆t = softplus (L∆(xt)) , (5) where LB, LC, L∆ are linear projection layers, and softplus(x) = log(1 + ex). The matrices At, Bt, Ct, ∆t can be obtained by taking Eq. 5 into Eq. 2. To simplify, we ignore the residual connection D and expand Eq. 1, the output ˆy = [y1, ··· , yt, ··· , yL] ∈ RL×C can be computed below: yt = Ctht, h t = tX i=1   tY j=i+1 Aj  Bixi, (6) which can be further described in matrix form below:   h1 h2 ... ht   =   B1 0 ··· 0 A2B1 B2 ··· 0 ... ... ... ...Qt i=2AiB1 Qt i=3AiB2 ··· Bt     x1 x2 ... xt  . (7) For an intuitive understanding, Eq. 7 resembles the self-attention mechanism with a mask M, specifically causal self-attention. In this context, M is a lower triangular matrix with elements set to 1. To further exam this, consider the transfer matrix W between ˆy and ˆx, i.e., (ˆy = W ˆx): Wi,j = Ci   iY k=j+1 Ak  Bj (8) = Ci   iY k=j+1 exp (∆kA)  Bj (9) = Ci exp   iX k=j+1 ∆kA  Bj (10) ≈ Ci exp   iX k=j+1 L∆(xk)>0 L∆ (xk) A  Bj, (11) where Wi,j represents the element in the i-th row and j-th column, the approximation in Eq. 11 is done using ReLU instead of softplus. Consider the notation below: Qi := Ci, Ti,j = exp   iX k=j+1 L∆(xk)>0 (L∆ (xk) A)  , Kj = \u0000 Bj \u0001T . (12) Thus, the Eq. 11 can be simplified to: Wi,j ≈ QiTi,jKT j . (13) This shows that the Selective SSM captures the influence of xi and xj using Qi and Kj, respectively, while Ti,j molding the token significance from xi to xj. Note that i ≤ j because W is a lower triangular matrix, indicating a strong relationship with causal self-attention [4, 2]. 15A.2 Global modeling of PointMamba In this subsection, we explain the global modeling of our PointMamba. Let’s consider the total input sequence as h ˆl1; ˆl2 i = \u0002 x1, ,··· , xl/2; xl/2+1, ··· , xl \u0003 where the sequence has a length l and l is even. ˆl1 comes from Hilbert serialization and the other half, ˆl2, comes from Trans-Hilbert. The large matrix in Eq. 7 can be represented as a partitioned matrix \u0014 X 0 Y Z \u0015 as below: B1 ... ... Ql 2 i=2AiB1 ··· B l 2 Ql 2 +1 i=2 AiB1 ··· A l 2 +1B l 2 B l 2 +1 ... ... ... ... ... Ql i=2AiB1 ··· Ql i= l 2 +1AiB l 2 Ql i= l 2 +2AiB l 2 +1 ··· Bl     (14) Note that the block Y , highlighted in gray , is associated with both ˆl1 (from B) and ˆl2 (from A), denoted as Y (ˆl1, ˆl2). The blocks X and Z only relate to half of the sequence, denoted as X(ˆl1) and Z(ˆl2), respectively. Thus, the hidden space output h ˆh1; ˆh2 iT = \u0002 h1, ··· , hl/2; hl/2+1, ··· , hl \u0003T can be compressed as below: \u0014ˆh1 ˆh2 \u0015 = \u0014 X(ˆl1)ˆl1 Y (ˆl1, ˆl2)ˆl2 + Z(ˆl2)ˆl2 \u0015 . (15) As in Fig. 3 and Eq. 15, the serialized points from Trans-Hilbert can receive global information from Hilbert serialization. Appendix B Concurrent Related Works Table 9: Classification performance comparisons with other state space model methods on three variants of the ScanObjectNN [49]. All results are reported without voting. Method Param. (M) FLOPs (G) ScanObjectNN OBJ_BG OBJ_ONLY PB_T50_RS Point Cloud Mamba [66] 34.2 45.0 - - 88.10 Mamba3D [24] 16.9 3.9 93.12 92.08 88.20 PoinTramba [53] 19.5 - 92.30 91.30 89.10 PointMamba (ours) 12.3 3.1 94.32 92.60 89.31 Some works on state space models for point cloud analysis appeared recently. This section discusses the differences between these methods and our PointMamba. Point Cloud Mamba (PCM) [66] combines an improved Mamba module, i.e., Vision Mamba [71], with PointMLP [37] (a strong point cloud analysis method), and incorporates consistent traverse serialization at each stage. To enhance Mamba’s capability in managing point sequences with varying orders, PCM introduces point prompts that convey the sequence’s arrangement rules. While these techniques improve the performance of state space models, they also introduce additional computational overhead and complexity in design. Mamba3D [24] is another recently proposed method. To obtain better global features, Mamba3D introduces an enhanced Vision Mamba [71] block, which includes both a token forward SSM and 16VanillaMamba block Hilbert Trans-Hilbert FPS … … 𝑁×KNN FPS: Farthest Point Sampling … …  … … Order indicator Order indicator … … … … … … … … … … … …3-rd7-th11-th PoolingConcat … … … … … … GlobalfeaturePer-pointfeatureC: Concatenate C Token embedding layer Seg. head Figure 6: The details of our PointMamba for segmentation task. a backward SSM that operates on the feature channel. It proposes a Local Norm Pooling block to extract local geometric features. PointTramba[53] introduces a hybrid approach that integrates Transformers and Mamba. It segments point clouds into groups and utilizes Transformers to capture intra-group dependencies, while Mamba models inter-group relationships using a bi-directional, importance-aware ordering strategy. As shown in Tab. 9, our PointMamba surpasses these concurrent methods by offering superior performance with reduced computational overhead. Note that our method is extremely simple and without complex design, which utilizes the vanilla Mamba block and abstains from incorporating modular designs from other baselines, thereby maintaining simplicity and efficiency in our approach. We believe such a method can better illustrate the potential of SSM in point cloud analysis tasks. Appendix C More experimental resutls C.1 Implement details Table 10: Implementation details for pre-training and downstream tasks. Configuration Pre-training Classification Segmentation ShapeNetCore ModelNet40 ScanObjectNN ShapeNetPart Optimizer AdamW AdamW AdamW AdamW Learning rate 1e-3 3e-4 5e-4 2e-4 Weight decay 5e-2 5e-2 5e-2 5e-2 Learning rate scheduler cosine cosine cosine cosine Training epochs 300 300 300 300 Warmup epochs 10 10 10 10 Batch size 128 32 32 16 Num. of encoder layers N 12 12 12 12 Num. of decoder layers 4 - - - Input points M 1024 1024 2048 2048 Num. of patches n 64 64 128 128 Patch size k 32 32 32 32 Augmentation Scale&Trans Scale&Trans Rotation - Pre-training Details. The ShapeNetCore dataset [5] is used for pre-training, including ∼51K clean 3D sample across 55 categories. The 1,024 input points are divided into 64 point patches, with each patch consisting of 32 points. The pre-training process includes 300 epochs, with a batch size of 128. More detail can be found in Tab. 10. Downstream tasks Details. Fig. 2 shows the pipeline of PointMamba for classification tasks. We report the overall accuracy without voting on the challenging ScanObjectNN [49] using 2,048 input points, and on ModelNet40 [57] using 1,024 input points. For segmentation on ShapeNetPart [58], as shown in Fig. 6, we use random, Hilbert, and Trans-Hilbert serializations, with order indicators applied on Hilbert/Trans-Hilbert serializations. Features from the 3-rd, 7-th, and last layer are pooled as global features after a simple feature fusion. These global features are then concatenated with per-point features and sent to the segmentation head. More detail can be found in Tab. 10. 17Table 11: The effect of masking strategy. The pre-training loss ( × 1000) along with fine- tuning accuracy (%) are reported. Masking ratio Loss OBJ-BG OBJ-ONLY 0.4 2.01 92.60 90.70 0.6 1.97 94.32 92.60 0.8 2.33 93.46 90.17 0.9 2.00 92.43 91.05 Table 12: The effect of classification token. Fine-tuning accuracy (%) are reported. Methods OBJ-BG OBJ-ONLY Before the sequence 93.63 91.05 After the sequence 94.32 90.19 Middle the sequence 93.98 90.71 Max Pool 93.39 91.36 Average Pool 94.32 92.60 Input Masking Reconstruction Input Masking Reconstruction Figure 7: The qualitative results of mask predictions of our PointMamba on ShapeNet validation set. C.2 Additional ablation study In this section, we do additional ablation studies on several hyper-parameters. Masking strategy for pre-training. By employing a serialization-based mask modeling paradigm, our PointMamba achieves superior performance. To find a proper masking strategy for our method, we compare two types of masking with varying ratios. The block masking [60] masks geometrically proximate point cloud patches, leading to a more challenging reconstruction target. In Tab. 11, we experimentally find that masking 60% of point patches by randomly choosing can achieve good performance. Usage of classification token. Previous works [12, 60, 38] often use a classification token [CLS] as a global token for classification. As in Tab. 12, we find that without[CLS] and utilizing only the average pooling of the final block’s output yields the best results for PointMamba. Appendix D Qualitative Analysis D.1 Mask modeling visualization As in Sec. 4.2, we customize a simple yet effective serialization-based mask modeling paradigm. By randomly masking about 60% of serialization-based point tokens, an asymmetric vanilla Mamba autoencoder is utilized to extract the point feature, with a simple prediction head for reconstruction. In Fig. 7, we present qualitative results of mask modeling on ShapeNet validation set. Despite a 60% masking ratio, our PointMamba effectively reconstructs the masked patches, providing a strong self-supervised knowledge for downstream tasks. D.2 Part segmentation visualization In this subsection, we present the qualitative results for part segmentation on the ShapeNetPart valida- tion set, including both the ground truth and the predicted results. As in Fig. 8, our PointMamba shows highly competitive results on part segmentation. 18Airplane Car Motorbike Lamp Chair Desk Laptop Skateboard GT Predicted Figure 8: The qualitative results of part segmentation of our PointMamba on ShapeNetPart. 19",
      "meta_data": {
        "arxiv_id": "2402.10739v5",
        "authors": [
          "Dingkang Liang",
          "Xin Zhou",
          "Wei Xu",
          "Xingkui Zhu",
          "Zhikang Zou",
          "Xiaoqing Ye",
          "Xiao Tan",
          "Xiang Bai"
        ],
        "published_date": "2024-02-16T14:56:13Z",
        "pdf_url": "https://arxiv.org/pdf/2402.10739v5.pdf",
        "github_url": "https://github.com/LMD0311/PointMamba"
      },
      "llm_extracted_info": {
        "main_contributions": "PointMamba addresses the quadratic complexity of Transformer-based models in point cloud analysis by introducing a novel State Space Model (SSM) approach, demonstrating global modeling capabilities with linear complexity. Key contributions include the proposal of PointMamba, leveraging space-filling curves for efficient point tokenization and utilizing a simple, non-hierarchical Mamba encoder. The method achieves superior performance across various point cloud analysis datasets (ScanObjectNN, ModelNet40, ShapeNetPart) while significantly reducing computational costs, including GPU memory usage and FLOPs, especially for long sequences (e.g., 30.2x faster inference, 24.9x less GPU memory, 5.2x less FLOPs compared to PointMAE with 32,768 tokens). It also introduces a customized serialization-based mask modeling paradigm for self-supervised learning, highlighting SSM's potential in 3D vision.",
        "methodology": "PointMamba transforms unstructured 3D point clouds into regular 1D sequences using a point scanning strategy. This involves Farthest Point Sampling (FPS) to select key points, followed by two space-filling curves (Hilbert and Trans-Hilbert) to generate two distinct serialized point sequences. A point tokenizer, composed of KNN to form patches and a lightweight PointNet for feature embedding, converts these sequences into point tokens. An 'Order Indicator' module, using linear transformations, maintains the distinct spatial characteristics of tokens from different scanning strategies. The core architecture is a plain, non-hierarchical Mamba encoder consisting of stacked Mamba blocks. Each Mamba block incorporates Layer Normalization, a Selective SSM (S6) mechanism (which makes parameters input-dependent, ensuring linear complexity and time-variant modeling), Depth-wise Convolution, and residual connections. For pre-training, PointMamba uses a serialization-based mask modeling paradigm: it randomly selects a space-filling curve for serialization, applies masking (60% ratio), and uses an asymmetric autoencoder (Mamba-based encoder and decoder) to reconstruct masked point patches via Chamfer Distance loss.",
        "experimental_setup": "The model was pre-trained on the ShapeNetCore dataset (51K 3D samples across 55 categories). For downstream tasks, it was evaluated on: real-world object classification using ScanObjectNN (15,000 objects, 15 categories, 3 variants: OBJ-BG, OBJ-ONLY, PB-T50-RS), synthetic object classification on ModelNet40 (12,311 CAD samples, 40 categories), few-shot learning on ModelNet40 (n-way, m-shot setup), and part segmentation on ShapeNetPart. Input point clouds consisted of 1024 points (ModelNet40) or 2048 points (ScanObjectNN, ShapeNetPart), divided into patches (e.g., 64 patches for 1024 points) with k=32 points per patch. The PointMamba encoder has 12 vanilla Mamba blocks with 384 hidden dimensions, and the decoder for pre-training uses 4 Mamba blocks. Training used AdamW optimizer with a cosine learning rate scheduler, 10 warmup epochs, and 300 total epochs. Masking ratio for pre-training was 60%. Evaluation metrics included Overall Accuracy (OA %) for classification and mean IoU (mIoU) for segmentation. Efficiency was compared using GPU memory (NVIDIA A800 80GB), FLOPs, and inference speed against Transformer-based counterparts.",
        "limitations": "The current work primarily focuses on the point cloud analysis task, without designing a unified Mamba-based foundation model that can handle various 3D vision tasks such as 3D object detection or full segmentation. Additionally, the research exclusively utilizes point clouds as training data, overlooking the potential benefits of combining them with 2D images or language knowledge for improved performance.",
        "future_research_directions": "Future work includes designing a unified Mamba-based foundation model that can generalize across various 3D vision tasks, such as 3D object classification, detection, and segmentation. Another promising direction is to explore the integration of 2D images or language knowledge with point cloud data to enhance model performance. Further research could also investigate the optimal combination of masked self-attention with Gated MLP within the Mamba framework, addressing current optimization challenges.",
        "experimental_code": "def farthest_point_sample(point, npoint):N, D = point.shape;xyz = point[:,:3];centroids = np.zeros((npoint,));distance = np.ones((N,)) * 1e10;farthest = np.random.randint(0, N);for i in range(npoint):centroids[i] = farthest;centroid = xyz[farthest, :];dist = np.sum((xyz - centroid) ** 2, -1);mask = dist < distance;distance[mask] = dist[mask];farthest = np.argmax(distance, -1);point = point[centroids.astype(np.int32)];return point;from pointnet2_ops import pointnet2_utils;def fps(data, number):fps_idx = pointnet2_utils.furthest_point_sample(data, number);fps_data = pointnet2_utils.gather_operation(data.transpose(1, 2).contiguous(), fps_idx).transpose(1, 2).contiguous();return fps_data;import torch;from .hilbert import encode as hilbert_encode_;from addict import Dict;class Point(Dict):def __init__(self, *args, **kwargs):super().__init__(*args, **kwargs);if \"batch\" not in self.keys() and \"offset\" in self.keys():self[\"batch\"] = offset2batch(self.offset);elif \"offset\" not in self.keys() and \"batch\" in self.keys():self[\"offset\"] = batch2offset(self.batch);def serialization(self, order=\"hilbert\", depth=None, shuffle_orders=False):assert \"batch\" in self.keys();if \"grid_coord\" not in self.keys():assert {\"grid_size\", \"coord\"}.issubset(self.keys());self[\"grid_coord\"] = torch.div(self.coord - self.coord.min(0)[0], self.grid_size, rounding_mode=\"trunc\").int();if depth is None:depth = int(self.grid_coord.max()).bit_length();self[\"serialized_depth\"] = depth;assert depth * 3 + len(self.offset).bit_length() <= 63;assert depth <= 16;code = [encode(self.grid_coord, self.batch, depth, order=order_) for order_ in order];code = torch.stack(code);order = torch.argsort(code);inverse = torch.zeros_like(order).scatter_(dim=1, index=order, src=torch.arange(0, code.shape[1], device=order.device).repeat(code.shape[0], 1));if shuffle_orders:perm = torch.randperm(code.shape[0]);code = code[perm];order = order[perm];inverse = inverse[perm];self[\"serialized_code\"] = code;self[\"serialized_order\"] = order;self[\"serialized_inverse\"] = inverse;@torch.inference_mode();def encode(grid_coord, batch=None, depth=16, order=\"hilbert\"):if order == \"hilbert\":code = hilbert_encode(grid_coord, depth=depth);elif order == \"hilbert-trans\":code = hilbert_encode(grid_coord[:, [1, 0, 2]], depth=depth);else:raise NotImplementedError;if batch is not None:batch = batch.long();code = batch << depth * 3 | code;return code;def hilbert_encode(grid_coord: torch.Tensor, depth: int = 16):return hilbert_encode_(grid_coord, num_dims=3, num_bits=depth);import torch.nn as nn;from knn_cuda import KNN;class Group(nn.Module):def __init__(self, num_group, group_size):super().__init__();self.num_group = num_group;self.group_size = group_size;self.knn = KNN(k=self.group_size, transpose_mode=True);def forward(self, xyz):batch_size, num_points, _ = xyz.shape;center = fps(xyz, self.num_group);_, idx = self.knn(xyz, center);assert idx.size(1) == self.num_group;assert idx.size(2) == self.group_size;idx_base = torch.arange(0, batch_size, device=xyz.device).view(-1, 1, 1) * num_points;idx = idx + idx_base;idx = idx.view(-1);neighborhood = xyz.view(batch_size * num_points, -1)[idx, :];neighborhood = neighborhood.view(batch_size, self.num_group, self.group_size, 3).contiguous();neighborhood = neighborhood - center.unsqueeze(2);return neighborhood, center;class Encoder(nn.Module):def __init__(self, encoder_channel):super().__init__();self.encoder_channel = encoder_channel;self.first_conv = nn.Sequential(nn.Conv1d(3, 128, 1),nn.BatchNorm1d(128),nn.ReLU(inplace=True),nn.Conv1d(128, 256, 1));self.second_conv = nn.Sequential(nn.Conv1d(512, 512, 1),nn.BatchNorm1d(512),nn.ReLU(inplace=True),nn.Conv1d(512, self.encoder_channel, 1));def forward(self, point_groups):bs, g, n, _ = point_groups.shape;point_groups = point_groups.reshape(bs * g, n, 3);feature = self.first_conv(point_groups.transpose(2, 1));feature_global = torch.max(feature, dim=2, keepdim=True)[0];feature = torch.cat([feature_global.expand(-1, -1, n), feature], dim=1);feature = self.second_conv(feature);feature_global = torch.max(feature, dim=2, keepdim=False)[0];return feature_global.reshape(bs, g, self.encoder_channel);def init_OrderScale(dim):gamma = nn.Parameter(torch.ones(dim));beta = nn.Parameter(torch.zeros(dim));nn.init.normal_(gamma, mean=1, std=.02);nn.init.normal_(beta, std=.02);return gamma, beta;def apply_OrderScale(x, gamma, beta):assert gamma.shape == beta.shape;if x.shape[-1] == gamma.shape[0]:return x * gamma + beta;elif x.shape[1] == gamma.shape[0]:return x * gamma.view(1, -1, 1, 1) + beta.view(1, -1, 1, 1);else:raise ValueError('the input tensor shape does not match the shape of the scale factor.');import math;from typing import Optional;import torch.nn.functional as F;from einops import rearrange, repeat;from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn;try:from causal_conv1d import causal_conv1d_fn, causal_conv1d_update;except ImportError:causal_conv1d_fn, causal_conv1d_update = None, None;try:from mamba_ssm.ops.triton.selective_state_update import selective_state_update;except ImportError:selective_state_update = None;try:from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn;except ImportError:RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None;class Mamba(nn.Module):def __init__(self,d_model,d_state=16,d_conv=4,expand=2,dt_rank=\"auto\",dt_min=0.001,dt_max=0.1,dt_init=\"random\",dt_scale=1.0,dt_init_floor=1e-4,conv_bias=True,bias=False,use_fast_path=True,layer_idx=None,device=None,dtype=None,):factory_kwargs = {\"device\": device, \"dtype\": dtype};super().__init__();self.d_model = d_model;self.d_state = d_state;self.d_conv = d_conv;self.expand = expand;self.d_inner = int(self.expand * self.d_model);self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank;self.use_fast_path = use_fast_path;self.layer_idx = layer_idx;self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs);self.conv1d = nn.Conv1d(in_channels=self.d_inner,out_channels=self.d_inner,bias=conv_bias,kernel_size=d_conv,groups=self.d_inner,padding=d_conv - 1,**factory_kwargs,);self.activation = \"silu\";self.act = nn.SiLU();self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs);self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs);dt_init_std = self.dt_rank**-0.5 * dt_scale;if dt_init == \"constant\":nn.init.constant_(self.dt_proj.weight, dt_init_std);elif dt_init == \"random\":nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std);else:raise NotImplementedError;dt = torch.exp(torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))+ math.log(dt_min)).clamp(min=dt_init_floor);inv_dt = dt + torch.log(-torch.expm1(-dt));with torch.no_grad():self.dt_proj.bias.copy_(inv_dt);self.dt_proj.bias._no_reinit = True;A = repeat(torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\"n -> d n\",d=self.d_inner,).contiguous();A_log = torch.log(A);self.A_log = nn.Parameter(A_log);self.A_log._no_weight_decay = True;self.D = nn.Parameter(torch.ones(self.d_inner, device=device));self.D._no_weight_decay = True;self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs);def forward(self, hidden_states, inference_params=None):batch, seqlen, dim = hidden_states.shape;conv_state, ssm_state = None, None;if inference_params is not None:conv_state, ssm_state = self._get_states_from_cache(inference_params, batch);if inference_params.seqlen_offset > 0:out, _, _ = self.step(hidden_states, conv_state, ssm_state);return out;xz = rearrange(self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\"d (b l) -> b d l\",l=seqlen,);if self.in_proj.bias is not None:xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\");A = -torch.exp(self.A_log.float());if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:out = mamba_inner_fn(xz,self.conv1d.weight,self.conv1d.bias,self.x_proj.weight,self.dt_proj.weight,self.out_proj.weight,self.out_proj.bias,A,None,None,self.D.float(),delta_bias=self.dt_proj.bias.float(),delta_softplus=True,);else:x, z = xz.chunk(2, dim=1);if conv_state is not None:conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)));if causal_conv1d_fn is None:x = self.act(self.conv1d(x)[..., :seqlen]);else:assert self.activation in [\"silu\", \"swish\"];x = causal_conv1d_fn(x=x,weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),bias=self.conv1d.bias,activation=self.activation,);x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"));dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1);dt = self.dt_proj.weight @ dt.t();dt = rearrange(dt, \"d (b l) -> b d l\", l=seqlen);B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous();C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous();assert self.activation in [\"silu\", \"swish\"];y = selective_scan_fn(x,dt,A,B,C,self.D.float(),z=z,delta_bias=self.dt_proj.bias.float(),delta_softplus=True,return_last_state=ssm_state is not None,);if ssm_state is not None:y, last_state = y;ssm_state.copy_(last_state);y = rearrange(y, \"b d l -> b l d\");out = self.out_proj(y);return out;from timm.models.layers import DropPath;class Block(nn.Module):def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False, drop_path=0.):super().__init__();self.residual_in_fp32 = residual_in_fp32;self.fused_add_norm = fused_add_norm;self.mixer = mixer_cls(dim);self.norm = norm_cls(dim);self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity();if self.fused_add_norm:assert RMSNorm is not None, \"RMSNorm import fails\";assert isinstance(self.norm, (nn.LayerNorm, RMSNorm)), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\";def forward(self, hidden_states: torch.Tensor, residual: Optional[torch.Tensor] = None, inference_params=None):hidden_states = hidden_states + self.drop_path(self.mixer(self.norm(hidden_states), inference_params=inference_params));return hidden_states;def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs);from functools import partial;class MixerModel(nn.Module):def __init__(self,d_model: int,n_layer: int,ssm_cfg=None,norm_epsilon: float = 1e-5,rms_norm: bool = False,initializer_cfg=None,fused_add_norm=False,residual_in_fp32=False,drop_out: int = 0.,drop_path=0.,device=None,dtype=None,) -> None:factory_kwargs = {\"device\": device, \"dtype\": dtype};super().__init__();self.residual_in_fp32 = residual_in_fp32;self.fused_add_norm = fused_add_norm;if self.fused_add_norm:if layer_norm_fn is None or rms_norm_fn is None:raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\");self.layers = nn.ModuleList([create_block(d_model,ssm_cfg=ssm_cfg,norm_epsilon=norm_epsilon,rms_norm=rms_norm,residual_in_fp32=residual_in_fp32,fused_add_norm=fused_add_norm,layer_idx=i,drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,**factory_kwargs,)for i in range(n_layer)]);self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(d_model, eps=norm_epsilon, **factory_kwargs);self.apply(partial(_init_weights,n_layer=n_layer,**(initializer_cfg if initializer_cfg is not None else {}),));self.drop_out = nn.Dropout(drop_out) if drop_out > 0. else nn.Identity();def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):return {i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)for i, layer in enumerate(self.layers)};def forward(self, input_ids, pos, inference_params=None):hidden_states = input_ids + pos;for layer in self.layers:hidden_states = layer(hidden_states, inference_params=inference_params);hidden_states = self.drop_out(hidden_states);hidden_states = self.norm_f(hidden_states.to(dtype=self.norm_f.weight.dtype));return hidden_states;class MambaDecoder(nn.Module):def __init__(self, embed_dim=384, depth=4, norm_layer=nn.LayerNorm, drop_path=0, config=None):super().__init__();self.blocks = MixerModel(d_model=embed_dim,n_layer=depth,rms_norm=config.rms_norm,drop_path=drop_path);self.head = nn.Identity();self.apply(self._init_weights);def _init_weights(self, m):if isinstance(m, nn.Linear):nn.init.xavier_uniform_(m.weight);if isinstance(m, nn.Linear) and m.bias is not None:nn.init.constant_(m.bias, 0);elif isinstance(m, nn.LayerNorm):nn.init.constant_(m.bias, 0);nn.init.constant_(m.weight, 1.0);def forward(self, x, pos, mask_pos):B, _, C = x.shape;x = self.blocks(x, pos);x = self.head(x[mask_pos, :].view(B, -1, C));return x;from extensions.chamfer_dist import ChamferDistanceL1, ChamferDistanceL2;import random;import numpy as np;class MaskMamba(nn.Module):def __init__(self, config, **kwargs):super().__init__();self.config = config;self.mask_ratio = config.mamba_config.mask_ratio;self.trans_dim = config.mamba_config.trans_dim;self.depth = config.mamba_config.depth;self.drop_path_rate = config.mamba_config.drop_path_rate;self.encoder_dims = config.mamba_config.encoder_dims;self.encoder = Encoder(encoder_channel=self.encoder_dims);self.mask_type = config.mamba_config.mask_type;self.pos_embed = nn.Sequential(nn.Linear(3, 128),nn.GELU(),nn.Linear(128, self.trans_dim),);dpr = [x.item() for x in torch.linspace(0, self.drop_path_rate, self.depth)];self.blocks = MixerModel(d_model=self.trans_dim,n_layer=self.depth,rms_norm=self.config.rms_norm,drop_path=dpr);self.OrderScale_gamma_1, self.OrderScale_beta_1 = init_OrderScale(self.trans_dim);self.OrderScale_gamma_2, self.OrderScale_beta_2 = init_OrderScale(self.trans_dim);self.apply(self._init_weights);def _init_weights(self, m):if isinstance(m, nn.Linear):trunc_normal_(m.weight, std=.02);if isinstance(m, nn.Linear) and m.bias is not None:nn.init.constant_(m.bias, 0);elif isinstance(m, nn.LayerNorm):nn.init.constant_(m.bias, 0);nn.init.constant_(m.weight, 1.0);elif isinstance(m, nn.Conv1d):trunc_normal_(m.weight, std=.02);if m.bias is not None:nn.init.constant_(m.bias, 0);def _mask_center_block(self, center, noaug=False):if noaug or self.mask_ratio == 0:return torch.zeros(center.shape[:2]).bool();mask_idx = [];for points in center:points = points.unsqueeze(0);index = random.randint(0, points.size(1) - 1);distance_matrix = torch.norm(points[:, index].reshape(1, 1, 3) - points, p=2,dim=-1);idx = torch.argsort(distance_matrix, dim=-1, descending=False)[0];ratio = self.mask_ratio;mask_num = int(ratio * len(idx));mask = torch.zeros(len(idx));mask[idx[:mask_num]] = 1;mask_idx.append(mask.bool());bool_masked_pos = torch.stack(mask_idx).to(center.device);return bool_masked_pos;def _mask_center_rand(self, center, noaug=False):B, G, _ = center.shape;if noaug or self.mask_ratio == 0:return torch.zeros(center.shape[:2]).bool();self.num_mask = int(self.mask_ratio * G);overall_mask = np.zeros([B, G]);for i in range(B):mask = np.hstack([np.zeros(G - self.num_mask),np.ones(self.num_mask),]);np.random.shuffle(mask);overall_mask[i, :] = mask;overall_mask = torch.from_numpy(overall_mask).to(torch.bool);return overall_mask.to(center.device);def forward(self, neighborhood, center, order, order_index, noaug=False):if self.mask_type == 'rand':bool_masked_pos = self._mask_center_rand(center, noaug=noaug);else:bool_masked_pos = self._mask_center_block(center, noaug=noaug);group_input_tokens = self.encoder(neighborhood);if order_index == 0:group_input_tokens = apply_OrderScale(group_input_tokens, self.OrderScale_gamma_1, self.OrderScale_beta_1);elif order_index == 1:group_input_tokens = apply_OrderScale(group_input_tokens, self.OrderScale_gamma_2, self.OrderScale_beta_2);else:assert False;batch_size, seq_len, C = group_input_tokens.size();x_vis = group_input_tokens[~bool_masked_pos].reshape(batch_size, -1, C);masked_center = center[~bool_masked_pos].reshape(batch_size, -1, 3);pos = self.pos_embed(masked_center);x_vis = self.blocks(x_vis, pos);return x_vis, bool_masked_pos, group_input_tokens;from timm.models.layers import trunc_normal_;class Point_MAE_Mamba_serializationV2(nn.Module):def __init__(self, config):super().__init__();self.config = config;self.trans_dim = config.mamba_config.trans_dim;self.MAE_encoder = MaskMamba(config);self.group_size = config.group_size;self.num_group = config.num_group;self.mask_token = nn.Parameter(torch.zeros(1, 1, self.trans_dim));self.decoder_pos_embed = nn.Sequential(nn.Linear(3, 128),nn.GELU(),nn.Linear(128, self.trans_dim));self.decoder_depth = config.mamba_config.decoder_depth;self.drop_path_rate = config.mamba_config.drop_path_rate;dpr = [x.item() for x in torch.linspace(0, self.drop_path_rate, self.decoder_depth)];self.MAE_decoder = MambaDecoder(embed_dim=self.trans_dim,depth=self.decoder_depth,drop_path=dpr,config=config,);self.group_divider = Group(num_group=self.num_group, group_size=self.group_size);self.increase_dim = nn.Sequential(nn.Conv1d(self.trans_dim, 3 * self.group_size, 1));trunc_normal_(self.mask_token, std=.02);self.loss = config.loss;self.build_loss_func(self.loss);def build_loss_func(self, loss_type):if loss_type == \"cdl1\":self.loss_func = ChamferDistanceL1().cuda();elif loss_type == 'cdl2':self.loss_func = ChamferDistanceL2().cuda();else:raise NotImplementedError;def forward(self, pts, vis=False, **kwargs):neighborhood, center = self.group_divider(pts);B, G, S, _ = neighborhood.shape;order_list = ['hilbert', 'hilbert-trans'];order_index = np.random.choice([i for i in range(len(order_list))], 1, replace=False);center, order, index_order, _, _ = serialization_func(center, None, None, order_list[order_index[0]]);neighborhood = neighborhood.flatten(0, 1)[order].reshape(B, G, S, -1).contiguous();x_vis, mask, group_input_tokens = self.MAE_encoder(neighborhood, center, order=order,order_index=order_index[0]);B, _, C = x_vis.shape;pos_emd_vis = self.decoder_pos_embed(center[~mask]).reshape(B, -1, C);pos_emd_mask = self.decoder_pos_embed(center[mask]).reshape(B, -1, C);x_full = torch.zeros_like(group_input_tokens, device=group_input_tokens.device);x_full[~mask] = x_vis.reshape(-1, C);x_full[mask] = self.mask_token.reshape(-1, C);pos_full = torch.zeros_like(group_input_tokens, device=group_input_tokens.device);pos_full[~mask] = pos_emd_vis.reshape(-1, C);pos_full[mask] = pos_emd_mask.reshape(-1, C);x_rec = self.MAE_decoder(x_full, pos_full, mask);B, M, C = x_rec.shape;rebuild_points = self.increase_dim(x_rec.transpose(1, 2)).transpose(1, 2).reshape(B * M, -1, 3);gt_points = neighborhood[mask].reshape(B * M, -1, 3);loss1 = self.loss_func(rebuild_points, gt_points);if vis:vis_points = neighborhood[~mask].reshape(B * (self.num_group - M), -1, 3);full_vis = vis_points + center[~mask].unsqueeze(1);full_rebuild = rebuild_points + center[mask].unsqueeze(1);full = torch.cat([full_vis, full_rebuild], dim=0);full_center = torch.cat([center[mask], center[~mask]], dim=0);ret2 = full_vis.reshape(-1, 3).unsqueeze(0);ret1 = full.reshape(-1, 3).unsqueeze(0);return ret1, ret2, full_center;else:return loss1",
        "experimental_info": "Point Scanning Strategy: Farthest Point Sampling (FPS) is employed. The number of sampled groups (`num_group`) and the size of each group (`group_size`) are configurable parameters. Space-filling curves: Hilbert and Trans-Hilbert curves are used for point cloud serialization. The serialization `depth` is typically set to `16`, and coordinate discretization before serialization uses a `grid_size` of `0.02`. Point Tokenizer: Patches are formed using K-Nearest Neighbors (KNN) with `group_size` neighbors. Feature embedding for these patches is performed by a PointNet-like structure, with `encoder_dims` as the channel dimension (e.g., `1024` or `384` for the transformer dimension). Order Indicator: The 'Order Indicator' module uses learnable `gamma` and `beta` parameters applied as linear transformations. Mamba Encoder/Decoder: The core architecture consists of stacked Mamba blocks (`mamba_ssm/modules/mamba_simple::Mamba`) within a `MixerModel`. The `d_model` (transformer dimension) is configurable (e.g., `384`), and the number of layers (`n_layer`) is configurable (e.g., encoder `12`, decoder `4`). The architecture utilizes `RMSNorm` for normalization, and `drop_path` is applied. Each Mamba block includes a `conv1d` with `d_conv=4` and an `expand=2` factor, along with a Selective SSM (S6) mechanism characterized by `d_state=16` and `dt_rank='auto'`. Pre-training: The pre-training objective uses an asymmetric autoencoder with Mamba-based encoder and decoder. Masking is applied to point patches with a `mask_ratio` of `0.6`, chosen via either a random (`rand`) or contiguous block (`_mask_center_block`) strategy. The reconstruction loss is calculated using Chamfer Distance, configurable as either L1 (`cdl1`) or L2 (`cdl2`). For serialization during pre-training, a space-filling curve is randomly selected between Hilbert and Trans-Hilbert for each training iteration."
      }
    },
    {
      "title": "BiPointNet: Binary Neural Network for Point Clouds",
      "abstract": "To alleviate the resource constraint for real-time point cloud applications\nthat run on edge devices, in this paper we present BiPointNet, the first model\nbinarization approach for efficient deep learning on point clouds. We discover\nthat the immense performance drop of binarized models for point clouds mainly\nstems from two challenges: aggregation-induced feature homogenization that\nleads to a degradation of information entropy, and scale distortion that\nhinders optimization and invalidates scale-sensitive structures. With\ntheoretical justifications and in-depth analysis, our BiPointNet introduces\nEntropy-Maximizing Aggregation (EMA) to modulate the distribution before\naggregation for the maximum information entropy, and Layer-wise Scale Recovery\n(LSR) to efficiently restore feature representation capacity. Extensive\nexperiments show that BiPointNet outperforms existing binarization methods by\nconvincing margins, at the level even comparable with the full precision\ncounterpart. We highlight that our techniques are generic, guaranteeing\nsignificant improvements on various fundamental tasks and mainstream backbones.\nMoreover, BiPointNet gives an impressive 14.7x speedup and 18.9x storage saving\non real-world resource-constrained devices.",
      "full_text": "Published as a conference paper at ICLR 2021 BIPOINT NET: B INARY NEURAL NETWORK FOR POINT CLOUDS Haotong Qin∗1,2, Zhongang Cai∗3, Mingyuan Zhang∗3, Yifu Ding1, Haiyu Zhao3, Shuai Yi3, Xianglong Liu†1, Hao Su4 1State Key Lab of Software Development Environment, Beihang University 2Shen Yuan Honors College, Beihang University 3SenseTime Research 4University of California, San Diego {qinhaotong,xlliu,zjdyf}@buaa.edu.cn {caizhongang, zhangmingyuan, zhaohaiyu, yishuai}@sensetime.com haosu@eng.ucsd.edu ABSTRACT To alleviate the resource constraint for real-time point cloud applications that run on edge devices, in this paper we present BiPointNet, the ﬁrst model binarization approach for efﬁcient deep learning on point clouds. We discover that the im- mense performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degra- dation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justiﬁcations and in-depth analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information en- tropy, and Layer-wise Scale Recovery (LSR) to efﬁciently restore feature repre- sentation capacity. Extensive experiments show that BiPointNet outperforms ex- isting binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic, guaranteeing signiﬁcant improvements on various fundamental tasks and main- stream backbones. Moreover, BiPointNet gives an impressive14.7×speedup and 18.9×storage saving on real-world resource-constrained devices. 1 I NTRODUCTION With the advent of deep neural networks that directly process raw point clouds (PointNet (Qi et al., 2017a) as the pioneering work), great success has been achieved in learning on point clouds (Qi et al., 2017b; Li et al., 2018; Wang et al., 2019a; Wu et al., 2019; Thomas et al., 2019; Liu et al., 2019b; Zhang et al., 2019b). Point cloud applications, such as autonomous driving and augmented reality, often require real-time interaction and fast response. However, computation for such applications is usually deployed on resource-constrained edge devices. To address the challenge, novel algorithms, such as Grid-GCN (Xu et al., 2020b), RandLA-Net (Hu et al., 2020), and PointV oxel (Liu et al., 2019d), have been proposed to accelerate those point cloud processing networks. While signiﬁcant speedup and memory footprint reduction have been achieved, these works still rely on expensive ﬂoating-point operations, leaving room for further optimization of the performance from the model quantization perspective. Model binarization (Rastegari et al., 2016; Bulat & Tzimiropoulos, 2019; Hubara et al., 2016; Wang et al., 2020; Zhu et al., 2019; Xu et al., 2019) emerged as one of the most promising approaches to optimize neural networks for better computational and memory usage efﬁciency. Binary Neural Networks (BNNs) leverage 1) compact binarized parameters that take small memory space, and 2) highly efﬁcient bitwise operations which are far less costly compared to the ﬂoating-point counterparts. Despite that in 2D vision tasks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014; Girshick, 2015; Russakovsky et al., 2015; Wang et al., 2019b; ∗equal contributions †corresponding author 1 arXiv:2010.05501v4  [cs.CV]  11 Jun 2021Published as a conference paper at ICLR 2021 n×3 n×3 input transform n×64 shared BiMLPs n×64 feature transform n×1024shared BiMLPs 1024 global feature input points EMA-max BiMLPs output scores Layer-wise Scale RecoveryEntropy-Maximizing Aggregation input feature transformation  unit 𝝉 𝝋 aggregation  unit aggregated featureoutput feature n×m binarized input feature m×k⨀ binarized weight original feature output feature 𝜶 learnable layer -wise scaling factor Figure 1: Overview of our BiPointNet on PointNet base model, applying Entropy-Maximizing Ag- gregation (EMA) and Layer-wise Scale Recovery (LSR). EMA consists of the transformation unit and the aggregation unit for maximizing the information entropy of feature after binarization. LSR with the learnable layer-wise scaling factor αis applied to address the scale distortion of bi-linear layers (which form the BiMLPs), ﬂexibly restore the distorted output to reasonable values Zhang et al., 2021) has been studied extensively by the model binarization community, the methods developed are not readily transferable for 3D point cloud networks due to the fundamental differ- ences between 2D images and 3D point clouds. First, to gain efﬁciency in processing unordered 3D points, many point cloud learning methods rely heavily on pooling layers with large receptive ﬁeld to aggregate point-wise features. As shown in PointNet (Qi et al., 2017b), global pooling provides a strong recognition capability. However, this practice poses challenges for binarization. Our analy- ses show that the degradation of feature diversity, a persistent problem with binarization (Liu et al., 2019a; Qin et al., 2020b; Xie et al., 2017), is signiﬁcantly ampliﬁed by the global aggregation func- tion (Figure 2), leading to homogenization of global features with limited discriminability. Second, the binarization causes immense scale distortion at the point-wise feature extraction stage, which is detrimental to model performance in two ways: the saturation of forward-propagated features and backward-propagated gradients hinders optimization, and the disruption of the scale-sensitive structures (Figure 3) results in the invalidation of their designated functionality. In this paper, we provide theoretical formulations of the above-mentioned phenomenons and obtain insights through in-depth analysis. Such understanding allows us to propose a method that turns full- precision point cloud networks into extremely efﬁcient yet strong binarized models (see the overview in Figure 1). To tackle the homogenization of the binarized features after passing the aggregation function, we study the correlation between the information entropy of binarization features and the performance of point cloud aggregation functions. We thus propose Entropy-Maximizing Aggrega- tion (EMA) that shifts the feature distribution towards the statistical optimum, effectively improving expression capability of the global features. Moreover, given maximized information entropy, we further developLayer-wise Scale Recovery(LSR) to efﬁciently restore the output scale that enhances optimization, which allows scale-sensitive structures to function properly. LSR uses only one learn- able parameter per layer, leading to negligible storage increment and computation overhead. Our BiPointNet is the ﬁrst binarization approaches to deep learning on point clouds, and it outper- forms existing binarization algorithms for 2D vision by convincing margins. It is even almost on par (within ∼1-2%) with the full-precision counterpart. Although we conduct most analysis on the PointNet baseline, we show that our methods are generic and can be readily extendable to other pop- ular backbones, such as PointNet++ (Qi et al., 2017b), PointCNN (Li et al., 2018), DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019), which are the representatives of mainstream cate- gories of point cloud feature extractors. Moreover, extensive experiments on multiple fundamental tasks on the point cloud, such as classiﬁcation, part segmentation, and semantic segmentation, high- light that our BiPointNet is task-agnostic. Besides, we highlight that our EMA and LSR are efﬁcient and easy to implement in practice: in the actual test on popular edge devices, BiPointNet achieves 14.7×speedup and 18.9×storage savings compared to the full-precision PointNet. Our code is released at https://github.com/htqin/BiPointNet. 2 R ELATED WORK Network Binarization.Recently, various quantization methods for neural networks have emerged, such as uniform quantization (Gong et al., 2019; Zhu et al., 2020), mixed-precision quantization (Wu 2Published as a conference paper at ICLR 2021 et al., 2018; Yu et al., 2020), and binarization. Among these methods, binarization enjoys compact binarized parameters and highly efﬁcient bitwise operations for extreme compression and accelera- tion (Rastegari et al., 2016; Qin et al., 2020a). In general, the forward and backward propagation of binarized models in the training process can be formulated as: Forward :b= sign(x) = {+1, if x≥0 −1, otherwise Backward : gx = {gb, if x∈(−1,1) 0, otherwise (1) where x denotes the element in ﬂoating-point weights and activations, b denotes the element in binarized weights Bw and activations Ba. gx, and gb donate the gradient ∂C ∂x and ∂C ∂b , respectively, where C is the cost function for the minibatch. In forward propagation, sign function is directly applied to obtain the binary parameters. In backward propagation, the Straight-Through Estimator (STE) (Bengio et al., 2013) is used to obtain the derivative of thesign function, avoiding getting all zero gradients. The existing binarization methods are designed to obtain accurate binarized networks by minimizing the quantization error (Rastegari et al., 2016; Zhou et al., 2016; Lin et al., 2017), improving loss function (Ding et al., 2019; Hou et al., 2017), reducing the gradient error (Liu et al., 2018; 2020), and designing novel structures and pipelines (Martinez et al., 2020). Unfortunately, we show in Sec 3 that these methods, designed for 2D vision tasks, are not readily transferable to 3D point clouds. Deep Learning on Point Clouds.PointNet (Qi et al., 2017a) is the ﬁrst deep learning model that processes raw point clouds directly. The basic building blocks proposed by PointNet such as MLP for point-wise feature extraction and max pooling for global aggregation (Guo et al., 2020) have become the popular design choices for various categories of newer backbones: 1) the pointwise MLP-based such as PointNet++ (Qi et al., 2017b); 2) the graph-based such as DGCNN (Xu et al., 2020b); 3) the convolution-based such as PointCNN (Li et al., 2018), PointConv (Wu et al., 2019) RS-CNN (Liu et al., 2019c) and KP-Conv (Thomas et al., 2019). Recently, methods are proposed for efﬁcient deep learning on point clouds through novel data structuring (Xu et al., 2020b), faster sampling (Hu et al., 2020), adaptive ﬁlters (Xu et al., 2020a), efﬁcient representation (Liu et al., 2019d) or convolution operation (Zhang et al., 2019b) . However, they still use expensive ﬂoating- point parameters and operations, which can be improved by binarization. 3 M ETHODS Binarized models operate on efﬁcient binary parameters, but often suffer large performance drop. Moreover, the unique characteristics of point clouds pose even more challenges. We observe there are two main problems: ﬁrst, aggregation of a large number of points leads to a severe loss of feature diversity; second, binarization induces an immense scale distortion, that undermines the functionality of scale-sensitive structures. In this section, we discuss our observations, and propose our BiPointNet with theoretical justiﬁcations. 3.1 B INARIZATION FRAMEWORK We ﬁrst give a brief introduction to our framework that binarizes a ﬂoating-point network. For example, deep learning models on point clouds typically contain multi-layer perceptrons (MLPs) for feature extraction. In contrast, the binarized models contain binary MLPs (BiMLPs), which are composed of binarized linear (bi-linear) layers. Bi-linear layers perform the extremely efﬁcient bitwise operations (XNOR and Bitcount) on the lightweight binary weight/activation. Speciﬁcally, the activation of the bi-linear layer is binarized to Ba, and is computed with the binarized weight Bw to obtain the output Z: Z = Ba ⊙Bw, (2) where ⊙denotes the inner product for vectors with bitwise operations XNOR and Bitcount. When Bw and Badenote the random variables inBw and Ba, we represent their probability mass function as pBw(bw), and pBa(ba). Moreover, we divide the BiPointNet into units for detailed discussions. In BiPointNet, the original data or feature X ∈ Rn×c ﬁrst enters the symmetric function Ω, which represents a composite function built by stacking several permutation equivariant and permutation invariant layers (e.g., nonlinear layer, bi-linear layer, max pooling). And then, the outputY ∈Rn×k is binarized to obtain 3Published as a conference paper at ICLR 2021 (a) Point-wise features to be aggregated. (b) Full-precision features aggregated  with max pooling. (d) Binarized features aggregated with  EMA.  (c) Binarized features aggregated with  max pooling.  Figure 2: Aggregation-induced feature homogenization. (a) shows the activation of each test sample in a batch of ModelNet40. In (b)-(d), the single feature vectors pooled from all points are mapped to colors. The diversity of colors represents the diversity of pooled features. The original aggregation design is incompatible with binarization, leading to the homogenization of output features in (c), whereas our proposed EMA retains high information entropy, shown in (d) the binary feature B ∈{−1,1}i×k, where itakes nwhen the feature is modeled independently and takes 1 when the feature is aggregated globally. The single unit is thus represented as B = sign(Y) = sign(Ω(X)). (3) Similarly, when B, Y and X denote the random variables sampled from B, Y and X, we represent their probability mass function as pB(b), pY(y) and pX(x). 3.2 E NTROPY -MAXIMIZING AGGREGATION Unlike images pixels that are arranged in regular lattices, point clouds are sets of points without any speciﬁc order. Hence, features are usually processed in a point-wise manner and aggregated explicitly through pooling layers. Our study shows that the aggregation function is a performance bottleneck of the binarized model, due to severe homogenization as shown in Figure 2. We apply information theory (Section 3.2.1) to quantify the effect of the loss of feature diversity, and ﬁnd that global feature aggregation leads to a catastrophic loss of information entropy. In Sec- tion 3.2.2, we propose the concept of Entropy-Maximizing Aggregation (EMA) that gives the sta- tistically maximum information entropy to effectively tackle the feature homogenization problem. 3.2.1 A GGREGATION -INDUCED FEATURE HOMOGENIZATION Ideally, the binarized tensor B should reﬂect the information in the original tensor Y as much as possible. From the perspective of information, maximizing mutual information can maximize the information ﬂow from the full-precision to the binarized parameters. Hence, our goal is equivalent to maximizing the mutual information I(Y; B) of the random variables Y and B: arg max Y,B I(Y; B) = H(B) −H(B |Y) (4) where H(B) is the information entropy, and H(B |Y) is the conditional entropy of B given Y. H(B |Y) = 0 as we use the deterministic sign function as the quantizer in binarization (see Section A.1 for details). Hence, the original objective function Eq. (4) is equivalent to: arg max B HB(B) = − ∑ b∈B pB(b) logpB(b), (5) where Bis the set of possible values of B. We then study the information properties of max pool- ing, which is a common aggregation function used in popular point cloud learning models such as PointNet. Let the max pooling be the last layer φof the multi-layer stacked Ω, and the input of φ is deﬁned as Xφ. The data ﬂow of Eq. (3) can be further expressed as B = sign(φ(Xφ)), and the information entropy HB of binarized feature Bcan be expressed as HB(Xφ) =− (∑ xφ≥0 pXφ(xφ) )n log (∑ xφ≥0 pXφ(xφ) )n − ( 1− (∑ xφ≥0 pXφ(xφ) )n) log ( 1− (∑ xφ≥0 pXφ(xφ) )n) (6) where nis the number of elements aggregated by the max pooling, and Xφ is the random variable sampled from Xφ. The brief derivation of Eq (6) is shown in Appendix A.2. Theorem 1 shows the information properties of max pooling with the normal distribution input on the binarized network architecture. 4Published as a conference paper at ICLR 2021 Theorem 1 For input Xφ of max pooling φwith arbitrary distribution, the information entropy of the binarized output goes to zero as n goes to inﬁnity, i.e., lim n→+∞ HB = 0 . And there exists a constant c, for any n1 and n2, if n1 >n2 >c, we have HB,n1 <HB,n2 , where nis the number of elements to be aggregated. The proof of Theorem 1 is included in Appendix A.2, which explains the severe feature homoge- nization after global feature pooling layers. As the number of points is typically large (e.g. 1024 points by convention in ModelNet40 classiﬁcation task), it signiﬁcantly reduces the information en- tropy HB of binarized feature B, i.e., the information ofY is hardly retained in B, leading to highly similar output features regardless of the input features to pooling layer as shown in Figure 2. Furthermore, Theorem 1 provides a theoretical justiﬁcation for the poor performance of existing binarization methods, transferred from 2D vision tasks to point cloud applications. In 2D vision, the aggregation functions are often used to gather local features with a small kernel sizen(e.g. n= 4 in ResNet (He et al., 2016; Liu et al., 2018) and VGG-Net (Simonyan & Zisserman, 2014) which use 2 ×2 pooling kernels). Hence, the feature homogenization problem on images is not as signiﬁcant as that on point clouds. 3.2.2 EMA FOR MAXIMUM INFORMATION ENTROPY Therefore, we need a class of aggregation functions that maximize the information entropy of B to avoid the aggregation-induced feature homogenization. We study the correlation between the information entropy HB of binary random variable B and the distribution of the full-precision random variable Y. We notice that the sign function used in binarization has a ﬁxed threshold and decision levels, so we get Proposition 1 about information entropy of Band the distribution of Y. Proposition 1 When the distribution of the random variable Y satisﬁes ∑ y<0 pY(y) =∑ y≥0 pY(y) = 0.5, the information entropy HB is maximized. The proof of Proposition 1 is shown in Appendix A.3. Therefore, theoretically, there is a distribution of Y that can maximize the mutual information of Y and B by maximizing the information entropy of the binary tensor B, so as to maximally retain the information of Y in B. To maximize the information entropy HB, we propose the EMA for feature aggregation in BiPoint- Net. The EMA is not one, but a class of binarization-friendly aggregation layers. Modifying the aggregation function in the full-precision neural network to a EMA keeps the entropy maximized by input transformation. The deﬁnition of EMA is Y = EMA(Xφ) = ϕ(τ(Xφ)), (7) where ϕdenotes the aggregation function (e.g. max pooling and average pooling) andτ denotes the transformation unit. Note that a standard normal distribution N(0,1) is assumed for Xφ because batch normalization layers are placed prior to the pooling layers by convention. τ can take many forms; we discover that a simple constant offset is already effective. The offset shifts the input so that the output distribution satisﬁes ∑ y<0 pY(y) = 0 .5, to maximize the information entropy of binary feature B. The transformation unit τ in our BiPointNet can be deﬁned as τ(Xφ) = Xφ−δ∗. When max pooling is applied as ϕ, we obtain the distribution offset δ∗for the input Xφ that maxi- mizes the information entropy HB by solving the objective function arg max δ HB(δ) =− ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n log ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n − ( 1 − ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n) log ( 1 − ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n) , (8) where n denotes the number of elements in each batch. For each n, we can obtain an optimized δ∗ max for Eq. (8), we include the pseudo code in the Appendix A.5. Moreover, we derive in the Appendix A.6 that when average pooling is used as ϕ, the solution to its objective function is expressed as δ = 0 . We thus obtain δ∗ avg = 0 . This means the solution 5Published as a conference paper at ICLR 2021 (a) (b) (c) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Full Prec. BNN BiPointNet Normal Gradient Ze ro Gradient Figure 4: (a) Information entropy of the aggregated features. With EMA, our BiPointNet achieves a higher information entropy. (b) Regularizer loss comparison. Our PointNet has a low loss, indicating that the scale distortion is reduced and T-Net is not disrupted. (c) Ratio of zero-gradient activations in back-propagation. LSR alleviates the scale distortion, enhancing the optimization process is not related to n. Hence, average pooling can be regarded as a ﬂexible alternative because its performance is independent of the input number n. In a nutshell, we provide two possible variants of ϕ: ﬁrst, we show that a simple shift is sufﬁcient to turn a max pooling layer into an EMA (EMA-max); second, average pooling can be directly used (EMA-avg) without modiﬁcation as a large number of points does not undermine its information entropy, making it adaptive to the dynamically changing number of point input. Note that modi- fying existing aggregation functions is only one way to achieve EMA; the theory also instructs the development of new binarization-friendly aggregation functions in the future. 3.3 O NE-SCALE -FITS -ALL : L AYER -WISE SCALE RECOVERY In this section, we show that binarization leads to feature scale distortion and study its cause. We conclude that the distortion is directly related to the number of feature channels. More importantly, we discuss the detriments of scale distortion from the perspectives of the functionality of scale- sensitive structures and the optimization. To address the severe scale distortion in feature due to binarization, we propose the Layer-wise Scale Recovery (LSR). In LSR, each bi-linear layer is added only one learnable scaling factor to recover the original scales of all binarized parameters, with negligible additional computational overhead and memory usage. 3.3.1 S CALE DISTORTION (a) Input point  cloud (b) PointNet (c) BiPointNet (d) BNN Figure 3: Scale Distortion. Fig- ures (b)-(d) show the trans- formed input. Compared with the input (a), the scales of (b) in full-precision PointNet and (c) in our BiPointNet are normal, while the scale of (d) in BNN is signiﬁcantly distorted The scale of parameters is deﬁned as the standard deviation σof their distribution. As we mentioned in Section 3.2, balanced bina- rized weights are used in the bi-linear layer aiming to maximize the entropy of the output after binarization, i.e., pBw(1) = 0 .5 and pBa(1) = 0.5. Theorem 2 When we let pBw(1) = 0 .5 and pBa(1) = 0 .5 in bi-linear layer to maximize the mutual information, for the binarized weight Bw ∈ {−1,+1}m×k and activation Ba ∈ {−1,+1}n×m, the probability mass function for the distribution of output Z can be represented as pZ(2i−m) = 0 .5mCi m,i ∈ {0,1,2,...,m }. The output has approximately a normal distribu- tion N(0,m). The proof of Theorem 2 is found in Appendix A.4. Theorem 2 shows that given the maximized information entropy, the scale of the output features is directly related to the number of feature channels. Hence, scale distortion is pervasive as a large number of channels is the design norm of deep learning neural networks for effective feature extraction. We discuss two major impacts of the scale distortion on the performance of binarized point cloud learning models. First, the scale distortion invalidates structures designed for 3D deep learning that 6Published as a conference paper at ICLR 2021 Table 1: Ablation study for our BiPointNet of various tasks on ModelNet40 (classiﬁcation), ShapeNet Parts (part segmentation), and S3DIS (semantic segmentation). EMA and LSR and com- plementary to each other, and they are useful across all three applications Method Bit-width Aggr. ModelNet40 ShapeNet Parts S3DIS OA mIoU mIoU OA Full Prec. 32/32 MAX 88.2 84.3 54.4 83.5 32/32 A VG 86.5 84.0 51.5 81.5 BNN 1/1 MAX 7.1 54.0 9.5 45.0 BNN-LSR 1/1 MAX 4.1 58.7 2.0 25.4 BNN-EMA 1/1 EMA-avg 11.3 53.0 9.9 46.8 1/1 EMA-max 16.2 47.3 8.5 47.2 Ours 1/1 EMA-avg 82.5 80.3 40.9 74.9 1/1 EMA-max 86.4 80.6 44.3 76.7 are sensitive to the scale of values. For example, the T-Net in PointNet is designed to predict an orthogonal transformation matrix for canonicalization of input and intermediate features (Qi et al., 2017a). The predicted matrix is regularized by minimizing the loss term Lreg = I −ZZT2 . However, this regularization is ineffective for theZ with huge variance as shown in Figure 3. Second, the scale distortion leads to a saturation of forward-propagated activations and backward- propagated gradients (Ding et al., 2019). In the binary neural networks, some modules (such as sign and Hardtanh) rely on the Straight-Through Estimator (STE) Bengio et al. (2013) for feature binarization or feature balancing. When the scale of their input is ampliﬁed, the gradient is truncated instead of increased proportionally. Such saturation, as shown in Fig 4(c), hinders learning and even leads to divergence. 3.3.2 LSR FOR OUTPUT SCALE RECOVERY To recover the scale and adjustment ability of output, we propose the LSR for bi-linear layers in our BiPointNet. We design a learnable layer-wise scaling factor αin our LSR. αis initialized by the ratio of the standard deviations between the output of bi-linear and full-precision counterpart: α0 = σ(A ⊗W)/σ(Ba ⊙Bw), (9) where σdenotes as the standard deviation. And the αis learnable during the training process. The calculation and derivative process of the bi-linear layer with our LSR are as follows: Forward :Z = α(Ba ⊙Bw) Backward : gα = gZ(Ba ⊙Bw), (10) where gα and gZ denotes the gradient ∂C ∂α and ∂C ∂Z , respectively. By applying the LSR in BiPointNet, we mitigate the scale distortion of output caused by binarization. Compared to existing methods, the advantages of LSR is summarized in two folds. First, LSR is efﬁcient. It not only abandons the adjustment of input activations to avoid expensive inference time computation, but also recovers the scale of all weights parameters in a layer collectively instead of expensive restoration in a channel-wise manner (Rastegari et al., 2016). Second, LSR serves the purpose of scale recovery that we show is more effective than other adaptation such as minimizing quantization errors (Qin et al., 2020b; Liu et al., 2018). 4 E XPERIMENTS In this section, we conduct extensive experiments to validate the effectiveness of our proposed Bi- PointNet for efﬁcient learning on point clouds. We ﬁrst ablate our method and demonstrate the con- tributions of EMA and LSR on three most fundamental tasks: classiﬁcation on ModelNet40 (Wu et al., 2015), part segmentation on ShapeNet (Chang et al., 2015), and semantic segmentation on S3DIS (Armeni et al., 2016). Moreover, we compare BiPointNet with existing binarization methods where our designs stand out. Besides, BiPointNet is put to the test on real-world devices with limited computational power and achieve extremely high speedup (14.7×) and storage saving (18.9×). The details of the datasets and the implementations are included in the Appendix E. 7Published as a conference paper at ICLR 2021 Table 2: Comparison of binarization methods on PointNet. EMA is critical; even if all meth- ods are equipped with our EMA, our LSR out- performs others with least number of scaling factors. OA: Overall Accuracy Method Bit-width Aggr. # Factors OA Full Prec. 32/32 MAX - 88.2 32/32 A VG - 86.5 BNN 1/1 MAX 0 7.1 1/1 EMA-avg 0 11.3 1/1 EMA-max 0 16.2 IR-Net 1/1 MAX 10097 7.3 1/1 EMA-avg 10097 22.0 1/1 EMA-max 10097 63.5 Bi-Real 1/1 MAX 10097 4.0 1/1 EMA-avg 10097 77.0 1/1 EMA-max 10097 77.5 ABC-Net 1/1 MAX 51 4.1 1/1 EMA-avg 51 68.9 1/1 EMA-max 51 77.8 XNOR++ 1/1 MAX 18 4.1 1/1 EMA-avg 18 73.8 1/1 EMA-max 18 78.4 XNOR 1/1 MAX 28529 64.9 1/1 EMA-avg 28529 78.2 1/1 EMA-max 28529 81.9 Ours 1/1 MAX 18 4.1 1/1 EMA-avg 18 82.5 1/1 EMA-max 18 86.4 Table 3: Our methods on mainstream back- bones. We use XNOR as a strong baseline for comparison. The techniques in our BiPointNet are generic to point cloud learning. Hence, they are easily extendable to other backbones Base Model Method Bit-width Aggr. OA PointNet (Vanilla) Full Prec. 32/32 MAX 86.8 XNOR 1/1 MAX 61.0 Ours 1/1 EMA-max 85.6 PointNet Full Prec. 32/32 MAX 88.2 XNOR 1/1 MAX 64.9 Ours 1/1 EMA-max 86.4 PointNet++ Full Prec. 32/32 MAX 90.0 XNOR 1/1 MAX 63.1 Ours 1/1 EMA-max 87.8 PointCNN Full Prec. 32/32 A VG 90.0 XNOR 1/1 A VG 83.0 Ours 1/1 EMA-avg 83.8 DGCNN Full Prec. 32/32 MAX 89.2 XNOR 1/1 MAX 51.5 Ours 1/1 EMA-max 83.4 PointConv Full Prec. 32/32 – 90.8 XNOR 1/1 – 83.1 Ours 1/1 – 87.9 4.1 A BLATION STUDY As shown in Table 1, the binarization model baseline suffers a catastrophic performance drop in the classiﬁcation task. EMA and LSR improve performance considerably when used alone, and they further close the gap between the binarized model and the full-precision counterpart when used together. In Figure 4, we further validate the effectiveness of EMA and LSR. We show that BiPointNet with EMA has its information entropy maximized during training, whereas the vanilla binarized network with max pooling gives limited and highly ﬂuctuating results. Also, we make use of the regularization loss Lreg = I −ZZT F for the feature transformation matrix of T-Net in PointNet as an indicator, the Lreg of the BiPointNet with LSR is much smaller than the vanilla binarized network, demonstrating LSR’s ability to reduce the scale distortion caused by binarization, allowing proper prediction of orthogonal transformation matrices. Moreover, we also include the results of two challenging tasks, part segmentation, and semantic seg- mentation, in Table 1. As we follow the original PointNet design for segmentation, which concate- nates pointwise features with max pooled global feature, segmentation suffers from the information loss caused by the aggregation function. EMA and LSR are proven to be effective: BiPointNet is approaching the full precision counterpart with only ∼4% mIoU difference on part segmentation and ∼10.4% mIoU gap on semantic segmentation. The full results of segmentation are presented in Appendix E.6. 4.2 C OMPARATIVE EXPERIMENTS In Table 2, we show that our BiPointNet outperforms other binarization methods such as BNN (Hubara et al., 2016), XNOR (Rastegari et al., 2016), Bi-Real (Liu et al., 2018), ABC-Net (Lin 8Published as a conference paper at ICLR 2021 3.16 0.17 0 0.5 1 1.5 2 2.5 3 3.5 ARM Devices PointNet BiPointNet Storage Usage (MB) 18.9× 67.3 131.8 5.5 9 0 20 40 60 80 100 120 140 A72 A53 PointNet BiPointNet Time Cost (ms) 14.7×12.1× 0 10 20 30 40 50 60 70 80 90 100 01020304050607080 Accuracy (%) IR-Net OursXNOR++ Bi-Real XNOR Time Cost (ms) BNN ABC-Net FP32 (a) (b) (c) Figure 5: (a) Time cost comparison. Our BiPointNet achieves 14.7×speedup on ARM A72 CPU device. (b) Storage usage comparison. Our BiPointNet enjoys 18.9×storage saving on all devices. (c) Speed vs accuracy trade-off plot. We evaluate various binarization methods (with our EMA-max) upon PointNet architecture on ARM A72 CPU device, our BiPointNet is the leading method in both speed and accuracy et al., 2017), XNOR++ (Bulat & Tzimiropoulos, 2019), and IR-Net (Qin et al., 2020b). Although these methods have been proven effective in 2D vision, they are not readily transferable to point clouds due to aggregation-induced feature homogenization. Even if we equip these methods with our EMA to mitigate information loss, our BiPointNet still performs better. We argue that existing approaches, albeit having many scaling factors, focus on minimizing quantization errors instead of recovering feature scales, which is critical to effective learning on point clouds. Hence, BiPointNet stands out with a negligible increase of parameters that are designed to restore feature scales. The detailed analysis of the performance of XNOR is found in Appendix C. Moreover, we highlight that our EMA and LSR are generic, and Table 3 shows improvements across several mainstream categories of point cloud deep learning models, including PointNet (Qi et al., 2017a), PointNet++ (Qi et al., 2017b), PointCNN (Li et al., 2018), DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019). 4.3 D EPLOYMENT EFFICIENCY ON REAL -WORLD DEVICES To further validate the efﬁciency of BiPointNet when deployed into the real-world edge devices, we further implement our BiPointNet on Raspberry Pi 4B with 1.5 GHz 64-bit quad-core ARM CPU Cortex-A72 and Raspberry Pi 3B with 1.2 GHz 64-bit quad-core ARM CPU Cortex-A53. We compare our BiPointNet with the PointNet in Figure 5(a) and Figure 5(b). We highlight that BiPointNet achieves 14.7×inference speed increase and 18.9×storage reduction over PointNet, which is recognized as a fast and lightweight model itself. Moreover, we implement various bina- rization methods over PointNet architecture and report their real speed performance on ARM A72 CPU device. As Figure 5(c), our BiPointNet surpasses all existing binarization methods in both speed and accuracy. Note that all binarization methods adopt our EMA and report their best accu- racy, which is the important premise that they can be reasonably applied to binarize the PointNet. 5 C ONCLUSION We propose BiPointNet, the ﬁrst binarization approach for efﬁcient learning on point clouds. We build a theoretical foundation to study the impact of binarization on point cloud learning models, and proposed EMA and LSR in BiPointNet to improve the performance. BiPointNet outperforms existing binarization methods, and it is easily extendable to a wide range of tasks and backbones, giving an impressive14.7×speedup and 18.9×storage saving on resource-constrained devices. Our work demonstrates the great potential of binarization. We hope our work can provide directions for future research. Acknowledgement This work was supported by National Natural Science Foundation of China (62022009, 61872021), Beijing Nova Program of Science and Technology (Z191100001119050), and State Key Lab of Software Development Environment (SKLSDE-2020ZX-06). 9Published as a conference paper at ICLR 2021 REFERENCES Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In IEEE CVPR, 2016. Yoshua Bengio, Nicholas L ´eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Adrian Bulat and Georgios Tzimiropoulos. Xnor-net++: Improved binary neural networks. In BMVC, 2019. Angel X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Qixing Huang, Zimo Li, S. Savarese, M. Savva, Shuran Song, H. Su, J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d model repository. CoRR, abs/1512.03012, 2015. Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. Regularizing activation distribution for training binarized deep networks. In IEEE CVPR, 2019. Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. CoRR, abs/1903.02428, 2019. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu- rate object detection and semantic segmentation. In IEEE CVPR, 2014. Ross B. Girshick. Fast r-cnn. IEEE ICCV, 2015. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In IEEE ICCV, 2019. Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3d point clouds: A survey. IEEE TPAMI, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In IEEE CVPR, 2016. Lu Hou, Quanming Yao, and James T. Kwok. Loss-aware binarization of deep networks. ICLR, 2017. Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efﬁcient semantic segmentation of large-scale point clouds. 2020. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In NeurIPS, 2016. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In NeurIPS, 2012. Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, X. Di, and B. Chen. Pointcnn: Convolution on x-transformed points. In NeurIPS, 2018. Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In NeurIPS, 2017. Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and David S. Doermann. Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation. In IEEE CVPR, 2019a. Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. IEEE CVPR, 2019b. Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8895–8904, 2019c. 10Published as a conference paper at ICLR 2021 Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In ECCV, 2018. Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions. In ECCV, 2020. Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efﬁcient 3d deep learning. In NeurIPS, 2019d. Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with real-to-binary convolutions. In ICLR, 2020. Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. IEEE CVPR, 2017a. Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, 2017b. Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks: A survey. Pattern Recognition, 2020a. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In IEEE CVPR, 2020b. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. In ECCV, 2016. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S Bernstein, et al. Imagenet large scale vi- sual recognition challenge. IJCV, 2015. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE CVPR, 2015. Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. IEEE ICCV, 2019. Yiru Wang, Weihao Gan, Wei Wu, and Junjie Yan. Dynamic curriculum learning for imbalanced data classiﬁcation. In IEEE ICCV, 2019a. Yunhe Wang, Chang Xu, Chao Xu, and Dacheng Tao. Packing convolutional neural networks in the frequency domain. IEEE TPAMI, 2019b. Ziwei Wang, Ziyi Wu, Jiwen Lu, and Jie Zhou. Bidet: An efﬁcient binarized object detector. In IEEE CVPR, 2020. B. Wu, Y . Wang, P. Zhang, Yuandong Tian, P. Vajda, and K. Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. CoRR, abs/1812.00090, 2018. Wenxuan Wu, Zhongang Qi, and Fuxin Li. Pointconv: Deep convolutional networks on 3d point clouds. IEEE CVPR, 2019. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In IEEE CVPR, 2015. Zizhao Wu, Ruyang Shou, Yunhai Wang, and Xinguo Liu. Interactive shape co-segmentation via label propagation. Computers & Graphics, 38:248–254, 2014. 11Published as a conference paper at ICLR 2021 Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. InArtiﬁcial Intelligence and Statistics, 2017. Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efﬁcient point-cloud segmentation. In ECCV, 2020a. Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, and U. Neumann. Grid-gcn for fast and scalable point cloud learning. 2020b. Yinghao Xu, Xin Dong, Yudian Li, and Hao Su. A main/subsidiary network framework for simpli- fying binary neural networks. In IEEE CVPR, 2019. Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):1–12, 2016. Haibao Yu, Q. Han, Jianbo Li, Jianping Shi, Guang-Liang Cheng, and Bin Fan. Search what you want: Barrier panelty nas for mixed precision quantization. In ECCV, 2020. Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, and Tao Mei. dabnn: A super fast inference framework for binary neural networks on ARM devices. In ACM MM, 2019a. Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. Diversifying sample generation for data-free quantization. In IEEE CVPR, 2021. Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Shellnet: Efﬁcient point cloud convolutional neural networks using concentric shells statistics. In IEEE ICCV, 2019b. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv, abs/1606.06160, 2016. Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards uniﬁed int8 training for convolutional neural network. InIEEE CVPR, 2020. Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or more networks per bit? In IEEE CVPR, 2019. 12Published as a conference paper at ICLR 2021 APPENDIX FOR BIPOINT NET A M AIN PROOFS AND DISCUSSION A.1 P ROOF OF ZERO CONDITIONAL ENTROPY In our BiPointNet, we hope that the binarized tensorB reﬂects the information in the original tensor Y as much as possible. From the perspective of information, our goal is equivalent to maximizing the mutual information I(Y; B) of the random variables Y and B: arg max Y,B I(Y; B) (11) = ∑ y∈Y,b∈B p(Y,B)(y,b) log p(Y,B)(y,b) pY(y)pB(b) (12) = ∑ y∈Y,b∈B p(Y,B)(y,b) log p(Y,B)(y,b) pY(y) − ∑ y∈Y,b∈B p(Y,B)(y,b) logpB(b) (13) = ∑ y∈Y,b∈B pY(y)pB|Y=y(b) logpB|Y=y(b) − ∑ y∈Y,b∈B p(Y,B)(y,b) logpB(b) (14) = ∑ y∈Y pY(y) (∑ b∈B pB|Y=y(b) logpB|Y=y(b) ) − ∑ b∈B (∑ y p(Y,B)(y,b) ) log pB(b) (15) = − ∑ y∈Y p(y)H(B |Y = y) − ∑ b∈B pB(b) logpB(b) (16) = −H(B |Y) + H(B) (17) = H(B) −H(B |Y), (18) where p(Y,B) and pY, pB are the joint and marginal probability mass functions of these discrete variables. H(B) is the information entropy, and H(B|Y) is the conditional entropy of B given Y. According to the Eq. (15) and Eq. (18), the conditional entropy H(Y |X) can be expressed as H(B |Y) = ∑ y∈Y pY(y) (∑ b∈B pB|Y=y(b) logpB|Y=y(b) ) . (19) Since we use the deterministic sign function as the quantizer in binarization, the value of B fully depends on the value of Y, pB|Y=y(b) = 0 or 1 in Eq. (4), i.e., every valueyhas a ﬁxed mapping to a binary value b. Then we have H(B |Y) = ∑ y∈Y pY(y)(0 + 0 +··· + 0) = 0. (20) Hence, the original objective function is equivalent to maximizing the information entropy H(B): arg max B HB(B) = − ∑ b∈B pB(b) logpB(b). (21) A.2 P ROOFS OF THEOREM 1 Theorem 1 For input Xφ of max pooling φwith arbitrary distribution, the information entropy of the binarized output to zero asnto inﬁnity, i.e., lim n→+∞ HB = 0. And there is a constantc, for any n1 and n2, if n1 >n2 >c, we have HB,n1 <HB,n2 , where nis the number of aggregated elements. 13Published as a conference paper at ICLR 2021 Proof. We obtain the correlation between the probability mass function of input Xφ and output Y of max pooling, intuitively, all values are negative to give a negative maximum value: ∑ y<0 pY(y) = ( ∑ xφ<0 pXφ(xφ) )n . (22) Since the sign function is applied as the quantizer, theHB(B) of binarized feature can be expressed as Eq. (6). (1) When Xφ obeys a arbitrary distribution, the probability mass function pXφ(xφ) must satisﬁes∑ xφ<0 pXφ(xφ) ≤1. According to Eq. (6), let t= ∑ xφ<0 pXφ(xφ), we have lim n→∞ HB(Xφ) = lim n→∞ −tn log tn −(1 −t)n log (1 −t)n (23) = − ( lim n→∞ tn ) log ( lim n→∞ tn ) − ( lim n→∞ (1 −t)n ) log ( lim n→∞ (1 −t)n ) (24) = −0 log 0 −1 log 1 (25) =0 (26) (2) For any n≥1, we can obtain the representation of the information entropy HB,n(Xφ): HB,n(Xφ) = − ( ∑ xφ<0 pXφ(xφ) )n log ( ∑ xφ<0 pXφ(xφ) )n − ( 1 − ( ∑ xφ<0 pXφ(xφ) )n) log ( 1 − ( ∑ xφ<0 pXφ(xφ) )n) , (27) Let pn = (∑ xφ<0 pXφ(xφ) )n , the HB,n(pn) can be expressed as HB,n(pn) = −pnlog pn −(1 −pn) log(1−pn), (28) and the derivative of HB,n(pn) is dHB,n(pn) dpB(pn) = log (1 −pn pn ) , (29) the HB,n(pn) is maximized when pn takes 0.5, and is positive correlation with pn when pn < 0.5 since the dHB,n(pn) dpB(pn) >0 when pn <0.5. Therefore, when the constant csatisﬁes pc = (∑ xφ<0 pXφ(xφ) )c ≥0.5, given the n1 > n2 > c, we have pn1 <pn2 <pc, and HB,n1 (Xφ) <HB,n2 (Xφ) <HB,c(Xφ). □ A.3 P ROOFS OF PROPOSITION 1 Proposition 1 When the distribution of the random variable Y satisﬁes ∑ y<0 pY(y) =∑ y≥0 pY(y) = 0.5, the information entropy HB is maximized. Proof. According to Eq (5), we have HB(B) = − ∑ b∈B pB(b) logpB(b) (30) = −pB(−1) logpB(−1) −pB(1) logpB(1) (31) = −pB(−1) logpB(−1) −(1 −pB(−1) log (1−pB(−1))) . (32) 14Published as a conference paper at ICLR 2021 Then we can get the derivative of HB(B) with respect to pB(−1) dHB(B) dpB(−1) = − ( log pB(−1) + pB(−1) pB(−1) ln 2 ) + ( log (1−pB(−1)) + 1 −pB(−1) (1 −pB(−1)) ln 2 ) (33) = −log pB(−1) + log (1−pB(−1)) − 1 ln 2 + 1 ln 2 (34) = log (1 −pB(−1) pB(−1) ) . (35) When we let dHB(B) dpB(−1) = 0 to maximize the HB(B), we have pB(−1) = 0.5. Sine the deterministic signfunction with the zero threshold is applied as the quantizer, the probability mass function ofB is represented as pB(b) =    ∑ y<0 pY(y) dy, if b= −1 ∑ y≥0 pY(y) dy, if b= 1, (36) and when the information entropy is maximized, we have ∑ y<0 pY(y) dy= 0.5. (37) □ A.4 D ISCUSSION AND PROOFS OF THEOREM 2 The bi-linear layers are widely used in our BiPointNet to model each point independently, and each linear layer outputs an intermediate feature. The calculation of the bi-linear layer is represented as Eq. (2). Since the random variable Bis sampled from Bw or Ba obeying Bernoulli distribution, the probability mass function of Bcan be represented as pB(b) = {p, if b= +1 1 −p, if b= −1, (38) where pis the probability of taking the value +1. The distribution of outputZ can be represented by the probability mass function of Bw and Ba. Proposition 2 In bi-linear layer, for the binarized weight Bw ∈ {−1,+1}m×k and activation Ba ∈{−1,+1}n×mwith probability mass functionpBw(1) = pw and pBa(1) = pa, the probability mass function for the distribution of output Z can be represented as pZ(2i−m) = Ci m(1 −pw − pa + 2pwpa)i(pw + pa −2pwpa)m−i,i ∈{0,1,2,...,m }. Proof. To simplify the notation in the following statements, we deﬁne A = Ba and W = Bw. Then, for each element Zi,j in output Z ∈{−1,+1}n×k, we have xi,j = m∑ k=1 Ai,k ×Wk,j. (39) Observe that Ai,k is independent to Wk,j and the value of both variables are either −1 or +1. Therefore, the discrete probability distribution of Ai,k ×Wk,j can be deﬁned as p(x) =    pwpa + (1 −pw) ×(1 −pa), if x= 1 pw ×(1 −pa) + (1−pw) ×pa, if x= −1 0, otherwise. (40) 15Published as a conference paper at ICLR 2021 Simplify the above equation p(x) =    1 −pw −pa + 2pwpa, if x= 1 pw + pa −2pwpa, if x= −1 0, otherwise. (41) Notice that xi,j can be parameterized as a binomial distribution. Then we have Pr(xi,j = l−(m−l)) = Cl m(1 −pw −pa + 2pwpa)l(pw + pa −2pwpa)m−l. (42) Observe that pZ obeys the same distribution as xi,j. Finally, we have pZ(2i−m) = Ci m(1 −pw −pa + 2pwpa)i(pw + pa −2pwpa)m−i,i ∈{0,1,2,...,m }. (43) □ Proposition 2 shows that the output distribution of the bi-linear layer depends on the probability mass functions of binarized weight and activation. Then we present the proofs of Theorem 2. Theorem 2 When we let pBw(1) = 0.5 and pBa(1) = 0.5 in bi-linear layer to maximize the mutual information, for the binarized weight Bw ∈{−1,+1}m×k and activation Ba ∈{−1,+1}n×m, the probability mass function for the distribution of output Z can be represented as pZ(2i−m) = 0.5mCi m,i ∈ {0,1,2,...,m }. The distribution of output is approximate normal distribution N(0,m). Proof. First, we prove that the distribution ofZcan be approximated as a normal distribution. For bi-linear layers in our BiPointNet, all weights and activations are binarized, which can be represented as Bw and Ba, respectively. And the value of an element z(i,j) in Z can be expressed as z(i,j) = m∑ k=1 ( bw(i,k) ×ba(k,j) ) , and the value of the element bw(i,k) ×ba(k,j) can be expressed as bw(i,k) ×ba(k,j) = {1, if bw(i,k) ⊻ba(k,j) = 1 −1, if bw(i,k) ⊻ba(k,j) = −1. (44) The bw(i,k)×ba(k,j) only can take from two values and its value can be considered as the result of one Bernoulli trial. Thus for the random variable Z sampled from the output tensor Z, the probability mass function, pZ can be expressed as pZ(2i−m) = Ci mpk e(1 −pe)n−k, (45) where pe denotes the probability that the element bw(i,k) ×ba(k,j) takes 1. Note that the Eq. (45) is completely equivalent to the representation in Proposition 2. According to the De Moivre–Laplace theorem, the normal distribution N(µ,σ2) can be used as an approximation of the binomial distri- bution under certain conditions, and the pZ(2i−m) can be approximated as pZ(2i−m) = Ci mpk e(1 −pe)n−k ≃ 1√ 2πnpe(1 −pe) e− (k−npe)2 2npe(1−pe) , (46) and then, we can get the mean µ = 0 and variance σ = √mof the approximated distribution N with the help of equivalent representation of pZ in Proposition 2. Now we give proof of this below. According to Proposition 2, when pw = pa = 0.5, we can rewrite the equation as 16Published as a conference paper at ICLR 2021 pZ(2i−m) = 0.5mCi m,i ∈{0,1,2,...,m }. (47) Then we move to calculate the mean and standard variation of this distribution. The mean of this distribution is deﬁned as µ(pZ) = ∑ (2i−m)0.5mCi m,i ∈{0,1,2,...,m }. (48) By the virtue of binomial coefﬁcient, we have (2i−m)0.5mCi m + (2(m−i) −m)0.5mCm−i m = 0.5m((2i−m)Ci m + (m−2i)Cm−i m ) (49) = 0.5m((2i−m)Ci m + (m−2i)Ci m) (50) = 0. (51) Besides, when mis an even number, we have(2i−m)0.5mCi m = 0,i = m 2 . These equations prove the symmetry of function (2i−m)0.5mCi m. Finally, we have µ(pZ) = ∑ (2i−m)0.5mCi m,i ∈{0,1,2,...,m } (52) = ∑ ((2i−m)0.5mCi m + (2(m−i) −m)0.5mCm−i m ),i ∈{0,1,2,..., m 2 } (53) = 0. (54) The standard variation of pZ is deﬁned as σ(pZ) = √(∑ |2i−m|20.5mCim ) (55) = √∑ (4i2 −4im+ m2) 0.5mCim (56) = √ 0.5m ( 4 ∑ i2Cim −4m ∑ iCim + m2 ∑ Cim ) . (57) To calculate the standard variation of pZ, we use Binomial Theorem and have several identical equations: ∑ Ci m = (1 + 1)m = 2m (58) ∑ iCi m = m(1 + 1)m−1 = m2m−1 (59) ∑ i2Ci m = m(m+ 1)(1 + 1)m−2 = m(m+ 1)m2m−2. (60) These identical equations help simplify Eq. (57): σ(pZ) = √ 0.5m ( 4 ∑ i2Cim −4m ∑ iCim + m2 ∑ Cim ) (61) = √ 0.5m(4m(m+ 1)2m−2 −4m22m−1 + m22m) (62) = √ 0.5m((m2 + m)2m −2m22m + m22m) (63) = √ 0.5m(m2m) (64) = √m. (65) Now we proved that, the distribution of output is approximate normal distribution N(0,m). □ 17Published as a conference paper at ICLR 2021 A.5 D ISCUSSION OF THE OPTIMAL δFOR EMA- MAX When the Xφ ∼N (0,1), the objective function of EMA-max to obtain optimal δ∗is represented as Eq. (8). It is difﬁcult to directly solve the objective function. To circumvent this issue, we use Monte Carlo simulation to approximate the value of the optimal δ∗ max as shown Algorithm 1. Algorithm 1Monte Carlo Simulation for EMA-max Input: The number nof points to be aggregated; the number of simulations m (e.g. 10000) Output: Estimated optimal δ∗ max for EMA-max 1: Creating an empty list F (represents elements sampled form distribution of aggregated feature) 2: for i= 0 to m do 3: Creating an empty list Ti (representing one channel of input feature) 4: for j = 0 to ndo 5: Sampling an element eij from the distribution N(0,1) 6: Adding the sampled element eij to the list Ti 7: end for 8: Adding an element represents the aggregated feature MAX(Ti) to F 9: end for 10: Estimating the optimal δ∗ max as δ∗ max = Median(F) (follow Proposition 1) A.6 D ISCUSSION OF THE OPTIMAL δFOR EMA- AVG When the Xφ ∼N(δ,1), the Y ∼N(δ,n−1) and the objective function of EMA-avg for obtaining optimal δ∗ avg can be represented as arg max δ HB(δ) = − ( ∑ xφ<0 1 n−1√ 2π e−(xφ−δ) 2 2 ) log ( ∑ xφ<0 1 n−1√ 2π e−(xφ−δ) 2 2 ) − ( ∑ xφ≥0 1 n−1√ 2π e−(xφ−δ) 2 2 ) log ( ∑ xφ≥0 1 n−1√ 2π e−(xφ−δ) 2 2 ) . (66) The solution of Eq. (66) is expressed as δ = 0, we thus obtain δ∗ avg = 0. This means the solution is not related to n. B I MPLEMENTATION OF BIPOINT NET ON ARM D EVICES B.1 O VERVIEW We further implement our BiPointNet on Raspberry Pi 4B with 1.5 GHz 64-bit quad-core ARM Cortex-A72 and Raspberry Pi 3B with 1.2 GHz 64-bit quad-core ARM Cortex-A53, and test the real speed that one can obtain in practice. Although the PointNet is a recognized high-efﬁciency model, the inference speed of BiPointNet is much faster. Compared to PointNet, BiPointNet enjoys up to 14.7×speedup and 18.9×storage saving. We utilize the SIMD instruction SSHL on ARM NEON to make inference framework daBNN (Zhang et al., 2019a) compatible with our BiPointNet and further optimize the implementa- tion for more efﬁcient inference. B.2 I MPLEMENTATION DETAILS Figure 6 shows the detailed structures of six PointNet implementations. In Full-Precision version (a), BN is merged into the later fully connected layer for speedup, which is widely chosen for deployment in real-world applications. In Binarization version (b)(c)(d)(e), we have to keep BN unmerged due to the binarization of later layers. Instead, we merge the scaling factor of LSR into BN layers. The HardTanh function is removed because it does not affect the binarized value of 18Published as a conference paper at ICLR 2021 input for the later layers. We test the quantization for the ﬁrst layer and last layer in the variants (b)(c)(d)(e). In the last variant(f), we drop the BN layers during training. The scaling factor is ignored during deployment because it does not change the sign of the output. Input Points Linear 3x64 Linear 64x128 Linear 128x1024 Max Pooling Linear 1024x512 Linear 512x256 Linear 256x40 Output nx3 Class (a) nx3 Class (b) Full Precision FC Binarization FC with BN Binarization FC w/o BN Max Pooling nx3 Class (c) nx3 Class (d) nx3 Class (e) nx3 Class (f) Figure 6: Structures of different PointNet implementations. Three fully connected layers are used in all six variants: Full Precision FC, Binarization FC with BN, Binarization FC w/o BN. Full Precision FC contains a full precision fully connected layer and a ReLU layer. Original BN is merged into the later layer. Binarization FC with BN also contains two layers: a quantized fully connected layer and a batch normalization layer. Binarization FC w/o BN is formed by a single quantized fully connected layer B.3 A BLATION ANALYSIS OF TIME COST AND QUANTIZATION SENSITIVITY Setup Bit-width FL LL BN OA Storage & Saving Ratio Time & Speedup Ratio A72 A53 (a) 32/32 32/32 32/32 Merged 86.8 3.16MB / 1.0× 131ms / 1.0× 67ms / 1.0× (b) 1/1 32/32 32/32 Not Merged 85.62 0.17MB / 18.9× 9.0ms / 14.7× 5.5ms / 12.1× (c) 1/1 32/32 1/1 Not Merged 84.60 0.12MB / 26.3× 9.0ms / 14.7× 5.3ms / 12.6× (d) 1/1 1/1 32/32 Not Merged 5.31 0.16MB / 19.7× 11.5ms / 11.4× 6.5ms / 10.3× (e) 1/1 1/1 1/1 Not Merged 4.86 0.12MB / 26.3× 11.4ms / 11.5× 6.4ms / 10.4× (f) 1/1 32/32 32/32 Not Used 85.13 0.15MB / 21.0× 8.1ms / 16.1× 4.8ms / 13.9× Table 4: Comparison of different conﬁgurations in deployment on ARM devices. The storage-saving ratio and speedup ratio are calculated according to the full precision model as the ﬁrst row illustrates. All the models use PointNet as the base model and EMA-max as the aggregation function. The accuracy performance is reported on the point cloud classiﬁcation task with the ModelNet40 dataset. FL: First Layer; LL: Last Layer Table 4 shows the detailed conﬁguration including overall accuracy, storage usage, and time cost of the above-mentioned six implementations. The result shows that binarization of the middle fully connected layers can extremely speed up the original model. We achieve 18.9×storage saving, 14.7×speedup on A72, and 12.1×speed on A53. The quantization of the last layer further helps save storage consumption and improves the speed with a slight performance drop. However, the quantization of the ﬁrst layer causes a drastic drop in accuracy without discernible computational cost reduction. The variant (f) without BN achieves comparable performance with variant (b). It suggests that our LSR method could be an ideal alternative to the original normalization layers to achieve a fully quantized model except for the ﬁrst layer. 19Published as a conference paper at ICLR 2021 C C OMPARISON BETWEEN LAYER -WISE SCALE RECOVERY AND OTHER METHODS In this section, we will analyze the difference between the LSR method with other model binarization methods. Theorem 2 shows the signiﬁcance of recovering scale in point cloud learning. However, IRNet and BiReal only consider the scale of weight but ignore the scale of input features. Therefore, these two methods cannot recover the scale of output due to scale distortion on the input feature. A major difference between these two methods is that LSR opts for layer-wise scaling factor while XNOR opts for point-wise one. Point-wise scale recovery needs dynamical computation during inference while our proposed LSR only has a layer-wise global scaling factor, which is independent of the input. As a result, our method can achieve higher speed in practice. 0 25 50 75 100 125 150 175 200 epoch 10 5 10 4 10 3 10 2 10 1 100 Information Entropy BNN XNOR BiPointNet Figure 7: The information entropy of BNN, XNOR and our BiPointNet Table 3 shows that XNOR can alleviate the aggregation-induced feature homogenization. The point- wise scaling factor helps the model to achieve comparable adjustment capacity as full-precision linear layers. Therefore, although XNOR suffers from feature homogenization at the beginning of the training process, it can alleviate this problem with the progress of training and achieve acceptable performance, as shown in Figure 7. D C OMPARISON WITH OTHER EFFICIENT LEARNING METHODS We compare our computation speedup and storage savings with several recently proposed methods to accelerate deep learning models on point clouds. Note that the comparison is for reference only; tests are conducted on different hardware, and for different tasks. Hence, direct comparison cannot give any meaningful conclusion. In Table 5, we show that BiPointNet achieves the most impressive acceleration. E E XPERIMENTS E.1 D ATASETS ModelNet40: ModelNet40 (Wu et al., 2015) for part segmentation. The ModelNet40 dataset is the most frequently used dataset for shape classiﬁcation. ModelNet is a popular benchmark for point cloud classiﬁcation. It contains 12,311 CAD models from 40 representative classes of objects. 20Published as a conference paper at ICLR 2021 Table 5: Comparison between BiPointNet and other approaches to efﬁcient learning on point clouds. Grid-GCN (Xu et al., 2020b) leverages novel data structuring strategy; RAND-LA Net (Hu et al., 2020) designs a faster sampling method; PointV oxel (Liu et al., 2019d) proposes an efﬁcient repre- sentation. These works, albeit achieving high performance, are not as effective as our binarization method in terms of model acceleration. The asterisk indicates the vanilla version Method Hardware Dataset Base Model Metric/ Performance Speedup BiPointNet ARM Cortex-A72 ModelNet4 PointNet* OA/85.6 12.1× BiPointNet ARM Cortex-A53 ModelNet40 PointNet* OA/85.6 14.7× Grid-GCN RTX 2080 GPU S3DIS PointNet mIoU/53.2 1.62× RandLA-Net RTX 2080Ti GPU S3DIS PointNet* mIoU/70.0 1.04× PointV oxel GTX 1080Ti GPU ShapeNet PointNet mIoU/46.9 2.46× ShapeNet Parts: ShapeNet Parts (Chang et al., 2015) for part segmentation. ShapeNet contains 16,881 shapes from 16 categories, 2,048 points are sampled from each training shape. Each shape is split into two to ﬁve parts depending on the category, making up to 50 parts in total. S3DIS: S3DIS for semantic segmentation (Armeni et al., 2016). S3DIS includes 3D scan point clouds for 6 indoor areas including 272 rooms in total, each point belongs to one of 13 semantic categories. We follow the ofﬁcial code (Qi et al., 2017a) for training and testing. E.2 I MPLEMENTATION DETAILS OF BIPOINT NET We follow the popular PyTorch implementation of PointNet and the recent geometric deep learning codebase (Fey & Lenssen, 2019) for the implementation of PointNet baselines. Our BiPointNet is built by binarizing the full-precision PointNet. All linear layers in PointNet except the ﬁrst and last one are binarized to bi-linear layer, and we select Hardtanh as our activation function instead of ReLU when we binarize the activation before the bi-linear layer. For the part segmentation task, we follow the convention (Wu et al., 2014; Yi et al., 2016) to train a model for each of the 16 classes. We also provide our PointNet baseline under this setting. Following previous works, we train 200 epochs, 250 epochs, 128 epochs on point cloud classiﬁca- tion, part segmentation, semantic segmentation respectively. To stably train the binarized models, we use a learning rate of 0.001 with Adam and Cosine Annealing learning rate decay for all binarized models on all three tasks. E.3 M ORE BACKBONES We also propose four other models: BiPointCNN, BiPointNet++, BiDGCCN, and BiPointConv, which are binarized versions of PointCNN (Li et al., 2018), PointNet++ (Qi et al., 2017b), DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019), respectively. This is attributed to the fact that all these variants have characteristics in common, such as linear layers for point-wise feature extraction and global pooling layers for feature aggregation (except PointConv, which does not have explicit aggregators). In PointNet++, DGCNN, and PointConv, we keep the ﬁrst layer and the last layer full-precision and binarize all the other layers. In PointCNN, we keep every ﬁrst layer of XConv full precision and keep the last layer of the classiﬁer full precision. E.4 B INARIZATION METHODS For comparison, we implement various representative binarization methods for 2D vision, including BNN (Hubara et al., 2016), XNOR-Net (Rastegari et al., 2016), Bi-Real Net (Liu et al., 2018), XNOR++ (Bulat & Tzimiropoulos, 2019), ABC-Net (Lin et al., 2017), and IR-Net (Qin et al., 2020b), to be applied on 3D point clouds. Note that the Case 1 version of XNOR++ is used in our experiments for a fair comparison, which applies layerwise learnable scaling factors to mini- mize the quantization error. These methods are implemented according to their open-source code or the description in their papers, and we take reference of their 3x3 convolution design when im- plementing the corresponding bi-linear layers. We follow their training process and hyperparameter 21Published as a conference paper at ICLR 2021 settings, but note that the speciﬁc shortcut structure in Bi-Real and IR-Net is ignored since it only applies to the ResNet architecture. E.5 T RAINING DETAILS Our BiPointNet is trained from scratch (random initialization) without leveraging any pre-trained model. Amongst the experiments, we apply Adam as our optimizer and use the cosine annealing learning rate scheduler to stably optimize the networks. To evaluate our BiPointNet on various network architectures, we mostly follow the hyper-parameter settings of the original papers (Qi et al., 2017a; Li et al., 2018; Qi et al., 2017b; Wang et al., 2019a). E.6 D ETAILED RESULTS OF SEGMENTATION We present the detailed results of part segmentation on ShapeNet Part in Table 6 and semantic segmentation on S3DIS in Table 7. The detailed results further prove the conclusion of Section 4.1 as EMA and LSR improve performance considerably in most of the categories (instead of huge performance in only a few categories). This validates the effectiveness and robustness of our method. 22Published as a conference paper at ICLR 2021 Table 6: Detailed results of our BiPointNet for part segmentation on ShapeNet Parts. aggr. mean aero bag cap car chair ear phone guitar knife lamp laptop motor mug pistol rocket skate board table # shapes 2690 76 55 898 3758 69 787 392 1547 451 202 184 283 66 152 5271 FP max 84.3 83.6 79.4 92.5 76.8 90.8 70.2 91.0 85.6 81.9 95.6 64.4 93.5 80.9 54.5 70.6 81.5 FP avg 84.0 83.4 78.5 90.8 76.3 90.0 73.1 90.8 84.3 80.8 95.5 61.7 93.8 81.6 56.2 72.2 81.8 BNN max 54.0 35.1 48.1 65.5 26.5 55.8 57.1 48.8 62.2 48.6 90.1 23.1 68.3 57.5 31.3 43.7 66.8 BNN ema-avg 53.0 39.8 46.5 57.5 24.1 58.2 56.2 44.0 50.0 53.0 81.0 16.9 48.8 36.3 25.7 43.7 63.3 BNN ema-max 47.3 37.9 46.2 44.6 24.1 61.3 38.2 33.5 42.6 50.8 48.6 16.9 49.0 25.2 26.8 43.7 50.30 LSR max 58.7 41.5 46.2 80.2 39.2 75.3 46.0 47.8 75.5 50.0 93.8 25.4 51.0 60.2 36.2 43.7 61.4 Ours ema-avg 80.3 79.3 71.9 85.5 66.1 87.7 65.6 84.1 82.8 76.0 94.8 42.7 91.8 75.9 47.2 59.1 79.7 Ours ema-max 80.6 79.5 69.7 86.1 67.4 88.6 68.5 87.4 83.0 74.9 95.1 44.8 91.6 76.3 47.7 56.9 79.5 23Published as a conference paper at ICLR 2021 Table 7: Detailed results of our BiPointNet for semantic segmentation on S3DIS. method aggr overall mIoU overall acc. area1 (mIoU/acc.) area2 (mIoU/acc.) area3 (mIoU/acc.) area4 (mIoU/acc.) area5 (mIoU/acc.) area6 (mIoU/acc.) ceiling IoU ﬂoor IoU wall IoU beam IoU column IoU window IoU door IoU table IoU chair IoU sofa IoU bookcase IoU board IoU clutter IoU FP max 54.4 83.5 61.7/86.2 38.0/76.8 62.4/88.0 45.0/82.4 45.3/83.3 70.0/89.2 91.1 93.8 72.8 50.3 34.6 52.0 58.0 55.8 51.3 14.5 44.4 43.4 45.2 FP avg 51.5 81.5 59.9/84.6 35.4/72.4 61.2/87.2 43.8/81.2 42.0/81.2 68.2/88.3 90.1 89.1 71.7 46.1 33.7 53.5 53.8 53.8 47.8 9.4 40.4 38.7 41.8 BNN max 9.5 45.0 9.6/44.0 9.8/50.5 8.3/41.9 9.3/42.5 9.5/45.8 9.8/41.6 45.5 40.6 28.1 0 0 0 0 0 7.7 0 0 0 2.1 BNN ema-avg 9.9 46.8 7.6/36.6 11.2/51.2 7.1/36.5 9.8/46.0 11.4/54.8 8.6/41.6 51.5 35.1 32.1 0 0 0 0 0.6 9.3 0 0 0 0.6 BNN ema-max 8.5 47.2 7.7/44.0 10.1/54.4 7.1/46.8 7.8/39.7 7.6/49.2 7.2/45.3 50.8 43.5 15.9 0 0 0 0 0 0 0 0 0 0 LSR max 2.0 25.4 2.0/26.0 2.1/27.0 2.0/25.7 1.8/22.8 2.0/25.8 1.9/24.5 25.4 0 0 0 0 0 0 0 0 0 0 0 0 Ours ema-avg 40.9 74.9 47.1/75.8 29.1/68.3 48.0/79.9 34.2/73.2 34.7/76.1 53.3/79.8 84.6 84.6 60.5 32.0 19.0 39.6 43.0 43.5 39.2 5.8 30.5 18.5 31.3 Ours ema-max 44.3 76.7 50.9/78.3 31.0/70.3 53.4/82.4 36.6/73.9 36.9/77.6 57.9/82.3 85.1 86.1 62.6 34.5 23.8 43.0 48.0 45.7 40.6 9.6 36.9 26.2 33.9 24",
      "meta_data": {
        "arxiv_id": "2010.05501v4",
        "authors": [
          "Haotong Qin",
          "Zhongang Cai",
          "Mingyuan Zhang",
          "Yifu Ding",
          "Haiyu Zhao",
          "Shuai Yi",
          "Xianglong Liu",
          "Hao Su"
        ],
        "published_date": "2020-10-12T07:54:51Z",
        "pdf_url": "https://arxiv.org/pdf/2010.05501v4.pdf",
        "github_url": "https://github.com/htqin/BiPointNet"
      },
      "llm_extracted_info": {
        "main_contributions": "BiPointNet is proposed as the first model binarization approach for efficient deep learning on point clouds, addressing resource constraints for real-time edge device applications. The paper identifies two key challenges causing immense performance drops in binarized models for point clouds: aggregation-induced feature homogenization (leading to information entropy degradation) and scale distortion (hindering optimization and invalidating scale-sensitive structures). To counter these, BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate feature distribution for maximum information entropy and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. BiPointNet achieves performance comparable to full-precision counterparts, significantly outperforming existing binarization methods. The techniques are generic, applicable across various fundamental tasks and mainstream backbones, and demonstrate impressive real-world deployment efficiency with 14.7x speedup and 18.9x storage saving on resource-constrained devices.",
        "methodology": "BiPointNet binarizes floating-point point cloud networks, typically MLPs, into binary MLPs (BiMLPs) composed of bi-linear layers that perform efficient bitwise operations. It incorporates two main techniques: 1. Entropy-Maximizing Aggregation (EMA): To tackle feature homogenization, EMA ensures the information entropy of binarized features is maximized by shifting the feature distribution towards a statistical optimum (where P(Y<0) = P(Y>=0) = 0.5). This is achieved by a transformation unit (e.g., a constant offset) before the aggregation function (max pooling or average pooling). For EMA-max, the optimal offset is found via Monte Carlo simulation, while for EMA-avg, it is 0. 2. Layer-wise Scale Recovery (LSR): To address scale distortion, LSR introduces a single learnable layer-wise scaling factor (α) for each bi-linear layer. This factor, initialized by the ratio of standard deviations between full-precision and binarized outputs, recovers the original scales, enhancing optimization and allowing scale-sensitive structures to function properly. Both techniques utilize the sign function for forward binarization and the Straight-Through Estimator (STE) for backpropagation.",
        "experimental_setup": "Extensive experiments were conducted on three fundamental point cloud tasks: classification on ModelNet40, part segmentation on ShapeNet Parts, and semantic segmentation on S3DIS. The primary base model for evaluation was PointNet, with further validation on mainstream backbones including PointNet++, PointCNN, DGCNN, and PointConv. Performance was measured using Overall Accuracy (OA) and mean Intersection over Union (mIoU). BiPointNet was compared against several existing binarization methods for 2D vision (BNN, XNOR-Net, Bi-Real Net, XNOR++, ABC-Net, IR-Net), sometimes augmented with EMA for a fairer comparison. Real-world deployment efficiency was tested on Raspberry Pi 4B (ARM Cortex-A72 CPU) and Raspberry Pi 3B (ARM Cortex-A53 CPU) to measure inference speedup and storage savings. Models were trained from scratch using the Adam optimizer and a cosine annealing learning rate scheduler.",
        "limitations": "The paper implies a limitation regarding the binarization of the first layer, noting that its quantization causes a drastic drop in accuracy without significant computational cost reduction, suggesting it's not effectively handled by the current approach. Also, the direct comparison of computation speedup and storage savings with other efficient learning methods is noted as for reference only, due to tests being conducted on different hardware and for different tasks, thus preventing direct conclusive comparisons.",
        "future_research_directions": "Future research could explore the development of new binarization-friendly aggregation functions, guided by the theoretical framework established in this work.",
        "experimental_code": "class BinaryQuantize(Function):    @staticmethod    def forward(ctx, input):        ctx.save_for_backward(input)        out = torch.sign(input)        return out    @staticmethod    def backward(ctx, grad_output):        input = ctx.saved_tensors        grad_input = grad_output        grad_input[input[0].gt(1)] = 0        grad_input[input[0].lt(-1)] = 0        return grad_input\nclass BiLinearLSR(torch.nn.Linear):    def __init__(self, in_features, out_features, bias=False, binary_act=True):        super(BiLinearLSR, self).__init__(in_features, out_features, bias=bias)        self.binary_act = binary_act        self.register_parameter('scale', Parameter(torch.Tensor([0.0]).squeeze()))    def reset_scale(self, input):        bw = self.weight        ba = input        bw = bw - bw.mean()        self.scale = Parameter((F.linear(ba, bw).std() / F.linear(torch.sign(ba), torch.sign(bw)).std()).float().to(ba.device))        if torch.isnan(self.scale):            self.scale = Parameter((bw.std() / torch.sign(bw).std()).float().to(ba.device))    def forward(self, input):        bw = self.weight        ba = input        bw = bw - bw.mean()        if self.scale.item() == 0.0:            self.reset_scale(input)        bw = BinaryQuantize().apply(bw)        bw = bw * self.scale        if self.binary_act:            ba = BinaryQuantize().apply(ba)        output = F.linear(ba, bw)        return output\nclass BiConv1dLSR(torch.nn.Conv1d):    def __init__(self, in_channels, out_channels, kernel_size, stride=1,                 padding=0, dilation=1, groups=1,                 bias=False, padding_mode='zeros'):        super(BiConv1dLSR, self).__init__(            in_channels, out_channels, kernel_size, stride, padding, dilation,            groups, bias, padding_mode)        self.register_parameter('scale', None)    def reset_scale(self, input):        bw = self.weight        ba = input        bw = bw - bw.mean()        if self.padding_mode == 'circular':            expanded_padding = ((self.padding[0] + 1) // 2, self.padding[0] // 2)            self.scale = Parameter((F.conv1d(F.pad(ba, expanded_padding, mode='circular'),                                  bw, self.bias, self.stride,                                  _single(0), self.dilation, self.groups).std() /                 F.conv1d(torch.sign(F.pad(ba, expanded_padding, mode='circular')),                         torch.sign(bw), self.bias, self.stride,                         _single(0), self.dilation, self.groups).std()).float().to(ba.device))        else:            self.scale = Parameter((F.conv1d(ba, bw, self.bias, self.stride, self.padding, self.dilation, self.groups).std()                                   / F.conv1d(torch.sign(ba), torch.sign(bw), self.bias, self.stride, self.padding, self.dilation, self.groups).std()).float().to(ba.device))    def forward(self, input):        bw = self.weight        ba = input        bw = bw - bw.mean()        if self.scale is None:            self.reset_scale(input)        bw = BinaryQuantize().apply(bw)        ba = BinaryQuantize().apply(ba)        bw = bw * self.scale        if self.padding_mode == 'circular':            expanded_padding = ((self.padding[0] + 1) // 2, self.padding[0] // 2)            return F.conv1d(F.pad(ba, expanded_padding, mode='circular'),                            bw, self.bias, self.stride,                            _single(0), self.dilation, self.groups)        return F.conv1d(ba, bw, self.bias, self.stride,                        self.padding, self.dilation, self.groups)\noffset_map = {    1024: -3.2041,    2048: -3.4025,    4096: -3.5836}\nclass BiSTN3d(nn.Module):    def __init__(self, channel, Linear=BiLinear, pool='max', affine=True, bi_first=False):        super(BiSTN3d, self).__init__()        if bi_first:            self.conv1 = Conv1d(channel, 64, Linear)        else:            self.conv1 = Conv1d(channel, 64, nn.Linear)        self.conv2 = Conv1d(64, 128, Linear)        self.conv3 = Conv1d(128, 1024, Linear)        self.fc1 = Linear(1024, 512)        self.fc2 = Linear(512, 256)        self.fc3 = Linear(256, 9)        self.bn1 = nn.BatchNorm1d(64, affine=affine)        self.bn2 = nn.BatchNorm1d(128, affine=affine)        self.bn3 = nn.BatchNorm1d(1024, affine=affine)        self.bn4 = nn.BatchNorm1d(512, affine=affine)        self.bn5 = nn.BatchNorm1d(256, affine=affine)        self.pool = pool    def forward(self, x):        batchsize, D, N = x.size()        x = F.hardtanh(self.bn1(self.conv1(x)))        x = F.hardtanh(self.bn2(self.conv2(x)))        if self.pool == 'max':            x = F.hardtanh(self.bn3(self.conv3(x)))            x = torch.max(x, 2, keepdim=True)[0]        elif self.pool == 'mean':            x = F.hardtanh(self.bn3(self.conv3(x)))            x = torch.mean(x, 2, keepdim=True)        elif self.pool == 'ema-max':            x = self.bn3(self.conv3(x)) + offset_map[N]            x = torch.max(x, 2, keepdim=True)[0]            x = x.view(-1, 1024)        x = x.view(-1, 1024)        x = F.hardtanh(self.bn4(self.fc1(x)))        x = F.hardtanh(self.bn5(self.fc2(x)))        x = self.fc3(x)        iden = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32))).view(1, 9).repeat(            batchsize, 1)        if x.is_cuda:            iden = iden.cuda()        x = x + iden        x = x.view(-1, 3, 3)        return x\nclass BiPointNetEncoder(nn.Module):    def __init__(self, Linear, global_feat=True, feature_transform=False, channel=3, pool='max', affine=True, tnet=True, bi_first=False, use_bn=True):        super(BiPointNetEncoder, self).__init__()        self.tnet = tnet        if self.tnet:            self.stn = BiSTN3d(channel, Linear, pool=pool, affine=affine, bi_first=bi_first)        if bi_first:            self.conv1 = Conv1d(channel, 64, Linear)        else:            self.conv1 = Conv1d(channel, 64, nn.Linear)        self.conv2 = Conv1d(64, 128, Linear)        self.conv3 = Conv1d(128, 1024, Linear)        self.bn1 = nn.BatchNorm1d(64, affine=affine)        self.bn2 = nn.BatchNorm1d(128, affine=affine)        self.bn3 = nn.BatchNorm1d(1024, affine=affine)        self.global_feat = global_feat        self.feature_transform = feature_transform        if self.tnet and self.feature_transform:            self.fstn = BiSTNkd(k=64, Linear=Linear, pool=pool, affine=affine, bi_first=bi_first)        self.pool = pool        self.use_bn = use_bn    def forward(self, x):        B, D, N = x.size()        if self.tnet:            trans = self.stn(x)        else:            trans = None        x = x.transpose(2, 1)        if D == 6:            x, feature = x.split(3, dim=2)        elif D == 9:            x, feature = x.split([3, 6], dim=2)        if self.tnet:            x = torch.bmm(x, trans)        if D > 3:            x = torch.cat([x, feature], dim=2)        x = x.transpose(2, 1)        if self.use_bn:            x = F.hardtanh(self.bn1(self.conv1(x)))        else:            x = F.hardtanh(self.conv1(x))        if self.tnet and self.feature_transform:            trans_feat = self.fstn(x)            x = x.transpose(2, 1)            x = torch.bmm(x, trans_feat)            x = x.transpose(2, 1)        else:            trans_feat = None        pointfeat = x        if self.use_bn:            x = F.hardtanh(self.bn2(self.conv2(x)))            x = self.bn3(self.conv3(x))        else:            x = F.hardtanh(self.conv2(x))            x = self.conv3(x)        if self.pool == 'max':            x = torch.max(x, 2, keepdim=True)[0]        elif self.pool == 'mean':            x = torch.mean(x, 2, keepdim=True)        elif self.pool == 'ema-max':            if self.use_bn:                x = torch.max(x, 2, keepdim=True)[0] + offset_map[N]            else:                x = torch.max(x, 2, keepdim=True)[0] - 0.3            x = x.view(-1, 1024)        x = x.view(-1, 1024)        if self.global_feat:            return x, trans, trans_feat        else:            x = x.view(-1, 1024, 1).repeat(1, 1, N)            return torch.cat([x, pointfeat], 1), trans, trans_feat\noffset_map = {  64: -2.2983,  1024: -3.2041}\nclass SAModule(torch.nn.Module):    def __init__(self, ratio, r, nn, aggr='max', random_start=True):        super(SAModule, self).__init__()        self.ratio = ratio        self.r = r        self.random_start = random_start        if aggr == 'max':            self.aggr = 'max'            self.ema_max = False        elif aggr == 'mean':            self.aggr = 'mean'            self.ema_max = False        elif aggr == 'bmax':            self.aggr = 'max'            self.ema_max = True        self.conv = PointConv(nn, aggr=self.aggr)    def forward(self, x, pos, batch):        idx = fps(pos, batch, ratio=self.ratio, random_start=self.random_start)        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],                          max_num_neighbors=64)        edge_index = torch.stack([col, row], dim=0)        x = self.conv(x, (pos, pos[idx]), edge_index)        if self.ema_max:            x = x + offset_map[64]        pos, batch = pos[idx], batch[idx]        return x, pos, batch\nclass GlobalSAModule(torch.nn.Module):    def __init__(self, nn, aggr='max'):        super(GlobalSAModule, self).__init__()        self.nn = nn        self.aggr = aggr        assert self.aggr in ['max', 'mean', 'ema-max']    def forward(self, x, pos, batch):        x = self.nn(torch.cat([x, pos], dim=1))        if self.aggr == 'max':            x = global_max_pool(x, batch)        elif self.aggr == 'mean':            x = global_mean_pool(x, batch)        elif self.aggr == 'ema-max':            x = global_max_pool(x, batch) + offset_map[1024]        pos = pos.new_zeros((x.size(0), 3))        batch = torch.arange(x.size(0), device=batch.device)        return x, pos, batch",
        "experimental_info": "BiPointNet binarizes point cloud networks, typically MLPs, using bi-linear layers that perform efficient bitwise operations. It incorporates two key techniques:\n\n1.  **Entropy-Maximizing Aggregation (EMA):** This technique addresses feature homogenization by maximizing the information entropy of binarized features, shifting their distribution towards a statistical optimum where P(Y<0) = P(Y>=0) = 0.5. EMA is implemented by adding a pre-computed constant offset to features before the aggregation function (specifically max pooling, denoted as 'ema-max'). The `offset_map` dictionaries contain these empirically determined offsets for different feature dimensions or point counts (e.g., -3.2041 for 1024-dim features, -2.2983 for 64-dim features). This is integrated into pooling operations within `BiSTN3d`, `BiPointNetEncoder` (for PointNet), and `SAModule`, `GlobalSAModule` (for PointNet2/DGCNN).\n\n2.  **Layer-wise Scale Recovery (LSR):** To mitigate scale distortion inherent in binarization, LSR introduces a single learnable layer-wise scaling factor (α) for each bi-linear layer. This factor is initialized by the ratio of standard deviations between the full-precision and binarized outputs of the layer, allowing the model to recover the original feature scales and improve optimization. LSR is implemented in `BiLinearLSR` for fully-connected layers and `BiConv1dLSR` for convolutional layers. The `scale` parameter is dynamically initialized during the first forward pass if it's zero.\n\n**Binarization Details:** Both techniques utilize the `torch.sign` function for the forward pass binarization (`BinaryQuantize` class). For backpropagation, a custom Straight-Through Estimator (STE) is used within `BinaryQuantize`, which clips gradients to zero where the input values are outside the range of [-1, 1].\n\n**Network Integration:** These techniques are applied to binarize standard point cloud networks like PointNet, PointNet++, and DGCNN. For example, `BasicBiPointNetEncoder` and `BasicBiPointNet2` replace full-precision linear/convolutional layers with `BiLinearLSR` (or `BiConv1dLSR`) and integrate the EMA offsets into their respective pooling mechanisms (e.g., STNs for PointNet, SA modules for PointNet2).\n\n**Training:** The overall training loop includes an NLL loss, an optimizer (e.g., Adam), and an optional learning rate scheduler. Orthogonal regularization can be applied to feature transformation matrices if they are present in the model (e.g., PointNet's T-Nets)."
      }
    },
    {
      "title": "Efficient Hierarchical Entropy Model for Learned Point Cloud Compression"
    },
    {
      "title": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis",
      "abstract": "Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian\nsplat representation has been introduced for novel view synthesis from sparse\nimage sets. Making such representations suitable for applications like network\nstreaming and rendering on low-power devices requires significantly reduced\nmemory consumption as well as improved rendering efficiency. We propose a\ncompressed 3D Gaussian splat representation that utilizes sensitivity-aware\nvector clustering with quantization-aware training to compress directional\ncolors and Gaussian parameters. The learned codebooks have low bitrates and\nachieve a compression rate of up to $31\\times$ on real-world scenes with only\nminimal degradation of visual quality. We demonstrate that the compressed splat\nrepresentation can be efficiently rendered with hardware rasterization on\nlightweight GPUs at up to $4\\times$ higher framerates than reported via an\noptimized GPU compute pipeline. Extensive experiments across multiple datasets\ndemonstrate the robustness and rendering speed of the proposed approach.",
      "full_text": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis Simon Niedermayr simon.niedermayr@tum.de Josef Stumpfegger ga87tux@mytum.de R¨udiger Westermann westermann@tum.de Technical University of Munich Abstract Recently, high-fidelity scene reconstruction with an op- timized 3D Gaussian splat representation has been intro- duced for novel view synthesis from sparse image sets. Mak- ing such representations suitable for applications like net- work streaming and rendering on low-power devices re- quires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity- aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to 31× on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to 4× higher framerates than reported via an opti- mized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach. 1. Introduction Novel view synthesis aims to generate new views of a 3D scene or object by interpolating from a sparse set of images with known camera parameters. NeRF [16] and its variants have proposed the use of direct volume rendering to learn a volumetric radiance field from which novel views can be rendered. However, expensive neural network evaluations prohibit efficient training and rendering. Recent research utilizes explicit scene representations such as voxel-based [24] or point-based structures [29] to enhance rendering ef- ficiency. The use of 3D voxel grids on the GPU in com- bination with a multiresolution hash encoding of the input [17] significantly reduces the operations needed and permits real-time performance. While achieving excellent reconstruction quality and speed, many NeRF-style approaches require exhaustive 93 FPS54 FPS 321 FPS211 FPS 25.2 PSNR /1.5 GB 25.0 PSNR /47 MB 31× Compression Figure 1. Our method achieves a31× compression at indiscernible loss in image quality and greatly improves rendering speed com- pared to [13]. Framerates in grey and white, respectively, are taken on NVIDIA’s RTX 3070M and RTX A5000 at 1080p resolution. memory resources. This affects both the training and ren- dering times and often prohibits the use of such represen- tations in applications like network streaming and mobile rendering. To overcome these limitations, dedicated com- pression schemes for the learned parametrizations on reg- ular grids have been proposed, including vector quantized feature encoding [15], learned tensor decomposition [3] or frequency domain transformations[20, 32]. Recently, differentiable 3D Gaussian splatting [13] has been introduced to generate a sparse adaptive scene repre- sentation that can be rendered at high speed on the GPU.The scene is modeled as a set of 3D Gaussians with shape and appearance parameters, which are optimized via differen- tiable rendering to match a set of recorded images. The optimized scenes usually consist of millions of Gaussians and require up to several gigabytes of storage and mem- ory. This makes rendering difficult or even impossible on low-end devices with limited video memory, such as hand- helds or head-mounted displays. Gaussians are rendered us- ing a specialized compute pipeline, which shows real-time performance on high-end GPUs. This pipeline, however, cannot be seamlessly integrated into VR/AR environments 1 arXiv:2401.02436v2  [cs.CV]  22 Jan 2024or games to work in tandem with hardware rasterization of polygon models. We address the storage and rendering issue of 3D Gaus- sian splatting by compressing the reconstructed Scene pa- rameters and rendering the compressed representation via GPU rasterization. To compress the scenes, we first analyze its components and observe that the SH coefficients and the multivariate Gaussian parameters take up the majority of storage space and are highly redundant. Inspired by previ- ous work on volumetric radiance field compression[15, 25] and deep network weight quantization, we derive a com- pression scheme that reduces the storage requirements of typical scenes by up to a factor of 31×. Our compression scheme consists of three main steps: • Sensitivity-aware clustering: We derive a sensitivity mea- sure for each scene parameter by calculating its contribu- tion to the training images. Color information and Gaus- sian parameters are encoded into compact codebooks via sensitivity-aware vector quantization. • Quantization-aware fine-tuning: To regain information that is lost during clustering we fine-tune the scene pa- rameters at reduced bit-rates using quantization-aware training. • Entropy encoding: 3D Gaussians are linearized along a space-filling curve to exploit the spatial coherence of scene parameters with entropy and run-length encoding. Further, we propose a renderer for the compressed scenes using GPU sorting and rasterization. It enables novel view synthesis in real-time, even on low-end devices, and can be easily integrated into applications rendering polygonal scene representations. Due to the reduced memory band- width requirements of the compressed representation and the use of hardware rasterization, a significant speed-up is achieved over the compute pipeline by Kerbl et al. [13]. We show the state-of-the-art quality of novel view ren- dering on benchmark datasets at significantly reduced mem- ory consumption and greatly improved rendering perfor- mance (Fig. 1). The compressed scenes can be used in applications requiring network streaming, and they can be rendered on low-end devices with limited video memory and bandwidth capacities. We perform a number of exper- iments on benchmark datasets to empirically validate our method across different scenarios. The contribution of each individual step is demonstrated with an ablation study. 2. Related Work Our work builds upon previous works in novel view synthe- sis via differentiable rendering and scene compression. Novel View SynthesisNeural Radiance Fields (NeRF) [16] use neural networks to model a 3D scene. They rep- resent the scene as a density field with direction-dependent colors that are rendered with volume rendering. The field is reconstructed from a set of images with known camera pa- rameters using gradient-based optimization of the volume rendering process. To speed up training and rendering efficiency, a number of different scene models have been proposed. Most often, structured space discretizations like voxel grids [10, 24, 28], octrees [6] or hash grids [17] are used to represent the scene. To avoid encoding empty space, point-based representa- tions have been proposed. Xu et al. [29] perform nearest neighbor search in a point cloud to aggregate local features, and R¨uckert et al. [21] render a point cloud with deep fea- tures and use deferred neural rendering to generate the final image. More recently, differentiable splatting [7, 13] has been positioned as a powerful alternative to NeRF-like ap- proaches for novel view synthesis. In particular, 3D Gaus- sian Splatting [13] offers state-of-the-art scene reconstruc- tion, by using a scene model consisting of an optimized set of 3D Gaussian kernels that can be rendered efficiently. Dif- ferentiable rendering on a set of training images is used to adaptively refine an initial set of Gaussian kernels and opti- mize their parameters. NeRF Compression While grid-based NeRF variants achieve high rendering performance due to GPU ray- marching, in particular, the use of full spatial grids intro- duces considerable storage costs. Tensor decomposition [3, 26], frequency domain transformation [20, 32] and voxel pruning [4] have been proposed to reduce the memory con- sumption of grid-based NeRFs. Takikawa et al. [25] per- form vector quantization during training with a learnable index operation. Li et al. [15] compress grid-based radiance fields by up to a factor of 100× using post-training vector quantization. The use of a hash encoding on the GPU in combination with vector quantization of latent features re- duces the required memory and permits high rendering per- formance [17] A number of works have especially addressed memory reduction during inference, to make grid-based scene rep- resentations more suitable for low-end devices with limited video memory [19, 27]. To our knowledge, our approach is the first that aims at the compression of point-based ra- diance fields to enable high-quality novel view synthesis at interactive frame rates on such devices. Quantization-Aware TrainingRastegari et al. [18] sim- ulate weight quantization during training to reduce quanti- zation errors when using low-precision weights for infer- ence. The use of quantization-aware training has been ex- plored for neural scene representations [8] and voxel-based NeRFs [12], demonstrating effective weight quantization with negligible loss in rendering quality. To reduce the size and latency of neural networks, vari- ous approaches for weight quantization have been explored [8, 11, 12, 18]. These methods rely on the observation that 2 Parameter Sensitivity Calculation Quantization-Aware  Fine-Tuning Codebook Gaussians Codebook Color ... ... Sensitivity Aware Vector Clustering Storage 26.8 PSNR / 47 MB Compressed Scene Reconstructed Scene 27.2 PSNR / 1.4 GB Morton Order Sorting Entropy & Run Length Encoding Figure 2. Proposed compression pipeline. Input is an optimized 3D Gaussian scene representation. First, a sensitivity measure is computed for the Gaussian parameters, and color and shape information is compressed into separate codebooks using sensitivity-aware and scale- invariant vector clustering. Next, the compressed scene is fine-tuned on the training images to recover lost information. Finally, the Gaussians are sorted in Morton order and further compressed using entropy and run-length encoding. The shown scene is from [2]. in most cases a lower weight precision is required for model inference than for training (e.g., 8-bit instead of 32-bit). In post-training quantization, the model weights are reduced to a lower bit representation after training. In quantization- aware training, the quantization is simulated during training while operations are performed at full precision to obtain numerically stable gradients. For storage and inference, the low precision weights can then be used with minor effects on the output. 3. Differentiable Gaussian Splatting Differentiable Gaussian splatting [13] builds upon EW A volume splatting [34] to efficiently compute the projections of 3D Gaussian kernels onto the 2D image plane. On top of that, differentiable rendering is used to optimize the num- ber and parameters of the Gaussian kernels that are used to model the scene. The final scene representation comprises a set of 3D Gaussians, each described by a covariance matrix Σ ∈ R3×3 centered at location x ∈ R3. The covariance matrix can be parameterized by a rotation matrix R and a scaling matrix S. For independent optimization of R and S, Kerbl et al . [13] represent the rotation with a quaternion q and scaling with a vector s, both of which can be converted into their respective matrices. In addition, each Gaussian has its own opacity α ∈ [0, 1] and a set of spherical harmonics (SH) coefficients to reconstruct a view-dependent color. The 2D projection of a 3D Gaussian is again a Gaussian with covariance Σ′ = JW ΣWT JT , (1) where W is the view transformation matrix and J is the Jacobian of the affine approximation of the projective trans- formation. This allows to evaluate the 2D color and opacity footprint of each projected Gaussian. A pixel’s color C is then computed by blending all N 2D Gaussians contribut- ing to this pixel in sorted order: C = X i∈N ciαi i−1Y j=1 (1 − αj). (2) Here, ci and αi, respectively, are the view-dependent color of a Gaussian and its opacity, modulated by the exponential falloff from the projected Gaussian’s center point. The position x, rotation q, scaling s, opacity α, and SH coefficients of each 3D Gaussian are optimized so that the rendered 2D Gaussians match the training images. For more details on the reconstruction process, we refer to the original paper by Kerbl et al. [13]. 4. Sensitivity-Aware Scene Compression We compress a set of optimized 3D Gaussian kernels as fol- lows: First, sensitivity-aware vector clustering is used to cluster the Gaussian appearance and shape parameters into compact codebooks (Sec. 4.1). Second, the clustered and other scene parameters are fine-tuned on the training im- ages to recover information lost due to clustering. We use quantization-aware training in this step to reduce the scene parameters to a lower bit-rate representation (Sec. 4.2). By linearizing the set of 3D Gaussians along a space-filling curve, entropy and run-length encoding can exploit the spa- tial coherence of Gaussian parameters to further compress the scene (Sec. 4.3). An overview of the proposed compres- sion pipeline is shown in Fig. 2. 4.1. Sensitivity-Aware Vector Clustering Inspired by volumetric NeRF compression [15, 25], we uti- lize vector clustering for compressing 3D Gaussian kernels. We use clustering to encode SH coefficients and Gaussian shape features (scale and rotation) into two separate code- books. As a result, each Gaussian can be compactly en- coded via two indices into the codebooks stored alongside. 3Parameter Sensitivity: The sensitivity of the recon- struction quality to changes of the Gaussian parameters is not consistent. While a slight change in one parameter of a Gaussian can cause a significant difference in the rendered image, a similar change in another parameter or the same parameter of another Gaussian can have low or no effect. We define the sensitivity S of image quality to changes in parameter p with respect to the training images as S(p) = 1PN i=1 Pi NX i=1 \f\f∂Ei ∂p \f\f. (3) N is the number of images in the training set used for scene reconstruction, and Pi is the number of pixels in imagei. E is the total image energy, i.e., the sum of the RGB com- ponents over all pixels. The sensitivity of E to changes in p is considered via the gradient of E with respect to p, i.e., a large gradient magnitude indicates high sensitivity to changes in the respective parameter. With this formulation, the sensitivity to every parameter can be computed with a single backward pass over each of the training images. Sensitivity-aware k-Means: Given a vector x ∈ RD, we define its sensitivity as the maximum over its compo- nent’s sensitivity: S(x) = max d∈[1..D] S(xd). (4) The sensitivity measure is then used for sensitivity-aware clustering, i.e., to compute codebooks C ∈ RK×D with K representatives ck ∈ RD (so-called centroids). We define the weighted distance between a vector x and a centroid ck as D(x, ck) = S(x)∥x − ck∥2 2. (5) A codebook is then obtained by using k-Means clustering with D as a similarity measure. The codebooks are initial- ized randomly with a uniform distribution within the min- imum and maximum values of each parameter. The cen- troids are computed with an iterative update strategy: In each step, the pairwise weighted distances between the vec- tors x and the codebook vectors ck are calculated, and each vector is assigned to the centroid to which it has the mini- mum distance. Each centroid is then updated as ck = 1P xi∈A(k) S(xi) X xi∈A(k) S(xi)xi (6) Where A(k) is the set of vectors assigned to centroid ck. For performance reasons, a batched clustering strategy is used [23]. In each update step, a random subset of vectors is picked and used to compute the update step. Then, the cen- troids are updated using the moving average with a decay factor λd. 0 max 0% 5% 100%percentage of Gaussians Garden 0 max sensitivity Truck 0 max Playroom Figure 3. Histograms of maximum sensitivity to changes of SH coefficients for different scenes. Only SH coefficients of a tiny fraction of all Gaussians strongly affect image quality. Color Compression: Each Gaussian stores SH coeffi- cients to represent the direction-dependent RGB color (e.g., 48 coefficients in [13]). We treat SH coefficients as vectors and compress them into a codebook using sensitivity-aware vector clustering. For volumetric NeRF models, Li et al. [15] have shown that only a small number of voxels contribute significantly to the training images. Thus, they propose to keep the color features that contribute the most and only compress the re- maining features with vector clustering. We observe a sim- ilar behavior for 3D Gaussians, as shown for some bench- mark scenes in Fig. 3. For a small percentage of all SH co- efficients (< 5%), the sensitivity measure indicates a high sensitivity towards the image quality. Thus, to keep the in- troduced rendering error low, we do not consider the SH vectors of Gaussians with a sensitivity higher than a thresh- old βc in the clustering process. These vectors are added to the codebook after clustering. Gaussian Shape Compression: A 3D Gaussian kernel can be parameterized with a rotation matrixR and a scaling vector s. We observe that for typical scenes, the shapes of the Gaussians are highly redundant up to a scaling factor. Thus, we re-parameterize the scaling vector s = ηˆs, where η = ∥s∥2 is the scalar scaling factor and ˆs = η−1s is the normalized scaling vector. With ˆS = diag(ˆs), the normal- ized covariance matrix is ˆΣ = (R ˆS)(R ˆS)T = 1 η2 Σ. (7) Clustering is then performed using the normalized co- variance matrices, and each Gaussian stores, in addition to a codebook index, the scalar scaling factor η. We com- pute the sensitivity to each of the matrix entries and perform sensitivity-aware vector quantization to compress them into a codebook. The sensitivity plots for Gaussian shape pa- rameters look mostly similar to the SH plots shown in Fig. 3. As for SH coefficients, Gaussians with a maximum sensitivity over a threshold βg are not considered for clus- tering and are added to the codebook. 4Note that k-Means clustering of normalized covariance matrices results in covariance matrices that are again nor- malized. However, due to floating point errors, clustering can lead to non-unit scaling vectors. To counteract this problem, we re-normalize each codebook vector after each update step by dividing it through the trace of the covariance metric. In the appendix, we prove both the normalization preserving properties of k-Means and re-normalization. After clustering, each codebook entry is decomposed into a rotation and scale parameter using an eigenvalue decomposition. This is required for quantization-aware training since direct optimization of the matrix is not possible[13]. In the final codebook, each matrix’s rotation and scaling parameters are encoded via 4 (quaternion) plus 3 (scaling) scalar values. 4.2. Quantization-Aware Fine-Tuning To regain information that is lost due to parameter quan- tization, the parameters can be fine-tuned on the training images after compression [15, 30]. To do so, we use the training setup described by Kerbl et al. [13]. We optimize for the position, opacity, and scaling factor of each Gaus- sian as well as the color and Gaussian codebook entries. For the two codebooks, the incoming gradients for each en- try are accumulated and then used to update the codebook parameters. For fine-tuning, we utilize quantization-aware training with Min-Max quantization (k-bit Quantization [18]) to rep- resent the scene parameters with fewer bits. In the for- ward pass, the quantization of a parameter p is simulated using a rounding operation considering the number of bits and the moving average of each parameter’s minimum and maximum values. The backward pass ignores the simulated quantization and calculates the gradient w.r.t. p as without quantization. After training, the parameters can be stored with only b-bit precision (e.g., 8-bit), while the minimum and maximum values required for re-scaling are stored at full precision (e.g., 32-bit float). Quantization of opacity is applied after the sigmoid ac- tivation function. Quantization of the scaling and rotation vector is applied before the respective normalization step. For the scale factor parameter, the quantization is applied before the activation (exponential function) to allow for a fine-grained representation of small Gaussians without los- ing the ability to model large ones. We quantize all Gaus- sian parameters despite position to an 8-bit representation with the Min-Max scheme. 16-bit float quantization is used for position, as a further reduction decreases the reconstruc- tion quality considerably. 4.3. Entropy Encoding After quantization-aware fine-tuning, the compressed scene representation consists of a set of Gaussians and the code- books storing SH coefficients and shape parameters. Indices into the codebooks are stored as 32-bit unsigned integers. The data is then compressed using DEFLATE [5], which utilizes a combination of the LZ77 [33] algorithm and Huff- man coding. In the reconstructed scenes, many features, such as color, scaling factor, and position, are spatially co- herent. By ordering the Gaussians according to their posi- tions along a Z-order curve in Morton order, the coherence can be exploited and the effectivity of run-length encoding (LZ77) can be improved. The effect on the compressed file size is analyzed in the ablation study in Sec. 6.4. Note that entropy encoding reduces the two codebook indices to their required bit-length according to the codebook sizes. 5. Novel View Rendering Kerbl et al. [13] propose a software rasterizer for differen- tiable rendering and novel view synthesis. To render 3D Gaussian scenes fast especially on low-power GPUs, our novel view renderer utilizes hardware rasterization. Preprocess: In a compute pre-pass, Gaussians whose 99% confidence interval does not intersect the view frustum after projection are discarded. For the remaining Gaussians, the direction-dependent color is computed with the SH co- efficients. The color, the Gaussian’s opacity, projected screen-space position, and covariance values are stored in an atomic linear-append buffer. The covariance values indi- cate the orientation and size of the 2D Gaussian into which a 3D Gaussian projects under the current viewing transfor- mation [34]. As in [13], Gaussians are then depth-sorted to enable order-dependent blending. We use the Onesweep sorting algorithm by Adinets and Merrill [1] to sort the Gaussians directly on the GPU. Due to its consistent per- formance, the implementation is well suited for embedding into real-time applications. Rendering: Gaussians are finally rendered in sorted or- der via GPU rasterization. For each Gaussian, one planar quad (a so-called splat) consisting of two triangles is ren- dered. A vertex shader computes the screen space vertex positions of each splat from the 2D covariance information. The size of a splat is set such that it covers the 99% confi- dence interval of the projected Gaussian. The vertex shader simply outputs the color computed in the pre-pass and the 2D splat center as input to the pixel shader. The pixel shader then discards fragments outside the 99% confidence inter- val. All remaining fragments use their distance to the splat center to compute the exponential color and opacity falloff and blend their final colors into the framebuffer. 6. Experiments 6.1. Datasets We evaluate our compression and rendering method on the Mip-Nerf360[2] indoor and outdoor scenes, two scenes 5Method 3D Gaussian Splatting Ours Dataset PSNR↑ SSIM↑ LPIPS↓ SIZE↓ PSNR↑ SSIM↑ LPIPS↓ SIZE↓ Compression Ratio↑ Synthetic-NeRF [16] 33.21 0 .969 0 .031 69 .89 32.936 0 .967 0 .033 3 .68 19.17 Mip-NeRF360 [2] 27.21 0 .815 0 .214 795 .26 26.981 0 .801 0 .238 28 .80 26.23 Tanks&Temples [14] 23.36 0 .841 0 .183 421 .90 23.324 0 .832 0 .194 17 .28 23.26 Deep Blending [9] 29.41 0 .903 0 .243 703 .77 29.381 0 .898 0 .253 25 .30 27.81 average* 26.58 0 .853 0 .213 640 .31 26.560 0 .844 0 .238 23 .73 25.77 Table 1. Quantitative comparison to 3D Gaussian Splatting. Size is measured in Megabytes. *Synthetic scenes are excluded. from the Tanks&Temples[14] and Deep Blending [9] dataset, and NeRF-Synthetic[16]. For Mip-Nerf360, Tanks&Temples and Deep Blending the reconstructions from Kerbl et al . [13] were used. We generated the 3D Gaussian representation for NeRF-Synthetic ourselves. 6.2. Implementation Details We use a decay factor λd = 0.8 for batched clustering with 800 update steps for the Gaussians and 100 for the SH co- efficients. A batch size of 218 is used for the color fea- tures, and 220 for the Gaussian parameters. We use 4096 as the default codebook size in all our experiments and set βc = 6 · 10−7 and βg = 3 · 10−6. We perform 5000 opti- mization steps of quantization-aware fine-tuning. The renderer is implemented with the WebGPU graph- ics API in the Rust programming language. Thus, it can run in a modern web browser on a large variety of devices. More details about the implementation can be found in the supplementary material. The source code is available at https://github.com/KeKsBoTer/c3dgs. 6.3. Results We use the scenes reconstructed by 3D Gaussian Splatting [13] and compress them using the proposed method. For all scenes, we evaluate the PSNR, SSIM, and LPIPS [31] before and after compression. Tab. 1 shows the results for different datasets. Our compression method achieves a compression ratio of up to 31× with an average of 26× at the indiscernible loss of quality (0.23 PSNR on average) for real-world scenes. Here, it should be noted that a difference of 0.5 PSNR is considered indistinguishable for the human eye [22]. For some of the scenes, Fig. 5 compares training images to the renderings of the uncompressed and compressed scenes. Fig. 4 shows close-up views of uncompressed and com- pressed synthetic scenes. More comparisons and results are given in the supplementary material. Image Quality LossFig. 5 shows that it is almost im- possible to spot the difference between the uncompressed and the compressed scenes. We also analyze the images from all test sets with the largest drop in PSRN. The image which could be reconstructed least accurately is shown in Fig. 6. We observe that the loss is mainly due to very subtle Baseline Compressed Baseline Compressed Figure 4. 3D Gaussian splatting of synthetic scenes [16]. Uncom- pressed (Baseline) vs. compressed scene. color shifts below what can be perceived by the human eye. Compression RuntimeThe compression process takes about 5-6 minutes and increases the reconstruction time by roughly 10%. The timings of each individual steps are given in the supplementary material. Rendering TimesWe see a significant increase of up to a factor of 4× in rendering speed (see Tab. 2). Roughly a 2x increase can be attributed to the compressed data’s reduced bandwidth requirements, hinting at the software rasterizer’s memory-bound performance by Kerbl et al. [13]. The ad- ditional speedup is achieved by the hardware rasterization- based renderer, which pays off on low- and high-end GPUs. Timings of the different rendering stages are given in the supplementary material. 6.4. Ablation Study In a number of experiments we evaluate the components of our compression pipeline. This includes a detailed analysis of the influence of the hyper-parameters. Loss Contribution Tab. 3 indicates that the most sig- 6Bicycle Train Playroom  Ground Truth  Baseline  Ours Figure 5. Ground truth images from the test set, results of Kerbl et al. [13] (Baseline), results using the compressed representation (Ours). a) Ground Truth  b) Baseline (35.90 PSNR)  c) Ours (34.15 PSNR)  d) Mean Absolute Error 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 Figure 6. Test image with the highest drop in PSNR in all scenes used in this work. d) Per pixel mean absolute error between Kerbl et al. [13] b) and our approach c). 7NVIDIA RTX A5000 NVIDIA RTX 3070M Intel UHD Graphics 11 AMD Radeon R9 380 Bicycle Kerbl et al. [13]93 54 - - Ours 215 134 9 41 Compressed 321 211 16 83 Bonsai Kerbl et al. [13]184 122 - - Ours 414 296 23 76 Compressed 502 380 28 128 Table 2. Rendering performance at 1080p resolution in frames per second, averaged over all training images. Bicycle consists of 6.1 million 3D Gaussians, Bonsai of 1.2 million 3D Gaussians. PSNR↑ SSIM↑ LPIPS↓ SIZE↓ baseline 27.179 0 .861 0 .115 1379.99 + Pruning 27.083 0 .856 0 .118 1217.25 + Color Clustering 25.941 0 .818 0 .178 278.41 + Gaussian Clustering25.781 0 .811 0 .186 164.15 + QA Finetune 26.746 0 .844 0 .144 86.69 + Encode 26.746 0 .844 0 .144 58.40 + Morton Order 26.746 0 .844 0 .144 46.57 Table 3. Losses introduced and regained by individual stages of the compression pipeline. Experiments were performed with the garden scene from Mip-Nerf360[2] nificant loss increase comes from the compression of the SH coefficients, which, on the other hand, gives the high- est memory reduction. Quantization of shape parameters can additionally reduce the memory by about 60%, only introducing a slight loss in image quality. Quantization- aware fine-tuning can regain much of the information that is lost due to quantization and further reduces the memory by about 50%. Entropy and run length encoding in combi- nation with Morton order layout saves an additional50% of the memory. Codebook Sizes SH coefficients and Gaussian shape parameters are compressed into codebooks of predefined sizes. Tab. 3 shows the effects of different codebook sizes on image quality. Errors were averaged over all test images, with the difference to the maximum error given in brack- ets. It can be seen that the codebook size has little effect on the average reconstruction error, independent of the scene. Nevertheless, larger codebooks reduce the maximum error with only minimal memory overhead. Sensitivity Thresholds The sensitivity thresholds βc and βg are used to decide whether to consider SH coeffi- cients and shape parameters for clustering. They offer a trade-off between quality and compression rate. The influ- ence of these values is analyzed in Tab. 5, showing in par- ticular the sensitivity of image quality to the quantization of SH coefficients. 6.5. Limitations As the main limitation for making the proposed compres- sion and rendering pipeline even more powerful, we see the current inability to aggressively compress the Gaus- PSNR↑ SSIM↑ LPIPS↓ SIZE↓ Color 1024 26.95(-0.67) 0.80(-0.02) 0.24(+0.03)28.47 2048 26.95(-0.62) 0.80(-0.02) 0.24(+0.03)28.65 4096 26.98(-0.63)0.80(-0.02)0.24(+0.03) 28.80 8192 27.00(-0.58) 0.80(-0.02) 0.24(-0.03)28.92 Gaussian 1024 26.95(-0.79) 0.80(-0.02) 0.24(+0.03)28.14 2048 26.97(-0.80) 0.80(-0.02) 0.24(+0.03)28.45 4096 26.98(-0.63)0.80(-0.02)0.24(+0.03) 28.80 8192 26.97(-0.60) 0.80(-0.02) 0.24(+0.03)29.06 Table 4. Average reconstruction error over the test images for different codebook sizes, including the maximum deviation from the baseline (+/−). Rows marked grey indicate the default con- figurations. Experiments were performed on the Mip-Nerf360[2] dataset. PSNR↑ SSIM↑ LPIPS↓ SIZE↓ baseline 26.976 0.801 0.238 28.80 βc 6.0·10−8 27.22(−0.25) 0.81(−0.00) 0.22(+0.00) 56.50 3.0·10−7 27.09(−0.41) 0.80(−0.01) 0.23(+0.02) 33.00 6.0·10−7 26.98(−0.63) 0.80(−0.02) 0.24(+0.03) 28.80 1.2·10−6 26.87(−0.75) 0.80(−0.02) 0.24(+0.04) 27.02 6.0·10−6 26.74(−0.94) 0.80(−0.02) 0.25(+0.04) 25.97 - 26.55(−1.47) 0.79(−0.03) 0.25(+0.05) 25.65 βg 3.0·10−7 27.05(−0.41) 0.80(−0.01) 0.23(+0.03) 33.90 1.5·10−6 27.00(−0.62) 0.80(−0.02) 0.24(+0.03) 29.95 3.0·10−6 26.98(−0.63) 0.80(−0.02) 0.24(+0.03) 28.80 6.0·10−6 26.91(−0.77) 0.80(−0.02) 0.24(+0.03) 28.08 3.0·10−5 26.86(−0.72) 0.80(−0.02) 0.24(+0.04) 27.30 - 26.80(−0.83) 0.80(−0.02) 0.25(+0.04) 27.10 Table 5. Sensitivity threshold ablation study. βc and βg are the sensitivity thresholds for controlling which SH vectors and shape parameters are clustered. The average error and in brackets the maximum deviation from the baseline are reported. The last row shows the results when no threshold is considered. The rows marked grey are the default configurations. Experiments were per- formed with Mip-Nerf360 [2] dataset. sians’ positions in 3D space. We performed experiments where positions were quantized to a lattice structure, and we even embedded these positional constraints into the Gaus- sian splatting training process. Unfortunately, we were not able to further compress the positions without introducing a significant error in the rendering process. 7. Conclusion We have introduced a novel compression and rendering pipeline for 3D Gaussians with color and shape parameters, achieving compression rates of up to 31 × and up to a 4 × increase in rendering speed. Our experiments with differ- ent datasets have shown that the compression introduces an indiscernible loss in image quality. The compressed data can be streamed over networks and rendered on low-power devices, making it suitable for mobile VR/AR applications and games. In the future, we aim to explore new approaches for reducing the memory footprint during the training phase, and additionally compressing positional information end- to-end. We also believe that 3D Gaussian splatting has the 8potential for reconstructing volumetric scenes, and we will investigate advanced options for compressing and rendering the optimized representations. References [1] Andy Adinets and Duane Merrill. Onesweep: A Faster Least Significant Digit Radix Sort for GPUs. arXiv preprint arXiv:2206.01784, 2022. 5 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5460–5469, New Orleans, LA, USA, 2022. IEEE. 3, 5, 6, 8, 11, 13, 15, 16 [3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In Computer Vision – ECCV 2022, pages 333–350, Cham, 2022. Springer Nature Switzerland. 1, 2 [4] Chenxi Lola Deng and Enzo Tartaglione. Compressing Explicit V oxel Grid Representations: fast NeRFs become also small. In 2023 IEEE/CVF Winter Conference on Ap- plications of Computer Vision (WACV) , pages 1236–1245, Waikoloa, HI, USA, 2023. IEEE. 2 [5] L. Peter Deutsch. DEFLATE Compressed Data Format Specification version 1.3, 1996. Issue: 1951 Num Pages: 17 Series: Request for Comments Published: RFC 1951. 5 [6] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenox- els: Radiance Fields without Neural Networks. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5491–5500, New Orleans, LA, USA, 2022. IEEE. 2 [7] Yiming Gao, Yan-Pei Cao, and Ying Shan. SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Re- construction of Indoor Scenes. In 2023 IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 108–118, Vancouver, BC, Canada, 2023. IEEE. 2 [8] Cameron Gordon, Shin-Fang Chng, Lachlan MacDonald, and Simon Lucey. On Quantizing Implicit Neural Rep- resentations. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pages 341–350, Waikoloa, HI, USA, 2023. IEEE. 2 [9] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep Blending for Free-viewpoint Image-based Rendering. ACM Transactions on Graphics (Proc. SIGGRAPH Asia) , 37(6):257:1–257:15, 2018. Publisher: ACM. 6, 11, 13, 14 [10] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural ra- diance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 5875–5884, 2021. 2 [11] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 2704–2713, 2018. 2 [12] Seungyeop Kang and Sungjoo Yoo. TernaryNeRF: Quantiz- ing V oxel Grid-based NeRF Models. In2022 IEEE Interna- tional Workshop on Rapid System Prototyping (RSP), pages 8–14, Shanghai, China, 2022. IEEE. 2 [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph., 42(4), 2023. Place: New York, NY , USA Publisher: Association for Com- puting Machinery. 1, 2, 3, 4, 5, 6, 7, 8, 12 [14] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG) , 36 (4):1–13, 2017. Publisher: ACM New York, NY , USA. 6, 11, 13, 14 [15] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing V olumetric Radiance Fields to 1 MB. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4222–4231, Vancouver, BC, Canada, 2023. IEEE. 1, 2, 3, 4, 5 [16] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. Communications of the ACM , 65(1):99–106, 2021. Publisher: ACM New York, NY , USA. 1, 2, 6, 11, 13, 17, 18 [17] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant neural graphics primitives with a mul- tiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1–15, 2022. Publisher: ACM New York, NY , USA. 1, 2 [18] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: ImageNet Classification Us- ing Binary Convolutional Neural Networks. In Computer Vision – ECCV 2016, pages 525–542, Cham, 2016. Springer International Publishing. 2, 5 [19] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini- vasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. ACM Trans- actions on Graphics (TOG) , 42(4):1–12, 2023. Publisher: ACM New York, NY , USA. 2 [20] Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, and Eunbyung Park. Masked Wavelet Representation for Compact Neural Radiance Fields. InPro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20680–20690, 2023. 1, 2 [21] Darius R ¨uckert, Linus Franke, and Marc Stamminger. ADOP: approximate differentiable one-pixel point render- ing. ACM Trans. Graph., 41(4):1–14, 2022. 2 [22] David Salomon and Giovanni Motta. Handbook of data com- pression. Springer Science & Business Media, 2010. 6 [23] D. Sculley. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web, pages 1177–1178, Raleigh North Carolina USA, 2010. ACM. 4 [24] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct V oxel Grid Optimization: Super-fast Convergence for Radiance 9Fields Reconstruction. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 5449–5459, New Orleans, LA, USA, 2022. IEEE. 1, 2 [25] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas M¨uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable Bitrate Neural Fields. In ACM SIGGRAPH 2022 Conference Proceedings, New York, NY , USA, 2022. Asso- ciation for Computing Machinery. event-place: Vancouver, BC, Canada. 2, 3 [26] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable NeRF via Rank-residual Decomposition. In Advances in Neural Information Process- ing Systems , pages 14798–14809. Curran Associates, Inc., 2022. 2 [27] Krishna Wadhwani and Tamaki Kojima. SqueezeNeRF: Fur- ther factorized FastNeRF for memory-efficient inference. In 2022 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition Workshops (CVPRW) , pages 2716–2724, New Orleans, LA, USA, 2022. IEEE. 2 [28] Sebastian Weiss and R ¨udiger Westermann. Differentiable Direct V olume Rendering. In IEEE Transactions on Visu- alization and Computer Graphics, pages 562–572, 2022. Is- sue: 1. 2 [29] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5438–5448, 2022. 1, 2 [30] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for Real-time Rendering of Neural Radiance Fields. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5732–5741, Montreal, QC, Canada, 2021. IEEE. 5 [31] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht- man, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 586–595, Salt Lake City, UT, 2018. IEEE. 6 [32] Tianli Zhao, Jiayuan Chen, Cong Leng, and Jian Cheng. TinyNeRF: Towards 100 x Compression of V oxel Radiance Fields. Proceedings of the AAAI Conference on Artificial In- telligence, 37(3):3588–3596, 2023. Number: 3. 1, 2 [33] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Trans. Inform. Theory, 23(3):337– 343, 1977. 5 [34] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross. EW A volume splatting. In Proceedings Visualization, 2001. VIS ’01., pages 29–538, San Diego, CA, USA, 2001. IEEE. 3, 5 10Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis Supplementary Material 8. Supplementary A. Detailed Scene Analysis For all scenes used in the paper, we report the PSNR, SSIM, LPIPS, memory consumption, and compression ratio of our approach. See Tab. 8 for Mip-Nerf360 [2], Tab. 10 for Deep Blending [9], Tab. 9 for Tanks&Temples [14], Tab. 11 for the synthetic scenes from [16]. B. Image Quality For all scenes used in the paper, a random test view is se- lected. The ground truth images are compared to the ren- derings of the uncompressed (baseline) and our compressed (Compressed) scene representation. See Figs. 11 and 12 for Mip-Nerf360 [2], Fig. 10 for Deep Blending [9], Fig. 9 for Tanks & Temples [14], Figs. 13 and 14 for the synthetic scenes from [16]. C. Memory Requirements Fig. 7 illustrates the memory requirements of different scene parameters. The coordinates of the 3D Gaussian cen- ter points and the codebook indices take up the most mem- ory in general. The amount of memory required by the color codebook varies significantly between different scenes. 0% 20% 40% 60% 80% 100% Flowers Garden Stump Treehill Room Counter Kitchen Bonsai Bicycle   Truck Train    Playroom Drjohnson     Chair Drums Ficus Hotdog Lego Materials Mic Ship Mip-Nerf360 T anks & T emples Deep  Blending NeRF Synthetic position color opacity  shape indices Figure 7. Storage size of different scene parameters in the com- pressed representation. Color is the codebook with all SH coeffi- cients. Shape is the codebook with the Gaussian parameters and η is the scaling factor. D. Timing Statistics We provide timings for the different stages of our compres- sion pipeline. Tab. 6 shows the average and maximum time required by each stage. It can be seen that the fine-tuning stage takes up 70% of the total time. Average Time↓ Maximum Time↓ Sensitivity Calculation 8.05 11 .38 Clustering 75.11 78 .41 QA Fine-tuning 213.30 278 .05 Encoding 2.69 5 .13 Total 299.15 365 .94 Table 6. Time requirements of the individual stages of the com- pression pipeline. We report the average and maximum time of each stage in seconds. The entropy and run-length encoding are grouped into the Encoding stage. Measurements were taken with an NVIDIA RTX A5000 graphics card. Additionally, we report timings for each stage of the novel view renderer. Tab. 7 shows the average times for two different scenes. It can be seen that the preprocessing stage is accelerated by a factor of 5× when using the compressed scene representation. Preprocess↓ Sorting↓ Rasterization↓ Total↓ Bicycle Uncompressed 1.46 0 .55 2 .81 4.82 Compressed 0.28 0 .48 2 .45 3.22 Bonsai Uncompressed 0.44 0 .20 1 .81 2.44 Compressed 0.09 0 .19 1 .67 1.95 Table 7. Timings in milliseconds for the different stages of our renderer. Evaluated on an NVIDIA RTX A5000 with scenes from Mip-Nerf360 [2] 11(a) Baseline  (b) Compressed Figure 8. Pruning failure case. Compared to the baseline recon- struction, some leaves have been removed in the compressed ver- sion due to pruning. E. Sensitivity Calculation and Pruning The sensitivity of a parameter is calculated using the gra- dient of the total image energy wrt. this parameter (see Eq. (3)). Kerbl et al . [13] clamp negative direction- dependent colors (i.e., resulting from the evaluation of the SH coefficients) to zero. For the clamped values, the partial derivatives are set to zero in the backward pass. This results in a sensitivity of zero for the respective SH coefficients, which is not desired since they possibly contribute to the training images. Therefore, we do not clamp colors when calculating the sensitivity. We observe that a notable number of Gaussians (up to 15%) do not have any impact on the training images. These particular splats exhibit zero sensitivity in the color param- eters. Consequently, we opt to eliminate these splats from the scene (called Pruning in Tab. 3). Experiments with higher pruning thresholds have shown that more Gaussians can be removed with minimal loss in PSNR. However, this can lead to fine details in the scene being removed, which we consider undesirable. An exam- ple of this can be seen in Fig. 8, where small leaves were removed from the reconstruction due to pruning. F. Covariance Matrix Clustering Given a rotation matrix R ∈ R3×3 and a scaling vector s ∈ R3 >0. The covariance matrix Σ is defined as [13] Σ = RSSRT = RS2RT , (8) with S = diag(s) . Since Σ is real and symmetric it holds that S2 = diag([λ1, λ2, λ3]T ) = diag([s2 1, s2 2, s2 3]T ), (9) where λi are the eigenvalues of Σ. By using the trace of Σ, the squared length of s can be calculated as Tr(Σ) = 3X i=1 λi = 3X i=1 s2 i = ∥s∥2 2 (10) Clustering Update Step In the following, we show that the clustering update step results in normalized covariance matrices as cluster centroids. Given N normalized covari- ance matrices ˆΣi with ∥si∥2 = 1 and respective weighting factors wi ∈ R>0. Their centroid ˆΣc is calculated as ˆΣc = 1PN i=1 wi NX i=1 wi ˆΣi (11) . By using Eq. (10) it holds that ∥sc∥2 2 = Tr(ˆΣc) (12) = Tr( 1PN i=1 wi NX i=1 wi ˆΣi) (13) = 1PN i=1 wi NX i=1 wiTr(ˆΣi) (14) = 1PN i=1 wi NX i=1 wi∥si∥2 2 (15) = 1 (16) This proves that the covariance matrix ˆΣc has a normalized scaling vector and thus iteself is in a normalized form. Covariance Matrix Normalization The following derivation proofs that a covariance matrix Σ can be trans- formed into its normalized form ˆΣ by dividing it by its trace, i.e., Σ Tr(Σ) = R S2 Tr(Σ)RT = R S ∥s∥2 S ∥s∥2 RT (17) = R ˆS2RT = ˆΣ (18) 123D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ bicycle 25.171 0 .762 0 .216 1450 .277 24.970 0 .751 0 .240 47 .147 30.761 bonsai 31.979 0 .938 0 .208 294 .415 31.347 0 .930 0 .217 12 .794 23.011 counter 28.888 0 .905 0 .204 289 .244 28.671 0 .896 0 .215 13 .789 20.977 flowers 21.448 0 .602 0 .341 860 .062 21.152 0 .584 0 .358 31 .140 27.619 garden 27.179 0 .861 0 .115 1379 .993 26.746 0 .844 0 .144 46 .565 29.636 kitchen 30.713 0 .923 0 .130 438 .099 30.262 0 .914 0 .140 18 .874 23.211 room 31.341 0 .916 0 .223 376 .853 31.138 0 .911 0 .231 15 .033 25.068 stump 26.562 0 .770 0 .219 1173 .522 26.285 0 .757 0 .250 40 .569 28.926 treehill 22.303 0 .631 0 .328 894 .903 22.256 0 .620 0 .351 33 .318 26.859 average 27.287 0 .812 0 .220 795 .263 26.981 0 .801 0 .238 28 .803 26.230 Table 8. Mip-Nerf360 [2] results. 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ train 21.770 0 .805 0 .217 242 .782 21.863 0 .798 0 .226 13 .249 18.324 truck 24.940 0 .871 0 .155 601 .030 24.823 0 .867 0 .161 21 .316 28.196 average 23.355 0 .838 0 .186 421 .906 23.343 0 .832 0 .194 17 .282 23.260 Table 9. Tanks&Temples [14] results 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ drjohnson 28.938 0 .896 0 .248 805 .358 28.871 0 .895 0 .254 28 .938 27.830 playroom 29.926 0 .901 0 .244 602 .186 29.891 0 .900 0 .252 21 .660 27.802 average 29.432 0 .898 0 .246 703 .772 29.381 0 .898 0 .253 25 .299 27.816 Table 10. Deep Blending [9] results 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ chair 35.864 0 .987 0 .012 70 .105 35.297 0 .985 0 .014 3 .575 19.609 drums 26.072 0 .954 0 .038 83 .665 25.941 0 .952 0 .040 3 .829 21.848 ficus 34.736 0 .987 0 .012 70 .177 34.559 0 .986 0 .013 3 .059 22.937 hotdog 37.646 0 .985 0 .021 34 .079 37.367 0 .984 0 .022 2 .725 12.505 lego 35.399 0 .981 0 .017 76 .071 34.802 0 .979 0 .020 4 .314 17.633 materials 29.861 0 .959 0 .035 71 .833 29.602 0 .957 0 .038 4 .021 17.862 mic 35.155 0 .991 0 .006 77 .563 34.913 0 .991 0 .007 3 .025 25.640 ship 30.954 0 .905 0 .111 75 .659 31.005 0 .905 0 .111 4 .938 15.322 average 33.211 0 .969 0 .031 69 .894 32.936 0 .967 0 .033 3 .686 19.170 Table 11. NeRF Synthetic [16] results 13Truck Train Ground Truth Baseline Compressed Figure 9. Random test views for each scene from Tanks&Temples [14] Playroom Drjohnson Ground Truth Baseline Compressed Figure 10. Random test views for each scene from Deep Blending [9] 14Flowers Garden Stump Treehill Room Ground Truth Baseline Compressed Figure 11. Random test views for each scene from Mip-NeRF360 [2] 15Counter Kitchen Bonsai Bicycle Ground Truth Baseline Compressed Figure 12. Random test views for each scene from Mip-NeRF360 [2] 16Chair Drums Ficus Hotdog Ground Truth Baseline Compressed Figure 13. Random test views for each scene from NeRF Synthetic [16] 17Lego Materials Mic Ship Ground Truth Baseline Compressed Figure 14. Random test views for each scene from NeRF Synthetic [16] 18",
      "meta_data": {
        "arxiv_id": "2401.02436v2",
        "authors": [
          "Simon Niedermayr",
          "Josef Stumpfegger",
          "Rüdiger Westermann"
        ],
        "published_date": "2023-11-17T14:40:43Z",
        "pdf_url": "https://arxiv.org/pdf/2401.02436v2.pdf",
        "github_url": "https://github.com/KeKsBoTer/c3dgs"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a compressed 3D Gaussian splat representation to address the high memory consumption and rendering inefficiencies of existing 3D Gaussian splatting methods for novel view synthesis. The key contributions include a compression pipeline utilizing sensitivity-aware vector clustering with quantization-aware training for directional colors and Gaussian parameters, achieving up to 31x compression with minimal quality degradation. Additionally, a novel renderer leveraging hardware rasterization is proposed, resulting in up to 4x higher framerates on lightweight GPUs compared to optimized GPU compute pipelines, making the representation suitable for low-power devices and network streaming applications.",
        "methodology": "The proposed methodology consists of three main steps: 1) Sensitivity-aware vector clustering, where a sensitivity measure (gradient of image energy) is computed for each Gaussian parameter. Color (SH coefficients) and Gaussian shape (normalized covariance matrices) parameters are compressed into separate codebooks using sensitivity-aware k-Means clustering, with highly sensitive parameters kept uncompressed. 2) Quantization-aware fine-tuning, where the clustered and other scene parameters (position, opacity, scaling factor, codebook entries) are optimized on training images using Min-Max quantization (8-bit for most, 16-bit float for position) to recover lost information. 3) Entropy encoding, where 3D Gaussians are linearized along a Z-order (Morton order) curve to exploit spatial coherence, then further compressed using DEFLATE (LZ77 and Huffman coding). For novel view rendering, a hardware rasterization pipeline is used. It involves a preprocess to discard out-of-frustum Gaussians, compute direction-dependent colors, and depth-sort Gaussians on the GPU (using Onesweep). Finally, Gaussians are rendered as planar quads (splats) using vertex and pixel shaders, with the pixel shader handling exponential color/opacity falloff and blending.",
        "experimental_setup": "The method was evaluated on several benchmark datasets: Mip-Nerf360, Tanks&Temples, Deep Blending, and NeRF-Synthetic. For Mip-Nerf360, Tanks&Temples, and Deep Blending, reconstructions from Kerbl et al. [13] were used, while 3D Gaussian representations for NeRF-Synthetic were self-generated. Quantitative evaluation used PSNR, SSIM, and LPIPS metrics. Rendering performance was measured in frames per second on various GPUs, including NVIDIA RTX A5000, NVIDIA RTX 3070M, Intel UHD Graphics 11, and AMD Radeon R9 380, at 1080p resolution. Key implementation details include a decay factor of 0.8 for batched clustering, 800 update steps for Gaussians, 100 for SH coefficients, batch sizes of 2^18 for color features and 2^20 for Gaussian parameters, a default codebook size of 4096, and specific sensitivity thresholds (βc=6·10−7, βg=3·10−6). Quantization-aware fine-tuning involved 5000 optimization steps. The renderer was implemented using the WebGPU graphics API in Rust. Ablation studies analyzed the impact of individual compression stages, codebook sizes, and sensitivity thresholds.",
        "limitations": "The main limitation identified is the inability to aggressively compress the Gaussians’ positions in 3D space without introducing significant error in the rendering process. Experiments with positional quantization to a lattice structure, even when embedded into the Gaussian splatting training, did not yield satisfactory results. Additionally, while pruning can remove Gaussians with zero sensitivity, setting higher pruning thresholds can lead to the removal of fine scene details, which is considered undesirable.",
        "future_research_directions": "Future research aims to explore novel approaches for reducing the memory footprint during the training phase of 3D Gaussian splatting. Another direction is to investigate methods for further compressing positional information in an end-to-end manner. The authors also plan to explore advanced options for compressing and rendering optimized representations for volumetric scenes, leveraging the potential of 3D Gaussian splatting in this area.",
        "experimental_code": "File Path: compress.py\nContent:\ndef calc_importance(\n    gaussians: GaussianModel, scene, pipeline_params\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    scaling = gaussians.scaling_qa(\n        gaussians.scaling_activation(gaussians._scaling.detach())\n    )\n    cov3d = gaussians.covariance_activation(\n        scaling, 1.0, gaussians.get_rotation.detach(), True\n    ).requires_grad_(True)\n    scaling_factor = gaussians.scaling_factor_activation(\n        gaussians.scaling_factor_qa(gaussians._scaling_factor.detach())\n    )\n\n    h1 = gaussians._features_dc.register_hook(lambda grad: grad.abs())\n    h2 = gaussians._features_rest.register_hook(lambda grad: grad.abs())\n    h3 = cov3d.register_hook(lambda grad: grad.abs())\n    background = torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32, device=\"cuda\")\n\n    gaussians._features_dc.grad = None\n    gaussians._features_rest.grad = None\n    num_pixels = 0\n    for camera in tqdm(scene.getTrainCameras(), desc=\"Calculating sensitivity\"):\n        cov3d_scaled = cov3d * scaling_factor.square()\n        rendering = render(\n            camera,\n            gaussians,\n            pipeline_params,\n            background,\n            clamp_color=False,\n            cov3d=cov3d_scaled,\n        )[\"render\"]\n        loss = rendering.sum()\n        loss.backward()\n        num_pixels += rendering.shape[1]*rendering.shape[2]\n\n    importance = torch.cat(\n        [gaussians._features_dc.grad, gaussians._features_rest.grad],\n        1,\n    ).flatten(-2)/num_pixels\n    cov_grad = cov3d.grad/num_pixels\n    h1.remove()\n    h2.remove()\n    h3.remove()\n    torch.cuda.empty_cache()\n    return importance.detach(), cov_grad.detach()\n\ndef run_vq(\n    model_params: ModelParams,\n    optim_params: OptimizationParams,\n    pipeline_params: PipelineParams,\n    comp_params: CompressionParams,\n):\n    gaussians = GaussianModel(\n        model_params.sh_degree, quantization=not optim_params.not_quantization_aware\n    )\n    scene = Scene(\n        model_params, gaussians, load_iteration=comp_params.load_iteration, shuffle=True\n    )\n\n    if comp_params.start_checkpoint:\n        (checkpoint_params, first_iter) = torch.load(comp_params.start_checkpoint)\n        gaussians.restore(checkpoint_params, optim_params)\n\n\n    timings ={}\n\n    # %%\n\n    start_time = time.time()\n    color_importance, gaussian_sensitivity = calc_importance(\n        gaussians, scene, pipeline_params\n    )\n    end_time = time.time()\n    timings[\"sensitivity_calculation\"] = end_time-start_time\n    # %%\n    print(\"vq compression..\")\n    with torch.no_grad():\n        start_time = time.time()\n        color_importance_n = color_importance.amax(-1)\n\n        gaussian_importance_n = gaussian_sensitivity.amax(-1)\n\n        torch.cuda.empty_cache()\n\n        color_compression_settings = CompressionSettings(\n            codebook_size=comp_params.color_codebook_size,\n            importance_prune=comp_params.color_importance_prune,\n            importance_include=comp_params.color_importance_include,\n            steps=int(comp_params.color_cluster_iterations),\n            decay=comp_params.color_decay,\n            batch_size=comp_params.color_batch_size,\n        )\n\n        gaussian_compression_settings = CompressionSettings(\n            codebook_size=comp_params.gaussian_codebook_size,\n            importance_prune=None,\n            importance_include=comp_params.gaussian_importance_include,\n            steps=int(comp_params.gaussian_cluster_iterations),\n            decay=comp_params.gaussian_decay,\n            batch_size=comp_params.gaussian_batch_size,\n        )\n\n        compress_gaussians(\n            gaussians,\n            color_importance_n,\n            gaussian_importance_n,\n            color_compression_settings if not comp_params.not_compress_color else None,\n            gaussian_compression_settings\n            if not comp_params.not_compress_gaussians\n            else None,\n            comp_params.color_compress_non_dir,\n            prune_threshold=comp_params.prune_threshold,\n        )\n        end_time = time.time()\n        timings[\"clustering\"]=end_time-start_time\n\n    gc.collect()\n    torch.cuda.empty_cache()\n    os.makedirs(comp_params.output_vq, exist_ok=True)\n\n    copyfile(\n        path.join(model_params.model_path, \"cfg_args\"),\n        path.join(comp_params.output_vq, \"cfg_args\"),\n    )\n    model_params.model_path = comp_params.output_vq\n\n    with open(\n        os.path.join(comp_params.output_vq, \"cfg_args_comp\"), \"w\"\n    ) as cfg_log_f:\n        cfg_log_f.write(str(Namespace(**vars(comp_params))))\n\n    iteration = scene.loaded_iter + comp_params.finetune_iterations\n    if comp_params.finetune_iterations > 0:\n\n        start_time = time.time()\n        finetune(\n            scene,\n            model_params,\n            optim_params,\n            comp_params,\n            pipeline_params,\n            testing_iterations=[\n                -1\n            ],\n            debug_from=-1,\n        )\n        end_time = time.time()\n        timings[\"finetune\"]=end_time-start_time\n\n        # %%\n    out_file = path.join(\n        comp_params.output_vq,\n        f\"point_cloud/iteration_{iteration}/point_cloud.npz\",\n    )\n    start_time = time.time()\n    gaussians.save_npz(out_file, sort_morton=not comp_params.not_sort_morton)\n    end_time = time.time()\n    timings[\"encode\"]=end_time-start_time\n    timings[\"total\"]=sum(timings.values())\n    with open(f\"{comp_params.output_vq}/times.json\",\"w\") as f:\n        json.dump(timings,f)\n    file_size = os.path.getsize(out_file) / 1024**2\n    print(f\"saved vq finetuned model to {out_file}\")\n\n    # eval model\n    print(\"evaluating...\")\n    metrics = render_and_eval(gaussians, scene, model_params, pipeline_params)\n    metrics[\"size\"] = file_size\n    print(metrics)\n    with open(f\"{comp_params.output_vq}/results.json\",\"w\") as f:\n        json.dump({f\"ours_{iteration}\":metrics},f,indent=4)\n\nFile Path: compression/vq.py\nContent:\nfrom dataclasses import dataclass\nimport math\nimport time\nimport torch\nfrom torch import nn\nfrom torch_scatter import scatter\nfrom typing import Tuple, Optional\nfrom tqdm import trange\nimport gc\nfrom scene.gaussian_model import GaussianModel\nfrom utils.splats import to_full_cov, extract_rot_scale\nfrom weighted_distance._C import weightedDistance\n\n\nclass VectorQuantize(nn.Module):\n    def __init__(\n        self,\n        channels: int,\n        codebook_size: int = 2**12,\n        decay: float = 0.5,\n    ) -> None:\n        super().__init__()\n        self.decay = decay\n        self.codebook = nn.Parameter(\n            torch.empty(codebook_size, channels), requires_grad=False\n        )\n        nn.init.kaiming_uniform_(self.codebook)\n        self.entry_importance = nn.Parameter(\n            torch.zeros(codebook_size), requires_grad=False\n        )\n        self.eps = 1e-5\n\n    def uniform_init(self, x: torch.Tensor):\n        amin, amax = x.aminmax()\n        self.codebook.data = torch.rand_like(self.codebook) * (amax - amin) + amin\n\n    def update(self, x: torch.Tensor, importance: torch.Tensor) -> torch.Tensor:\n        with torch.no_grad():\n            min_dists, idx = weightedDistance(x.detach(), self.codebook.detach())\n            acc_importance = scatter(\n                importance, idx, 0, reduce=\"sum\", dim_size=self.codebook.shape[0]\n            )\n\n            ema_inplace(self.entry_importance, acc_importance, self.decay)\n\n            codebook = scatter(\n                x * importance[:, None],\n                idx,\n                0,\n                reduce=\"sum\",\n                dim_size=self.codebook.shape[0],\n            )\n\n            ema_inplace(\n                self.codebook,\n                codebook / (acc_importance[:, None] + self.eps),\n                self.decay,\n            )\n\n            return min_dists\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        return_dists: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        min_dists, idx = weightedDistance(x.detach(), self.codebook.detach())\n        if return_dists:\n            return self.codebook[idx], idx, min_dists\n        else:\n            return self.codebook[idx], idx\n\n\ndef ema_inplace(moving_avg: torch.Tensor, new: torch.Tensor, decay: float):\n    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))\n\n\ndef vq_features(\n    features: torch.Tensor,\n    importance: torch.Tensor,\n    codebook_size: int,\n    vq_chunk: int = 2**16,\n    steps: int = 1000,\n    decay: float = 0.8,\n    scale_normalize: bool = False,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    importance_n = importance/importance.max()\n    vq_model = VectorQuantize(\n        channels=features.shape[-1],\n        codebook_size=codebook_size,\n        decay=decay,\n    ).to(device=features.device)\n\n    vq_model.uniform_init(features)\n\n    errors = []\n    for i in trange(steps):\n        batch = torch.randint(low=0, high=features.shape[0], size=[vq_chunk])\n        vq_feature = features[batch]\n        error = vq_model.update(vq_feature, importance=importance_n[batch]).mean().item()\n        errors.append(error)\n        if scale_normalize:\n            # this computes the trace of the codebook covariance matrices\n            # we devide by the trace to ensure that matrices have normalized eigenvalues / scales\n            tr = vq_model.codebook[:, [0, 3, 5]].sum(-1)\n            vq_model.codebook /= tr[:, None]\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    start = time.time()\n    _, vq_indices = vq_model(features)\n    torch.cuda.synchronize(device=vq_indices.device)\n    end = time.time()\n    print(f\"calculating indices took {end-start} seconds \")\n    return vq_model.codebook.data.detach(), vq_indices.detach()\n\n\ndef join_features(\n    all_features: torch.Tensor,\n    keep_mask: torch.Tensor,\n    codebook: torch.Tensor,\n    codebook_indices: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    keep_features = all_features[keep_mask]\n    compressed_features = torch.cat([codebook, keep_features], 0)\n\n    indices = torch.zeros(\n        len(all_features), dtype=torch.long, device=all_features.device\n    )\n    indices[~keep_mask] = codebook_indices\n    indices[keep_mask] = torch.arange(len(keep_features), device=indices.device) + len(\n        codebook\n    )\n\n    return compressed_features, indices\n\n\n@dataclass\nclass CompressionSettings:\n    codebook_size: int\n    importance_prune: float\n    importance_include: float\n    steps: int\n    decay: float\n    batch_size: int\n\n\ndef compress_color(\n    gaussians: GaussianModel,\n    color_importance: torch.Tensor,\n    color_comp: CompressionSettings,\n    color_compress_non_dir: bool,\n):\n    keep_mask = color_importance > color_comp.importance_include\n\n    print(\n        f\"color keep: {keep_mask.float().mean()*100:.2f}%\"\n    )\n\n    vq_mask_c = ~keep_mask\n\n    # remove zero sh component\n    if color_compress_non_dir:\n        n_sh_coefs = gaussians.get_features.shape[1]\n        color_features = gaussians.get_features.detach().flatten(-2)\n    else:\n        n_sh_coefs = gaussians.get_features.shape[1] - 1\n        color_features = gaussians.get_features[:, 1:].detach().flatten(-2)\n    if vq_mask_c.any():\n        print(\"compressing color...\")\n        color_codebook, color_vq_indices = vq_features(\n            color_features[vq_mask_c],\n            color_importance[vq_mask_c],\n            color_comp.codebook_size,\n            color_comp.batch_size,\n            color_comp.steps,\n        )\n    else:\n        color_codebook = torch.empty(\n            (0, color_features.shape[-1]), device=color_features.device\n        )\n        color_vq_indices = torch.empty(\n            (0,), device=color_features.device, dtype=torch.long\n        )\n\n    all_features = color_features\n    compressed_features, indices = join_features(\n        all_features, keep_mask, color_codebook, color_vq_indices\n    )\n\n    gaussians.set_color_indexed(compressed_features.reshape(-1, n_sh_coefs, 3), indices)\n\ndef compress_covariance(\n    gaussians: GaussianModel,\n    gaussian_importance: torch.Tensor,\n    gaussian_comp: CompressionSettings,\n):\n\n    keep_mask_g = gaussian_importance > gaussian_comp.importance_include\n\n    vq_mask_g = ~keep_mask_g\n\n    print(f\"gaussians keep: {keep_mask_g.float().mean()*100:.2f}%\")\n\n    covariance = gaussians.get_normalized_covariance(strip_sym=True).detach()\n\n    if vq_mask_g.any():\n        print(\"compressing gaussian splats...\")\n        cov_codebook, cov_vq_indices = vq_features(\n            covariance[vq_mask_g],\n            gaussian_importance[vq_mask_g],\n            gaussian_comp.codebook_size,\n            gaussian_comp.batch_size,\n            gaussian_comp.steps,\n            scale_normalize=True,\n        )\n    else:\n        cov_codebook = torch.empty(\n            (0, covariance.shape[1], 1), device=covariance.device\n        )\n        cov_vq_indices = torch.empty((0,), device=covariance.device, dtype=torch.long)\n\n    compressed_cov, cov_indices = join_features(\n        covariance,\n        keep_mask_g,\n        cov_codebook,\n        cov_vq_indices,\n    )\n\n    rot_vq, scale_vq = extract_rot_scale(to_full_cov(compressed_cov))\n\n    gaussians.set_gaussian_indexed(\n        rot_vq.to(compressed_cov.device),\n        scale_vq.to(compressed_cov.device),\n        cov_indices,\n    )\n\n\ndef compress_gaussians(\n    gaussians: GaussianModel,\n    color_importance: torch.Tensor,\n    gaussian_importance: torch.Tensor,\n    color_comp: Optional[CompressionSettings],\n    gaussian_comp: Optional[CompressionSettings],\n    color_compress_non_dir: bool,\n    prune_threshold:float=0.,\n):\n    with torch.no_grad():\n        if prune_threshold >= 0:\n            non_prune_mask = color_importance > prune_threshold\n            print(f\"prune: {(1-non_prune_mask.float().mean())*100:.2f}%\")\n            gaussians.mask_splats(non_prune_mask)\n            gaussian_importance = gaussian_importance[non_prune_mask]\n            color_importance = color_importance[non_prune_mask]\n        \n        if color_comp is not None:\n            compress_color(\n                gaussians,\n                color_importance,\n                color_comp,\n                color_compress_non_dir,\n            )\n        if gaussian_comp is not None:\n            compress_covariance(\n                gaussians,\n                gaussian_importance,\n                gaussian_comp,\n            )\n\nFile Path: finetune.py\nContent:\nimport os\nimport torch\nfrom random import randint\nfrom utils.loss_utils import l1_loss,  ssim\nfrom gaussian_renderer import render\nfrom scene import Scene\nimport uuid\nfrom tqdm import tqdm\nfrom utils.image_utils import psnr\nfrom argparse import  Namespace\n\ndef finetune(scene: Scene, dataset, opt, comp, pipe, testing_iterations, debug_from):\n    prepare_output_and_logger(comp.output_vq, dataset)\n\n    first_iter = scene.loaded_iter\n    max_iter = first_iter + comp.finetune_iterations\n\n    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]\n    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n\n    iter_start = torch.cuda.Event(enable_timing=True)\n    iter_end = torch.cuda.Event(enable_timing=True)\n\n    scene.gaussians.training_setup(opt)\n    scene.gaussians.update_learning_rate(first_iter)\n\n    viewpoint_stack = None\n    ema_loss_for_log = 0.0\n    progress_bar = tqdm(range(first_iter, max_iter), desc=\"Training progress\")\n    first_iter += 1\n    for iteration in range(first_iter, max_iter + 1):\n        iter_start.record()\n\n        # Pick a random Camera\n        if not viewpoint_stack:\n            viewpoint_stack = scene.getTrainCameras().copy()\n        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack) - 1))\n\n        # Render\n        if (iteration - 1) == debug_from:\n            pipe.debug = True\n        render_pkg = render(viewpoint_cam, scene.gaussians, pipe, background)\n        image, viewspace_point_tensor, visibility_filter, radii = (\n            render_pkg[\"render\"],\n            render_pkg[\"viewspace_points\"],\n            render_pkg[\"visibility_filter\"],\n            render_pkg[\"radii\"],\n        )\n\n        # Loss\n        gt_image = viewpoint_cam.original_image.cuda()\n        Ll1 = l1_loss(image, gt_image)\n        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (\n            1.0 - ssim(image, gt_image)\n        )\n        loss.backward()\n\n        iter_end.record()\n        scene.gaussians.update_learning_rate(iteration)\n\n        with torch.no_grad():\n            # Progress bar\n            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n            if iteration % 10 == 0:\n                progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{7}f}\"})\n                progress_bar.update(10)\n            if iteration == max_iter:\n                progress_bar.close()\n\n            # Optimizer step\n            if iteration < max_iter:\n                scene.gaussians.optimizer.step()\n                scene.gaussians.optimizer.zero_grad()\n\n\nFile Path: scene/gaussian_model.py\nContent:\n#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use\n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\nimport torch\nimport numpy as np\nfrom utils.general_utils import inverse_sigmoid, get_expon_lr_func\nfrom torch import nn\nimport os\nfrom utils.system_utils import mkdir_p\nfrom plyfile import PlyData, PlyElement\nfrom utils.general_utils import strip_symmetric, build_scaling_rotation\nfrom enum import Enum\n\n\nclass ColorMode(Enum):\n    NOT_INDEXED = 0\n    ALL_INDEXED = 1\n\n\nclass GaussianModel:\n    def setup_functions(self):\n        def build_covariance_from_scaling_rotation(\n            scaling, scaling_modifier, rotation, strip_sym=True\n        ):\n            L = build_scaling_rotation(scaling_modifier * scaling, rotation)\n            actual_covariance = L @ L.transpose(1, 2)\n            if strip_sym:\n                return strip_symmetric(actual_covariance)\n            else:\n                return actual_covariance\n\n        self.scaling_activation = lambda x: torch.nn.functional.normalize(\n            torch.nn.functional.relu(x)\n        )\n        self.scaling_inverse_activation = lambda x: x\n        self.scaling_factor_activation = torch.exp\n        self.scaling_factor_inverse_activation = torch.log\n\n        self.covariance_activation = build_covariance_from_scaling_rotation\n\n        self.opacity_activation = torch.sigmoid\n        self.inverse_opacity_activation = inverse_sigmoid\n\n        self.rotation_activation = torch.nn.functional.normalize\n\n    def __init__(self, sh_degree: int, quantization=True):\n        self.active_sh_degree = 0\n        self.max_sh_degree = sh_degree\n        self._xyz = torch.empty(0)\n        self._features_dc = torch.empty(0)\n        self._features_rest = torch.empty(0)\n        self._scaling = torch.empty(0)\n        self._scaling_factor = torch.empty(0)\n        self._rotation = torch.empty(0)\n        self._opacity = torch.empty(0)\n        self.max_radii2D = torch.empty(0)\n        self.xyz_gradient_accum = torch.empty(0)\n        self.denom = torch.empty(0)\n        self.optimizer = None\n        self.percent_dense = 0\n        self.spatial_lr_scale = 0\n\n        # quantization related stuff\n        self._feature_indices = None\n        self._gaussian_indices = None\n\n        self.quantization = quantization\n        self.color_index_mode = ColorMode.NOT_INDEXED\n\n        self.features_dc_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.features_dc_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.features_rest_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.features_rest_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.opacity_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.scaling_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.scaling_factor_qa = torch.ao.quantization.FakeQuantize(\n            dtype=torch.qint8\n        ).cuda()\n        self.rotation_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.xyz_qa = FakeQuantizationHalf.apply\n\n        if not self.quantization:\n            self.features_dc_qa.disable_fake_quant()\n            self.features_dc_qa.disable_observer()\n            self.features_rest_qa.disable_fake_quant()\n            self.features_rest_qa.disable_observer()\n            \n            self.scaling_qa.disable_fake_quant()\n            self.scaling_qa.disable_observer()\n            self.scaling_factor_qa.disable_fake_quant()\n            self.scaling_factor_qa.disable_observer()\n\n            self.rotation_qa.disable_fake_quant()\n            self.rotation_qa.disable_observer()\n            self.xyz_qa = lambda x: x\n\n        self.setup_functions()\n\n    @property\n    def get_scaling(self):\n        scaling_n = self.scaling_qa(self.scaling_activation(self._scaling))\n        scaling_factor = self.scaling_factor_activation(\n            self.scaling_factor_qa(self._scaling_factor)\n        )\n        if self.is_gaussian_indexed:\n            return scaling_factor * scaling_n[self._gaussian_indices]\n        else:\n            return scaling_factor * scaling_n\n\n    @property\n    def get_scaling_normalized(self):\n        return self.scaling_qa(self.scaling_activation(self._scaling))\n\n    @property\n    def get_scaling_factor(self):\n        return self.scaling_factor_activation(\n            self.scaling_factor_qa(self._scaling_factor)\n        )\n\n    @property\n    def get_rotation(self):\n        rotation = self.rotation_activation(self.rotation_qa(self._rotation))\n        if self.is_gaussian_indexed:\n            return rotation[self._gaussian_indices]\n        else:\n            return rotation\n\n    @property\n    def _rotation_post_activation(self):\n        return self.rotation_activation(self.rotation_qa(self._rotation))\n\n    @property\n    def get_xyz(self):\n        return self.xyz_qa(self._xyz)\n\n    @property\n    def get_features(self):\n        features_dc = self.features_dc_qa(self._features_dc)\n        features_rest = self.features_rest_qa(self._features_rest)\n\n        if self.color_index_mode == ColorMode.ALL_INDEXED:\n            return torch.cat((features_dc, features_rest), dim=1)[self._feature_indices]\n        else:\n            return torch.cat((features_dc, features_rest), dim=1)\n        \n    @property\n    def _get_features_raw(self):\n        features_dc = self.features_dc_qa(self._features_dc)\n        features_rest = self.features_rest_qa(self._features_rest)\n        return torch.cat((features_dc, features_rest), dim=1)\n\n    @property\n    def get_opacity(self):\n        return self.opacity_qa(self.opacity_activation(self._opacity))\n\n    def save_npz(\n        self,\n        path,\n        compress: bool = True,\n        half_precision: bool = False,\n        sort_morton=False,\n    ):\n        with torch.no_grad():\n            if sort_morton:\n                self._sort_morton()\n            if isinstance(path, str):\n                mkdir_p(os.path.dirname(os.path.abspath(path)))\n\n            dtype = torch.half if half_precision else torch.float32\n\n            save_dict = dict()\n\n            save_dict[\"quantization\"] = self.quantization\n\n            # save position\n            if self.quantization:\n                save_dict[\"xyz\"] = self.get_xyz.detach().half().cpu().numpy()\n            else:\n                save_dict[\"xyz\"] = self._xyz.detach().cpu().numpy()\n\n            # save color features\n            if self.quantization:\n                features_dc_q = torch.quantize_per_tensor(\n                    self._features_dc.detach(),\n                    self.features_dc_qa.scale,\n                    self.features_dc_qa.zero_point,\n                    self.features_dc_qa.dtype,\n                ).int_repr()\n                save_dict[\"features_dc\"] = features_dc_q.cpu().numpy()\n                save_dict[\"features_dc_scale\"] = self.features_dc_qa.scale.cpu().numpy()\n                save_dict[\n                    \"features_dc_zero_point\"\n                ] = self.features_dc_qa.zero_point.cpu().numpy()\n\n                features_rest_q = torch.quantize_per_tensor(\n                    self._features_rest.detach(),\n                    self.features_rest_qa.scale,\n                    self.features_rest_qa.zero_point,\n                    self.features_rest_qa.dtype,\n                ).int_repr()\n                save_dict[\"features_rest\"] = features_rest_q.cpu().numpy()\n                save_dict[\"features_rest_scale\"] = self.features_rest_qa.scale.cpu().numpy()\n                save_dict[\n                    \"features_rest_zero_point\"\n                ] = self.features_rest_qa.zero_point.cpu().numpy()\n            else:\n                save_dict[\"features_dc\"] = self._features_dc.detach().cpu().numpy()\n                save_dict[\"features_rest\"] = self._features_rest.detach().cpu().numpy()\n\n            # save opacity\n            if self.quantization:\n                opacity = self.opacity_activation(self._opacity).detach()\n                opacity_q = torch.quantize_per_tensor(\n                    opacity,\n                    scale=self.opacity_qa.scale,\n                    zero_point=self.opacity_qa.zero_point,\n                    dtype=self.opacity_qa.dtype,\n                ).int_repr()\n                save_dict[\"opacity\"] = opacity_q.cpu().numpy()\n                save_dict[\"opacity_scale\"] = self.opacity_qa.scale.cpu().numpy()\n                save_dict[\n                    \"opacity_zero_point\"\n                ] = self.opacity_qa.zero_point.cpu().numpy()\n            else:\n                save_dict[\"opacity\"] = self._opacity.detach().to(dtype).cpu().numpy()\n\n            # save indices\n            if self.is_color_indexed:\n                save_dict[\"feature_indices\"] = (\n                    self._feature_indices.detach().contiguous().cpu().int().numpy()\n                )\n            if self.is_gaussian_indexed:\n                save_dict[\"gaussian_indices\"] = (\n                    self._gaussian_indices.detach().contiguous().cpu().int().numpy()\n                )\n\n            # save scaling\n            if self.quantization:\n                scaling = self.scaling_activation(self._scaling.detach())\n                scaling_q = torch.quantize_per_tensor(\n                    scaling,\n                    scale=self.scaling_qa.scale,\n                    zero_point=self.scaling_qa.zero_point,\n                    dtype=self.scaling_qa.dtype,\n                ).int_repr()\n                save_dict[\"scaling\"] = scaling_q.cpu().numpy()\n                save_dict[\"scaling_scale\"] = self.scaling_qa.scale.cpu().numpy()\n                save_dict[\n                    \"scaling_zero_point\"\n                ] = self.scaling_qa.zero_point.cpu().numpy()\n\n                scaling_factor = self._scaling_factor.detach()\n                scaling_factor_q = torch.quantize_per_tensor(\n                    scaling_factor,\n                    scale=self.scaling_factor_qa.scale,\n                    zero_point=self.scaling_factor_qa.zero_point,\n                    dtype=self.scaling_factor_qa.dtype,\n                ).int_repr()\n                save_dict[\"scaling_factor\"] = scaling_factor_q.cpu().numpy()\n                save_dict[\n                    \"scaling_factor_scale\"\n                ] = self.scaling_factor_qa.scale.cpu().numpy()\n                save_dict[\n                    \"scaling_factor_zero_point\"\n                ] = self.scaling_factor_qa.zero_point.cpu().numpy()\n            else:\n                save_dict[\"scaling\"] = self._scaling.detach().to(dtype).cpu().numpy()\n                save_dict[\"scaling_factor\"] = (\n                    self._scaling_factor.detach().to(dtype).cpu().numpy()\n                )\n\n            # save rotation\n            if self.quantization:\n                rotation = self.rotation_activation(self._rotation).detach()\n                rotation_q = torch.quantize_per_tensor(\n                    rotation,\n                    scale=self.rotation_qa.scale,\n                    zero_point=self.rotation_qa.zero_point,\n                    dtype=self.rotation_qa.dtype,\n                ).int_repr()\n                save_dict[\"rotation\"] = rotation_q.cpu().numpy()\n                save_dict[\"rotation_scale\"] = self.rotation_qa.scale.cpu().numpy()\n                save_dict[\n                    \"rotation_zero_point\"\n                ] = self.rotation_qa.zero_point.cpu().numpy()\n            else:\n                save_dict[\"rotation\"] = self._rotation.detach().to(dtype).cpu().numpy()\n\n            save_fn = np.savez_compressed if compress else np.savez\n            save_fn(path, **save_dict)\n\n    def _sort_morton(self):\n        with torch.no_grad():\n            xyz_q = (\n                (2**21 - 1)\n                * (self._xyz - self._xyz.min(0).values)\n                / (self._xyz.max(0).values - self._xyz.min(0).values)\n            ).long()\n            order = mortonEncode(xyz_q).sort().indices\n            self._xyz = nn.Parameter(self._xyz[order], requires_grad=True)\n            self._opacity = nn.Parameter(self._opacity[order], requires_grad=True)\n            self._scaling_factor = nn.Parameter(\n                self._scaling_factor[order], requires_grad=True\n            )\n\n            if self.is_color_indexed:\n                self._feature_indices = nn.Parameter(\n                    self._feature_indices[order], requires_grad=False\n                )\n            else:\n                self._features_rest = nn.Parameter(\n                    self._features_rest[order], requires_grad=True\n                )\n                self._features_dc = nn.Parameter(\n                    self._features_dc[order], requires_grad=True\n                )\n\n            if self.is_gaussian_indexed:\n                self._gaussian_indices = nn.Parameter(\n                    self._gaussian_indices[order], requires_grad=False\n                )\n            else:\n                self._scaling = nn.Parameter(self._scaling[order], requires_grad=True)\n                self._rotation = nn.Parameter(self._rotation[order], requires_grad=True)\n\n    def mask_splats(self, mask: torch.Tensor):\n        with torch.no_grad():\n            self._xyz = nn.Parameter(self._xyz[mask], requires_grad=True)\n            self._opacity = nn.Parameter(self._opacity[mask], requires_grad=True)\n            self._scaling_factor = nn.Parameter(\n                self._scaling_factor[mask], requires_grad=True\n            )\n\n            if self.is_color_indexed:\n                self._feature_indices = nn.Parameter(\n                    self._feature_indices[mask], requires_grad=False\n                )\n            else:\n                self._features_dc = nn.Parameter(\n                    self._features_dc[mask], requires_grad=True\n                )\n                self._features_rest = nn.Parameter(\n                    self._features_rest[mask], requires_grad=True\n                )\n            if self.is_gaussian_indexed:\n                self._gaussian_indices = nn.Parameter(\n                    self._gaussian_indices[mask], requires_grad=False\n                )\n            else:\n                self._scaling = nn.Parameter(self._scaling[mask], requires_grad=True)\n                self._rotation = nn.Parameter(self._rotation[mask], requires_grad=True)\n\n    def set_color_indexed(self, features: torch.Tensor, indices: torch.Tensor):\n        self._feature_indices = nn.Parameter(indices, requires_grad=False)\n        self._features_dc = nn.Parameter(features[:, :1].detach(), requires_grad=True)\n        self._features_rest = nn.Parameter(features[:, 1:].detach(), requires_grad=True)\n        self.color_index_mode = ColorMode.ALL_INDEXED\n\n    def set_gaussian_indexed(\n        self, rotation: torch.Tensor, scaling: torch.Tensor, indices: torch.Tensor\n    ):\n        self._gaussian_indices = nn.Parameter(indices.detach(), requires_grad=False)\n        self._rotation = nn.Parameter(rotation.detach(), requires_grad=True)\n        self._scaling = nn.Parameter(scaling.detach(), requires_grad=True)\n\n\nclass FakeQuantizationHalf(torch.autograd.Function):\n    \"\"\"performs fake quantization for half precision\"\"\"\n\n    @staticmethod\n    def forward(_, x: torch.Tensor) -> torch.Tensor:\n        return x.half().float()\n\n    @staticmethod\n    def backward(_, grad_output: torch.Tensor) -> torch.Tensor:\n        return grad_output\n\n\ndef splitBy3(a):\n    x = a & 0x1FFFFF  # we only look at the first 21 bits\n    x = (x | x << 32) & 0x1F00000000FFFF\n    x = (x | x << 16) & 0x1F0000FF0000FF\n    x = (x | x << 8) & 0x100F00F00F00F00F\n    x = (x | x << 4) & 0x10C30C30C30C30C3\n    x = (x | x << 2) & 0x1249249249249249\n    return x\n\n\ndef mortonEncode(pos: torch.Tensor) -> torch.Tensor:\n    x, y, z = pos.unbind(-1)\n    answer = torch.zeros(len(pos), dtype=torch.long, device=pos.device)\n    answer |= splitBy3(x) | splitBy3(y) << 1 | splitBy3(z) << 2\n    return answer\nFile Path: gaussian_renderer/__init__.py\nContent:\n#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use\n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\nimport torch\nimport math\nfrom diff_gaussian_rasterization import (\n    GaussianRasterizationSettings,\n    GaussianRasterizer,\n    GaussianRasterizerIndexed,\n)\nfrom scene.gaussian_model import ColorMode, GaussianModel\nfrom utils.sh_utils import eval_sh\n\n\ndef render(\n    viewpoint_camera,\n    pc: GaussianModel,\n    pipe,\n    bg_color: torch.Tensor,\n    scaling_modifier=1.0,\n    override_color=None,\n    clamp_color: bool = True,\n    cov3d: torch.Tensor = None,\n):\n    \"\"\"\n    Render the scene.\n\n    Background tensor (bg_color) must be on GPU!\n    \"\"\"\n\n    # Create zero tensor. We will use it to make pytorch return gradients of the 2D (screen-space) means\n    screenspace_points = (\n        torch.zeros_like(\n            pc.get_xyz, dtype=pc.get_xyz.dtype, requires_grad=True, device=\"cuda\"\n        )\n        + 0\n    )\n    try:\n        screenspace_points.retain_grad()\n    except:\n        pass\n\n    # Set up rasterization configuration\n    tanfovx = math.tan(viewpoint_camera.FoVx * 0.5)\n    tanfovy = math.tan(viewpoint_camera.FoVy * 0.5)\n\n    raster_settings = GaussianRasterizationSettings(\n        image_height=int(viewpoint_camera.image_height),\n        image_width=int(viewpoint_camera.image_width),\n        tanfovx=tanfovx,\n        tanfovy=tanfovy,\n        bg=bg_color,\n        scale_modifier=scaling_modifier,\n        viewmatrix=viewpoint_camera.world_view_transform,\n        projmatrix=viewpoint_camera.full_proj_transform,\n        sh_degree=pc.active_sh_degree,\n        campos=viewpoint_camera.camera_center,\n        prefiltered=False,\n        debug=pipe.debug,\n        clamp_color=clamp_color,\n    )\n\n    if pc.color_index_mode == ColorMode.ALL_INDEXED and pc.is_gaussian_indexed:\n        rasterizer = GaussianRasterizerIndexed(raster_settings=raster_settings)\n\n        means3D = pc.get_xyz\n        means2D = screenspace_points\n        opacity = pc.get_opacity\n        shs = pc._get_features_raw\n        scales = pc.get_scaling_normalized\n        scale_factors = pc.get_scaling_factor\n        rotations = pc._rotation_post_activation\n\n        # Rasterize visible Gaussians to image, obtain their radii (on screen).\n        rendered_image, radii = rasterizer(\n            means3D=means3D,\n            means2D=means2D,\n            shs=shs,\n            sh_indices=pc._feature_indices,\n            g_indices=pc._gaussian_indices,\n            colors_precomp=None,\n            opacities=opacity,\n            scales=scales,\n            scale_factors=scale_factors,\n            rotations=rotations,\n            cov3D_precomp=None,\n        )\n\n        # Those Gaussians that were frustum culled or had a radius of 0 were not visible.\n        # They will be excluded from value updates used in the splitting criteria.\n        return {\n            \"render\": rendered_image,\n            \"viewspace_points\": screenspace_points,\n            \"visibility_filter\": radii > 0,\n            \"radii\": radii,\n        }\n    else:\n        rasterizer = GaussianRasterizer(raster_settings=raster_settings)\n\n        means3D = pc.get_xyz\n        means2D = screenspace_points\n        opacity = pc.get_opacity\n\n        # If precomputed 3d covariance is provided, use it. If not, then it will be computed from\n        # scaling / rotation by the rasterizer.\n        scales = None\n        rotations = None\n        cov3D_precomp = cov3d\n        if cov3D_precomp is None:\n            if pipe.compute_cov3D_python:\n                cov3D_precomp = pc.get_covariance(scaling_modifier)\n            else:\n                scales = pc.get_scaling\n                rotations = pc.get_rotation\n\n        # If precomputed colors are provided, use them. Otherwise, if it is desired to precompute colors\n        # from SHs in Python, do it. If not, then SH -> RGB conversion will be done by rasterizer.\n        shs = None\n        colors_precomp = None\n        if override_color is None:\n            if pipe.convert_SHs_python:\n                shs_view = pc.get_features.transpose(1, 2).view(\n                    -1, 3, (pc.max_sh_degree + 1) ** 2\n                )\n                dir_pp = pc.get_xyz - viewpoint_camera.camera_center.repeat(\n                    pc.get_features.shape[0], 1\n                )\n                dir_pp_normalized = dir_pp / dir_pp.norm(dim=1, keepdim=True)\n                sh2rgb = eval_sh(pc.active_sh_degree, shs_view, dir_pp_normalized)\n                colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)\n            else:\n                shs = pc.get_features\n        else:\n            colors_precomp = override_color\n\n        #\n        # Rasterize visible Gaussians to image, obtain their radii (on screen).\n        rendered_image, radii = rasterizer(\n            means3D=means3D,\n            means2D=means2D,\n            shs=shs,\n            colors_precomp=colors_precomp,\n            opacities=opacity,\n            scales=scales,\n            rotations=rotations,\n            cov3D_precomp=cov3D_precomp,\n        )\n\n        # Those Gaussians that were frustum culled or had a radius of 0 were not visible.\n        # They will be excluded from value updates used in the splitting criteria.\n        return {\n            \"render\": rendered_image,\n            \"viewspace_points\": screenspace_points,\n            \"visibility_filter\": radii > 0,\n            \"radii\": radii,\n        }\n\nFile Path: submodules/diff-gaussian-rasterization/diff_gaussian_rasterization/__init__.py\nContent:\n#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use\n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\nfrom typing import NamedTuple\nimport torch.nn as nn\nimport torch\nfrom . import _C\n\n# TOOD add indexed methods\n\n\ndef cpu_deep_copy_tuple(input_tuple):\n    copied_tensors = [\n        item.cpu().clone() if isinstance(item, torch.Tensor) else item\n        for item in input_tuple\n    ]\n    return tuple(copied_tensors)\n\n\ndef rasterize_gaussians(\n    means3D,\n    means2D,\n    sh,\n    colors_precomp,\n    opacities,\n    scales,\n    rotations,\n    cov3Ds_precomp,\n    raster_settings,\n):\n    return _RasterizeGaussians.apply(\n        means3D,\n        means2D,\n        sh,\n        colors_precomp,\n        opacities,\n        scales,\n        rotations,\n        cov3Ds_precomp,\n        raster_settings,\n    )\n\n\ndef rasterize_gaussians_indexed(\n    means3D,\n    means2D,\n    sh,\n    sh_indices,\n    g_indices,\n    colors_precomp,\n    opacities,\n    scales,\n    scale_factors,\n    rotations,\n    cov3Ds_precomp,\n    raster_settings,\n):\n    return _RasterizeGaussiansIndexed.apply(\n        means3D,\n        means2D,\n        sh,\n        sh_indices,\n        g_indices,\n        colors_precomp,\n        opacities,\n        scales,\n        scale_factors,\n        rotations,\n        cov3Ds_precomp,\n        raster_settings,\n    )\n\n\nclass _RasterizeGaussians(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx,\n        means3D,\n        means2D,\n        sh,\n        colors_precomp,\n        opacities,\n        scales,\n        rotations,\n        cov3Ds_precomp,\n        raster_settings,\n    ):\n        # Restructure arguments the way that the C++ lib expects them\n        args = (\n            raster_settings.bg,\n            means3D,\n            colors_precomp,\n            opacities,\n            scales,\n            rotations,\n            raster_settings.scale_modifier,\n            cov3Ds_precomp,\n            raster_settings.viewmatrix,\n            raster_settings.projmatrix,\n            raster_settings.tanfovx,\n            raster_settings.tanfovy,\n            raster_settings.image_height,\n            raster_settings.image_width,\n            sh,\n            raster_settings.sh_degree,\n            raster_settings.campos,\n            raster_settings.prefiltered,\n            raster_settings.debug,\n            raster_settings.clamp_color,\n        )\n\n        # Invoke C++/CUDA rasterizer\n        if raster_settings.debug:\n            cpu_args = cpu_deep_copy_tuple(\n                args\n            )  # Copy them before they can be corrupted\n            try:\n                (\n                    num_rendered,\n                    color,\n                    radii,\n                    geomBuffer,\n                    binningBuffer,\n                    imgBuffer,\n                ) = _C.rasterize_gaussians(*args)\n            except Exception as ex:\n                torch.save(cpu_args, \"snapshot_fw.dump\")\n                print(\n                    \"\\nAn error occured in forward. Please forward snapshot_fw.dump for debugging.\"\n                )\n                raise ex\n        else:\n            (\n                num_rendered,\n                color,\n                radii,\n                geomBuffer,\n                binningBuffer,\n                imgBuffer,\n            ) = _C.rasterize_gaussians(*args)\n\n        # Keep relevant tensors for backward\n        ctx.raster_settings = raster_settings\n        ctx.num_rendered = num_rendered\n        ctx.save_for_backward(\n            colors_precomp,\n            means3D,\n            scales,\n            rotations,\n            cov3Ds_precomp,\n            radii,\n            sh,\n            geomBuffer,\n            binningBuffer,\n            imgBuffer,\n        )\n        return color, radii\n\n    @staticmethod\n    def backward(ctx, grad_out_color, _):\n        # Restore necessary values from context\n        num_rendered = ctx.num_rendered\n        raster_settings = ctx.raster_settings\n        (\n            colors_precomp,\n            means3D,\n            scales,\n            rotations,\n            cov3Ds_precomp,\n            radii,\n            sh,\n            geomBuffer,\n            binningBuffer,\n            imgBuffer,\n        ) = ctx.saved_tensors\n\n        # Restructure args as C++ method expects them\n        args = (\n            raster_settings.bg,\n            means3D,\n            radii,\n            colors_precomp,\n            scales,\n            rotations,\n            raster_settings.scale_modifier,\n            cov3Ds_precomp,\n            raster_settings.viewmatrix,\n            raster_settings.projmatrix,\n            raster_settings.tanfovx,\n            raster_settings.tanfovy,\n            grad_out_color,\n            sh,\n            raster_settings.sh_degree,\n            raster_settings.campos,\n            geomBuffer,\n            num_rendered,\n            binningBuffer,\n            imgBuffer,\n            raster_settings.debug,\n        )\n\n        # Compute gradients for relevant tensors by invoking backward method\n        if raster_settings.debug:\n            cpu_args = cpu_deep_copy_tuple(\n                args\n            )  # Copy them before they can be corrupted\n            try:\n                (\n                    grad_means2D,\n                    grad_colors_precomp,\n                    grad_opacities,\n                    grad_means3D,\n                    grad_cov3Ds_precomp,\n                    grad_sh,\n                    grad_scales,\n                    grad_rotations,\n                ) = _C.rasterize_gaussians_backward(*args)\n            except Exception as ex:\n                torch.save(cpu_args, \"snapshot_bw.dump\")\n                print(\n                    \"\\nAn error occured in backward. Writing snapshot_bw.dump for debugging.\\n\"\n                )\n                raise ex\n        else:\n            (\n                grad_means2D,\n                grad_colors_precomp,\n                grad_opacities,\n                grad_means3D,\n                grad_cov3Ds_precomp,\n                grad_sh,\n                grad_scales,\n                grad_rotations,\n            ) = _C.rasterize_gaussians_backward(*args)\n\n        grads = (\n            grad_means3D,\n            grad_means2D,\n            grad_sh,\n            grad_colors_precomp,\n            grad_opacities,\n            grad_scales,\n            grad_rotations,\n            grad_cov3Ds_precomp,\n            None,\n        )\n\n        return grads\n\n\nclass _RasterizeGaussiansIndexed(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx,\n        means3D,\n        means2D,\n        sh,\n        sh_indices,\n        g_inidices,\n        colors_precomp,\n        opacities,\n        scales,\n        scale_factors,\n        rotations,\n        cov3Ds_precomp,\n        raster_settings,\n    ):\n        # Restructure arguments the way that the C++ lib expects them\n        args = (\n            raster_settings.bg,\n            means3D,\n            colors_precomp,\n            opacities,\n            scales,\n            scale_factors,\n            rotations,\n            raster_settings.scale_modifier,\n            cov3Ds_precomp,\n            raster_settings.viewmatrix,\n            raster_settings.projmatrix,\n            raster_settings.tanfovx,\n            raster_settings.tanfovy,\n            raster_settings.image_height,\n            raster_settings.image_width,\n            sh,\n            raster_settings.sh_degree,\n            raster_settings.campos,\n            sh_indices,\n            g_inidices,\n            raster_settings.prefiltered,\n            raster_settings.debug,\n            raster_settings.clamp_color,\n        )\n\n        # Invoke C++/CUDA rasterizer\n        if raster_settings.debug:\n            cpu_args = cpu_deep_copy_tuple(\n                args\n            )  # Copy them before they can be corrupted\n            try:\n                (\n                    num_rendered,\n                    color,\n                    radii,\n                    geomBuffer,\n                    binningBuffer,\n                    imgBuffer,\n                ) = _C.rasterize_gaussians_indexed(*args)\n            except Exception as ex:\n                torch.save(cpu_args, \"snapshot_fw.dump\")\n                print(\n                    \"\\nAn error occured in forward. Please forward snapshot_fw.dump for debugging.\"\n                )\n                raise ex\n        else:\n            (\n                num_rendered,\n                color,\n                radii,\n                geomBuffer,\n                binningBuffer,\n                imgBuffer,\n            ) = _C.rasterize_gaussians_indexed(*args)\n\n        # Keep relevant tensors for backward\n        ctx.raster_settings = raster_settings\n        ctx.num_rendered = num_rendered\n        ctx.save_for_backward(\n            colors_precomp,\n            means3D,\n            scales,\n            scale_factors,\n            rotations,\n            cov3Ds_precomp,\n            radii,\n            sh,\n            geomBuffer,\n            binningBuffer,\n            imgBuffer,\n            sh_indices,\n            g_inidices,\n        )\n        return color, radii\n\n    @staticmethod\n    def backward(ctx, grad_out_color, _):\n        # Restore necessary values from context\n        num_rendered = ctx.num_rendered\n        raster_settings = ctx.raster_settings\n        (\n            colors_precomp,\n            means3D,\n            scales,\n            scale_factors,\n            rotations,\n            cov3Ds_precomp,\n            radii,\n            sh,\n            geomBuffer,\n            binningBuffer,\n            imgBuffer,\n            sh_indices,\n            g_inidices,\n        ) = ctx.saved_tensors\n\n        # Restructure args as C++ method expects them\n        args = (\n            raster_settings.bg,\n            means3D,\n            radii,\n            colors_precomp,\n            scales,\n            scale_factors,\n            rotations,\n            raster_settings.scale_modifier,\n            cov3Ds_precomp,\n            raster_settings.viewmatrix,\n            raster_settings.projmatrix,\n            raster_settings.tanfovx,\n            raster_settings.tanfovy,\n            grad_out_color,\n            sh,\n            raster_settings.sh_degree,\n            raster_settings.campos,\n            geomBuffer,\n            num_rendered,\n            binningBuffer,\n            imgBuffer,\n            raster_settings.debug,\n            sh_indices,\n            g_inidices,\n        )\n\n        # Compute gradients for relevant tensors by invoking backward method\n        if raster_settings.debug:\n            cpu_args = cpu_deep_copy_tuple(\n                args\n            )  # Copy them before they can be corrupted\n            try:\n                (\n                    grad_means2D,\n                    grad_colors_precomp,\n                    grad_opacities,\n                    grad_means3D,\n                    grad_cov3Ds_precomp,\n                    grad_sh,\n                    grad_scales,\n                    grad_scale_factors,\n                    grad_rotations,\n                ) = _C.rasterize_gaussians_backward_indexed(*args)\n            except Exception as ex:\n                torch.save(cpu_args, \"snapshot_bw.dump\")\n                print(\n                    \"\\nAn error occured in backward. Writing snapshot_bw.dump for debugging.\\n\"\n                )\n                raise ex\n        else:\n            (\n                grad_means2D,\n                grad_colors_precomp,\n                grad_opacities,\n                grad_means3D,\n                grad_cov3Ds_precomp,\n                grad_sh,\n                grad_scales,\n                grad_scale_factors,\n                grad_rotations,\n            ) = _C.rasterize_gaussians_backward_indexed(*args)\n\n        grads = (\n            grad_means3D,\n            grad_means2D,\n            grad_sh,\n            None,\n            None,\n            grad_colors_precomp,\n            grad_opacities,\n            grad_scales,\n            grad_scale_factors,\n            grad_rotations,\n            grad_cov3Ds_precomp,\n            None,\n        )\n\n        return grads",
        "experimental_info": "The methodology involves: 1) Sensitivity-aware vector clustering, 2) Quantization-aware fine-tuning, and 3) Entropy encoding, followed by hardware rasterization for novel view rendering.\n\n**1. Sensitivity-aware Vector Clustering:**\n*   **Sensitivity Measure:** Computed by summing absolute gradients of image energy with respect to color (SH features) and Gaussian shape (covariance) parameters over training images.\n*   **Clustering Settings (Color SH coefficients):**\n    *   `color_codebook_size`: 4096 (2^12)\n    *   `color_importance_include`: 0.0000006 (0.6 * 1e-6) - sensitivity threshold for keeping parameters uncompressed.\n    *   `color_importance_prune`: 0.0\n    *   `color_cluster_iterations`: 100\n    *   `color_decay`: 0.8 (for Exponential Moving Average in VQ)\n    *   `color_batch_size`: 262144 (2^18)\n    *   `color_compress_non_dir`: True (implies compressing all SH coefficients, not just DC component)\n    *   `not_compress_color`: False (compression is enabled)\n*   **Clustering Settings (Gaussian Shape - normalized covariance matrices):**\n    *   `gaussian_codebook_size`: 4096 (2^12)\n    *   `gaussian_importance_include`: 0.000003 (0.3 * 1e-5) - sensitivity threshold for keeping parameters uncompressed.\n    *   `gaussian_cluster_iterations`: 800\n    *   `gaussian_decay`: 0.8 (for EMA in VQ)\n    *   `gaussian_batch_size`: 1048576 (2^20)\n    *   `not_compress_gaussians`: False (compression is enabled)\n*   **Pruning:** Gaussians with `color_importance` below `prune_threshold` of 0.0 are pruned before clustering.\n*   **Clustering Algorithm:** Utilizes `weightedDistance` (a custom CUDA kernel) within `VectorQuantize` for k-Means clustering, with weights derived from the sensitivity measure. Gaussian shape clustering includes `scale_normalize=True` to ensure normalized eigenvalues/scales.\n\n**2. Quantization-aware Fine-tuning:**\n*   **Fine-tuning Iterations:** `finetune_iterations`: 5000\n*   **Quantization Scheme:**\n    *   Position (`_xyz`): 16-bit float (`FakeQuantizationHalf.apply`).\n    *   Color features (`_features_dc`, `_features_rest`), opacity (`_opacity`), scaling (`_scaling`), scaling factor (`_scaling_factor`), and rotation (`_rotation`): 8-bit integer (`torch.ao.quantization.FakeQuantize(dtype=torch.qint8)`).\n    *   `not_quantization_aware`: False (quantization-aware training is enabled).\n*   **Optimization Parameters (applied during fine-tuning):**\n    *   `position_lr_init`: 0.00016\n    *   `position_lr_final`: 0.0000016\n    *   `position_lr_delay_mult`: 0.01\n    *   `position_lr_max_steps`: 30000\n    *   `feature_lr`: 0.0025\n    *   `opacity_lr`: 0.05\n    *   `scaling_lr`: 0.005\n    *   `rotation_lr`: 0.001\n    *   `lambda_dssim`: 0.2 (weight for DSSIM loss component)\n\n**3. Entropy Encoding:**\n*   **Z-order (Morton order) Curve:** Gaussians are linearized (sorted) along a Z-order curve before saving (`_sort_morton` is called if `not_sort_morton` is False, which is the default). Morton encoding uses 21 bits per spatial dimension (`0x1FFFFF`).\n*   **Further Compression:** The `save_npz` function uses `np.savez_compressed` (DEFLATE algorithm - LZ77 + Huffman coding) to save the quantized and indexed Gaussian parameters, scales, and zero points.\n\n**4. Novel View Rendering (Hardware Rasterization Pipeline):**\n*   **Rasterization:** A hardware rasterization pipeline (`diff_gaussian_rasterization`) is used, specifically `GaussianRasterizerIndexed` when Gaussian parameters (color and shape) are compressed and indexed.\n*   **Preprocess and GPU Sorting:** The CUDA rasterizer handles frustum culling, direction-dependent color computation (if not precomputed), and depth-sorting of Gaussians on the GPU (implicitly through the rasterization process, typically using a parallel sorting algorithm like Onesweep in the underlying CUDA implementation, though not explicitly detailed in the provided code, it's a common technique for 3DGS rasterization).\n*   **Shaders:** Gaussians are rendered as planar quads (splats) using vertex and pixel shaders, with the pixel shader handling exponential color/opacity falloff and blending."
      }
    },
    {
      "title": "MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models",
      "abstract": "We present a novel compression algorithm for reducing the storage of LiDAR\nsensor data streams. Our model exploits spatio-temporal relationships across\nmultiple LiDAR sweeps to reduce the bitrate of both geometry and intensity\nvalues. Towards this goal, we propose a novel conditional entropy model that\nmodels the probabilities of the octree symbols by considering both coarse level\ngeometry and previous sweeps' geometric and intensity information. We then use\nthe learned probability to encode the full data stream into a compact one. Our\nexperiments demonstrate that our method significantly reduces the joint\ngeometry and intensity bitrate over prior state-of-the-art LiDAR compression\nmethods, with a reduction of 7-17% and 15-35% on the UrbanCity and\nSemanticKITTI datasets respectively.",
      "full_text": "MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models Sourav Biswas1,2 Jerry Liu1 Kelvin Wong1,3 Shenlong Wang1,3 Raquel Urtasun1,3 1Uber Advanced Technologies Group 2University of Waterloo 3University of Toronto {souravb,jerryl,kelvin.wong,slwang,urtasun}@uber.com Abstract We present a novel compression algorithm for reducing the storage of LiDAR sensor data streams. Our model exploits spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values. Towards this goal, we propose a novel conditional entropy model that models the probabilities of the octree symbols by considering both coarse level geometry and previous sweeps’ geometric and intensity information. We then use the learned probability to encode the full data stream into a compact one. Our experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduction of 7–17% and 6–19% on the UrbanCity and SemanticKITTI datasets respectively. 1 Introduction The past decade has witnessed numerous innovations in intelligent systems, thanks to an explosion of progress in sensing and AI algorithms. In particular, LiDAR sensors are extensively used in various applications such as indoor rovers, unmanned aerial vehicles, and self-driving cars to accurately capture the 3D geometry of the scene. Yet the rapid adoption of LiDAR has brought about a key challenge—dealing with the mounting storage costs associated with the massive inﬂux of LiDAR data. For instance, a 64-line Velodyne LiDAR continuously scanning a given scene produces over3 billion pointsin a single hour. Hence, developing efﬁcient and effective compression algorithms to store such 3D point cloud data streams is crucial to reduce the storage and communication bandwidth. Unlike its well-studied image and video counterparts, point cloud stream compression is a challenging yet under-explored problem. Many prior approaches have focused on encoding a point cloud stream as independent sweeps, where each sweep captures a rough 360-degree rotation of the sensor. Early approaches exploit a variety of compact data structures to represent the point cloud in a memory- efﬁcient manner, such as octrees [1], KD-trees [2], and spherical images [3]. More recent works along this direction utilize powerful machine learning models to encode redundant geometric correlations within these data structures for better compression [4, 5, 6]. In general, most of these aforementioned approaches do not make effective use of temporal correlations within point clouds. Moreover, these prior approaches have largely focused on compressing the geometric structure of the point cloud (the spatial coordinates); yet there has been little attention paid towards compression of other attributes, e.g. LiDAR intensity, which are crucial for many downstream tasks. Compressing such attributes along with geometric structure can make a signiﬁcant impact on reducing storage. In this paper, we present a novel, learning-based compression algorithm that comprehensively reduces the storage of LiDAR sensor data streams. Our method extends the recent success of octree-structured deep entropy models [6] for single LiDAR sweep compression to intensity-valued LiDAR streaming data. Speciﬁcally, we propose a novel deep conditional entropy modelthat models the probabilities of the octree symbols and associated intensity values by exploiting spatio-temporal correlations within the data: taking both coarse level information at the current sweep, as well as relevant neighboring 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2011.07590v2  [eess.IV]  8 Jan 2021(a) Top-down Pass(b) Bottom-up Pass Sweep t-1 Sweep t (2) Prior Octree Dependence(1) Ancestral Node Dependence (3) Spatio-Temporal Aggregation z x y  Octree Occupancy Entropy Model (Sec. 2.3)Intensity Entropy Model (Sec. 2.4) Sweept -1 Sweep t Geometry Bitstream Probability Estimation Intensity Bitstream Continuous Convolution Continuous Convolution z x y  Sweep t-1 Sweep t Input Point Cloud àOctree (Sec 2.1) Probability Estimation Figure 1: Comprehensive overview of our method. Our point cloud stream is serialized into an octree repre- sentation (Sec 2.1). We apply a spatio-temporal entropy model to the octree occupancy bytestream (Sec. 2.3), modeling ancestral dependence, prior octree dependence, and octree alignment. We also apply a deep entropy model to model the intensity stream (Sec. 2.4). nodes information from the previous sweep. Unlike prior approaches, our method models the joint entropy across an entire point cloud sequence, while unifying geometry and attribute compression into the same framework. We validate the performance of our approach on two large datasets, namely UrbanCity [ 7] and SemanticKITTI [8]. The experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduc- tion of 7–17% on UrbanCity and 6–19% on SemanticKITTI. We also conduct extensive experiments showcasing superior performance against prior works on numerous downstream perception tasks. 2 Multi-Sweep LiDAR Compression In this work, we propose a comprehensive framework for thelossy compression of LiDAR point cloud streams, by exploiting the spatio-temporal redundancies through a learned entropy model. We aim to maximize the reconstruction quality of these point clouds while reducing their joint bitrate. Every point in a LiDAR point cloud contains both a spatial 3D location (x,y,z ), as well as an intensity value r, and we jointly compress both. Our method is shown in Fig. 1. We ﬁrst quantize and encode all point spatial coordinates in the stream into an octree representation, where leaves represent the quantized points and intermediate nodes contain 8-bit symbols representing child occupancies (Sec. 2.1). We then present a novel deep entropy model (Sec. 2.2): a probability model that utilizes spatio-temporal contextto predict occupancy symbols for each node (Sec. 2.3), as well as intensity values for each point for intensity compression (Sec. 2.4). The outputs of these entropy models are ﬁnally fed into a lossless entropy coding algorithm, such as range coding, to produce the ﬁnal bitstream (Sec. 2.5). 2.1 Octree Representation Octree Structure and Bit Representation:LiDAR point clouds are intrinsically challenging to process due to their sparsity and inherently unstructured nature. A tool to counteract these challenges is to use a tree-based data structure, such as an octree or KD-tree, to efﬁciently partition the space. Inspired by [1, 6], we quantize and represent every point cloud in our stream as an octree with an associated depth value D, corresponding to the quantized precision of the point cloud. Speciﬁcally, an octree can be constructed from a 3D point cloud by ﬁrst partitioning the spatial region into 8 octants, and recursively partitioning each octant until each node contains at most one point, or until Dis reached. The resulting octree contains both intermediate nodes and leaf nodes. Each intermediate node can be represented by an 8-bit occupancy symbol x, representing the occupancies of its children; each node also has an implied spatial position. Each leaf node contains one point of the point cloud, and stores the offset between the point and its corner position, as well as the point intensity. We determine the intensity value of each point in the quantized point cloud by taking that of its nearest neighbor in the original point cloud. The number of bits allocated to each leaf node is level-dependent; an octree with D= kwill store k−ibits for a leaf node at level i,i ≤k. Hence, 2the octree is memory-efﬁcient—shared bits are encoded with intermediate nodes and residual bits with leaves. Serialization: We serialize the octree into two (uncompressed) bytestreams by traversing the octree in breadth-ﬁrst order. The ﬁrst bytestream contains the intermediate node occupancy symbols in breadth-ﬁrst order, and the second bytestream contains the leaf node offsets/intensities encountered during traversal. Our entropy model focuses primarily on the node occupancies/intensities—we demonstrate in our supplementary materials that leaf offsets do not contain meaningful patterns we can exploit. Hence for subsequent sections we denote P(t) = (X(t),R(t)), where X(t) = {x(t) 1 ,..., x(t) mt } is the set of occupancy symbols, and R(t) = {r(t) 1 ,..., r(t) nt }is the set of intensities. The serialization is lossless; the only loss comes from D-dependent octree quantization. This gives a guarantee on reconstruction quality and allows compression efforts to solely focus on bitrate reduction. 2.2 Octree-Based Conditional Entropy Module The octree sequence is now fed into ourentropy model. Our entropy model is a probability model that approximates the unknown joint distribution of point clouds pdata with our own distribution p(·; w). Since we convert our point clouds to octree representations, the probability model is equivalent to modeling p(P(1),..., P(n); w). According to the classic Shannon’s source coding theorem [9], the expected bitrate for the point cloud stream is tightly approximated by the cross-entropy between the real point cloud stream distribution and our parametrized model: Epdata [−log p(P(1),..., P(n); w)]. We then assume that the joint probability factorizes as follows: log p(P(1),..., P(n); w) = ∑ t log p(P(t)|P(t−1); w) (1) = ∑ t {log p(X(t)|P(t−1); w) + logp(R(t)|X(t),P(t−1); w)} (2) We make a 1st-order Markov assumption: a given octree P(t) only depends on the sweep preced- ing it, P(t−1). We then factor the octree into two entropy models: the node occupancy model p(X(t)|P(t−1); w), and the intensity model p(R(t)|X(t),P(t−1); w) conditioned on occupancies. The dependence only on past sweeps makes the model applicable to an online LiDAR stream setting. 2.3 Occupancy Entropy Model We obtain our node occupancy model by continuing to factorize the occupancy probabilities: p(X(t)|P(t−1); w) = ∏ i p(x(t) i |X(t) ans(i),P(t−1); w) (3) Here, X(t) ans(i) = {x(t) pa(i),x(t) pa(pa(i)),..., x(t) pa(...(pa(i)))}represents the set of ancestor nodes of x(t) i and P(t−1) represents the point cloud from previous sweep. As seen above, we simplify the autoregressive dependency on ancestors nodes on the octree for the given timestamp, as well as all the nodes at the previous timestamp. We model p(·|X(t) ans(i),P(t−1); w) with a deep neural network. The architecture has two backbones, namely the ancestral node dependencemodule which encodes recurrent dependencies on the ancestor nodes X(t) ans(i) from the current sweep’s octree as well as a prior octree dependencemodule which models information passed from the previous sweep. Fig. 1 depicts the architecture of such network. Ancestral Node Dependence: Our ancestral node dependence module is a recurrent network deﬁned over an ancestral, top-down octree path. Inspired by [ 6], we feed a context feature ci for every node xi through a multi-layer perceptron (MLP) to extract an initial hidden embed- ding h(t) i,0 = σ0(ci; w), where σ0(·; w) denotes a MLP with learnable parameter w. Context 3features include the current octree level, octant spatial location, and parent occupancy; they are known beforehand per node xi and computed to facilitate representation learning. We then per- form Kans rounds of aggregation between every node’s embedding and its parental embedding: h(t) i,k = σk([h(t) i,k−1,h(t) pa(i),k−1]; w). As shorthand, we denote this entire tree-structured recurrent backbone branch as h(t) i = fans(x(t) i ,X(t) ans(i)). Temporal Octree Dependence: We also incorporate the previous octree P(t−1) into the current entropy model at time t through a temporal octree dependencemodule. We thus ﬁrst align the previous octree into the sensor coordinate frame of the current octree. Unlike the current octree where we only have access to parental information, we can construct features that make use of all information within the previous octree, containing both top-down ancestral information as well as bottom-up child information. We exploit this fact by designing a two-stream feature backbone to compute embeddings for every octree node at time t−1, inspired by tree-structured message passing algorithms [10, 11]. The forward pass stream is the same as the ancestral dependence module above, generating top-down features from ancestors: h(t−1) j = fans(x(t−1) j ,X(t−1) ans(j) ). After the top-down pass, we design a bottom-up aggregation pass, a recurrent network that produces aggregated features from descendants to the current node. Unlike the ancestral module in which each node only has one parent, the number of children per node can vary, and we desire that the output is invariant to the ordering of the children. Hence, we resolve this by designing the following function inspired by deep sets [12]: g(t−1) j = fagg,1(h(t−1) j + ∑ c∈child(j) fagg,2(g(t−1) c )), which produces the ﬁnal aggregated embedding feature containing both top-down and bottom-up context. Spatio-Temporal Aggregation: The ﬁnal step incorporates the set of aggregated features in the previous octree {g(t−1) j }, with ancestral features in the current octree {h(t) i }to help with occupancy prediction in the current octree. A key observation is that only a subset of spatially proximal nodes in the previous sweep can contribute to better prediction for a given node at timet; moreover, the relative location of each neighbor should deﬁne its relative importance. Inspired by this fact, we employ continuous convolutions[13] to process previous octree features at the current node. A continuous conv. layer aggregates features from neighboring points to a given node in the following manner: hi = ∑ j∈N(i) σ(pj −pi)hj where N(i) is the i-th node’sk-nearest neighboring nodes in 3D space from the (t−1) sweep at the same level as i, pi is the 3D position of each node, and σdenotes a learned MLP. We use a separate MLP with a continuous conv. layer per octree level to process the aggregated features in the previous octree {g(t−1) j }j∈N(i) and produce an embedding feature g(t) i,st. Entropy Header: Finally, the warped feature g(t) i,st and ancestral features h(t) i are aggre- gated through a ﬁnal MLP to output a 256-dimensional softmax of probability values p(x(t) i |X(t) ans(i),P(t−1); w), corresponding to the predicted 8-bit occupancy for node i, time t. 2.4 Intensity Entropy Model The goal of the intensity entropy model is to compress extraneous intensities tied to each spatial point coordinate. We assume these intensities are bounded and discrete, so compression is lossless; if they are continuous, there will be a loss incurred through discretization. The model factorizes as follows: p(R(t)|X(t),P(t−1); w) = ∏ i p(r(t) i |X(t),P(t−1); w) (4) The intent of conditioning on the occupancies X(t) is not to directly use their values per se, but to emphasize that intensity decoding occurs after the point spatial coordinates have already been reconstructed in R3. Therefore, we can directly make use of the spatial position corresponding to each intensity R(t) i in compression. We aim to leverage temporal correlations between point intensities across consecutive timestamps to better model the entropy of r(t) i . Similar to node occupancy predic- tion above, there is the challenge of how to incorporate previous intensity information when there are no direct correspondences between the two point clouds. We again employ continuous convolutions 40 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 0.00 0.05 0.10 0.15Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 70 80 90 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 0.00 0.05 0.10 0.15 0.20Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 60 70 80 90 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Figure 2: Bitrate vs. reconstruction quality curves on UrbanCity (top) and KITTI (bottom). From left-to-right: F1 with τgeo = 10cmand τint = 0(↑), point-to-point chamfer distance (↓), point-to-plane PSNR (↑). to resolve this challenge. Let RN(i) be the set of nearest neighbor intensities {r(t−1) j }j∈N(i), where nearest neighbor is deﬁned by spatial proximity of previous point jto the current point i. We apply an MLP with a continuous conv. layer that takes the past intensities r(t−1) j as input and outputs an embedding feature for each node i. This feature is then fed through a linear layer and softmax to output intensity probability values. In our setting we assume our intensity value is an 8-bit integer, so the resulting probability vector is 256-dimensional p(r(t) i |X(t),P(t−1); w). 2.5 Entropy Coding Process Encoding: We integrate our entropy model with an entropy coding algorithm (range coding [14]) to produced the ﬁnal compressed bitstream. During the encoding pass, for every octree in the stream, the entropy model is applied across the octree occupancy bytestream, as well as across the intensities per point, to predict the respective probability distributions. We note that encoding only requires one batch GPU pass per sweep for the occupancy and intensity models. The resulting distributions are then passed to range coding which compresses the occupancies and intensities into two bitstreams. Decoding: The same entropy models are used during decoding. First, the occupancy entropy model is run, given the already decoded past octree, to produce distributions that recover the occupancy serialization and spatial coordinates of the current point cloud. Then, the intensity entropy model is run, given the already decoded intensities in the past point cloud, to produce distributions that recover the current point intensities. Note that our model is well-setup for parallel computation during decoding, for both the occupancies and intensities. As mentioned in Sec. 2.3, the dependence on ancestral nodes instead of all past nodes allows us to only run at most O(D) GPU passes for the occupancy model per sweep. Moreover, the assumed independence between intensities in the current sweep, given the past, allows us to only run 1 GPU pass per sweep for the intensity entropy model. 2.6 Learning Both our occupancy and intensity entropy models are trained end-to-end with cross-entropy loss, for every node x(t) i ∈X(t) i and intensity r(t) i ∈R(t) i , for every point cloud in a stream: ℓ= EP∼pdata [ − ∑ t ∑ i log p(x(t) i,gt|X(t) ans(i),P(t−1); w) − ∑ t ∑ i log p(r(t) i,gt|X(t),P(t−1); w) ] (5) 5Oracle (UrbanCity): Bitrate 104  Ours: F1 92.4, Bitrate 9.3  Draco: F1 80.8, Bitrate 9.4  MPEG: F1 59.1, Bitrate 11.4 Oracle (UrbanCity): Bitrate 104  Ours: F1 99.2, Bitrate 13.7  Draco: F1 92.3, Bitrate 13.8  MPEG: F1 81.5, Bitrate 16.2 Oracle (KITTI): Bitrate 104  Ours: F1 90.2, Bitrate 5.6  Draco: F1 87.1, Bitrate 5.7  MPEG: F1 61.0, Bitrate 9.5 Oracle (KITTI): Bitrate 104  Ours: F1 98.6, Bitrate 10.1  Draco: F1 96.9, Bitrate 10.1  MPEG: F1 79.9, Bitrate 12.9 Figure 3: Qualitative results on UrbanCity and KITTI. Points are colored by intensity. Here, x(t) i,gt and r(t) i,gt denote the ground-truth values of the node occupancies/intensities, respectively. As mentioned above, minimizing cross-entropy loss is equivalent to our goal of reducing expected bitrate of the point cloud stream. 2.7 Discussion and Related Works Our approach belongs to a family of point cloud compression algorithms based on tree data struc- tures [15, 2, 16, 17, 18, 19, 20, 1, 20, 21, 22, 23, 24]. Tree-based algorithms are advantageous since they use spatial-partioning data structures that can efﬁciently represent sparse and non-uniformly dense 3D point clouds. Two notable examples are Google’s Draco [2] and the MPEG anchor [25], which use a KD-tree codec [ 15] and an octree codec [ 1] respectively. To exploit temporal redun- dancies, the MPEG anchor encodes successive point clouds as block-based rigid transformations of previous point clouds; this, however, narrows its usefulness to scenes with limited motion. Moreover, these prior works use simple entropy models that do not fully exploit redundant information hidden in LiDAR point clouds; e.g., repetitive local structures, objects with strong shape priors, etc. In contrast, we use a learned entropy model to directly capture these dependencies. Our approach is also related to work in deep point cloud compression [ 4, 5, 26, 27, 28, 29, 6]. In particular, both our approach and the prior state-of-the-art [6] use deep entropy models that operate on octree structures directly. However, they do not model temporal redundancies between successive point clouds and compress LiDAR geometry only. In this work, we propose a uniﬁed framework that aggregates spatio-temporal context to jointly compress both LiDAR geometry and intensity. Finally, our work is inspired by recent successes in deep image compression [30, 31, 32, 33, 34, 35, 36] and video compression [37, 38, 39, 40, 41, 42, 43], many of which use deep entropy models. 3 Experimental Evaluation We evaluate our LiDAR compression method on two large-scale datasets. Holding reconstruction quality equal, our framework for joint geometry and intensity compression achieves a 7–17% and 6– 19% bitrate reduction over OctSqueeze [6], the prior state-of-the-art in deep point cloud compression, on UrbanCity and SemanticKITTI. Holding bitrate equal, our method’s reconstructions also have a smaller realism gap on downstream tasks. 6Spatial Bitrate O T B CC D = 12 D = 14 D = 16 ✓ 2.91 8.12 14.16 ✓ ✓ 2.87 8.04 14.08 ✓ ✓ ✓ 2.72 7.90 13.95 ✓ ✓ ✓ ✓ 2.55 7.64 13.79 Table 1: Abalation study of occupancy entropy model on UrbanCity. O, T, and B stand for using past oc- cupancy bytes, top-down aggregated features, and bottom-up aggregated features. CC indicates using continuous conv. D stands for the octree’s max depth. Intensity Bitrate Encoder P D = 12 D = 14 D = 16 zlib[45] 2.42 4.79 5.23 MLP ✓ 2.31 4.62 5.01 CC ✓ 2.13 4.30 4.68 Table 2: Ablation study of intensity entropy model on SemanticKITTI. zlib is an off-the-shelf library [45]; MLP is our model without continuous conv.; and CC is our ﬁnal model. P stands for using past intensity information. D stands for the octree’s max depth. 3.1 Experimental Details We validate the performance of our approach on two datasets: UrbanCity [7] and SemanticKITTI [8]. UrbanCity: UrbanCity is a large-scale dataset collected by a ﬂeet of self-driving vehicles in several cities across North America [7]. Every sequence consists of 250 Velodyne HDL-64E LiDAR sweeps sampled at 10Hz, each containing a 3D point cloud (as 32-bit ﬂoats) and their intensity values (as 8-bit unsigned integers). The average size of each sweep is 80,156 points. We train our entropy models on 5000 sequences and evaluate on a test set of 500. Every sweep in UrbanCity is annotated with per-point semantic labels for the vehicle, pedestrian, motorbike, road, and background classes, as well as bird’s eye view bounding boxes for the ﬁrst three classes. We use these labels to perform downstream perception experiments on the same train/test split. SemanticKITTI: We also conduct compression and downstream perception experiments on Se- manticKITTI [8], which enhances the KITTI [44] dataset with dense semantic labels for each LiDAR sweep. It consists of 22 driving sequences containing a total of 43,552 Velodyne HDL-64E LiDAR sweeps sampled at 10Hz. The average size of each sweep is 120,402 points. In our experiments, we use the ofﬁcial train/test splits: sequences 00 to 10 (except for 08) for training and sequences 11 to 21 to evaluate reconstruction quality. Since semantic labels for the test split are unavailable, we evaluate downstream tasks on the validation sequence 08. Baselines: We compare against a number of state-of-the-art LiDAR compression algorithms: Huang et al.’s deep octree-based method (OctSqueeze) [6], Google’s KD-tree based method (Draco) [2], Mekuria et al.’s octree-based MPEG anchor (MPEG Anchor) [1]1, and MPEG TMC132. From discussions with the authors, “MPEG Anchor” in [ 6] is a custom implementation that uses an empirical histogram distribution to compress octree occupancy symbols; we report this baseline as Octree. As OctSqueeze and Octree do not compress LiDAR intensities, we augment them with an off-the-shelf lossless compression algorithm [45]. In particular, we ﬁrst assign an intensity to each encoded point based on the intensity of its nearest neighbour in the original point cloud. Then, we compress the resulting bytestream. For MPEG Anchor, we use the built-in PCL color coder in the authors’ implementation, which encodes the average intensity at each leaf node in the octree with range coding. Similarly, for Draco and MPEG TMC13, we use their built-in attributes coders. We also compare against a video compression based algorithm using LiDAR’s range image representation (MPEG Range). As this baseline was uncompetitive, we report its results in the supplementary. Implementation Details: In our experiments, we construct octrees over a 400m ×400m ×400m region of interest centered on the ego-vehicle. By varying the octree’s maximum depth from 11 to 16, we can control our method’s bitrate-distortion tradeoff, with spatial quantization errors ranging from 9.75cm (at depth 11) to 0.3cm (at depth 16). We train and evaluate individual entropy models at each depth from 11 to 16, which we found gave the best results. Our models use Kans = 4 rounds of aggregation and k= 5 nearest neighbors for continuous convolution. Our method is implemented in PyTorch [46] and we use Horovod [47] to distribute training over 16 GPUs. We train our models over 150,000 steps using the Adam optimizer [48] with a learning rate of 1e−4 and a batch size of 16. 1We use the authors’ implementation:https://github.com/cwi-dis/cwi-pcl-codec . 2MPEG TMC13 reference implementation: https://github.com/MPEGGroup/mpeg-pcc-tmc13 75 10 15 20 25 30 Overall Bits Per Point (BPP) 15 20 25 30 35 40Mean IOU Bitrate vs. IOU (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 40 50 60 70 80 90Mean IOU Bitrate vs. IOU (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 83.00 83.25 83.50 83.75 84.00 84.25Vehicle AP@70% IOU Bitrate vs. Vehicle AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 76 77 78Pedestrian AP@50% IOU Bitrate vs. Pedestrian AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 67 68 69 70Motorbike AP@50% IOU Bitrate vs. Motorbike AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle Figure 4: Bitrate vs. downstream task performance. Top: mean IoU for semantic segmentation on SemanticKITTI (left) and UrbanCity (right). Bottom: AP for vehicle, pedestrian, and motorbike detection on UrbanCity. Oracle IOU: 37.5, Bitrate: 104.0  Ours IOU: 36.8, Bitrate: 13.1  Oracle AP: 88.5, Bitrate: 104.0  Ours AP: 88.5, Bitrate: 9.9 Figure 5: Left: Semantic segmentation on SemanticKITTI. Right: Object detection on UrbanCity. Even at very low bitrates, our reconstructions have a minimal realism gap for downstream tasks. Metrics: We report reconstruction metrics in terms of F1 score, point-to-point (D1) Chamfer distance [ 6], and point-to-plane (D2) PSNR [ 49]. Point-to-point and point-to-plane errors are standard MPEG metrics [25]. But whereas they measure reconstruction quality in terms of geometry only, F1 measures this in terms of both geometry and intensity. Reconstruction metrics are averaged across sweeps and bitrate is the average number of bits used to store a LiDAR point. Following standard practice, we do not count the one-time transmission of network weights since it is negligible compared to the size of long LiDAR streams; e.g. 1 hour. See our supplementary materials for details. 3.2 Results Quantitative Results: In Fig. 2, we report bitratevs. reconstruction quality curves for all competing methods on UrbanCity and SemanticKITTI. The leftmost ﬁgures show the trade-off between overall bitrate vs. F1. Here, we see that our method outperforms the prior state-of-the-art and, holding reconstruction quality equal, achieves a 7–17% (resp., 6–19%) relative reduction in bitrate versus OctSqueeze on UrbanCity (resp., SemanticKITTI). Our model also outperforms MPEG TMC13— the MPEG point cloud compression standard—especially at lower bitrates. The right two ﬁgures show the trade-off between spatial bitrate vs. Chamfer distance and PSNR respectively. Although our method shares a common octree data structure with OctSqueeze ( resp., Octree), and thus have the same reconstruction quality, we achieve a 5–30% ( resp., 15–45%) reduction in spatial bitrates on UrbanCity by additionally exploiting temporal information; similar results also hold in SemanticKITTI. These results validate our uniﬁed framework for geometry and intensity compression using spatial-temporal information. Qualitative Results: In Fig. 3, we show reconstructions from our method, Draco, and MPEG Anchor on UrbanCity and SemanticKITTI. At similar bitrates, our method yields higher quality reconstructions than the competing methods in terms of both geometry and intensity. For example, from the ﬁrst and third rows of Fig. 3, we can see that our method produces faithful reconstructions even at high compression rates. In contrast, Draco and MPEG Anchor produce apparent artifacts. 8Ablation Studies: We perform two ablation studies on our occupancy and intensity entropy models. In Tab. 1, we ablate how to incorporate past information to lower the entropy of our occupancy model. We start with using the past octree’s occupancy bytes (O) and then progressively add top-down and bottom-up aggregated features (T and B respectively), and ﬁnally continuous convolutions ( CC). We see that, holding reconstruction quality equal, each aspect of our model consistently reduces bitrates. In Tab. 2, we compare three compression methods for the intensity model: the zlib library, a multi-layer perceptron entropy model ( MLP), and our ﬁnal model ( CC). Note that both MLP and CC conditions on context from neighboring points in the past sweep; zlib does not. However, whereas MLP uses context from one neighbor only, CC aggregates context from multiple neighbors via continuous convolutions. Our results show that learning to incorporate past context reduces intensity bitrates by 4–5%, and that this improvement is strengthened to 11–12% by using continuous convolutions to align information across space and time. Please see our supplementary for details. Impact on Downstream Tasks:To study the impact of compression on downstream perception tasks, we ﬁrst train segmentation and detection models on uncompressed LiDAR for SemanticKITTI and UrbanCity. Note that these models use both LiDAR geometry and intensity as input (see supplementary for details). Next, we evaluate the models on LiDAR reconstructions obtained from various compression schemes and report their performance as a function of overall bitrate. For segmentation on SemanticKITTI and UrbanCity, we report mean IOU using voxelized ground truth labels at a 10cm resolution. For detection on UrbanCity, we report AP at 50% IOU for pedestrians and motorbikes and 70% IOU for vehicles. In Fig. 4 and 5, we see that our method’s reconstructions have the smallest realism gap for downstream tasks across all bitrates. This result is especially pronounced for segmentation models, which are more sensitive to ﬁne-grained geometric and intensity details. 4 Conclusion We have presented a novel LiDAR point cloud compression algorithm using a deep entropy model which exploits spatio-temporal redundancies between successive LiDAR point clouds. We showed that we can compress point clouds at identical reconstruction quality to the state-of-the-art while lowering bitrate signiﬁcantly, as well as compress LiDAR intensity values effectively which was not as extensively explored by prior works. Furthermore, we showed our compression can be applied to downstream self-driving perception tasks without hindering performance. Looking forward, we plan to extend our method to jointly compress data streams from entire sensor suites. Broader Impact On an immediate level, our contributions are directly applicable as a data compression algorithm in a novel problem setting: the greater we can maximize the performance of such an algorithm, the more we can reduce the storage cost and space required by point clouds. We hope that this in turn unlocks a milestone towards fulﬁlling our ultimate vision: scaling up the research and deployment of intelligent robots, such as self-driving vehicles, that will revolutionize the safety, efﬁciency, and convenience of our transportation infrastructure. By capturing the 3D geometry of the scene, LiDAR sensors have proven to be crucial in effective and safe prediction/planning of these robots. Currently, LiDAR sensors are not only expensive due to the upfront cost, but also due to the recurring costs of the massive quantities of data they generate. Good point cloud and LiDAR compression algorithms will thus help to democratize the usage of LiDAR by making it more feasible for people to own and operate. Perhaps just as importantly, our responsibility as researchers in a novel problem area led us to carefully consider the downstream impacts of such a compression algorithm—if the primary usage of LiDAR currently is on perception tasks, such as detection and segmentation, then we need to demonstrate how compression bitrate affects perception performance, helping the community determine the acceptable bitrate at which compression can be used for safe vision and robotics applications. We hope that our work inspires the community to further advance sensor compression in addition to the traditional image and video settings. References [1] Rufael Mekuria, Kees Blom, and Pablo Cesar. Design, implementation and evaluation of a point cloud codec for tele-immersive video. In IEEE IEEE Transactions on Circuits and Systems for 9Video Technology, 2016. [2] Google. Draco 3d data compresison. https://github.com/google/draco, 2017. [3] Chenxi Tu, E. Takeuchi, C. Miyajima, and K. Takeda. Compressing continuous point cloud data using image compression methods. In 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), 2016. [4] Chenxi Tu, Eijiro Takeuchi, Alexander Carballo, and Kazuya Takeda. Real-time streaming point cloud compression for 3d lidar sensor using u-net. IEEE Access, 2019. [5] C. Tu, E. Takeuchi, A. Carballo, and K. Takeda. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 International Conference on Robotics and Automation (ICRA), 2019. [6] Lila Huang, Shenlong Wang, Kelvin Wong, Jerry Liu, and Raquel Urtasun. Octsqueeze: Octree- structured entropy model for lidar compression. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2020. [7] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end perception and prediction with tracking in the loop, 2020. [8] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In IEEE International Conference on Computer Vision, ICCV), 2019. [9] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 1948. [10] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009. [11] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Generalized belief propagation. In Proceedings of the 13th International Conference on Neural Information Processing Systems, NIPS’00, page 668–674, Cambridge, MA, USA, 2000. MIT Press. [12] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3391–3401. Curran Associates, Inc., 2017. [13] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep parametric continuous convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2018. [14] G. Nigel Martin. * range encoding: an algorithm for removing redundancy from a digitised message. 1979. [15] O. Devillers and P. . Gandoin. Geometric compression for interactive transmission. In Proceed- ings Visualization 2000. VIS 2000 (Cat. No.00CH37145), 2000. [16] Ruwen Schnabel and Reinhard Klein. Octree-based point-cloud compression. In Proceedings of the 3rd Eurographics / IEEE VGTC Conference on Point-Based Graphics, SPBG’06, page 111–121, Goslar, DEU, 2006. Eurographics Association. [17] Yan Huang, Jingliang Peng, C.-.C. Jay Kuo, and M. Gopi. A generic scheme for progressive point cloud coding. IEEE Transactions on Visualization and Computer Graphics, 2008. [18] Cha Zhang, Dinei Florencio, and Charles Loop. Point cloud attribute compression with graph transform. IEEE - Institute of Electrical and Electronics Engineers, October 2014. [19] Dorina Thanou, Philip Chou, and Pascal Frossard. Graph-based motion estimation and compen- sation for dynamic 3d point cloud compressio,. pages 3235–3239, 09 2015. [20] Ricardo L. de Queiroz and Philip A. Chou. Compression of 3d point clouds using a region- adaptive hierarchical transform. Trans. Img. Proc., 25(8):3947–3956, August 2016. [21] Diogo C. Garcia and Ricardo L. de Queiroz. Context-based octree coding for point-cloud video. In 2017 IEEE International Conference on Image Processing (ICIP), 2017. [22] Yiting Shao, Zhaobin Zhang, Zhu Li, Kui Fan, and Ge Li. Attribute compression of 3d point clouds using laplacian sparsity optimized graph transform. 2017 IEEE Visual Communications and Image Processing (VCIP), pages 1–4, 2017. 10[23] Diogo C. Garcia and Ricardo L. de Queiroz. Intra-frame context-based octree coding for point- cloud geometry. In 2018 25th IEEE International Conference on Image Processing (ICIP), 2018. [24] Diogo C. Garcia, Tiago A. Fonseca, Renan U. Ferreira, and Ricardo L. de Queiroz. Geom- etry coding for dynamic voxelized point clouds using octrees and multiple contexts. IEEE Transactions on Image Processing, 2020. [25] S. Schwarz, M. Preda, V . Baroncini, M. Budagavi, P. Cesar, P. A. Chou, R. A. Cohen, M. Kri- voku´ca, S. Lasserre, Z. Li, J. Llach, K. Mammou, R. Mekuria, O. Nakagami, E. Siahaan, A. Tabatabai, A. M. Tourapis, and V . Zakharchenko. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2019. [26] Maurice Quach, Giuseppe Valenzise, and Frederic Dufaux. Learning convolutional transforms for lossy point cloud geometry compression, 2019. [27] Jianqiang Wang, Hao Zhu, Z. Ma, Tong Chen, Haojie Liu, and Qiu Shen. Learned point cloud geometry compression. ArXiv, abs/1909.12037, 2019. [28] Wei Yan, Yiting Shao, Shan Liu, Thomas H. Li, Zhu Li, and Ge Li. Deep autoencoder-based lossy geometry compression for point clouds. CoRR, abs/1905.03691, 2019. [29] Tianxin Huang and Yong Liu. 3d point cloud geometry compression on deep learning. In Proceedings of the 27th ACM International Conference on Multimedia, MM ’19, page 890–898, New York, NY , USA, 2019. Association for Computing Machinery. [30] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] David Minnen, Johannes Ballé, and George Toderici. Joint autoregressive and hierarchical priors for learned image compression. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 10794–10803, 2018. [32] George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele Covell. Full resolution image compression with recurrent neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5435–5443. IEEE Computer Society, 2017. [33] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 4394–4402. IEEE Computer Society, 2018. [34] Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár. Lossy image compression with compressive autoencoders. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [35] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Practical full resolution learned lossless image compression. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10629–10638. Computer Vision Foundation / IEEE, 2019. [36] James Townsend, Thomas Bird, and David Barber. Practical lossless compression with latent variables using bits back coding. In International Conference on Learning Representations, 2019. [37] Chao-Yuan Wu, Nayan Singhal, and Philipp Krähenbühl. Video compression through image interpolation. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII, volume 11212 of Lecture Notes in Computer Science, pages 425–440. Springer, 2018. [38] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao. DVC: an end-to-end deep video compression framework. In IEEE Conference on Computer Vision and 11Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 11006–11015. Computer Vision Foundation / IEEE, 2019. [39] Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander G. Anderson, and Lubomir D. Bourdev. Learned video compression. In 2019 IEEE/CVF International Conference on Com- puter Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 3453– 3462. IEEE, 2019. [40] AmirHossein Habibian, Ties van Rozendaal, Jakub M. Tomczak, and Taco Cohen. Video compression with rate-distortion autoencoders. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 7032–7041. IEEE, 2019. [41] Abdelaziz Djelouah, Joaquim Campos, Simone Schaub-Meyer, and Christopher Schroers. Neural inter-frame compression for video coding. In The IEEE International Conference on Computer Vision (ICCV), October 2019. [42] Ren Yang, Fabian Mentzer, Luc Van Gool, and Radu Timofte. Learning for video compression with hierarchical quality and recurrent enhancement, 2020. [43] Jianping Lin, Dong Liu, Houqiang Li, and Feng Wu. M-lvc: Multiple frames prediction for learned video compression, 2020. [44] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. [45] Jean loup Gailly and Mark Adler. zlib. https://github.com/madler/zlib, 1995. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, 2019. [47] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799, 2018. [48] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [49] D. Tian, H. Ochimizu, C. Feng, R. Cohen, and A. Vetro. Geometric distortion metrics for point cloud compression. In 2017 IEEE International Conference on Image Processing (ICIP), 2017. [50] Igor Pavlov. Lzma. https://sourceforge.net/p/scoremanager/discussion/457976/ thread/c262da00/, 1998. [51] Julian Seward. bzip2. https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/ source/src/util/compress/bzip2/README, 1996. [52] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A modern library for 3D data processing. arXiv:1801.09847, 2018. [53] Chris Zhang, Wenjie Luo, and Raquel Urtasun. Efﬁcient convolutions for real-time semantic segmentation of 3d point clouds. In 2018 International Conference on 3D Vision, 3DV 2018, Verona, Italy, September 5-8, 2018, pages 399–408. IEEE Computer Society, 2018. [54] Yuxin Wu and Kaiming He. Group normalization. CoRR, abs/1803.08494, 2018. [55] Bin Yang, Wenjie Luo, and Raquel Urtasun. PIXOR: real-time 3d object detection from point clouds. CoRR, abs/1902.06326, 2019. 12A Additional Experiments A.1 Compression of Leaf Offsets We mention in Sec. 2.1 of the main paper that we do not attempt to compress the leaf offsets from the octree. The reason is that we experimented with a few compression baselines and were not able to obtain a bitrate improvement over the uncompressed leaf offsets. We experiment with the zlib [45], LZMA [50], and bzip2 [51] compression algorithms on the leaf offset stream from UrbanCity. The results are shown in Tab. 3; we surprisingly found that in all cases the compressed string was longer than the uncompressed one. Uncompressed zlib [45] LZMA [50] bzip2 [51] Avg. Bytes / Sweep 102429.31 102468.93 102493.84 103242.28 Table 3: Comparison of compression algorithms on leaf offsets from UrbanCity, in terms of average bytes per sweep. There can be room for future work in entropy modeling the leaf offsets, but our current hypothesis is that since the intermediate octree nodes already encode the shared bits between points, the leaf offsets represent residual bits that can be considered “higher-frequency” artifacts (similar to residual frames in video compression), and are therefore harder to compress. A.2 Using a Range Image Representation We mention in Sec. 3.1 of the main paper that we designed a range image-based compression baseline. Towards this goal, we ﬁrst converted point cloud streams in UrbanCity and KITTI into range image representations, which store LiDAR packet data into a 2D matrix. We consider two possible range image representations. The ﬁrst contains dimensions Hlid ×Wazm, where the height dimension represents the separate laser ID’sof the LiDAR sensor, and the width dimension represents the discretized azimuth bins between -180 ◦and 180◦. Each pixel value represents the distance returned by the laser ID at the speciﬁc azimuth angle. Such a representation requires sufﬁcient auxiliary calibration and vehicle information in order to reconstruct the points in Euclidean space— for instance, a separate transform matrix per laser and velocity information to compensate for rolling shutter effects. We use this representation for UrbanCity because we have access to most required information; unfortunately, not every log contains detailed calibration or precise velocity information, requiring us to use approximations. The second representation simply projects the spatial coordinates of the point cloud sweep into the coordinate frame of the sensor, and does not require a map between laser ID and Euclidean space. Such an image contains dimensions Hpitch ×Wazm, where the height dimension now represents discretized pitch angles; each pixel value now represents the distance of a given point from the sensor frame at a given pitch and azimuth bin. We use this representation for our KITTI point clouds, since the dataset does not provide detailed laser calibration information. We explore both geometry-only and geometry + intensity representations. Spatial positions are encoded in the 8-bit R,G channels of the png image (16 bits total). If intensity is encoded, it is encoded in the B channel. We run H.264 on the png image sequence as our compression algorithm. We evaluate on the same reconstruction metrics: point-to-point Chamfer distance and point-to-plane PSNR (geometry), and F1 score (geometry + intensity). We show here in Fig. 6, that the results were uncompetitive—the range image representation under- performs other baselines and our approach on every evaluation metric. We observe that even the “lossless” representation (the right-most point on the curves) does not yield perfect reconstruction metrics. This can be surprising for the laser ID representation in UrbanCity. But we hypothesize that the errors come from approximations of the true calibration values (which are not obtainable for every log), as well as the velocity used in rolling shutter compensation—we found that small perturbations in these calibration values yield a large variance in reconstruction quality and metrics. 130 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 6: Bitrate vs. reconstruction quality curves on UrbanCity (top) and KITTI (bottom). From left-to-right: F1 with τgeo = 10cmand τint = 0(↑), point-to-point chamfer distance (↓), point-to-plane PSNR (↑). B Additional Architecture Details In this section we provide additional architecture details of our octree occupancy and intensity entropy models (Secs. 2.3 and 2.4 in main paper). We also provide architecture details of the models used in the ablation studies of the occupancy and intensity model (Tab. 1, Tab. 2 in main paper). B.1 Occupancy Entropy Model Ancestral Node Dependence: The context feature ci consists of the octree level of the current node (1– 16), spatial location of the node’s octant(x,y,z ), octant index of the node relative to its parent (0–8), and parent occupancy byte (0–255), as well as occupancy byte in the corresponding node in the previous octree (0–255 if exists, 0 otherwise). The initial feature extractor is a 4-layer MLP with fully-connected (fc) layers and intermediate ReLU activations. The hidden layer dimension is 128. Then, every aggregation round consists of a 2-layer fc/ReLU MLP with a 256-dimensional input (concatenating with the ancestor feature), and a hidden dimension of 128. We set the number of aggregation rounds, Kans, to 4. Temporal Octree Dependence: The top-down pass to generate h(t−1) j has essentially the same architecture as the ancestral node dependence module above. The one difference is that each context feature additionally includes the “ground-truth” occupancy byte of each node, since each node in sweep t−1 has already been decoded. Moreover, each hidden dimension is 64 instead of 128. Next, recall that the bottom-up aggregation pass has the following formulation: g(t−1) j = fagg,1(h(t−1) j + ∑ c∈child(j) fagg,2(g(t−1) c )) Here, fagg,2 is a 2-layer fc/ReLU MLP taking a 64-dim input and outputting a 32-dim intermediate embedding. fagg,1 is a 2-layer fc/ReLU MLP taking a (32 + 64)-dim embedding (child embedding + top-down embedding), and outputting a 64-dim embedding for the current node j. The bottom-up pass is run starting from the lowest level D(where there are no children) back up to level 0. Spatio-Temporal Aggregation and Entropy Header: Recall that the continuous convolution layer has the formulation hi = ∑ j∈N(i) σ(pj −pi)hj where N(i) is the i-th node’sk-nearest neighbors in sweep t−1, at the same octree level as node i, and pi is the 3D position of each node. Here, σis a learned kernel function, and it is parameterized by 14an MLP, inspired by [13]. The MLP contains 3 fc/ReLU layers (no ReLU in last layer), with output dimensions 16, 32, and 64 respectively. The continuous conv layer produces the warped feature g(t) i,st. The warped feature g(t) i,st and ancestral feature h(t) i are aggregated through a ﬁnal, 4-layer fc/ReLU MLP with hidden dim 128. The prediction header outputs a softmaxed, 256-dim vector of occupancy predictions. B.2 Intensity Entropy Model The input to the intensity entropy MLP consists of the k-nearest neighbor intensities in sweep t−1: {r(t−1) j }j∈N(i). We set k = 5. In addition to the raw intensity value, we include the following features per r(t−1) j : spatial (x,y,z ) position ∈R3, delta vector to current point ∈R3, and 1-D distance value. Hence each point contains an 8-dimensional feature. Each feature per r(t−1) j is then independently given to a 4-layer MLP, consisting of fc layers and ReLU activations. The dimension of each hidden layer is 128. Then, the koutput features are input to a continuous convolution layer to produce a single 128-dimensional embedding. The kernel function σof the continuous conv. is parameterized with the same MLP as the one used in spatio-temporal aggregation in the occupancy model. The ﬁnal predictor is a fc layer and softmax with a 256-dim. output. B.3 Ablation Study Architectures We ﬁrst describe the architectures of the occupancy ablation in Tab. 1 of the main paper. • O uses the past occupancy byte to model temporal dependence. The byte is taken from the corresponding node in the previous octree if it exists; if it does not, the feature is zeroed out. This past occupancy byte is then appended to the context feature ci (along with parent occupancy byte, octree level, etc.) and fed to the ancestral dependence module. There is no temporal octree dependence module or spatio-temporal aggregation; the ﬁnal prediction header is directly attached to the ancestral feature. • O,T includes the temporal octree dependence module, but removes the bottom-up pass. Hence the ﬁnal feature produced from this module is h(t−1) j (as opposed to g(t−1) j ). There does not exist a spatio-temporal aggregation module using continuous convolutions to produce an embedding for every nodei. Instead, we use a simpler “exact matching” heuristic similar to including the occupancy bytes— h(t−1) j will only be included as a feature for node iin sweep t, if node jcorresponds to the same octant in sweep (t−1) as node iin sweep t. If there is no exact correspondence, the feature is zeroed out. • O,T,B includes the full temporal octree dependence module, including the bottom-up pass to produce g(t−1) j . As with the above, we do not include our spatio-temporal aggregation module but rather use the exact matching heuristic to include g(t−1) j in the corresponding nodes iin sweep tonly if the correspondence exists. • O,T,B,CC includes our full model, including using spatio-temporal aggregation with continuous convolutions to produce an embedding feature for every node i. We now describe the architectures of the intensity ablation in Tab. 2 of the main paper. • MLP only utilizes context from one neighbor in sweep t−1. First, the nearest neighbor to node iis obtained in sweep t−1. We take the neighbor’s corresponding intensity, the delta vector to the current position ∈R3, and 1-D distance value as inputs, and feed it through a 4-layer fc/ReLU MLP and a ﬁnal softmax predictor head to output 256-dim probabilities. • CC contains the full intensity model with continuous convolutions. For architecture details see B.2. 150 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 0) Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 5) Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 5)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 5)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 0) Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 5) Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 5)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 5)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 7: Bitrate vs. F1 curves on UrbanCity (top three rows) and KITTI (bottom three rows). We report F1 across various spatial and intensity thresholds: τgeo ∈{5cm,10cm,15cm}and τint ∈{0,5,10}. 16C Additional Experiment Details C.1 Reconstruction Metrics In Sec. 3.3 of the main text, we report reconstruction quality in terms of three metrics: F1 score, point-to-point Chamfer Distance [6], and point-to-plane PSNR [49]. In the following, we explain each metric in detail. Let P= {(pi,ri)}N i=1 be an input LiDAR point cloud, where each pi ∈R3 denotes a point’s spatial coordinates andri ∈{0,..., 255}its intensity. Furthermore, let ˆP= {(ˆpj,ˆrj)}M j=1 be its reconstruction, where ˆpj and ˆrj are similarly deﬁned. Our ﬁrst metric is an F1 score that measures reconstruction quality in terms of both geometry and intensity: F1(P, ˆP) = 2 ×# true positives 2 ×# true positives + # false positives + # false negatives (6) where a reconstructed point (ˆpj,ˆrj) ∈ ˆPis a true positive if and only if there exists a point (pi,ri) ∈P such that ∥pi −ˆpj∥2 ≤τgeo and |ri −ˆrj|≤ τint. False positivesare the reconstructed points in ˆPthat are not true positives, and false negativesare the original points in Pfor which no reconstructed point is a true positive. In our experiments, we use τgeo = 10cm and τint = 0, and we report F1 as a function of overall bitrates; i.e., the number of bits to store p and r. We further report the F1 score for τgeo ∈{5cm,10cm,15cm}and τint ∈{0,5,10}in Fig. 7. Following the MPEG standards, we also use two standard metrics that measure reconstruction quality in terms of geometry only [ 25]. We report these metrics as a function of spatial bitrates; i.e., the number of bits to store p. The ﬁrst such metric measures the point-to-point error between the original point cloud Pand the reconstructed point cloud ˆP; this metric is often called the D1 error in the MPEG standards. In our paper, we report this metric as a symmetric Chamfer distance: CDsym(P, ˆP) = max { CD(P, ˆP),CD( ˆP,P) } (7) where CD(P, ˆP) = 1 |P| ∑ pi∈P min ˆpj∈ˆP ∥pi −ˆpj∥2 (8) The second metric measures the point-to-place error between the original point cloud Pand the reconstructed point cloud ˆP; this metric is often called the D2 error in the MPEG standards. In our paper, we report this metric in terms of its peak signal-to-noise ratio (PSNR): PSNR(P, ˆP) = 10 log10 3r2 max{MSE(P, ˆP),MSE(P, ˆP)} (9) where MSE(P, ˆP) = 1 |P| ∑ i((pi −ˆpi) ·ˆni)2 is the mean squared point-to-plane distance, ˆni is the normal vector on ˆpi, ˆpi = argminˆp∈ˆP∥pi −ˆp∥2 2 is pi’s nearest neighbor point in ˆP, and r is the peak constant value. We estimate the normal ni at each point pi ∈P using the Open3D function estimate_normals with k= 12 nearest neighbors [52], and we compute the normal ˆni corresponding to each point ˆpi ∈ ˆPby taking the normal of its nearest neighbor in the original point cloud P. Following the MPEG standard, for each dataset, we compute ras the maximum nearest neighbor distance among all point clouds in the dataset: r= max P max pi∈P min j̸=i ∥pi −pj∥2 (10) For UrbanCity, we use r= 98.69 and for SemanticKITTI, we use r= 59.70. For completeness, we also report the point-to-point error in terms of its peak signal-to-noise ratio and the point-to-plane error as a symmetric Chamfer distance in Fig. 8. C.2 Downstream Experiment Details In this section, we provide additional details for our downstream perception experiments. 170 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 40 60 80 100Point-to-Point PSNR Bitrate v. D1 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Plane Chamfer Distance Bitrate v. D2 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 40 60 80 100Point-to-Point PSNR Bitrate v. D1 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.1 0.2 0.3Point-to-Plane Chamfer Distance Bitrate v. D2 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 8: Bitrate vs. reconstruction curves on UrbanCity (top two rows) and KITTI (bottom two rows). We report point-to-point (D1) and point-to-plane (D2) errors in terms of Chamfer distances (left) and PSNR (right). C.2.1 Semantic Segmentation We use a modiﬁed version of the LiDAR semantic segmentation model described in [6]. Input Representation: Our model takes as input T bird’s eye view (“BEV”) occupancy grids of the past T input LiDAR point clouds {P(t−T+1),..., P(t)}, stacked along the height dimension (i.e., the z-axis). By treating the height dimension as multi-dimensional input features, we have a compact input representation on which we can use 2D convolutions [ 53]. Each voxel in the occupancy grids store the average intensity value of the points occupying its volume, or 0 if it contains no points. We use a region of interest of 160m ×160m ×5m centered on the ego-vehicle, T = 5 past LiDAR point clouds, and a voxel resolution of 0.15625cm, yielding an input volume x of size (T ×Z) ×W ×H = 160 ×1024 ×1024. Architecture Details: Our model architecture consists of two components: (1) a backbone feature extractor; and (2) a semantic segmentation head. The backbone feature extractor CNNBEV is a feature pyramid network based on the backbone architecture of [7]: fBEV = CNNBEV(x) (11) where fBEV ∈RCBEV×W/4×H/4 and CBEV = 256. 18The semantic segmentation head CNNsem consists of four 2D convolution blocks with 128 hidden channels 3, followed by a 1 ×1 convolution layer: fsem = CNNsem(fBEV) (12) where fsem ∈R(K×Z)×W/4×H/4 and Kis the number of classes plus an additional ignore class. To extract per-point predictions, we ﬁrst reshape fsem into a K×Z×W/4 ×H/4 logits tensor, then use trilinear interpolation to extract per-point K-dimensional logits, and ﬁnally apply softmax. Training Details: We use the cross-entropy loss to train our semantic segmentation model. For SemanticKITTI, we follow [8] and reweight the loss at each point by the inverse of the frequency of its ground truth class; this helps to counteract the effects of severe class imbalance. Moreover, we use data augmentation by randomly scaling the point cloud by s∼Uniform(0.95,1.05), rotating it by θ∼Uniform(−π/4,π/4), and reﬂecting it along the xand y-axes. We use the Adam optimizer [48] with a learning rate of 4e−4 and a batch size of 12, and we train until convergence. C.2.2 Object Detection We use a modiﬁed version of the LiDAR object detection model described in [6]. It largely follows the same architecture as our semantic segmentation model, with a few modiﬁcations to adapt it for object detection. We describe these modiﬁcations below. Architecture Details: Our object detection model consists of two components: (1) a backbone feature extractor; and (2) an object detection head. The backbone feature extractor here shares an identical architecture to that of the semantic segmentation model. The object detection head consists of four 2D convolution blocks with 128 hidden channels followed by a 1 ×1 convolution layer to predict a bounding box bi,k and detection score αi,k for every BEV pixel iand class k. Each bounding box bi,k is parameterized by (∆x,∆y,log w,log h,sin θ,cos θ), where (∆x,∆y) are the position offsets to the object’s center,(w,h) are the width and height of its bounding box, and θis its heading angle. To remove duplicate bounding boxes predictions, we use non-maximum suppression. Training Details: We use a combination of classiﬁcation and regression losses to train our de- tection model. In particular, for object classiﬁcation, we use a binary cross-entropy loss with online hard negative mining, where positive and negative BEV pixels are determined based on their distance to an object center [ 55]. For bounding box regression, we use a smooth ℓ1 loss on ∆x,∆y,log w,log h,sin θ,cos θ. We use the Adam optimizer [48] with a learning rate of 4e−4 and a batch size of 12, and we train until convergence. D Additional Qualitative Results In Fig. 9 and 10, we compare the reconstruction quality of our method versus Draco [2] and MPEG anchor [1]. Then, in Figs. 11, 12, and 13, we visualize results from semantic segmentation and object detection on SemanticKITTI and UrbanCity. As shown in these ﬁgures, our compression algorithm yields the best reconstruction quality at comparable or lower bitrates than the competing methods. E Change Log ArXiv v2: We updated our reconstruction metrics to use the standard MPEG deﬁnitions [ 25]. Furthermore, we added bitrate vs. F1 curves for a number of spatial and intensity thresholds. We also updated our Draco baseline to use its built-in attributes coder. 3Each 2D convolution block consists of a 3 ×3 convolution, GroupNorm [54], and ReLU. 19Oracle (UrbanCity): Bitrate 104.0  Ours: F1 92.9 Bitrate 10.0 Draco: F1 85.1 Bitrate 10.9  MPEG: F1 53.4 Bitrate 10.4 Figure 9: Qualitative results on UrbanCity. Points are colored by intensity. Oracle (KITTI): Bitrate 104.0  Ours: F1 90.8 Bitrate 5.6 Draco: F1 89.2 Bitrate 5.8  MPEG: F1 69.2 Bitrate 11.0 Figure 10: Qualitative results on SemanticKITTI. Points are colored by intensity. 20Oracle (KITTI): IOU 31.3 Bitrate 104.0  Ours: IOU 29.5 Bitrate 6.7 Draco: IOU 29.0 Bitrate 8.4  MPEG: IOU 26.3 Bitrate 13.0 Figure 11: Semantic segmentation results on SemanticKITTI. IOU is averaged over all classes. Oracle (KITTI): IOU 97.2 Bitrate 104.0  Ours: IOU 94.3 Bitrate 19.8 Draco: IOU 88.3 Bitrate 20.1  MPEG: IOU 85.2 Bitrate 20.5 Figure 12: Semantic segmentation results on UrbanCity. IOU is averaged over all classes. 21Oracle (KITTI): AP 90.6 Bitrate 104.0  Ours: AP 90.4 Bitrate 14.5 Draco: AP 89.4 Bitrate 16.6  MPEG: AP 91.2 Bitrate 17.6 Figure 13: Object detection results on UrbanCity. AP is averaged over the vehicle, pedestrian, and motorbike classes. 22",
      "meta_data": {
        "arxiv_id": "2011.07590v2",
        "authors": [
          "Sourav Biswas",
          "Jerry Liu",
          "Kelvin Wong",
          "Shenlong Wang",
          "Raquel Urtasun"
        ],
        "published_date": "2020-11-15T17:41:14Z",
        "pdf_url": "https://arxiv.org/pdf/2011.07590v2.pdf",
        "github_url": "https://github.com/MPEGGroup/mpeg-pcc-tmc13"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper presents MuSCLE (Multi Sweep Compression of LiDAR using Deep Entropy Models), a novel, learning-based compression algorithm for LiDAR sensor data streams. It addresses the problem of high storage costs associated with massive LiDAR data by exploiting spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values. A key contribution is a novel deep conditional entropy model that unifies geometry and attribute compression, modeling probabilities of octree symbols by considering coarse-level geometry and information from previous sweeps. The method significantly reduces joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods (7–17% on UrbanCity and 6–19% on SemanticKITTI) and shows superior performance on downstream perception tasks.",
        "methodology": "The proposed method performs lossy compression of LiDAR point cloud streams by leveraging spatio-temporal redundancies through a learned entropy model. It jointly compresses spatial 3D locations (x,y,z) and intensity values (r). First, point spatial coordinates are quantized and encoded into an octree representation, where intermediate nodes use 8-bit occupancy symbols and leaf nodes store offsets and intensity. The octree sequence is then fed into an entropy model based on a 1st-order Markov assumption, factorizing the joint probability into an Occupancy Entropy Model and an Intensity Entropy Model. The Occupancy Entropy Model factorizes probabilities based on ancestral nodes of the current sweep and all nodes of the previous sweep. It uses an Ancestral Node Dependence module (a recurrent network over a top-down octree path with MLPs) and a Temporal Octree Dependence module. The latter aligns the previous octree, constructing features using a two-stream feature backbone (top-down and bottom-up aggregation via a deep sets-inspired function). A Spatio-Temporal Aggregation step uses continuous convolutions to process previous octree features based on spatial proximity. The Intensity Entropy Model compresses 8-bit integer intensities, conditioned on current sweep occupancies. It leverages temporal correlations across consecutive timestamps by employing continuous convolutions to aggregate information from nearest neighbor intensities in the previous sweep. Finally, the predicted probability distributions from both entropy models are fed into a lossless entropy coding algorithm (range coding) to produce the final bitstream. Both models are trained end-to-end with cross-entropy loss.",
        "experimental_setup": "The method was evaluated on two large-scale datasets: UrbanCity (5000 training, 500 test sequences, each 250 Velodyne HDL-64E LiDAR sweeps at 10Hz, ~80,156 points, 3D + 8-bit intensity, semantic labels) and SemanticKITTI (sequences 00-10 for training, 11-21 for reconstruction evaluation, 08 for downstream tasks; 43,552 Velodyne HDL-64E sweeps at 10Hz, ~120,402 points, dense semantic labels). Baselines included OctSqueeze (prior state-of-the-art, augmented with zlib for intensity), Google’s Draco (KD-tree based, built-in attributes coder), MPEG Anchor (octree-based, PCL color coder), MPEG TMC13 (reference implementation, built-in attributes coders), and a custom Octree baseline. Implementation details involve constructing octrees over a 400m × 400m × 400m region, with octree depths varied from 11 to 16 to control bitrate-distortion tradeoffs. The models use Kans=4 aggregation rounds and k=5 nearest neighbors for continuous convolution. Training was done in PyTorch, distributed with Horovod on 16 GPUs, for 150,000 steps using the Adam optimizer (1e-4 learning rate, batch size 16). Reconstruction quality was measured by F1 score (geometry + intensity, with τgeo = 10cm, τint = 0), point-to-point Chamfer Distance (D1), and point-to-plane PSNR (D2). Downstream task performance was evaluated using semantic segmentation (mean IoU with 10cm voxel resolution) and object detection (AP at 50% IOU for pedestrians/motorbikes, 70% IOU for vehicles) on reconstructions.",
        "limitations": "The intensity entropy model assumes that intensity values are bounded and discrete; if intensities are continuous, loss is incurred through discretization. Leaf offsets from the octree were not effectively compressed, as experimented with zlib, LZMA, and bzip2, suggesting that these residual bits are 'higher-frequency artifacts' and harder to compress with current methods. Prior approaches like the MPEG anchor are limited to scenes with restricted motion due to their block-based rigid transformations. The current model makes a 1st-order Markov assumption, where a given octree depends only on the immediately preceding sweep. An alternative range image-based compression baseline was found to be uncompetitive, with reconstruction errors potentially stemming from approximations of true calibration values and velocity in rolling shutter compensation.",
        "future_research_directions": "The authors plan to extend the method to jointly compress data streams from entire sensor suites, moving beyond just LiDAR. They also suggest that there is potential for future work in entropy modeling the leaf offsets, despite current difficulties. More broadly, the work aims to inspire the community to further advance sensor compression techniques beyond the traditional image and video settings."
      }
    },
    {
      "title": "Efficient Hierarchical Entropy Model for Learned Point Cloud Compression"
    },
    {
      "title": "ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression"
    },
    {
      "title": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis",
      "abstract": "Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian\nsplat representation has been introduced for novel view synthesis from sparse\nimage sets. Making such representations suitable for applications like network\nstreaming and rendering on low-power devices requires significantly reduced\nmemory consumption as well as improved rendering efficiency. We propose a\ncompressed 3D Gaussian splat representation that utilizes sensitivity-aware\nvector clustering with quantization-aware training to compress directional\ncolors and Gaussian parameters. The learned codebooks have low bitrates and\nachieve a compression rate of up to $31\\times$ on real-world scenes with only\nminimal degradation of visual quality. We demonstrate that the compressed splat\nrepresentation can be efficiently rendered with hardware rasterization on\nlightweight GPUs at up to $4\\times$ higher framerates than reported via an\noptimized GPU compute pipeline. Extensive experiments across multiple datasets\ndemonstrate the robustness and rendering speed of the proposed approach.",
      "full_text": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis Simon Niedermayr simon.niedermayr@tum.de Josef Stumpfegger ga87tux@mytum.de R¨udiger Westermann westermann@tum.de Technical University of Munich Abstract Recently, high-fidelity scene reconstruction with an op- timized 3D Gaussian splat representation has been intro- duced for novel view synthesis from sparse image sets. Mak- ing such representations suitable for applications like net- work streaming and rendering on low-power devices re- quires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity- aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to 31× on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to 4× higher framerates than reported via an opti- mized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach. 1. Introduction Novel view synthesis aims to generate new views of a 3D scene or object by interpolating from a sparse set of images with known camera parameters. NeRF [16] and its variants have proposed the use of direct volume rendering to learn a volumetric radiance field from which novel views can be rendered. However, expensive neural network evaluations prohibit efficient training and rendering. Recent research utilizes explicit scene representations such as voxel-based [24] or point-based structures [29] to enhance rendering ef- ficiency. The use of 3D voxel grids on the GPU in com- bination with a multiresolution hash encoding of the input [17] significantly reduces the operations needed and permits real-time performance. While achieving excellent reconstruction quality and speed, many NeRF-style approaches require exhaustive 93 FPS54 FPS 321 FPS211 FPS 25.2 PSNR /1.5 GB 25.0 PSNR /47 MB 31× Compression Figure 1. Our method achieves a31× compression at indiscernible loss in image quality and greatly improves rendering speed com- pared to [13]. Framerates in grey and white, respectively, are taken on NVIDIA’s RTX 3070M and RTX A5000 at 1080p resolution. memory resources. This affects both the training and ren- dering times and often prohibits the use of such represen- tations in applications like network streaming and mobile rendering. To overcome these limitations, dedicated com- pression schemes for the learned parametrizations on reg- ular grids have been proposed, including vector quantized feature encoding [15], learned tensor decomposition [3] or frequency domain transformations[20, 32]. Recently, differentiable 3D Gaussian splatting [13] has been introduced to generate a sparse adaptive scene repre- sentation that can be rendered at high speed on the GPU.The scene is modeled as a set of 3D Gaussians with shape and appearance parameters, which are optimized via differen- tiable rendering to match a set of recorded images. The optimized scenes usually consist of millions of Gaussians and require up to several gigabytes of storage and mem- ory. This makes rendering difficult or even impossible on low-end devices with limited video memory, such as hand- helds or head-mounted displays. Gaussians are rendered us- ing a specialized compute pipeline, which shows real-time performance on high-end GPUs. This pipeline, however, cannot be seamlessly integrated into VR/AR environments 1 arXiv:2401.02436v2  [cs.CV]  22 Jan 2024or games to work in tandem with hardware rasterization of polygon models. We address the storage and rendering issue of 3D Gaus- sian splatting by compressing the reconstructed Scene pa- rameters and rendering the compressed representation via GPU rasterization. To compress the scenes, we first analyze its components and observe that the SH coefficients and the multivariate Gaussian parameters take up the majority of storage space and are highly redundant. Inspired by previ- ous work on volumetric radiance field compression[15, 25] and deep network weight quantization, we derive a com- pression scheme that reduces the storage requirements of typical scenes by up to a factor of 31×. Our compression scheme consists of three main steps: • Sensitivity-aware clustering: We derive a sensitivity mea- sure for each scene parameter by calculating its contribu- tion to the training images. Color information and Gaus- sian parameters are encoded into compact codebooks via sensitivity-aware vector quantization. • Quantization-aware fine-tuning: To regain information that is lost during clustering we fine-tune the scene pa- rameters at reduced bit-rates using quantization-aware training. • Entropy encoding: 3D Gaussians are linearized along a space-filling curve to exploit the spatial coherence of scene parameters with entropy and run-length encoding. Further, we propose a renderer for the compressed scenes using GPU sorting and rasterization. It enables novel view synthesis in real-time, even on low-end devices, and can be easily integrated into applications rendering polygonal scene representations. Due to the reduced memory band- width requirements of the compressed representation and the use of hardware rasterization, a significant speed-up is achieved over the compute pipeline by Kerbl et al. [13]. We show the state-of-the-art quality of novel view ren- dering on benchmark datasets at significantly reduced mem- ory consumption and greatly improved rendering perfor- mance (Fig. 1). The compressed scenes can be used in applications requiring network streaming, and they can be rendered on low-end devices with limited video memory and bandwidth capacities. We perform a number of exper- iments on benchmark datasets to empirically validate our method across different scenarios. The contribution of each individual step is demonstrated with an ablation study. 2. Related Work Our work builds upon previous works in novel view synthe- sis via differentiable rendering and scene compression. Novel View SynthesisNeural Radiance Fields (NeRF) [16] use neural networks to model a 3D scene. They rep- resent the scene as a density field with direction-dependent colors that are rendered with volume rendering. The field is reconstructed from a set of images with known camera pa- rameters using gradient-based optimization of the volume rendering process. To speed up training and rendering efficiency, a number of different scene models have been proposed. Most often, structured space discretizations like voxel grids [10, 24, 28], octrees [6] or hash grids [17] are used to represent the scene. To avoid encoding empty space, point-based representa- tions have been proposed. Xu et al. [29] perform nearest neighbor search in a point cloud to aggregate local features, and R¨uckert et al. [21] render a point cloud with deep fea- tures and use deferred neural rendering to generate the final image. More recently, differentiable splatting [7, 13] has been positioned as a powerful alternative to NeRF-like ap- proaches for novel view synthesis. In particular, 3D Gaus- sian Splatting [13] offers state-of-the-art scene reconstruc- tion, by using a scene model consisting of an optimized set of 3D Gaussian kernels that can be rendered efficiently. Dif- ferentiable rendering on a set of training images is used to adaptively refine an initial set of Gaussian kernels and opti- mize their parameters. NeRF Compression While grid-based NeRF variants achieve high rendering performance due to GPU ray- marching, in particular, the use of full spatial grids intro- duces considerable storage costs. Tensor decomposition [3, 26], frequency domain transformation [20, 32] and voxel pruning [4] have been proposed to reduce the memory con- sumption of grid-based NeRFs. Takikawa et al. [25] per- form vector quantization during training with a learnable index operation. Li et al. [15] compress grid-based radiance fields by up to a factor of 100× using post-training vector quantization. The use of a hash encoding on the GPU in combination with vector quantization of latent features re- duces the required memory and permits high rendering per- formance [17] A number of works have especially addressed memory reduction during inference, to make grid-based scene rep- resentations more suitable for low-end devices with limited video memory [19, 27]. To our knowledge, our approach is the first that aims at the compression of point-based ra- diance fields to enable high-quality novel view synthesis at interactive frame rates on such devices. Quantization-Aware TrainingRastegari et al. [18] sim- ulate weight quantization during training to reduce quanti- zation errors when using low-precision weights for infer- ence. The use of quantization-aware training has been ex- plored for neural scene representations [8] and voxel-based NeRFs [12], demonstrating effective weight quantization with negligible loss in rendering quality. To reduce the size and latency of neural networks, vari- ous approaches for weight quantization have been explored [8, 11, 12, 18]. These methods rely on the observation that 2 Parameter Sensitivity Calculation Quantization-Aware  Fine-Tuning Codebook Gaussians Codebook Color ... ... Sensitivity Aware Vector Clustering Storage 26.8 PSNR / 47 MB Compressed Scene Reconstructed Scene 27.2 PSNR / 1.4 GB Morton Order Sorting Entropy & Run Length Encoding Figure 2. Proposed compression pipeline. Input is an optimized 3D Gaussian scene representation. First, a sensitivity measure is computed for the Gaussian parameters, and color and shape information is compressed into separate codebooks using sensitivity-aware and scale- invariant vector clustering. Next, the compressed scene is fine-tuned on the training images to recover lost information. Finally, the Gaussians are sorted in Morton order and further compressed using entropy and run-length encoding. The shown scene is from [2]. in most cases a lower weight precision is required for model inference than for training (e.g., 8-bit instead of 32-bit). In post-training quantization, the model weights are reduced to a lower bit representation after training. In quantization- aware training, the quantization is simulated during training while operations are performed at full precision to obtain numerically stable gradients. For storage and inference, the low precision weights can then be used with minor effects on the output. 3. Differentiable Gaussian Splatting Differentiable Gaussian splatting [13] builds upon EW A volume splatting [34] to efficiently compute the projections of 3D Gaussian kernels onto the 2D image plane. On top of that, differentiable rendering is used to optimize the num- ber and parameters of the Gaussian kernels that are used to model the scene. The final scene representation comprises a set of 3D Gaussians, each described by a covariance matrix Σ ∈ R3×3 centered at location x ∈ R3. The covariance matrix can be parameterized by a rotation matrix R and a scaling matrix S. For independent optimization of R and S, Kerbl et al . [13] represent the rotation with a quaternion q and scaling with a vector s, both of which can be converted into their respective matrices. In addition, each Gaussian has its own opacity α ∈ [0, 1] and a set of spherical harmonics (SH) coefficients to reconstruct a view-dependent color. The 2D projection of a 3D Gaussian is again a Gaussian with covariance Σ′ = JW ΣWT JT , (1) where W is the view transformation matrix and J is the Jacobian of the affine approximation of the projective trans- formation. This allows to evaluate the 2D color and opacity footprint of each projected Gaussian. A pixel’s color C is then computed by blending all N 2D Gaussians contribut- ing to this pixel in sorted order: C = X i∈N ciαi i−1Y j=1 (1 − αj). (2) Here, ci and αi, respectively, are the view-dependent color of a Gaussian and its opacity, modulated by the exponential falloff from the projected Gaussian’s center point. The position x, rotation q, scaling s, opacity α, and SH coefficients of each 3D Gaussian are optimized so that the rendered 2D Gaussians match the training images. For more details on the reconstruction process, we refer to the original paper by Kerbl et al. [13]. 4. Sensitivity-Aware Scene Compression We compress a set of optimized 3D Gaussian kernels as fol- lows: First, sensitivity-aware vector clustering is used to cluster the Gaussian appearance and shape parameters into compact codebooks (Sec. 4.1). Second, the clustered and other scene parameters are fine-tuned on the training im- ages to recover information lost due to clustering. We use quantization-aware training in this step to reduce the scene parameters to a lower bit-rate representation (Sec. 4.2). By linearizing the set of 3D Gaussians along a space-filling curve, entropy and run-length encoding can exploit the spa- tial coherence of Gaussian parameters to further compress the scene (Sec. 4.3). An overview of the proposed compres- sion pipeline is shown in Fig. 2. 4.1. Sensitivity-Aware Vector Clustering Inspired by volumetric NeRF compression [15, 25], we uti- lize vector clustering for compressing 3D Gaussian kernels. We use clustering to encode SH coefficients and Gaussian shape features (scale and rotation) into two separate code- books. As a result, each Gaussian can be compactly en- coded via two indices into the codebooks stored alongside. 3Parameter Sensitivity: The sensitivity of the recon- struction quality to changes of the Gaussian parameters is not consistent. While a slight change in one parameter of a Gaussian can cause a significant difference in the rendered image, a similar change in another parameter or the same parameter of another Gaussian can have low or no effect. We define the sensitivity S of image quality to changes in parameter p with respect to the training images as S(p) = 1PN i=1 Pi NX i=1 \f\f∂Ei ∂p \f\f. (3) N is the number of images in the training set used for scene reconstruction, and Pi is the number of pixels in imagei. E is the total image energy, i.e., the sum of the RGB com- ponents over all pixels. The sensitivity of E to changes in p is considered via the gradient of E with respect to p, i.e., a large gradient magnitude indicates high sensitivity to changes in the respective parameter. With this formulation, the sensitivity to every parameter can be computed with a single backward pass over each of the training images. Sensitivity-aware k-Means: Given a vector x ∈ RD, we define its sensitivity as the maximum over its compo- nent’s sensitivity: S(x) = max d∈[1..D] S(xd). (4) The sensitivity measure is then used for sensitivity-aware clustering, i.e., to compute codebooks C ∈ RK×D with K representatives ck ∈ RD (so-called centroids). We define the weighted distance between a vector x and a centroid ck as D(x, ck) = S(x)∥x − ck∥2 2. (5) A codebook is then obtained by using k-Means clustering with D as a similarity measure. The codebooks are initial- ized randomly with a uniform distribution within the min- imum and maximum values of each parameter. The cen- troids are computed with an iterative update strategy: In each step, the pairwise weighted distances between the vec- tors x and the codebook vectors ck are calculated, and each vector is assigned to the centroid to which it has the mini- mum distance. Each centroid is then updated as ck = 1P xi∈A(k) S(xi) X xi∈A(k) S(xi)xi (6) Where A(k) is the set of vectors assigned to centroid ck. For performance reasons, a batched clustering strategy is used [23]. In each update step, a random subset of vectors is picked and used to compute the update step. Then, the cen- troids are updated using the moving average with a decay factor λd. 0 max 0% 5% 100%percentage of Gaussians Garden 0 max sensitivity Truck 0 max Playroom Figure 3. Histograms of maximum sensitivity to changes of SH coefficients for different scenes. Only SH coefficients of a tiny fraction of all Gaussians strongly affect image quality. Color Compression: Each Gaussian stores SH coeffi- cients to represent the direction-dependent RGB color (e.g., 48 coefficients in [13]). We treat SH coefficients as vectors and compress them into a codebook using sensitivity-aware vector clustering. For volumetric NeRF models, Li et al. [15] have shown that only a small number of voxels contribute significantly to the training images. Thus, they propose to keep the color features that contribute the most and only compress the re- maining features with vector clustering. We observe a sim- ilar behavior for 3D Gaussians, as shown for some bench- mark scenes in Fig. 3. For a small percentage of all SH co- efficients (< 5%), the sensitivity measure indicates a high sensitivity towards the image quality. Thus, to keep the in- troduced rendering error low, we do not consider the SH vectors of Gaussians with a sensitivity higher than a thresh- old βc in the clustering process. These vectors are added to the codebook after clustering. Gaussian Shape Compression: A 3D Gaussian kernel can be parameterized with a rotation matrixR and a scaling vector s. We observe that for typical scenes, the shapes of the Gaussians are highly redundant up to a scaling factor. Thus, we re-parameterize the scaling vector s = ηˆs, where η = ∥s∥2 is the scalar scaling factor and ˆs = η−1s is the normalized scaling vector. With ˆS = diag(ˆs), the normal- ized covariance matrix is ˆΣ = (R ˆS)(R ˆS)T = 1 η2 Σ. (7) Clustering is then performed using the normalized co- variance matrices, and each Gaussian stores, in addition to a codebook index, the scalar scaling factor η. We com- pute the sensitivity to each of the matrix entries and perform sensitivity-aware vector quantization to compress them into a codebook. The sensitivity plots for Gaussian shape pa- rameters look mostly similar to the SH plots shown in Fig. 3. As for SH coefficients, Gaussians with a maximum sensitivity over a threshold βg are not considered for clus- tering and are added to the codebook. 4Note that k-Means clustering of normalized covariance matrices results in covariance matrices that are again nor- malized. However, due to floating point errors, clustering can lead to non-unit scaling vectors. To counteract this problem, we re-normalize each codebook vector after each update step by dividing it through the trace of the covariance metric. In the appendix, we prove both the normalization preserving properties of k-Means and re-normalization. After clustering, each codebook entry is decomposed into a rotation and scale parameter using an eigenvalue decomposition. This is required for quantization-aware training since direct optimization of the matrix is not possible[13]. In the final codebook, each matrix’s rotation and scaling parameters are encoded via 4 (quaternion) plus 3 (scaling) scalar values. 4.2. Quantization-Aware Fine-Tuning To regain information that is lost due to parameter quan- tization, the parameters can be fine-tuned on the training images after compression [15, 30]. To do so, we use the training setup described by Kerbl et al. [13]. We optimize for the position, opacity, and scaling factor of each Gaus- sian as well as the color and Gaussian codebook entries. For the two codebooks, the incoming gradients for each en- try are accumulated and then used to update the codebook parameters. For fine-tuning, we utilize quantization-aware training with Min-Max quantization (k-bit Quantization [18]) to rep- resent the scene parameters with fewer bits. In the for- ward pass, the quantization of a parameter p is simulated using a rounding operation considering the number of bits and the moving average of each parameter’s minimum and maximum values. The backward pass ignores the simulated quantization and calculates the gradient w.r.t. p as without quantization. After training, the parameters can be stored with only b-bit precision (e.g., 8-bit), while the minimum and maximum values required for re-scaling are stored at full precision (e.g., 32-bit float). Quantization of opacity is applied after the sigmoid ac- tivation function. Quantization of the scaling and rotation vector is applied before the respective normalization step. For the scale factor parameter, the quantization is applied before the activation (exponential function) to allow for a fine-grained representation of small Gaussians without los- ing the ability to model large ones. We quantize all Gaus- sian parameters despite position to an 8-bit representation with the Min-Max scheme. 16-bit float quantization is used for position, as a further reduction decreases the reconstruc- tion quality considerably. 4.3. Entropy Encoding After quantization-aware fine-tuning, the compressed scene representation consists of a set of Gaussians and the code- books storing SH coefficients and shape parameters. Indices into the codebooks are stored as 32-bit unsigned integers. The data is then compressed using DEFLATE [5], which utilizes a combination of the LZ77 [33] algorithm and Huff- man coding. In the reconstructed scenes, many features, such as color, scaling factor, and position, are spatially co- herent. By ordering the Gaussians according to their posi- tions along a Z-order curve in Morton order, the coherence can be exploited and the effectivity of run-length encoding (LZ77) can be improved. The effect on the compressed file size is analyzed in the ablation study in Sec. 6.4. Note that entropy encoding reduces the two codebook indices to their required bit-length according to the codebook sizes. 5. Novel View Rendering Kerbl et al. [13] propose a software rasterizer for differen- tiable rendering and novel view synthesis. To render 3D Gaussian scenes fast especially on low-power GPUs, our novel view renderer utilizes hardware rasterization. Preprocess: In a compute pre-pass, Gaussians whose 99% confidence interval does not intersect the view frustum after projection are discarded. For the remaining Gaussians, the direction-dependent color is computed with the SH co- efficients. The color, the Gaussian’s opacity, projected screen-space position, and covariance values are stored in an atomic linear-append buffer. The covariance values indi- cate the orientation and size of the 2D Gaussian into which a 3D Gaussian projects under the current viewing transfor- mation [34]. As in [13], Gaussians are then depth-sorted to enable order-dependent blending. We use the Onesweep sorting algorithm by Adinets and Merrill [1] to sort the Gaussians directly on the GPU. Due to its consistent per- formance, the implementation is well suited for embedding into real-time applications. Rendering: Gaussians are finally rendered in sorted or- der via GPU rasterization. For each Gaussian, one planar quad (a so-called splat) consisting of two triangles is ren- dered. A vertex shader computes the screen space vertex positions of each splat from the 2D covariance information. The size of a splat is set such that it covers the 99% confi- dence interval of the projected Gaussian. The vertex shader simply outputs the color computed in the pre-pass and the 2D splat center as input to the pixel shader. The pixel shader then discards fragments outside the 99% confidence inter- val. All remaining fragments use their distance to the splat center to compute the exponential color and opacity falloff and blend their final colors into the framebuffer. 6. Experiments 6.1. Datasets We evaluate our compression and rendering method on the Mip-Nerf360[2] indoor and outdoor scenes, two scenes 5Method 3D Gaussian Splatting Ours Dataset PSNR↑ SSIM↑ LPIPS↓ SIZE↓ PSNR↑ SSIM↑ LPIPS↓ SIZE↓ Compression Ratio↑ Synthetic-NeRF [16] 33.21 0 .969 0 .031 69 .89 32.936 0 .967 0 .033 3 .68 19.17 Mip-NeRF360 [2] 27.21 0 .815 0 .214 795 .26 26.981 0 .801 0 .238 28 .80 26.23 Tanks&Temples [14] 23.36 0 .841 0 .183 421 .90 23.324 0 .832 0 .194 17 .28 23.26 Deep Blending [9] 29.41 0 .903 0 .243 703 .77 29.381 0 .898 0 .253 25 .30 27.81 average* 26.58 0 .853 0 .213 640 .31 26.560 0 .844 0 .238 23 .73 25.77 Table 1. Quantitative comparison to 3D Gaussian Splatting. Size is measured in Megabytes. *Synthetic scenes are excluded. from the Tanks&Temples[14] and Deep Blending [9] dataset, and NeRF-Synthetic[16]. For Mip-Nerf360, Tanks&Temples and Deep Blending the reconstructions from Kerbl et al . [13] were used. We generated the 3D Gaussian representation for NeRF-Synthetic ourselves. 6.2. Implementation Details We use a decay factor λd = 0.8 for batched clustering with 800 update steps for the Gaussians and 100 for the SH co- efficients. A batch size of 218 is used for the color fea- tures, and 220 for the Gaussian parameters. We use 4096 as the default codebook size in all our experiments and set βc = 6 · 10−7 and βg = 3 · 10−6. We perform 5000 opti- mization steps of quantization-aware fine-tuning. The renderer is implemented with the WebGPU graph- ics API in the Rust programming language. Thus, it can run in a modern web browser on a large variety of devices. More details about the implementation can be found in the supplementary material. The source code is available at https://github.com/KeKsBoTer/c3dgs. 6.3. Results We use the scenes reconstructed by 3D Gaussian Splatting [13] and compress them using the proposed method. For all scenes, we evaluate the PSNR, SSIM, and LPIPS [31] before and after compression. Tab. 1 shows the results for different datasets. Our compression method achieves a compression ratio of up to 31× with an average of 26× at the indiscernible loss of quality (0.23 PSNR on average) for real-world scenes. Here, it should be noted that a difference of 0.5 PSNR is considered indistinguishable for the human eye [22]. For some of the scenes, Fig. 5 compares training images to the renderings of the uncompressed and compressed scenes. Fig. 4 shows close-up views of uncompressed and com- pressed synthetic scenes. More comparisons and results are given in the supplementary material. Image Quality LossFig. 5 shows that it is almost im- possible to spot the difference between the uncompressed and the compressed scenes. We also analyze the images from all test sets with the largest drop in PSRN. The image which could be reconstructed least accurately is shown in Fig. 6. We observe that the loss is mainly due to very subtle Baseline Compressed Baseline Compressed Figure 4. 3D Gaussian splatting of synthetic scenes [16]. Uncom- pressed (Baseline) vs. compressed scene. color shifts below what can be perceived by the human eye. Compression RuntimeThe compression process takes about 5-6 minutes and increases the reconstruction time by roughly 10%. The timings of each individual steps are given in the supplementary material. Rendering TimesWe see a significant increase of up to a factor of 4× in rendering speed (see Tab. 2). Roughly a 2x increase can be attributed to the compressed data’s reduced bandwidth requirements, hinting at the software rasterizer’s memory-bound performance by Kerbl et al. [13]. The ad- ditional speedup is achieved by the hardware rasterization- based renderer, which pays off on low- and high-end GPUs. Timings of the different rendering stages are given in the supplementary material. 6.4. Ablation Study In a number of experiments we evaluate the components of our compression pipeline. This includes a detailed analysis of the influence of the hyper-parameters. Loss Contribution Tab. 3 indicates that the most sig- 6Bicycle Train Playroom  Ground Truth  Baseline  Ours Figure 5. Ground truth images from the test set, results of Kerbl et al. [13] (Baseline), results using the compressed representation (Ours). a) Ground Truth  b) Baseline (35.90 PSNR)  c) Ours (34.15 PSNR)  d) Mean Absolute Error 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 Figure 6. Test image with the highest drop in PSNR in all scenes used in this work. d) Per pixel mean absolute error between Kerbl et al. [13] b) and our approach c). 7NVIDIA RTX A5000 NVIDIA RTX 3070M Intel UHD Graphics 11 AMD Radeon R9 380 Bicycle Kerbl et al. [13]93 54 - - Ours 215 134 9 41 Compressed 321 211 16 83 Bonsai Kerbl et al. [13]184 122 - - Ours 414 296 23 76 Compressed 502 380 28 128 Table 2. Rendering performance at 1080p resolution in frames per second, averaged over all training images. Bicycle consists of 6.1 million 3D Gaussians, Bonsai of 1.2 million 3D Gaussians. PSNR↑ SSIM↑ LPIPS↓ SIZE↓ baseline 27.179 0 .861 0 .115 1379.99 + Pruning 27.083 0 .856 0 .118 1217.25 + Color Clustering 25.941 0 .818 0 .178 278.41 + Gaussian Clustering25.781 0 .811 0 .186 164.15 + QA Finetune 26.746 0 .844 0 .144 86.69 + Encode 26.746 0 .844 0 .144 58.40 + Morton Order 26.746 0 .844 0 .144 46.57 Table 3. Losses introduced and regained by individual stages of the compression pipeline. Experiments were performed with the garden scene from Mip-Nerf360[2] nificant loss increase comes from the compression of the SH coefficients, which, on the other hand, gives the high- est memory reduction. Quantization of shape parameters can additionally reduce the memory by about 60%, only introducing a slight loss in image quality. Quantization- aware fine-tuning can regain much of the information that is lost due to quantization and further reduces the memory by about 50%. Entropy and run length encoding in combi- nation with Morton order layout saves an additional50% of the memory. Codebook Sizes SH coefficients and Gaussian shape parameters are compressed into codebooks of predefined sizes. Tab. 3 shows the effects of different codebook sizes on image quality. Errors were averaged over all test images, with the difference to the maximum error given in brack- ets. It can be seen that the codebook size has little effect on the average reconstruction error, independent of the scene. Nevertheless, larger codebooks reduce the maximum error with only minimal memory overhead. Sensitivity Thresholds The sensitivity thresholds βc and βg are used to decide whether to consider SH coeffi- cients and shape parameters for clustering. They offer a trade-off between quality and compression rate. The influ- ence of these values is analyzed in Tab. 5, showing in par- ticular the sensitivity of image quality to the quantization of SH coefficients. 6.5. Limitations As the main limitation for making the proposed compres- sion and rendering pipeline even more powerful, we see the current inability to aggressively compress the Gaus- PSNR↑ SSIM↑ LPIPS↓ SIZE↓ Color 1024 26.95(-0.67) 0.80(-0.02) 0.24(+0.03)28.47 2048 26.95(-0.62) 0.80(-0.02) 0.24(+0.03)28.65 4096 26.98(-0.63)0.80(-0.02)0.24(+0.03) 28.80 8192 27.00(-0.58) 0.80(-0.02) 0.24(-0.03)28.92 Gaussian 1024 26.95(-0.79) 0.80(-0.02) 0.24(+0.03)28.14 2048 26.97(-0.80) 0.80(-0.02) 0.24(+0.03)28.45 4096 26.98(-0.63)0.80(-0.02)0.24(+0.03) 28.80 8192 26.97(-0.60) 0.80(-0.02) 0.24(+0.03)29.06 Table 4. Average reconstruction error over the test images for different codebook sizes, including the maximum deviation from the baseline (+/−). Rows marked grey indicate the default con- figurations. Experiments were performed on the Mip-Nerf360[2] dataset. PSNR↑ SSIM↑ LPIPS↓ SIZE↓ baseline 26.976 0.801 0.238 28.80 βc 6.0·10−8 27.22(−0.25) 0.81(−0.00) 0.22(+0.00) 56.50 3.0·10−7 27.09(−0.41) 0.80(−0.01) 0.23(+0.02) 33.00 6.0·10−7 26.98(−0.63) 0.80(−0.02) 0.24(+0.03) 28.80 1.2·10−6 26.87(−0.75) 0.80(−0.02) 0.24(+0.04) 27.02 6.0·10−6 26.74(−0.94) 0.80(−0.02) 0.25(+0.04) 25.97 - 26.55(−1.47) 0.79(−0.03) 0.25(+0.05) 25.65 βg 3.0·10−7 27.05(−0.41) 0.80(−0.01) 0.23(+0.03) 33.90 1.5·10−6 27.00(−0.62) 0.80(−0.02) 0.24(+0.03) 29.95 3.0·10−6 26.98(−0.63) 0.80(−0.02) 0.24(+0.03) 28.80 6.0·10−6 26.91(−0.77) 0.80(−0.02) 0.24(+0.03) 28.08 3.0·10−5 26.86(−0.72) 0.80(−0.02) 0.24(+0.04) 27.30 - 26.80(−0.83) 0.80(−0.02) 0.25(+0.04) 27.10 Table 5. Sensitivity threshold ablation study. βc and βg are the sensitivity thresholds for controlling which SH vectors and shape parameters are clustered. The average error and in brackets the maximum deviation from the baseline are reported. The last row shows the results when no threshold is considered. The rows marked grey are the default configurations. Experiments were per- formed with Mip-Nerf360 [2] dataset. sians’ positions in 3D space. We performed experiments where positions were quantized to a lattice structure, and we even embedded these positional constraints into the Gaus- sian splatting training process. Unfortunately, we were not able to further compress the positions without introducing a significant error in the rendering process. 7. Conclusion We have introduced a novel compression and rendering pipeline for 3D Gaussians with color and shape parameters, achieving compression rates of up to 31 × and up to a 4 × increase in rendering speed. Our experiments with differ- ent datasets have shown that the compression introduces an indiscernible loss in image quality. The compressed data can be streamed over networks and rendered on low-power devices, making it suitable for mobile VR/AR applications and games. In the future, we aim to explore new approaches for reducing the memory footprint during the training phase, and additionally compressing positional information end- to-end. We also believe that 3D Gaussian splatting has the 8potential for reconstructing volumetric scenes, and we will investigate advanced options for compressing and rendering the optimized representations. References [1] Andy Adinets and Duane Merrill. Onesweep: A Faster Least Significant Digit Radix Sort for GPUs. arXiv preprint arXiv:2206.01784, 2022. 5 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5460–5469, New Orleans, LA, USA, 2022. IEEE. 3, 5, 6, 8, 11, 13, 15, 16 [3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In Computer Vision – ECCV 2022, pages 333–350, Cham, 2022. Springer Nature Switzerland. 1, 2 [4] Chenxi Lola Deng and Enzo Tartaglione. Compressing Explicit V oxel Grid Representations: fast NeRFs become also small. In 2023 IEEE/CVF Winter Conference on Ap- plications of Computer Vision (WACV) , pages 1236–1245, Waikoloa, HI, USA, 2023. IEEE. 2 [5] L. Peter Deutsch. DEFLATE Compressed Data Format Specification version 1.3, 1996. Issue: 1951 Num Pages: 17 Series: Request for Comments Published: RFC 1951. 5 [6] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenox- els: Radiance Fields without Neural Networks. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5491–5500, New Orleans, LA, USA, 2022. IEEE. 2 [7] Yiming Gao, Yan-Pei Cao, and Ying Shan. SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Re- construction of Indoor Scenes. In 2023 IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 108–118, Vancouver, BC, Canada, 2023. IEEE. 2 [8] Cameron Gordon, Shin-Fang Chng, Lachlan MacDonald, and Simon Lucey. On Quantizing Implicit Neural Rep- resentations. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pages 341–350, Waikoloa, HI, USA, 2023. IEEE. 2 [9] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep Blending for Free-viewpoint Image-based Rendering. ACM Transactions on Graphics (Proc. SIGGRAPH Asia) , 37(6):257:1–257:15, 2018. Publisher: ACM. 6, 11, 13, 14 [10] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural ra- diance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 5875–5884, 2021. 2 [11] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 2704–2713, 2018. 2 [12] Seungyeop Kang and Sungjoo Yoo. TernaryNeRF: Quantiz- ing V oxel Grid-based NeRF Models. In2022 IEEE Interna- tional Workshop on Rapid System Prototyping (RSP), pages 8–14, Shanghai, China, 2022. IEEE. 2 [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph., 42(4), 2023. Place: New York, NY , USA Publisher: Association for Com- puting Machinery. 1, 2, 3, 4, 5, 6, 7, 8, 12 [14] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG) , 36 (4):1–13, 2017. Publisher: ACM New York, NY , USA. 6, 11, 13, 14 [15] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing V olumetric Radiance Fields to 1 MB. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4222–4231, Vancouver, BC, Canada, 2023. IEEE. 1, 2, 3, 4, 5 [16] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. Communications of the ACM , 65(1):99–106, 2021. Publisher: ACM New York, NY , USA. 1, 2, 6, 11, 13, 17, 18 [17] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant neural graphics primitives with a mul- tiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1–15, 2022. Publisher: ACM New York, NY , USA. 1, 2 [18] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: ImageNet Classification Us- ing Binary Convolutional Neural Networks. In Computer Vision – ECCV 2016, pages 525–542, Cham, 2016. Springer International Publishing. 2, 5 [19] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini- vasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. ACM Trans- actions on Graphics (TOG) , 42(4):1–12, 2023. Publisher: ACM New York, NY , USA. 2 [20] Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, and Eunbyung Park. Masked Wavelet Representation for Compact Neural Radiance Fields. InPro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20680–20690, 2023. 1, 2 [21] Darius R ¨uckert, Linus Franke, and Marc Stamminger. ADOP: approximate differentiable one-pixel point render- ing. ACM Trans. Graph., 41(4):1–14, 2022. 2 [22] David Salomon and Giovanni Motta. Handbook of data com- pression. Springer Science & Business Media, 2010. 6 [23] D. Sculley. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web, pages 1177–1178, Raleigh North Carolina USA, 2010. ACM. 4 [24] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct V oxel Grid Optimization: Super-fast Convergence for Radiance 9Fields Reconstruction. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 5449–5459, New Orleans, LA, USA, 2022. IEEE. 1, 2 [25] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas M¨uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable Bitrate Neural Fields. In ACM SIGGRAPH 2022 Conference Proceedings, New York, NY , USA, 2022. Asso- ciation for Computing Machinery. event-place: Vancouver, BC, Canada. 2, 3 [26] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable NeRF via Rank-residual Decomposition. In Advances in Neural Information Process- ing Systems , pages 14798–14809. Curran Associates, Inc., 2022. 2 [27] Krishna Wadhwani and Tamaki Kojima. SqueezeNeRF: Fur- ther factorized FastNeRF for memory-efficient inference. In 2022 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition Workshops (CVPRW) , pages 2716–2724, New Orleans, LA, USA, 2022. IEEE. 2 [28] Sebastian Weiss and R ¨udiger Westermann. Differentiable Direct V olume Rendering. In IEEE Transactions on Visu- alization and Computer Graphics, pages 562–572, 2022. Is- sue: 1. 2 [29] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5438–5448, 2022. 1, 2 [30] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for Real-time Rendering of Neural Radiance Fields. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5732–5741, Montreal, QC, Canada, 2021. IEEE. 5 [31] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht- man, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 586–595, Salt Lake City, UT, 2018. IEEE. 6 [32] Tianli Zhao, Jiayuan Chen, Cong Leng, and Jian Cheng. TinyNeRF: Towards 100 x Compression of V oxel Radiance Fields. Proceedings of the AAAI Conference on Artificial In- telligence, 37(3):3588–3596, 2023. Number: 3. 1, 2 [33] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Trans. Inform. Theory, 23(3):337– 343, 1977. 5 [34] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross. EW A volume splatting. In Proceedings Visualization, 2001. VIS ’01., pages 29–538, San Diego, CA, USA, 2001. IEEE. 3, 5 10Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis Supplementary Material 8. Supplementary A. Detailed Scene Analysis For all scenes used in the paper, we report the PSNR, SSIM, LPIPS, memory consumption, and compression ratio of our approach. See Tab. 8 for Mip-Nerf360 [2], Tab. 10 for Deep Blending [9], Tab. 9 for Tanks&Temples [14], Tab. 11 for the synthetic scenes from [16]. B. Image Quality For all scenes used in the paper, a random test view is se- lected. The ground truth images are compared to the ren- derings of the uncompressed (baseline) and our compressed (Compressed) scene representation. See Figs. 11 and 12 for Mip-Nerf360 [2], Fig. 10 for Deep Blending [9], Fig. 9 for Tanks & Temples [14], Figs. 13 and 14 for the synthetic scenes from [16]. C. Memory Requirements Fig. 7 illustrates the memory requirements of different scene parameters. The coordinates of the 3D Gaussian cen- ter points and the codebook indices take up the most mem- ory in general. The amount of memory required by the color codebook varies significantly between different scenes. 0% 20% 40% 60% 80% 100% Flowers Garden Stump Treehill Room Counter Kitchen Bonsai Bicycle   Truck Train    Playroom Drjohnson     Chair Drums Ficus Hotdog Lego Materials Mic Ship Mip-Nerf360 T anks & T emples Deep  Blending NeRF Synthetic position color opacity  shape indices Figure 7. Storage size of different scene parameters in the com- pressed representation. Color is the codebook with all SH coeffi- cients. Shape is the codebook with the Gaussian parameters and η is the scaling factor. D. Timing Statistics We provide timings for the different stages of our compres- sion pipeline. Tab. 6 shows the average and maximum time required by each stage. It can be seen that the fine-tuning stage takes up 70% of the total time. Average Time↓ Maximum Time↓ Sensitivity Calculation 8.05 11 .38 Clustering 75.11 78 .41 QA Fine-tuning 213.30 278 .05 Encoding 2.69 5 .13 Total 299.15 365 .94 Table 6. Time requirements of the individual stages of the com- pression pipeline. We report the average and maximum time of each stage in seconds. The entropy and run-length encoding are grouped into the Encoding stage. Measurements were taken with an NVIDIA RTX A5000 graphics card. Additionally, we report timings for each stage of the novel view renderer. Tab. 7 shows the average times for two different scenes. It can be seen that the preprocessing stage is accelerated by a factor of 5× when using the compressed scene representation. Preprocess↓ Sorting↓ Rasterization↓ Total↓ Bicycle Uncompressed 1.46 0 .55 2 .81 4.82 Compressed 0.28 0 .48 2 .45 3.22 Bonsai Uncompressed 0.44 0 .20 1 .81 2.44 Compressed 0.09 0 .19 1 .67 1.95 Table 7. Timings in milliseconds for the different stages of our renderer. Evaluated on an NVIDIA RTX A5000 with scenes from Mip-Nerf360 [2] 11(a) Baseline  (b) Compressed Figure 8. Pruning failure case. Compared to the baseline recon- struction, some leaves have been removed in the compressed ver- sion due to pruning. E. Sensitivity Calculation and Pruning The sensitivity of a parameter is calculated using the gra- dient of the total image energy wrt. this parameter (see Eq. (3)). Kerbl et al . [13] clamp negative direction- dependent colors (i.e., resulting from the evaluation of the SH coefficients) to zero. For the clamped values, the partial derivatives are set to zero in the backward pass. This results in a sensitivity of zero for the respective SH coefficients, which is not desired since they possibly contribute to the training images. Therefore, we do not clamp colors when calculating the sensitivity. We observe that a notable number of Gaussians (up to 15%) do not have any impact on the training images. These particular splats exhibit zero sensitivity in the color param- eters. Consequently, we opt to eliminate these splats from the scene (called Pruning in Tab. 3). Experiments with higher pruning thresholds have shown that more Gaussians can be removed with minimal loss in PSNR. However, this can lead to fine details in the scene being removed, which we consider undesirable. An exam- ple of this can be seen in Fig. 8, where small leaves were removed from the reconstruction due to pruning. F. Covariance Matrix Clustering Given a rotation matrix R ∈ R3×3 and a scaling vector s ∈ R3 >0. The covariance matrix Σ is defined as [13] Σ = RSSRT = RS2RT , (8) with S = diag(s) . Since Σ is real and symmetric it holds that S2 = diag([λ1, λ2, λ3]T ) = diag([s2 1, s2 2, s2 3]T ), (9) where λi are the eigenvalues of Σ. By using the trace of Σ, the squared length of s can be calculated as Tr(Σ) = 3X i=1 λi = 3X i=1 s2 i = ∥s∥2 2 (10) Clustering Update Step In the following, we show that the clustering update step results in normalized covariance matrices as cluster centroids. Given N normalized covari- ance matrices ˆΣi with ∥si∥2 = 1 and respective weighting factors wi ∈ R>0. Their centroid ˆΣc is calculated as ˆΣc = 1PN i=1 wi NX i=1 wi ˆΣi (11) . By using Eq. (10) it holds that ∥sc∥2 2 = Tr(ˆΣc) (12) = Tr( 1PN i=1 wi NX i=1 wi ˆΣi) (13) = 1PN i=1 wi NX i=1 wiTr(ˆΣi) (14) = 1PN i=1 wi NX i=1 wi∥si∥2 2 (15) = 1 (16) This proves that the covariance matrix ˆΣc has a normalized scaling vector and thus iteself is in a normalized form. Covariance Matrix Normalization The following derivation proofs that a covariance matrix Σ can be trans- formed into its normalized form ˆΣ by dividing it by its trace, i.e., Σ Tr(Σ) = R S2 Tr(Σ)RT = R S ∥s∥2 S ∥s∥2 RT (17) = R ˆS2RT = ˆΣ (18) 123D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ bicycle 25.171 0 .762 0 .216 1450 .277 24.970 0 .751 0 .240 47 .147 30.761 bonsai 31.979 0 .938 0 .208 294 .415 31.347 0 .930 0 .217 12 .794 23.011 counter 28.888 0 .905 0 .204 289 .244 28.671 0 .896 0 .215 13 .789 20.977 flowers 21.448 0 .602 0 .341 860 .062 21.152 0 .584 0 .358 31 .140 27.619 garden 27.179 0 .861 0 .115 1379 .993 26.746 0 .844 0 .144 46 .565 29.636 kitchen 30.713 0 .923 0 .130 438 .099 30.262 0 .914 0 .140 18 .874 23.211 room 31.341 0 .916 0 .223 376 .853 31.138 0 .911 0 .231 15 .033 25.068 stump 26.562 0 .770 0 .219 1173 .522 26.285 0 .757 0 .250 40 .569 28.926 treehill 22.303 0 .631 0 .328 894 .903 22.256 0 .620 0 .351 33 .318 26.859 average 27.287 0 .812 0 .220 795 .263 26.981 0 .801 0 .238 28 .803 26.230 Table 8. Mip-Nerf360 [2] results. 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ train 21.770 0 .805 0 .217 242 .782 21.863 0 .798 0 .226 13 .249 18.324 truck 24.940 0 .871 0 .155 601 .030 24.823 0 .867 0 .161 21 .316 28.196 average 23.355 0 .838 0 .186 421 .906 23.343 0 .832 0 .194 17 .282 23.260 Table 9. Tanks&Temples [14] results 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ drjohnson 28.938 0 .896 0 .248 805 .358 28.871 0 .895 0 .254 28 .938 27.830 playroom 29.926 0 .901 0 .244 602 .186 29.891 0 .900 0 .252 21 .660 27.802 average 29.432 0 .898 0 .246 703 .772 29.381 0 .898 0 .253 25 .299 27.816 Table 10. Deep Blending [9] results 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ chair 35.864 0 .987 0 .012 70 .105 35.297 0 .985 0 .014 3 .575 19.609 drums 26.072 0 .954 0 .038 83 .665 25.941 0 .952 0 .040 3 .829 21.848 ficus 34.736 0 .987 0 .012 70 .177 34.559 0 .986 0 .013 3 .059 22.937 hotdog 37.646 0 .985 0 .021 34 .079 37.367 0 .984 0 .022 2 .725 12.505 lego 35.399 0 .981 0 .017 76 .071 34.802 0 .979 0 .020 4 .314 17.633 materials 29.861 0 .959 0 .035 71 .833 29.602 0 .957 0 .038 4 .021 17.862 mic 35.155 0 .991 0 .006 77 .563 34.913 0 .991 0 .007 3 .025 25.640 ship 30.954 0 .905 0 .111 75 .659 31.005 0 .905 0 .111 4 .938 15.322 average 33.211 0 .969 0 .031 69 .894 32.936 0 .967 0 .033 3 .686 19.170 Table 11. NeRF Synthetic [16] results 13Truck Train Ground Truth Baseline Compressed Figure 9. Random test views for each scene from Tanks&Temples [14] Playroom Drjohnson Ground Truth Baseline Compressed Figure 10. Random test views for each scene from Deep Blending [9] 14Flowers Garden Stump Treehill Room Ground Truth Baseline Compressed Figure 11. Random test views for each scene from Mip-NeRF360 [2] 15Counter Kitchen Bonsai Bicycle Ground Truth Baseline Compressed Figure 12. Random test views for each scene from Mip-NeRF360 [2] 16Chair Drums Ficus Hotdog Ground Truth Baseline Compressed Figure 13. Random test views for each scene from NeRF Synthetic [16] 17Lego Materials Mic Ship Ground Truth Baseline Compressed Figure 14. Random test views for each scene from NeRF Synthetic [16] 18",
      "meta_data": {
        "arxiv_id": "2401.02436v2",
        "authors": [
          "Simon Niedermayr",
          "Josef Stumpfegger",
          "Rüdiger Westermann"
        ],
        "published_date": "2023-11-17T14:40:43Z",
        "pdf_url": "https://arxiv.org/pdf/2401.02436v2.pdf",
        "github_url": "https://github.com/KeKsBoTer/c3dgs"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a compressed 3D Gaussian splat representation for novel view synthesis, addressing the high memory consumption and rendering inefficiencies of previous 3D Gaussian splatting. The key contributions include a compression scheme that utilizes sensitivity-aware vector clustering with quantization-aware training for directional colors and Gaussian parameters, achieving up to 31x compression with minimal visual quality degradation. It also proposes an efficient novel view renderer leveraging hardware rasterization on lightweight GPUs, resulting in up to 4x higher framerates than optimized GPU compute pipelines and seamless integration into VR/AR environments.",
        "methodology": "The compression scheme consists of three main steps: 1) Sensitivity-aware vector clustering, where a sensitivity measure is computed for each scene parameter (SH coefficients, Gaussian shape via normalized covariance matrices) based on its contribution to training images. K-Means clustering with a weighted distance metric is used to encode these parameters into compact codebooks, with highly sensitive Gaussians being exempt from clustering. 2) Quantization-aware fine-tuning, which uses Min-Max quantization (e.g., 8-bit for most, 16-bit float for position) to represent parameters with fewer bits after compression, and fine-tunes them on training images to recover lost information. 3) Entropy encoding, where 3D Gaussians are linearized along a Z-order curve (Morton order) to exploit spatial coherence, followed by DEFLATE (LZ77 + Huffman coding) and run-length encoding for further compression. The novel view renderer utilizes a compute pre-pass to discard out-of-frustum Gaussians, compute view-dependent colors, and depth-sort Gaussians on the GPU using the Onesweep algorithm. Final rendering is performed via GPU hardware rasterization, projecting each Gaussian as a planar quad (splat) and blending colors in the pixel shader.",
        "experimental_setup": "The method was evaluated on multiple benchmark datasets: Mip-Nerf360, Tanks&Temples, Deep Blending, and NeRF-Synthetic. For Mip-Nerf360, Tanks&Temples, and Deep Blending, reconstructions from Kerbl et al. [13] were used, while for NeRF-Synthetic, 3D Gaussian representations were self-generated. Quantitative metrics used for evaluation were PSNR, SSIM, and LPIPS for image quality, alongside compression ratio (SIZE↓) and rendering framerates (FPS). Implementation details included a decay factor λd=0.8 for batched clustering with 800 update steps for Gaussians and 100 for SH coefficients, batch sizes of 2^18 for color features and 2^20 for Gaussian parameters, a default codebook size of 4096, sensitivity thresholds βc = 6·10^-7 and βg = 3·10^-6, and 5000 optimization steps for quantization-aware fine-tuning. The renderer was implemented using the WebGPU graphics API in Rust, tested on NVIDIA RTX A5000, NVIDIA RTX 3070M, Intel UHD Graphics 11, and AMD Radeon R9 380. An ablation study analyzed the contribution of individual pipeline stages and the influence of codebook sizes and sensitivity thresholds.",
        "limitations": "The main limitation identified is the current inability to aggressively compress the Gaussians' positions in 3D space. Experiments involving quantizing positions to a lattice structure or embedding positional constraints directly into the Gaussian splatting training process did not succeed in further compressing positions without introducing significant errors in the rendering process.",
        "future_research_directions": "Future research aims to explore new approaches for reducing the memory footprint during the training phase. Additionally, compressing positional information end-to-end is a key direction. The authors also believe that 3D Gaussian splatting has potential for reconstructing volumetric scenes and plan to investigate advanced options for compressing and rendering these optimized representations.",
        "experimental_code": "File Path: compress.py\nContent:\n    def calc_importance(\n        gaussians: GaussianModel, scene, pipeline_params\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        scaling = gaussians.scaling_qa(\n            gaussians.scaling_activation(gaussians._scaling.detach())\n        )\n        cov3d = gaussians.covariance_activation(\n            scaling, 1.0, gaussians.get_rotation.detach(), True\n        ).requires_grad_(True)\n        scaling_factor = gaussians.scaling_factor_activation(\n            gaussians.scaling_factor_qa(gaussians._scaling_factor.detach())\n        )\n\n        h1 = gaussians._features_dc.register_hook(lambda grad: grad.abs())\n        h2 = gaussians._features_rest.register_hook(lambda grad: grad.abs())\n        h3 = cov3d.register_hook(lambda grad: grad.abs())\n        background = torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32, device=\"cuda\")\n\n        gaussians._features_dc.grad = None\n        gaussians._features_rest.grad = None\n        num_pixels = 0\n        for camera in tqdm(scene.getTrainCameras(), desc=\"Calculating sensitivity\"):\n            cov3d_scaled = cov3d * scaling_factor.square()\n            rendering = render(\n                camera,\n                gaussians,\n                pipeline_params,\n                background,\n                clamp_color=False,\n                cov3d=cov3d_scaled,\n            )[\"render\"]\n            loss = rendering.sum()\n            loss.backward()\n            num_pixels += rendering.shape[1]*rendering.shape[2]\n\n        importance = torch.cat(\n            [gaussians._features_dc.grad, gaussians._features_rest.grad],\n            1,\n        ).flatten(-2)/num_pixels\n        cov_grad = cov3d.grad/num_pixels\n        h1.remove()\n        h2.remove()\n        h3.remove()\n        torch.cuda.empty_cache()\n        return importance.detach(), cov_grad.detach()\n\n    def run_vq(\n        model_params: ModelParams,\n        optim_params: OptimizationParams,\n        pipeline_params: PipelineParams,\n        comp_params: CompressionParams,\n    ):\n        gaussians = GaussianModel(\n            model_params.sh_degree, quantization=not optim_params.not_quantization_aware\n        )\n        scene = Scene(\n            model_params, gaussians, load_iteration=comp_params.load_iteration, shuffle=True\n        )\n\n        if comp_params.start_checkpoint:\n            (checkpoint_params, first_iter) = torch.load(comp_params.start_checkpoint)\n            gaussians.restore(checkpoint_params, optim_params)\n\n\n        timings ={}\n\n        # %%\n\n        start_time = time.time()\n        color_importance, gaussian_sensitivity = calc_importance(\n            gaussians, scene, pipeline_params\n        )\n        end_time = time.time()\n        timings[\"sensitivity_calculation\"] = end_time-start_time\n        # %%\n        print(\"vq compression..\")\n        with torch.no_grad():\n            start_time = time.time()\n            color_importance_n = color_importance.amax(-1)\n\n            gaussian_importance_n = gaussian_sensitivity.amax(-1)\n\n            torch.cuda.empty_cache()\n\n            color_compression_settings = CompressionSettings(\n                codebook_size=comp_params.color_codebook_size,\n                importance_prune=comp_params.color_importance_prune,\n                importance_include=comp_params.color_importance_include,\n                steps=int(comp_params.color_cluster_iterations),\n                decay=comp_params.color_decay,\n                batch_size=comp_params.color_batch_size,\n            )\n\n            gaussian_compression_settings = CompressionSettings(\n                codebook_size=comp_params.gaussian_codebook_size,\n                importance_prune=None,\n                importance_include=comp_params.gaussian_importance_include,\n                steps=int(comp_params.gaussian_cluster_iterations),\n                decay=comp_params.gaussian_decay,\n                batch_size=comp_params.gaussian_batch_size,\n            )\n\n            compress_gaussians(\n                gaussians,\n                color_importance_n,\n                gaussian_importance_n,\n                color_compression_settings if not comp_params.not_compress_color else None,\n                gaussian_compression_settings\n                if not comp_params.not_compress_gaussians\n                else None,\n                comp_params.color_compress_non_dir,\n                prune_threshold=comp_params.prune_threshold,\n            )\n            end_time = time.time()\n            timings[\"clustering\"]=end_time-start_time\n\n        gc.collect()\n        torch.cuda.empty_cache()\n        os.makedirs(comp_params.output_vq, exist_ok=True)\n\n        copyfile(\n            path.join(model_params.model_path, \"cfg_args\"),\n            path.join(comp_params.output_vq, \"cfg_args\"),\n        )\n        model_params.model_path = comp_params.output_vq\n\n        with open(\n            os.path.join(comp_params.output_vq, \"cfg_args_comp\"), \"w\"\n        ) as cfg_log_f:\n            cfg_log_f.write(str(Namespace(**vars(comp_params))))\n\n        iteration = scene.loaded_iter + comp_params.finetune_iterations\n        if comp_params.finetune_iterations > 0:\n\n            start_time = time.time()\n            finetune(\n                scene,\n                model_params,\n                optim_params,\n                comp_params,\n                pipeline_params,\n                testing_iterations=[\n                    -1\n                ],\n                debug_from=-1,\n            )\n            end_time = time.time()\n            timings[\"finetune\"]=end_time-start_time\n\n            # %%\n        out_file = path.join(\n            comp_params.output_vq,\n            f\"point_cloud/iteration_{iteration}/point_cloud.npz\",\n        )\n        start_time = time.time()\n        gaussians.save_npz(out_file, sort_morton=not comp_params.not_sort_morton)\n        end_time = time.time()\n        timings[\"encode\"]=end_time-start_time\n        timings[\"total\"]=sum(timings.values())\n        with open(f\"{comp_params.output_vq}/times.json\",\"w\") as f:\n            json.dump(timings,f)\n        file_size = os.path.getsize(out_file) / 1024**2\n        print(f\"saved vq finetuned model to {out_file}\")\n\n        # eval model\n        print(\"evaluating...\")\n        metrics = render_and_eval(gaussians, scene, model_params, pipeline_params)\n        metrics[\"size\"] = file_size\n        print(metrics)\n        with open(f\"{comp_params.output_vq}/results.json\",\"w\") as f:\n            json.dump({f\"ours_{iteration}\":metrics},f,indent=4)\n",
        "experimental_info": "The `calc_importance` function computes sensitivity for SH coefficients and Gaussian shape (normalized covariance matrices) by accumulating absolute gradients with respect to a rendering loss. The `run_vq` function orchestrates the compression pipeline. It first calls `calc_importance` to obtain sensitivity measures. These measures are then passed to `compress_gaussians` for sensitivity-aware vector clustering. After clustering, `finetune` is called for quantization-aware fine-tuning. Finally, the compressed model is saved using `gaussians.save_npz`, with an option to sort Gaussians along a Z-order curve (Morton order) if `comp_params.not_sort_morton` is False. Metrics and timings are also saved."
      }
    },
    {
      "title": "Neural NeRF Compression",
      "abstract": "Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing\ndetailed 3D scenes through continuous volumetric representations. Recent NeRFs\nutilize feature grids to improve rendering quality and speed; however, these\nrepresentations introduce significant storage overhead. This paper presents a\nnovel method for efficiently compressing a grid-based NeRF model, addressing\nthe storage overhead concern. Our approach is based on the non-linear transform\ncoding paradigm, employing neural compression for compressing the model's\nfeature grids. Due to the lack of training data involving many i.i.d scenes, we\ndesign an encoder-free, end-to-end optimized approach for individual scenes,\nusing lightweight decoders. To leverage the spatial inhomogeneity of the latent\nfeature grids, we introduce an importance-weighted rate-distortion objective\nand a sparse entropy model employing a masking mechanism. Our experimental\nresults validate that our proposed method surpasses existing works in terms of\ngrid-based NeRF compression efficacy and reconstruction quality.",
      "full_text": "Neural NeRF Compression Tuan Pham1 Stephan Mandt1 Abstract Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing detailed 3D scenes through continuous volumetric representations. Recent NeRFs utilize feature grids to improve rendering quality and speed; however, these rep- resentations introduce significant storage over- head. This paper presents a novel method for efficiently compressing a grid-based NeRF model, addressing the storage overhead concern. Our approach is based on the non-linear transform coding paradigm, employing neural compression for compressing the model’s feature grids. Due to the lack of training data involving many i.i.d scenes, we design an encoder-free, end-to-end optimized approach for individual scenes, using lightweight decoders. To leverage the spatial inho- mogeneity of the latent feature grids, we introduce an importance-weighted rate-distortion objective and a sparse entropy model employing a mask- ing mechanism. Our experimental results vali- date that our proposed method surpasses existing works in terms of grid-based NeRF compression efficacy and reconstruction quality. 1. Introduction Over the past few years, the field of 3D scene modeling and reconstruction has been revolutionized by the advent of Neural Radiance Fields (NeRF) methodologies (Mildenhall et al., 2021; Zhang et al., 2020; Barron et al., 2021). NeRFs offer a sophisticated method for 3D reconstruction, with the ability to synthesize novel viewpoints from limited 2D data. Yet, the original NeRF model requires millions of MLP queries, which causes slow training and rendering. To address these efficiency concerns, recent NeRF advance- ments have transitioned to the integration of an explicit grid representation (Yu et al., 2021; Sun et al., 2022; Fridovich- 1Department of Computer Science, University of California Irvine. Correspondence to: Tuan Pham <tuan.pham@uci.edu>. Proceedings of the41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Keil et al., 2022; Chen et al., 2022; Fridovich-Keil et al., 2023; Chan et al., 2022). While significantly accelerating training and rendering processes, this change also poses a new challenge: the storage cost for saving the explicit grid NeRF representation increases. This problem is crucial, especially in real-world (e.g., large-scale VR and AR) ap- plications where storage and transmission impose critical constraints. Our work seeks to significantly reduce the storage costs of NeRFs. Inspired by neural image compression methodol- ogy (Yang et al., 2023b), we apply non-linear transform coding techniques (Ball´e et al., 2020) to compress the ex- plicit grid NeRF representation efficiently. However, we sidestep the conventional auto-encoder approach in favor of an iterative inference framework, in which we jointly optimize the latent code along with a lightweight decoder. We further take account of the NeRF grid importance scores while reconstructing the scene to boost the efficiency of our compression model. Lastly, we propose a novel entropy model that masks uninformative feature grid points. Utiliz- ing a rate-distortion objective, we can choose from various compression levels. Our proposed approach departs from previous works on compressing explicit grid NeRF represen- tations (Li et al., 2023a;b; Deng & Tartaglione, 2023) based on voxel pruning and/or vector quantization (Gray, 1984) while taking into account the varying importance levels of different voxel grid locations. To show the effectiveness of our proposed method, we per- form extensive experiments on four different datasets. Our results show that our model is capable of compressing di- verse NeRF scenes to a much smaller size and improves over previous works in terms of rate-distortion performance. 2. Background 2.1. Neural Radiance Fields Neural radiance fields (Mildenhall et al., 2021) mark a paradigm shift in 3D scene representation using deep neural networks. Unlike traditional approaches that employ dis- crete structures such as point clouds or meshes, NeRFs model a scene using a continuous volumetric function F : (x, d) → (c, σ). Here, an input comprising of spa- tial coordinates x and a viewing direction d is mapped to 1 arXiv:2406.08943v1  [cs.CV]  13 Jun 2024Neural NeRF Compression an output representing color c and volume density σ. For each pixel, the estimated color ˆC(r) for the correspond- ing ray r can be calculated by: ˆC(r) = NX i=1 Ti · αi · ci, (1) with the following definitions: • αi = 1− exp(−σiδi) is the probability of light being absorbed at the i-th point, dependent on the volume density σi and the distance δi between adjacent sam- pled points on the ray. • Ti = Qi j=1(1 − αj) represents the accumulated trans- mittance, or the remaining light that has not been ab- sorbed before reaching the i-th point. NeRF is then trained to minimize total squared error loss between the rendered and true pixel colors. Lrender = X r || ˆC(r) − C(r)||2 2 (2) Despite NeRF’s ability to provide intricate scene details with a relatively compact neural network, the computational de- mand remains a significant constraint. The evaluation over the volume often requires thousands of network evaluations per pixel. To reduce training and inference time, recent research has employed explicit grid structure into NeRF. More specifically, they introduce voxel grids (Sun et al., 2022; Fridovich-Keil et al., 2022) or decomposed feature planes (Chen et al., 2022; Fridovich-Keil et al., 2023; Chan et al., 2022) into the model, and query point features via trilinear or bilinear interpolation. While this notably speeds up training and inference, it does come at the expense of greater storage needs from saving the feature grids. 2.2. Neural Compression Neural compression utilizes neural networks to perform end-to-end learned data compression (Yang et al., 2023b). Traditional compression algorithms are handcrafted and specifically tailored to the characteristics of the data they compress, such as JPEG (Wallace, 1991) for images or MP3 for audio. In contrast, neural compression seeks to learn efficient data representations directly from the data, exemplified by the nonlinear transform coding paradigm (Ball´e et al., 2020). Existing lossy neural compression methods (Ball ´e et al., 2016; 2018; Minnen et al., 2018; Cheng et al., 2020; Mat- subara et al., 2022; Yang & Mandt, 2023a;b; Yang et al., 2023a) often leverage an auto-encoder architecture (Kingma & Welling, 2014). Here, an encoder E maps data X to con- tinuous latent representations Z = E(X). This continuous Z is then quantized to integers byQ, resulting in ˆZ = Q(Z). An entropy model P is used to transmit ˆZ losslessly. Fi- nally, a decoder D receives the quantized latent code ˆZ and reconstructs the original data ˆX = D(ˆZ). One commonly trains the encoder E, the decoder D and the entropy model P jointly using a rate-distortion objective: L(E, D, P) =EX∼p(X)[d(X, D(Q(E(X))) − λ log2 P(Q(E(X)))] (3) where d(·, ·) is a distortion loss and the second term is the rate loss that measures the expected code length. The pa- rameter λ balances between the two loss terms. At training time, the quantizer Q is typically replaced with injecting uniform noise (Ball´e et al., 2016). See (Yang et al., 2023b) for a detailed review of neural compression. 3. Method In this section, we describe our method for grid-based NeRF compression. Our primary focus is on the compression of the TensoRF-VM model (Chen et al., 2022), characterized by its decomposed 2D feature plane structure (Kolda & Bader, 2009). We select TensoRF-VM because of its profi- cient 3D scene modeling capabilities, often outperforming alternative methods like Plenoxels (Fridovich-Keil et al., 2022) or DVGO (Sun et al., 2022). Our method has the po- tential to be applied to other grid-based NeRF architectures. Problem setting. We have a TensoRF-VM model that was pre-trained for a single scene, and our task is to reduce its size through compression while maintaining its reconstruc- tion quality. We assume that we have access to the training dataset comprising view images at compressing time. Notation. The three feature planes (or matrix components) of TensoRF-VM are denoted by {Pi}3 i=1, in which sub- script i signifies the index of the planes and each Pi ∈ RCi×Hi×Wi . In practice, {Pi}3 i=1 is the channel-wise con- catenation of the density planes and appearance planes of TensoRF-VM. The vector components are not considered in our compression and, hence, are not represented in our notation. For indexing a specific spatial grid location j in the feature plane i, we employ a superscript, represented as Pj i . 3.1. Compressing the feature planes Most storage for compressing TensoRF-VM is spent on the feature grids. To illustrate this, we analyze a trained model for the Lego scene from the Synthetic-NeRF dataset (Mildenhall et al., 2021). In this model, the 2D feature planes take 67.61 MB, while the other components, such as the rendering MLP, the rendering mask, and the vector 2Neural NeRF Compression Decoder  Latent codes Reconstructed feature planes TensoRF-VM rendering Entropy Model  Pre-trained  feature planes Decoder  Latent codes Reconstructed feature planes TensoRF-VM rendering Entropy Model   Bitstring Training Rendering Figure 1: Overview of our model.At training time (left), we learn the three latent codes {Zi}3 i=1 to reconstruct the three frozen feature planes {Pi}3 i=1. The reconstructed feature planes {ˆPi}3 i=1. are used to render the scene and calculate the rendering loss. The entropy model P is used to calculate the rate loss and compress the latent codes to bitstring. At rendering time (right), we use P to decompress the bitstring to latent codes {ˆZi}3 i=1 and then reconstruct the feature planes {ˆPi}3 i=1. components, take only 1.21 MB. Given this disparity, we focus on compressing TensoRF-VM’s feature planes. In more detail, we can define an encoder E that embeds the three feature planes {Pi}3 i=1 to latent codes {Zi}3 i=1, in which the Zi may have lower resolution than Pi. The latent codes are quantized to {ˆZi}3 i=1 and compressed with en- tropy coding using an entropy model P. At rendering time, we decompress the quantized latent codes {ˆZi}3 i=1 and for- ward them to the decoder D to reconstruct the three feature planes {ˆPi}3 i=1. We then use {ˆPi}3 i=1 to query sampling point features and render the scene. The compressed NeRF model includes the compressed latent codes, the decoder, the entropy model, and the other components. It is crucial to highlight that we only need to reconstruct the three feature planes once, and all subsequent querying operations for the sampling points are executed on these reconstructed planes. Thus, the decompression process only adds minimal overhead to the overall rendering procedure. Per-scene optimization. The conventional approach to neural image compression (Ball ´e et al., 2016; 2018) in- volves training a compression model on a big dataset con- taining thousands of images. However, applying this same training method to NeRF compression presents three chal- lenges: First, we need a dataset with numerous 3D objects. Although datasets like Objaverse (Deitke et al., 2023) or Objaverse-XL (Deitke et al., 2024) exist, they are synthetic datasets and only contain a single object for each scene. Ad- ditionally, pre-trained NeRF models are required for every 3D object in the dataset, demanding significant computa- tional resources and storage. Finally, we cannot adapt other components of the NeRF model, such as the rendering MLP and the vector components of the TensorF-VM model. Due to these challenges, we optimize each NeRF scene individ- ually, a process we refer to as per-scene optimization. In this approach, the compressor is overfitted to each NeRF scene, which results in improved compression performance. Transform coding without encoder.In nonlinear trans- form coding (Ball ´e et al., 2020; Yang et al., 2023b), one usually employs an encoder to obtain the latent code of a new data point via amortized inference (Kingma & Welling, 2014; Gershman & Goodman, 2014). This is essential for compressing a new data point quickly in a single network pass. Nonetheless, in the case of per-scene TensoRF-VM compression, our primary objective is to compress merely the three feature planes, and our decoder is overfitted to a single scene. Moreover, using an encoder for amortized inference leads to an irreducible amortization gap in opti- mization (Cremer et al., 2018; Marino et al., 2018), which has been shown to degrade compression performance (Cam- pos et al., 2019; Yang et al., 2020). For these reasons, we remove the encoder and directly learn the three latent codes {Zi}3 i=1 for each scene. More specif- ically, we initialize the {Zi}3 i=1 as a tensor of zeros, and jointly optimize {Zi}3 i=1 with the decoder D and the en- tropy model P. At decoding time, the receiver thus receives a binary code along with the entropy model and decoder to reconstruct the sample, all three of which are counted towards the bitrate. Architecture design. Since we must transmit the decoder D along with the latent code {Zi}3 i=1 to decompress the scene, it’s essential for the decoder to be lightweight. Yang & Mandt (2023b) established a lightweight decoder for neu- ral image compression. We found that a two-layer trans- posed convolutional neural network with SELU activation (Klambauer et al., 2017) is effective for our needs. 3Neural NeRF Compression 3.2. Importance-weighted training loss Our model is trained end-to-end (on top of the pre-trained NeRF) with a rate-distortion loss. The rate loss is defined as the log-likelihood of the entropy model P, and it ensures that the compressed feature planes have low relative entropy to the prior P. For the distortion loss, we discover that using only the NeRF rendering loss Lrender is not sufficient; we also need to use an L2 feature plane reconstruction loss for good rendering quality. However, reconstructing the entire feature planes is not the most efficient approach for compression. Prior research (Li et al., 2023a;b; Deng & Tartaglione, 2023) has illustrated that these feature grids possess significant redundancy and could be pruned to decrease the size of the model. Conse- quently, if we were to reconstruct every single grid location, it would inevitably lead to additional storage costs. To address this issue, we suggest computing weight maps, defined below, that we use to re-weight the feature plane reconstruction loss. With this approach, our model is guided to reconstruct only high-density grid locations while ignor- ing the less populated ones, ensuring a more optimized and effectively compressed representation. For each feature plane Pi ∈ RCi×Wi×Hi , we define Wi ∈ RWi×Hi as the corresponding weight map, shared across all feature channels. These weight maps are constructed based on the rendering importance score {Ii}3 i=1 by Li et al. (2023a;b), defined next. As follows, we consider feature plane i and grid location j. The collection of sampling points xk ∈ R3 in the vicinity of location j (upon projection) shall be denoted as Nj. Since the coordinates of xk are continuous and the grid locations discrete, we distribute the ”mass” of each xk onto the rele- vant grid locations using bilinear interpolation, resulting in the interpolation weights ωi kj for sampling point xk ∈ Nj. In addition, each sampling point xk in Eq. 1 has a corre- sponding transmittance coefficient Tk · αk that we interpret as its importance. This lead to the following importance scores for each grid location j in plane i, Ij i = X k∈Nj ωi kj · Tk · αk (4) In sum, each importance score Ij i is a weighted aggregate of the individual importance scores of the neighboring sam- pling points xk over the feature grid. Finally, we apply a log-transform to the importance maps {Ii}3 i=1, and then normalize them to the range of [0, 1] to get the weights {Wi}3 i=1: Wi = normalize(log(Ii + ϵ)), (5) in which ϵ = 0.01 to ensure that the log is well-defined. 3.3. Masked entropy model Applying neural compression to TensoRF-VM enables us to use a wide range of different entropy models. In this section, we design a simple but effective entropy model that works well for TensoRF-VM compression, exploiting the spatial sparsity of the feature plane representation. Theoretically, a learned entropy model, P, should result in a close-to-optimal coding strategy, provided the model is flexible enough. In practice, we observed that a predomi- nant portion of the learned latent code is zero, especially in the background. This observation might be attributed to our choice of initializing the latent codes as zero tensors and the fact that large parts of the feature planes are not used for ren- dering. Such sparsity is poorly captured using the standard entropy models used in neural image compression (Ball ´e et al., 2016; 2018), leading to entropy coding inefficiencies. To design a better entropy model, we construct a spike-and- slab prior, oftentimes used in Bayesian statistics (Mitchell & Beauchamp, 1988). To this end, we construct binary masks {Mi}3 i=1 into our entropy model P. The model P compresses grid features Pj i only when Mj i = 1, allow- ing selective compression of specific features and avoiding others. Those masks are learnable and can be treated as additional parameters of P. In more detail, we design P to be a fully factorized probabil- ity distribution as in Ball ´e et al. (2016) and Ball ´e et al. (2018). Every grid location is independent and identi- cally distributed by binary mixture, consisting of a non- parametric distribution pθ(·) with learnable θ, and a Dirac mass δ(·) at zero. For each latent code ˆZi to be compressed, we establish a corresponding binary mask Mi that has the same spatial size and is shared across features channels. The conditional probability distribution P (given the mask) is then factorized across spatial locations j as: PMi (ˆZi) = Y j p(ˆZj i |Mj i ); p(ˆZj i |Mj i ) = ( δ(ˆZj i ) if Mj i = 0 pθ(ˆZj i ) if Mj i = 1. (6) We stress that we could also entropy-code the masks under a hyperprior p(M), but found little benefit to do so in practice. This construction implies that, ifMj i = 0, then we designate ˆZj i = 0. Thus, the input to the decoder D can be calculated as ˆZi ⊙ Mi, and the reconstructed planes are ˆPi = D(ˆZi ⊙ Mi). (7) However, since the masks Mi are binary, they cannot be learned directly. To address this, we turn to the Gumbel- Softmax trick (Jang et al., 2016; Yang et al., 2020) to fa- 4Neural NeRF Compression cilitate the learning of Mi. For each Mi, we define the binary probabilities denoted by π0 Mi and π1 Mi indicating Mi = 0/1, respectively. At training time, we sample Mi using the straight-through Gumbel-Softmax estimator (Ben- gio et al., 2013; Jang et al., 2016): Mi = arg max j∈{0,1} (gj + logπj Mi ) (8) in which gj are i.i.d samples drawn from Gumbel(0, 1). The straight-through Gumbel-Softmax estimator allows us to calculate the gradients of πj Mi . We then optimize the mask probabilities πj Mi following the rate-distortion loss: L =Lrender({ˆPi}3 i=1) + 3X i=1 \u0010 ||(Pi − ˆPi) ⊙ Wi||2 2 − λ log2 PMi (ˆZi) \u0011 , (9) where ˆPi is calculated with Equation 7. In practice, we use an annealing softmax temperature τ that decays from 10 to 0.1 to calculate the softmax gradients. 4. Experiments As follows, we empirically demonstrate that our proposed approach of learning a lightweight, per-scene neural com- pression model without an encoder outperforms existing approaches based on vector quantization and models trained on multiple scenes in terms of rate-distortion performance. 4.1. Experiment Setting Datasets. We perform our experiments on 4 datasets: • Synthetic-NeRF (Mildenhall et al., 2021): This dataset contains 8 scenes at resolution 800 × 800 rendered by Blender. Each scene contains 100 training views and 200 testing views. • Synthetic-NSVF (Liu et al., 2020): This dataset also contains 8 rendered scenes at resolution 800 × 800. However Synthetic-NSVF contains more complex ge- ometry and lightning effects compared to Synthetic- NeRF. • LLFF (Mildenhall et al., 2019): LLFF contains 8 real- world scenes made of forward-facing images with non empty background. We use the resolution 1008 × 756. • Tanks and Temples (Knapitsch et al., 2017): We use 5 real-world scenes: Barn, Caterpillar, Family, Ignatus, Truck from the Tanks and Temples dataset to experi- ment with. They have the resolution of 1920 × 1080. In our compression experiments, we initially train a TensoRF-VM model for every scene within the datasets listed above. We use the default TensoRF-VM 192 hyperpa- rameters, as detailed in (Chen et al., 2022). Subsequently, we apply our proposed method to compress these trained models. All experimental procedures are executed using Py- Torch (Paszke et al., 2019) on NVIDIA RTX A6000 GPUs. Baselines. We compare our compression paradigm with: The original NeRF model with MLP (Mildenhall et al., 2021), the uncompressed TensoRF-CP and TensoRF-VM from Chen et al. (2022), two prior compression meth- ods for TensoRF-VM based on pruning and vector quan- tization named VQ-TensoRF from Li et al. (2023a) and Re:TensoRF from Deng & Tartaglione (2023). Hyperparameters. As discussed in Section 3.1, our de- coder has two transposed convolutional layers with SELU activation (Klambauer et al., 2017). They both have a kernel size of 3, with stride 2 and padding 1. Thus, each layer has an upsampling factor of 2. Given a feature plane sized Ci × Wi × Hi, we initialize the corresponding latent code Zi to have the size of CZi × Wi/4 × Hi/4. Having a decoder with more parameters will enhance the model’s decoding ability while also increase its size. In light of this trade-off, we introduce two configura- tions: ECTensoRF-H (stands for Entropy Coded Ten- soRF - high compression) employs latent codes with 192 channels and a decoder with 96 hidden channels, while ECTensoRF-L (low compression) has 384 la- tent channels and 192 decoder hidden channels. Re- garding the hyperparameter λ, we experiment within the set {0.02, 0.01, 0.005, 0.001, 0.0005, 0.0002, 0.0001}, with higher λ signifying a more compact model. We train our models for 30, 000 iterations with Adam opti- mizer (Kingma & Ba, 2015). We use an initial learning rate of 0.02 for the latent codes and 0.001 for the networks, and apply an exponential learning rate decay. 4.2. Results We first compare our results with the baselines quantitatively. We use the PSNR and SSIM (Wang et al., 2004) metrics to evaluate the reconstruction quality. The compression rate is determined by the compressed file size in MB. Quantitative Results. Table 1 showcases quantitative re- sults in both rate and distortion in the high-quality/low- distortion regime, where the reconstruction quality of all compressed TensoRF models are close to other uncom- pressed performances for novel view synthesis. This regime is particularly relevant in NeRF compression applications, where high-quality renderings for compressed models are typically expected. Compared to the other two TensoRF compression baselines, 5Neural NeRF Compression Table 1: Quantitative results comparing our method versus the baselines. PSNR is measured in dB, while the sizes are in MB. We choose the λ to balance between the reconstruction quality and storage size. Methods Synthetic-NeRF Synthetic-NSVF LLFF Tanks and Temples PSNR SSIM Size PSNR SSIM Size PSNR SSIM Size PSNR SSIM Size Uncom- pressed NeRF 31.01 0.947 5.0 - - - 26.50 0.811 5.0 25.78 0.864 5.0 TensoRF-CP 31.56 0.949 3.9 34.48 0.971 3.9 - - - 27.59 0.897 3.9 TensoRF-VM 33.09 0.963 67.6 36.72 0.982 71.6 26.70 0.836 179.8 28.54 0.921 72.6 Com- pressed VQ-TensoRF 32.86 0.960 3.6 36.16 0.980 4.1 26.46 0.824 8.8 28.20 0.913 3.3 Re:TensoRF 32.81 0.956 7.9 36.14 0.978 8.5 26.55 0.797 20.2 28.24 0.907 6.7 TC-TensoRF-L (ours) 32.93 0.961 3.4 36.34 0.980 4.0 26.44 0.826 4.9 28.42 0.915 2.9 TC-TensoRF-H (ours) 32.31 0.956 1.6 35.33 0.974 1.6 25.72 0.786 1.7 28.08 0.907 1.6 Table 2: Relative improvement of our method versus VQ-TensoRF. BD-PSNR and BD-rate measure the average difference in PSNR and bitrate between the two methods. Synthetic-NeRF Synthetic-NSVF Tanks and Temples BD-PSNR 0.279 dB 0.289 dB 0.344 dB BD-Rate 28.827 % 21.104 % 16.717 % VQ-TensoRF and Re:TensoRF, our variant ECTensoRF-L shows superior reconstruction performance in this regime in terms of both the PSNR and SSIM metrics while simul- taneously maintaining a reduced file size across 3 datasets: Synthetic-NeRF, Synthetic-NSVF, and Tanks & Temples. In the case of the LLFF dataset, we are slightly behind VQ-TensoRF and Re:TensoRF in PSNR. Despite this, our achieved SSIM values surpass both baselines and, remark- ably, the size of our compressed files is just about half of VQ- TensoRF and a mere quarter when compared to Re:TensoRF. For a smaller number of channels, our ECTensoRF-H is able to compress the model sizes to less than 2MB while maintaining a decent reconstruction quality. Notably, our ECTensoRF-H has a similar SSIM as Re:TensoRF on Synthetic-NeRF and Tanks&Temples. Qualitative Results. We compare rendered images from the Synthetic-NeRF dataset, using VQ-TensoRF and our conpression method for both configurations: ECTensoRF-L and ECTensoRF-H in Figure 3. Visually, there is minimal disparity between the uncompressed and compressed Ten- soRF models. We further show more qualitative results for the other datasets in the Appendix. Rate-distortion performance. The rate-distortion curve is widely used in neural compression to compare the compression performance across different compression level. Here we analyze the rate-distortion curve of our ECTensoRF-L with various λ values versus VQ-TensoRF with various codebook size. For the VQ-TensoRF evalua- tions, we employed the officially released code and utilized the same pre-trained TensoRF models for consistency. 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 File Size (MB) 31.0 31.5 32.0 32.5 33.0PSNR (dB) VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 File Size (MB) 0.9425 0.9450 0.9475 0.9500 0.9525 0.9550 0.9575 0.9600 0.9625SSIM VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed Figure 2: Comparison of rate-distortion curves between our proposed methods and the baseline VQ-TensoRF on the Synthetic-NeRF dataset. The upper figure illustrates PSNR against file size, and the lower figure showcases SSIM in relation to file size. 6Neural NeRF Compression UncompressedVQ-TensoRF ECTensoRF-L ECTensoRF-H Figure 3: Qualitative results on Chair and Mic scenes from the Synthetic-NeRF dataset. From left to right: uncompressed, VQ-TensorF (average size 3.6 MB), ECTensoRF-L (3.4 MB), ECTensoRF-H (1.6 MB). Our decompressed renderings are barely distinguishable in quality from both uncompressed and VQ-compressed versions at a significantly reduced file size. Figure 2 shows that our ECTensoRF-L outpaces VQ- TensoRF across various levels of compression in Synthetic- NeRF dataset with both PSNR and SSIM metrics. Rate- distortion curves for other datasets can be found in the Ap- pendix A.2. Moreover, Table 2 shows the relative improvement of our method over VQ-TensoRF using Bjontegaard Delta (BD) BD-PSNR and BD-rate metrics (Bjontegaard, 2001), high- lighting that our model achieves better PSNR and bit-rate across various compression levels. Training and rendering time.Training an uncompressed TensoRF model for a scene from the Synthetic-NeRF dataset takes around 15 minutes on an NVIDIA A6000 GPU. Run- ning on top of that, our compression method takes an ad- ditional 40 minutes. Our framework is slower than the baseline VQ-TensoRF, which runs in 7 minutes on the same hardware. Regarding rendering, our approach adds a negli- gible overhead of roughly 2 seconds for the decompression of parameters. Once decompressed, the rendering procedure is the same as TensoRF. Compression details. The average storage size break- down of our model on the Synthetic-NeRF dataset (with the configuration from Table 1) is provided in the Table 3. For the feature planes, we compresse them with the learned entropy model. All the other components (the renderer MLP, decoder, density/appearance vectors, learned masks, entropy Table 3: Storage size breakdown. Component Size (MB) Feature planes 1.657 Decoder 1.380 Other components 0.366 Total 3.403 bottleneck parameters and model config) are packed into a single file and compressed with LZ77 (ziv, 1977). 5. Ablation Studies We conduct experiments to verify our design choices. We test on the Synthetic-NeRF datasets, with our ECTensoRF-L architecture. 5.1. Advantages of per-scene optimization As outlined in Section 3.1, our compression methodology is optimized on a per-scene basis. However, this raises the question: how is the performance of traditional nonlin- ear transform coding on TensoRF compression, even on a small-scale dataset? To address this, we conduct a compar- ative analysis. We compress the TensoRF model by first training a compression network using an encoder-decoder architecture, similar to traditional nonlinear transform cod- 7Neural NeRF Compression 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 File Size (MB) 24 26 28 30 32 34 36PSNR (dB) ECTensoRF-L ECTensoRF-H NTC Figure 4: Rate-distortion comparison between traditional nonlinear transform coding (green), trained across 7 scenes, and our per-scene compression methods (orange, blue). ing. Specifically, we train the compression network on seven scenes from the Synthetic-NeRF dataset and tested the trained model on the remaining scene (Lego). We com- pare this approach with our per-scene optimized model. Figure 4 shows the results of this experiment. Compared to per-scene training, pre-trained NTC suffers from inferior reconstruction quality. More specifically, the maximum PSNR that the pre-trained NTC can achieve is only 24.52 dB, which is 12.03 dB lower than the uncompressed PSNR value (36.55 dB) and also much lower than the PSNR values of per-scene trained models. However, we note that the major advantage of pre-trained NTC is a much faster compression time. Using a pre-trained NTC model also avoids the need to transmit the entropy model and the decoder, as we assume that the receiver always has access to them, which is similar to the image compression setting. 5.2. Ablation on other design choices Using the encoder We first show the sub-optimal perfor- mance of ECTensoRF-L compression with an encoder. As discussed in Section 3.1, using an encoder leads to an irre- ducible amortization gap in optimization, and the resulting compression performance is worse, as shown in Figure 5. Training without Importance-Weighted Loss.We ex- amine the rate-distortion curves of ECTensoRF-L, trained both with and without importance weight, as depicted in Figure 5. At an identical PSNR of 32.98 dB, employing importance weight in training our model helps reduce the file size from 4.59 MB to 3.92 MB. The Effect of the Masked Entropy Model.To demon- strate the efficacy of our masked entropy model, we un- dertook a comparative analysis between the compression 2.0 2.5 3.0 3.5 4.0 4.5 File Size (MB) 31.25 31.50 31.75 32.00 32.25 32.50 32.75 33.00PSNR (dB) ECTensoRF-L ECTensoRF-L with Encoder Uncompressed 2.0 2.5 3.0 3.5 4.0 4.5 File Size (MB) 31.25 31.50 31.75 32.00 32.25 32.50 32.75 33.00PSNR (dB) ECTensoRF-L ECTensoRF-L with Factorized Prior ECTensoRF-L without Importance Weight Uncompressed Figure 5: Ablation studies. Top: rate-distortion comparison of our approach against a version with encoder, trained on a single scene. Bottom: comparisons between model versions with factorized prior and without importance weight. performance of ECTensoRF-L using the conventional factor- ized prior (Ball´e et al., 2016; 2018) and our masked model. The results related to rate distortion curves can be found in the bottom plot of Figure 5. It’s noteworthy that, due to the additional overhead intro- duced by sending the masks, our results lag slightly behind the factorized prior in a low-rate setting. Yet, in medium to high-rate regimes, our prior emerges superior compared to the traditional factorized prior. To illustrate, for a PSNR value of 32.98 dB, the compressed file with the factorized prior occupies 4.26 MB. In contrast, our method employing the proposed masked entropy model results in a reduced file size of 3.92 MB. To further understand the behavior of our masked entropy model, we visualize the masks learned for the Chair and Mic scene from Synthetic-NeRF dataset in Figure 6. We can observe that the masks resemble the rendering objects when viewed from different angles, and they inherently ignore the background. This behavior is similar to the pruning 8Neural NeRF Compression Figure 6: Ablation studies. We show the sparsity masks of our entropy model learned on the Chair and Mic scene. strategies employed in prior grid-based NeRF compression works (Li et al., 2023a;b; Deng & Tartaglione, 2023). More experimental results. We further conduct experi- ments on different latents initialization and end-to-end train- ing in the Appendix A.2. 6. Related Works and Discussion Grid-based NeRF compression. Since storage cost is a significant challenge of grid-based NeRF, several meth- ods were proposed to solve this problem. Li et al. (2023a) introduces a three-stage approach, integrating voxel prun- ing and vector quantization (Gray, 1984) through a learn- able codebook. Similarly, Re:NeRF (Deng & Tartaglione, 2023) employs voxel pruning, but adopts a strategy of se- quentially removing and reintegrating parameters to prevent a significant drop in performance. Meanwhile, Takikawa et al. (2022) adopts the codebook idea from Instant-NGP (M¨uller et al., 2022), but substitutes hash-based encoding with a learned mapping that associates grid positions to corresponding codebook indices. However this approach requires considerable training memory. Li et al. (2023b) ap- plies downsampling to the voxels and employs a network to enhance render quality. Our method shares some similarity to Li et al. (2023b), but we learn the downsampled latent codes with a novel entropy model to effectively compress them. Additionally, while our masked factorized prior also resembles the pruning mechanism used in previous works, our method differentiates itself by adaptively learning the masks instead of relying on fixed thresholds. Neural compression for NeRF. Applying neural com- pression to NeRF is a relatively young field. Bird et al. (2021) learns an entropy model to compress the MLP-based NeRF (Mildenhall et al., 2021) network weights, based on the prior model compression work of Oktay et al. (2019). In contrast, our work focuses on compressing the feature grids of grid-based NeRF. We additionally improve the conven- tional compression procedure and propose a novel entropy model. Concurrent to our work, Li et al. (2024) also applies neural compression to TensoRF by leveraging a pretrained image comression network. Discussion. Throughout this paper, our emphasis has been on applying neural compression techniques specifically to TensoRF. Nonetheless, our method has the potential to be applied to other grid-based NeRF methods beyond just Ten- soRF, such as Triplanes (Chan et al., 2022; Fridovich-Keil et al., 2023), Factor Fields (Chen et al., 2023) or DVGO (Sun et al., 2022). Taking DVGO as an example, we can learn a 4D latent code and have an entropy model to model its probability density. Then a decoder may decode this 4D latent code to render the scene. 7. Conclusion In this study, we present a novel approach to applying neural compression to the TensoRF model, a prominent grid-based NeRF method. Our approach adapts traditional neural com- pression techniques, commonly used in image and video compression, to NeRF models. We develop an efficient per-scene optimization scheme and propose various designs, such as importance-weighted feature reconstruction and a masked entropy model. Our experiments demonstrate that we can significantly reduce storage requirements of a NeRF model with only a minimal compromise in rendering qual- ity, and outperform previous NeRF compression baselines. More importantly, our compression method only adds mini- mal overhead to the rendering process. Limitation and future work.One limitation of our neural compression approach is the longer training time compared to the baseline VQ-TensoRF, as mentioned in Section 4. Additionally, the final compressed model still includes the cost of transmitting the decoder. Future work could focus on reducing compression time, learning a network compression model (Oktay et al., 2019; Girish et al., 2022) to compress the decoder network, and applying our method to other NeRF architectures. Impact Statement Neural compression is a collection of methods that advance data compression with end-to-end learning approaches. Bi- ased training data may influence how models reconstruct data and may lead to misrepresentations, e.g., of individuals, especially at low bitrates. 9Neural NeRF Compression Acknowledgements The authors acknowledge support from the National Sci- ence Foundation (NSF) under an NSF CAREER Award (2047418), award numbers 2003237 and 2007719, by the Department of Energy under grant DE-SC0022331, the IARPA WRIV A program, and by gifts from Qualcomm and Disney. We also thank Justus Will for his meticulous proofreading and valuable suggestions for this paper. References A universal algorithm for sequential data compression. IEEE Transactions on information theory, 23(3):337–343, 1977. Ball´e, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2016. Ball´e, J., Minnen, D., Singh, S., Hwang, S. J., and Johnston, N. Variational image compression with a scale hyperprior. In International Conference on Learning Representations, 2018. Ball´e, J., Chou, P. A., Minnen, D., Singh, S., Johnston, N., Agustsson, E., Hwang, S. J., and Toderici, G. Nonlinear transform coding. IEEE Journal of Selected Topics in Signal Processing, 15(2):339–353, 2020. Barron, J. T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., and Srinivasan, P. P. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5855–5864, 2021. Bengio, Y ., L´eonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for con- ditional computation. arXiv preprint arXiv:1308.3432, 2013. Bird, T., Ball´e, J., Singh, S., and Chou, P. A. 3d scene com- pression through entropy penalized neural representation functions. In 2021 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2021. Bjontegaard, G. Calculation of average psnr differences between rd-curves. ITU SG16 Doc. VCEG-M33, 2001. Campos, J., Meierhans, S., Djelouah, A., and Schroers, C. Content adaptive optimization for neural image compres- sion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 0–0, 2019. Chan, E. R., Lin, C. Z., Chan, M. A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L. J., Tremblay, J., Khamis, S., et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022. Chen, A., Xu, Z., Geiger, A., Yu, J., and Su, H. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision, pp. 333–350. Springer, 2022. Chen, A., Xu, Z., Wei, X., Tang, S., Su, H., and Geiger, A. Factor fields: A unified framework for neural fields and beyond. arXiv preprint arXiv:2302.01226, 2023. Cheng, Z., Sun, H., Takeuchi, M., and Katto, J. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Cremer, C., Li, X., and Duvenaud, D. Inference subopti- mality in variational autoencoders. In International Con- ference on Machine Learning, pp. 1078–1086. PMLR, 2018. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., and Farhadi, A. Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13142– 13153, 2023. Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A., Laforte, C., V oleti, V ., Gadre, S. Y ., et al. Objaverse-xl: A universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. Deng, C. L. and Tartaglione, E. Compressing explicit voxel grid representations: fast nerfs become also small. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pp. 1236–1245, 2023. Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., and Kanazawa, A. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 5501–5510, 2022. Fridovich-Keil, S., Meanti, G., Warburg, F. R., Recht, B., and Kanazawa, A. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12479–12488, 2023. Gershman, S. and Goodman, N. Amortized inference in probabilistic reasoning. In Proceedings of the annual meeting of the cognitive science society, volume 36, 2014. 10Neural NeRF Compression Girish, S., Gupta, K., Singh, S., and Shrivastava, A. Lilnetx: Lightweight networks with extreme model compression and structured sparsification. In The Eleventh Interna- tional Conference on Learning Representations, 2022. Gray, R. Vector quantization. IEEE Assp Magazine, 1(2): 4–29, 1984. Jang, E., Gu, S., and Poole, B. Categorical reparameteriza- tion with gumbel-softmax. In International Conference on Learning Representations, 2016. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In International Conference on Learning Repre- sentations, 2014. Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. Self-normalizing neural networks. Advances in neural information processing systems, 30, 2017. Knapitsch, A., Park, J., Zhou, Q.-Y ., and Koltun, V . Tanks and temples: Benchmarking large-scale scene reconstruc- tion. ACM Transactions on Graphics (ToG), 36(4):1–13, 2017. Kolda, T. G. and Bader, B. W. Tensor decompositions and applications. SIAM review, 51(3):455–500, 2009. Li, L., Shen, Z., Wang, Z., Shen, L., and Bo, L. Compressing volumetric radiance fields to 1 mb. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4222–4231, 2023a. Li, L., Wang, Z., Shen, Z., Shen, L., and Tan, P. Compact real-time radiance fields with neural codebook. In 2023 IEEE International Conference on Multimedia and Expo (ICME), pp. 2189–2194. IEEE, 2023b. Li, S., Li, H., Liao, Y ., and Yu, L. Nerfcodec: Neu- ral feature compression meets neural radiance fields for memory-efficient scene representation. arXiv preprint arXiv:2404.02185, 2024. Liu, L., Gu, J., Zaw Lin, K., Chua, T.-S., and Theobalt, C. Neural sparse voxel fields. Advances in Neural Informa- tion Processing Systems, 33:15651–15663, 2020. Marino, J., Yue, Y ., and Mandt, S. Iterative amortized infer- ence. In International Conference on Machine Learning, pp. 3403–3412. PMLR, 2018. Matsubara, Y ., Yang, R., Levorato, M., and Mandt, S. Su- pervised compression for resource-constrained edge com- puting systems. In Proceedings of the IEEE/CVF Win- ter Conference on Applications of Computer Vision, pp. 2685–2695, 2022. Mildenhall, B., Srinivasan, P. P., Ortiz-Cayon, R., Kalantari, N. K., Ramamoorthi, R., Ng, R., and Kar, A. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 38(4):1–14, 2019. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance fields for view synthesis. Communica- tions of the ACM, 65(1):99–106, 2021. Minnen, D., Ball´e, J., and Toderici, G. D. Joint autoregres- sive and hierarchical priors for learned image compres- sion. Advances in neural information processing systems, 31, 2018. Mitchell, T. J. and Beauchamp, J. J. Bayesian variable selection in linear regression. Journal of the american statistical association, 83(404):1023–1032, 1988. M¨uller, T., Evans, A., Schied, C., and Keller, A. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4): 1–15, 2022. Oktay, D., Ball´e, J., Singh, S., and Shrivastava, A. Scalable model compression by entropy penalized reparameteriza- tion. In International Conference on Learning Represen- tations, 2019. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Sun, C., Sun, M., and Chen, H.-T. Direct voxel grid op- timization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp. 5459–5469, 2022. Takikawa, T., Evans, A., Tremblay, J., M¨uller, T., McGuire, M., Jacobson, A., and Fidler, S. Variable bitrate neural fields. In ACM SIGGRAPH 2022 Conference Proceed- ings, pp. 1–9, 2022. Wallace, G. K. The jpeg still picture compression standard. Communications of the ACM, 34(4):30–44, 1991. Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: from error visibility to struc- tural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. Yang, R. and Mandt, S. Lossy image compression with conditional diffusion models. Advances in Neural Infor- mation Processing Systems, 36, 2023a. 11Neural NeRF Compression Yang, R., Yang, Y ., Marino, J., and Mandt, S. Insights from generative modeling for neural video compression. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 2023a. Yang, Y . and Mandt, S. Computationally-efficient neural im- age compression with shallow decoders. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 530–540, 2023b. Yang, Y ., Bamler, R., and Mandt, S. Improving inference for neural image compression. Advances in Neural Infor- mation Processing Systems, 33:573–584, 2020. Yang, Y ., Mandt, S., Theis, L., et al. An introduction to neural data compression. Foundations and Trends® in Computer Graphics and Vision, 15(2):113–200, 2023b. Yu, A., Li, R., Tancik, M., Li, H., Ng, R., and Kanazawa, A. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5752–5761, 2021. Zhang, K., Riegler, G., Snavely, N., and Koltun, V . Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020. 12Neural NeRF Compression A. Appendix A.1. Algorithm Algorithm 1TensoRF-VM compression Input: Pretrained TensoRF-VM model Output: Compressed TensoRF-VM model Calculate {Wi}3 i=1 using Eq 4 and 5 Initialize {Zi}3 i=1 as 0-tensors Initialize decoder D and entropy model P with masks parameters {π0 Mi }3 i=1 and {π1 Mi }3 i=1 while not converged do Sample {Mi}3 i=1 using Gumbel-Softmax as in Eq 8 Reconstruct {ˆPi}3 i=1 by Eq 7 Render the scene with {ˆPi}3 i=1 Calculate the loss in Eq 9 and update the model end while A.2. More experimental results A.2.1. R ATE-DISTORTION COMPARISON ON OTHER DATASETS We further compare the rate-distortion curves of ECTensoRF and the baseline VQ-TensoRF on the Synthetic-NSVF, LLFF and Tanks&Temples datasets in Figure 7, 8 and 9. 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 File Size (MB) 33.0 33.5 34.0 34.5 35.0 35.5 36.0 36.5PSNR (dB) VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 File Size (MB) 0.960 0.965 0.970 0.975 0.980SSIM VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed Figure 7: Comparison on Synthetic-NSVF dataset. A.2.2. A DDITIONAL EXPERIMENTS Latent initialization. We compare the performance of Gaussian initialization and Zero initialization of the latents code. The results are shown in Table 4. 13Neural NeRF Compression 1 2 3 4 5 6 7 8 9 File Size (MB) 25.6 25.8 26.0 26.2 26.4 26.6PSNR (dB) VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed 2 3 4 5 6 7 8 File Size (MB) 0.79 0.80 0.81 0.82 0.83 0.84SSIM VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed Figure 8: Comparison on LLFF dataset. 1.0 1.5 2.0 2.5 3.0 3.5 File Size (MB) 26.75 27.00 27.25 27.50 27.75 28.00 28.25 28.50PSNR (dB) VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed 1.0 1.5 2.0 2.5 3.0 3.5 File Size (MB) 0.885 0.890 0.895 0.900 0.905 0.910 0.915 0.920SSIM VQ-TensoRF ECTensoRF-L (ours) ECTensoRF-H (ours) Uncompressed Figure 9: Comparison on Tanks and Temples dataset. Table 4: Comparison of Zero Initialization vs. Gaussian Initialization λ Values Zero Initialization Gaussian Initialization PSNR (dB) Size (MB) PSNR (dB) Size (MB) 2e-2 31.31 1.86 31.13 1.85 1e-2 31.80 1.99 31.75 1.99 5e-3 32.25 2.20 32.26 2.20 1e-3 32.83 3.00 32.83 3.01 5e-4 32.93 3.40 32.92 3.42 2e-4 32.98 3.92 32.98 3.94 1e-4 33.00 4.24 32.99 4.26 End-to-end training. We conduct experiments to compare the performance of our two-stage training (by first using a pre-trained TensoRF model, and train the compression model) and a single stage (by training the compression model from scratch). We show the results in Table 5. 14Neural NeRF Compression Table 5: Comparison of End-to-End Training vs. Two Stages Training λ Values End-to-End Training Two Stages Training PSNR (dB) Size (MB) PSNR (dB) Size (MB) 2e-2 25.86 1.69 31.31 1.86 1e-2 28.30 1.71 31.80 1.99 5e-3 30.05 1.81 32.25 2.20 1e-3 31.29 2.39 32.83 3.00 5e-4 31.53 2.57 32.93 3.40 2e-4 31.86 2.69 32.98 3.92 1e-4 31.80 2.95 33.00 4.24 Hyperprior model. We also perform experiments using a version of hyperprior model (Ball ´e et al., 2018) with our masking mechanism. More specifically, we apply masking on both the hyper-latents and latents. Both type of latents are directly optimized without using amortized inference. The hyper decoder has two transposed convolutional layers with SELU activation. We show the result on NeRF-Synthetic on Table 6. Table 6: Comparison of ECTensorF-L with and without Hyperprior λ Values ECTensorF-L + Hyperprior ECTensorF-L PSNR (dB) Size (MB) PSNR (dB) Size (MB) 2e-2 31.31 1.92 31.31 1.86 1e-2 31.92 2.04 31.80 1.99 5e-3 32.35 2.25 32.25 2.20 1e-3 32.85 2.97 32.83 3.00 5e-4 32.93 3.32 32.93 3.40 2e-4 32.98 3.72 32.98 3.92 1e-4 33.00 3.95 33.00 4.24 At lower bit rates, the hyperprior is slightly worse than the ECTensoRF-L baseline because of the irreducible cost to transmit the hyper decoder and hyper entropy model. At higher bit rates, the compression performance with the hyperprior method is better than using only a single entropy model, which aligns with prior observations in image compression (Ball´e et al., 2018). Preliminary results for Factor Fields. We show the potential of applying our method to other grid-based NeRF architectures. We choose Factor Fields (Chen et al., 2023) to experiment with. We show the result of our method for Factor Fields in Table 7. Note that for Factor Fields, we compress the basis 4D tensors and do not compress the coefficient 4D tensors. Table 7: Factor Fields experiments λ Values PSNR (dB) Rate (MB) 1e-3 26.19 1.12 1e-4 29.67 1.23 1e-5 31.35 1.82 Uncompressed 33.09 18.89 A.2.3. M ORE QUALITATIVE RESULTS We show qualitative results on all scenes from Synthetic-NeRF, Synthetic-NSVF, LLFF and Tanks&Temples datasets in Figure 10, 11, 12 and 13. 15Neural NeRF Compression Figure 10: Qualitative results on Tanks and Temples dataset. From left to right: TensoRF, ECTensoRF-L, ECTensoRF-H, ECTensoRF-L difference and ECTensoRF-H difference. 16Neural NeRF Compression Figure 11: Qualitative results on Synthetic-NeRF dataset. From left to right: TensoRF, ECTensoRF-L, ECTensoRF-H, ECTensoRF-L difference and ECTensoRF-H difference. 17Neural NeRF Compression Figure 12: Qualitative results on Synthetic-NSVF dataset. From left to right: TensoRF, ECTensoRF-L, ECTensoRF-H, ECTensoRF-L difference and ECTensoRF-H difference. 18Neural NeRF Compression Figure 13: Qualitative results on LLFF dataset. From left to right: TensoRF, ECTensoRF-L, ECTensoRF-H, ECTensoRF-L difference and ECTensoRF-H difference. 19",
      "meta_data": {
        "arxiv_id": "2406.08943v1",
        "authors": [
          "Tuan Pham",
          "Stephan Mandt"
        ],
        "published_date": "2024-06-13T09:12:26Z",
        "pdf_url": "https://arxiv.org/pdf/2406.08943v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the significant storage overhead introduced by explicit grid representations in Neural Radiance Fields (NeRFs), which improve rendering quality and speed but increase storage costs. The main contribution is a novel method for efficiently compressing grid-based NeRF models, specifically TensoRF-VM, using a non-linear transform coding paradigm with neural compression for feature grids. The proposed approach employs an encoder-free, end-to-end optimized method for individual scenes, utilizing lightweight decoders, an importance-weighted rate-distortion objective, and a sparse entropy model with a masking mechanism. Experimental results demonstrate that this method surpasses existing works in terms of grid-based NeRF compression efficacy and reconstruction quality, significantly reducing model size while maintaining high rendering quality.",
        "methodology": "The method is based on non-linear transform coding, applied to compress the 2D feature planes of the TensoRF-VM model. Instead of a conventional auto-encoder, it uses an encoder-free, per-scene optimization approach, where latent codes, a lightweight decoder (two-layer transposed convolutional network with SELU activation), and an entropy model are jointly optimized for each scene. A key aspect is the importance-weighted training loss, combining NeRF rendering loss with an L2 feature plane reconstruction loss re-weighted by importance maps derived from rendering importance scores (Tk · αk). A novel masked entropy model, employing a spike-and-slab prior and binary masks learned via the Gumbel-Softmax trick, is introduced to exploit spatial sparsity and selectively compress informative feature grid points.",
        "experimental_setup": "Experiments were conducted on four datasets: Synthetic-NeRF, Synthetic-NSVF (both 800x800 resolution), LLFF (1008x756), and Tanks and Temples (1920x1080). A TensoRF-VM model was pre-trained for each scene, then compressed using the proposed method. Baselines included the original MLP-based NeRF, uncompressed TensoRF-CP and TensoRF-VM, and two prior compression methods for TensoRF-VM: VQ-TensoRF (voxel pruning/vector quantization) and Re:TensoRF (voxel pruning/sequential parameter removal). Evaluation metrics were PSNR (dB) and SSIM for reconstruction quality, and compressed file size (MB) for compression rate. Two configurations, ECTensoRF-H (high compression, 192 latent channels, 96 decoder hidden channels) and ECTensoRF-L (low compression, 384 latent channels, 192 decoder hidden channels), were tested. Models were trained for 30,000 iterations using Adam optimizer on NVIDIA RTX A6000 GPUs with PyTorch.",
        "limitations": "One limitation of the neural compression approach is its longer training time (approximately 40 minutes per scene) compared to baselines like VQ-TensoRF (7 minutes). Additionally, the final compressed model still includes the cost of transmitting the decoder parameters, which contributes to the overall file size.",
        "future_research_directions": "Future work could focus on reducing the compression training time. Another direction is to learn a network compression model to compress the decoder network itself, further reducing its transmission cost. The authors also suggest applying their method to other grid-based NeRF architectures beyond TensoRF, such as Triplanes, Factor Fields, or DVGO, by adapting the latent code and entropy model designs to these different representations."
      }
    },
    {
      "title": "BiPointNet: Binary Neural Network for Point Clouds",
      "abstract": "To alleviate the resource constraint for real-time point cloud applications\nthat run on edge devices, in this paper we present BiPointNet, the first model\nbinarization approach for efficient deep learning on point clouds. We discover\nthat the immense performance drop of binarized models for point clouds mainly\nstems from two challenges: aggregation-induced feature homogenization that\nleads to a degradation of information entropy, and scale distortion that\nhinders optimization and invalidates scale-sensitive structures. With\ntheoretical justifications and in-depth analysis, our BiPointNet introduces\nEntropy-Maximizing Aggregation (EMA) to modulate the distribution before\naggregation for the maximum information entropy, and Layer-wise Scale Recovery\n(LSR) to efficiently restore feature representation capacity. Extensive\nexperiments show that BiPointNet outperforms existing binarization methods by\nconvincing margins, at the level even comparable with the full precision\ncounterpart. We highlight that our techniques are generic, guaranteeing\nsignificant improvements on various fundamental tasks and mainstream backbones.\nMoreover, BiPointNet gives an impressive 14.7x speedup and 18.9x storage saving\non real-world resource-constrained devices.",
      "full_text": "Published as a conference paper at ICLR 2021 BIPOINT NET: B INARY NEURAL NETWORK FOR POINT CLOUDS Haotong Qin∗1,2, Zhongang Cai∗3, Mingyuan Zhang∗3, Yifu Ding1, Haiyu Zhao3, Shuai Yi3, Xianglong Liu†1, Hao Su4 1State Key Lab of Software Development Environment, Beihang University 2Shen Yuan Honors College, Beihang University 3SenseTime Research 4University of California, San Diego {qinhaotong,xlliu,zjdyf}@buaa.edu.cn {caizhongang, zhangmingyuan, zhaohaiyu, yishuai}@sensetime.com haosu@eng.ucsd.edu ABSTRACT To alleviate the resource constraint for real-time point cloud applications that run on edge devices, in this paper we present BiPointNet, the ﬁrst model binarization approach for efﬁcient deep learning on point clouds. We discover that the im- mense performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degra- dation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justiﬁcations and in-depth analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information en- tropy, and Layer-wise Scale Recovery (LSR) to efﬁciently restore feature repre- sentation capacity. Extensive experiments show that BiPointNet outperforms ex- isting binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic, guaranteeing signiﬁcant improvements on various fundamental tasks and main- stream backbones. Moreover, BiPointNet gives an impressive14.7×speedup and 18.9×storage saving on real-world resource-constrained devices. 1 I NTRODUCTION With the advent of deep neural networks that directly process raw point clouds (PointNet (Qi et al., 2017a) as the pioneering work), great success has been achieved in learning on point clouds (Qi et al., 2017b; Li et al., 2018; Wang et al., 2019a; Wu et al., 2019; Thomas et al., 2019; Liu et al., 2019b; Zhang et al., 2019b). Point cloud applications, such as autonomous driving and augmented reality, often require real-time interaction and fast response. However, computation for such applications is usually deployed on resource-constrained edge devices. To address the challenge, novel algorithms, such as Grid-GCN (Xu et al., 2020b), RandLA-Net (Hu et al., 2020), and PointV oxel (Liu et al., 2019d), have been proposed to accelerate those point cloud processing networks. While signiﬁcant speedup and memory footprint reduction have been achieved, these works still rely on expensive ﬂoating-point operations, leaving room for further optimization of the performance from the model quantization perspective. Model binarization (Rastegari et al., 2016; Bulat & Tzimiropoulos, 2019; Hubara et al., 2016; Wang et al., 2020; Zhu et al., 2019; Xu et al., 2019) emerged as one of the most promising approaches to optimize neural networks for better computational and memory usage efﬁciency. Binary Neural Networks (BNNs) leverage 1) compact binarized parameters that take small memory space, and 2) highly efﬁcient bitwise operations which are far less costly compared to the ﬂoating-point counterparts. Despite that in 2D vision tasks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014; Girshick, 2015; Russakovsky et al., 2015; Wang et al., 2019b; ∗equal contributions †corresponding author 1 arXiv:2010.05501v4  [cs.CV]  11 Jun 2021Published as a conference paper at ICLR 2021 n×3 n×3 input transform n×64 shared BiMLPs n×64 feature transform n×1024shared BiMLPs 1024 global feature input points EMA-max BiMLPs output scores Layer-wise Scale RecoveryEntropy-Maximizing Aggregation input feature transformation  unit 𝝉 𝝋 aggregation  unit aggregated featureoutput feature n×m binarized input feature m×k⨀ binarized weight original feature output feature 𝜶 learnable layer -wise scaling factor Figure 1: Overview of our BiPointNet on PointNet base model, applying Entropy-Maximizing Ag- gregation (EMA) and Layer-wise Scale Recovery (LSR). EMA consists of the transformation unit and the aggregation unit for maximizing the information entropy of feature after binarization. LSR with the learnable layer-wise scaling factor αis applied to address the scale distortion of bi-linear layers (which form the BiMLPs), ﬂexibly restore the distorted output to reasonable values Zhang et al., 2021) has been studied extensively by the model binarization community, the methods developed are not readily transferable for 3D point cloud networks due to the fundamental differ- ences between 2D images and 3D point clouds. First, to gain efﬁciency in processing unordered 3D points, many point cloud learning methods rely heavily on pooling layers with large receptive ﬁeld to aggregate point-wise features. As shown in PointNet (Qi et al., 2017b), global pooling provides a strong recognition capability. However, this practice poses challenges for binarization. Our analy- ses show that the degradation of feature diversity, a persistent problem with binarization (Liu et al., 2019a; Qin et al., 2020b; Xie et al., 2017), is signiﬁcantly ampliﬁed by the global aggregation func- tion (Figure 2), leading to homogenization of global features with limited discriminability. Second, the binarization causes immense scale distortion at the point-wise feature extraction stage, which is detrimental to model performance in two ways: the saturation of forward-propagated features and backward-propagated gradients hinders optimization, and the disruption of the scale-sensitive structures (Figure 3) results in the invalidation of their designated functionality. In this paper, we provide theoretical formulations of the above-mentioned phenomenons and obtain insights through in-depth analysis. Such understanding allows us to propose a method that turns full- precision point cloud networks into extremely efﬁcient yet strong binarized models (see the overview in Figure 1). To tackle the homogenization of the binarized features after passing the aggregation function, we study the correlation between the information entropy of binarization features and the performance of point cloud aggregation functions. We thus propose Entropy-Maximizing Aggrega- tion (EMA) that shifts the feature distribution towards the statistical optimum, effectively improving expression capability of the global features. Moreover, given maximized information entropy, we further developLayer-wise Scale Recovery(LSR) to efﬁciently restore the output scale that enhances optimization, which allows scale-sensitive structures to function properly. LSR uses only one learn- able parameter per layer, leading to negligible storage increment and computation overhead. Our BiPointNet is the ﬁrst binarization approaches to deep learning on point clouds, and it outper- forms existing binarization algorithms for 2D vision by convincing margins. It is even almost on par (within ∼1-2%) with the full-precision counterpart. Although we conduct most analysis on the PointNet baseline, we show that our methods are generic and can be readily extendable to other pop- ular backbones, such as PointNet++ (Qi et al., 2017b), PointCNN (Li et al., 2018), DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019), which are the representatives of mainstream cate- gories of point cloud feature extractors. Moreover, extensive experiments on multiple fundamental tasks on the point cloud, such as classiﬁcation, part segmentation, and semantic segmentation, high- light that our BiPointNet is task-agnostic. Besides, we highlight that our EMA and LSR are efﬁcient and easy to implement in practice: in the actual test on popular edge devices, BiPointNet achieves 14.7×speedup and 18.9×storage savings compared to the full-precision PointNet. Our code is released at https://github.com/htqin/BiPointNet. 2 R ELATED WORK Network Binarization.Recently, various quantization methods for neural networks have emerged, such as uniform quantization (Gong et al., 2019; Zhu et al., 2020), mixed-precision quantization (Wu 2Published as a conference paper at ICLR 2021 et al., 2018; Yu et al., 2020), and binarization. Among these methods, binarization enjoys compact binarized parameters and highly efﬁcient bitwise operations for extreme compression and accelera- tion (Rastegari et al., 2016; Qin et al., 2020a). In general, the forward and backward propagation of binarized models in the training process can be formulated as: Forward :b= sign(x) = {+1, if x≥0 −1, otherwise Backward : gx = {gb, if x∈(−1,1) 0, otherwise (1) where x denotes the element in ﬂoating-point weights and activations, b denotes the element in binarized weights Bw and activations Ba. gx, and gb donate the gradient ∂C ∂x and ∂C ∂b , respectively, where C is the cost function for the minibatch. In forward propagation, sign function is directly applied to obtain the binary parameters. In backward propagation, the Straight-Through Estimator (STE) (Bengio et al., 2013) is used to obtain the derivative of thesign function, avoiding getting all zero gradients. The existing binarization methods are designed to obtain accurate binarized networks by minimizing the quantization error (Rastegari et al., 2016; Zhou et al., 2016; Lin et al., 2017), improving loss function (Ding et al., 2019; Hou et al., 2017), reducing the gradient error (Liu et al., 2018; 2020), and designing novel structures and pipelines (Martinez et al., 2020). Unfortunately, we show in Sec 3 that these methods, designed for 2D vision tasks, are not readily transferable to 3D point clouds. Deep Learning on Point Clouds.PointNet (Qi et al., 2017a) is the ﬁrst deep learning model that processes raw point clouds directly. The basic building blocks proposed by PointNet such as MLP for point-wise feature extraction and max pooling for global aggregation (Guo et al., 2020) have become the popular design choices for various categories of newer backbones: 1) the pointwise MLP-based such as PointNet++ (Qi et al., 2017b); 2) the graph-based such as DGCNN (Xu et al., 2020b); 3) the convolution-based such as PointCNN (Li et al., 2018), PointConv (Wu et al., 2019) RS-CNN (Liu et al., 2019c) and KP-Conv (Thomas et al., 2019). Recently, methods are proposed for efﬁcient deep learning on point clouds through novel data structuring (Xu et al., 2020b), faster sampling (Hu et al., 2020), adaptive ﬁlters (Xu et al., 2020a), efﬁcient representation (Liu et al., 2019d) or convolution operation (Zhang et al., 2019b) . However, they still use expensive ﬂoating- point parameters and operations, which can be improved by binarization. 3 M ETHODS Binarized models operate on efﬁcient binary parameters, but often suffer large performance drop. Moreover, the unique characteristics of point clouds pose even more challenges. We observe there are two main problems: ﬁrst, aggregation of a large number of points leads to a severe loss of feature diversity; second, binarization induces an immense scale distortion, that undermines the functionality of scale-sensitive structures. In this section, we discuss our observations, and propose our BiPointNet with theoretical justiﬁcations. 3.1 B INARIZATION FRAMEWORK We ﬁrst give a brief introduction to our framework that binarizes a ﬂoating-point network. For example, deep learning models on point clouds typically contain multi-layer perceptrons (MLPs) for feature extraction. In contrast, the binarized models contain binary MLPs (BiMLPs), which are composed of binarized linear (bi-linear) layers. Bi-linear layers perform the extremely efﬁcient bitwise operations (XNOR and Bitcount) on the lightweight binary weight/activation. Speciﬁcally, the activation of the bi-linear layer is binarized to Ba, and is computed with the binarized weight Bw to obtain the output Z: Z = Ba ⊙Bw, (2) where ⊙denotes the inner product for vectors with bitwise operations XNOR and Bitcount. When Bw and Badenote the random variables inBw and Ba, we represent their probability mass function as pBw(bw), and pBa(ba). Moreover, we divide the BiPointNet into units for detailed discussions. In BiPointNet, the original data or feature X ∈ Rn×c ﬁrst enters the symmetric function Ω, which represents a composite function built by stacking several permutation equivariant and permutation invariant layers (e.g., nonlinear layer, bi-linear layer, max pooling). And then, the outputY ∈Rn×k is binarized to obtain 3Published as a conference paper at ICLR 2021 (a) Point-wise features to be aggregated. (b) Full-precision features aggregated  with max pooling. (d) Binarized features aggregated with  EMA.  (c) Binarized features aggregated with  max pooling.  Figure 2: Aggregation-induced feature homogenization. (a) shows the activation of each test sample in a batch of ModelNet40. In (b)-(d), the single feature vectors pooled from all points are mapped to colors. The diversity of colors represents the diversity of pooled features. The original aggregation design is incompatible with binarization, leading to the homogenization of output features in (c), whereas our proposed EMA retains high information entropy, shown in (d) the binary feature B ∈{−1,1}i×k, where itakes nwhen the feature is modeled independently and takes 1 when the feature is aggregated globally. The single unit is thus represented as B = sign(Y) = sign(Ω(X)). (3) Similarly, when B, Y and X denote the random variables sampled from B, Y and X, we represent their probability mass function as pB(b), pY(y) and pX(x). 3.2 E NTROPY -MAXIMIZING AGGREGATION Unlike images pixels that are arranged in regular lattices, point clouds are sets of points without any speciﬁc order. Hence, features are usually processed in a point-wise manner and aggregated explicitly through pooling layers. Our study shows that the aggregation function is a performance bottleneck of the binarized model, due to severe homogenization as shown in Figure 2. We apply information theory (Section 3.2.1) to quantify the effect of the loss of feature diversity, and ﬁnd that global feature aggregation leads to a catastrophic loss of information entropy. In Sec- tion 3.2.2, we propose the concept of Entropy-Maximizing Aggregation (EMA) that gives the sta- tistically maximum information entropy to effectively tackle the feature homogenization problem. 3.2.1 A GGREGATION -INDUCED FEATURE HOMOGENIZATION Ideally, the binarized tensor B should reﬂect the information in the original tensor Y as much as possible. From the perspective of information, maximizing mutual information can maximize the information ﬂow from the full-precision to the binarized parameters. Hence, our goal is equivalent to maximizing the mutual information I(Y; B) of the random variables Y and B: arg max Y,B I(Y; B) = H(B) −H(B |Y) (4) where H(B) is the information entropy, and H(B |Y) is the conditional entropy of B given Y. H(B |Y) = 0 as we use the deterministic sign function as the quantizer in binarization (see Section A.1 for details). Hence, the original objective function Eq. (4) is equivalent to: arg max B HB(B) = − ∑ b∈B pB(b) logpB(b), (5) where Bis the set of possible values of B. We then study the information properties of max pool- ing, which is a common aggregation function used in popular point cloud learning models such as PointNet. Let the max pooling be the last layer φof the multi-layer stacked Ω, and the input of φ is deﬁned as Xφ. The data ﬂow of Eq. (3) can be further expressed as B = sign(φ(Xφ)), and the information entropy HB of binarized feature Bcan be expressed as HB(Xφ) =− (∑ xφ≥0 pXφ(xφ) )n log (∑ xφ≥0 pXφ(xφ) )n − ( 1− (∑ xφ≥0 pXφ(xφ) )n) log ( 1− (∑ xφ≥0 pXφ(xφ) )n) (6) where nis the number of elements aggregated by the max pooling, and Xφ is the random variable sampled from Xφ. The brief derivation of Eq (6) is shown in Appendix A.2. Theorem 1 shows the information properties of max pooling with the normal distribution input on the binarized network architecture. 4Published as a conference paper at ICLR 2021 Theorem 1 For input Xφ of max pooling φwith arbitrary distribution, the information entropy of the binarized output goes to zero as n goes to inﬁnity, i.e., lim n→+∞ HB = 0 . And there exists a constant c, for any n1 and n2, if n1 >n2 >c, we have HB,n1 <HB,n2 , where nis the number of elements to be aggregated. The proof of Theorem 1 is included in Appendix A.2, which explains the severe feature homoge- nization after global feature pooling layers. As the number of points is typically large (e.g. 1024 points by convention in ModelNet40 classiﬁcation task), it signiﬁcantly reduces the information en- tropy HB of binarized feature B, i.e., the information ofY is hardly retained in B, leading to highly similar output features regardless of the input features to pooling layer as shown in Figure 2. Furthermore, Theorem 1 provides a theoretical justiﬁcation for the poor performance of existing binarization methods, transferred from 2D vision tasks to point cloud applications. In 2D vision, the aggregation functions are often used to gather local features with a small kernel sizen(e.g. n= 4 in ResNet (He et al., 2016; Liu et al., 2018) and VGG-Net (Simonyan & Zisserman, 2014) which use 2 ×2 pooling kernels). Hence, the feature homogenization problem on images is not as signiﬁcant as that on point clouds. 3.2.2 EMA FOR MAXIMUM INFORMATION ENTROPY Therefore, we need a class of aggregation functions that maximize the information entropy of B to avoid the aggregation-induced feature homogenization. We study the correlation between the information entropy HB of binary random variable B and the distribution of the full-precision random variable Y. We notice that the sign function used in binarization has a ﬁxed threshold and decision levels, so we get Proposition 1 about information entropy of Band the distribution of Y. Proposition 1 When the distribution of the random variable Y satisﬁes ∑ y<0 pY(y) =∑ y≥0 pY(y) = 0.5, the information entropy HB is maximized. The proof of Proposition 1 is shown in Appendix A.3. Therefore, theoretically, there is a distribution of Y that can maximize the mutual information of Y and B by maximizing the information entropy of the binary tensor B, so as to maximally retain the information of Y in B. To maximize the information entropy HB, we propose the EMA for feature aggregation in BiPoint- Net. The EMA is not one, but a class of binarization-friendly aggregation layers. Modifying the aggregation function in the full-precision neural network to a EMA keeps the entropy maximized by input transformation. The deﬁnition of EMA is Y = EMA(Xφ) = ϕ(τ(Xφ)), (7) where ϕdenotes the aggregation function (e.g. max pooling and average pooling) andτ denotes the transformation unit. Note that a standard normal distribution N(0,1) is assumed for Xφ because batch normalization layers are placed prior to the pooling layers by convention. τ can take many forms; we discover that a simple constant offset is already effective. The offset shifts the input so that the output distribution satisﬁes ∑ y<0 pY(y) = 0 .5, to maximize the information entropy of binary feature B. The transformation unit τ in our BiPointNet can be deﬁned as τ(Xφ) = Xφ−δ∗. When max pooling is applied as ϕ, we obtain the distribution offset δ∗for the input Xφ that maxi- mizes the information entropy HB by solving the objective function arg max δ HB(δ) =− ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n log ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n − ( 1 − ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n) log ( 1 − ( ∑ xφ≥0 1√ 2π e− ( xφ−δ )2 2 )n) , (8) where n denotes the number of elements in each batch. For each n, we can obtain an optimized δ∗ max for Eq. (8), we include the pseudo code in the Appendix A.5. Moreover, we derive in the Appendix A.6 that when average pooling is used as ϕ, the solution to its objective function is expressed as δ = 0 . We thus obtain δ∗ avg = 0 . This means the solution 5Published as a conference paper at ICLR 2021 (a) (b) (c) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Full Prec. BNN BiPointNet Normal Gradient Ze ro Gradient Figure 4: (a) Information entropy of the aggregated features. With EMA, our BiPointNet achieves a higher information entropy. (b) Regularizer loss comparison. Our PointNet has a low loss, indicating that the scale distortion is reduced and T-Net is not disrupted. (c) Ratio of zero-gradient activations in back-propagation. LSR alleviates the scale distortion, enhancing the optimization process is not related to n. Hence, average pooling can be regarded as a ﬂexible alternative because its performance is independent of the input number n. In a nutshell, we provide two possible variants of ϕ: ﬁrst, we show that a simple shift is sufﬁcient to turn a max pooling layer into an EMA (EMA-max); second, average pooling can be directly used (EMA-avg) without modiﬁcation as a large number of points does not undermine its information entropy, making it adaptive to the dynamically changing number of point input. Note that modi- fying existing aggregation functions is only one way to achieve EMA; the theory also instructs the development of new binarization-friendly aggregation functions in the future. 3.3 O NE-SCALE -FITS -ALL : L AYER -WISE SCALE RECOVERY In this section, we show that binarization leads to feature scale distortion and study its cause. We conclude that the distortion is directly related to the number of feature channels. More importantly, we discuss the detriments of scale distortion from the perspectives of the functionality of scale- sensitive structures and the optimization. To address the severe scale distortion in feature due to binarization, we propose the Layer-wise Scale Recovery (LSR). In LSR, each bi-linear layer is added only one learnable scaling factor to recover the original scales of all binarized parameters, with negligible additional computational overhead and memory usage. 3.3.1 S CALE DISTORTION (a) Input point  cloud (b) PointNet (c) BiPointNet (d) BNN Figure 3: Scale Distortion. Fig- ures (b)-(d) show the trans- formed input. Compared with the input (a), the scales of (b) in full-precision PointNet and (c) in our BiPointNet are normal, while the scale of (d) in BNN is signiﬁcantly distorted The scale of parameters is deﬁned as the standard deviation σof their distribution. As we mentioned in Section 3.2, balanced bina- rized weights are used in the bi-linear layer aiming to maximize the entropy of the output after binarization, i.e., pBw(1) = 0 .5 and pBa(1) = 0.5. Theorem 2 When we let pBw(1) = 0 .5 and pBa(1) = 0 .5 in bi-linear layer to maximize the mutual information, for the binarized weight Bw ∈ {−1,+1}m×k and activation Ba ∈ {−1,+1}n×m, the probability mass function for the distribution of output Z can be represented as pZ(2i−m) = 0 .5mCi m,i ∈ {0,1,2,...,m }. The output has approximately a normal distribu- tion N(0,m). The proof of Theorem 2 is found in Appendix A.4. Theorem 2 shows that given the maximized information entropy, the scale of the output features is directly related to the number of feature channels. Hence, scale distortion is pervasive as a large number of channels is the design norm of deep learning neural networks for effective feature extraction. We discuss two major impacts of the scale distortion on the performance of binarized point cloud learning models. First, the scale distortion invalidates structures designed for 3D deep learning that 6Published as a conference paper at ICLR 2021 Table 1: Ablation study for our BiPointNet of various tasks on ModelNet40 (classiﬁcation), ShapeNet Parts (part segmentation), and S3DIS (semantic segmentation). EMA and LSR and com- plementary to each other, and they are useful across all three applications Method Bit-width Aggr. ModelNet40 ShapeNet Parts S3DIS OA mIoU mIoU OA Full Prec. 32/32 MAX 88.2 84.3 54.4 83.5 32/32 A VG 86.5 84.0 51.5 81.5 BNN 1/1 MAX 7.1 54.0 9.5 45.0 BNN-LSR 1/1 MAX 4.1 58.7 2.0 25.4 BNN-EMA 1/1 EMA-avg 11.3 53.0 9.9 46.8 1/1 EMA-max 16.2 47.3 8.5 47.2 Ours 1/1 EMA-avg 82.5 80.3 40.9 74.9 1/1 EMA-max 86.4 80.6 44.3 76.7 are sensitive to the scale of values. For example, the T-Net in PointNet is designed to predict an orthogonal transformation matrix for canonicalization of input and intermediate features (Qi et al., 2017a). The predicted matrix is regularized by minimizing the loss term Lreg = I −ZZT2 . However, this regularization is ineffective for theZ with huge variance as shown in Figure 3. Second, the scale distortion leads to a saturation of forward-propagated activations and backward- propagated gradients (Ding et al., 2019). In the binary neural networks, some modules (such as sign and Hardtanh) rely on the Straight-Through Estimator (STE) Bengio et al. (2013) for feature binarization or feature balancing. When the scale of their input is ampliﬁed, the gradient is truncated instead of increased proportionally. Such saturation, as shown in Fig 4(c), hinders learning and even leads to divergence. 3.3.2 LSR FOR OUTPUT SCALE RECOVERY To recover the scale and adjustment ability of output, we propose the LSR for bi-linear layers in our BiPointNet. We design a learnable layer-wise scaling factor αin our LSR. αis initialized by the ratio of the standard deviations between the output of bi-linear and full-precision counterpart: α0 = σ(A ⊗W)/σ(Ba ⊙Bw), (9) where σdenotes as the standard deviation. And the αis learnable during the training process. The calculation and derivative process of the bi-linear layer with our LSR are as follows: Forward :Z = α(Ba ⊙Bw) Backward : gα = gZ(Ba ⊙Bw), (10) where gα and gZ denotes the gradient ∂C ∂α and ∂C ∂Z , respectively. By applying the LSR in BiPointNet, we mitigate the scale distortion of output caused by binarization. Compared to existing methods, the advantages of LSR is summarized in two folds. First, LSR is efﬁcient. It not only abandons the adjustment of input activations to avoid expensive inference time computation, but also recovers the scale of all weights parameters in a layer collectively instead of expensive restoration in a channel-wise manner (Rastegari et al., 2016). Second, LSR serves the purpose of scale recovery that we show is more effective than other adaptation such as minimizing quantization errors (Qin et al., 2020b; Liu et al., 2018). 4 E XPERIMENTS In this section, we conduct extensive experiments to validate the effectiveness of our proposed Bi- PointNet for efﬁcient learning on point clouds. We ﬁrst ablate our method and demonstrate the con- tributions of EMA and LSR on three most fundamental tasks: classiﬁcation on ModelNet40 (Wu et al., 2015), part segmentation on ShapeNet (Chang et al., 2015), and semantic segmentation on S3DIS (Armeni et al., 2016). Moreover, we compare BiPointNet with existing binarization methods where our designs stand out. Besides, BiPointNet is put to the test on real-world devices with limited computational power and achieve extremely high speedup (14.7×) and storage saving (18.9×). The details of the datasets and the implementations are included in the Appendix E. 7Published as a conference paper at ICLR 2021 Table 2: Comparison of binarization methods on PointNet. EMA is critical; even if all meth- ods are equipped with our EMA, our LSR out- performs others with least number of scaling factors. OA: Overall Accuracy Method Bit-width Aggr. # Factors OA Full Prec. 32/32 MAX - 88.2 32/32 A VG - 86.5 BNN 1/1 MAX 0 7.1 1/1 EMA-avg 0 11.3 1/1 EMA-max 0 16.2 IR-Net 1/1 MAX 10097 7.3 1/1 EMA-avg 10097 22.0 1/1 EMA-max 10097 63.5 Bi-Real 1/1 MAX 10097 4.0 1/1 EMA-avg 10097 77.0 1/1 EMA-max 10097 77.5 ABC-Net 1/1 MAX 51 4.1 1/1 EMA-avg 51 68.9 1/1 EMA-max 51 77.8 XNOR++ 1/1 MAX 18 4.1 1/1 EMA-avg 18 73.8 1/1 EMA-max 18 78.4 XNOR 1/1 MAX 28529 64.9 1/1 EMA-avg 28529 78.2 1/1 EMA-max 28529 81.9 Ours 1/1 MAX 18 4.1 1/1 EMA-avg 18 82.5 1/1 EMA-max 18 86.4 Table 3: Our methods on mainstream back- bones. We use XNOR as a strong baseline for comparison. The techniques in our BiPointNet are generic to point cloud learning. Hence, they are easily extendable to other backbones Base Model Method Bit-width Aggr. OA PointNet (Vanilla) Full Prec. 32/32 MAX 86.8 XNOR 1/1 MAX 61.0 Ours 1/1 EMA-max 85.6 PointNet Full Prec. 32/32 MAX 88.2 XNOR 1/1 MAX 64.9 Ours 1/1 EMA-max 86.4 PointNet++ Full Prec. 32/32 MAX 90.0 XNOR 1/1 MAX 63.1 Ours 1/1 EMA-max 87.8 PointCNN Full Prec. 32/32 A VG 90.0 XNOR 1/1 A VG 83.0 Ours 1/1 EMA-avg 83.8 DGCNN Full Prec. 32/32 MAX 89.2 XNOR 1/1 MAX 51.5 Ours 1/1 EMA-max 83.4 PointConv Full Prec. 32/32 – 90.8 XNOR 1/1 – 83.1 Ours 1/1 – 87.9 4.1 A BLATION STUDY As shown in Table 1, the binarization model baseline suffers a catastrophic performance drop in the classiﬁcation task. EMA and LSR improve performance considerably when used alone, and they further close the gap between the binarized model and the full-precision counterpart when used together. In Figure 4, we further validate the effectiveness of EMA and LSR. We show that BiPointNet with EMA has its information entropy maximized during training, whereas the vanilla binarized network with max pooling gives limited and highly ﬂuctuating results. Also, we make use of the regularization loss Lreg = I −ZZT F for the feature transformation matrix of T-Net in PointNet as an indicator, the Lreg of the BiPointNet with LSR is much smaller than the vanilla binarized network, demonstrating LSR’s ability to reduce the scale distortion caused by binarization, allowing proper prediction of orthogonal transformation matrices. Moreover, we also include the results of two challenging tasks, part segmentation, and semantic seg- mentation, in Table 1. As we follow the original PointNet design for segmentation, which concate- nates pointwise features with max pooled global feature, segmentation suffers from the information loss caused by the aggregation function. EMA and LSR are proven to be effective: BiPointNet is approaching the full precision counterpart with only ∼4% mIoU difference on part segmentation and ∼10.4% mIoU gap on semantic segmentation. The full results of segmentation are presented in Appendix E.6. 4.2 C OMPARATIVE EXPERIMENTS In Table 2, we show that our BiPointNet outperforms other binarization methods such as BNN (Hubara et al., 2016), XNOR (Rastegari et al., 2016), Bi-Real (Liu et al., 2018), ABC-Net (Lin 8Published as a conference paper at ICLR 2021 3.16 0.17 0 0.5 1 1.5 2 2.5 3 3.5 ARM Devices PointNet BiPointNet Storage Usage (MB) 18.9× 67.3 131.8 5.5 9 0 20 40 60 80 100 120 140 A72 A53 PointNet BiPointNet Time Cost (ms) 14.7×12.1× 0 10 20 30 40 50 60 70 80 90 100 01020304050607080 Accuracy (%) IR-Net OursXNOR++ Bi-Real XNOR Time Cost (ms) BNN ABC-Net FP32 (a) (b) (c) Figure 5: (a) Time cost comparison. Our BiPointNet achieves 14.7×speedup on ARM A72 CPU device. (b) Storage usage comparison. Our BiPointNet enjoys 18.9×storage saving on all devices. (c) Speed vs accuracy trade-off plot. We evaluate various binarization methods (with our EMA-max) upon PointNet architecture on ARM A72 CPU device, our BiPointNet is the leading method in both speed and accuracy et al., 2017), XNOR++ (Bulat & Tzimiropoulos, 2019), and IR-Net (Qin et al., 2020b). Although these methods have been proven effective in 2D vision, they are not readily transferable to point clouds due to aggregation-induced feature homogenization. Even if we equip these methods with our EMA to mitigate information loss, our BiPointNet still performs better. We argue that existing approaches, albeit having many scaling factors, focus on minimizing quantization errors instead of recovering feature scales, which is critical to effective learning on point clouds. Hence, BiPointNet stands out with a negligible increase of parameters that are designed to restore feature scales. The detailed analysis of the performance of XNOR is found in Appendix C. Moreover, we highlight that our EMA and LSR are generic, and Table 3 shows improvements across several mainstream categories of point cloud deep learning models, including PointNet (Qi et al., 2017a), PointNet++ (Qi et al., 2017b), PointCNN (Li et al., 2018), DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019). 4.3 D EPLOYMENT EFFICIENCY ON REAL -WORLD DEVICES To further validate the efﬁciency of BiPointNet when deployed into the real-world edge devices, we further implement our BiPointNet on Raspberry Pi 4B with 1.5 GHz 64-bit quad-core ARM CPU Cortex-A72 and Raspberry Pi 3B with 1.2 GHz 64-bit quad-core ARM CPU Cortex-A53. We compare our BiPointNet with the PointNet in Figure 5(a) and Figure 5(b). We highlight that BiPointNet achieves 14.7×inference speed increase and 18.9×storage reduction over PointNet, which is recognized as a fast and lightweight model itself. Moreover, we implement various bina- rization methods over PointNet architecture and report their real speed performance on ARM A72 CPU device. As Figure 5(c), our BiPointNet surpasses all existing binarization methods in both speed and accuracy. Note that all binarization methods adopt our EMA and report their best accu- racy, which is the important premise that they can be reasonably applied to binarize the PointNet. 5 C ONCLUSION We propose BiPointNet, the ﬁrst binarization approach for efﬁcient learning on point clouds. We build a theoretical foundation to study the impact of binarization on point cloud learning models, and proposed EMA and LSR in BiPointNet to improve the performance. BiPointNet outperforms existing binarization methods, and it is easily extendable to a wide range of tasks and backbones, giving an impressive14.7×speedup and 18.9×storage saving on resource-constrained devices. Our work demonstrates the great potential of binarization. We hope our work can provide directions for future research. Acknowledgement This work was supported by National Natural Science Foundation of China (62022009, 61872021), Beijing Nova Program of Science and Technology (Z191100001119050), and State Key Lab of Software Development Environment (SKLSDE-2020ZX-06). 9Published as a conference paper at ICLR 2021 REFERENCES Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In IEEE CVPR, 2016. Yoshua Bengio, Nicholas L ´eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Adrian Bulat and Georgios Tzimiropoulos. Xnor-net++: Improved binary neural networks. In BMVC, 2019. Angel X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Qixing Huang, Zimo Li, S. Savarese, M. Savva, Shuran Song, H. Su, J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d model repository. CoRR, abs/1512.03012, 2015. Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. Regularizing activation distribution for training binarized deep networks. In IEEE CVPR, 2019. Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. CoRR, abs/1903.02428, 2019. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu- rate object detection and semantic segmentation. In IEEE CVPR, 2014. Ross B. Girshick. Fast r-cnn. IEEE ICCV, 2015. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In IEEE ICCV, 2019. Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3d point clouds: A survey. IEEE TPAMI, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In IEEE CVPR, 2016. Lu Hou, Quanming Yao, and James T. Kwok. Loss-aware binarization of deep networks. ICLR, 2017. Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efﬁcient semantic segmentation of large-scale point clouds. 2020. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In NeurIPS, 2016. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In NeurIPS, 2012. Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, X. Di, and B. Chen. Pointcnn: Convolution on x-transformed points. In NeurIPS, 2018. Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In NeurIPS, 2017. Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and David S. Doermann. Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation. In IEEE CVPR, 2019a. Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. IEEE CVPR, 2019b. Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8895–8904, 2019c. 10Published as a conference paper at ICLR 2021 Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In ECCV, 2018. Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions. In ECCV, 2020. Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efﬁcient 3d deep learning. In NeurIPS, 2019d. Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with real-to-binary convolutions. In ICLR, 2020. Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. IEEE CVPR, 2017a. Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, 2017b. Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks: A survey. Pattern Recognition, 2020a. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In IEEE CVPR, 2020b. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. In ECCV, 2016. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S Bernstein, et al. Imagenet large scale vi- sual recognition challenge. IJCV, 2015. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE CVPR, 2015. Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. IEEE ICCV, 2019. Yiru Wang, Weihao Gan, Wei Wu, and Junjie Yan. Dynamic curriculum learning for imbalanced data classiﬁcation. In IEEE ICCV, 2019a. Yunhe Wang, Chang Xu, Chao Xu, and Dacheng Tao. Packing convolutional neural networks in the frequency domain. IEEE TPAMI, 2019b. Ziwei Wang, Ziyi Wu, Jiwen Lu, and Jie Zhou. Bidet: An efﬁcient binarized object detector. In IEEE CVPR, 2020. B. Wu, Y . Wang, P. Zhang, Yuandong Tian, P. Vajda, and K. Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. CoRR, abs/1812.00090, 2018. Wenxuan Wu, Zhongang Qi, and Fuxin Li. Pointconv: Deep convolutional networks on 3d point clouds. IEEE CVPR, 2019. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In IEEE CVPR, 2015. Zizhao Wu, Ruyang Shou, Yunhai Wang, and Xinguo Liu. Interactive shape co-segmentation via label propagation. Computers & Graphics, 38:248–254, 2014. 11Published as a conference paper at ICLR 2021 Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. InArtiﬁcial Intelligence and Statistics, 2017. Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efﬁcient point-cloud segmentation. In ECCV, 2020a. Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, and U. Neumann. Grid-gcn for fast and scalable point cloud learning. 2020b. Yinghao Xu, Xin Dong, Yudian Li, and Hao Su. A main/subsidiary network framework for simpli- fying binary neural networks. In IEEE CVPR, 2019. Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):1–12, 2016. Haibao Yu, Q. Han, Jianbo Li, Jianping Shi, Guang-Liang Cheng, and Bin Fan. Search what you want: Barrier panelty nas for mixed precision quantization. In ECCV, 2020. Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, and Tao Mei. dabnn: A super fast inference framework for binary neural networks on ARM devices. In ACM MM, 2019a. Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. Diversifying sample generation for data-free quantization. In IEEE CVPR, 2021. Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Shellnet: Efﬁcient point cloud convolutional neural networks using concentric shells statistics. In IEEE ICCV, 2019b. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv, abs/1606.06160, 2016. Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards uniﬁed int8 training for convolutional neural network. InIEEE CVPR, 2020. Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or more networks per bit? In IEEE CVPR, 2019. 12Published as a conference paper at ICLR 2021 APPENDIX FOR BIPOINT NET A M AIN PROOFS AND DISCUSSION A.1 P ROOF OF ZERO CONDITIONAL ENTROPY In our BiPointNet, we hope that the binarized tensorB reﬂects the information in the original tensor Y as much as possible. From the perspective of information, our goal is equivalent to maximizing the mutual information I(Y; B) of the random variables Y and B: arg max Y,B I(Y; B) (11) = ∑ y∈Y,b∈B p(Y,B)(y,b) log p(Y,B)(y,b) pY(y)pB(b) (12) = ∑ y∈Y,b∈B p(Y,B)(y,b) log p(Y,B)(y,b) pY(y) − ∑ y∈Y,b∈B p(Y,B)(y,b) logpB(b) (13) = ∑ y∈Y,b∈B pY(y)pB|Y=y(b) logpB|Y=y(b) − ∑ y∈Y,b∈B p(Y,B)(y,b) logpB(b) (14) = ∑ y∈Y pY(y) (∑ b∈B pB|Y=y(b) logpB|Y=y(b) ) − ∑ b∈B (∑ y p(Y,B)(y,b) ) log pB(b) (15) = − ∑ y∈Y p(y)H(B |Y = y) − ∑ b∈B pB(b) logpB(b) (16) = −H(B |Y) + H(B) (17) = H(B) −H(B |Y), (18) where p(Y,B) and pY, pB are the joint and marginal probability mass functions of these discrete variables. H(B) is the information entropy, and H(B|Y) is the conditional entropy of B given Y. According to the Eq. (15) and Eq. (18), the conditional entropy H(Y |X) can be expressed as H(B |Y) = ∑ y∈Y pY(y) (∑ b∈B pB|Y=y(b) logpB|Y=y(b) ) . (19) Since we use the deterministic sign function as the quantizer in binarization, the value of B fully depends on the value of Y, pB|Y=y(b) = 0 or 1 in Eq. (4), i.e., every valueyhas a ﬁxed mapping to a binary value b. Then we have H(B |Y) = ∑ y∈Y pY(y)(0 + 0 +··· + 0) = 0. (20) Hence, the original objective function is equivalent to maximizing the information entropy H(B): arg max B HB(B) = − ∑ b∈B pB(b) logpB(b). (21) A.2 P ROOFS OF THEOREM 1 Theorem 1 For input Xφ of max pooling φwith arbitrary distribution, the information entropy of the binarized output to zero asnto inﬁnity, i.e., lim n→+∞ HB = 0. And there is a constantc, for any n1 and n2, if n1 >n2 >c, we have HB,n1 <HB,n2 , where nis the number of aggregated elements. 13Published as a conference paper at ICLR 2021 Proof. We obtain the correlation between the probability mass function of input Xφ and output Y of max pooling, intuitively, all values are negative to give a negative maximum value: ∑ y<0 pY(y) = ( ∑ xφ<0 pXφ(xφ) )n . (22) Since the sign function is applied as the quantizer, theHB(B) of binarized feature can be expressed as Eq. (6). (1) When Xφ obeys a arbitrary distribution, the probability mass function pXφ(xφ) must satisﬁes∑ xφ<0 pXφ(xφ) ≤1. According to Eq. (6), let t= ∑ xφ<0 pXφ(xφ), we have lim n→∞ HB(Xφ) = lim n→∞ −tn log tn −(1 −t)n log (1 −t)n (23) = − ( lim n→∞ tn ) log ( lim n→∞ tn ) − ( lim n→∞ (1 −t)n ) log ( lim n→∞ (1 −t)n ) (24) = −0 log 0 −1 log 1 (25) =0 (26) (2) For any n≥1, we can obtain the representation of the information entropy HB,n(Xφ): HB,n(Xφ) = − ( ∑ xφ<0 pXφ(xφ) )n log ( ∑ xφ<0 pXφ(xφ) )n − ( 1 − ( ∑ xφ<0 pXφ(xφ) )n) log ( 1 − ( ∑ xφ<0 pXφ(xφ) )n) , (27) Let pn = (∑ xφ<0 pXφ(xφ) )n , the HB,n(pn) can be expressed as HB,n(pn) = −pnlog pn −(1 −pn) log(1−pn), (28) and the derivative of HB,n(pn) is dHB,n(pn) dpB(pn) = log (1 −pn pn ) , (29) the HB,n(pn) is maximized when pn takes 0.5, and is positive correlation with pn when pn < 0.5 since the dHB,n(pn) dpB(pn) >0 when pn <0.5. Therefore, when the constant csatisﬁes pc = (∑ xφ<0 pXφ(xφ) )c ≥0.5, given the n1 > n2 > c, we have pn1 <pn2 <pc, and HB,n1 (Xφ) <HB,n2 (Xφ) <HB,c(Xφ). □ A.3 P ROOFS OF PROPOSITION 1 Proposition 1 When the distribution of the random variable Y satisﬁes ∑ y<0 pY(y) =∑ y≥0 pY(y) = 0.5, the information entropy HB is maximized. Proof. According to Eq (5), we have HB(B) = − ∑ b∈B pB(b) logpB(b) (30) = −pB(−1) logpB(−1) −pB(1) logpB(1) (31) = −pB(−1) logpB(−1) −(1 −pB(−1) log (1−pB(−1))) . (32) 14Published as a conference paper at ICLR 2021 Then we can get the derivative of HB(B) with respect to pB(−1) dHB(B) dpB(−1) = − ( log pB(−1) + pB(−1) pB(−1) ln 2 ) + ( log (1−pB(−1)) + 1 −pB(−1) (1 −pB(−1)) ln 2 ) (33) = −log pB(−1) + log (1−pB(−1)) − 1 ln 2 + 1 ln 2 (34) = log (1 −pB(−1) pB(−1) ) . (35) When we let dHB(B) dpB(−1) = 0 to maximize the HB(B), we have pB(−1) = 0.5. Sine the deterministic signfunction with the zero threshold is applied as the quantizer, the probability mass function ofB is represented as pB(b) =    ∑ y<0 pY(y) dy, if b= −1 ∑ y≥0 pY(y) dy, if b= 1, (36) and when the information entropy is maximized, we have ∑ y<0 pY(y) dy= 0.5. (37) □ A.4 D ISCUSSION AND PROOFS OF THEOREM 2 The bi-linear layers are widely used in our BiPointNet to model each point independently, and each linear layer outputs an intermediate feature. The calculation of the bi-linear layer is represented as Eq. (2). Since the random variable Bis sampled from Bw or Ba obeying Bernoulli distribution, the probability mass function of Bcan be represented as pB(b) = {p, if b= +1 1 −p, if b= −1, (38) where pis the probability of taking the value +1. The distribution of outputZ can be represented by the probability mass function of Bw and Ba. Proposition 2 In bi-linear layer, for the binarized weight Bw ∈ {−1,+1}m×k and activation Ba ∈{−1,+1}n×mwith probability mass functionpBw(1) = pw and pBa(1) = pa, the probability mass function for the distribution of output Z can be represented as pZ(2i−m) = Ci m(1 −pw − pa + 2pwpa)i(pw + pa −2pwpa)m−i,i ∈{0,1,2,...,m }. Proof. To simplify the notation in the following statements, we deﬁne A = Ba and W = Bw. Then, for each element Zi,j in output Z ∈{−1,+1}n×k, we have xi,j = m∑ k=1 Ai,k ×Wk,j. (39) Observe that Ai,k is independent to Wk,j and the value of both variables are either −1 or +1. Therefore, the discrete probability distribution of Ai,k ×Wk,j can be deﬁned as p(x) =    pwpa + (1 −pw) ×(1 −pa), if x= 1 pw ×(1 −pa) + (1−pw) ×pa, if x= −1 0, otherwise. (40) 15Published as a conference paper at ICLR 2021 Simplify the above equation p(x) =    1 −pw −pa + 2pwpa, if x= 1 pw + pa −2pwpa, if x= −1 0, otherwise. (41) Notice that xi,j can be parameterized as a binomial distribution. Then we have Pr(xi,j = l−(m−l)) = Cl m(1 −pw −pa + 2pwpa)l(pw + pa −2pwpa)m−l. (42) Observe that pZ obeys the same distribution as xi,j. Finally, we have pZ(2i−m) = Ci m(1 −pw −pa + 2pwpa)i(pw + pa −2pwpa)m−i,i ∈{0,1,2,...,m }. (43) □ Proposition 2 shows that the output distribution of the bi-linear layer depends on the probability mass functions of binarized weight and activation. Then we present the proofs of Theorem 2. Theorem 2 When we let pBw(1) = 0.5 and pBa(1) = 0.5 in bi-linear layer to maximize the mutual information, for the binarized weight Bw ∈{−1,+1}m×k and activation Ba ∈{−1,+1}n×m, the probability mass function for the distribution of output Z can be represented as pZ(2i−m) = 0.5mCi m,i ∈ {0,1,2,...,m }. The distribution of output is approximate normal distribution N(0,m). Proof. First, we prove that the distribution ofZcan be approximated as a normal distribution. For bi-linear layers in our BiPointNet, all weights and activations are binarized, which can be represented as Bw and Ba, respectively. And the value of an element z(i,j) in Z can be expressed as z(i,j) = m∑ k=1 ( bw(i,k) ×ba(k,j) ) , and the value of the element bw(i,k) ×ba(k,j) can be expressed as bw(i,k) ×ba(k,j) = {1, if bw(i,k) ⊻ba(k,j) = 1 −1, if bw(i,k) ⊻ba(k,j) = −1. (44) The bw(i,k)×ba(k,j) only can take from two values and its value can be considered as the result of one Bernoulli trial. Thus for the random variable Z sampled from the output tensor Z, the probability mass function, pZ can be expressed as pZ(2i−m) = Ci mpk e(1 −pe)n−k, (45) where pe denotes the probability that the element bw(i,k) ×ba(k,j) takes 1. Note that the Eq. (45) is completely equivalent to the representation in Proposition 2. According to the De Moivre–Laplace theorem, the normal distribution N(µ,σ2) can be used as an approximation of the binomial distri- bution under certain conditions, and the pZ(2i−m) can be approximated as pZ(2i−m) = Ci mpk e(1 −pe)n−k ≃ 1√ 2πnpe(1 −pe) e− (k−npe)2 2npe(1−pe) , (46) and then, we can get the mean µ = 0 and variance σ = √mof the approximated distribution N with the help of equivalent representation of pZ in Proposition 2. Now we give proof of this below. According to Proposition 2, when pw = pa = 0.5, we can rewrite the equation as 16Published as a conference paper at ICLR 2021 pZ(2i−m) = 0.5mCi m,i ∈{0,1,2,...,m }. (47) Then we move to calculate the mean and standard variation of this distribution. The mean of this distribution is deﬁned as µ(pZ) = ∑ (2i−m)0.5mCi m,i ∈{0,1,2,...,m }. (48) By the virtue of binomial coefﬁcient, we have (2i−m)0.5mCi m + (2(m−i) −m)0.5mCm−i m = 0.5m((2i−m)Ci m + (m−2i)Cm−i m ) (49) = 0.5m((2i−m)Ci m + (m−2i)Ci m) (50) = 0. (51) Besides, when mis an even number, we have(2i−m)0.5mCi m = 0,i = m 2 . These equations prove the symmetry of function (2i−m)0.5mCi m. Finally, we have µ(pZ) = ∑ (2i−m)0.5mCi m,i ∈{0,1,2,...,m } (52) = ∑ ((2i−m)0.5mCi m + (2(m−i) −m)0.5mCm−i m ),i ∈{0,1,2,..., m 2 } (53) = 0. (54) The standard variation of pZ is deﬁned as σ(pZ) = √(∑ |2i−m|20.5mCim ) (55) = √∑ (4i2 −4im+ m2) 0.5mCim (56) = √ 0.5m ( 4 ∑ i2Cim −4m ∑ iCim + m2 ∑ Cim ) . (57) To calculate the standard variation of pZ, we use Binomial Theorem and have several identical equations: ∑ Ci m = (1 + 1)m = 2m (58) ∑ iCi m = m(1 + 1)m−1 = m2m−1 (59) ∑ i2Ci m = m(m+ 1)(1 + 1)m−2 = m(m+ 1)m2m−2. (60) These identical equations help simplify Eq. (57): σ(pZ) = √ 0.5m ( 4 ∑ i2Cim −4m ∑ iCim + m2 ∑ Cim ) (61) = √ 0.5m(4m(m+ 1)2m−2 −4m22m−1 + m22m) (62) = √ 0.5m((m2 + m)2m −2m22m + m22m) (63) = √ 0.5m(m2m) (64) = √m. (65) Now we proved that, the distribution of output is approximate normal distribution N(0,m). □ 17Published as a conference paper at ICLR 2021 A.5 D ISCUSSION OF THE OPTIMAL δFOR EMA- MAX When the Xφ ∼N (0,1), the objective function of EMA-max to obtain optimal δ∗is represented as Eq. (8). It is difﬁcult to directly solve the objective function. To circumvent this issue, we use Monte Carlo simulation to approximate the value of the optimal δ∗ max as shown Algorithm 1. Algorithm 1Monte Carlo Simulation for EMA-max Input: The number nof points to be aggregated; the number of simulations m (e.g. 10000) Output: Estimated optimal δ∗ max for EMA-max 1: Creating an empty list F (represents elements sampled form distribution of aggregated feature) 2: for i= 0 to m do 3: Creating an empty list Ti (representing one channel of input feature) 4: for j = 0 to ndo 5: Sampling an element eij from the distribution N(0,1) 6: Adding the sampled element eij to the list Ti 7: end for 8: Adding an element represents the aggregated feature MAX(Ti) to F 9: end for 10: Estimating the optimal δ∗ max as δ∗ max = Median(F) (follow Proposition 1) A.6 D ISCUSSION OF THE OPTIMAL δFOR EMA- AVG When the Xφ ∼N(δ,1), the Y ∼N(δ,n−1) and the objective function of EMA-avg for obtaining optimal δ∗ avg can be represented as arg max δ HB(δ) = − ( ∑ xφ<0 1 n−1√ 2π e−(xφ−δ) 2 2 ) log ( ∑ xφ<0 1 n−1√ 2π e−(xφ−δ) 2 2 ) − ( ∑ xφ≥0 1 n−1√ 2π e−(xφ−δ) 2 2 ) log ( ∑ xφ≥0 1 n−1√ 2π e−(xφ−δ) 2 2 ) . (66) The solution of Eq. (66) is expressed as δ = 0, we thus obtain δ∗ avg = 0. This means the solution is not related to n. B I MPLEMENTATION OF BIPOINT NET ON ARM D EVICES B.1 O VERVIEW We further implement our BiPointNet on Raspberry Pi 4B with 1.5 GHz 64-bit quad-core ARM Cortex-A72 and Raspberry Pi 3B with 1.2 GHz 64-bit quad-core ARM Cortex-A53, and test the real speed that one can obtain in practice. Although the PointNet is a recognized high-efﬁciency model, the inference speed of BiPointNet is much faster. Compared to PointNet, BiPointNet enjoys up to 14.7×speedup and 18.9×storage saving. We utilize the SIMD instruction SSHL on ARM NEON to make inference framework daBNN (Zhang et al., 2019a) compatible with our BiPointNet and further optimize the implementa- tion for more efﬁcient inference. B.2 I MPLEMENTATION DETAILS Figure 6 shows the detailed structures of six PointNet implementations. In Full-Precision version (a), BN is merged into the later fully connected layer for speedup, which is widely chosen for deployment in real-world applications. In Binarization version (b)(c)(d)(e), we have to keep BN unmerged due to the binarization of later layers. Instead, we merge the scaling factor of LSR into BN layers. The HardTanh function is removed because it does not affect the binarized value of 18Published as a conference paper at ICLR 2021 input for the later layers. We test the quantization for the ﬁrst layer and last layer in the variants (b)(c)(d)(e). In the last variant(f), we drop the BN layers during training. The scaling factor is ignored during deployment because it does not change the sign of the output. Input Points Linear 3x64 Linear 64x128 Linear 128x1024 Max Pooling Linear 1024x512 Linear 512x256 Linear 256x40 Output nx3 Class (a) nx3 Class (b) Full Precision FC Binarization FC with BN Binarization FC w/o BN Max Pooling nx3 Class (c) nx3 Class (d) nx3 Class (e) nx3 Class (f) Figure 6: Structures of different PointNet implementations. Three fully connected layers are used in all six variants: Full Precision FC, Binarization FC with BN, Binarization FC w/o BN. Full Precision FC contains a full precision fully connected layer and a ReLU layer. Original BN is merged into the later layer. Binarization FC with BN also contains two layers: a quantized fully connected layer and a batch normalization layer. Binarization FC w/o BN is formed by a single quantized fully connected layer B.3 A BLATION ANALYSIS OF TIME COST AND QUANTIZATION SENSITIVITY Setup Bit-width FL LL BN OA Storage & Saving Ratio Time & Speedup Ratio A72 A53 (a) 32/32 32/32 32/32 Merged 86.8 3.16MB / 1.0× 131ms / 1.0× 67ms / 1.0× (b) 1/1 32/32 32/32 Not Merged 85.62 0.17MB / 18.9× 9.0ms / 14.7× 5.5ms / 12.1× (c) 1/1 32/32 1/1 Not Merged 84.60 0.12MB / 26.3× 9.0ms / 14.7× 5.3ms / 12.6× (d) 1/1 1/1 32/32 Not Merged 5.31 0.16MB / 19.7× 11.5ms / 11.4× 6.5ms / 10.3× (e) 1/1 1/1 1/1 Not Merged 4.86 0.12MB / 26.3× 11.4ms / 11.5× 6.4ms / 10.4× (f) 1/1 32/32 32/32 Not Used 85.13 0.15MB / 21.0× 8.1ms / 16.1× 4.8ms / 13.9× Table 4: Comparison of different conﬁgurations in deployment on ARM devices. The storage-saving ratio and speedup ratio are calculated according to the full precision model as the ﬁrst row illustrates. All the models use PointNet as the base model and EMA-max as the aggregation function. The accuracy performance is reported on the point cloud classiﬁcation task with the ModelNet40 dataset. FL: First Layer; LL: Last Layer Table 4 shows the detailed conﬁguration including overall accuracy, storage usage, and time cost of the above-mentioned six implementations. The result shows that binarization of the middle fully connected layers can extremely speed up the original model. We achieve 18.9×storage saving, 14.7×speedup on A72, and 12.1×speed on A53. The quantization of the last layer further helps save storage consumption and improves the speed with a slight performance drop. However, the quantization of the ﬁrst layer causes a drastic drop in accuracy without discernible computational cost reduction. The variant (f) without BN achieves comparable performance with variant (b). It suggests that our LSR method could be an ideal alternative to the original normalization layers to achieve a fully quantized model except for the ﬁrst layer. 19Published as a conference paper at ICLR 2021 C C OMPARISON BETWEEN LAYER -WISE SCALE RECOVERY AND OTHER METHODS In this section, we will analyze the difference between the LSR method with other model binarization methods. Theorem 2 shows the signiﬁcance of recovering scale in point cloud learning. However, IRNet and BiReal only consider the scale of weight but ignore the scale of input features. Therefore, these two methods cannot recover the scale of output due to scale distortion on the input feature. A major difference between these two methods is that LSR opts for layer-wise scaling factor while XNOR opts for point-wise one. Point-wise scale recovery needs dynamical computation during inference while our proposed LSR only has a layer-wise global scaling factor, which is independent of the input. As a result, our method can achieve higher speed in practice. 0 25 50 75 100 125 150 175 200 epoch 10 5 10 4 10 3 10 2 10 1 100 Information Entropy BNN XNOR BiPointNet Figure 7: The information entropy of BNN, XNOR and our BiPointNet Table 3 shows that XNOR can alleviate the aggregation-induced feature homogenization. The point- wise scaling factor helps the model to achieve comparable adjustment capacity as full-precision linear layers. Therefore, although XNOR suffers from feature homogenization at the beginning of the training process, it can alleviate this problem with the progress of training and achieve acceptable performance, as shown in Figure 7. D C OMPARISON WITH OTHER EFFICIENT LEARNING METHODS We compare our computation speedup and storage savings with several recently proposed methods to accelerate deep learning models on point clouds. Note that the comparison is for reference only; tests are conducted on different hardware, and for different tasks. Hence, direct comparison cannot give any meaningful conclusion. In Table 5, we show that BiPointNet achieves the most impressive acceleration. E E XPERIMENTS E.1 D ATASETS ModelNet40: ModelNet40 (Wu et al., 2015) for part segmentation. The ModelNet40 dataset is the most frequently used dataset for shape classiﬁcation. ModelNet is a popular benchmark for point cloud classiﬁcation. It contains 12,311 CAD models from 40 representative classes of objects. 20Published as a conference paper at ICLR 2021 Table 5: Comparison between BiPointNet and other approaches to efﬁcient learning on point clouds. Grid-GCN (Xu et al., 2020b) leverages novel data structuring strategy; RAND-LA Net (Hu et al., 2020) designs a faster sampling method; PointV oxel (Liu et al., 2019d) proposes an efﬁcient repre- sentation. These works, albeit achieving high performance, are not as effective as our binarization method in terms of model acceleration. The asterisk indicates the vanilla version Method Hardware Dataset Base Model Metric/ Performance Speedup BiPointNet ARM Cortex-A72 ModelNet4 PointNet* OA/85.6 12.1× BiPointNet ARM Cortex-A53 ModelNet40 PointNet* OA/85.6 14.7× Grid-GCN RTX 2080 GPU S3DIS PointNet mIoU/53.2 1.62× RandLA-Net RTX 2080Ti GPU S3DIS PointNet* mIoU/70.0 1.04× PointV oxel GTX 1080Ti GPU ShapeNet PointNet mIoU/46.9 2.46× ShapeNet Parts: ShapeNet Parts (Chang et al., 2015) for part segmentation. ShapeNet contains 16,881 shapes from 16 categories, 2,048 points are sampled from each training shape. Each shape is split into two to ﬁve parts depending on the category, making up to 50 parts in total. S3DIS: S3DIS for semantic segmentation (Armeni et al., 2016). S3DIS includes 3D scan point clouds for 6 indoor areas including 272 rooms in total, each point belongs to one of 13 semantic categories. We follow the ofﬁcial code (Qi et al., 2017a) for training and testing. E.2 I MPLEMENTATION DETAILS OF BIPOINT NET We follow the popular PyTorch implementation of PointNet and the recent geometric deep learning codebase (Fey & Lenssen, 2019) for the implementation of PointNet baselines. Our BiPointNet is built by binarizing the full-precision PointNet. All linear layers in PointNet except the ﬁrst and last one are binarized to bi-linear layer, and we select Hardtanh as our activation function instead of ReLU when we binarize the activation before the bi-linear layer. For the part segmentation task, we follow the convention (Wu et al., 2014; Yi et al., 2016) to train a model for each of the 16 classes. We also provide our PointNet baseline under this setting. Following previous works, we train 200 epochs, 250 epochs, 128 epochs on point cloud classiﬁca- tion, part segmentation, semantic segmentation respectively. To stably train the binarized models, we use a learning rate of 0.001 with Adam and Cosine Annealing learning rate decay for all binarized models on all three tasks. E.3 M ORE BACKBONES We also propose four other models: BiPointCNN, BiPointNet++, BiDGCCN, and BiPointConv, which are binarized versions of PointCNN (Li et al., 2018), PointNet++ (Qi et al., 2017b), DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019), respectively. This is attributed to the fact that all these variants have characteristics in common, such as linear layers for point-wise feature extraction and global pooling layers for feature aggregation (except PointConv, which does not have explicit aggregators). In PointNet++, DGCNN, and PointConv, we keep the ﬁrst layer and the last layer full-precision and binarize all the other layers. In PointCNN, we keep every ﬁrst layer of XConv full precision and keep the last layer of the classiﬁer full precision. E.4 B INARIZATION METHODS For comparison, we implement various representative binarization methods for 2D vision, including BNN (Hubara et al., 2016), XNOR-Net (Rastegari et al., 2016), Bi-Real Net (Liu et al., 2018), XNOR++ (Bulat & Tzimiropoulos, 2019), ABC-Net (Lin et al., 2017), and IR-Net (Qin et al., 2020b), to be applied on 3D point clouds. Note that the Case 1 version of XNOR++ is used in our experiments for a fair comparison, which applies layerwise learnable scaling factors to mini- mize the quantization error. These methods are implemented according to their open-source code or the description in their papers, and we take reference of their 3x3 convolution design when im- plementing the corresponding bi-linear layers. We follow their training process and hyperparameter 21Published as a conference paper at ICLR 2021 settings, but note that the speciﬁc shortcut structure in Bi-Real and IR-Net is ignored since it only applies to the ResNet architecture. E.5 T RAINING DETAILS Our BiPointNet is trained from scratch (random initialization) without leveraging any pre-trained model. Amongst the experiments, we apply Adam as our optimizer and use the cosine annealing learning rate scheduler to stably optimize the networks. To evaluate our BiPointNet on various network architectures, we mostly follow the hyper-parameter settings of the original papers (Qi et al., 2017a; Li et al., 2018; Qi et al., 2017b; Wang et al., 2019a). E.6 D ETAILED RESULTS OF SEGMENTATION We present the detailed results of part segmentation on ShapeNet Part in Table 6 and semantic segmentation on S3DIS in Table 7. The detailed results further prove the conclusion of Section 4.1 as EMA and LSR improve performance considerably in most of the categories (instead of huge performance in only a few categories). This validates the effectiveness and robustness of our method. 22Published as a conference paper at ICLR 2021 Table 6: Detailed results of our BiPointNet for part segmentation on ShapeNet Parts. aggr. mean aero bag cap car chair ear phone guitar knife lamp laptop motor mug pistol rocket skate board table # shapes 2690 76 55 898 3758 69 787 392 1547 451 202 184 283 66 152 5271 FP max 84.3 83.6 79.4 92.5 76.8 90.8 70.2 91.0 85.6 81.9 95.6 64.4 93.5 80.9 54.5 70.6 81.5 FP avg 84.0 83.4 78.5 90.8 76.3 90.0 73.1 90.8 84.3 80.8 95.5 61.7 93.8 81.6 56.2 72.2 81.8 BNN max 54.0 35.1 48.1 65.5 26.5 55.8 57.1 48.8 62.2 48.6 90.1 23.1 68.3 57.5 31.3 43.7 66.8 BNN ema-avg 53.0 39.8 46.5 57.5 24.1 58.2 56.2 44.0 50.0 53.0 81.0 16.9 48.8 36.3 25.7 43.7 63.3 BNN ema-max 47.3 37.9 46.2 44.6 24.1 61.3 38.2 33.5 42.6 50.8 48.6 16.9 49.0 25.2 26.8 43.7 50.30 LSR max 58.7 41.5 46.2 80.2 39.2 75.3 46.0 47.8 75.5 50.0 93.8 25.4 51.0 60.2 36.2 43.7 61.4 Ours ema-avg 80.3 79.3 71.9 85.5 66.1 87.7 65.6 84.1 82.8 76.0 94.8 42.7 91.8 75.9 47.2 59.1 79.7 Ours ema-max 80.6 79.5 69.7 86.1 67.4 88.6 68.5 87.4 83.0 74.9 95.1 44.8 91.6 76.3 47.7 56.9 79.5 23Published as a conference paper at ICLR 2021 Table 7: Detailed results of our BiPointNet for semantic segmentation on S3DIS. method aggr overall mIoU overall acc. area1 (mIoU/acc.) area2 (mIoU/acc.) area3 (mIoU/acc.) area4 (mIoU/acc.) area5 (mIoU/acc.) area6 (mIoU/acc.) ceiling IoU ﬂoor IoU wall IoU beam IoU column IoU window IoU door IoU table IoU chair IoU sofa IoU bookcase IoU board IoU clutter IoU FP max 54.4 83.5 61.7/86.2 38.0/76.8 62.4/88.0 45.0/82.4 45.3/83.3 70.0/89.2 91.1 93.8 72.8 50.3 34.6 52.0 58.0 55.8 51.3 14.5 44.4 43.4 45.2 FP avg 51.5 81.5 59.9/84.6 35.4/72.4 61.2/87.2 43.8/81.2 42.0/81.2 68.2/88.3 90.1 89.1 71.7 46.1 33.7 53.5 53.8 53.8 47.8 9.4 40.4 38.7 41.8 BNN max 9.5 45.0 9.6/44.0 9.8/50.5 8.3/41.9 9.3/42.5 9.5/45.8 9.8/41.6 45.5 40.6 28.1 0 0 0 0 0 7.7 0 0 0 2.1 BNN ema-avg 9.9 46.8 7.6/36.6 11.2/51.2 7.1/36.5 9.8/46.0 11.4/54.8 8.6/41.6 51.5 35.1 32.1 0 0 0 0 0.6 9.3 0 0 0 0.6 BNN ema-max 8.5 47.2 7.7/44.0 10.1/54.4 7.1/46.8 7.8/39.7 7.6/49.2 7.2/45.3 50.8 43.5 15.9 0 0 0 0 0 0 0 0 0 0 LSR max 2.0 25.4 2.0/26.0 2.1/27.0 2.0/25.7 1.8/22.8 2.0/25.8 1.9/24.5 25.4 0 0 0 0 0 0 0 0 0 0 0 0 Ours ema-avg 40.9 74.9 47.1/75.8 29.1/68.3 48.0/79.9 34.2/73.2 34.7/76.1 53.3/79.8 84.6 84.6 60.5 32.0 19.0 39.6 43.0 43.5 39.2 5.8 30.5 18.5 31.3 Ours ema-max 44.3 76.7 50.9/78.3 31.0/70.3 53.4/82.4 36.6/73.9 36.9/77.6 57.9/82.3 85.1 86.1 62.6 34.5 23.8 43.0 48.0 45.7 40.6 9.6 36.9 26.2 33.9 24",
      "meta_data": {
        "arxiv_id": "2010.05501v4",
        "authors": [
          "Haotong Qin",
          "Zhongang Cai",
          "Mingyuan Zhang",
          "Yifu Ding",
          "Haiyu Zhao",
          "Shuai Yi",
          "Xianglong Liu",
          "Hao Su"
        ],
        "published_date": "2020-10-12T07:54:51Z",
        "pdf_url": "https://arxiv.org/pdf/2010.05501v4.pdf",
        "github_url": "https://github.com/htqin/BiPointNet"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces BiPointNet, the first model binarization approach for efficient deep learning on point clouds, specifically targeting resource-constrained edge devices. It identifies and addresses two primary challenges in binarizing point cloud models: aggregation-induced feature homogenization, which degrades information entropy, and scale distortion, which hinders optimization and invalidates scale-sensitive structures. BiPointNet proposes Entropy-Maximizing Aggregation (EMA) to modulate feature distribution for maximum information entropy and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. The method achieves significant performance comparable to full-precision models, providing an impressive 14.7x speedup and 18.9x storage saving on real-world edge devices, and is shown to be generic across various fundamental tasks and mainstream backbones.",
        "methodology": "BiPointNet binarizes full-precision point cloud networks into efficient binary neural networks (BNNs) that utilize binary weights and activations with bitwise operations. The core methodology involves two key components:\n1. Entropy-Maximizing Aggregation (EMA): This component tackles feature homogenization, a severe information loss problem in binarized point cloud models due to global pooling. Based on theoretical justification (Theorem 1 and Proposition 1), EMA aims to maximize the information entropy of binarized features by ensuring a balanced distribution of full-precision features. It achieves this by introducing a learnable transformation unit (τ) before the aggregation function (φ) to shift the input distribution. Two variants are presented: EMA-max, which uses max pooling with a derived optimal constant offset (δ*), and EMA-avg, which uses average pooling and naturally requires a δ* of zero, making it adaptive to varying input point numbers.\n2. Layer-wise Scale Recovery (LSR): This addresses the significant scale distortion caused by binarization, which impacts the functionality of scale-sensitive structures (e.g., PointNet's T-Net) and leads to saturation in forward and backward gradients. LSR integrates a single, learnable layer-wise scaling factor (α) into each bi-linear layer. This factor is initialized using the ratio of standard deviations between the full-precision and binarized layer outputs and is learned during training. LSR efficiently restores the distorted output scales collectively for all parameters in a layer with negligible computational and memory overhead.",
        "experimental_setup": "Extensive experiments were conducted to validate BiPointNet's effectiveness and efficiency. The method was evaluated on three fundamental tasks:\n- Classification: Using the ModelNet40 dataset.\n- Part Segmentation: Using the ShapeNet Parts dataset.\n- Semantic Segmentation: Using the S3DIS dataset.\nBiPointNet's techniques were demonstrated as generic by applying them to various mainstream point cloud backbones, including PointNet, PointNet++, PointCNN, DGCNN, and PointConv. The validation methodology included:\n- Ablation studies: To quantify the individual and combined contributions of EMA and LSR to performance.\n- Comparative experiments: BiPointNet was compared against several existing binarization methods developed for 2D vision (BNN, XNOR-Net, Bi-Real Net, XNOR++, ABC-Net, IR-Net), with all methods optionally equipped with EMA for fair comparison.\n- Real-world deployment efficiency tests: Conducted on resource-constrained edge devices, specifically Raspberry Pi 4B (ARM Cortex-A72 CPU) and Raspberry Pi 3B (ARM Cortex-A53 CPU), to measure actual inference speedup and storage savings. Training involved Adam optimizer with cosine annealing learning rate scheduling, initiated from scratch without pre-trained models.",
        "limitations": "One notable limitation highlighted by the research is that the quantization of the first layer in BiPointNet models can lead to a drastic drop in accuracy without providing a discernible computational cost reduction. This suggests that for optimal performance, the first layer might need to remain in full precision, limiting the extent of full binarization. The paper primarily focuses on overcoming the limitations of existing binarization methods when transferred to point clouds, rather than extensively detailing other inherent weaknesses of its own approach.",
        "future_research_directions": "The authors suggest that their work provides directions for future research. Specifically, the theoretical framework of Entropy-Maximizing Aggregation (EMA) could guide the development of new, binarization-friendly aggregation functions. Further research could also focus on developing strategies to effectively quantize the first layer of point cloud networks without significant accuracy degradation, aiming towards achieving truly fully binarized models. Additionally, extending the application of BiPointNet's binarization techniques to other emerging 3D deep learning architectures and tasks beyond those covered in the paper is a potential area for exploration.",
        "experimental_code": "import torch\nimport torch.nn.functional as F\nfrom torch.nn import Sequential as Seq, Linear as Lin, ReLU, Hardtanh, BatchNorm1d as BN\nfrom torch.nn.modules.utils import _single\nfrom torch.autograd import Function\nfrom torch.nn import Parameter\nimport math\nimport torch.nn as nn\n\n\nclass BinaryQuantize(Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        out = torch.sign(input)\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input = ctx.saved_tensors\n        grad_input = grad_output\n        grad_input[input[0].gt(1)] = 0\n        grad_input[input[0].lt(-1)] = 0\n        return grad_input\n\n\nclass BiLinearLSR(torch.nn.Linear):\n    def __init__(self, in_features, out_features, bias=False, binary_act=True):\n        super(BiLinearLSR, self).__init__(in_features, out_features, bias=bias)\n        self.binary_act = binary_act\n        self.register_parameter('scale', Parameter(torch.Tensor([0.0]).squeeze()))\n\n    def reset_scale(self, input):\n        bw = self.weight\n        ba = input\n        bw = bw - bw.mean()\n        self.scale = Parameter((F.linear(ba, bw).std() / F.linear(torch.sign(ba), torch.sign(bw)).std()).float().to(ba.device))\n        if torch.isnan(self.scale):\n            self.scale = Parameter((bw.std() / torch.sign(bw).std()).float().to(ba.device))\n\n    def forward(self, input):\n        bw = self.weight\n        ba = input\n        bw = bw - bw.mean()\n\n        if self.scale.item() == 0.0:\n            self.reset_scale(input)\n\n        bw = BinaryQuantize().apply(bw)\n        bw = bw * self.scale\n        if self.binary_act:\n            ba = BinaryQuantize().apply(ba)\n        output = F.linear(ba, bw)\n        return output\n\n\nclass BiConv1dLSR(torch.nn.Conv1d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1,\n                 bias=False, padding_mode='zeros'):\n        super(BiConv1dLSR, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            groups, bias, padding_mode)\n        self.register_parameter('scale', None)\n\n    def reset_scale(self, input):\n        bw = self.weight\n        ba = input\n        bw = bw - bw.mean()\n        if self.padding_mode == 'circular':\n            expanded_padding = ((self.padding[0] + 1) // 2, self.padding[0] // 2)\n            self.scale = Parameter((F.conv1d(F.pad(ba, expanded_padding, mode='circular'),\\\n                                  bw, self.bias, self.stride,\\\n                                  _single(0), self.dilation, self.groups).std() / \\\n                F.conv1d(torch.sign(F.pad(ba, expanded_padding, mode='circular')),\\\n                         torch.sign(bw), self.bias, self.stride,\\\n                         _single(0), self.dilation, self.groups).std()).float().to(ba.device))\n        else:\n            self.scale = Parameter((F.conv1d(ba, bw, self.bias, self.stride, self.padding, self.dilation, self.groups).std() \\\n                                   / F.conv1d(torch.sign(ba), torch.sign(bw), self.bias, self.stride, self.padding, self.dilation, self.groups).std()).float().to(ba.device))\n\n    def forward(self, input):\n        bw = self.weight\n        ba = input\n        bw = bw - bw.mean()\n        if self.scale is None:\n            self.reset_scale(input)\n        bw = BinaryQuantize().apply(bw)\n        ba = BinaryQuantize().apply(ba)\n        bw = bw * self.scale\n\n        if self.padding_mode == 'circular':\n            expanded_padding = ((self.padding[0] + 1) // 2, self.padding[0] // 2)\n            return F.conv1d(F.pad(ba, expanded_padding, mode='circular'),\n                            bw, self.bias, self.stride,\n                            _single(0), self.dilation, self.groups)\n        return F.conv1d(ba, bw, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n# EMA offsets for DGCNN (models/dgcnn.py)\noffset_map_dgcnn = {\n  20: -1.8268,\n  1024: -3.2041\n}\n\n# Snippet showing EMA application in BasicBiDGCNN.forward\n# ...\n# if self.ema_max:\n#     x1 = x1 + offset_map_dgcnn[20]\n# x2 = self.conv2(x1, batch)\n# if self.ema_max:\n#     x2 = x2 + offset_map_dgcnn[20]\n# out = self.lin1(torch.cat([x1, x2], dim=1))\n# if self.pool == 'max':\n#     out = global_max_pool(out, batch)\n#     if self.ema_max:\n#         out = out + offset_map_dgcnn[1024]\n# else:\n#     out = global_mean_pool(out, batch)\n# ...\n\n# Helper Conv1d for PointNet (models/pointnet.py)\nclass Conv1d(nn.Module):\n    def __init__(self, inplane, outplane, Linear):\n        super().__init__()\n        self.lin = Linear(inplane, outplane)\n\n    def forward(self, x):\n        B, C, N = x.shape\n        x = x.permute(0, 2, 1).contiguous().view(-1, C)\n        x = self.lin(x).view(B, N, -1).permute(0, 2, 1).contiguous()\n        return x\n\n# EMA offsets for PointNet (models/pointnet.py)\noffset_map_pointnet = {\n    1024: -3.2041,\n    2048: -3.4025,\n    4096: -3.5836\n}\n\n# Snippet showing EMA application in BiSTN3d.forward/BiSTNkd.forward\n# ...\n# if self.pool == 'ema-max':\n#     x = self.bn3(self.conv3(x)) + offset_map_pointnet[N] # N is num_points\n#     x = torch.max(x, 2, keepdim=True)[0]\n#     x = x.view(-1, 1024)\n# ...\n\n# Snippet showing EMA application in BiPointNetEncoder.forward (after conv3 and before pooling)\n# ...\n# if self.pool == 'ema-max':\n#     if self.use_bn:\n#         x = torch.max(x, 2, keepdim=True)[0] + offset_map_pointnet[N]\n#     else:\n#         x = torch.max(x, 2, keepdim=True)[0] - 0.3 # fixed offset for non-BN case\n#     x = x.view(-1, 1024)\n# ...\n\n# EMA offsets for PointNet2 (models/pointnet2.py)\noffset_map_pointnet2 = {\n  64: -2.2983,\n  1024: -3.2041\n}\n\n# Snippet showing EMA application in SAModule.forward (within PointNet2)\n# ...\n# x = self.conv(x, (pos, pos[idx]), edge_index)\n# if self.ema_max:\n#     x = x + offset_map_pointnet2[64] # 64 is max_num_neighbors\n# pos, batch = pos[idx], batch[idx]\n# return x, pos, batch\n# ...\n\n# Snippet showing EMA application in GlobalSAModule.forward (within PointNet2)\n# ...\n# if self.aggr == 'ema-max':\n#     x = global_max_pool(x, batch) + offset_map_pointnet2[1024]\n# ...",
        "experimental_info": "The BiPointNet method addresses binarization challenges in point cloud networks through two core components: Entropy-Maximizing Aggregation (EMA) and Layer-wise Scale Recovery (LSR).\n\n1.  **Entropy-Maximizing Aggregation (EMA)**:\n    *   **Mechanism**: EMA mitigates information loss from global pooling by shifting feature distributions to maximize entropy. This is primarily implemented for the `ema-max` pooling strategy.\n    *   **Implementation**: Pre-defined constant offsets (`delta*`) are stored in `offset_map` dictionaries (e.g., `offset_map_dgcnn`, `offset_map_pointnet`, `offset_map_pointnet2`). These offsets are empirically determined for different layer outputs or neighborhood sizes (e.g., 20, 64 for k-NN; 1024, 2048, 4096 for global features or point cloud sizes).\n    *   **Application**: In the forward pass of models (e.g., `BasicBiDGCNN`, PointNet's `BiSTN3d`, `BiSTNkd`, `BiPointNetEncoder`, PointNet2's `SAModule`, `GlobalSAModule`), when `ema-max` pooling is selected, these offsets are added to the feature maps either before or during the max-pooling operation. This shifts the input distribution, thereby maximizing the information entropy of the binarized features. For instance, in DGCNN, features `x1`, `x2`, and the globally pooled `out` are all shifted by their corresponding `offset_map` values. For PointNet, the offset `offset_map_pointnet[N]` is adaptively applied based on the number of points `N` in the segment. EMA-avg corresponds to standard mean pooling without offsets.\n\n2.  **Layer-wise Scale Recovery (LSR)**:\n    *   **Mechanism**: LSR addresses scale distortion introduced by binarization, which is critical for maintaining the functionality of scale-sensitive network structures.\n    *   **Implementation**: This is integrated into custom binarized layers: `BiLinearLSR` (for linear layers) and `BiConv1dLSR` (for 1D convolutional layers). Each such layer contains a single, learnable `self.scale` parameter.\n    *   **Initialization**: The `reset_scale` method dynamically initializes `self.scale` at the beginning of training (or if it's unset). It computes the ratio of the standard deviation of the full-precision output to the standard deviation of the binarized output for that specific layer. This ensures a data-driven initial value for scale recovery. The `scripts/main.py` suggests a `scale_dynamic_init` flag to enable this dynamic initialization.\n    *   **Forward Pass**: During inference, after binarizing the weights (and optionally activations) using `BinaryQuantize`, these binary weights are multiplied by the `self.scale` factor, effectively restoring the output scale of the layer. The `self.scale` parameter is then learned during training.\n    *   **Activations**: `Hardtanh` is frequently used as the activation function in these binarized networks, providing bounded outputs suitable for binarization or intermediate representations.\n\n**Overall Integration**: BiPointNet architectures, such as `BiDGCNNLSREMax`, `BiPointNetLSREMax`, and `BiPointNet2LSREMax`, combine LSR for scale preservation in binarized layers with EMA for improved feature aggregation. Variants like `BiPointNetLSRMean` demonstrate the flexibility to use LSR with different aggregation strategies (e.g., mean pooling instead of `ema-max`)."
      }
    },
    {
      "title": "LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection",
      "abstract": "Due to highly constrained computing power and memory, deploying 3D\nlidar-based detectors on edge devices equipped in autonomous vehicles and\nrobots poses a crucial challenge. Being a convenient and straightforward model\ncompression approach, Post-Training Quantization (PTQ) has been widely adopted\nin 2D vision tasks. However, applying it directly to 3D lidar-based tasks\ninevitably leads to performance degradation. As a remedy, we propose an\neffective PTQ method called LiDAR-PTQ, which is particularly curated for 3D\nlidar detection (both SPConv-based and SPConv-free). Our LiDAR-PTQ features\nthree main components, \\textbf{(1)} a sparsity-based calibration method to\ndetermine the initialization of quantization parameters, \\textbf{(2)} a\nTask-guided Global Positive Loss (TGPL) to reduce the disparity between the\nfinal predictions before and after quantization, \\textbf{(3)} an adaptive\nrounding-to-nearest operation to minimize the layerwise reconstruction error.\nExtensive experiments demonstrate that our LiDAR-PTQ can achieve\nstate-of-the-art quantization performance when applied to CenterPoint (both\nPillar-based and Voxel-based). To our knowledge, for the very first time in\nlidar-based 3D detection tasks, the PTQ INT8 model's accuracy is almost the\nsame as the FP32 model while enjoying $3\\times$ inference speedup. Moreover,\nour LiDAR-PTQ is cost-effective being $30\\times$ faster than the\nquantization-aware training method. Code will be released at\n\\url{https://github.com/StiphyJay/LiDAR-PTQ}.",
      "full_text": "LIDAR-PTQ: P OST-TRAINING QUANTIZATION FOR POINT CLOUD 3D O BJECT DETECTION Sifan Zhou1,2* , Liang Li2, Xinyu Zhang2, Bo Zhang2, Shipeng Bai3*, Miao Sun4 Ziyu Zhao1, Xiaobo Lu1†, Xiangxiang Chu2†‡ 1Southeast University 2Meituan Inc 3Zhejiang University 4Nanyang Technological University sifanjay@gmail.com, xblu2013@126.com, chuxiangxiang@meituan.com ABSTRACT Due to highly constrained computing power and memory, deploying 3D lidar-based detectors on edge devices equipped in autonomous vehicles and robots poses a crucial challenge. Being a convenient and straightforward model compression approach, Post-Training Quantization (PTQ) has been widely adopted in 2D vision tasks. However, applying it directly to 3D lidar-based tasks inevitably leads to performance degradation. As a remedy, we propose an effective PTQ method called LiDAR-PTQ, which is particularly curated for 3D lidar detection (both SPConv-based and SPConv-free). Our LiDAR-PTQ features three main compo- nents, (1) a sparsity-based calibration method to determine the initialization of quantization parameters, (2) a Task-guided Global Positive Loss (TGPL) to reduce the disparity between the final predictions before and after quantization, (3) an adaptive rounding-to-nearest operation to minimize the layerwise reconstruction error. Extensive experiments demonstrate that our LiDAR-PTQ can achieve state- of-the-art quantization performance when applied to CenterPoint (both Pillar-based and V oxel-based). To our knowledge, for the very first time in lidar-based 3D detection tasks, the PTQ INT8 model’s accuracy is almost the same as the FP32 model while enjoying 3× inference speedup. Moreover, our LiDAR-PTQ is cost- effective being 30× faster than the quantization-aware training method. Code will be released at https://github.com/StiphyJay/LiDAR-PTQ. 1 INTRODUCTION LiDAR-based 3D detection has a wide range of applications in self-driving and robotics. It is important to detect the objects in the surrounding environment fastly and accurately, which places a high demand for both performance and latency. Currently, mainstream grid-based 3D detectors convert the irregular point cloud into arranged grids (voxels/pillars), and achieve top-ranking perfor- mance (Jiageng Mao, 2023) while facing a crucial challenge when deploying 3D lidar-based models on resource-limited edge devices. Therefore, it is important to improve the efficiency of grid-based 3D perception methods (e.g., reduce memory and computation cost). Quantization is an efficient model compression approach for high-efficiency computation by reducing the number of bits for activation and weight representation. Compared to quantization-aware training (QAT) methods, which require access to all labeled training data and substantial computation re- sources, Post-training quantization (PTQ) is more suitable for fast and effective industrial applications. This is because PTQ only needs a small number of unlabeled samples as calibration set. Besides, PTQ does not necessitate retraining the network with all available labeled data, resulting in a shorter quantization process. Although several advanced PTQ methods (Nagel et al., 2020; Li et al., 2021; Wei et al., 2022; Yao et al., 2022) have been proposed for RGB-based detection tasks, applying it directly to 3D lidar-based tasks inevitably leads to performance degradation due to the differences between images and point clouds. As shown in Fig 1, the inherent sparsity and irregular distribution of LiDAR point clouds present new challenges for the quantization of 3D Lidar-based detectors. (1) The sparsity of point cloud. *Work done as an intern at Meituan. † Corresponding author. ‡Project leader. 1 arXiv:2401.15865v1  [cs.CV]  29 Jan 2024Redundant empty pillarNon-empty pillarForeground points (90% zero pixels on BEV features)(only 10% non-empty gridprojected from 2D/3D Pillar/V oxel)(4m x 2m vehicle only occupies 40 x 20 pixels on the final 1504x 1504feature map) Figure 1: The sparsity of point cloud on 3D LiDAR-based object detection. Orange area means empty area, blue point means the point cloud (non-empty area) in a scenario, green box means the 3D Bboxes, and red point means foreground points. Different from dense RGB images, non-zero pixels only occupy a very limited part of the whole scenario (about 10% in Waymo dataset (Sun et al., 2020)). For example, the huge number of zero pixel lead to significant differences in activation distribution compared to dense RGB-based tasks. (2) Larger arithmetic range. Compared with the 8-bit (0-255) RGB images, the point coordinates after voxelization are located in a 1504 × 1504 × 40 (voxel size = 0.1m) 3D space in Waymo dataset, which makes it more susceptible to the effects of quantization (such as clipping error). (3) Imbalance between foreground instances and large redundant background area. For example, based on CenterPoint-V oxel (Yin et al., 2021), a vehicle with4m × 2m occupies only 40 × 20 pixels in the input 1504 × 1504 BEV feature map. Such small foreground instances and large perception ranges in 3D detection require the quantized model to have less information loss to maintain detection performance. Therefore, these challenges hinder the direct application of quantization methods developed for 2D vision tasks to 3D point cloud tasks. To tackle the above challenge, we propose an effective PTQ method called LiDAR-PTQ, which is specifically curated for 3D LiDAR-based object detection tasks. Firstly, we introduce a sparsity-based calibration method to determine the initialization of quantization parameters on parameter space. Secondly, we propose Task-guided Global Positive Loss (TGPL) to find the quantization parameter on model space that is suitable for final output performance. Thirdly, 10 50Inference Speed (FPS) Accuracy  (mAPH/ L2) 66646260 56 1000 CenterPoint-V oxel -0.07 150 FP32 58 54 CenterPoint-Pillar -1.25-3.02-4.54 -0.20-2.18-1.26-4.05 QDROP (ICLR2022)PD-Quant (CVPR2023)LiDAR-PTQ (Ours) BRECQ (ICLR2021)INT8 FP32 INT8 Figure 2: Performance comparison we utilize an adaptive rounding value to mitigate the performance gap between the quantized and the full precision model. The proposed LiDAR-PTQ framework is a general and effective quantization method for both SPConv-based and SPConv-free 3D detection models. Extensive experiments on various datasets evaluate that our LiDAR-PTQ can achieve state-of-the-art quantization performance (Fig 2) when applied to CenterPoint (both Pillar- based and V oxel-based). To our knowledge, for the very first time in LiDAR-based 3D detection tasks, the PTQ INT8 model’s accuracy is almost the same as the FP32 model while enjoying 3× inference speedup. Moreover, our LiDAR-PTQ is cost-effective being 30× faster than QAT method. We will release our code to the community. Here, we summarize our main contributions as follows: • Unveiling the root cause of performance collapse in the quantization of the 3D LiDAR-based detection model. Furthermore, we propose the sparsity-based calibration method to initialize the quantization parameter. • TGPL: A Task-guided Global Positive Loss (TGPL) function to minimize the output disparity on model space which helps improve the quantized performance. • LiDAR-PTQ: a general and effective quantization method for both SPConv-based and SPConv-free 3D detection models. Extensive experiments demonstrate LiDAR-PTQ can achieve state-of-the-art quantization performance on CenterPoint (both Pillar-based and V oxel-based). 2• To our knowledge, for the very first time in LiDAR-based 3D detection tasks, the PTQ INT8 model’s accuracy is almost the same as the FP32 model while enjoying 3× inference speedup. Moreover, LiDAR-PTQ is cost-effective being 30× faster than QAT method. 2 PRELIMINARIES LiDAR-based 3D object detection.Given a point set with N points in the 3D space, which is defined as P = {pi = [xi, yi, zi, ri]T ∈ RN×4}, where xi, yi, zi denote the coordinate values of each point along the axes X, Y , Z, respectively, andri is the laser reflection intensity. Given a set of object in the 3D scene B = {bj = [xj, yj, zj, hj, wj, lj, θj, cj]T ∈ RM×8}, where M is the total number of objects, bi is the i-th object in the scene, xj, yj, zj is the object’s center, hj, wj, lj is the object’s size, θj is the object’s heading angle and cj is the object’s class. The task of LiDAR-based 3D object detection is to detect the 3D boxes B from the point cloud P accurately. Quantization for tensor.The quantization operation is defined as the mapping of a floating-point (FP) value x (weights or activations) to an integer value xint according to the following equation: xint = clamp(⌊x s ⌉ + z, qmin, qmax) (1) where ⌊·⌉ is the rounding-to-nearest operator, which results in the rounding error ∆r. The function clamp(·) clips the values that lie outside of the integer range [qmin, qmax], incurring a clipping error ∆c. xint represents the quantized integer value. z is zero-point. s denotes the quantization scale factor, which reflects the proportional relationship between FP values and integers. [qmin, qmax] is the quantization range determined by the bit-width b. Here, we adopt uniform signed symmetric quantization, as it is the most widely used in TensorRT (Migacz, 2017) and brings significant acceleration effect. Therefore, qmin = −2b−1 and qmax = 2b−1 − 1. Nonuniform quantization (Jeon et al., 2022) is challenging to deploy on hardware, so we disregard it in this work. Generally, weights can be quantized without any need for calibration data. Therefore, the quantization of weights is commonly solved using grid search or analytical approximations with closed-form solution (Banner et al., 2019; Nagel et al., 2021) to minimize the mean squared error (MSE) in PTQ. However, activation quantization is input-dependent, so often requires a few batches of calibration data for the estimation of the dynamic ranges to converge. To approximate the real-valued inputx, we perform the de-quantization step: ˆx = (xint − z) · s (2) where ˆx is the de-quantized FP value with an error that is introduced during the quantization process. Quantization range.If we want to reduce clipping error ∆c, we can increase the quantization scale factor s to expand the quantization range. However, increasings leads to increased rounding error ∆r because ∆r lies in the range \u0002 −s 2 , s 2 \u0003 . Therefore, the key problem is how to choose the quantization range (xmin, xmax) to achieve the right trade-off between clipping and rounding error. Specifically, when we set fixed bit-width b, the quantization scale factor s is determined by the quantization range: s = (xmax − xmin) / \u0000 2b − 1 \u0001 (3) There are two common methods for quantization range setting. i): Max-min calibration.We can define the quantization range as: xmax = max(|x|), xmin = −xmax (4) to cover the whole dynamic range of the floating-point value x. This leads to no clipping error. However, this approach is sensitive to outliers as strong outliers may cause excessive rounding errors. ii): Entropy calibration.TensorRT (Migacz, 2017) minimize the information loss betweenx and ˆx based on the KL divergence to determine the quantization range: arg min xmin,xmax DKL(x, ˆx) (5) where DKL denotes the Kullback-Leibler divergence function. The entropy calibration will saturate the activations above a certain threshold to remove outliers. More details refer to the appendix. Quantization for network.For a float model with N layer, we primarily focus on the quantization of convolutional layers or linear layers, which mainly involves the handling of weights and activations. 3For a given layer Li, we initially execute quantization operations on its weight and input tensor, as illustrated in Eq 14 and 2, yielding ˆWi and ˆIi. Consequently, the quantized output of this layer can be expressed as follows. ˆAi = f(BN(ˆIi ⊛ ˆWi) (6) where ⊛ denotes the convolution operator, BN(·) is the Batch-Normalization procedure, and f(·) is the activation function. Quantization works generally take into account the convolution, Batch Normalization (BN), and activation layers. 3 METHODOLOGY Here, we first conduct PTQ ablation study on the CenterPoint-Pillar (Yin et al., 2021) model using two different calibrators (Entropy and Max-min) on Waymo val set. As shown in Table 1, when using INT8 quantization, the performance drop is severely compromised for both the calibration method, especially for the entropy calibrator with a significant accuracy drop of -38.67 mAPH/L2. Table 1: Ablation study. Method Bits(W/A) LEVEL2 mAPH Mean Vehicle Pedestrian Full Prec. 32/32 60.32 65.42 55.23 Entropy 8/8 21.65(-38.67) 29.41(-36.02) 11.89(-43.34) Max-Min 8/8 52.91(-7.41) 55.37(-10.05) 50.45(-4.78) However, directly employ- ing the Max-min calibrator yielded better results, yet not unsatisfactory. It is entirely contrary to our experience in 2D model quantization, where entropy calibration effectively mitigates the impact of out- liers, thereby achieving superior results (Nagel et al., 2021). Similar observations are also discussed in St¨acker et al. (2021). This anomaly prompts us to propose a general and effective PTQ method for 3D LiDAR-based detectors. In Waymo dataset, the official evaluation tools evaluated the methods in two difficulty levels: LEVEL 1 for boxes with more than five LiDAR points, and LEVEL 2 for boxes with at least one LiDAR point. Here we report the metrics in Mean Average Precision with Heading / LEVEL 2 (mAPH/L2), which is a widely adopted metric by the community. 3.1 L IDAR-PTQ F RAMEWORK In this paper, we propose a post-training quantization framework for point cloud models, termed LiDAR-PTQ. Our LiDAR-PTQ could enable the quantized model to achieve almost the same performance as the FP mode, and there is no extra huge computation cost and access to labeled training data. This framework primarily comprises three components. i) Sparsity-based calibration: We employ a Max-min calibrator equipped with a lightweight grid search to appropriately initialize the quantization parameters for both weights and activations. ii) Task-guided Global Positive Loss (TGPL): This component utilizes a specially designed foreground-aware global supervision to further optimize the quantization parameters of activation. iii) Adaptive rounding-to-nearest: This module aims to mitigate the weight rounding error ∆r by minimizing the layer-wise reconstruction error. In summary, our method first initializes the quantization parameters for weights and activations through a search in the parameter space, and then further refine them through a process of supervised optimization in the model space. Consequently, our method is capable of achieving a quantized accuracy that almost matches their float counterparts for certain lidar detectors. We formulate the our LiDAR-PTQ algorithm for a full precision 3D detector in Algorithm 2. Next, we will provide detailed explanations for these three parts. 3.2 S PARSITY -BASED CALIBRATION Here, in order to delve into the underlying reasons for the huge performance gap (31.29 mAPH/L2 in Tab 1) between Max-min and entropy calibrator. We statistically analyze the numerical distribution of feature maps of both RGB-based models and LiDAR-based object detection models, and visualize the main diversity as shown in Fig 3. The main reasons affecting the quantization performance can be summarized in two points: 4Algorithm 1LiDAR-PTQ quantization Input: Pretrained FP model with N layers; Calibration dataset Dc, iteration T. Output: quantization parameters of both activation and weight in network, i.e., weight scale sw, weight zero-point zw, activation scale sa, activation zero-point za and adaptive rounding value for weight θ. 1: Optimize only weight quantization parameters sw and zw to minimize Eq 16 in every layer using the grid search algorithm; 2: input Dc to FP network to get the FP final output Ofp 3: for Ln = {Li|i = 1, 2, ...N} do 4: Optimize only activation quantization parameters sa and za to minimize Eq 16 in layer Li using the grid search algorithm; 5: Collect input data Ii to the FP layer Li; 6: Input Ii to quantized layer Lq i and FP layer Li to get quantized output ˆAi and FP output Ai; 7: Input ˆAi to the following FP network to get output ˆOpar of partial-quantized network; 8: for all j = 1, 2, . . . , T-iteration do 9: Check quantized output ˆAi and FP output and calculate Llocal using Eq 11; 10: Check partial-quantized network output ˆOpar and FP final output Ofp to calculate LTGPL using Eq 9; 11: Optimize quantization parameters sw, zw, sa, and za, θ of layer Li to minimize Ltotal using Eq 12; 12: end for 13: Quantize layer Li with the learnable quantization parameters sw, zw, sa, and za, θ; 14: end for (a) RGB image data distribution(b) LiDAR point cloud data distribution Entropy rangeMin-max range Entropy range Min-max range Figure 3: The diagram of data distribution for RGB-based and LiDAR-based object detection. Orange and green denote the data distribution of the entire feature map and foreground feature. i) Huge sparsity lead to inappropriate quantization range.As shown in Fig 1 and 3, the sparsity of point cloud makes the whole BEV feature map exist a large number of zero pixels. Therefore, the entropy calibrator will statistic the feature value including zero pixels (≈ 90%) to minimize the information loss, which leads to the values outside the quantization range being clipped. However, these truncated values contain rich geometric representations that could used for final object detection. ii) Point cloud features are more sensitive to quantization range.Point cloud explicitly gauges spatial distances, and shapes of objects by collecting laser measurement signals from environments. During voxelization process, the raw point cloud coordinates, i.e.,x, y, z in the ego-vehicle coordinate system are encoded as part of voxel features that preserve essential geometric information. Specif- ically, the arithmetic range of the input point cloud coordinates increases with detection distance. Therefore, the arithmetic range in the voxel feature is strongly correlated with detection distance. In other words, the arithmetic range of point cloud is relevant to the geometrics. Furthermore, we also conduct an ablation study with different range distances on waymo val set. As shown in Tab 2, we find that the decline in accuracy is exacerbated as the distance increases. Table 2: Ablation study in different range . Method Bits(W/A) Vehicle LEVEL2 mAPHMean 0-30 m 30-50 m 50- ∞m Full Prec. 32/32 88.77 65.23 38.09Entropy 8/8 60.03 (-28.74) (-32.4%)22.46(-42.77) (-65.6%)5.90(-32.19) (-84.5%)Max-min 8/8 87.14 (-1.63) (-1.8%)57.49(-7.74) (-11.9%)22.98(-15.11) (-39.7%) For entropy calibrator, the quantized performance on long-range metrics (50m - inf) is terribly damaged (5.90 mAPH/L2, up to 84.5% drop), while accuracy on short-range metrics (0-30m) remains well (60.03 mAPH/L2, 32.4% drop). This is because the entropy calibrator 5provides an inappropriate quantization range, resulting in a significant clipping error. Therefore, a large number of values with geometrics info are truncated and consequently a substantial degradation in model accuracy. On the contrary, for the Max-min calibrator, which covers the whole dynamic range of the FP activation, the values with geometric information are preserved effectively. Therefore, its performance in different range metrics performs well, especially on short-range metrics (0-30m), which only drops 1.63 mAPH/L2 (1.8%) than FP model. Drawing upon the above findings, we conclude that the commonly used calibration method for RGB images is sub-optimal, while Max-min is more suitable for 3D point clouds. Therefore, we adopt Max-min calibrator for both weights and activations to mitigate the impact of high sparsity. Besides, to get more finer-grained scale factor and avoid the influence of outliers on rounding error ∆r, a lightweight grid search (Banner et al., 2019; Choukroun et al., 2019) is incorporated to further optimize the quantization parameters. Specifically, for a weight or activation tensor X, firstly obtain the xmax and xmin according to Eq.4, and calculate the initial quantization parameter s0 following Eq. 15. Then linearly divide the interval [αs0, βs0] into T candidate bins, denoted as {st}T t=1. α, β and T are designed to control the search range and granularity. Finally, search {st}T t=1 to find the optimal sopt that minimizes the quantization error, arg min st ∥(X − ˆX(sl))∥2 F (7) ∥ · ∥2 F is the Frobenius norm (MSE Loss). Refer to appendix for more details about grid search. 3.3 T ASK -GUIDED GLOBAL POSITIVE LOSS The aforementioned calibration initialization approach can effectively improve the quantization accuracy of lidar detectors, but there is still a significant gap compared with the float model. Both empirical and theoretical evidence (Li et al., 2021; Wei et al., 2022; Liu et al., 2023a) suggest that solely minimizing the quantization error in parameter space dose not guarantee equivalent minimization in final task loss within model space. Therefore, it becomes imperative to devise a global supervisory signal specifically tailored for 3D LiDAR-based detection tasks. This supervision would enable further fine-tuning of the quantization parameters p to achieve higher quantized precision. It is crucial to emphasize that this fine-tuning process does not involve labeled training data. Only need to minimize the distance between float output Of and the quantized model’s output ˆO, as depicted in Eq 8 arg min p (Of − ˆO) (8) In this paper, we propose Task-guided Global Positive Loss (TGPL) function to constrain the output disparity between the quantized and FP models. Our TGPL function features two characteristics that contribute to improving the performance of the quantized method: i) Optimal quantization parameter on model space.The TGPL function compares the final output difference between the FP and the quantized models rather than the difference in each layer’s output. ii) Task-guided.As mentioned in Sec1 and Fig 1, there exists extreme imbalance between small informative foreground instances and large redundant background areas in Lidar-based detection tasks. For sparse 3D scenes, it is sub-optimal to imitate all feature pixels on dense 2D images. TGPL function is designed to leverage cues in the FP model’s classification response to guide the quantized model to focus on the important area (i.e. positive sample location) that is relevant to final tasks. In detail, we filter all prediction boxes from the FP model by a threshold γ, then we select the top K boxes. Then we perform NMS (Neubeck & Van Gool, 2006) to get the final prediction as positive boxes (pusedo-labels). Specifically, inspired by the Gaussian label assignment in CenterPoint (Yin et al., 2021), we define positive positions in a soft way with center-peak Gaussian distribution. Finally, for the classification branch, we use the focal loss (Lin et al., 2017) as the heatmap loss Lcls. For the 3D box regression, we make use of the L1 loss Lreg to supervise their localization offsets, size and orientation. The overall TGPL loss consists of two parts as follows: LTGPL = Lcls + αLreg, (9) 63.4 A DAPTIVE ROUNDING -TO-NEAREST Through grid search initialization and TGPL function constrain, the performance of quantized model has been greatly improved, but there is still a gap in achieving comparable accuracy with FP model. Recently, some methods (Wei et al., 2022; Liu et al., 2023a) optimize a variable, called rounding values, to determine whether weight values will be rounded up or down during the quantization process. In this way, the Eq 14 in weight quantization can be formulated as follows: xint = clamp(⌊x + θ s ⌉ + z, qmin, qmax), (10) where θ is the optimization variable for each weight value to decide rounding results up or down (Nagel et al., 2020), i.e., θ S ranges from 0 to 1. Inspired by AdaRound(Nagel et al., 2020), we add a local reconstruction item to help learn the rounding value θ. The local reconstruction item as follows: LLocal = ∥(Wi ⊛ Ii − ˆWi ⊛ Ii)∥2 F (11) where ∥·∥ 2 F is the Frobenius norm and ˆWi are the soft-quantized weights are calculated by Eq 10 and Eq 2. This operation allows us to adapt the rounding value to minimize information loss according to the calibration data, ensuring that the quantization process preserves important details. By adjusting the rounding value, we can achieve better performance of LiDAR-PTQ. Finally, the overall loss of our LiDAR-PTQ consists of two parts as follows: Ltotal = λ1Llocal + λ2LTGPL , (12) 4 EXPERIMENTS Dataset. To evaluate the effectiveness of our proposed Lidar-PTQ, we conduct main experiments on large-scale autonomous driving datasets, Waymo Open Dataset (WOD) (Sun et al., 2020). Implementation Details.In WOD dataset, we randomly sample 256 frames point cloud data from the training set as the calibration data. The calibration set proportions is 0.16% (256/158,081) for WOD. We set the first and the last layer of the network to keep full precision. The learning rate for the activation quantization scaling factor is 5e-5, and for weight quantization rounding is 5e-3. In TGPL loss, we set γ as 0.1, and K as 500. More details in supplements. 4.1 P ERFORMANCE COMPARISON ON WAYMO DATASET Table 3: Performance comparison on Waymo val set. ‡: reimplementation by us. Models Methods Bits(W/A) Mean (L2) Vehicle (L2) Pedestrian (L2) Cyclist (L2) mAP mAPH mAP mAPH mAP mAPH mAP mAPH Full Prec. 32/32 65.78 60.32 65.92 65.42 65.65 55.23 - - CP- Pillar BRECQ 8/8 61.73 56.27 61.87 61.36 61.59 51.18 - - QDROP 8/8 63.60 58.14 63.74 63.23 63.46 53.04 - - PD-QUANT 8/8 64.59 59.06 64.87 64.21 64.32 53.91 - - QAT‡ 8/8 65.56 60.08 65.69 65.17 65.44 54.99 - - LiDAR-PTQ 8/8 65.60 60.12 65.64 65.14 65.55 55.11 - - Full Prec. 32/32 67.67 65.25 66.29 65.79 68.04 62.35 68.69 67.61 CP- V oxel BRECQ 8/8 63.15 60.71 62.53 62.03 63.22 57.49 63.71 62.60 QDROP 8/8 64.70 62.23 63.97 63.38 64.90 59.17 65.24 64.13 PD-QUANT 8/8 66.45 64.00 65.11 64.56 66.91 61.18 67.32 66.25 QAT‡ 8/8 67.63 65.20 66.28 65.76 67.98 62.28 68.62 67.55 LiDAR-PTQ 8/8 67.60 65.18 66.27 65.78 67.95 62.24 68.60 67.52 Due to there are no PTQ methods specially designed for 3D LiDAR-based detection tasks, we reimplement several advanced PTQ methods in 2D RGB-based vision tasks, which are BRECQ (Li et al., 2021), QDROP (Wei et al., 2022) and PD-Quant (Liu et al., 2023a). Specifically, we select well-known CenterPoint (Yin et al., 2021) as our full precision model and report the quantized performance on WOD (Sun et al., 2020) dataset. Because it includes SPConv-based and SPConv-free models, which could effectively verify the generalization of our LiDAR-PTQ. As shown in Tab 3, LiDAR-PTQ achieves state-of-the-art performance and outperforms BRECQ and QDrop by a large margin of 3.87 and 2.00 on CenterPoint-Pillar model and 4.45 and 2.90 on CenterPoint-V oxel model. 7For PD-Quant, a state-of-the-art PTQ method specially designed for RGB-based vision tasks, but it has suboptimal performance on LiDAR-based tasks. Specifically, to solve the over-fitting problem on the calibration set, PD-Quant adjusts activation according to FP model’s BN layer. However, for the point cloud which is more sensitive to the arithmetic range, this design is ineffective and time-consuming, and will lead to accuracy loss. Notably, our LiDAR-PTQ achieves on-par or even superior accuracy than the QAT model and almost without performance drop than the float model. 4.2 T HE EFFECTIVENESS OF LIDAR-PTQ FOR FULLY SPARSE DETECTOR Table 4: The performance of FSD on Waymo val set. Models Methods Bits(W/A) Mean (L2) Vehicle (L2) Pedestrian (L2) Cyclist (L2) mAP mAPH mAP mAPH mAP mAPH mAP mAPH Full Prec. 32/32 73.01 70.94 70.34 69.98 73.95 69.16 74.75 73.69 FSD Entropy 8/8 10.54 9.44 0.06 0.06 21.88 18.86 9.69 9.41 Max-min 8/8 71.24 69.37 68.42 68.18 72.09 67.60 73.22 72.34 LiDAR-PTQ 8/8 72.84 70.73 69.95 69.62 73.85 68.95 74.71 73.63 Recently, there are emerging of some fully sparse 3D detectors, like FSD (Fan et al., 2022), FSD++ (Fan et al., 2023) and V oxelNext (Chen et al., 2023), etc. Here, we take FSD as an example to validate the effectiveness of our LiDAR-PTQ on fully sparse detectors. As shown in Tab 4, adopting entropy calibration still leads to a significant accuracy drop of-61.50. We discover that quantized FSD readily delivers the desired performance while employing a vanilla max-min calibration. Nonetheless, using LiDAR-PTQ can further achieve comparable accuracy to its float counterpart. The experiments demonstrate that LiDAR-PTQ is also applicable to fully sparse detectors. 4.3 A BLATION STUDY Table 5: Ablation study of different components of LiDAR-PTQ on Waymoval set. Models Methods Bits(W/A) Mean (L2) Vehicle (L2) Pedestrian (L2) mAP mAPH mAP mAPH mAP mAPH Full Prec. 32/32 65.78 60.32 65.92 65.42 65.65 55.23 CP- Pillar Max-min 8/8 57.33 52.91 55.64 55.37 59.02 50.45 +GRID S 8/8 63.66 58.39 63.37 62.87 63.96 53.91 +TGPL 8/8 64.81 59.40 65.12 64.53 64.50 54.27 +Round 8/8 65.60 60.12 65.64 65.14 65.55 55.11 Here, we conduct ablation study of different components in our LiDAR-PTQ based on CenterPoint- Pillar model to verify their effects. As shown in Tab 5, based on the selected Max-min calibrator, we could obtain 5.48 mAPH/L2 performance gain by using a lightweight grid search method. However, grid search only minimizes reconstruction error in parameter space, which is not equivalent to minimize the final performance loss. Therefore, by introducing the proposed TGPL function to fine-tune quantization parameters in model space, the performance of quantized model coud be 59.40 mAPH/L2. Finally, by introducing an adaptive rounding value, a freedom degree (Eq 10) is added to mitigate the final performance gap and achieve almost the same performance as the FP model (60.12 vs 60.32). Notably, the performance of FP model is the upper limit of the quantized model because we focus on post-training quantization without labeled training data. 4.4 I NFERENCE ACCELERATION Here, we compared the speed of CenterPoint before and after quantization on an NVIDIA Jeston AGX Orin. This is a resource-constrained edge GPU platform that is widely used in real-word self-driving cars. The speed of quantized model enjoying 3× inference speedup, which demonstrates that our LiDAR-PTQ can effectively improve the efficiency of 3D detection model on edge devices. CP-V oxel 12.4 (FP32) 113.4 (INT8)CP-Pillar0 10 20 40 80 Real time (>10 FPS) Frames Per Second (FPS) 120 38.4 (FP32)31.0 (INT8) NVIDIA JestonAGX Orin Method Bits(W/A) Latency FPS CP- Pillar 32/32 26.01 38.4 8/8 8.82 113.4 CP- V oxel 32/32 80.64 12.4 8/8 32.26 31.0 84.5 C OMPUTATION EFFICIENCY LiDAR-PTQ requires additional computation and fine-tuning process compared to other traditional PTQ methods, resulting in increased time costs. While the quantization time is a limitation of LiDAR- PTQ, compared with other advanced PTQ methods, LiDAR-PTQ’s additional time cost is acceptable. Model QAT BRECQ QDROP PD-QUANTLiDAR-PTQ CP-Pillar 93.82 1.93 1.82 6.72 2.75CP-V oxel 80.511.73 1.64 6.13 2.12 Furthermore, QAT method, the quantiza- tion time of LiDAR-PTQ is very short. For example, CenterPoint-Pillar will take 94 GPU/hour to achieve the same performance as the FP model on WOD dataset, while LiDAR-PTQ takes only 3 GPU/hour, which is 30× faster than the QAT method. It also proves that our LiDAR-PTQ is cost-effective. 5 R ELATED WORKS Post-training Quantization (PTQ).As mentioned in (Krishnamoorthi, 2018), existing quantization methods can be divided into two categories: (1) Quantization-Aware Training (QAT) and (2) Post- Training Quantization (PTQ). QAT methods (Wei et al., 2018; Li et al., 2019; Esser et al., 2019; Zhuang et al., 2020; Chen et al., 2021) require access to all labeled training data, which may not be feasible due to data privacy and security concerns. Compared to Quantization-aware Training (QAT) methods, Post-training quantization (PTQ) methods are simpler to use and allow for quantization with limited unlabeled data. Currently, there are many methods (Wu et al., 2020; Nahshan et al., 2021; Yuan et al., 2022; Liu et al., 2023b; Chu et al., 2024) designed for 2D vision tasks. AdaRound (Nagel et al., 2020) formulates the rounding task as a layer-wise quadratic unconstrained binary optimization problem and achieves a better performance. Based on AdaRound, BRECQ (Li et al., 2021) proposes utilizing block reconstruction to further enhance the accuracy of post-training quantization (PTQ). After that, QDrop (Wei et al., 2022) randomly drops the quantization of activations during PTQ and achieves new state-of-the-art accuracy. PD-Quant (Liu et al., 2023a) considers the global difference of models before and after quantization and adjusts the distribution of activation by BN layer statistics to alleviate the overfitting problem. However, these methods are specially designed for RGB images, and they are not readily transferable to LiDAR point cloud with substantial modal differences. Quantization for 3D Object Detection.With the wide application of 3D object detection, in autonomous driving and robotics, some quantization methods are designed to improve inference speed for onboard deployment applications. With the advance of quantization techniques based on RGB image, QD-BEV (Zhang et al., 2023) achieves smaller size and faster speed than baseline BevFormer (Li et al., 2022) using QAT and distillation on multi-camera 3D detection tasks. For LiDAR-based 3D detection, especially for fully convolutional methods, like PointPillars (Lang et al., 2019), FCOS-LIDAR (Tian et al., 2022), FastPillars (Zhou et al., 2023), etc., effective quantization solutions could significantly speedup their latency to meet the practical requirements. (St ¨acker et al., 2021) find that directly using INT8 quantization for 2D CNN will bring significant performance drop on PointPillars (Lang et al., 2019), where the reduction is even more severe for the entropy calibrator. Besides, BiPointNet (Qin et al., 2021) is a binarization quantization method, which focuses on classification and segmentation tasks based on point cloud captured from small CAD simulation. To the best of our knowledge, there is no quantization solution designed for large-scale outdoor LiDAR-based 3D object detection methods in self-driving. 6 C ONCLUSION AND FUTURE WORK In this paper, we analyze the root cause of the performance degradation of point cloud data during the quantization process. Then we propose an effective PTQ method called LiDAR-PTQ, which is particularly designed for 3D LiDAR-based object detection tasks. Our LiDAR-PTQ features three main components: (1) a sparsity-based calibration method to determine the initialization of quantiza- tion parameters (2) a Task-guided Global Positive Loss (TGPL) to reduce the disparity on the final task. (3). an adaptive rounding-to-nearest operation to minimize the layer-wise reconstruction error. Extensive experiments demonstrate that our LiDAR-PTQ can achieve state-of-the-art performance on CenterPoint (both pillar-based and voxel-based). To our knowledge, for the very first time in lidar-based 3D detection tasks, the PTQ INT8 model’s accuracy is almost the same as the FP32 model while enjoying 3× inference speedup. Moreover, our LiDAR-PTQ is cost-effective being 30× faster than the quantization-aware training method. Given its effectiveness and efficiency, we hope that our LiDAR-PTQ can serve as a valuable quantization tool for current mainstream grid-based 3D detectors and push the development of practical deployment of 3D detection models on edge devices. Besides, we believe that the low-bit quantization of 3D detectors will bring further efficiency improvement. This remains an open problem for future research. 9ACKNOWLEDGMENTS We thank anonymous reviewers for their kind help of this work. This work was supported by National Key R&D Program of China (No. 2022ZD0118700), National Natural Science Foundation of China (No.62271143), and the Big Data Computing Center of Southeast University. REFERENCES Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. Advances in Neural Information Processing Systems, 32, 2019. 3, 6 Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9297–9307, 2019. 13 Holger Caesar, Varun Bankiti, Alex Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. Nuscenes: A multimodal dataset for autonomous driving. pp. 11621–11631, 2020. 13 Peng Chen, Jing Liu, Bohan Zhuang, Mingkui Tan, and Chunhua Shen. Aqd: Towards accurate quantized object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 104–113, 2021. 9 Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. V oxelnext: Fully sparse voxelnet for 3d object detection and tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 8 Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efficient inference. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp. 3009–3018. IEEE, 2019. 6 Xiangxiang Chu, Liang Li, and Bo Zhang. Make repvgg greater again: A quantization-aware approach. AAAI, 2024. 9 Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen- dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. 9 Lue Fan, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Fully sparse 3d object detection. Advances in Neural Information Processing Systems, 35:351–363, 2022. 8 Lue Fan, Yuxue Yang, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Super sparse 3d object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 8 Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro. Mr. biq: Post-training non-uniform quantization based on minimizing the reconstruction error. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12329–12338, 2022. 3 Xiaogang Wang Hongsheng Li Jiageng Mao, Shaoshuai Shi. 3d object detection for autonomous driving: A comprehensive survey. IJCV, 2023. 1 Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. 9 Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpil- lars: Fast encoders for object detection from point clouds. In CVPR, pp. 12697–12705, 2019. 9, 12 Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully quantized network for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2810–2819, 2019. 9 10Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. 2021. 1, 6, 7, 9 Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IX, pp. 1–18. Springer, 2022. 9 Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017. 6 Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang, Xinggang Wang, and Wenyu Liu. Pd-quant: Post-training quantization based on prediction difference metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24427–24437, 2023a. 6, 7, 9 Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers. 2023b. 9 Szymon Migacz. 8-bit inference with tensorrt. In GPU technology conference, volume 2, pp. 5, 2017. 3 Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pp. 7197–7206. PMLR, 2020. 1, 7, 9 Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tij- men Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. 3, 4 Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and Avi Mendelson. Loss aware post-training quantization. volume 110, pp. 3245–3262. Springer, 2021. 9 Alexander Neubeck and Luc Van Gool. Efficient non-maximum suppression. In 18th International Conference on Pattern Recognition (ICPR’06), volume 3, pp. 850–855. IEEE, 2006. 6 Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, and Hao Su. Bipointnet: Binary neural network for point clouds. In ICLR, 2021. 9 Lukas St¨acker, Juncong Fei, Philipp Heidenreich, Frank Bonarens, Jason Rambach, Didier Stricker, and Christoph Stiller. Deployment of deep neural networks for object detection on edge ai devices with runtime optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop, pp. 1015–1022, 2021. 4, 9 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. pp. 2446–2454, 2020. 2, 7, 13 Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. InEuropean conference on computer vision, pp. 685–702. Springer, 2020. 13 Zhi Tian, Xiangxiang Chu, Xiaoming Wang, Xiaolin Wei, and Chunhua Shen. Fully convolutional one-stage 3d object detection on lidar range images. Advances in Neural Information Processing Systems, 35:34899–34911, 2022. 9 Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: randomly dropping quantization for extremely low-bit post-training quantization. 2022. 1, 6, 7, 9 Yi Wei, Xinyu Pan, Hongwei Qin, Wanli Ouyang, and Junjie Yan. Quantization mimic: Towards very tiny cnn for object detection. In Proceedings of the European conference on computer vision (ECCV), pp. 267–283, 2018. 9 11Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, and Debing Zhang. Easyquant: Post-training quantization via scale optimization. arXiv preprint arXiv:2006.16669, 2020. 9 Hongyi Yao, Pu Li, Jian Cao, Xiangcheng Liu, Chenying Xie, and Bingzhang Wang. Rapq: Rescuing accuracy for power-of-two low-bit post-training quantization. arXiv preprint arXiv:2204.12322, 2022. 1 Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. pp. 11784–11793, 2021. 2, 4, 6, 7, 12, 13 Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. pp. 191–207, 2022. 9 Yifan Zhang, Zhen Dong, Huanrui Yang, Ming Lu, Cheng-Ching Tseng, Yandong Guo, Kurt Keutzer, Li Du, and Shanghang Zhang. Qd-bev: Quantization-aware view-guided distillation for multi-view 3d object detection. 2023. 9 Sifan Zhou, Zhi Tian, Xiangxiang Chu, Xinyu Zhang, Bo Zhang, Xiaobo Lu, Chengjian Feng, Zequn Jie, Patrick Yin Chiang, and Lin Ma. Fastpillars: A deployment-friendly pillar-based 3d detector. arXiv preprint arXiv:2302.02367, 2023. 9 Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning for point cloud based 3d object detection. pp. 4490–4499, 2018. 12 Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492, 2019. 13 Bohan Zhuang, Lingqiao Liu, Mingkui Tan, Chunhua Shen, and Ian Reid. Training quantized neural networks with a full-precision auxiliary module. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1488–1497, 2020. 9 APPENDIX A: L IDAR-PTQ FOR DIFFERENT DETECTORS CenterPoint (Yin et al., 2021) integrates two milestone works in LiDAR-based BEV de- tection, V oxelNet (Zhou & Tuzel, 2018) and PointPillars (Lang et al., 2019) as CP- Pillar and CP-V oxel. In particular, CP-Pillar and CP-V oxel have different network design. Method representationbackbone neck head CP-Pillar Pillar dense dense dense CP-V oxel V oxel sparse dense dense FSD Point+V oxel sparse sparse sparse The CP-Pillar model is a fully dense convolutional network, while the CP-V oxel model includes SP- Conv and dense convolution. Our results on Cen- terPoint (-pillar and -voxel) demonstrate that: i) Lidar-PTQ is applicable to pillar-based and voxel- based detectors. ii) Lidar-PTQ is applicable to SPConv and dense convolution operations. Table 6: Performance comparison on nuScene val set. We show the NDS, and mAP for each class. Abbreviations: construction vehicle (CV), pedestrian (Ped), motorcycle (Motor), bicycle (BC) and traffic cone (TC). Models Methods Bits(W/A) NDS mAP Car Truck Bus Trailer CV Ped Motor BC TC Barrier Full Prec. 32/32 60.3 50.0 83.8 50.6 61.8 31.2 9.2 79.4 44.1 20.2 57.7 61.3 CP- Pillar BRECQ 8/8 56.9 43.6 75.9 41.4 54.3 21.6 3.8 78.1 37.4 15.7 55.0 53.3 QDROP 8/8 57.8 45.9 78.8 44.2 57.0 23.8 5.2 78.4 40.1 17.6 56.7 56.8 PD-QUANT 8/8 59.6 48.3 81.8 47.6 59.4 28.2 7.8 78.4 41.6 19.8 57.6 61.0 LiDAR-PTQ 8/8 60.2 49.8 83.7 50.8 61.8 30.6 9.0 79.0 43.6 20.4 57.8 61.0 Full Prec. 32/32 64.8 56.6 84.6 54.5 66.7 36.4 16.9 83.1 56.1 39.6 64.0 64.3 CP- V oxel BRECQ 8/8 62.0 51.2 76.5 46.8 60.5 28.9 12.5 80.4 53.7 34.8 58.8 59.1 QDROP 8/8 63.2 54.0 82.1 48.5 64.9 32.9 15.1 81.1 55.1 36.9 60.9 63.7 PD-QUANT 8/8 63.7 55.2 83.7 51.1 66.6 34.1 16.6 82.8 55.1 36.4 62.6 63.2 LiDAR-PTQ 8/8 64.7 56.5 84.6 54.2 66.7 36.4 16.6 83.3 56.0 39.4 63.6 64.4 12APPENDIX B: P ERFORMANCE COMPARISON ON NU SCENES DATASET To further evaluate the effectiveness of LiDAR-PTQ, we also conducted experiments on nuScenes (Caesar et al., 2020) dataset. Our performance evaluation involves two metrics, average precision (mAP) and nuScenes detection score (NDS). NDS is a weighted average of mAP and other attributes metrics, including translation, scale, orientation, velocity, and other box attributes. As shown in Tab 6, LiDAR-PTQ achieves state-of-the-art performance and outperforms BRECQ and QDrop by a large margin of 6.2 mAP and 3.9 mAP on CenterPoint-Pillar model and 5.3 mAP and 2.5 mAP on CenterPoint-V oxel model. Consistent with the accuracy on the Waymo dataset, our LiDAR-PTQ also achieves almost the same performance as the full precision model on nuScenes dataset. APPENDIX C: L IDAR-PTQ FOR POINT CLOUD SEGMENTATION Table 7: The PTQ performance of SPVNAS on SemanticKITTI val set. Method mIoU car bicycle motorcycle truck other-vehicle person bicyclist motorcyclist road parking sidewalk other-ground building fence vegetation trunk terrain pole traffic sign Full Prec. 65.0 96.3 49.0 77.6 74.4 51.8 75.2 88.2 5.7 93.4 44.6 81.0 3.5 89.5 56.5 87.8 68.4 75.1 67.1 49.6 Entropy 46.9 92.9 34.7 72.1 20.4 37.2 48.5 80.9 5.1 47.8 16.9 28.7 0.2 79.9 47.5 82.9 57.0 44.0 55.9 38.8 Max-min 62.4 94.5 46.2 75.3 73.0 50.2 73.6 86.4 5.7 92.3 41.5 78.9 2.1 87.3 53.4 85.1 65.3 71.8 63.0 48.6 LiDAR-PTQ64.9 96.3 48.7 78.0 74.3 52.2 74.5 87.9 5.9 93.3 44.0 80.9 3.5 89.4 56.4 87.6 68.3 74.5 67.2 49.5 Additionally, we conducted experiments on SemanticKITTI (Behley et al., 2019) dataset for point cloud segmentation to further evaluate the generalization of LiDAR-PTQ. Specifically, we utilize SPVNAS (Tang et al., 2020) as our baseline, which is a representative work in point cloud segmenta- tion task. As shown in Tab 7, adopting entropy calibration leads to a significant accuracy drop of 18.09 mIOU. As for a vanilla max-min calibration, there is still a performance drop 2.64 mIOU for quantized SPVNAS. However, LiDAR-PTQ can further achieve comparable accuracy to its float counterpart. This demonstrates the effectiveness of LiDAR-PTQ on point cloud segmentation tasks as well. APPENDIX D: E XPERIEMNTS DETAILS Dataset. NuScenes dataset (Caesar et al., 2020) uses a LiDAR with 32 lines to collect data, containing 1000 scenes with 700, 150, and 150 scenes for training, validation, and testing, respectively. The metrics of the 3D detection task are mean Average Precision (mAP) and the nuScenes detection score (NDS). Waymo Open Dataset (Sun et al., 2020) uses a LiDAR with 64 beams to collect data, containing 1150 sequences in total, 798 for training, 202 for validation, and 150 for testing. The metrics of the 3D detection task are mAP and mAPH (mAP weighted by heading). In Waymo, LEVEL1 and LEVEL2 are two difficulty levels corresponding to boxes with more than five LiDAR points and boxes with at least one LiDAR point. The detection range in nuScenes and WOD is 50 meters (cover area of 100m × 100m) and 75 meters (cover area of 150m × 150m). Implementation Details.All the FP models in our paper use CenterPoint(Yin et al., 2021) official open-source codes based on Det3D (Zhu et al., 2019) framework. In WOD dataset, we randomly sample 256 frames point cloud data from the training set as the calibration data. The calibration set proportions is 0.16% (256/158,081) for WOD. In nuScenes dataset, the calibration set proportions are 0.91% (256/28,130). We set the first and the last layer of the network to keep full precision. We execute block reconstruction for the backbone and layer reconstruction for the neck and the head with a batch size of 4, respectively. Note that we do not consider using Int8 quantization for the PFN in CenterPoint-Pillar, since the input is 3D coordinates, with approximate range ±102 m and accuracy 0.01 m, so that Int8 quantization in FPN would result in a significant loss of information. The learning rate for the activation quantization scaling factor is 5e-5, and for weight quantization rounding, the learning rate is 5e-3. In TGPL loss, we set γ as 0.1, and K as 500. We execute all experiments on a single Nvidia Tesla V100 GPU. For the speed test, the inference time of all comparison methods is 13measured on an NVIDIA Jeston AGX Orin, a resource-constrained edge GPU platform widely used in real-world autonomous driving. APPENDIX E: E NTROPY CALIBRATION METHOD Given the original and quantized data distribution p(i) and q(i) as follows: DKL(p(i), q(i)) = X i p(i) logp(i) − p(i) logq(i) (13) The entropy calibration method in Algorithm3 Algorithm 2Entropy calibration method Input: FP32 histogram H with N bins, and bit-width b. Output: threshold with min(DKL(p(i), q(i))). Require: len(p) = len(q) 1: for i in range(2b−1, N) do 2: ref dist p(i) = [bin[0], ...,bin[i − 1]] 3: outliers count = sum(bin[i],bin[i + 1], . . . ,bin[N − 1]) 4: ref dist p(i)[i − 1]+ = outliers count 5: p(i) =ref dist p(i)/sum(ref dist p(i)) 6: quantize candidate dist q(i) from [ bin[0], . . . , bin[i − 1]] into 2b−1 levels 7: candidate dist q(i)=interp1d((bin[0], ...,bin[127]), (bin[0], ...,bin[i − 1]),method=′linear′) 8: q(i) =candidate dist q(i)/sum(candidate dist q(i)) 9: divergence[i] = DKL(p(i), q(i)) using Eq 13 10: end for 11: m = argmin \u0000 D = \u0002 divergence[2b−1 − 1], ...,divergence[N − 1] \u0003\u0001 12: threshold = (m + 0.5) ∗ (widthbin) 13: return threshold APPENDIX F: G IRD SEARCH For a weight or activation tensor X, we can get their initial quantization scale factor using the following equation: ˆx = (clamp(⌊x s ⌉ + z, qmin, qmax) − z) · s (14) s = (xmax − xmin) / \u0000 2b − 1 \u0001 (15) arg min st ∥(X − ˆX(sl))∥2 F (16) ∥ · ∥2 F is the Frobenius norm (MSE Loss). Refer to appendix for more details about grid search. Then linearly divide the interval [αs0, βs0] into T candidate bins, denoted as {st}T t=1. α, β and T are designed to control the search range and granularity. Finally, search {st}T t=1 to find the optimal sopt that minimizes the quantization error, The entropy calibration method in Algorithm3 14Algorithm 3Grid search Input: the input of full precision tensor X, bit-width b and T bins. Output: scale factor sopt with min(∥(X − ˆX(sl))∥2 F ). 1: using xmax = max(|x|) get max value of tensor X 2: set range = xmax, cbest = 100 3: set vmin = xmin and vmax = xmax 4: for i in range(1, T) do 5: threshold = range/T/i 6: xmin = −threshold, xmax = threshold 7: get scale st with xmin and xmax using Eq 15 8: input the quantized value ˆx and FP value x using Eq 14 to get score c 9: update vmin and vmax when c < cbest and update cbest = c 10: end for 11: get vmin and vmax with the minimal score c 12: get final scale sopt with vmin and vmax using Eq 15 13: return scale sopt 15",
      "meta_data": {
        "arxiv_id": "2401.15865v1",
        "authors": [
          "Sifan Zhou",
          "Liang Li",
          "Xinyu Zhang",
          "Bo Zhang",
          "Shipeng Bai",
          "Miao Sun",
          "Ziyu Zhao",
          "Xiaobo Lu",
          "Xiangxiang Chu"
        ],
        "published_date": "2024-01-29T03:35:55Z",
        "pdf_url": "https://arxiv.org/pdf/2401.15865v1.pdf",
        "github_url": "https://github.com/StiphyJay/LiDAR-PTQ"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the significant performance degradation of Post-Training Quantization (PTQ) when directly applied to 3D LiDAR-based object detection tasks, which is crucial for deploying models on resource-constrained edge devices. It proposes LiDAR-PTQ, an effective PTQ method tailored for 3D LiDAR detection (both SPConv-based and SPConv-free). Key contributions include unveiling the root cause of performance collapse in 3D LiDAR quantization, proposing a sparsity-based calibration method for quantization parameter initialization, introducing a Task-guided Global Positive Loss (TGPL) to minimize output disparity on the model space, and an adaptive rounding-to-nearest operation. LiDAR-PTQ achieves near FP32 model accuracy with INT8 quantization, provides a 3x inference speedup, and is 30x faster than quantization-aware training (QAT) methods.",
        "methodology": "The LiDAR-PTQ framework comprises three main components: (1) **Sparsity-based Calibration**: It employs a Max-min calibrator with a lightweight grid search to initialize quantization parameters for weights and activations. This approach is chosen after analyzing the unique challenges of LiDAR point clouds, such as high sparsity and sensitivity to arithmetic range, which make entropy calibration suboptimal. The grid search minimizes the Frobenius norm (MSE Loss) for optimal scale factor selection. (2) **Task-guided Global Positive Loss (TGPL)**: This loss function is designed to fine-tune quantization parameters by minimizing the disparity between the final outputs of the full-precision (FP) and quantized models. It focuses on important foreground areas by using a task-guided mechanism, filtering FP model predictions to generate pseudo-labels for positive samples and utilizing Focal Loss for classification and L1 Loss for 3D box regression. (3) **Adaptive Rounding-to-Nearest**: Inspired by AdaRound, this module introduces an optimizable variable for each weight to adaptively determine rounding direction, minimizing layer-wise reconstruction error (Frobenius norm). The overall loss combines a local reconstruction item and the TGPL.",
        "experimental_setup": "Experiments were conducted on large-scale autonomous driving datasets: Waymo Open Dataset (WOD) and nuScenes for 3D object detection, and SemanticKITTI for point cloud segmentation to demonstrate generalization. Calibration data consisted of 256 randomly sampled frames (0.16% for WOD, 0.91% for nuScenes). The full-precision models used were CenterPoint (Pillar-based and Voxel-based) for mainstream detectors and FSD for fully sparse detectors. SPVNAS was used for point cloud segmentation. Comparison methods included reimplementations of advanced 2D RGB-based PTQ methods (BRECQ, QDROP, PD-Quant) and Quantization-Aware Training (QAT). Performance was evaluated using mAPH/L2 on Waymo, NDS and mAP on nuScenes, and mIoU on SemanticKITTI. The first and last layers of the network were kept at full precision. Inference acceleration was measured on an NVIDIA Jeston AGX Orin, an edge GPU platform.",
        "limitations": "LiDAR-PTQ requires additional computation and a fine-tuning process compared to other traditional PTQ methods, leading to increased time costs. While acknowledged as a limitation, the paper argues this additional time cost is acceptable given its significant benefits, especially being 30 times faster than QAT.",
        "future_research_directions": "The paper suggests that low-bit quantization of 3D detectors remains an open problem for future research, implying that further efficiency improvements beyond INT8 could be explored. It also hopes that LiDAR-PTQ will serve as a valuable quantization tool, pushing the development of practical deployment of 3D detection models on edge devices."
      }
    },
    {
      "title": "Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis",
      "abstract": "In this paper, we propose binary sparse convolutional networks called BSC-Net\nfor efficient point cloud analysis. We empirically observe that sparse\nconvolution operation causes larger quantization errors than standard\nconvolution. However, conventional network quantization methods directly\nbinarize the weights and activations in sparse convolution, resulting in\nperformance drop due to the significant quantization loss. On the contrary, we\nsearch the optimal subset of convolution operation that activates the sparse\nconvolution at various locations for quantization error alleviation, and the\nperformance gap between real-valued and binary sparse convolutional networks is\nclosed without complexity overhead. Specifically, we first present the shifted\nsparse convolution that fuses the information in the receptive field for the\nactive sites that match the pre-defined positions. Then we employ the\ndifferentiable search strategies to discover the optimal opsitions for active\nsite matching in the shifted sparse convolution, and the quantization errors\nare significantly alleviated for efficient point cloud analysis. For fair\nevaluation of the proposed method, we empirically select the recently advances\nthat are beneficial for sparse convolution network binarization to construct a\nstrong baseline. The experimental results on Scan-Net and NYU Depth v2 show\nthat our BSC-Net achieves significant improvement upon our srtong baseline and\noutperforms the state-of-the-art network binarization methods by a remarkable\nmargin without additional computation overhead for binarizing sparse\nconvolutional networks.",
      "full_text": "Binarizing Sparse Convolutional Networks for Efﬁcient Point Cloud Analysis Xiuwei Xu1,2, Ziwei Wang 1,2, Jie Zhou 1,2, Jiwen Lu 1,2* 1Department of Automation, Tsinghua University, China 2Beijing National Research Center for Information Science and Technology, China {xxw21, wang-zw18}@mails.tsinghua.edu.cn; {jzhou, lujiwen}@tsinghua.edu.cn Abstract In this paper, we propose binary sparse convolutional networks called BSC-Net for efﬁcient point cloud analysis. We empirically observe that sparse convolution operation causes larger quantization errors than standard convolu- tion. However, conventional network quantization methods directly binarize the weights and activations in sparse con- volution, resulting in performance drop due to the signif- icant quantization loss. On the contrary, we search the optimal subset of convolution operation that activates the sparse convolution at various locations for quantization error alleviation, and the performance gap between real- valued and binary sparse convolutional networks is closed without complexity overhead. Speciﬁcally, we ﬁrst present the shifted sparse convolution that fuses the information in the receptive ﬁeld for the active sites that match the pre- deﬁned positions. Then we employ the differentiable search strategies to discover the optimal opsitions for active site matching in the shifted sparse convolution, and the quanti- zation errors are signiﬁcantly alleviated for efﬁcient point cloud analysis. For fair evaluation of the proposed method, we empirically select the recently advances that are bene- ﬁcial for sparse convolution network binarization to con- struct a strong baseline. The experimental results on Scan- Net and NYU Depth v2 show that our BSC-Net achieves sig- niﬁcant improvement upon our srtong baseline and outper- forms the state-of-the-art network binarization methods by a remarkable margin without additional computation over- head for binarizing sparse convolutional networks. 1. Introduction 3D deep learning on point clouds [6, 12, 25, 27] has been widely adopted in a wide variety of downstream applica- tions including autonomous driving, AR/VR and robotics due to its strong discriminative power and generalization ability. In these applications, real-time interaction and fast *Corresponding author. ... (a) (b) Sparseconv ... ... Shifted Sparseconv ... Channels Channels Figure 1. Demonstration of sparse convolution and the proposed shifted sparse convolution. (a) Sparse convolution only operates when the center of kernel slides over the active sites. (b) Our shifted sparse convolution performs different operations for each group of output channels, which brings more information from the neighbor active sites. response are required to guarantee safety and practicality. Submanifold sparse convolution (we call it ”sparse con- volution” for short in the rest of this paper) [12] is one of the most popular and basic operator for point cloud analy- sis, which ﬁrst voxelizes the point clouds and then applies 3D convolution on the voxels while keeping the same spar- sity pattern throughout the layers of the network. Sparse convolution is widely adopted in most state-of-the-art ar- chitectures for point cloud analysis and so it is desirable to further improve its efﬁciency for more practical applica- tion. We opt for architecture-agnostic methods such as em- ploying network binarization to achieve this goal. Binarized neural networks [19,36] restrict the bitwidth of weights and activations to only one bit and substitute the multiplication- addition by xnor-bitcount operations, which decreases the 1 arXiv:2303.15493v1  [cs.CV]  27 Mar 2023storage and computational cost by 32×and 64×respec- tively. We empirically ﬁnd sparse convolution operation brings larger quantization errors compared to standard con- volution, which leads to signiﬁcant performance degrada- tion when directly applying existing network binarization methods due to the large quantization errors. In this paper, we present BSC-Net to learn binary sparse convolutional networks for efﬁcient point cloud analysis in resource-exhaustive scenarios. Instead of directly binariz- ing the weights and activations in sparse convolutional net- works, we search the optimal subset of convolution oper- ation that activates the sparse convolution at various loca- tions for binarization. The acquired convolution patterns signiﬁcantly reduces the quantization errors in deployment, and achieves remarkable performance enhancement with- out extra computational cost. More speciﬁcally, we propose the shifted sparse convolutional networks whose convolu- tion operations are activated for active sites consistent with the pre-deﬁed locations, and the optimal positions for ac- tive site matching across various channels are obtained via differentiable search strategies. Therefore, the quantization errors in the ﬁxed convoltion operations are signiﬁcantly al- leviated by leveraging the shifted sparse convolution with the searched active site matching locations. Moreover, we empirically select the recently advances that are beneﬁcial for sparse convolution network binarization to construct a strong baseline. Extensive experimental results on Scan- Net and NYU Depth v2 for semantic segmentation of point clouds show that our BSC-Net reduces the operations per second (OPs) by 92.4% with only 3% mIOU degradation. 2. Related Work Network quantization: Network quantiztaion has been widely studied in computer vision due to the signiﬁcant enhancement in storage and computation efﬁciency. Con- ventional methods can be divided into two categories in- cluding networks in one bit and multiple bits. For the ﬁrst regard, weights and activations in networks are binarized with extremely high compression ratio. Hubara et al. [15] and Courbariaux et al. [8] substituted the add-multiplication (MAC) of real-valued models by xnor-bitcount operations, and emplyed the straight-through estimators (STE) to up- date the network parameters with back-propagation. Raste- gari et al. [29] further presented scaling factors for weights and activations quantization error minimization. To recover the capacity degradation caused by aggressive quantization, Liu et al. [20] added extra shortcut connections between consecutive layers to diversify the feature maps. Bulat et al. [1] modiﬁed the search space and strategy with stabil- ity regularization for the optimal architecture acquisition of binary networks. Qin et al. [28] maximized the informa- tion entropy in binary features and leveraged learnable scal- ing factors for information retention in point cloud analy- sis. Since increasing the weight and activation bitwidths can signiﬁcantly enhance the model capability, networks in multiple bits are proposed for better performance. Choi et al. [5] optimized the activation clipping threshold to ﬁnd the right quantization scale. Zhang et al. [38] further searches the optimal quantizer basis and encoding for accurate quan- tization. Lee et al. [16] adaptively scaled the gradient ele- ment in STE to calibrate the direction of parameter update with minimal discretization errors. However, multi-bit net- works still suffers from heavy computational and storage cost. Directly applying existing network binarization meth- ods to submanifold sparse convolution destructs the geo- metric structure in the scene and degrades the feature infor- mativeness signiﬁcantly. Sparse convolution networks: Increased attention has been paid to 3D deep learning on point cloud in recent years, which is important for autonomous driving, AR/VR and robotics. Due to the unordered property of point cloud data, voxelizing the points and applying convolution on 3D grids is a natural solution [4, 26, 40]. However, as point cloud only covers the surfaces of objects/scenes and the most space in 3D scans is empty, the dense volumetric rep- resentation is inherently inefﬁcient. Moreover, the compu- tational cost and memory requirement both increase cubi- cally with voxel resolution, thus making it infeasible to train a voxel-based model with high-resolution inputs. To handle this problem, sparse convolution [10, 11] were proposed to restricts computation and storage to “active” sites (i.e. vox- els which are not empty). However, as convolutional opera- tor will increase the number of active sites with each layer, the feature sparsity is reduced accordingly. To further im- prove the efﬁciency of sparse CNNs, Graham et al. [12] in- troduced submanifold sparse convolution, which only con- ducts convolution when the center of kernel slides over ac- tive sites and keeps the same level of sparsity throughout the network. This made it practical to train networks with more convolution layers, such as UNet [30], FCN [21] and ResNets [14]. Choy et al. [6] proposed Minkowski Engine, which extended submanifold sparse convolution to higher dimensions. Tang et al. [33] further combined point-based model and sparse CNN to achieve both accurate and efﬁ- cient 3D perception for large scale scenes. In particular, sparse convolutional networks are able to adopt common deep architectures from 2D vision, which can help stan- dardize deep learning for point cloud, and they are widely utilized in state-of-the-art models for various tasks, such as semantic segmentation [24], object detection [31, 35] and instance segmentation [17, 32, 34]. Differentiable search: In order to reduce the search com- plexity during the exploration process, differentiable search has been widely used in network architecture search [18], mixed-precision quantization [3, 37] and continual learn- ing [39]. During differentiable search, the superstruc- 2ture containing all choices as different components is con- structed, where the importance weights for each branch is optimized with gradient descent for optimal solution acqui- sition. Liu et al. [18] relaxed the space of network archi- tectures to be continuous, and jointly optimized the branch importance weights and parameters of the hypernet for net- work architecture search. Cai et al. [3] assigned different bitwidths to various branches in the supernet for mixed- precision quantization, and chose the bitwidth in the com- ponent with the largest importance weight to be the quan- tization strategy during inference to achieve the optimal accuracy-complexity trade-off. Guan et al. [13] updated the feature weights through the presented bridge loss which en- hanced the knowledge distillation between the students and teachers. In this paper, we extend differentiable search for the discovery of optimal position for active site matching in shifted sparse convolution, where the search cost is signiﬁ- cantly reduced for exploration in large space. 3. Approach In this section, we ﬁrst brieﬂy introduce the prelimi- nary concept of sparse convolution and network binariza- tion. Then we conduct experiments to show the quantiza- tion errors of network binarization methods in different con- volution patterns, and introduce the shifted sparse convolu- tion (SFSC) operation which is activated for sites in various locations of the receptive ﬁeld. Finally, we demonstrate the differentiable search to discover the optimal position for ac- tive site matching in SFSC, and construct the BSC-Net with alleviated quantization errors and enhanced performance. 3.1. Preliminaries Let xu be an input feature vector of an active site, located at 3-dimensional coordinates u ∈RD. As shown in Figure 1(a), the general sparse convolution [6, 12] F0 by a kernel for xu is formulated as: F0(W,xu) = ∑ i∈ND(u) Wixu+i (1) where ND(u) denotes the list of offsets in the 3- dimensional cube centered at origin u. The convolution kernel can be break down and assigned to each offset pa- rameterized by Wi. Sparse convolution is a practical substitution for vanilla 3D convolution, and skips the non-active regions that only operates when the center of convolutional kernel covers ac- tive voxels. Speciﬁcally, active voxels are stored as sparse tensors for the ﬁxed convolution operations, where all ac- tive synapes between input and output voxels are found to perform convolution. Therefore, the memory requirement and computational cost are signiﬁcantly reduced in sparse convolutional networks. To further reduce the complexity during inference, network binarization can be leveraged for 50 52 54 56 58 60 62 64Sign correspondence (%) conv spconv shift-spconv Figure 2. Sign correspondence of activations for the ﬁrst binary layer when binarizing convolutional network, sparse convolutional network and shifted sparse convolutional network for point cloud segmentation on ScanNet dataset. All networks share the same kernel weights. We sort x-axis (different patterns of sparse convo- lution) by their sign correspondence for better visualization. weight and activation quantization. In a 1-bit sparse convo- lutional layer, both convolutional kernels and activations are binarized to −1 and +1. In this way, the time-consuming ﬂoating-point matrix multiplication can be replaced by bit- wise XNOR and popcount operations: Al b = sign(popcount(XNOR(Wl b,Al−1 b ))) (2) where Al b and Wl b represent the binarized activations and weights in the lth layer respectively, and Wl b is deﬁned as the binarzed version of the real-valued latent weights Wl r via Wl b = sign(Wl r). 3.2. Shifted Sparse Convolution Since the ﬁxed operation in sparse convolution is only activated when the central input in the receptive ﬁeld is ac- tive, the constrained exploration of the neighbor active sites makes sparse convolutional networks less robust to bina- rization. To show this, we calculate the sign correspon- dence (the proportion of activations in binary network that own same signs with the corresponding real-valued activa- tions, which can measure the quantization error as proved in [29]) for convolutional network and sparse convolutional network with inputs from the ScanNet dataset. We choose the activations of the ﬁrst binary layer to avoid the accu- mulation of quantization errors and adopt the same kernel weights for both networks. As shown in Figure 2, the sign correspondences for convolutional layer and sparse convo- lutional layer are 63.1% and 58.4% respectively, which con- ﬁrms that sparse convolution will bring larger quantization errors than standard convolution. However, it is infeasible to adopt convolutional layers in point cloud analysis networks for reducing quantization er- rors due to the large computational cost from growing active sites. As an alternative, we try to explore the subset of con- volution. For a single active site, a 3 ×3 ×3 convolution kernel will operate 27 times while sparse convolution kernel only operates at the center. What if we keep the same num- ber of operations with sparse convolution but operates at 3other location? To answer it, we extend sparse convolution to enable it to active at different locations. Here we propose the shifted sparse convolution(SFSC) shown in Figure 1(b), which is deﬁned as: Fk(W,xu) = ∑ i∈ND(u+sk) Wixu+i (3) sk ∈R3, k∈{1,2,...,n s} where u + sk is the center of shifted cube instead of u. ND(u + sk) is then comprised of the offsets in the shifted cube w.r.t. u. ns is the number of all unique shifts. For example, for a 3 ×3 ×3 sparse convolution operation, there are up to 33 −1 = 26possible shifts. For a general sparse convolution operation, it conducts convolution only when the kernel center overlaps with ac- tive sites. While in our SFSC operation, the kernel cen- ter can shift to any other locations of the kernel. We use Fns = {F0,F1,F2,...,F ns}to represent the set of all SFSC operations. Note that we consider the general sparse convo- lution as a special case of SFSC ( F0). In a SFSC layer, instead of applying the same sparse convolution operation for all output channels as in a general sparse convolutional layer, we uniformly divide the output channels into several groups (namely channel group), each with a speciﬁc SFSC operation. It can be formulated as: y= concat(f1(W1,x),...,f ng (Wng ,x)), fi ∈Fns (4) where x and y are the input and output of this layer. ng indicates the number of channel groups. Wi refers to the weights for the i-th SFSC operation. The outputs of all SFSC operations are concatenated along the channel dimen- sion, resulting in a tensor with the same shape as the output of a general sparse convolutional layer. We randomly sample 50 shift conﬁgurations for SFSC layers and compute the sign correspondence, which is shown in Figure 2. It can be seen that different SFSC layers vary a lot in quantization errors and a proportion of them are more robust to binarization compared to sparse convo- lutional layer. In another word, if we can ﬁnd out the (near) optimal conﬁgurations for all SFSC layers in a network, the quantization error can be reduced without additional com- putational cost. 3.3. Efﬁcient Search for Shift Operation Due to the huge design space of shift operation, it is in- feasible to decide an optimal conﬁguration for the whole network: the shifted channels and shift directions may be different in each layer, and the total number of possible architectures will be (84)13 = 9.1 ×1046 for a network with 13 SFSC layers, each layer with 4 channel groups and 8 available shift directions. Although manually designed BSC-Net, which shares the same shift strategy in all SFSC layers, is able to reduce the impact of binarization on the + ...  ...  Composite Sparseconv + ..  ..  ..  ..  Optimal direction: Optimal direction: .. ...  Channel GroupsEfﬁcient Search for Optimal Shifts Composite Sparseconv Figure 3. Demonstration of our efﬁcient search method for shift operation. For each SFSC layer and each channel group, we com- bine all the shift operations in the search space into a 5 ×5 ×5 sparse convolution and assign each direction with a soft selector indicating the importance of the corresponding shift operation, which enables us to directly search the best shift operations via end-to-end gradient descent. ⊕stand for summation. network performance, we resort to automatic architecture search for a better performance. In this section, without fur- ther explanation, the default kernel size for original sparse convolution and SFSC is 3 ×3 ×3. In our BSC-Net, the optimal shift direction for each channel group and each layer may differ. Thus the prob- lem is to search the optimal shift direction for each channel group in the SFSC layer. We formulate this by searching the optimal fi in (4.4): fi = ns∑ j=1 oa ijFj, i∈{1,2,...,n g} (5) s.t. ∑ j oa ij = 1, oa ∈{0,1}. where oa is a binary selector of the shift direction. As searching in a discrete space makes it hard to optimize the choices, we reformulate the discrete search space as a con- tinuous one by switching fi to a composite function f∗ i : f∗ i = ns∑ j=1 πa ijFj, i∈{1,2,...,n g} (6) s.t.πa ∈[0,1], πa ij = 1 1 + exp(−αij) where the constraints on weight πa are eliminated by intro- ducing a set of real architecture parameters{αij}. This sig- moid relaxation [7] will not introduce competition among different SFSC operations as in softmax relaxation [3], which we ﬁnd to be a better way to search for BSC-Net. In this way, the composition of SFSC operations are learned by gradient descent in the space of continuous real parameters {αij}, which can be optimized end-to-end efﬁciently. 4However, according to (6), the computation and memory increase linearly with the size of search space. All available SFSC operations need to be conducted in weighted summa- tion f∗ i = ∑ns j=1 πa ijFj. Moreover, each SFSC layer owns different parameters, increasing the difﬁculty of network optimization. To this end, we propose an efﬁcient search method, which absorb all the operations in search space into a larger sparse convolution, as shown in Figure 3. In this way, we convert the SFSC layer into a 5 ×5 ×5 composite sparse convolutional layer, which is used to con- struct a supernet. This enables us to efﬁciently search the optimal architecture parameters by end-to-end optimiza- tion, regardless of the search space. However, it should be clariﬁed that although the size of search space will not affect the computational efﬁciency of the supernet, a large search space will make the optimization of architecture parameters hard to converge, thus deteriorate the ﬁnal performance. Once the supernet is converged, the optimal BSC-Net must be derived by discretizing the soft selector variables πa of (6) into the binary selectors oa required by (5). In or- der to make sure the performance of supernet can precisely reﬂect the capability of BSC-Net, we constrain πa in each SFSC layer by a conﬁdence loss: Lc = − 1 ng ·ns ng∑ i ns∑ j |πij −0.5| (7) which pushes πa to discrete values. Optimization approach: In order to decouple the weights and architecture parameters for robust learning [3], we adopt an alternating optimization approach: 1) ﬁx the {αij}and optimize {Wi}; 2) ﬁx {Wi}and update {αij}. When we derive the BSC-Net from a converged super- net, both weights and architecture parameters need to be considered. Here we ﬁnd the following strategy works best: we ﬁrst train the supernet with binary weight and activa- tion to search for the optimal architecture parameters, from which we choose the shift directions with the highest archi- tecture parameters. Then we initialize the searched BSC- Net with the weights from the supernet and follow the same training procedure as our baseline (introduced in Section 4). 4. Experiment To investigate the performance of the proposed method, we conduct experiments on several indoor scene datasets including NYU Depth v2 (NYUDv2) [23] and ScanNet [9]. We ﬁrst introduce the datasets, evaluation metrics and im- plementation details, which is followed by a strong baseline designed for binarization of sparse convolution networks. Then we compare our BSC-Net with the state-of-the-art network binarization methods on sparse convolutional net- works. Finally we design ablation studies to show the ef- fectiveness and efﬁciency of the presented BSC-Net. 4.1. Experimental Settings Datasets and metrics: We conduct experiments on two indoor datasets including NYU Depth v2 (NYUDv2) [23] and ScanNet [9]. NYUDv2 contains 1,449 RGB-D scene images, where 795 images are split for training and 654 images for testing. Following [12], we adopt 40-class set- ting where all pixels are labeled with 40 classes and convert the RGB-D images into 3D point clouds. As the horizontal and vertical directions of spatial dimensions in the RGB- D images are discrete, we voxelize the 3D point clouds to 1cm bins by only discretizing the depth dimension. ScanNet consists of 1513 reconstructed indoor scenes with 21 cate- gories, which are split into 1201 and 312 scenes for training and validation respectively. We adopt two popular settings of ScanNet containing 2cm voxelization and 5cm voxeliza- tion as done in [6]. We report the mean intersection of union (mIoU), mean per-point classiﬁcation accuracy (mAcc) and overall point- wise classiﬁcation accuracy (Acc) for NYUDv2. For Scan- Net, we report mIoU and mAcc. We use the same calcula- tion method in [19] to count the binary operations (BOPs) and ﬂoating point operations (FLOPs), where the total op- erations for model computation complexity evaluation is counted by OPs = BOPs/64 + FLOPs. The storage cost are measured by summing the number of real-valued parame- ters and that of binary numbers divided by 32. Implementation details: Following [12], we adopt dif- ferent network architectures for the various datasets. For NYUDv2, we perform experiments with FCN [21] net- works in different sizes, namely FCN-S (small) and FCN-H (huge). For ScanNet, we leverage the U-Net [30] architec- ture in small and huge sizes represented as UNET-S and UNET-H. The small and huge models differ in numbers of ﬁlters and sparse convolutional layers per level, which re- sults in capacity variations of point cloud analysis. We use Adam with a stepwise scheduler to optimize the network pa- rameters. The training hyperparamters are introduced in the supplementary materials in detail. We perform data aug- mentation by applying random afﬁne transformers to the point cloud. For our BSC-Net, the shift distance in SFSC operations is set to one and the number of channel groups which em- ploy different shift directions is assigned to 8. The search space of directions contains shifting to 8 operations repre- sented by (±1,±1,±1) and staying still without shift. Lim- iting the search space of shift directions for channel groups in each layer signiﬁcantly reduces the search difﬁculty while maintains the exploration capability. We also eval- uated two variations of our BSC-Net called BSC-Baseline and BSC-Manual to demonstrate the effectiveness of the presented techniques. BSC-Baseline represents the frame- work that binarizing the sparse convolutional networks with all beneﬁcial recently advances combined (refer to Section 5Table 1. The mIoU of binarzed sparse convolutional networks on ScanNet of different baseline techniques, where the UNET architectures are leveraged. The methods from left to right indicate (1) removing all the skip connections; (2) replacing PReLU with Hardtanh; (3) calculating scaling factor for both activations and weights; (4) using STE to approximate the gradient; (5) removing the skip connections for downsampling/upsampling layers; (6) directly training network with binary weights and activations. Method Simplify BS Simplify AF Modify SF Simplify GA Simplify DS/US Simplify Init. Full baseline mIoU (%) 37.4 50.5 46.1 49.9 47.3/48.7 34.1 51.7 4.2), and BSC-Manual stands for the network binarization for network consisted of SFSC layers with manually deﬁned shift conﬁgurations instead of the searched ones. In BSC- Manual, We set 1 2 of the channel groups unshifted, 1 4 shift to (+1,+1,0) and 1 4 shift to (−1,−1,0) for NYUDv2, and 1 2 of the channel groups unshifted, 1 4 shift to (+1,+1,+1) and 1 4 shift to (−1,−1,−1) for ScanNet. BSC-Baseline and BSC-Manual are trained in the same way, while BSC-Net is trained with an additional searching stage ﬁrst. 4.2. Strong Baseline Since network binarization degrades the performance sizably, techniques for accuracy improvements have been studied in recent works of model quantization. To show the performance improvement comes from our proposed method rather than other tricks, we build a strong baseline for binarizing sparse convolutional networks from the re- cently advances. Through the empirical study shown in Ta- ble 1, we discover the beneﬁcial techniques for performance enhancement and list as below. Block sturcture: We use the same block structure as ReActNet [19], where the operations are ordered as Binarization→SparseConv→BatchNorm→Activation in each basic block. Activation function: PReLU [14, 19] considers the nega- tive inputs with better convergence, and we substitute all ReLU activation layers with PReLU to strengthen the per- formance. Scaling factor: We only calculate the layer-wise scaling factor for weights as demonstrated in [22], which is the mean absolute value offull-precision weights. Gradient approximation: A piecewise polynomial func- tion [20] is used to approximate the sign function, which acquires more accurate gradient during back propagation. Downsampling/upsampling: Following [20], the skip con- nection for downsampling layer is composed of an average pooling land a real-valued convolutional layer with kernel size 1. We also verify that an unpooling layer with a full- precision convolution with kernel size 1 is beneﬁcial in the skip connection for upsampling layer. Initialization: We ﬁrst pretrain the network with full- precision weights and activations for initialization. Then the model with binary weights and activations is trained for binarization. 4.3. Comparison with State-of-the-art In this section, we compare our method with state-of- the-art binarization methods, including XNOR-Net [29], XNOR-Net++ [2], BiPointNet [28], Bi-Real-Net [20] and ReActNet [19]. We also provide the performance of the real valued models for reference. Experiments are conducted on NYUDv2 and ScanNet. Results on NYUDv2: Table 3 illustrates the compari- son of storage, operations per second (OPs) and semantic segmentation results across several popular network bina- rization methods and our BSC-Net. Bi-Real-Net performs best among previous methods, which shows the gradient ap- proximation method and the skip connection structure are general and effective in sparse convolutional network bi- narization. Although BiPointNet is designed for 3D point cloud analysis, it fails to achieve satisfactory performance because the operations such as maxpooling and point-wise MLP used in PointNet are not adopted in sparse convolu- tional networks. BSC-Baseline outperforms the previous methods by a large margin and its performance is further boosted by the proposed SFSC module, i.e. BSC-Manual. When adopting the efﬁcient differentiable search method, BSC-Net achieves the state-of-the-art performance in both architectures of FCN, while the extra computataional over- head is negligible compared to previous methods.Observing the last low in Table 3, The performance gap between real valued FCN-H and our BSC-Net has even been narrowed to less than 3%, which shows the great application potentials of our method. Results on ScanNet: Different from NYUDv2 in which the point clouds are generated from single RGB-D images, ScanNet provides larger and more complete point cloud scenes via 3D reconstruction. Therefore, we can evaluate our BSC-Net on ScanNet with different resolutions of the input point cloud after voxelization as shown in Table 2. Following [12], we evaluate all results three times to address the problem of the number of voxels being greatly smaller than that of points. Similar to NYUDv2, BSC-Baseline out- performs the previous state-of-the-art. We found the gap between BSC-Baseline and previous methods were larger because the upsampling layer in UNET is implemented by deconvolution, which is more sensitive to binarization than the interpolation used in FCN. In each setting, BSC- Manual gains consistent improvement over BSC-Baseline, and BSC-Net further achieves state-of-the-art performances 6Table 2. Semantic segmentation results (%), model storage cost (M) and computation cost in OPs of different methods on ScanNet validation set. Param. means the model storage cost (M). 5cm voxel and 2cm voxel refer to different resolutions of the input point cloud after voxelization. Method Param. 5cm voxel 2cm voxel OPs mIoU mAcc OPs mIoU mAcc UNET-S Real valued 4.335 1.21 ×109 65.2 73.3 5.32 ×109 68.7 78.5 XNOR-Net 0.136 8.07 ×107 33.3 38.9 3.79 ×108 21.0 26.1 XNOR-Net++ 0.136 8.07 ×107 12.6 15.9 3.79 ×108 11.2 13.7 BiPointNet 0.136 8.07 ×107 30.1 36.2 3.79 ×108 18.4 20.7 Bi-Real-Net 0.138 8.12 ×107 48.3 56.6 3.82 ×108 51.2 63.3 ReActNet 0.138 8.12 ×107 43.6 50.2 3.82 ×108 46.9 52.9 BSC-Baseline 0.139 8.12 ×107 51.7 61.8 3.82 ×108 54.9 65.3 BSC-Manual 0.139 8.12 ×107 53.2 63.7 3.82 ×108 57.8 66.6 BSC-Net 0.139 8.12 ×107 54.4 65.2 3.82 ×108 61.4 70.4 UNET-H Real valued 30.104 7.65 ×109 67.6 75.1 3.38 ×1010 71.0 79.0 XNOR-Net 0.939 3.61 ×108 46.6 53.5 1.75 ×109 34.9 40.1 XNOR-Net++ 0.939 3.61 ×108 13.3 16.4 1.75 ×109 12.8 16.2 BiPointNet 0.939 3.61 ×108 45.2 52.4 1.75 ×109 34.3 39.8 Bi-Real-Net 0.948 3.63 ×108 53.4 63.2 1.76 ×109 57.3 66.9 ReActNet 0.949 3.63 ×108 52.2 59.0 1.76 ×109 57.1 67.0 BSC-Baseline 0.952 3.63 ×108 56.0 65.9 1.76 ×109 59.3 68.3 BSC-Manual 0.952 3.63 ×108 59.3 68.7 1.76 ×109 62.2 70.1 BSC-Net 0.952 3.63 ×108 62.2 70.5 1.76 ×109 63.9 71.6 Table 3. Semantic segmentation results (%), model storage cost (M) and computation cost in OPs of different methods on NYUDv2 test set. Param. means the model storage cost (M). Method Param. OPs mIoU mAcc Acc FCN-S Real valued 2.496 1.24 ×109 33.9 47.7 64.7 XNOR-Net 0.108 1.72 ×108 22.1 32.7 57.3 XNOR-Net++ 0.108 1.72 ×108 8.5 13.5 43.9 BiPointNet 0.108 1.72 ×108 24.9 35.7 59.3 Bi-Real-Net 0.110 1.75 ×108 27.3 38.4 60.0 ReActNet 0.110 1.75 ×108 25.4 36.6 58.9 BSC-Baseline 0.110 1.75 ×108 27.8 39.9 60.1 BSC-Manual 0.110 1.75 ×108 28.7 40.9 60.2 BSC-Net 0.110 1.75 ×108 29.7 42.1 61.2 FCN-H Real valued 10.025 4.82 ×109 36.9 50.4 67.2 XNOR-Net 0.357 3.08 ×108 27.1 38.8 59.6 XNOR-Net++ 0.357 3.08 ×108 8.4 13.6 43.2 BiPointNet 0.357 3.08 ×108 28.1 40.8 60.1 Bi-Real-Net 0.361 3.09 ×108 30.4 41.8 61.1 ReActNet 0.361 3.09 ×108 27.0 40.1 58.9 BSC-Baseline 0.362 3.09 ×108 32.0 44.4 63.3 BSC-Manual 0.362 3.09 ×108 32.5 45.1 63.5 BSC-Net 0.362 3.09 ×108 33.9 46.2 64.5 which proves the effectiveness of differentiable search strat- egy. We also noticed that the improvement of BSC-Net over BSC-Baseline on ScanNet is larger than that on NYUDv2, that shows our method can exploit richer geometric infor- mation from 3D point clouds than 2.5D depth map. 4.4. Ablation Study We conduct ablation studies to show how different hy- perparamters and strategies inﬂuence the performance of the proposed BSC-Net. We study the effects of the search space and number of channel group as well as searching strategy in our differentiable search method on the ﬁnal per- formance. The experiments are conducted on ScanNet (5cm voxelization) using UNET-S. Performance w.r.t. searching hyperparamters: In or- der to reduce the search cost as well as the optimization dif- ﬁculties, we search the optimal shift strategies for different layers in BSC-Net from a subset of the whole search space, and we also partition the channels into groups which share the same shift strategies. Table 4 demonstrates the perfor- mance variation for BSC-Net with different search space and group numbers of SFSC operations, where S and Cd k represent the convolutions staying still and those shifted to the direction of the k-th vertex of a cube. Observing the third, fourth and seventh rows, we con- clude that the mIoU and mAcc of BSC-Net improves as the size of the search space increases. The improvement from search space in size 3 to that in size 5 is much higher than that from size 5 to size 9, which indicates that large search space causes optimization difﬁculties in differen- tiable search method and a subset of the whole search space contains the solution near to the optimal one. According to the last four rows in Table 4, the performance achieves the optimal for medium numbers of groups in the differentiable 7Table 4. The effects of search space and group number in differ- entiable search method on the ﬁnal performance. Search space Group number mIoU(%) mAcc(%) Baseline: {S} – 51.7 61.8 {S, C1, C8} 8 53.6 63.9 {S, C1, C4, C6, C7} 8 54.2 64.9 {S, C1, C2, C3, 2 52.9 63.8 4 53.4 64.6 C4, C5, C6, C7, C8} 8 54.4 65.2 16 54.0 64.5 Table 5. The effects of relaxation and derivation strategy in differ- entiable search method on the ﬁnal performance. ∗ indicates the architecture parameters are frozen. Relaxation Derivation mIoU(%) mAcc(%) Random D(32,32) →D(1,1) 51.5 61.2 Softmax S(32,32) →S(1,1) →D(1,1) 51.8 62.0 S∗(32,32) →S(1,1) →D(1,1) 52.3 63.2 S(1,1) →D(32,32) →D(1,1) 53.5 64.5 Sigmoid S(32,32) →S(1,1) →D(1,1) 52.6 64.0 S∗(32,32) →S(1,1) →D(1,1) 53.7 64.7 S(1,1) →D(32,32) →D(1,1) 54.4 65.2 search. Increasing the numbers causes the optimization dif- ﬁculties due to the large search space, and decreasing the numbers excludes the promising solution due to the chan- nel correlation. Performance w.r.t. searching strategy: We further in- vestigate the effects of searching strategy in Table 5, includ- ing relaxation method for the binary selector in (5) and strat- egy for deriving BSC-Net from the supernet. For softmax relaxation, {πα ij}are deﬁned as πa ij = exp(αij)∑ k exp(αik) , and the conﬁdence constraint in (7) is changed to: Lc = − ng∑ i ns∑ j Iijlogπa ij, Iij =    1, j = argmax k πa ik 0, otherwise (8) which pushes πa to a one-hot tensor for better derivation. We use S(W,A) and D(W,A) to represent supernet and derived BSC-Net with W-bit weights and A-bit activations. We ﬁrst conduct random assignment on the shift direc- tions for each layer and channel group. As shown in the ﬁrst row, the performance of BSC-Net falls even behind of the baseline, which shows searching or manually designing a proper SFSC conﬁguration is essential for realizing the potential of BSC-Net. The last six rows show that sigmoid relaxation is better than softmax, which is due to softmax will bring competition between different SFSC operations and hurts the performance of supernet. The results also tes- tify the superiority of our derivation strategy. Notably, we should emphasize that the SFSC operation is proposed for reducing quantization error, which does not Ground-truth BiPointNet  Bi-Real-Net BSC-Net  Figure 4. Visualization results of different methods. work for real-valued networks. We do not observe an obvi- ous change in performance when equipping the real-valued UNET or FCN with SFSC. 4.5. Visualization Results We viusalize the segmentation prediction for different methods in Figure 4. Black regions in the ground-truth refer to undeﬁned categories. The predictions of previous meth- ods are discontinuous and they misclassify the shelf as wall or window, while our BSC-Net outputs smooth and accu- rate predictions. The results in red boxes also provide an intuitive explanation on our method: when binarized, sparse convolutional networks fail to fully explore the neighbor ac- tive sites with increased quantization errors and thus predict discontinuous results. On the contrary, BSC-Net better ex- ploits the neighborhood and reduces the quantization error. 5. Conclusion In this paper, we have presented BSC-Net that learns bi- nary sparse convolutional networks for efﬁcient point cloud analysis. We present the shifted sparse convolution that is activated for receptive ﬁeld whose pre-deﬁned locations match active sites. By searching the optimal positions for active site matching in shifted sparse convolution, the quantization errors in binarized sparse convolutional net- works are alleviated signiﬁcantly without additional com- putational cost. For fair evaluation, we combine previous techniques to construct a strong baseline. Extensive exper- iments on semantic segmentation of point clouds demon- strate the superiority of BSC-Net. Acknowledgements This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 62125603, and in part by a grant from the Beijing Academy of Artiﬁcial Intelligence (BAAI). 8+ + (a) (b) Input SSC SSC block Conv Deconv Unpooling Linear Softmax (c) Output Features Figure 5. The overall frameworks of FCN (a) and UNET (b) which are constructed with the basic blocks in (c). Supplementary Material A. Overview In this supplementary material, we detail the network ar- chitectures and training hyperparamters used in our exper- iments. Section B shows how to construct the whole net- work with basic blocks. Section C details the training hy- perparamters used in our experiments. B. Network Architecture B.1. Overall Framework We illustrate the architectures of FCN and UNET in Fig- ure 5. Only two levels of downsampling/upsampling are shown in the ﬁgure for simplicity. Note that for each level only one SSC block is drawn, while in fact there may be one or two blocks according to the size of networks. FCN-S has 16 ﬁlters in the input layer, and one SSC block per level. FCN-H has 24 ﬁlters in the input layer, and two SSC blocks per level. Both networks use eight levels of downsampling and upsampling. We increase the number of ﬁlters in the networks when downsampling: in particu- lar, we add 16 (S) or 24 (H) ﬁlters every time we reduce the scale. UNET-S has 16 initial ﬁlters and one SSC block per level. UNET-H has 32 initial ﬁlters and two SSC blocks per level. Both networks use six levels of downsampling and upsampling. Each downsampling operation adds 16 (S) or 32 (H) ﬁlters, while upsampling operation subtracts the same numbers of ﬁlters correspondingly. When binarizing the network, the ﬁrst SSC layer and the linear layers are kept real-valued following previous meth- ods [19, 20, 22]. We adopt our SFSC module in the SSC block for BSC-Manual and BSC-Net, which is detailed in next subsection. B.2. Block Detail We detail the structure of basic blocks contained in the binary FCN and UNET in Figure 6. For SSC layer, the projection indicates identity mapping when the input and output channel are equal, otherwise it is a 1-bit 1 ×1 con- volution operation. For Conv and DeConv blocks, we keep the 1×1 convolution operation real-valued, which is proved to be essential for binarizing performance [20, 22]. C. Training Hyperparamters There are 2 training stages when training BSC-Baseline and BSC-Manual while 3 stages when training BSC-Net. For all stages, we set max epoch as 128, weight decay as 0 and adopt Adam optimizer with a stepwise scheduler which steps at 60 and 100 epoch (reduce the learning rate by a fac- tor of 10). The initial learning rates for the ﬁrst and second stage when training BSC-Baseline and BSC-Manual are set to 0.001 and 0.0002. While for training BSC-Net, the initial learning rates for the three stages are set to 0.001, 0.001 and 0.0002. The weight of the conﬁdence loss is set to 0.1 for FCN and 0.01 for UNET. Note that for training UNET-H on 2cm voxel, we double the max epoch and the stepping epochs as the huge network is hard to converge. 9Sign 3x3 SSC BatchNorm PReLU Sign 1-bit 3x3 SSC BatchNorm PReLU Projection Sign 1-bit 3x3 Conv, s=2 BatchNorm PReLU 2x2 Pooling 1x1 Conv Sign 1-bit 3x3 DeConv, s=2 BatchNorm PReLU 2x2 UnPooling 1x1 Conv (a) (b) (c) (d) Figure 6. Details of the basic blocks. (a) SSC layer, (b) SSC block, (c) Conv layer, (d) DeConv layer. References [1] Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Bats: Binary architecture search. In ECCV, pages 309–325, 2020. 2 [2] Adrian Bulat and Georgios Tzimiropoulos. Xnor- net++: Improved binary neural networks. arXiv preprint arXiv:1909.13863, 2019. 6 [3] Zhaowei Cai and Nuno Vasconcelos. Rethinking differen- tiable search for mixed-precision neural networks. In CVPR, pages 2349–2358, 2020. 2, 3, 4, 5 [4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 2 [5] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activa- tion for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. 2 [6] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In CVPR, pages 3075–3084, 2019. 1, 2, 3, 5 [7] Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li. Fair darts: Eliminating unfair advantages in differentiable ar- chitecture search. In ECCV, pages 465–480. Springer, 2020. 4 [8] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830 , 2016. 2 [9] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, pages 5828–5839, 2017. 5 [10] Martin Engelcke, Dushyant Rao, Dominic Zeng Wang, Chi Hay Tong, and Ingmar Posner. V ote3deep: Fast ob- ject detection in 3d point clouds using efﬁcient convolutional neural networks. In ICRA, pages 1355–1361, 2017. 2 [11] Benjamin Graham. Spatially-sparse convolutional neural networks. arXiv preprint arXiv:1409.6070, 2014. 2 [12] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In CVPR, pages 9224–9232, 2018. 1, 2, 3, 5, 6 [13] Yushuo Guan, Pengyu Zhao, Bingxuan Wang, Yuanxing Zhang, Cong Yao, Kaigui Bian, and Jian Tang. Differen- tiable feature aggregation search for knowledge distillation. In ECCV, pages 469–484, 2020. 3 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 2, 6 [15] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El- Yaniv, and Yoshua Bengio. Binarized neural networks. In NeurIPS, pages 4114–4122, 2016. 2 [16] Junghyup Lee, Dohyung Kim, and Bumsub Ham. Network quantization with element-wise gradient scaling. In CVPR, pages 6448–6457, 2021. 2 [17] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmentation in 3d scenes using semantic superpoint tree networks. In ICCV, pages 2783–2792, 2021. 2 [18] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. 2, 3 [19] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang- Ting Cheng. Reactnet: Towards precise binary neural net- work with generalized activation functions. In ECCV, page 143–159, 2020. 1, 5, 6, 9 [20] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the per- formance of 1-bit cnns with improved representational ca- pability and advanced training algorithm. In ECCV, pages 722–737, 2018. 2, 6, 9 10[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431–3440, 2015. 2, 5 [22] Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tz- imiropoulos. Training binary neural networks with real- to-binary convolutions. arXiv preprint arXiv:2003.11535 , 2020. 6, 9 [23] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 5 [24] Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. Mix3d: Out-of-context data aug- mentation for 3d scenes. In 3DV, pages 116–125. IEEE, 2021. 2 [25] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. In CVPR, pages 652–660, 2017. 1 [26] Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. V olumetric and multi-view cnns for object classiﬁcation on 3d data. In CVPR, pages 5648–5656, 2016. 2 [27] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, pages 5099–5108, 2017. 1 [28] Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, and Hao Su. Bipoint- net: Binary neural network for point clouds. arXiv preprint arXiv:2010.05501, 2020. 2, 6 [29] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi- nary convolutional neural networks. In ECCV, pages 525– 542, 2016. 2, 3, 6 [30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234–241. Springer, 2015. 2, 5 [31] Danila Rukhovich, Anna V orontsova, and Anton Konushin. Fcaf3d: Fully convolutional anchor-free 3d object detection. arXiv preprint arXiv:2112.00322, 2021. 2 [32] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d for 3d seman- tic instance segmentation. arXiv preprint arXiv:2210.03105, 2022. 2 [33] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efﬁcient 3d architec- tures with sparse point-voxel convolution. In ECCV, 2020. 2 [34] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and Chang D Yoo. Softgroup for 3d instance segmentation on point clouds. In CVPR, pages 2708–2717, 2022. 2 [35] Haiyang Wang, Lihe Ding, Shaocong Dong, Shaoshuai Shi, Aoxue Li, Jianan Li, Zhenguo Li, and Liwei Wang. Ca- group3d: Class-aware grouping for 3d object detection on point clouds. arXiv preprint arXiv:2210.04264, 2022. 2 [36] Ziwei Wang, Jiwen Lu, Ziyi Wu, and Jie Zhou. Learning ef- ﬁcient binarized object detectors with information compres- sion. TPAMI, 2021. 1 [37] Ziwei Wang, Changyuan Wang, Xiuwei Xu, Jie Zhou, and Jiwen Lu. Quantformer: Learning extremely low-precision vision transformers. TPAMI, 2022. 2 [38] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In ECCV, pages 365–382, 2018. 2 [39] Yanzhe Zhang, Xuezhi Wang, and Diyi Yang. Continual sequence generation with adaptive compositional modules. arXiv preprint arXiv:2203.10652, 2022. 2 [40] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning for point cloud based 3d object detection. In CVPR, pages 4490–4499, 2018. 2 11",
      "meta_data": {
        "arxiv_id": "2303.15493v1",
        "authors": [
          "Xiuwei Xu",
          "Ziwei Wang",
          "Jie Zhou",
          "Jiwen Lu"
        ],
        "published_date": "2023-03-27T13:47:06Z",
        "pdf_url": "https://arxiv.org/pdf/2303.15493v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the significant performance degradation in binarized sparse convolutional networks for point cloud analysis, caused by larger quantization errors compared to standard convolutions. It proposes Binary Sparse Convolutional Networks (BSC-Net), introducing 'shifted sparse convolution (SFSC)' and 'differentiable search strategies' to find optimal active site matching positions. This approach effectively alleviates quantization errors without incurring additional computational overhead during inference. Key findings include achieving significant improvements over strong baselines and state-of-the-art binarization methods, demonstrated by reducing operations per second (OPs) by 92.4% with only 3% mIOU degradation on datasets like ScanNet and NYU Depth v2.",
        "methodology": "The methodology centers on 'Shifted Sparse Convolution (SFSC)' and a 'differentiable search' mechanism. SFSC extends traditional sparse convolution by allowing the kernel center to shift to various locations within the receptive field, thereby fusing more information for active sites. Output channels are uniformly divided into channel groups, each applying a specific SFSC operation. To discover the optimal shift configurations across layers and channel groups, a 'differentiable search strategy' is employed. This involves relaxing the discrete search space into a continuous one using 'sigmoid relaxation' and optimizing architectural parameters via end-to-end gradient descent. An 'efficient search method' constructs a supernet using a 5x5x5 composite sparse convolutional layer to encompass all search space operations, and a 'confidence loss' is used to encourage soft selectors towards discrete values. Optimization proceeds via an alternating approach for network weights and architecture parameters. A 'strong baseline' for binarizing sparse convolution networks is established by integrating beneficial techniques like a specific block structure, PReLU activation, layer-wise scaling factors, piecewise polynomial gradient approximation, enhanced skip connections for downsampling/upsampling, and full-precision pretraining.",
        "experimental_setup": "Experiments are conducted on two indoor scene datasets: 'NYU Depth v2 (NYUDv2)' for 40-class semantic segmentation (1cm voxelization) and 'ScanNet' for 21-category semantic segmentation (2cm and 5cm voxelization). Evaluation metrics include 'mean Intersection over Union (mIoU)', 'mean per-point classification accuracy (mAcc)', and 'overall point-wise classification accuracy (Acc)' for NYUDv2, and 'mIoU' and 'mAcc' for ScanNet. Computational complexity is measured by 'OPs (BOPs/64 + FLOPs)' and storage cost by model parameters. Network architectures used are 'FCN-S/H' for NYUDv2 and 'UNET-S/H' for ScanNet. Training utilizes the 'Adam optimizer' with a 'stepwise scheduler', 'random affine transformations' for data augmentation, and specific learning rates/confidence loss weights. The proposed 'BSC-Net' is compared against 'BSC-Baseline' (strong baseline without SFSC/search) and 'BSC-Manual' (SFSC with manually defined shifts), as well as state-of-the-art binarization methods (XNOR-Net, XNOR-Net++, BiPointNet, Bi-Real-Net, ReActNet) and real-valued models. Ablation studies investigate search space size, number of channel groups, relaxation methods (softmax vs. sigmoid), and derivation strategies.",
        "limitations": "The primary limitations include 'optimization difficulties' that arise with increasing search space size for shift operations, potentially deteriorating final performance despite the efficient search method mitigating computational burden during inference. Additionally, 'decreasing the number of channel groups' can lead to sub-optimal solutions due to channel correlation. The 'Shifted Sparse Convolution (SFSC)' technique itself is specifically designed to reduce quantization error and 'does not provide obvious performance improvements in real-valued networks', indicating its benefits are tied to the binarization context. While the method reduces computational overhead during inference, the training process for the supernet (to search for optimal shifts) involves a 'larger composite sparse convolution layer', implying increased computational and memory demands during the search phase compared to training a fixed architecture.",
        "future_research_directions": "Potential future research directions include: developing 'more robust and scalable differentiable search strategies' capable of exploring larger and more complex architectural spaces without convergence issues; investigating 'adaptive or dynamic shift configurations' that could adjust during inference based on input characteristics; extending the concept of 'quantization-aware specialized operations' (like SFSC) to other forms of sparse networks or different hardware constraints; and further narrowing the 'performance gap between binarized and full-precision models' through continued innovation in binarization techniques."
      }
    },
    {
      "title": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis",
      "abstract": "Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian\nsplat representation has been introduced for novel view synthesis from sparse\nimage sets. Making such representations suitable for applications like network\nstreaming and rendering on low-power devices requires significantly reduced\nmemory consumption as well as improved rendering efficiency. We propose a\ncompressed 3D Gaussian splat representation that utilizes sensitivity-aware\nvector clustering with quantization-aware training to compress directional\ncolors and Gaussian parameters. The learned codebooks have low bitrates and\nachieve a compression rate of up to $31\\times$ on real-world scenes with only\nminimal degradation of visual quality. We demonstrate that the compressed splat\nrepresentation can be efficiently rendered with hardware rasterization on\nlightweight GPUs at up to $4\\times$ higher framerates than reported via an\noptimized GPU compute pipeline. Extensive experiments across multiple datasets\ndemonstrate the robustness and rendering speed of the proposed approach.",
      "full_text": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis Simon Niedermayr simon.niedermayr@tum.de Josef Stumpfegger ga87tux@mytum.de R¨udiger Westermann westermann@tum.de Technical University of Munich Abstract Recently, high-fidelity scene reconstruction with an op- timized 3D Gaussian splat representation has been intro- duced for novel view synthesis from sparse image sets. Mak- ing such representations suitable for applications like net- work streaming and rendering on low-power devices re- quires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity- aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to 31× on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to 4× higher framerates than reported via an opti- mized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach. 1. Introduction Novel view synthesis aims to generate new views of a 3D scene or object by interpolating from a sparse set of images with known camera parameters. NeRF [16] and its variants have proposed the use of direct volume rendering to learn a volumetric radiance field from which novel views can be rendered. However, expensive neural network evaluations prohibit efficient training and rendering. Recent research utilizes explicit scene representations such as voxel-based [24] or point-based structures [29] to enhance rendering ef- ficiency. The use of 3D voxel grids on the GPU in com- bination with a multiresolution hash encoding of the input [17] significantly reduces the operations needed and permits real-time performance. While achieving excellent reconstruction quality and speed, many NeRF-style approaches require exhaustive 93 FPS54 FPS 321 FPS211 FPS 25.2 PSNR /1.5 GB 25.0 PSNR /47 MB 31× Compression Figure 1. Our method achieves a31× compression at indiscernible loss in image quality and greatly improves rendering speed com- pared to [13]. Framerates in grey and white, respectively, are taken on NVIDIA’s RTX 3070M and RTX A5000 at 1080p resolution. memory resources. This affects both the training and ren- dering times and often prohibits the use of such represen- tations in applications like network streaming and mobile rendering. To overcome these limitations, dedicated com- pression schemes for the learned parametrizations on reg- ular grids have been proposed, including vector quantized feature encoding [15], learned tensor decomposition [3] or frequency domain transformations[20, 32]. Recently, differentiable 3D Gaussian splatting [13] has been introduced to generate a sparse adaptive scene repre- sentation that can be rendered at high speed on the GPU.The scene is modeled as a set of 3D Gaussians with shape and appearance parameters, which are optimized via differen- tiable rendering to match a set of recorded images. The optimized scenes usually consist of millions of Gaussians and require up to several gigabytes of storage and mem- ory. This makes rendering difficult or even impossible on low-end devices with limited video memory, such as hand- helds or head-mounted displays. Gaussians are rendered us- ing a specialized compute pipeline, which shows real-time performance on high-end GPUs. This pipeline, however, cannot be seamlessly integrated into VR/AR environments 1 arXiv:2401.02436v2  [cs.CV]  22 Jan 2024or games to work in tandem with hardware rasterization of polygon models. We address the storage and rendering issue of 3D Gaus- sian splatting by compressing the reconstructed Scene pa- rameters and rendering the compressed representation via GPU rasterization. To compress the scenes, we first analyze its components and observe that the SH coefficients and the multivariate Gaussian parameters take up the majority of storage space and are highly redundant. Inspired by previ- ous work on volumetric radiance field compression[15, 25] and deep network weight quantization, we derive a com- pression scheme that reduces the storage requirements of typical scenes by up to a factor of 31×. Our compression scheme consists of three main steps: • Sensitivity-aware clustering: We derive a sensitivity mea- sure for each scene parameter by calculating its contribu- tion to the training images. Color information and Gaus- sian parameters are encoded into compact codebooks via sensitivity-aware vector quantization. • Quantization-aware fine-tuning: To regain information that is lost during clustering we fine-tune the scene pa- rameters at reduced bit-rates using quantization-aware training. • Entropy encoding: 3D Gaussians are linearized along a space-filling curve to exploit the spatial coherence of scene parameters with entropy and run-length encoding. Further, we propose a renderer for the compressed scenes using GPU sorting and rasterization. It enables novel view synthesis in real-time, even on low-end devices, and can be easily integrated into applications rendering polygonal scene representations. Due to the reduced memory band- width requirements of the compressed representation and the use of hardware rasterization, a significant speed-up is achieved over the compute pipeline by Kerbl et al. [13]. We show the state-of-the-art quality of novel view ren- dering on benchmark datasets at significantly reduced mem- ory consumption and greatly improved rendering perfor- mance (Fig. 1). The compressed scenes can be used in applications requiring network streaming, and they can be rendered on low-end devices with limited video memory and bandwidth capacities. We perform a number of exper- iments on benchmark datasets to empirically validate our method across different scenarios. The contribution of each individual step is demonstrated with an ablation study. 2. Related Work Our work builds upon previous works in novel view synthe- sis via differentiable rendering and scene compression. Novel View SynthesisNeural Radiance Fields (NeRF) [16] use neural networks to model a 3D scene. They rep- resent the scene as a density field with direction-dependent colors that are rendered with volume rendering. The field is reconstructed from a set of images with known camera pa- rameters using gradient-based optimization of the volume rendering process. To speed up training and rendering efficiency, a number of different scene models have been proposed. Most often, structured space discretizations like voxel grids [10, 24, 28], octrees [6] or hash grids [17] are used to represent the scene. To avoid encoding empty space, point-based representa- tions have been proposed. Xu et al. [29] perform nearest neighbor search in a point cloud to aggregate local features, and R¨uckert et al. [21] render a point cloud with deep fea- tures and use deferred neural rendering to generate the final image. More recently, differentiable splatting [7, 13] has been positioned as a powerful alternative to NeRF-like ap- proaches for novel view synthesis. In particular, 3D Gaus- sian Splatting [13] offers state-of-the-art scene reconstruc- tion, by using a scene model consisting of an optimized set of 3D Gaussian kernels that can be rendered efficiently. Dif- ferentiable rendering on a set of training images is used to adaptively refine an initial set of Gaussian kernels and opti- mize their parameters. NeRF Compression While grid-based NeRF variants achieve high rendering performance due to GPU ray- marching, in particular, the use of full spatial grids intro- duces considerable storage costs. Tensor decomposition [3, 26], frequency domain transformation [20, 32] and voxel pruning [4] have been proposed to reduce the memory con- sumption of grid-based NeRFs. Takikawa et al. [25] per- form vector quantization during training with a learnable index operation. Li et al. [15] compress grid-based radiance fields by up to a factor of 100× using post-training vector quantization. The use of a hash encoding on the GPU in combination with vector quantization of latent features re- duces the required memory and permits high rendering per- formance [17] A number of works have especially addressed memory reduction during inference, to make grid-based scene rep- resentations more suitable for low-end devices with limited video memory [19, 27]. To our knowledge, our approach is the first that aims at the compression of point-based ra- diance fields to enable high-quality novel view synthesis at interactive frame rates on such devices. Quantization-Aware TrainingRastegari et al. [18] sim- ulate weight quantization during training to reduce quanti- zation errors when using low-precision weights for infer- ence. The use of quantization-aware training has been ex- plored for neural scene representations [8] and voxel-based NeRFs [12], demonstrating effective weight quantization with negligible loss in rendering quality. To reduce the size and latency of neural networks, vari- ous approaches for weight quantization have been explored [8, 11, 12, 18]. These methods rely on the observation that 2 Parameter Sensitivity Calculation Quantization-Aware  Fine-Tuning Codebook Gaussians Codebook Color ... ... Sensitivity Aware Vector Clustering Storage 26.8 PSNR / 47 MB Compressed Scene Reconstructed Scene 27.2 PSNR / 1.4 GB Morton Order Sorting Entropy & Run Length Encoding Figure 2. Proposed compression pipeline. Input is an optimized 3D Gaussian scene representation. First, a sensitivity measure is computed for the Gaussian parameters, and color and shape information is compressed into separate codebooks using sensitivity-aware and scale- invariant vector clustering. Next, the compressed scene is fine-tuned on the training images to recover lost information. Finally, the Gaussians are sorted in Morton order and further compressed using entropy and run-length encoding. The shown scene is from [2]. in most cases a lower weight precision is required for model inference than for training (e.g., 8-bit instead of 32-bit). In post-training quantization, the model weights are reduced to a lower bit representation after training. In quantization- aware training, the quantization is simulated during training while operations are performed at full precision to obtain numerically stable gradients. For storage and inference, the low precision weights can then be used with minor effects on the output. 3. Differentiable Gaussian Splatting Differentiable Gaussian splatting [13] builds upon EW A volume splatting [34] to efficiently compute the projections of 3D Gaussian kernels onto the 2D image plane. On top of that, differentiable rendering is used to optimize the num- ber and parameters of the Gaussian kernels that are used to model the scene. The final scene representation comprises a set of 3D Gaussians, each described by a covariance matrix Σ ∈ R3×3 centered at location x ∈ R3. The covariance matrix can be parameterized by a rotation matrix R and a scaling matrix S. For independent optimization of R and S, Kerbl et al . [13] represent the rotation with a quaternion q and scaling with a vector s, both of which can be converted into their respective matrices. In addition, each Gaussian has its own opacity α ∈ [0, 1] and a set of spherical harmonics (SH) coefficients to reconstruct a view-dependent color. The 2D projection of a 3D Gaussian is again a Gaussian with covariance Σ′ = JW ΣWT JT , (1) where W is the view transformation matrix and J is the Jacobian of the affine approximation of the projective trans- formation. This allows to evaluate the 2D color and opacity footprint of each projected Gaussian. A pixel’s color C is then computed by blending all N 2D Gaussians contribut- ing to this pixel in sorted order: C = X i∈N ciαi i−1Y j=1 (1 − αj). (2) Here, ci and αi, respectively, are the view-dependent color of a Gaussian and its opacity, modulated by the exponential falloff from the projected Gaussian’s center point. The position x, rotation q, scaling s, opacity α, and SH coefficients of each 3D Gaussian are optimized so that the rendered 2D Gaussians match the training images. For more details on the reconstruction process, we refer to the original paper by Kerbl et al. [13]. 4. Sensitivity-Aware Scene Compression We compress a set of optimized 3D Gaussian kernels as fol- lows: First, sensitivity-aware vector clustering is used to cluster the Gaussian appearance and shape parameters into compact codebooks (Sec. 4.1). Second, the clustered and other scene parameters are fine-tuned on the training im- ages to recover information lost due to clustering. We use quantization-aware training in this step to reduce the scene parameters to a lower bit-rate representation (Sec. 4.2). By linearizing the set of 3D Gaussians along a space-filling curve, entropy and run-length encoding can exploit the spa- tial coherence of Gaussian parameters to further compress the scene (Sec. 4.3). An overview of the proposed compres- sion pipeline is shown in Fig. 2. 4.1. Sensitivity-Aware Vector Clustering Inspired by volumetric NeRF compression [15, 25], we uti- lize vector clustering for compressing 3D Gaussian kernels. We use clustering to encode SH coefficients and Gaussian shape features (scale and rotation) into two separate code- books. As a result, each Gaussian can be compactly en- coded via two indices into the codebooks stored alongside. 3Parameter Sensitivity: The sensitivity of the recon- struction quality to changes of the Gaussian parameters is not consistent. While a slight change in one parameter of a Gaussian can cause a significant difference in the rendered image, a similar change in another parameter or the same parameter of another Gaussian can have low or no effect. We define the sensitivity S of image quality to changes in parameter p with respect to the training images as S(p) = 1PN i=1 Pi NX i=1 \f\f∂Ei ∂p \f\f. (3) N is the number of images in the training set used for scene reconstruction, and Pi is the number of pixels in imagei. E is the total image energy, i.e., the sum of the RGB com- ponents over all pixels. The sensitivity of E to changes in p is considered via the gradient of E with respect to p, i.e., a large gradient magnitude indicates high sensitivity to changes in the respective parameter. With this formulation, the sensitivity to every parameter can be computed with a single backward pass over each of the training images. Sensitivity-aware k-Means: Given a vector x ∈ RD, we define its sensitivity as the maximum over its compo- nent’s sensitivity: S(x) = max d∈[1..D] S(xd). (4) The sensitivity measure is then used for sensitivity-aware clustering, i.e., to compute codebooks C ∈ RK×D with K representatives ck ∈ RD (so-called centroids). We define the weighted distance between a vector x and a centroid ck as D(x, ck) = S(x)∥x − ck∥2 2. (5) A codebook is then obtained by using k-Means clustering with D as a similarity measure. The codebooks are initial- ized randomly with a uniform distribution within the min- imum and maximum values of each parameter. The cen- troids are computed with an iterative update strategy: In each step, the pairwise weighted distances between the vec- tors x and the codebook vectors ck are calculated, and each vector is assigned to the centroid to which it has the mini- mum distance. Each centroid is then updated as ck = 1P xi∈A(k) S(xi) X xi∈A(k) S(xi)xi (6) Where A(k) is the set of vectors assigned to centroid ck. For performance reasons, a batched clustering strategy is used [23]. In each update step, a random subset of vectors is picked and used to compute the update step. Then, the cen- troids are updated using the moving average with a decay factor λd. 0 max 0% 5% 100%percentage of Gaussians Garden 0 max sensitivity Truck 0 max Playroom Figure 3. Histograms of maximum sensitivity to changes of SH coefficients for different scenes. Only SH coefficients of a tiny fraction of all Gaussians strongly affect image quality. Color Compression: Each Gaussian stores SH coeffi- cients to represent the direction-dependent RGB color (e.g., 48 coefficients in [13]). We treat SH coefficients as vectors and compress them into a codebook using sensitivity-aware vector clustering. For volumetric NeRF models, Li et al. [15] have shown that only a small number of voxels contribute significantly to the training images. Thus, they propose to keep the color features that contribute the most and only compress the re- maining features with vector clustering. We observe a sim- ilar behavior for 3D Gaussians, as shown for some bench- mark scenes in Fig. 3. For a small percentage of all SH co- efficients (< 5%), the sensitivity measure indicates a high sensitivity towards the image quality. Thus, to keep the in- troduced rendering error low, we do not consider the SH vectors of Gaussians with a sensitivity higher than a thresh- old βc in the clustering process. These vectors are added to the codebook after clustering. Gaussian Shape Compression: A 3D Gaussian kernel can be parameterized with a rotation matrixR and a scaling vector s. We observe that for typical scenes, the shapes of the Gaussians are highly redundant up to a scaling factor. Thus, we re-parameterize the scaling vector s = ηˆs, where η = ∥s∥2 is the scalar scaling factor and ˆs = η−1s is the normalized scaling vector. With ˆS = diag(ˆs), the normal- ized covariance matrix is ˆΣ = (R ˆS)(R ˆS)T = 1 η2 Σ. (7) Clustering is then performed using the normalized co- variance matrices, and each Gaussian stores, in addition to a codebook index, the scalar scaling factor η. We com- pute the sensitivity to each of the matrix entries and perform sensitivity-aware vector quantization to compress them into a codebook. The sensitivity plots for Gaussian shape pa- rameters look mostly similar to the SH plots shown in Fig. 3. As for SH coefficients, Gaussians with a maximum sensitivity over a threshold βg are not considered for clus- tering and are added to the codebook. 4Note that k-Means clustering of normalized covariance matrices results in covariance matrices that are again nor- malized. However, due to floating point errors, clustering can lead to non-unit scaling vectors. To counteract this problem, we re-normalize each codebook vector after each update step by dividing it through the trace of the covariance metric. In the appendix, we prove both the normalization preserving properties of k-Means and re-normalization. After clustering, each codebook entry is decomposed into a rotation and scale parameter using an eigenvalue decomposition. This is required for quantization-aware training since direct optimization of the matrix is not possible[13]. In the final codebook, each matrix’s rotation and scaling parameters are encoded via 4 (quaternion) plus 3 (scaling) scalar values. 4.2. Quantization-Aware Fine-Tuning To regain information that is lost due to parameter quan- tization, the parameters can be fine-tuned on the training images after compression [15, 30]. To do so, we use the training setup described by Kerbl et al. [13]. We optimize for the position, opacity, and scaling factor of each Gaus- sian as well as the color and Gaussian codebook entries. For the two codebooks, the incoming gradients for each en- try are accumulated and then used to update the codebook parameters. For fine-tuning, we utilize quantization-aware training with Min-Max quantization (k-bit Quantization [18]) to rep- resent the scene parameters with fewer bits. In the for- ward pass, the quantization of a parameter p is simulated using a rounding operation considering the number of bits and the moving average of each parameter’s minimum and maximum values. The backward pass ignores the simulated quantization and calculates the gradient w.r.t. p as without quantization. After training, the parameters can be stored with only b-bit precision (e.g., 8-bit), while the minimum and maximum values required for re-scaling are stored at full precision (e.g., 32-bit float). Quantization of opacity is applied after the sigmoid ac- tivation function. Quantization of the scaling and rotation vector is applied before the respective normalization step. For the scale factor parameter, the quantization is applied before the activation (exponential function) to allow for a fine-grained representation of small Gaussians without los- ing the ability to model large ones. We quantize all Gaus- sian parameters despite position to an 8-bit representation with the Min-Max scheme. 16-bit float quantization is used for position, as a further reduction decreases the reconstruc- tion quality considerably. 4.3. Entropy Encoding After quantization-aware fine-tuning, the compressed scene representation consists of a set of Gaussians and the code- books storing SH coefficients and shape parameters. Indices into the codebooks are stored as 32-bit unsigned integers. The data is then compressed using DEFLATE [5], which utilizes a combination of the LZ77 [33] algorithm and Huff- man coding. In the reconstructed scenes, many features, such as color, scaling factor, and position, are spatially co- herent. By ordering the Gaussians according to their posi- tions along a Z-order curve in Morton order, the coherence can be exploited and the effectivity of run-length encoding (LZ77) can be improved. The effect on the compressed file size is analyzed in the ablation study in Sec. 6.4. Note that entropy encoding reduces the two codebook indices to their required bit-length according to the codebook sizes. 5. Novel View Rendering Kerbl et al. [13] propose a software rasterizer for differen- tiable rendering and novel view synthesis. To render 3D Gaussian scenes fast especially on low-power GPUs, our novel view renderer utilizes hardware rasterization. Preprocess: In a compute pre-pass, Gaussians whose 99% confidence interval does not intersect the view frustum after projection are discarded. For the remaining Gaussians, the direction-dependent color is computed with the SH co- efficients. The color, the Gaussian’s opacity, projected screen-space position, and covariance values are stored in an atomic linear-append buffer. The covariance values indi- cate the orientation and size of the 2D Gaussian into which a 3D Gaussian projects under the current viewing transfor- mation [34]. As in [13], Gaussians are then depth-sorted to enable order-dependent blending. We use the Onesweep sorting algorithm by Adinets and Merrill [1] to sort the Gaussians directly on the GPU. Due to its consistent per- formance, the implementation is well suited for embedding into real-time applications. Rendering: Gaussians are finally rendered in sorted or- der via GPU rasterization. For each Gaussian, one planar quad (a so-called splat) consisting of two triangles is ren- dered. A vertex shader computes the screen space vertex positions of each splat from the 2D covariance information. The size of a splat is set such that it covers the 99% confi- dence interval of the projected Gaussian. The vertex shader simply outputs the color computed in the pre-pass and the 2D splat center as input to the pixel shader. The pixel shader then discards fragments outside the 99% confidence inter- val. All remaining fragments use their distance to the splat center to compute the exponential color and opacity falloff and blend their final colors into the framebuffer. 6. Experiments 6.1. Datasets We evaluate our compression and rendering method on the Mip-Nerf360[2] indoor and outdoor scenes, two scenes 5Method 3D Gaussian Splatting Ours Dataset PSNR↑ SSIM↑ LPIPS↓ SIZE↓ PSNR↑ SSIM↑ LPIPS↓ SIZE↓ Compression Ratio↑ Synthetic-NeRF [16] 33.21 0 .969 0 .031 69 .89 32.936 0 .967 0 .033 3 .68 19.17 Mip-NeRF360 [2] 27.21 0 .815 0 .214 795 .26 26.981 0 .801 0 .238 28 .80 26.23 Tanks&Temples [14] 23.36 0 .841 0 .183 421 .90 23.324 0 .832 0 .194 17 .28 23.26 Deep Blending [9] 29.41 0 .903 0 .243 703 .77 29.381 0 .898 0 .253 25 .30 27.81 average* 26.58 0 .853 0 .213 640 .31 26.560 0 .844 0 .238 23 .73 25.77 Table 1. Quantitative comparison to 3D Gaussian Splatting. Size is measured in Megabytes. *Synthetic scenes are excluded. from the Tanks&Temples[14] and Deep Blending [9] dataset, and NeRF-Synthetic[16]. For Mip-Nerf360, Tanks&Temples and Deep Blending the reconstructions from Kerbl et al . [13] were used. We generated the 3D Gaussian representation for NeRF-Synthetic ourselves. 6.2. Implementation Details We use a decay factor λd = 0.8 for batched clustering with 800 update steps for the Gaussians and 100 for the SH co- efficients. A batch size of 218 is used for the color fea- tures, and 220 for the Gaussian parameters. We use 4096 as the default codebook size in all our experiments and set βc = 6 · 10−7 and βg = 3 · 10−6. We perform 5000 opti- mization steps of quantization-aware fine-tuning. The renderer is implemented with the WebGPU graph- ics API in the Rust programming language. Thus, it can run in a modern web browser on a large variety of devices. More details about the implementation can be found in the supplementary material. The source code is available at https://github.com/KeKsBoTer/c3dgs. 6.3. Results We use the scenes reconstructed by 3D Gaussian Splatting [13] and compress them using the proposed method. For all scenes, we evaluate the PSNR, SSIM, and LPIPS [31] before and after compression. Tab. 1 shows the results for different datasets. Our compression method achieves a compression ratio of up to 31× with an average of 26× at the indiscernible loss of quality (0.23 PSNR on average) for real-world scenes. Here, it should be noted that a difference of 0.5 PSNR is considered indistinguishable for the human eye [22]. For some of the scenes, Fig. 5 compares training images to the renderings of the uncompressed and compressed scenes. Fig. 4 shows close-up views of uncompressed and com- pressed synthetic scenes. More comparisons and results are given in the supplementary material. Image Quality LossFig. 5 shows that it is almost im- possible to spot the difference between the uncompressed and the compressed scenes. We also analyze the images from all test sets with the largest drop in PSRN. The image which could be reconstructed least accurately is shown in Fig. 6. We observe that the loss is mainly due to very subtle Baseline Compressed Baseline Compressed Figure 4. 3D Gaussian splatting of synthetic scenes [16]. Uncom- pressed (Baseline) vs. compressed scene. color shifts below what can be perceived by the human eye. Compression RuntimeThe compression process takes about 5-6 minutes and increases the reconstruction time by roughly 10%. The timings of each individual steps are given in the supplementary material. Rendering TimesWe see a significant increase of up to a factor of 4× in rendering speed (see Tab. 2). Roughly a 2x increase can be attributed to the compressed data’s reduced bandwidth requirements, hinting at the software rasterizer’s memory-bound performance by Kerbl et al. [13]. The ad- ditional speedup is achieved by the hardware rasterization- based renderer, which pays off on low- and high-end GPUs. Timings of the different rendering stages are given in the supplementary material. 6.4. Ablation Study In a number of experiments we evaluate the components of our compression pipeline. This includes a detailed analysis of the influence of the hyper-parameters. Loss Contribution Tab. 3 indicates that the most sig- 6Bicycle Train Playroom  Ground Truth  Baseline  Ours Figure 5. Ground truth images from the test set, results of Kerbl et al. [13] (Baseline), results using the compressed representation (Ours). a) Ground Truth  b) Baseline (35.90 PSNR)  c) Ours (34.15 PSNR)  d) Mean Absolute Error 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 Figure 6. Test image with the highest drop in PSNR in all scenes used in this work. d) Per pixel mean absolute error between Kerbl et al. [13] b) and our approach c). 7NVIDIA RTX A5000 NVIDIA RTX 3070M Intel UHD Graphics 11 AMD Radeon R9 380 Bicycle Kerbl et al. [13]93 54 - - Ours 215 134 9 41 Compressed 321 211 16 83 Bonsai Kerbl et al. [13]184 122 - - Ours 414 296 23 76 Compressed 502 380 28 128 Table 2. Rendering performance at 1080p resolution in frames per second, averaged over all training images. Bicycle consists of 6.1 million 3D Gaussians, Bonsai of 1.2 million 3D Gaussians. PSNR↑ SSIM↑ LPIPS↓ SIZE↓ baseline 27.179 0 .861 0 .115 1379.99 + Pruning 27.083 0 .856 0 .118 1217.25 + Color Clustering 25.941 0 .818 0 .178 278.41 + Gaussian Clustering25.781 0 .811 0 .186 164.15 + QA Finetune 26.746 0 .844 0 .144 86.69 + Encode 26.746 0 .844 0 .144 58.40 + Morton Order 26.746 0 .844 0 .144 46.57 Table 3. Losses introduced and regained by individual stages of the compression pipeline. Experiments were performed with the garden scene from Mip-Nerf360[2] nificant loss increase comes from the compression of the SH coefficients, which, on the other hand, gives the high- est memory reduction. Quantization of shape parameters can additionally reduce the memory by about 60%, only introducing a slight loss in image quality. Quantization- aware fine-tuning can regain much of the information that is lost due to quantization and further reduces the memory by about 50%. Entropy and run length encoding in combi- nation with Morton order layout saves an additional50% of the memory. Codebook Sizes SH coefficients and Gaussian shape parameters are compressed into codebooks of predefined sizes. Tab. 3 shows the effects of different codebook sizes on image quality. Errors were averaged over all test images, with the difference to the maximum error given in brack- ets. It can be seen that the codebook size has little effect on the average reconstruction error, independent of the scene. Nevertheless, larger codebooks reduce the maximum error with only minimal memory overhead. Sensitivity Thresholds The sensitivity thresholds βc and βg are used to decide whether to consider SH coeffi- cients and shape parameters for clustering. They offer a trade-off between quality and compression rate. The influ- ence of these values is analyzed in Tab. 5, showing in par- ticular the sensitivity of image quality to the quantization of SH coefficients. 6.5. Limitations As the main limitation for making the proposed compres- sion and rendering pipeline even more powerful, we see the current inability to aggressively compress the Gaus- PSNR↑ SSIM↑ LPIPS↓ SIZE↓ Color 1024 26.95(-0.67) 0.80(-0.02) 0.24(+0.03)28.47 2048 26.95(-0.62) 0.80(-0.02) 0.24(+0.03)28.65 4096 26.98(-0.63)0.80(-0.02)0.24(+0.03) 28.80 8192 27.00(-0.58) 0.80(-0.02) 0.24(-0.03)28.92 Gaussian 1024 26.95(-0.79) 0.80(-0.02) 0.24(+0.03)28.14 2048 26.97(-0.80) 0.80(-0.02) 0.24(+0.03)28.45 4096 26.98(-0.63)0.80(-0.02)0.24(+0.03) 28.80 8192 26.97(-0.60) 0.80(-0.02) 0.24(+0.03)29.06 Table 4. Average reconstruction error over the test images for different codebook sizes, including the maximum deviation from the baseline (+/−). Rows marked grey indicate the default con- figurations. Experiments were performed on the Mip-Nerf360[2] dataset. PSNR↑ SSIM↑ LPIPS↓ SIZE↓ baseline 26.976 0.801 0.238 28.80 βc 6.0·10−8 27.22(−0.25) 0.81(−0.00) 0.22(+0.00) 56.50 3.0·10−7 27.09(−0.41) 0.80(−0.01) 0.23(+0.02) 33.00 6.0·10−7 26.98(−0.63) 0.80(−0.02) 0.24(+0.03) 28.80 1.2·10−6 26.87(−0.75) 0.80(−0.02) 0.24(+0.04) 27.02 6.0·10−6 26.74(−0.94) 0.80(−0.02) 0.25(+0.04) 25.97 - 26.55(−1.47) 0.79(−0.03) 0.25(+0.05) 25.65 βg 3.0·10−7 27.05(−0.41) 0.80(−0.01) 0.23(+0.03) 33.90 1.5·10−6 27.00(−0.62) 0.80(−0.02) 0.24(+0.03) 29.95 3.0·10−6 26.98(−0.63) 0.80(−0.02) 0.24(+0.03) 28.80 6.0·10−6 26.91(−0.77) 0.80(−0.02) 0.24(+0.03) 28.08 3.0·10−5 26.86(−0.72) 0.80(−0.02) 0.24(+0.04) 27.30 - 26.80(−0.83) 0.80(−0.02) 0.25(+0.04) 27.10 Table 5. Sensitivity threshold ablation study. βc and βg are the sensitivity thresholds for controlling which SH vectors and shape parameters are clustered. The average error and in brackets the maximum deviation from the baseline are reported. The last row shows the results when no threshold is considered. The rows marked grey are the default configurations. Experiments were per- formed with Mip-Nerf360 [2] dataset. sians’ positions in 3D space. We performed experiments where positions were quantized to a lattice structure, and we even embedded these positional constraints into the Gaus- sian splatting training process. Unfortunately, we were not able to further compress the positions without introducing a significant error in the rendering process. 7. Conclusion We have introduced a novel compression and rendering pipeline for 3D Gaussians with color and shape parameters, achieving compression rates of up to 31 × and up to a 4 × increase in rendering speed. Our experiments with differ- ent datasets have shown that the compression introduces an indiscernible loss in image quality. The compressed data can be streamed over networks and rendered on low-power devices, making it suitable for mobile VR/AR applications and games. In the future, we aim to explore new approaches for reducing the memory footprint during the training phase, and additionally compressing positional information end- to-end. We also believe that 3D Gaussian splatting has the 8potential for reconstructing volumetric scenes, and we will investigate advanced options for compressing and rendering the optimized representations. References [1] Andy Adinets and Duane Merrill. Onesweep: A Faster Least Significant Digit Radix Sort for GPUs. arXiv preprint arXiv:2206.01784, 2022. 5 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5460–5469, New Orleans, LA, USA, 2022. IEEE. 3, 5, 6, 8, 11, 13, 15, 16 [3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In Computer Vision – ECCV 2022, pages 333–350, Cham, 2022. Springer Nature Switzerland. 1, 2 [4] Chenxi Lola Deng and Enzo Tartaglione. Compressing Explicit V oxel Grid Representations: fast NeRFs become also small. In 2023 IEEE/CVF Winter Conference on Ap- plications of Computer Vision (WACV) , pages 1236–1245, Waikoloa, HI, USA, 2023. IEEE. 2 [5] L. Peter Deutsch. DEFLATE Compressed Data Format Specification version 1.3, 1996. Issue: 1951 Num Pages: 17 Series: Request for Comments Published: RFC 1951. 5 [6] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenox- els: Radiance Fields without Neural Networks. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5491–5500, New Orleans, LA, USA, 2022. IEEE. 2 [7] Yiming Gao, Yan-Pei Cao, and Ying Shan. SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Re- construction of Indoor Scenes. In 2023 IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 108–118, Vancouver, BC, Canada, 2023. IEEE. 2 [8] Cameron Gordon, Shin-Fang Chng, Lachlan MacDonald, and Simon Lucey. On Quantizing Implicit Neural Rep- resentations. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pages 341–350, Waikoloa, HI, USA, 2023. IEEE. 2 [9] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep Blending for Free-viewpoint Image-based Rendering. ACM Transactions on Graphics (Proc. SIGGRAPH Asia) , 37(6):257:1–257:15, 2018. Publisher: ACM. 6, 11, 13, 14 [10] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural ra- diance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 5875–5884, 2021. 2 [11] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 2704–2713, 2018. 2 [12] Seungyeop Kang and Sungjoo Yoo. TernaryNeRF: Quantiz- ing V oxel Grid-based NeRF Models. In2022 IEEE Interna- tional Workshop on Rapid System Prototyping (RSP), pages 8–14, Shanghai, China, 2022. IEEE. 2 [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph., 42(4), 2023. Place: New York, NY , USA Publisher: Association for Com- puting Machinery. 1, 2, 3, 4, 5, 6, 7, 8, 12 [14] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG) , 36 (4):1–13, 2017. Publisher: ACM New York, NY , USA. 6, 11, 13, 14 [15] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing V olumetric Radiance Fields to 1 MB. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4222–4231, Vancouver, BC, Canada, 2023. IEEE. 1, 2, 3, 4, 5 [16] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. Communications of the ACM , 65(1):99–106, 2021. Publisher: ACM New York, NY , USA. 1, 2, 6, 11, 13, 17, 18 [17] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant neural graphics primitives with a mul- tiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1–15, 2022. Publisher: ACM New York, NY , USA. 1, 2 [18] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: ImageNet Classification Us- ing Binary Convolutional Neural Networks. In Computer Vision – ECCV 2016, pages 525–542, Cham, 2016. Springer International Publishing. 2, 5 [19] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini- vasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. ACM Trans- actions on Graphics (TOG) , 42(4):1–12, 2023. Publisher: ACM New York, NY , USA. 2 [20] Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, and Eunbyung Park. Masked Wavelet Representation for Compact Neural Radiance Fields. InPro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20680–20690, 2023. 1, 2 [21] Darius R ¨uckert, Linus Franke, and Marc Stamminger. ADOP: approximate differentiable one-pixel point render- ing. ACM Trans. Graph., 41(4):1–14, 2022. 2 [22] David Salomon and Giovanni Motta. Handbook of data com- pression. Springer Science & Business Media, 2010. 6 [23] D. Sculley. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web, pages 1177–1178, Raleigh North Carolina USA, 2010. ACM. 4 [24] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct V oxel Grid Optimization: Super-fast Convergence for Radiance 9Fields Reconstruction. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 5449–5459, New Orleans, LA, USA, 2022. IEEE. 1, 2 [25] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas M¨uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable Bitrate Neural Fields. In ACM SIGGRAPH 2022 Conference Proceedings, New York, NY , USA, 2022. Asso- ciation for Computing Machinery. event-place: Vancouver, BC, Canada. 2, 3 [26] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable NeRF via Rank-residual Decomposition. In Advances in Neural Information Process- ing Systems , pages 14798–14809. Curran Associates, Inc., 2022. 2 [27] Krishna Wadhwani and Tamaki Kojima. SqueezeNeRF: Fur- ther factorized FastNeRF for memory-efficient inference. In 2022 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition Workshops (CVPRW) , pages 2716–2724, New Orleans, LA, USA, 2022. IEEE. 2 [28] Sebastian Weiss and R ¨udiger Westermann. Differentiable Direct V olume Rendering. In IEEE Transactions on Visu- alization and Computer Graphics, pages 562–572, 2022. Is- sue: 1. 2 [29] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5438–5448, 2022. 1, 2 [30] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for Real-time Rendering of Neural Radiance Fields. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5732–5741, Montreal, QC, Canada, 2021. IEEE. 5 [31] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht- man, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 586–595, Salt Lake City, UT, 2018. IEEE. 6 [32] Tianli Zhao, Jiayuan Chen, Cong Leng, and Jian Cheng. TinyNeRF: Towards 100 x Compression of V oxel Radiance Fields. Proceedings of the AAAI Conference on Artificial In- telligence, 37(3):3588–3596, 2023. Number: 3. 1, 2 [33] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Trans. Inform. Theory, 23(3):337– 343, 1977. 5 [34] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross. EW A volume splatting. In Proceedings Visualization, 2001. VIS ’01., pages 29–538, San Diego, CA, USA, 2001. IEEE. 3, 5 10Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis Supplementary Material 8. Supplementary A. Detailed Scene Analysis For all scenes used in the paper, we report the PSNR, SSIM, LPIPS, memory consumption, and compression ratio of our approach. See Tab. 8 for Mip-Nerf360 [2], Tab. 10 for Deep Blending [9], Tab. 9 for Tanks&Temples [14], Tab. 11 for the synthetic scenes from [16]. B. Image Quality For all scenes used in the paper, a random test view is se- lected. The ground truth images are compared to the ren- derings of the uncompressed (baseline) and our compressed (Compressed) scene representation. See Figs. 11 and 12 for Mip-Nerf360 [2], Fig. 10 for Deep Blending [9], Fig. 9 for Tanks & Temples [14], Figs. 13 and 14 for the synthetic scenes from [16]. C. Memory Requirements Fig. 7 illustrates the memory requirements of different scene parameters. The coordinates of the 3D Gaussian cen- ter points and the codebook indices take up the most mem- ory in general. The amount of memory required by the color codebook varies significantly between different scenes. 0% 20% 40% 60% 80% 100% Flowers Garden Stump Treehill Room Counter Kitchen Bonsai Bicycle   Truck Train    Playroom Drjohnson     Chair Drums Ficus Hotdog Lego Materials Mic Ship Mip-Nerf360 T anks & T emples Deep  Blending NeRF Synthetic position color opacity  shape indices Figure 7. Storage size of different scene parameters in the com- pressed representation. Color is the codebook with all SH coeffi- cients. Shape is the codebook with the Gaussian parameters and η is the scaling factor. D. Timing Statistics We provide timings for the different stages of our compres- sion pipeline. Tab. 6 shows the average and maximum time required by each stage. It can be seen that the fine-tuning stage takes up 70% of the total time. Average Time↓ Maximum Time↓ Sensitivity Calculation 8.05 11 .38 Clustering 75.11 78 .41 QA Fine-tuning 213.30 278 .05 Encoding 2.69 5 .13 Total 299.15 365 .94 Table 6. Time requirements of the individual stages of the com- pression pipeline. We report the average and maximum time of each stage in seconds. The entropy and run-length encoding are grouped into the Encoding stage. Measurements were taken with an NVIDIA RTX A5000 graphics card. Additionally, we report timings for each stage of the novel view renderer. Tab. 7 shows the average times for two different scenes. It can be seen that the preprocessing stage is accelerated by a factor of 5× when using the compressed scene representation. Preprocess↓ Sorting↓ Rasterization↓ Total↓ Bicycle Uncompressed 1.46 0 .55 2 .81 4.82 Compressed 0.28 0 .48 2 .45 3.22 Bonsai Uncompressed 0.44 0 .20 1 .81 2.44 Compressed 0.09 0 .19 1 .67 1.95 Table 7. Timings in milliseconds for the different stages of our renderer. Evaluated on an NVIDIA RTX A5000 with scenes from Mip-Nerf360 [2] 11(a) Baseline  (b) Compressed Figure 8. Pruning failure case. Compared to the baseline recon- struction, some leaves have been removed in the compressed ver- sion due to pruning. E. Sensitivity Calculation and Pruning The sensitivity of a parameter is calculated using the gra- dient of the total image energy wrt. this parameter (see Eq. (3)). Kerbl et al . [13] clamp negative direction- dependent colors (i.e., resulting from the evaluation of the SH coefficients) to zero. For the clamped values, the partial derivatives are set to zero in the backward pass. This results in a sensitivity of zero for the respective SH coefficients, which is not desired since they possibly contribute to the training images. Therefore, we do not clamp colors when calculating the sensitivity. We observe that a notable number of Gaussians (up to 15%) do not have any impact on the training images. These particular splats exhibit zero sensitivity in the color param- eters. Consequently, we opt to eliminate these splats from the scene (called Pruning in Tab. 3). Experiments with higher pruning thresholds have shown that more Gaussians can be removed with minimal loss in PSNR. However, this can lead to fine details in the scene being removed, which we consider undesirable. An exam- ple of this can be seen in Fig. 8, where small leaves were removed from the reconstruction due to pruning. F. Covariance Matrix Clustering Given a rotation matrix R ∈ R3×3 and a scaling vector s ∈ R3 >0. The covariance matrix Σ is defined as [13] Σ = RSSRT = RS2RT , (8) with S = diag(s) . Since Σ is real and symmetric it holds that S2 = diag([λ1, λ2, λ3]T ) = diag([s2 1, s2 2, s2 3]T ), (9) where λi are the eigenvalues of Σ. By using the trace of Σ, the squared length of s can be calculated as Tr(Σ) = 3X i=1 λi = 3X i=1 s2 i = ∥s∥2 2 (10) Clustering Update Step In the following, we show that the clustering update step results in normalized covariance matrices as cluster centroids. Given N normalized covari- ance matrices ˆΣi with ∥si∥2 = 1 and respective weighting factors wi ∈ R>0. Their centroid ˆΣc is calculated as ˆΣc = 1PN i=1 wi NX i=1 wi ˆΣi (11) . By using Eq. (10) it holds that ∥sc∥2 2 = Tr(ˆΣc) (12) = Tr( 1PN i=1 wi NX i=1 wi ˆΣi) (13) = 1PN i=1 wi NX i=1 wiTr(ˆΣi) (14) = 1PN i=1 wi NX i=1 wi∥si∥2 2 (15) = 1 (16) This proves that the covariance matrix ˆΣc has a normalized scaling vector and thus iteself is in a normalized form. Covariance Matrix Normalization The following derivation proofs that a covariance matrix Σ can be trans- formed into its normalized form ˆΣ by dividing it by its trace, i.e., Σ Tr(Σ) = R S2 Tr(Σ)RT = R S ∥s∥2 S ∥s∥2 RT (17) = R ˆS2RT = ˆΣ (18) 123D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ bicycle 25.171 0 .762 0 .216 1450 .277 24.970 0 .751 0 .240 47 .147 30.761 bonsai 31.979 0 .938 0 .208 294 .415 31.347 0 .930 0 .217 12 .794 23.011 counter 28.888 0 .905 0 .204 289 .244 28.671 0 .896 0 .215 13 .789 20.977 flowers 21.448 0 .602 0 .341 860 .062 21.152 0 .584 0 .358 31 .140 27.619 garden 27.179 0 .861 0 .115 1379 .993 26.746 0 .844 0 .144 46 .565 29.636 kitchen 30.713 0 .923 0 .130 438 .099 30.262 0 .914 0 .140 18 .874 23.211 room 31.341 0 .916 0 .223 376 .853 31.138 0 .911 0 .231 15 .033 25.068 stump 26.562 0 .770 0 .219 1173 .522 26.285 0 .757 0 .250 40 .569 28.926 treehill 22.303 0 .631 0 .328 894 .903 22.256 0 .620 0 .351 33 .318 26.859 average 27.287 0 .812 0 .220 795 .263 26.981 0 .801 0 .238 28 .803 26.230 Table 8. Mip-Nerf360 [2] results. 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ train 21.770 0 .805 0 .217 242 .782 21.863 0 .798 0 .226 13 .249 18.324 truck 24.940 0 .871 0 .155 601 .030 24.823 0 .867 0 .161 21 .316 28.196 average 23.355 0 .838 0 .186 421 .906 23.343 0 .832 0 .194 17 .282 23.260 Table 9. Tanks&Temples [14] results 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ drjohnson 28.938 0 .896 0 .248 805 .358 28.871 0 .895 0 .254 28 .938 27.830 playroom 29.926 0 .901 0 .244 602 .186 29.891 0 .900 0 .252 21 .660 27.802 average 29.432 0 .898 0 .246 703 .772 29.381 0 .898 0 .253 25 .299 27.816 Table 10. Deep Blending [9] results 3D Gaussian Splatting Ours Scene PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ SIZE ↓ Ratio ↑ chair 35.864 0 .987 0 .012 70 .105 35.297 0 .985 0 .014 3 .575 19.609 drums 26.072 0 .954 0 .038 83 .665 25.941 0 .952 0 .040 3 .829 21.848 ficus 34.736 0 .987 0 .012 70 .177 34.559 0 .986 0 .013 3 .059 22.937 hotdog 37.646 0 .985 0 .021 34 .079 37.367 0 .984 0 .022 2 .725 12.505 lego 35.399 0 .981 0 .017 76 .071 34.802 0 .979 0 .020 4 .314 17.633 materials 29.861 0 .959 0 .035 71 .833 29.602 0 .957 0 .038 4 .021 17.862 mic 35.155 0 .991 0 .006 77 .563 34.913 0 .991 0 .007 3 .025 25.640 ship 30.954 0 .905 0 .111 75 .659 31.005 0 .905 0 .111 4 .938 15.322 average 33.211 0 .969 0 .031 69 .894 32.936 0 .967 0 .033 3 .686 19.170 Table 11. NeRF Synthetic [16] results 13Truck Train Ground Truth Baseline Compressed Figure 9. Random test views for each scene from Tanks&Temples [14] Playroom Drjohnson Ground Truth Baseline Compressed Figure 10. Random test views for each scene from Deep Blending [9] 14Flowers Garden Stump Treehill Room Ground Truth Baseline Compressed Figure 11. Random test views for each scene from Mip-NeRF360 [2] 15Counter Kitchen Bonsai Bicycle Ground Truth Baseline Compressed Figure 12. Random test views for each scene from Mip-NeRF360 [2] 16Chair Drums Ficus Hotdog Ground Truth Baseline Compressed Figure 13. Random test views for each scene from NeRF Synthetic [16] 17Lego Materials Mic Ship Ground Truth Baseline Compressed Figure 14. Random test views for each scene from NeRF Synthetic [16] 18",
      "meta_data": {
        "arxiv_id": "2401.02436v2",
        "authors": [
          "Simon Niedermayr",
          "Josef Stumpfegger",
          "Rüdiger Westermann"
        ],
        "published_date": "2023-11-17T14:40:43Z",
        "pdf_url": "https://arxiv.org/pdf/2401.02436v2.pdf",
        "github_url": "https://github.com/KeKsBoTer/c3dgs"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces a compressed 3D Gaussian splat representation for accelerated novel view synthesis, addressing the high memory consumption and rendering inefficiency of existing 3D Gaussian splatting methods. The main contributions include a compression scheme utilizing sensitivity-aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters, achieving up to 31x compression rates with minimal visual quality degradation. Furthermore, it proposes an efficient hardware rasterization-based renderer for the compressed representation, enabling up to 4x higher framerates on lightweight GPUs and seamless integration into applications like VR/AR environments and games.",
        "methodology": "The methodology consists of a three-step compression pipeline. First, sensitivity-aware vector clustering is applied to compress Gaussian appearance (SH coefficients) and shape (scale and rotation) parameters into compact codebooks. This involves defining a sensitivity measure based on the gradient of image energy to parameter changes and using sensitivity-aware k-Means clustering, while exempting highly sensitive parameters. Second, quantization-aware fine-tuning is performed on the training images to recover information lost during clustering, using Min-Max quantization (e.g., 8-bit for most, 16-bit float for position) and accumulating gradients for codebook updates. Third, entropy encoding is used, where 3D Gaussians are linearized along a Z-order (Morton) curve to exploit spatial coherence, followed by DEFLATE (LZ77 and Huffman coding) and run-length encoding. For novel view rendering, a hardware rasterization-based approach is used. It includes a compute pre-pass to discard out-of-frustum Gaussians and compute view-dependent colors, followed by GPU-based depth-sorting (Onesweep algorithm). Finally, Gaussians are rendered as planar quads (splats) using vertex and pixel shaders for screen-space projection, exponential color/opacity falloff, and blending into the framebuffer.",
        "experimental_setup": "The method was evaluated on multiple benchmark datasets including Mip-Nerf360, Tanks&Temples, Deep Blending, and NeRF-Synthetic. Quantitative comparisons were made against the original 3D Gaussian Splatting method using metrics such as PSNR, SSIM, and LPIPS for image quality, and compression ratio (size reduction in MB). Rendering performance was measured in frames per second (FPS) on various GPUs, including NVIDIA RTX A5000, NVIDIA RTX 3070M, Intel UHD Graphics, and AMD Radeon R9 380, at 1080p resolution. Implementation details include a decay factor of 0.8 for batched clustering, 800/100 update steps for Gaussians/SH coefficients, batch sizes of 2^18/2^20 for color/Gaussian parameters, a default codebook size of 4096, and sensitivity thresholds βc = 6·10−7 and βg = 3·10−6. Quantization-aware fine-tuning involved 5000 optimization steps. The renderer was implemented using the WebGPU graphics API in Rust. An ablation study was conducted to analyze the contribution of individual compression pipeline stages and hyper-parameters.",
        "limitations": "The main limitation identified is the current inability to aggressively compress the Gaussians' positions in 3D space without introducing significant errors in the rendering process. Experiments with quantizing positions to a lattice structure or embedding positional constraints into the Gaussian splatting training process did not yield further compression without considerable quality degradation.",
        "future_research_directions": "Future research directions include exploring new approaches for reducing the memory footprint during the training phase of 3D Gaussian splatting. The authors also aim to investigate methods for additionally compressing positional information end-to-end. Furthermore, they plan to investigate advanced options for compressing and rendering optimized representations for volumetric scenes, as they believe 3D Gaussian splatting has potential for reconstructing such scenes.",
        "experimental_code": "def calc_importance(gaussians: GaussianModel, scene, pipeline_params) -> Tuple[torch.Tensor, torch.Tensor]:\n    scaling = gaussians.scaling_qa(gaussians.scaling_activation(gaussians._scaling.detach()))\n    cov3d = gaussians.covariance_activation(scaling, 1.0, gaussians.get_rotation.detach(), True).requires_grad_(True)\n    scaling_factor = gaussians.scaling_factor_activation(gaussians.scaling_factor_qa(gaussians._scaling_factor.detach()))\n    h1 = gaussians._features_dc.register_hook(lambda grad: grad.abs())\n    h2 = gaussians._features_rest.register_hook(lambda grad: grad.abs())\n    h3 = cov3d.register_hook(lambda grad: grad.abs())\n    background = torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32, device=\"cuda\")\n    gaussians._features_dc.grad = None\n    gaussians._features_rest.grad = None\n    num_pixels = 0\n    for camera in tqdm(scene.getTrainCameras(), desc=\"Calculating sensitivity\"):\n        cov3d_scaled = cov3d * scaling_factor.square()\n        rendering = render(camera, gaussians, pipeline_params, background, clamp_color=False, cov3d=cov3d_scaled)[\"render\"]\n        loss = rendering.sum()\n        loss.backward()\n        num_pixels += rendering.shape[1]*rendering.shape[2]\n    importance = torch.cat([gaussians._features_dc.grad, gaussians._features_rest.grad], 1,).flatten(-2)/num_pixels\n    cov_grad = cov3d.grad/num_pixels\n    h1.remove()\n    h2.remove()\n    h3.remove()\n    torch.cuda.empty_cache()\n    return importance.detach(), cov_grad.detach()\n\nclass VectorQuantize(nn.Module):\n    def __init__(self, channels: int, codebook_size: int = 2**12, decay: float = 0.5) -> None:\n        super().__init__()\n        self.decay = decay\n        self.codebook = nn.Parameter(torch.empty(codebook_size, channels), requires_grad=False)\n        nn.init.kaiming_uniform_(self.codebook)\n        self.entry_importance = nn.Parameter(torch.zeros(codebook_size), requires_grad=False)\n        self.eps = 1e-5\n    def uniform_init(self, x: torch.Tensor):\n        amin, amax = x.aminmax()\n        self.codebook.data = torch.rand_like(self.codebook) * (amax - amin) + amin\n    def update(self, x: torch.Tensor, importance: torch.Tensor) -> torch.Tensor:\n        with torch.no_grad():\n            min_dists, idx = weightedDistance(x.detach(), self.codebook.detach())\n            acc_importance = scatter(importance, idx, 0, reduce=\"sum\", dim_size=self.codebook.shape[0])\n            ema_inplace(self.entry_importance, acc_importance, self.decay)\n            codebook = scatter(x * importance[:, None], idx, 0, reduce=\"sum\", dim_size=self.codebook.shape[0])\n            ema_inplace(self.codebook, codebook / (acc_importance[:, None] + self.eps), self.decay)\n            return min_dists\n    def forward(self, x: torch.Tensor, return_dists: bool = False) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        min_dists, idx = weightedDistance(x.detach(), self.codebook.detach())\n        if return_dists:\n            return self.codebook[idx], idx, min_dists\n        else:\n            return self.codebook[idx], idx\n\ndef vq_features(features: torch.Tensor, importance: torch.Tensor, codebook_size: int, vq_chunk: int = 2**16, steps: int = 1000, decay: float = 0.8, scale_normalize: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n    importance_n = importance/importance.max()\n    vq_model = VectorQuantize(channels=features.shape[-1], codebook_size=codebook_size, decay=decay,).to(device=features.device)\n    vq_model.uniform_init(features)\n    errors = []\n    for i in trange(steps):\n        batch = torch.randint(low=0, high=features.shape[0], size=[vq_chunk])\n        vq_feature = features[batch]\n        error = vq_model.update(vq_feature, importance=importance_n[batch]).mean().item()\n        errors.append(error)\n        if scale_normalize:\n            tr = vq_model.codebook[:, [0, 3, 5]].sum(-1)\n            vq_model.codebook /= tr[:, None]\n    gc.collect()\n    torch.cuda.empty_cache()\n    start = time.time()\n    _, vq_indices = vq_model(features)\n    torch.cuda.synchronize(device=vq_indices.device)\n    end = time.time()\n    print(f\"calculating indices took {end-start} seconds \")\n    return vq_model.codebook.data.detach(), vq_indices.detach()\n\ndef compress_color(gaussians: GaussianModel, color_importance: torch.Tensor, color_comp: CompressionSettings, color_compress_non_dir: bool):\n    keep_mask = color_importance > color_comp.importance_include\n    print(f\"color keep: {keep_mask.float().mean()*100:.2f}%\")\n    vq_mask_c = ~keep_mask\n    if color_compress_non_dir:\n        n_sh_coefs = gaussians.get_features.shape[1]\n        color_features = gaussians.get_features.detach().flatten(-2)\n    else:\n        n_sh_coefs = gaussians.get_features.shape[1] - 1\n        color_features = gaussians.get_features[:, 1:].detach().flatten(-2)\n    if vq_mask_c.any():\n        print(\"compressing color...\")\n        color_codebook, color_vq_indices = vq_features(color_features[vq_mask_c], color_importance[vq_mask_c], color_comp.codebook_size, color_comp.batch_size, color_comp.steps,)\n    else:\n        color_codebook = torch.empty((0, color_features.shape[-1]), device=color_features.device)\n        color_vq_indices = torch.empty((0,), device=color_features.device, dtype=torch.long)\n    all_features = color_features\n    compressed_features, indices = join_features(all_features, keep_mask, color_codebook, color_vq_indices)\n    gaussians.set_color_indexed(compressed_features.reshape(-1, n_sh_coefs, 3), indices)\n\ndef compress_covariance(gaussians: GaussianModel, gaussian_importance: torch.Tensor, gaussian_comp: CompressionSettings):\n    keep_mask_g = gaussian_importance > gaussian_comp.importance_include\n    vq_mask_g = ~keep_mask_g\n    print(f\"gaussians keep: {keep_mask_g.float().mean()*100:.2f}%\")\n    covariance = gaussians.get_normalized_covariance(strip_sym=True).detach()\n    if vq_mask_g.any():\n        print(\"compressing gaussian splats...\")\n        cov_codebook, cov_vq_indices = vq_features(covariance[vq_mask_g], gaussian_importance[vq_mask_g], gaussian_comp.codebook_size, gaussian_comp.batch_size, gaussian_comp.steps, scale_normalize=True,)\n    else:\n        cov_codebook = torch.empty((0, covariance.shape[1], 1), device=covariance.device)\n        cov_vq_indices = torch.empty((0,), device=covariance.device, dtype=torch.long)\n    compressed_cov, cov_indices = join_features(covariance, keep_mask_g, cov_codebook, cov_vq_indices,)\n    rot_vq, scale_vq = extract_rot_scale(to_full_cov(compressed_cov))\n    gaussians.set_gaussian_indexed(rot_vq.to(compressed_cov.device), scale_vq.to(compressed_cov.device), cov_indices,)\n\ndef compress_gaussians(gaussians: GaussianModel, color_importance: torch.Tensor, gaussian_importance: torch.Tensor, color_comp: Optional[CompressionSettings], gaussian_comp: Optional[CompressionSettings], color_compress_non_dir: bool, prune_threshold:float=0.,):\n    with torch.no_grad():\n        if prune_threshold >= 0:\n            non_prune_mask = color_importance > prune_threshold\n            print(f\"prune: {(1-non_prune_mask.float().mean())*100:.2f}%\")\n            gaussians.mask_splats(non_prune_mask)\n            gaussian_importance = gaussian_importance[non_prune_mask]\n            color_importance = color_importance[non_prune_mask]\n        if color_comp is not None:\n            compress_color(gaussians, color_importance, color_comp, color_compress_non_dir,)\n        if gaussian_comp is not None:\n            compress_covariance(gaussians, gaussian_importance, gaussian_comp,)\n\ndef finetune(scene: Scene, dataset, opt, comp, pipe, testing_iterations, debug_from):\n    prepare_output_and_logger(comp.output_vq, dataset)\n    first_iter = scene.loaded_iter\n    max_iter = first_iter + comp.finetune_iterations\n    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]\n    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n    iter_start = torch.cuda.Event(enable_timing=True)\n    iter_end = torch.cuda.Event(enable_timing=True)\n    scene.gaussians.training_setup(opt)\n    scene.gaussians.update_learning_rate(first_iter)\n    viewpoint_stack = None\n    ema_loss_for_log = 0.0\n    progress_bar = tqdm(range(first_iter, max_iter), desc=\"Training progress\")\n    first_iter += 1\n    for iteration in range(first_iter, max_iter + 1):\n        iter_start.record()\n        if not viewpoint_stack:\n            viewpoint_stack = scene.getTrainCameras().copy()\n        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack) - 1))\n        if (iteration - 1) == debug_from:\n            pipe.debug = True\n        render_pkg = render(viewpoint_cam, scene.gaussians, pipe, background)\n        image, viewspace_point_tensor, visibility_filter, radii = (\n            render_pkg[\"render\"],\n            render_pkg[\"viewspace_points\"],\n            render_pkg[\"visibility_filter\"],\n            render_pkg[\"radii\"],\n        )\n        gt_image = viewpoint_cam.original_image.cuda()\n        Ll1 = l1_loss(image, gt_image)\n        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))\n        loss.backward()\n        iter_end.record()\n        scene.gaussians.update_learning_rate(iteration)\n        with torch.no_grad():\n            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n            if iteration % 10 == 0:\n                progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{7}f}\"})\n                progress_bar.update(10)\n            if iteration == max_iter:\n                progress_bar.close()\n            if iteration < max_iter:\n                scene.gaussians.optimizer.step()\n                scene.gaussians.optimizer.zero_grad()\n\nclass GaussianModel:\n    # ... (rest of the class methods omitted for brevity)\n    def __init__(self, sh_degree: int, quantization=True):\n        # ... (initialization of parameters)\n        self._feature_indices = None\n        self._gaussian_indices = None\n        self.quantization = quantization\n        self.color_index_mode = ColorMode.NOT_INDEXED\n        self.features_dc_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.features_rest_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.opacity_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.scaling_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.scaling_factor_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.rotation_qa = torch.ao.quantization.FakeQuantize(dtype=torch.qint8).cuda()\n        self.xyz_qa = FakeQuantizationHalf.apply\n        if not self.quantization:\n            self.features_dc_qa.disable_fake_quant()\n            self.features_dc_qa.disable_observer()\n            self.features_rest_qa.disable_fake_quant()\n            self.features_rest_qa.disable_observer()\n            self.scaling_qa.disable_fake_quant()\n            self.scaling_qa.disable_observer()\n            self.scaling_factor_qa.disable_fake_quant()\n            self.scaling_factor_qa.disable_observer()\n            self.rotation_qa.disable_fake_quant()\n            self.rotation_qa.disable_observer()\n            self.xyz_qa = lambda x: x\n        self.setup_functions()\n\n    @property\n    def get_xyz(self):\n        return self.xyz_qa(self._xyz)\n\n    @property\n    def get_features(self):\n        features_dc = self.features_dc_qa(self._features_dc)\n        features_rest = self.features_rest_qa(self._features_rest)\n        if self.color_index_mode == ColorMode.ALL_INDEXED:\n            return torch.cat((features_dc, features_rest), dim=1)[self._feature_indices]\n        else:\n            return torch.cat((features_dc, features_rest), dim=1)\n    \n    @property\n    def get_scaling(self):\n        scaling_n = self.scaling_qa(self.scaling_activation(self._scaling))\n        scaling_factor = self.scaling_factor_activation(self.scaling_factor_qa(self._scaling_factor))\n        if self.is_gaussian_indexed:\n            return scaling_factor * scaling_n[self._gaussian_indices]\n        else:\n            return scaling_factor * scaling_n\n\n    @property\n    def get_rotation(self):\n        rotation = self.rotation_activation(self.rotation_qa(self._rotation))\n        if self.is_gaussian_indexed:\n            return rotation[self._gaussian_indices]\n        else:\n            return rotation\n\n    @property\n    def get_opacity(self):\n        return self.opacity_qa(self.opacity_activation(self._opacity))\n\n    def save_npz(self, path, compress: bool = True, half_precision: bool = False, sort_morton=False):\n        with torch.no_grad():\n            if sort_morton:\n                self._sort_morton()\n            # ... (rest of saving logic with quantization)\n            save_dict = dict()\n            save_dict[\"quantization\"] = self.quantization\n            if self.quantization:\n                save_dict[\"xyz\"] = self.get_xyz.detach().half().cpu().numpy()\n            else:\n                save_dict[\"xyz\"] = self._xyz.detach().cpu().numpy()\n            # ... (saving of other quantized/non-quantized parameters)\n            if self.is_color_indexed:\n                save_dict[\"feature_indices\"] = (self._feature_indices.detach().contiguous().cpu().int().numpy())\n            if self.is_gaussian_indexed:\n                save_dict[\"gaussian_indices\"] = (self._gaussian_indices.detach().contiguous().cpu().int().numpy())\n            save_fn = np.savez_compressed if compress else np.savez\n            save_fn(path, **save_dict)\n\n    def _sort_morton(self):\n        with torch.no_grad():\n            xyz_q = ((2**21 - 1) * (self._xyz - self._xyz.min(0).values) / (self._xyz.max(0).values - self._xyz.min(0).values)).long()\n            order = mortonEncode(xyz_q).sort().indices\n            self._xyz = nn.Parameter(self._xyz[order], requires_grad=True)\n            self._opacity = nn.Parameter(self._opacity[order], requires_grad=True)\n            self._scaling_factor = nn.Parameter(self._scaling_factor[order], requires_grad=True)\n            if self.is_color_indexed:\n                self._feature_indices = nn.Parameter(self._feature_indices[order], requires_grad=False)\n            else:\n                self._features_rest = nn.Parameter(self._features_rest[order], requires_grad=True)\n                self._features_dc = nn.Parameter(self._features_dc[order], requires_grad=True)\n            if self.is_gaussian_indexed:\n                self._gaussian_indices = nn.Parameter(self._gaussian_indices[order], requires_grad=False)\n            else:\n                self._scaling = nn.Parameter(self._scaling[order], requires_grad=True)\n                self._rotation = nn.Parameter(self._rotation[order], requires_grad=True)\n\nclass FakeQuantizationHalf(torch.autograd.Function):\n    \"\"\"performs fake quantization for half precision\"\"\"\n    @staticmethod\n    def forward(_, x: torch.Tensor) -> torch.Tensor:\n        return x.half().float()\n    @staticmethod\n    def backward(_, grad_output: torch.Tensor) -> torch.Tensor:\n        return grad_output\n\ndef mortonEncode(pos: torch.Tensor) -> torch.Tensor:\n    x, y, z = pos.unbind(-1)\n    answer = torch.zeros(len(pos), dtype=torch.long, device=pos.device)\n    answer |= splitBy3(x) | splitBy3(y) << 1 | splitBy3(z) << 2\n    return answer\n\ndef render(viewpoint_camera, pc: GaussianModel, pipe, bg_color: torch.Tensor, scaling_modifier=1.0, override_color=None, clamp_color: bool = True, cov3d: torch.Tensor = None):\n    # ... (initial setup)\n    raster_settings = GaussianRasterizationSettings(\n        image_height=int(viewpoint_camera.image_height),\n        image_width=int(viewpoint_camera.image_width),\n        tanfovx=tanfovx,\n        tanfovy=tanfovy,\n        bg=bg_color,\n        scale_modifier=scaling_modifier,\n        viewmatrix=viewpoint_camera.world_view_transform,\n        projmatrix=viewpoint_camera.full_proj_transform,\n        sh_degree=pc.active_sh_degree,\n        campos=viewpoint_camera.camera_center,\n        prefiltered=False,\n        debug=pipe.debug,\n        clamp_color=clamp_color,\n    )\n    if pc.color_index_mode == ColorMode.ALL_INDEXED and pc.is_gaussian_indexed:\n        rasterizer = GaussianRasterizerIndexed(raster_settings=raster_settings)\n        means3D = pc.get_xyz\n        means2D = screenspace_points\n        opacity = pc.get_opacity\n        shs = pc._get_features_raw\n        scales = pc.get_scaling_normalized\n        scale_factors = pc.get_scaling_factor\n        rotations = pc._rotation_post_activation\n        rendered_image, radii = rasterizer(\n            means3D=means3D,\n            means2D=means2D,\n            shs=shs,\n            sh_indices=pc._feature_indices,\n            g_indices=pc._gaussian_indices,\n            colors_precomp=None,\n            opacities=opacity,\n            scales=scales,\n            scale_factors=scale_factors,\n            rotations=rotations,\n            cov3D_precomp=None,\n        )\n        return {\"render\": rendered_image, \"viewspace_points\": screenspace_points, \"visibility_filter\": radii > 0, \"radii\": radii,}\n    else:\n        # ... (original rasterization path)\n        pass\n\nclass GaussianRasterizerIndexed(nn.Module):\n    def __init__(self, raster_settings):\n        super().__init__()\n        self.raster_settings = raster_settings\n    def forward(self, means3D, means2D, opacities, sh_indices, g_indices, shs=None, colors_precomp=None, scales=None, scale_factors=None, rotations=None, cov3D_precomp=None,):\n        raster_settings = self.raster_settings\n        if (shs is None and colors_precomp is None) or (shs is not None and colors_precomp is not None):\n            raise Exception(\"Please provide excatly one of either SHs or precomputed colors!\")\n        if ((scales is None or rotations is None) and cov3D_precomp is None) or ((scales is not None or rotations is not None) and cov3D_precomp is not None):\n            raise Exception(\"Please provide exactly one of either scale/rotation pair or precomputed 3D covariance!\")\n        if shs is None: shs = torch.Tensor([])\n        if colors_precomp is None: colors_precomp = torch.Tensor([])\n        if scales is None: scales = torch.Tensor([])\n        if scale_factors is None: scale_factors = torch.Tensor([])\n        if rotations is None: rotations = torch.Tensor([])\n        if cov3D_precomp is None: cov3D_precomp = torch.Tensor([])\n        return rasterize_gaussians_indexed(\n            means3D, means2D, shs, sh_indices, g_indices, colors_precomp, opacities, scales, scale_factors, rotations, cov3D_precomp, raster_settings,\n        )",
        "experimental_info": "The methodology consists of a three-step compression pipeline.\n\n**Step 1: Sensitivity-aware vector clustering.**\nThis step compresses Gaussian appearance (SH coefficients) and shape (scale and rotation) parameters into compact codebooks. A sensitivity measure is defined based on the gradient of image energy to parameter changes, which is computed by the `calc_importance` function. Sensitivity-aware k-Means clustering is then applied using `VectorQuantize` and `vq_features`, with highly sensitive parameters being exempted based on importance thresholds (e.g., `color_importance_include`, `gaussian_importance_include`, `prune_threshold`). The actual compression into codebooks is performed by `compress_color` and `compress_covariance` functions, which are called by `compress_gaussians`.\n\nExperimental Settings for this step are defined in `CompressionParams`:\n- `color_codebook_size`: The size of the codebook for color parameters (default: 2**12).\n- `color_importance_include`: Threshold for including colors in codebook (default: 0.6*1e-6).\n- `color_importance_prune`: Pruning threshold for colors (default: 0.0).\n- `color_cluster_iterations`: Number of clustering iterations for colors (default: 100).\n- `color_decay`: Decay rate for EMA in clustering for colors (default: 0.8).\n- `color_batch_size`: Batch size for color clustering (default: 2**18).\n- `color_weights_per_param`: Whether to use separate weights per parameter for colors (default: False).\n- `color_compress_non_dir`: Whether to compress non-directional color components (default: True).\n- `not_compress_color`: Flag to disable color compression (default: False).\n- `gaussian_codebook_size`: The size of the codebook for Gaussian shape parameters (default: 2**12).\n- `gaussian_importance_include`: Threshold for including Gaussians in codebook (default: 0.3*1e-5).\n- `gaussian_cluster_iterations`: Number of clustering iterations for Gaussians (default: 800).\n- `gaussian_decay`: Decay rate for EMA in clustering for Gaussians (default: 0.8).\n- `gaussian_batch_size`: Batch size for Gaussian clustering (default: 2**20).\n- `not_compress_gaussians`: Flag to disable Gaussian compression (default: False).\n- `prune_threshold`: General pruning threshold for splats (default: 0.).\n\n**Step 2: Quantization-aware fine-tuning.**\nThis step is performed on the training images to recover information lost during clustering. Min-Max quantization (e.g., 8-bit for most parameters, 16-bit float for position) is applied through `torch.ao.quantization.FakeQuantize` layers within the `GaussianModel` (e.g., `features_dc_qa`, `scaling_qa`, `xyz_qa = FakeQuantizationHalf.apply`). The `finetune` function orchestrates the training loop, accumulating gradients for codebook updates implicitly via the optimizer setup (`scene.gaussians.training_setup(opt)`).\n\nExperimental Settings for this step include:\n- `finetune_iterations`: Number of iterations for quantization-aware fine-tuning (default: 5000).\n- `not_quantization_aware`: Flag to disable quantization-aware training (default: False).\n\n**Step 3: Entropy encoding.**\n3D Gaussians are linearized along a Z-order (Morton) curve to exploit spatial coherence, implemented by `_sort_morton` and `mortonEncode` functions in `GaussianModel`. This sorting is an option during the model saving process in `gaussians.save_npz`. After linearization, DEFLATE (LZ77 and Huffman coding) and run-length encoding are applied, which is implicitly handled by the `.npz` compression format and the overall pipeline's focus on efficient representation.\n\nExperimental Settings for this step include:\n- `not_sort_morton`: Flag to disable Morton sorting during saving (default: False).\n\n**Step 4: Novel view rendering.**\nNovel view rendering uses a hardware rasterization-based approach. The `render` function in `gaussian_renderer/__init__.py`, specifically when `pc.color_index_mode == ColorMode.ALL_INDEXED and pc.is_gaussian_indexed`, utilizes the `GaussianRasterizerIndexed` for rendering compressed Gaussian representations. This rasterizer handles the compute pre-pass to discard out-of-frustum Gaussians, compute view-dependent colors, GPU-based depth-sorting (implicitly part of the `diff-gaussian-rasterization` submodule), and rendering as planar quads (splats) using vertex and pixel shaders for screen-space projection, exponential color/opacity falloff, and blending into the framebuffer."
      }
    },
    {
      "title": "A Statistical Manifold Framework for Point Cloud Data"
    },
    {
      "title": "OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries",
      "abstract": "Occupancy prediction has increasingly garnered attention in recent years for\nits fine-grained understanding of 3D scenes. Traditional approaches typically\nrely on dense, regular grid representations, which often leads to excessive\ncomputational demands and a loss of spatial details for small objects. This\npaper introduces OctreeOcc, an innovative 3D occupancy prediction framework\nthat leverages the octree representation to adaptively capture valuable\ninformation in 3D, offering variable granularity to accommodate object shapes\nand semantic regions of varying sizes and complexities. In particular, we\nincorporate image semantic information to improve the accuracy of initial\noctree structures and design an effective rectification mechanism to refine the\noctree structure iteratively. Our extensive evaluations show that OctreeOcc not\nonly surpasses state-of-the-art methods in occupancy prediction, but also\nachieves a 15%-24% reduction in computational overhead compared to\ndense-grid-based methods.",
      "full_text": "OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries Yuhang Lu1, Xinge Zhu2, Tai Wang3,†, and Yuexin Ma1,† 1 ShanghaiTech University 2 The Chinese University of Hong Kong 3 Shanghai AI Laboratory {luyh2,mayuexin}@shanghaitech.edu.cn, taiwang.me@gmail.com Abstract. Occupancy prediction has increasingly garnered attention in recent years for its fine-grained understanding of 3D scenes. Traditional approaches typically rely on dense, regular grid representations, which often leads to excessive computational demands and a loss of spatial details for small objects. This paper introduces OctreeOcc, an innova- tive 3D occupancy prediction framework that leverages the octree rep- resentation to adaptively capture valuable information in 3D, offering variable granularity to accommodate object shapes and semantic regions of varying sizes and complexities. In particular, we incorporate image semantic information to improve the accuracy of initial octree struc- tures and design an effective rectification mechanism to refine the octree structure iteratively. Our extensive evaluations show that OctreeOcc not only surpasses state-of-the-art methods in occupancy prediction, but also achieves a15% − 24% reduction in computational overhead compared to dense-grid-based methods. Keywords: 3D Scene Understanding· Occupancy Prediction· Octree 1 Introduction Holistic 3D scene understanding is a pivotal aspect of a stable and reliable visual perception system, especially for real-world applications such as autonomous driving. Occupancy, as a classical representation, has been renascent recently with more datasets support and exploration in learning-based approaches. Such occupancy prediction tasks aim at partitioning the 3D scene into grid cells and predicting semantic labels for each voxel. It is particularly an essential solution for recognizing irregularly shaped objects and also enables the open-set under- standing [40], further benefiting downstream tasks, like prediction and planning. Existing occupancy prediction methods [9,12,22,51,56] typically construct dense and regular grid representations, same as the ground truth. While such approach is intuitive and direct, it overlooks the statistical and geometric proper- ties of 3D environments. In fact, the 3D scene is composed of foreground objects † Corresponding authors arXiv:2312.03774v3  [cs.CV]  19 Mar 20242 Y. Lu et al. Barrier Bicycle  Bus  Car  Construction Vehicle  Motorcycle Pedestrian Traffic Cone Trailer Truck (a) Instance Size Inbalance  (b) 3D and 2D Visualization of Octree Structures Fig.1: Scale difference of various categories and octree representation. (a) presents a comparison of the average space occupied by each type of object, indicating different granularities are required for different semantic regions. (b) shows the superiority of octree representations, where we can apply specific granularity not only for different objects but also for different parts of the object, which can reduce computational overhead while preserving spatial information. and background regions with various shapes and sizes. For example, the space occupied by larger objects, such as buses, are considerably more extensive than that taken up by smaller items like traffic cones (Fig. 1a). Consequently, em- ploying a uniform voxel resolution to depict the scene proves to be inefficient, leading to computational waste for larger objects and a lack of geometry de- tails for smaller ones. Considering the large computation cost of aforementioned works, some recent works attempted to mitigate the heavy memory footprint by utilizing other representations, such as 2D planes in TPVFormer [11] and coarse voxels in PanoOcc [50], or modeling non-empty regions by depth estimation [18]. However, these methods suffer from the loss of spatial information because of too coarse representations or accumulated estimated depth errors. To reduce the computational overhead and meanwhile improve the predic- tion accuracy, we propose to use octree [29] representation for the occupancy prediction, which can flexibly adapt to objects and semantic regions with vari- ous shapes and sizes. As a tree-based data structure, it recursively divides the 3D space into eight octants, thus allowing coarse spatial partition for large regions and fine-grained processing for small objects or complex details (Fig. 1b). In- corporating octree representation, we proposeOctreeOcc, an efficient and multi- granularity method for occupancy prediction. It constructs octree queries by pre- dicting the octree structure from the stored features of leaf nodes at each level of the tree. However, directly predicting 3D structure from 2D images is chal- lenging due to the lack of depth and occlusion issues. To address this problem, we first propose Semantic-guided Octree Initializationthat incorporates image semantic information to produce more accurate initial structures. And then, we devise anIterative Structure Rectificationmodule that predicts new octree structure from the encoded query to rectify the low-confidence region of the original prediction, further improving the prediction precision. Ourextensiveevaluationsagainststate-of-the-artoccupancypredictionmeth- ods show thatOctreeOcc outperforms others on nuScenes and SemanticKITTIOctreeOcc 3 datasets, reducing computational overhead by15% − 24% for dense-grid-based methods. Ablation studies further validate the effectiveness of each module within our method. Our contributions can be summarized as follows: • We introduce OctreeOcc, a 3D occupancy prediction approach based on multi-granularity octree queries. This method facilitates spatial sparsifica- tion, significantly decreasing the number of voxels needed to accurately de- pict a scene, yet retains critical spatial details. • We propose a semantic-guided octree initialization module and and iterative structure rectification module. These enhancements empower the network with a robust initial setup and the ability to dynamically refine the octree, leading to a more efficient and effective representation. • Comprehensive experiments demonstrate that OctreeOcc achieves state-of- the-art performance and reduces computational overhead, highlighting the feasibility and potential of octree structures in 3D occupancy prediction. 2 Related Work 2.1 Camera-based 3D Perception Camera-based 3D perception has gained significant traction in recent years due to its ease of deployment, cost-effectiveness, and the preservation of in- tricate visual attributes. According to the view transformation paradigm, these methods can be categorized into three distinct types. LSS-based approaches [9,19,20,27,33,34] explicitly lift multi-view image features into 3D space through depth prediction. Another category of works [13,21,53] implicitly derives depth information by querying from 3D to 2D. Notably, projection-free methods [24– 26,48]haverecentlydemonstratedexceptionalperformance.Whilecommendable progress has been made in detection, this approach compromises the comprehen- sive representation of the overall scene in 3D space and proves less effective in recognizing irregularly shaped objects. Consequently, there is a burgeoning in- terest in methodologies aimed at acquiring a dense voxel representation through the camera, facilitating a more comprehensive understanding of 3D space. 2.2 3D Occupancy Prediction 3D occupancy prediction involves the prediction of both occupancy and seman- tic attributes for all voxels encompassed within a three-dimensional scene, par- ticularly valuable for autonomous vehicular navigation. Recently, some valu- able datasets [39,43,49] have been proposed, boosting more and more research works [3,10–12,18,28,30–32,38,40,50,51,54–56] in this field. Most of the research in occupancy prediction focuses on dense voxel modeling. MonoScene [3] pioneers a camera-based approach using a 3D UNet architecture. OccDepth [30] improves 2D-to-3D geometric projection using stereo-depth information. OccFormer [56] decomposes the 3D processing into the local and global transformer pathways along the horizontal plane. SurroundOcc [51] achieves fine-grained results with4 Y. Lu et al. multiscale supervision. Symphonies [12] introduces instance queries to enhance scene representation. Nevertheless, owing to the high resolution of regular voxel representation and sparse context distribution in 3D scenes, these methods encounter substantial computational overhead and efficiency issues. Some approaches recognize this problem and attempt to address it by reducing the number of modeled voxels. For instance, TPVFormer [11] employs a strategy of modeling the three-view 2D planes and subsequently recovering 3D spatial information from them. How- ever, its performance degrades due to the lack of 3D information. PanoOcc [50] initially represents scenes at the coarse-grained level and then upsamples them to the fine-grained level, but the lack of information from coarse-grained model- ing cannot be adequately addressed by the up-sampling process. VoxFormer [18] mitigates computational complexity by initially identifying non-empty regions through depth estimation and modeling only those specific areas. However, the effectiveness of this process is heavily contingent on the accuracy of depth esti- mation. In contrast, our approach provides different granularity of modeling for different regions by predicting the octree structure, which reduces the number of voxels to be modeled while preserving the spatial information, thereby reducing the computational overhead and maintaining the prediction accuracy. 2.3 Octree-Based 3D Representation The octree structure [29] has been widely used in the field of computer graph- ics to boost the process of rendering or reconstruction [7,15,41,46]. Due to its spatial efficiency and ease of implementation on GPU architectures, researchers have also applied octrees in efficient point cloud learning and various related works [16,42,45,47]. OctFormer [44] and OcTr [57] utilize multi-granularity fea- tures of octree to capture a comprehensive global context, thereby enhancing the efficiency of understanding point clouds at the scene level. Furthermore, cer- tain studies [6,35] adopt octree representation for effectively compressing point cloud data. These works have highlighted the versatility and effectiveness of octree-based methodologies in point cloud analysis and processing applications. However, unlike constructing an octree from 3D point clouds, we are the first to predict the octree structure of a 3D scene from images, which is more challenging owing to the absence of explicit spatial information inherent in 2D images. 3 Methodology In this section, we introduce details of our efficient and multi-granularity occu- pancy prediction methodOctreeOcc. After defining the problem and providing an overview of our method in Sec. 3.1 and 3.2, we introduce key components of our method in order. In Sec. 3.3, we outline how we define octree queries and transform dense queries into octree queries. Next, we utilize image semantic pri- ors to construct a high-quality initialized octree structure, as detailed in Sec. 3.4. Once the initialized octree query is obtained, we encode it and refine the octreeOctreeOcc 5 Multi-view Images  Backbone Img Features … V oxel Query Octree Mask Semantic-Guided Octree  Initialization : 3D to 2D  projection : parent query : leaf query Octree Encoder Temporal  Self  Attention Image Cross  Attention Occupancy Prediction Iterative  Structure  Rectification  Octree Decoder Historical Octree Query Current Octree Query Octree Query Fig.2: Overall framework of OctreeOcc. Given multi-view images, we extract multi-scale image features utilizing an image backbone. Subsequently, the initial octree structure is derived through image segmentation priors, and the transformation of dense queries into octree queries is effected. Following this, we concomitantly refine octree queries and rectify the octree structure through the octree encoder. Finally, we decode from the octree query and obtain occupancy prediction outcomes for this frame. For better visualisation, the diagram of Iterative Structure Rectification module shows octree query and mask in 2D form(quadtree). structure in Sec. 3.5. Finally, Sec. 3.6 and 3.7 describe the process of generating octree ground truth and how to supervise the network, respectively. 3.1 Problem Setup Camera-based occupancy prediction aims to predict the present occupancy state and semantics of each voxel grid within the scene using input from multi-view camera images. Specifically, we consider a set of N multi-view images I = {Ii ∈ RH×W×3}N i=1, together with camera intrinsicsK = {Ki ∈ R3×3}N i=1 and extrinsics T = {Ti ∈ R4×4}N i=1 as input, and the objective of the model is to predict the 3D semantic voxel volumeO ∈ {w0, w1, ..., wC}X×Y ×Z, whereH, W indicate the resolution of input image andX, Y , Z denote the volume resolu- tion (e.g. 200×200×16). The primary focus lies in accurately distinguishing the empty class (w0) and other semantic classes (w1 ∼ wC) for every position in the 3D space, which entails the network learning both the geometric and semantic information inherent in the data. 3.2 Overview Given a set of multi-view imagesI = {Ii ∈ RH×W×3}N i=1, we extract multi- view image featuresF = {Fi ∈ RH×W×C}N i=1. Simultaneously, we initialize the dense voxel queryQdense ∈ RX×Y ×Z×C. To enhance computational efficiency,6 Y. Lu et al. we transformQdense into sparse representationQoctree ∈ RN×C, leveraging oc- tree structure information (i.e. octree mask) derived from segmentation priors. During encoding, we utilizeQoctree to gather information, including temporal fusion and sampling from image featuresF, while also rectifying the octree struc- ture. Upon encodingQoctree, to conform to the output format, we convert it back to Qdense and apply a Multi-Layer Perceptron (MLP) to obtain the final occu- pancy predictionO ∈ RX×Y ×Z×K. Here,H, W indicate the resolution of input image andX, Y , Z denote the volume resolution.N means the number of octree query ,C denotes the feature dimension andK indicates the number of classes. 3.3 Octree Query Given that objects within 3D scenes exhibit diverse granularities, employing dense queries [51,56] overlooks this variation and leads to inefficiency. To ad- dress this, we propose sparse and multi-granularity octree queries, leveraging the octree structure. This approach creates adaptable voxel representations tailored to semantic regions at different scales. Octree Mask. To effectively construct the octree query, it’s essential to un- derstand its underlying structure. An octree partitions each node into eight child nodes within 3D space, each representing equal subdivisions of the par- ent node. This recursive process begins with the initial level and proceeds with gradual splitting. At every level, a voxel query serves either as a leaf query if it remains unsplit or as a parent query if it undergoes division. We obtain this geometric information by maintaining a learnable octree mask, denoting as Mo = {Ml o ∈ R X 2l , Y 2l , Z 2l }L−1 l=1 , whereX, Y , Z denote the ground truth’s volume resolution. TheL denotes the depth of the octree, representing the existence of L different resolutions of queries, withL − 1 splits being performed from the top to the bottom of the octree. The value in the octree mask represents the probability that a voxel at that level requires a split, which is initialized through segmentation priors (Sec. 3.4), rectified during query encoding (Sec. 3.5), and supervised by octree ground truth (Sec. 3.7). Transformation between octree query and dense voxel query.During the query encoding process, we prioritize efficiency by leveraging octree queries. This involves transforming the initial dense queriesQdense into octree queries Qoctree using learned structural information. To determine the octree structure, we need to binarise the learned octree mask. Due to the fact that most of the voxels in the scene at various resolutions do not necessitate splitting, neural network-based prediction binarization is susceptible to pattern collapse, tending to predict all as non-split, leading to a decrease in performance. To mitigate this issue, we introduce a manually defined query selection ratio, where a subset of voxels with the highest probability of splitting is selected for division through the top-k mechanism. The transformation fromQdense toQoctree begins at the finest granularity, we downsampleQdense to each level through average pooling and retain queries that do not require splitting (leaf queries) with the assistance of the binarized octreeOctreeOcc 7 mask. This process iterates until reaching the top of the octree. By retaining all leaf queries, we establish Qoctree ∈ RN×C, where N = N1 + N2 + . . .+ NL represents the total count of leaf queries, L indicates the depth of octree. Conversely, applying the inverse operation of this process allows the conversion of Qoctree back intoQdense for the final output. Further details are provided in Supplementary. 3.4 Semantic-Guided Octree Initialization Predicting octree structure from an initialised query via neural network can yield sub-optimal results due to the inherent lack of meaningful information in the query. To overcome this limitation, we employ the semantic priors inherent in images as crucial references. Specifically, we adopt UNet [37] to segment the in- put multi-view imagesI and obtain the segment mapIseg = {Ii seg ∈ RH×W }N i=1. We then generate sampling pointsp = {pi ∈ R3}X×Y ×Z i=1 , with each point cor- responding to the center coordinates of dense voxel queries. Subsequently, we project these points onto various image views. The projection from sampling point pi = (xi, yi, zi) to its corresponding 2D reference point(uij, vij) on the j-th image view is formulated as: πj(pi) = (uij, vij), (1) where πj(pi) denotes the projection of the i-th sampling point at locationpi on the j-th camera view. We project the pointpi onto the acquired semantic segmentation map Iseg through the described projection process. To prioritize effective areas like foreground objects and buildings in the initial structure, we employ anunbalancedassignmentapproach,where thehighestweightis assigned to voxels projecting to foreground areas, followed by lesser weights for voxels projecting to buildings or vegetation, and the least weight for voxels projecting to roads, etc. During this process, we determine the weights of each voxel at the finest granularity, denoted asW ∈ RX×Y ×Z. Subsequently, we employ average pooling to downsampleW to each level of the octree, resulting in an initial octree mask Mo = {Ml o ∈ R X 2l , Y 2l , Z 2l }L−1 l=1 . Here,X, Y , andZ represent the resolution of the ground truth volume, whileL denotes the depth of the octree. Further details are provided in the Supplementary. 3.5 Octree Encoder Given octree queriesQoctree and extracted image featuresF, the octree encoder updates both the octree query features and the octree structure. Referring to the querying paradigm in dense query-based methods [39, 50], we adopt ef- ficient deformable attention [59] for temporal self-attention(TSA) and image cross-attention(ICA). In accurately representing the driving scene, temporal information plays a crucial role. By leveraging historical octree queries Qt−1, we align it to the8 Y. Lu et al. Fig.3: Illustration of octree structure rectification.The left figure displays the initially predicted octree structure and the right figure depicts the octree structure after Iterative Structure Rectification. It is evident that the predicted octree structure becomes more consistent with the object’s shape following the rectification module. current octree queries by the ego-vehicle motion transformation. Given historal octree queries Qt−1 ∈ RN,C , a current octree queryq located at p = (x, y, z), the TSA is represented by: T SA(q, Qt−1) = M1X m=1 DeformAttn (q, p, Qt−1), (2) where M1 indicates the number of sampling points. Implementing it within the voxel-based self-attention ensures that each octree query interacts exclusively with local voxels of interest, keeping the computational cost manageable. Image cross-attention is devised to enhance the interaction between multi- scale image features and octree queries. Specifically, for an octree queryq, we can obtain its centre’s 3D coordinate(x, y, z) as reference pointRefx,y,z. Then we project the 3D point to images like formula 1 and perform deformable attention: ICA (q, F) = 1 N X n∈N M2X m=1 DeformAttn (q, πn(Refx,y,z), Fn), (3) where N denotes the camera view,m indexes the reference points , andM2 is the total number of sampling points for each query.Fn is the image features of the n-th camera view. Iterative Structure Rectification Module.The initial octree structure is derived from information obtained through image segmentation, but it may not precisely correspond to the actual scene due to potential segmentation and pro- jection errors. Nonetheless, the encoded octree query captures crucial spatial in- formation. Thus, the predicted octree structure based on this query complements and rectifies the initial structure prediction, allowing us to mitigate limitations arising from segmentation and projection issues. Specifically,wepartitiontheoctreestructureintotwoparts:thehigh-confidence regions and the low-confidence regions, as Fig. 2 shows. By sorting the octree split probability values stored in the octree mask in descending order and se- lecting the top k% of regions at each level, we can identify the locations ofOctreeOcc 9 high-confidence regions. For these regions, predictions are relatively more accu- rate and no additional adjustments are required in this iteration. For regions where confidence remains low, we first extract the query features correspond- ing to those areas by utilizing the index of low-confidence regions, denoted as Qlcr = {Ql lcr ∈ RNl×C}L−1 l=1 , whereNl represents the number of low-confidence queries in levell. We then employ a Multi-layer Perceptron (MLP) to predict octree split probabilities fromQlcr. Subsequently, we apply a weighted sum with the previous split probability predictions of low-confidence regions to obtain rec- tified predictions. These refined predictions are concatenated with the preserved probability predictions of high-confidence regions, culminating in the generation of the final rectified octree mask. More details are shown in Supplementary. It is worth noting that, due to the iterative nature of structure updates, re- gions initially considered high confidence may not necessarily remain unchanged, as they might be partitioned into low-confidence regions in the next iteration. 3.6 Octree Ground Truth Generation We derive the octree ground truth from the semantic occupancy ground truth. Specifically, for a voxel at levell in the octree, we identify its corresponding 8L−l voxels in the semantic occupancy ground truth. If these voxels share the same labels, we deem the voxel at levell unnecessary to split (assigned a value of 0); otherwise, it necessitates division (assigned a value of 1), as the current resolution is insufficient to represent it adequately. Through this process, each voxel at each level is assigned a binary value of 0 or 1. Then we obtain the octree ground truthGoctree = {Gl octree ∈ R X 2l , Y 2l , Z 2l }L−1 l=1 . Here,L represents the depth of the octree, whileX, Y , andZ denote the volume resolution of the semantic occupancy ground truth.Goctree is employed to supervise the octree mask using focal loss, facilitating the network in learning the octree structure information. 3.7 Loss Function To train the model, we use focal lossLfocal , lovasz-softmax lossLls, dice loss Ldice, affinity loss Lgeo scal and Lsem scal from MonoScene [3]. In addition, we also use focal loss to supervise the octree prediction. The overall loss functionL = Lfocal + Lls + Ldice + Lgeo scal + Lsem scal + Loctree. 4 Experiments In this section, we commence by introducing the datasets (Sec. 4.1), evaluation metrics (Sec. 4.2), and implementation details (Sec. 4.3). Subsequently, we eval- uate our method on 3D occupancy prediction and semantic scene completion tasks (Sec. 4.4) to demonstrate its effectiveness. Additionally, we conduct ab- lation studies (Sec. 4.5) and provide more analysis (Sec. 4.6) of our method to validate its efficacy and superiority.10 Y. Lu et al. 4.1 Datasets Occ3D-nuScenes [43]constitutes a re-annotation of the nuScenes dataset [2], where precise ground truth labels for occupancy have been derived by aggregat- ing LiDAR scans and human annotations. These annotations adhere to the ego coordinate system framework, consisting of 700 instances for training and 150 for validation. The spatial extents of occupancy along the X and Y axes in ego- coordinates are limited to the range of -40 meters to 40 meters, while the Z-axis spans from -1 meter to 5.4 meters. The occupancy labels are categorized into 17 distinct classes, with each class representing a volumetric space of 0.4 meters in each dimension (0.4m × 0.4m × 0.4m). Additionally, the dataset includes semantic labeling for the 18th category, denoted as “free\", which represents the empty regions within a scene. Finally, Occ3D-nuScenes provides visibility masks for both LIDAR and camera modalities. SemanticKITTI [1] comprises 22 distinct outdoor driving scenarios, with a focus on areas located in the forward trajectory of the vehicle. Each sample in this dataset covers a spatial extent ranging from [0.0m, -25.6m, -2.0m, 51.2m, 25.6m, 4.4m], with a voxel granularity set at [0.2m, 0.2m, 0.2m]. The dataset consists of volumetric representations, specifically in the form of 256×256×32 voxel grids. These grids undergo meticulous annotation with 21 distinct semantic classes. The voxel data is derived through a rigorous post-processing procedure applied to Lidar scans. 4.2 Evaluation metrics Both3Doccupancypredictionandsemanticscenecompletionutilizeintersection- over-union (mIoU) over all classes as evaluation metrics, calculated as follows: mIoU = 1 C CX c=1 TPc TPc + FPc + FNc , (4) where TPc, FPc, andFNc correspond to the number of true positive, false pos- itive, and false negative predictions for classci, andC is the number of classes. 4.3 Implementation Details Based on previous research, we set the input image size to 900×1600 and employ ResNet101-DCN [5] as the image backbone. Multi-scale features are extracted from the Feature Pyramid Network (FPN) [23] with downsampling sizes of 1/8, 1/16, 1/32, and 1/64. The feature dimensionC is set to 256. The octree depth is 3, and the initial query resolution is 50×50×4. We choose query selection ratios of 20% and 60% for the two divisions. The octree encoder comprises three layers, each composed of Temporal Self-Attention (TSA), Image Cross-Attention (ICA), and Iterative Structure Rectification (ISR) modules. BothM1 and M2 are set to 4. In ISR, the top 10% predictions are considered high-confidence in level 1, and 30% in level 2. The loss weights are uniformly set to 1.0. For optimization,OctreeOcc 11 Table 1: 3D Occupancy prediction performanceon Occ3D-nuScenes dataset. “⋆” denotes training with the camera mask. Method ImageBackboneReferencemIoU others barrier bicycle bus car const. veh. motorcycle pedestrain traffic cone trailer truck drive. suf. other flat sidewalk terrain manmade vegetation MonoScene [3]ResNet101CVPR’226.061.75 7.23 4.26 4.93 9.38 5.67 3.98 3.01 5.90 4.45 7.17 14.91 6.32 7.92 7.43 1.01 7.65BEVDet [9]ResNet101arxiv’2111.732.09 15.29 0.0 4.18 12.97 1.35 0.0 0.43 0.13 6.59 6.66 52.72 19.04 26.45 21.78 14.51 15.26BEVFormer [21]ResNet101ECCV’2223.675.03 38.79 9.98 34.41 41.09 13.24 16.50 18.15 17.83 18.66 27.70 48.95 27.73 29.08 25.38 15.41 14.46BEVStereo [19]ResNet101AAAI’2324.515.73 38.41 7.88 38.70 41.20 17.56 17.33 14.69 10.31 16.84 29.62 54.08 28.92 32.68 26.54 18.74 17.49TPVFormer [11]ResNet101CVPR’2328.346.67 39.20 14.24 41.54 46.98 19.21 22.64 17.87 14.54 30.20 35.51 56.18 33.65 35.69 31.61 19.97 16.12OccFormer [56]ResNet101ICCV’2321.935.94 30.29 12.32 34.40 39.17 14.44 16.45 17.22 9.27 13.90 26.36 50.99 30.96 34.66 22.73 6.76 6.97CTF-Occ [43]ResNet101NeurIPS’2328.538.09 39.33 20.56 38.29 42.24 16.93 24.52 22.72 21.05 22.98 31.11 53.33 33.84 37.98 33.23 20.79 18.00RenderOcc [32]ResNet101ICRA’2426.114.84 31.72 10.72 27.67 26.45 13.87 18.2 17.67 17.84 21.19 23.25 63.20 36.42 46.21 44.26 19.58 20.72BEVDet4D [8]⋆ Swin-Barxiv’2242.0212.1549.63 25.1 52.02 54.46 27.87 27.99 28.94 27.23 36.43 42.22 82.31 43.29 54.46 57.9 48.61 43.55PanoOcc [50]⋆ ResNet101CVPR’2442.1311.67 50.48 29.64 49.44 55.52 23.2933.2630.55 30.99 34.43 42.57 83.31 44.23 54.40 56.04 45.94 40.40FB-OCC [22]⋆ ResNet101ICCV’2343.4112.10 50.2332.3148.55 52.8931.2031.2530.78 32.3337.06 40.2283.34 49.27 57.13 59.8847.67 41.76 Ours⋆ ResNet101N/A44.0211.9651.7029.9353.52 56.7730.83 33.17 30.65 29.9937.76 43.8783.17 44.52 55.45 58.8649.52 46.33 Table 2: 3D Semantic Scene Completionperformance on SemanticKITTI dataset. Method ReferenceIoU mIoU road sidewalk parking other-ground building car truck bicycle motorcycle other-vehicle vegetation trunk terrain person bicyclist motorcyclist fence pole traf.-sign LMSCNet [36]3DV’2028.61 6.7040.68 18.22 4.38 0.00 10.31 18.33 0.00 0.00 0.00 0.00 13.66 0.02 20.54 0.00 0.00 0.00 1.21 0.00 0.00AICNet [17]CVPR’2029.59 8.3143.55 20.55 11.97 0.07 12.94 14.71 4.53 0.00 0.00 0.00 15.37 2.90 28.71 0.00 0.00 0.00 2.52 0.06 0.003DSketch [4]CVPR’2033.30 7.5041.32 21.63 0.00 0.00 14.81 18.59 0.00 0.00 0.00 0.00 19.09 0.00 26.40 0.00 0.00 0.00 0.73 0.00 0.00JS3C-Net [52]AAAI’2138.98 10.3150.49 23.74 11.94 0.07 15.03 24.65 4.41 0.00 0.00 6.15 18.11 4.33 26.86 0.67 0.27 0.00 3.94 3.77 1.45MonoScene [3]CVPR’2236.86 11.0856.52 26.72 14.27 0.46 14.09 23.26 6.98 0.61 0.45 1.48 17.89 2.81 29.64 1.86 1.20 0.00 5.84 4.14 2.25TPVFormer [11]CVPR’2335.61 11.3656.50 25.8720.60 0.8513.88 23.81 8.08 0.36 0.05 4.35 16.92 2.26 30.38 0.51 0.89 0.00 5.94 3.14 1.52VoxFormer [18]CVPR’2344.02 12.3554.76 26.35 15.50 0.70 17.65 25.79 5.63 0.59 0.51 3.77 24.395.0829.96 1.78 3.32 0.007.647.11 4.18OccFormer [56]ICCV’2336.5013.4658.84 26.8819.61 0.31 14.40 25.0925.530.81 1.19 8.52 19.63 3.9332.632.78 2.82 0.00 5.61 4.26 2.86Symphonies [12]CVPR’2441.44 13.4455.78 26.77 14.57 0.1918.7627.23 15.991.44 2.28 9.5224.50 4.32 28.493.19 8.090.00 6.188.99 5.39 Ours N/A44.7113.1255.13 26.74 18.68 0.65 18.6928.0716.43 0.64 0.71 6.0325.264.89 32.47 2.25 2.57 0.00 4.01 3.72 2.36 we employ Adam [14] optimizer with a learning rate of 2e-4 and weight decay of 0.01. The batch size is 8, and the model is trained for 24 epochs, consuming around 3 days on 8 NVIDIA A100 GPUs. 4.4 Results 3D Occupancy Prediction.In Tab. 1, we provide comparasion experiments with other state-of-the-art occupancy prediction methodds on Occ3d-nus vali- dation set. The performance of FBOCC [22] relies on open-source code, which we evaluate after ensuring consistency in details (utilizing the same backbone, image resolution, and excluding CBGS [58]) for a fair comparison. Performance for other methods are reported in a series of works [32,43,50]. Our approach demonstrates superior performance on mIoU compared to previous methods, particularly excelling in foreground classes such as barriers, cars, and buses, as well as in scene structure classes like manmade and vegetation. This highlights that processing scenes using multiple granularities aligns better with the scene characteristics, enhancing overall expressiveness. Moreover, we evaluate the efficiency of our approach by comparing it to alter- native methods utilizing diverse query forms, as depicted in Tab 3. The results indicate that our approach not only surpasses these methods in terms of per- formance but also demonstrates significantly reduced memory usage and lower latency compared to dense queries, approaching the levels observed with 2D queries. Clearly, sparse octree queries effectively mitigate computational over- head while ensuring robust performance. Fig. 4 displays qualitative results ob-12 Y. Lu et al. CAM_FRONT CAM_BACKCAM_FRONT_LEFT CAM_BACK_RIGHTCAM_BACK_LEFTCAM_FRONT_RIGHT PanoOcc FBOCC Ours GT Barrier Bicycle Bus Car Const. Veh. Motor Ped. Traf. C. Trailer Truck Drive. Surf. Flat Sidewalk Terrain Manmade Vegetation Fig.4: Qualitative results on Occ3D-nuScenes validation set, where the resolution of the voxel predictions is 200×200×16. Table 3: Comparison of query form and efficiencywith SOTA methods on the Occ3D-nuScenes (left table) and SemanticKITTI (right table) datasset. Methods Query FormmIoU Latency Memory BEVFormer [21]2D BEV 23.67 302ms 25100MTPVFormer [11]2D tri-plane28.34 341ms 29000MPanoOcc [50] 3D voxel 42.13 502ms 35000MFBOCC [22]3D voxels & 2D BEV43.41 463ms 31000M Ours Octree Query44.02386ms 26500M Methods Query FormIoU mIoU Latency Memory TPVFormer [11]2D tri-plane35.61 11.36 179ms 23000M OccFormer [56]3D voxel36.50 13.46 172ms 22400M VoxFormer [18]3D voxel44.02 12.35 177ms 23200M Symphonics [12]3D voxel41.44 13.44 187ms 22000M Ours Octree Query44.7113.12 162ms 19000M tained by our methods and some other methods, illustrating that our approach comprehensively understands the structure of the scene, showcasing superior performance in scene understanding. 3D Semantic Scene Completion.To better evaluate the effectiveness of our approach, we conduct comparative experiments for the Semantic Scene Com- pletion (SSC) task. As demonstrated in Tab 2, we compare our results with those of other SSC methods on the SemanticKITTI validation set. Our model demonstrates a more accurate perception of space due to octree construction and correction, outperforming others in IoU metric for geometry reconstruction. Additionally, for specific semantic classes such as car and vegetation, we achieve superior results, attributed to the enhanced treatment of objects facilitated by multi-granularity modeling. Additionally, Tab 3 also indicates that our method consumes fewer computational resources than other dense query-based methods. 4.5 Ablation Study In this subsection, we perform ablation studies on the Occ3d-nus validation set to assess the effectiveness of our proposed modules. All the experiments are conducted on the NVIDIA A40 GPU with reducing the input image size to 0.3x. Effectiveness of Octree Queries.To validate the superiority of octree queries, we maintained consistent TSA and ICA settings while removing the proposed oc-OctreeOcc 13 Table 4: Ablation experiments of Modules on Occ3d-nuScenes dataset. Baseline Octree Query Sem.Init. Iter.Rec.mIoU Latency Memory (a) ✓ 34.17 266 ms 27200M(b) ✓ ✓ 34.91 218 ms 18300M(c) ✓ ✓ ✓ 36.63 214 ms 17900M(d) ✓ ✓ ✓ 35.88 227 ms 18500M (e) ✓ ✓ ✓ ✓ 37.40224 ms 18500M Table 5: Comparison of octree structure qualityat different stages. Stage mIoUlevel 1 to 2level 2 to 3(a)Initialized w/o unbalanced assignment45.79 33.60Initialized w. unbalanced assignment57.34 51.28 (b) After the 1st Rectification60.13 53.95After the 2nd Rectification62.27 56.79 tree structure initialization and rectification modules. This facilitated a compar- ison with the baseline employing dense queries of size 100×100×16 (the largest size achievable within memory limitations). As illustrated in Tab. 4 (b), experi- mental results consistently demonstrate our outperformance, with a notable 0.8 mIoU advantage, despite achieving a memory saving of approximately 9G. This underscores the adeptness of octree prediction in allocating queries with vary- ing granularities to diverse semantic regions. Moreover, the constructed octree queries exhibit adaptability to various object shapes, thereby optimizing the utilization of computational resources. Effectiveness of Semantic-guided Structure Initialization module.To underscorethesignificanceoftheinitialoctreestructure,weexcludetheSemantic- guided Octree Initialization module and employ a MLP to predict the octree structure from randomly initialized queries. This results in a 1.7 mIoU perfor- mance decrease, highlighting the inaccuracy of structurally predicted informa- tion from the initialized query due to the absence of valid information cod- ing. Incorporating semantic priors proves crucial as they enhance the quality of the initialized octree, thereby improving overall model performance. Meanwhile, Tab. 5(a) evaluates the effectiveness of the initial octree structure. We can ob- serve that assigning different initial weights to voxels projected onto different semantic regions directs the initial structure to focus more on the effective areas of the scene, thus improving the quality of the octree structure. Effectiveness of Iteritive Structure Rectification module.We addition- ally perform an ablation study on the Iterative Structure Rectification module, as shown in Tab. 4(d). The incorporation of this module has led to noticeable im- provements in performance. Meanwhile, Tab. 5(b) shows this rectification gradu- ally rectifies areas where structural predictions are incorrect. Consequently, this refinement contributes to the efficiency of octree query expression, positively impacting overall performance. 4.6 More Analysis In this subsection, we provide more analysis experiments on the Occ3d-nus val- idation set. All the experiments are conducted on the NVIDIA A40 GPU with reducing the input image size to 0.3x. Discussion on the depth of octree.Tab. 6 presents experiments on octree depth variations. (a)-(d) show performance with varying 3D query sizes, while (e)-(h) depict octree query performance with different depths and initial resolu- tions. Comparing (e) to (c) and (f) to (d) reveals our approach achieves compa- rable performance to dense queries, significantly reducing resource consumption.14 Y. Lu et al. Table 6: Ablation for different octree depthon Occ3d-nuScenesval set. Query Form Octree Depth Query Resolution mIoU Latency Memory (a) 3D voxel N/A 50×50×4 28.51 129ms 7400M (b) 50×50×16 31.67 186ms 11600M (c) 100 ×100 ×8 32.21 204ms 17400M (d) 100 ×100 ×16 34.17 266ms 27200M (e) Octree Query 2 50×50×4/100 ×100 ×8 32.02 182ms 12400M (f) 3 50×50×4/100 ×100 ×8/100 ×100 ×16 33.76 207ms 16800M (g) 4 25×25×2/50 ×50×4/100 ×100 ×8/100 ×100 ×16 34.88 193 ms 15800M (h) 3 50×50×4/100 ×100 ×8/200 ×200 ×16 37.40 224ms 18500M Table 7: Ablation for the choice of query selection ratioon Occ3d-nuScenesval set. Selection Ratio mIoU Latency Memory (a) 10%, 60% 34.47 191ms 14500M (b) 15%, 60% 35.01 203ms 16300M (c) 25%, 50% 36.73 220ms 18000M (d) 25%, 60% 36.12 255ms 21000M (e) 20%, 60% 37.40 224ms 18500M Table 8: Comparison with an- other octree method. mIoULatencyMemory baseline 34.10 266 ms 27200M OGN [42]33.39 212 ms 24300M Ours 37.40 224 ms 18500M (g) shows that the predetermined octree depth should not be excessive. While reducing memory usage, the imperfect predictions of octree splitting result in accumulated errors, leading to performance degradation as the depth increases. Discussion on query selection ratio of each level in the octree. In Tab. 7, we present results from maintaining different query ratios in various oc- tree level. Results show that too few or too many queries degrade performance. Inadequate queries result in an imperfect scene representation, especially for detailed regions (a,b vs e). Conversely, excessive queries impact computational efficiency, particularly for coarse-grained regions with empty spaces (c vs d and c,d vs e). Optimizing query numbers based on scene object granularity distribu- tion ensures effective handling of semantic regions at different levels. Analysis of Various Usage of Octree. As a classic technique, octree is employed in various tasks [7,15,41,42,46]. Despite differences in addressed prob- lems, we compare our method with OGN [42], which proposes an octree-based upsampling approach. We keep the similar setup in Tab. 4 (b) but substitute the deconvolution decoder with OGN’s octree decoder. Results in Tab. 8 indicate that employing octree solely in the decoder fails to mitigate excessive computa- tional costs and yields sub-optimal performance, mainly due to the high query count during encoding. 5 Conclusions In conclusion, our paper introduces OctreeOcc, a novel 3D occupancy predic- tion framework that addresses the limitations of dense-grid representations in understanding 3D scenes. OctreeOcc’s adaptive utilization of octree represen- tations enables the capture of valuable information with variable granularity, catering to objects of diverse sizes and complexities. Our extensive experimental results affirm OctreeOcc’s capability to attain state-of-the-art performance in 3D occupancy prediction while concurrently reducing computational overhead.OctreeOcc 15 References 1. Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J.: Semantickitti: A dataset for semantic scene understanding of lidar sequences. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9297–9307 (2019) 10 2. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11621–11631 (2020) 10 3. Cao, A.Q., de Charette, R.: Monoscene: Monocular 3d semantic scene completion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3991–4001 (June 2022) 3, 9, 11, 20 4. Chen, X., Lin, K.Y., Qian, C., Zeng, G., Li, H.: 3d sketch-aware semantic scene completion via semi-supervised structure prior. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4193–4202 (2020) 11 5. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable con- volutional networks. In: 2017 IEEE International Conference on Computer Vision (ICCV). pp. 764–773 (2017).https://doi.org/10.1109/ICCV.2017.89 10 6. Fu, C., Li, G., Song, R., Gao, W., Liu, S.: Octattention: Octree-based large-scale contexts model for point cloud compression. In: Proceedings of the AAAI Confer- ence on Artificial Intelligence. vol. 36, pp. 625–633 (2022) 4 7. Häne, C., Tulsiani, S., Malik, J.: Hierarchical surface prediction for 3d object re- construction. In: 2017 International Conference on 3D Vision (3DV). pp. 412–420. IEEE (2017) 4, 14 8. Huang, J., Huang, G.: Bevdet4d: Exploit temporal cues in multi-camera 3d object detection. arXiv preprint arXiv:2203.17054 (2022) 11 9. Huang, J., Huang, G., Zhu, Z., Yun, Y., Du, D.: Bevdet: High-performance multi- camera 3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790 (2021) 1, 3, 11 10. Huang, Y., Zheng, W., Zhang, B., Zhou, J., Lu, J.: Selfocc: Self-supervised vision- based 3d occupancy prediction (2023) 3 11. Huang, Y., Zheng, W., Zhang, Y., Zhou, J., Lu, J.: Tri-perspective view for vision- based 3d semantic occupancy prediction. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition. pp. 9223–9232 (2023) 2, 3, 4, 11, 12 12. Jiang, H., Cheng, T., Gao, N., Zhang, H., Liu, W., Wang, X.: Symphonize 3d semantic scene completion with contextual instance queries. arXiv preprint arXiv:2306.15670 (2023) 1, 3, 4, 11, 12 13. Jiang, Y., Zhang, L., Miao, Z., Zhu, X., Gao, J., Hu, W., Jiang, Y.G.: Polarformer: Multi-camera 3d object detection with polar transformer. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 1042–1050 (2023) 3 14. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017) 11 15. Koneputugodage, C.H., Ben-Shabat, Y., Gould, S.: Octree guided unoriented sur- face reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16717–16726 (2023) 4, 14 16. Lei, H., Akhtar, N., Mian, A.: Octree guided cnn with spherical kernels for 3d point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9631–9640 (2019) 416 Y. Lu et al. 17. Li, J., Han, K., Wang, P., Liu, Y., Yuan, X.: Anisotropic convolutional networks for 3d semantic scene completion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3351–3359 (2020) 11 18. Li, Y., Yu, Z., Choy, C., Xiao, C., Alvarez, J.M., Fidler, S., Feng, C., Anandkumar, A.: Voxformer: Sparse voxel transformer for camera-based 3d semantic scene com- pletion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9087–9098 (June 2023) 2, 3, 4, 11, 12 19. Li, Y., Bao, H., Ge, Z., Yang, J., Sun, J., Li, Z.: Bevstereo: Enhancing depth estimation in multi-view 3d object detection with dynamic temporal stereo (2022) 3, 11 20. Li, Y., Ge, Z., Yu, G., Yang, J., Wang, Z., Shi, Y., Sun, J., Li, Z.: Bevdepth: Acquisition of reliable depth for multi-view 3d object detection. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 1477–1485 (2023) 3 21. Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Qiao, Y., Dai, J.: Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotem- poral transformers. In: European conference on computer vision. pp. 1–18. Springer (2022) 3, 11, 12 22. Li, Z., Yu, Z., Wang, W., Anandkumar, A., Lu, T., Alvarez, J.M.: Fb-bev: Bev representation from forward-backward view transformations. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6919–6928 (2023) 1, 11, 12, 22 23. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2117–2125 (2017) 10 24. Lin,X.,Lin,T.,Pei,Z.,Huang,L.,Su,Z.:Sparse4d:Multi-view3dobjectdetection with sparse spatial-temporal fusion. arXiv preprint arXiv:2211.10581 (2022) 3 25. Liu, Y., Wang, T., Zhang, X., Sun, J.: Petr: Position embedding transformation for multi-view 3d object detection. In: European Conference on Computer Vision. pp. 531–548. Springer (2022) 3 26. Liu, Y., Yan, J., Jia, F., Li, S., Gao, A., Wang, T., Zhang, X.: Petrv2: A unified framework for 3d perception from multi-camera images. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3262–3272 (2023) 3 27. Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D.L., Han, S.: Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation. In: 2023 IEEE International Conference on Robotics and Automation (ICRA). pp. 2774– 2781. IEEE (2023) 3 28. Ma, Q., Tan, X., Qu, Y., Ma, L., Zhang, Z., Xie, Y.: Cotr: Compact oc- cupancy transformer for vision-based 3d occupancy prediction. arXiv preprint arXiv:2312.01919 (2023) 3 29. Meagher, D.: Geometric modeling using octree encoding. Computer graphics and image processing19(2), 129–147 (1982) 2, 4 30. Miao, R., Liu, W., Chen, M., Gong, Z., Xu, W., Hu, C., Zhou, S.: Oc- cdepth: A depth-aware method for 3d semantic scene completion. arXiv preprint arXiv:2302.13540 (2023) 3 31. Ming, Z., Berrio, J.S., Shan, M., Worrall, S.: Inversematrixvt3d: An efficient projection matrix-based approach for 3d occupancy prediction. arXiv preprint arXiv:2401.12422 (2024) 3 32. Pan, M., Liu, J., Zhang, R., Huang, P., Li, X., Liu, L., Zhang, S.: Renderocc: Vision-centric 3d occupancy prediction with 2d rendering supervision (2023) 3, 11OctreeOcc 17 33. Park, J., Xu, C., Yang, S., Keutzer, K., Kitani, K., Tomizuka, M., Zhan, W.: Time will tell: New outlooks and a baseline for temporal multi-view 3d object detection. arXiv preprint arXiv:2210.02443 (2022) 3 34. Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In: Computer Vision–ECCV 2020: 16th Eu- ropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16. pp. 194–210. Springer (2020) 3 35. Que, Z., Lu, G., Xu, D.: Voxelcontext-net: An octree based framework for point cloud compression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6042–6051 (2021) 4 36. Roldao, L., de Charette, R., Verroust-Blondet, A.: Lmscnet: Lightweight multiscale 3d semantic completion. In: 2020 International Conference on 3D Vision (3DV). pp. 111–119. IEEE (2020) 11 37. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed- ical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, Oc- tober 5-9, 2015, Proceedings, Part III 18. pp. 234–241. Springer (2015) 7 38. Silva, S., Wannigama, S.B., Ragel, R., Jayatilaka, G.: S2tpvformer: Spatio- temporal tri-perspective view for temporally coherent 3d semantic occupancy pre- diction. arXiv preprint arXiv:2401.13785 (2024) 3 39. Sima, C., Tong, W., Wang, T., Chen, L., Wu, S., Deng, H., Gu, Y., Lu, L., Luo, P., Lin, D., Li, H.: Scene as occupancy (2023) 3, 7 40. Tan, Z., Dong, Z., Zhang, C., Zhang, W., Ji, H., Li, H.: Ovo: Open-vocabulary occupancy. arXiv preprint arXiv:2305.16133 (2023) 1, 3 41. Tang, J.H., Chen, W., Yang, J., Wang, B., Liu, S., Yang, B., Gao, L.: Octfield: Hierarchical implicit functions for 3d modeling. arXiv preprint arXiv:2111.01067 (2021) 4, 14 42. Tatarchenko, M., Dosovitskiy, A., Brox, T.: Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. In: Proceedings of the IEEE international conference on computer vision. pp. 2088–2096 (2017) 4, 14 43. Tian, X., Jiang, T., Yun, L., Wang, Y., Wang, Y., Zhao, H.: Occ3d: A large- scale 3d occupancy prediction benchmark for autonomous driving. arXiv preprint arXiv:2304.14365 (2023) 3, 10, 11 44. Wang, P.S.: Octformer: Octree-based transformers for 3d point clouds. arXiv preprint arXiv:2305.03045 (2023) 4 45. Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X.: O-cnn: Octree-based con- volutional neural networks for 3d shape analysis. ACM Transactions On Graphics (TOG) 36(4), 1–11 (2017) 4 46. Wang, P.S., Liu, Y., Tong, X.: Dual octree graph networks for learning adaptive volumetric shape representations. ACM Transactions on Graphics (TOG)41(4), 1–15 (2022) 4, 14 47. Wang, P.S., Sun, C.Y., Liu, Y., Tong, X.: Adaptive o-cnn: A patch-based deep representation of 3d shapes. ACM Transactions on Graphics (TOG)37(6), 1–11 (2018) 4 48. Wang, S., Liu, Y., Wang, T., Li, Y., Zhang, X.: Exploring object-centric tem- poral modeling for efficient multi-view 3d object detection. arXiv preprint arXiv:2303.11926 (2023) 3 49. Wang, X., Zhu, Z., Xu, W., Zhang, Y., Wei, Y., Chi, X., Ye, Y., Du, D., Lu, J., Wang, X.: Openoccupancy: A large scale benchmark for surrounding semantic occupancy perception. arXiv preprint arXiv:2303.03991 (2023) 318 Y. Lu et al. 50. Wang, Y., Chen, Y., Liao, X., Fan, L., Zhang, Z.: Panoocc: Unified occu- pancy representation for camera-based 3d panoptic segmentation. arXiv preprint arXiv:2306.10013 (2023) 2, 3, 4, 7, 11, 12, 22 51. Wei, Y., Zhao, L., Zheng, W., Zhu, Z., Zhou, J., Lu, J.: Surroundocc: Multi-camera 3doccupancypredictionforautonomousdriving.In:ProceedingsoftheIEEE/CVF International Conference on Computer Vision (ICCV). pp. 21729–21740 (October 2023) 1, 3, 6 52. Yan, X., Gao, J., Li, J., Zhang, R., Li, Z., Huang, R., Cui, S.: Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 3101–3109 (2021) 11 53. Yang, C., Chen, Y., Tian, H., Tao, C., Zhu, X., Zhang, Z., Huang, G., Li, H., Qiao, Y., Lu, L., et al.: Bevformer v2: Adapting modern image backbones to bird’s-eye- view recognition via perspective supervision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 17830–17839 (2023) 3 54. Yu, Z., Shu, C., Deng, J., Lu, K., Liu, Z., Yu, J., Yang, D., Li, H., Chen, Y.: Flashocc: Fast and memory-efficient occupancy prediction via channel-to-height plugin. arXiv preprint arXiv:2311.12058 (2023) 3 55. Zhang, H., Yan, X., Bai, D., Gao, J., Wang, P., Liu, B., Cui, S., Li, Z.: Radocc: Learning cross-modality occupancy knowledge through rendering assisted distilla- tion. arXiv preprint arXiv:2312.11829 (2023) 3 56. Zhang, Y., Zhu, Z., Du, D.: Occformer: Dual-path transformer for vision-based 3d semantic occupancy prediction. arXiv preprint arXiv:2304.05316 (2023) 1, 3, 6, 11, 12 57. Zhou, C., Zhang, Y., Chen, J., Huang, D.: Octr: Octree-based transformer for 3d object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5166–5175 (2023) 4 58. Zhu, B., Jiang, Z., Zhou, X., Li, Z., Yu, G.: Class-balanced grouping and sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492 (2019) 11 59. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020) 7OctreeOcc 19 Appendix A More Details In this section, we provide detailed explanations of our proposed modules. Forthe Semantic-GuidedOctreeInitialization ,ourapproachcommences with acquiring semantic segmentation labels for images by projecting occupancy labels onto the surround-view images. Subsequently, a UNet is trained using theselabels.Theinitializationprocessentailsrandomlyinitializingdensequeries, where each query’s center point serves as a reference point projected onto the range-view images. If a reference point is projected onto a ground pixel (i.e., driveable surface, other flat, or sidewalk), the probability increases by 0.1. Con- versely, if projected onto a background pixel (excluding ground classes), the probability increases by 0.5. Projection onto a foreground pixel increases the probability of requiring a split at that position by 1.0. This process assigns a split probability to each query, and the octree mask is constructed through average pooling, capturing split probabilities at different query levels. After ob- taining the octree mask, we designate the top 20% confidence queries as parent queries in level 1, while the remaining queries become Leaf queries and remain unsplit. Moving to level 2, after splitting the parent queries into octants, the top 60% confidence positions are selected as new parent queries, and the remainder as leaf queries. By storing leaf queries at each level, we construct a sparse and multi-granularity octree structure for queries. In Iterative Structure Rectification, at level 1, we retain predictions for the top 10% of positions with confidence. For the remaining positions, a 2-layer MLP is utilized to predict probabilities. These new probabilities are blended with the existing probabilities, with a weight distribution of 60% for the new probabilities and 40% for the old ones. The top 10% of positions with the new probability values are identified as the required splits, shaping the structure of the new level 1. Similarly, at level 2, predictions for the top 30% of positions with confidence are preserved. For positions not in the top 30%, probabilities are predicted using a 2-layer MLP. The new probabilities are computed by merging them with the original probabilities, with an even weight distribution of 50% for each. The top 30% of the new probability values are then selected as the positions necessitating splitting, delineating the structure of the new level 2. B Octree node index calculation The hierarchical structure of the octree, particularly the assignment of queries to respective levels, is determined based on the octree maskMo and the query se- lection ratio denoted asα = {α1, α2, . . . , αL−1}. These ratios govern the number of subdivisions at each level, thereby defining the hierarchical organization of the octree. The procedure is as follows. For level l, queries withMl o values within the top αl percentile are identified as candidates for octant subdivision. Sub- sequently, within octants that have undergone one previous subdivision at the20 Y. Lu et al. next level, queries are once again selected based on their values falling within the top α2 percentile, initiating another round of subdivision. This process continues iteratively until reaching the final level of the octree. Simultaneously, exploiting the octree structure facilitates the direct con- version of sparse octree queries into dense queries to align with the desired output shape. For a query qoctree at level l with the index (a, b, c), the in- dexes of its corresponding 8L−l children nodes in level L are determined by (a ×2L−l + aoffset , b×2L−l + boffset , c×2L−l + coffset ), whereaoffset , boffset , and coffset are independent, ranging from0 to 2L−l. Here,L denotes the depth of the octree. During this process, we allocate the feature ofqoctree to all of these positions. By iteratively applying this procedure to all queries at each level, we effectively transformQoctree into Qdense. C More discussion of octree initialization Given that the FloSP method outlined in MonoScene [3] incorporates a 3D to 2D projection operation, similar to our initialization approach, we additionally adapted this method for comparison. Specifically, we employed FloSP to extract 3D voxel features from 2D image features. Subsequently, we applied a Multi- Layer Perceptron (MLP) topredictthe splitting probability ofeachvoxel, replac- ing the randomly initialized queries used in the original ablation experiments. The results indicate that, although this operation outperforms predictions from randomly initialized queries, it is still constrained by insufficient information, resulting in a decline in overall performance. Table 9:More ablation of octree initialization Initialization Method mIoU (a)) Randomly initialised queries 34.91 (b) Voxel features from FLoSP 35.72 (c) Semantic-Guided Octree Initialization37.40 D More Visualization Fig. 5 shows additional visualizations of proposed OctreeOcc. Evidently, our ap- proach, leveraging the multi-granularity octree modeling, demonstrates superior performance particularly in the categories of truck, bus, and manmade objects. Fig. 6 illustrates the results of occupancy prediction alongside the corre- sponding octree structure. For clarity in visualization, we employ distinct colors to represent voxels at various levels of the octree prediction, based on their occu- pancy status. For improved visualization, only a portion correctly correspondingOctreeOcc 21 to the occupancy prediction is displayed, rather than the entire octree structure, ensuring clarity and focus on the relevant information. Level 3 (voxel size: 0.4m × 0.4m × 0.4m) is depicted in light gray, level 2 (voxel size: 0.8m× 0.8m × 0.8m) in medium gray, and level 1 (voxel size: 1.6m× 1.6m × 1.6m) in dark gray. It’s worth noting that level 1 voxels, predominantly situated in free space and within objects, might be less intuitively discernible. Nonetheless, this image underscores the efficacy of octree modeling, which tailors voxel sizes to different semantic regions, enhancing representation accuracy.22 Y. Lu et al. CAM_FRONT CAM_BACKCAM_FRONT_LEFT CAM_BACK_RIGHTCAM_BACK_LEFTCAM_FRONT_RIGHT PanoOcc FBOCC Ours GT Barrier Bicycle Bus Car Const. Veh. Motor Ped. Traf. C. Trailer Truck Drive. Surf. Flat Sidewalk Terrain Manmade Vegetation Fig.5: More visualization on Occ3D-nuScenes validation set.The first row displays input multi-view images, while the second row showcases the occupancy pre- diction results of PanoOcc [50], FBOCC [22], our methods, and the ground truth.OctreeOcc 23 CAM_FRONT CAM_BACKCAM_FRONT_LEFT CAM_BACK_RIGHTCAM_BACK_LEFTCAM_FRONT_RIGHT Barrier Bicycle Bus Car Const. Veh. Motor Ped. Traf. C. Trailer Truck Drive. Surf. Flat Sidewalk Terrain ManmadeVegetation Occupancy Prediction Octree Prediction Occ Pred: Level 1 Level 2 Level 3 Octree Pred: Fig.6: Visulization of octree structure.The first row displays input multi-view images, while the second and third rows showcase the occupancy prediction results and the corresponding octree structure prediction results.",
      "meta_data": {
        "arxiv_id": "2312.03774v3",
        "authors": [
          "Yuhang Lu",
          "Xinge Zhu",
          "Tai Wang",
          "Yuexin Ma"
        ],
        "published_date": "2023-12-06T02:52:54Z",
        "pdf_url": "https://arxiv.org/pdf/2312.03774v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "OctreeOcc addresses the computational demands and loss of spatial details in 3D occupancy prediction by proposing an innovative framework that leverages multi-granularity octree representation. The main contributions include: 1) Introducing OctreeOcc for efficient and multi-granularity 3D occupancy prediction, which enables spatial sparsification, significantly reducing the number of voxels while retaining critical spatial details. 2) Proposing a semantic-guided octree initialization module that incorporates image semantic information to produce more accurate initial octree structures. 3) Designing an iterative structure rectification module to dynamically refine the octree structure, mitigating errors from initial segmentation and projection. 4) Demonstrating state-of-the-art performance in occupancy prediction and achieving a 15%-24% reduction in computational overhead compared to dense-grid-based methods.",
        "methodology": "OctreeOcc employs sparse and multi-granularity octree queries, where a learnable octree mask partitions the 3D space recursively. The dense voxel queries are transformed into sparse octree queries by retaining leaf queries (unsplit voxels) at each level, guided by a binarized octree mask. The binarization uses a manually defined query selection ratio (top-k mechanism) to avoid pattern collapse. The octree structure is initialized using a semantic-guided approach, where input multi-view images are segmented (via UNet) to assign unbalanced weights to voxel projection points based on their semantic regions (foreground, building, vegetation, road). An Octree Encoder updates octree query features and structure using Temporal Self-Attention (TSA) for historical query fusion and Image Cross-Attention (ICA) for interaction with multi-scale image features. An Iterative Structure Rectification module refines the octree structure by identifying low-confidence regions, extracting their query features, and using an MLP to predict new split probabilities which are then blended with previous predictions. Octree ground truth is generated from semantic occupancy ground truth, assigning a binary split value based on label consistency of child voxels. The model is supervised with a combined loss function including focal, lovasz-softmax, dice, affinity losses, and an additional focal loss for octree prediction.",
        "experimental_setup": "The method was evaluated on two datasets: Occ3D-nuScenes and SemanticKITTI. Occ3D-nuScenes provides precise occupancy ground truth (0.4m x 0.4m x 0.4m voxels, 17 semantic classes + 'free') for 700 training and 150 validation instances. SemanticKITTI features 22 outdoor driving scenarios (0.2m x 0.2m x 0.2m voxels, 21 semantic classes) in 256x256x32 grids. Performance was measured using intersection-over-union (mIoU). The implementation used 900x1600 input images, ResNet101-DCN as the image backbone with FPN for multi-scale features, and a feature dimension of 256. An octree depth of 3 was used, with an initial query resolution of 50x50x4 and query selection ratios of 20% and 60% for the two divisions. The octree encoder had three layers, each with Temporal Self-Attention (M1=4), Image Cross-Attention (M2=4), and Iterative Structure Rectification (top 10% high-confidence in level 1, 30% in level 2). The model was trained with Adam optimizer (learning rate 2e-4, weight decay 0.01) for 24 epochs using a batch size of 8 on 8 NVIDIA A100 GPUs. Ablation studies were conducted on Occ3D-nuScenes to validate individual module effectiveness and analyze octree depth and query selection ratios.",
        "limitations": "The paper highlights several limitations. Directly predicting 3D structure from 2D images is challenging due to the inherent lack of depth and occlusion issues. Initial octree structure prediction from neural networks can yield sub-optimal results due to the absence of meaningful information in randomly initialized queries. The semantic-guided initialization may not precisely correspond to the actual scene due to potential segmentation and projection errors. Neural network-based binarization for the octree mask is susceptible to pattern collapse, tending to predict all voxels as non-split, which is mitigated by a manually defined query selection ratio. Furthermore, a predetermined excessive octree depth can lead to accumulated errors from imperfect octree splitting predictions, causing performance degradation despite memory reduction. Lastly, both too few or too many queries at various octree levels degrade performance, with inadequate queries resulting in imperfect scene representation (especially for detailed regions) and excessive queries impacting computational efficiency (particularly for coarse-grained regions).",
        "future_research_directions": "Based on the identified limitations and discussions, potential future research directions include: 1) Developing methods for adaptively learning and optimizing octree depth and query selection ratios, rather than relying on predetermined values, to dynamically suit scene characteristics. 2) Enhancing the quality and robustness of the initial octree structure, possibly by integrating more sophisticated 2D-to-3D lifting techniques or richer 3D priors beyond semantic guidance, to further reduce reliance on potentially error-prone segmentation and projection. 3) Exploring more advanced and dynamic iterative structure rectification mechanisms to improve error correction in low-confidence regions. 4) Investigating the applicability and benefits of the efficient, multi-granularity octree representation to other 3D perception tasks, such as 3D object detection, tracking, or semantic mapping, where computational efficiency is critical. 5) Researching fully end-to-end learning approaches for octree structure generation to minimize reliance on intermediate priors or manual heuristics."
      }
    },
    {
      "title": "OctField: Hierarchical Implicit Functions for 3D Modeling",
      "abstract": "Recent advances in localized implicit functions have enabled neural implicit\nrepresentation to be scalable to large scenes. However, the regular subdivision\nof 3D space employed by these approaches fails to take into account the\nsparsity of the surface occupancy and the varying granularities of geometric\ndetails. As a result, its memory footprint grows cubically with the input\nvolume, leading to a prohibitive computational cost even at a moderately dense\ndecomposition. In this work, we present a learnable hierarchical implicit\nrepresentation for 3D surfaces, coded OctField, that allows high-precision\nencoding of intricate surfaces with low memory and computational budget. The\nkey to our approach is an adaptive decomposition of 3D scenes that only\ndistributes local implicit functions around the surface of interest. We achieve\nthis goal by introducing a hierarchical octree structure to adaptively\nsubdivide the 3D space according to the surface occupancy and the richness of\npart geometry. As octree is discrete and non-differentiable, we further propose\na novel hierarchical network that models the subdivision of octree cells as a\nprobabilistic process and recursively encodes and decodes both octree structure\nand surface geometry in a differentiable manner. We demonstrate the value of\nOctField for a range of shape modeling and reconstruction tasks, showing\nsuperiority over alternative approaches.",
      "meta_data": {
        "arxiv_id": "2111.01067v1",
        "authors": [
          "Jia-Heng Tang",
          "Weikai Chen",
          "Jie Yang",
          "Bo Wang",
          "Songrun Liu",
          "Bo Yang",
          "Lin Gao"
        ],
        "published_date": "2021-11-01T16:29:39Z",
        "pdf_url": "https://arxiv.org/pdf/2111.01067v1.pdf"
      }
    },
    {
      "title": "MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models",
      "abstract": "We present a novel compression algorithm for reducing the storage of LiDAR\nsensor data streams. Our model exploits spatio-temporal relationships across\nmultiple LiDAR sweeps to reduce the bitrate of both geometry and intensity\nvalues. Towards this goal, we propose a novel conditional entropy model that\nmodels the probabilities of the octree symbols by considering both coarse level\ngeometry and previous sweeps' geometric and intensity information. We then use\nthe learned probability to encode the full data stream into a compact one. Our\nexperiments demonstrate that our method significantly reduces the joint\ngeometry and intensity bitrate over prior state-of-the-art LiDAR compression\nmethods, with a reduction of 7-17% and 15-35% on the UrbanCity and\nSemanticKITTI datasets respectively.",
      "full_text": "MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models Sourav Biswas1,2 Jerry Liu1 Kelvin Wong1,3 Shenlong Wang1,3 Raquel Urtasun1,3 1Uber Advanced Technologies Group 2University of Waterloo 3University of Toronto {souravb,jerryl,kelvin.wong,slwang,urtasun}@uber.com Abstract We present a novel compression algorithm for reducing the storage of LiDAR sensor data streams. Our model exploits spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values. Towards this goal, we propose a novel conditional entropy model that models the probabilities of the octree symbols by considering both coarse level geometry and previous sweeps’ geometric and intensity information. We then use the learned probability to encode the full data stream into a compact one. Our experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduction of 7–17% and 6–19% on the UrbanCity and SemanticKITTI datasets respectively. 1 Introduction The past decade has witnessed numerous innovations in intelligent systems, thanks to an explosion of progress in sensing and AI algorithms. In particular, LiDAR sensors are extensively used in various applications such as indoor rovers, unmanned aerial vehicles, and self-driving cars to accurately capture the 3D geometry of the scene. Yet the rapid adoption of LiDAR has brought about a key challenge—dealing with the mounting storage costs associated with the massive inﬂux of LiDAR data. For instance, a 64-line Velodyne LiDAR continuously scanning a given scene produces over3 billion pointsin a single hour. Hence, developing efﬁcient and effective compression algorithms to store such 3D point cloud data streams is crucial to reduce the storage and communication bandwidth. Unlike its well-studied image and video counterparts, point cloud stream compression is a challenging yet under-explored problem. Many prior approaches have focused on encoding a point cloud stream as independent sweeps, where each sweep captures a rough 360-degree rotation of the sensor. Early approaches exploit a variety of compact data structures to represent the point cloud in a memory- efﬁcient manner, such as octrees [1], KD-trees [2], and spherical images [3]. More recent works along this direction utilize powerful machine learning models to encode redundant geometric correlations within these data structures for better compression [4, 5, 6]. In general, most of these aforementioned approaches do not make effective use of temporal correlations within point clouds. Moreover, these prior approaches have largely focused on compressing the geometric structure of the point cloud (the spatial coordinates); yet there has been little attention paid towards compression of other attributes, e.g. LiDAR intensity, which are crucial for many downstream tasks. Compressing such attributes along with geometric structure can make a signiﬁcant impact on reducing storage. In this paper, we present a novel, learning-based compression algorithm that comprehensively reduces the storage of LiDAR sensor data streams. Our method extends the recent success of octree-structured deep entropy models [6] for single LiDAR sweep compression to intensity-valued LiDAR streaming data. Speciﬁcally, we propose a novel deep conditional entropy modelthat models the probabilities of the octree symbols and associated intensity values by exploiting spatio-temporal correlations within the data: taking both coarse level information at the current sweep, as well as relevant neighboring 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2011.07590v2  [eess.IV]  8 Jan 2021(a) Top-down Pass(b) Bottom-up Pass Sweep t-1 Sweep t (2) Prior Octree Dependence(1) Ancestral Node Dependence (3) Spatio-Temporal Aggregation z x y  Octree Occupancy Entropy Model (Sec. 2.3)Intensity Entropy Model (Sec. 2.4) Sweept -1 Sweep t Geometry Bitstream Probability Estimation Intensity Bitstream Continuous Convolution Continuous Convolution z x y  Sweep t-1 Sweep t Input Point Cloud àOctree (Sec 2.1) Probability Estimation Figure 1: Comprehensive overview of our method. Our point cloud stream is serialized into an octree repre- sentation (Sec 2.1). We apply a spatio-temporal entropy model to the octree occupancy bytestream (Sec. 2.3), modeling ancestral dependence, prior octree dependence, and octree alignment. We also apply a deep entropy model to model the intensity stream (Sec. 2.4). nodes information from the previous sweep. Unlike prior approaches, our method models the joint entropy across an entire point cloud sequence, while unifying geometry and attribute compression into the same framework. We validate the performance of our approach on two large datasets, namely UrbanCity [ 7] and SemanticKITTI [8]. The experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduc- tion of 7–17% on UrbanCity and 6–19% on SemanticKITTI. We also conduct extensive experiments showcasing superior performance against prior works on numerous downstream perception tasks. 2 Multi-Sweep LiDAR Compression In this work, we propose a comprehensive framework for thelossy compression of LiDAR point cloud streams, by exploiting the spatio-temporal redundancies through a learned entropy model. We aim to maximize the reconstruction quality of these point clouds while reducing their joint bitrate. Every point in a LiDAR point cloud contains both a spatial 3D location (x,y,z ), as well as an intensity value r, and we jointly compress both. Our method is shown in Fig. 1. We ﬁrst quantize and encode all point spatial coordinates in the stream into an octree representation, where leaves represent the quantized points and intermediate nodes contain 8-bit symbols representing child occupancies (Sec. 2.1). We then present a novel deep entropy model (Sec. 2.2): a probability model that utilizes spatio-temporal contextto predict occupancy symbols for each node (Sec. 2.3), as well as intensity values for each point for intensity compression (Sec. 2.4). The outputs of these entropy models are ﬁnally fed into a lossless entropy coding algorithm, such as range coding, to produce the ﬁnal bitstream (Sec. 2.5). 2.1 Octree Representation Octree Structure and Bit Representation:LiDAR point clouds are intrinsically challenging to process due to their sparsity and inherently unstructured nature. A tool to counteract these challenges is to use a tree-based data structure, such as an octree or KD-tree, to efﬁciently partition the space. Inspired by [1, 6], we quantize and represent every point cloud in our stream as an octree with an associated depth value D, corresponding to the quantized precision of the point cloud. Speciﬁcally, an octree can be constructed from a 3D point cloud by ﬁrst partitioning the spatial region into 8 octants, and recursively partitioning each octant until each node contains at most one point, or until Dis reached. The resulting octree contains both intermediate nodes and leaf nodes. Each intermediate node can be represented by an 8-bit occupancy symbol x, representing the occupancies of its children; each node also has an implied spatial position. Each leaf node contains one point of the point cloud, and stores the offset between the point and its corner position, as well as the point intensity. We determine the intensity value of each point in the quantized point cloud by taking that of its nearest neighbor in the original point cloud. The number of bits allocated to each leaf node is level-dependent; an octree with D= kwill store k−ibits for a leaf node at level i,i ≤k. Hence, 2the octree is memory-efﬁcient—shared bits are encoded with intermediate nodes and residual bits with leaves. Serialization: We serialize the octree into two (uncompressed) bytestreams by traversing the octree in breadth-ﬁrst order. The ﬁrst bytestream contains the intermediate node occupancy symbols in breadth-ﬁrst order, and the second bytestream contains the leaf node offsets/intensities encountered during traversal. Our entropy model focuses primarily on the node occupancies/intensities—we demonstrate in our supplementary materials that leaf offsets do not contain meaningful patterns we can exploit. Hence for subsequent sections we denote P(t) = (X(t),R(t)), where X(t) = {x(t) 1 ,..., x(t) mt } is the set of occupancy symbols, and R(t) = {r(t) 1 ,..., r(t) nt }is the set of intensities. The serialization is lossless; the only loss comes from D-dependent octree quantization. This gives a guarantee on reconstruction quality and allows compression efforts to solely focus on bitrate reduction. 2.2 Octree-Based Conditional Entropy Module The octree sequence is now fed into ourentropy model. Our entropy model is a probability model that approximates the unknown joint distribution of point clouds pdata with our own distribution p(·; w). Since we convert our point clouds to octree representations, the probability model is equivalent to modeling p(P(1),..., P(n); w). According to the classic Shannon’s source coding theorem [9], the expected bitrate for the point cloud stream is tightly approximated by the cross-entropy between the real point cloud stream distribution and our parametrized model: Epdata [−log p(P(1),..., P(n); w)]. We then assume that the joint probability factorizes as follows: log p(P(1),..., P(n); w) = ∑ t log p(P(t)|P(t−1); w) (1) = ∑ t {log p(X(t)|P(t−1); w) + logp(R(t)|X(t),P(t−1); w)} (2) We make a 1st-order Markov assumption: a given octree P(t) only depends on the sweep preced- ing it, P(t−1). We then factor the octree into two entropy models: the node occupancy model p(X(t)|P(t−1); w), and the intensity model p(R(t)|X(t),P(t−1); w) conditioned on occupancies. The dependence only on past sweeps makes the model applicable to an online LiDAR stream setting. 2.3 Occupancy Entropy Model We obtain our node occupancy model by continuing to factorize the occupancy probabilities: p(X(t)|P(t−1); w) = ∏ i p(x(t) i |X(t) ans(i),P(t−1); w) (3) Here, X(t) ans(i) = {x(t) pa(i),x(t) pa(pa(i)),..., x(t) pa(...(pa(i)))}represents the set of ancestor nodes of x(t) i and P(t−1) represents the point cloud from previous sweep. As seen above, we simplify the autoregressive dependency on ancestors nodes on the octree for the given timestamp, as well as all the nodes at the previous timestamp. We model p(·|X(t) ans(i),P(t−1); w) with a deep neural network. The architecture has two backbones, namely the ancestral node dependencemodule which encodes recurrent dependencies on the ancestor nodes X(t) ans(i) from the current sweep’s octree as well as a prior octree dependencemodule which models information passed from the previous sweep. Fig. 1 depicts the architecture of such network. Ancestral Node Dependence: Our ancestral node dependence module is a recurrent network deﬁned over an ancestral, top-down octree path. Inspired by [ 6], we feed a context feature ci for every node xi through a multi-layer perceptron (MLP) to extract an initial hidden embed- ding h(t) i,0 = σ0(ci; w), where σ0(·; w) denotes a MLP with learnable parameter w. Context 3features include the current octree level, octant spatial location, and parent occupancy; they are known beforehand per node xi and computed to facilitate representation learning. We then per- form Kans rounds of aggregation between every node’s embedding and its parental embedding: h(t) i,k = σk([h(t) i,k−1,h(t) pa(i),k−1]; w). As shorthand, we denote this entire tree-structured recurrent backbone branch as h(t) i = fans(x(t) i ,X(t) ans(i)). Temporal Octree Dependence: We also incorporate the previous octree P(t−1) into the current entropy model at time t through a temporal octree dependencemodule. We thus ﬁrst align the previous octree into the sensor coordinate frame of the current octree. Unlike the current octree where we only have access to parental information, we can construct features that make use of all information within the previous octree, containing both top-down ancestral information as well as bottom-up child information. We exploit this fact by designing a two-stream feature backbone to compute embeddings for every octree node at time t−1, inspired by tree-structured message passing algorithms [10, 11]. The forward pass stream is the same as the ancestral dependence module above, generating top-down features from ancestors: h(t−1) j = fans(x(t−1) j ,X(t−1) ans(j) ). After the top-down pass, we design a bottom-up aggregation pass, a recurrent network that produces aggregated features from descendants to the current node. Unlike the ancestral module in which each node only has one parent, the number of children per node can vary, and we desire that the output is invariant to the ordering of the children. Hence, we resolve this by designing the following function inspired by deep sets [12]: g(t−1) j = fagg,1(h(t−1) j + ∑ c∈child(j) fagg,2(g(t−1) c )), which produces the ﬁnal aggregated embedding feature containing both top-down and bottom-up context. Spatio-Temporal Aggregation: The ﬁnal step incorporates the set of aggregated features in the previous octree {g(t−1) j }, with ancestral features in the current octree {h(t) i }to help with occupancy prediction in the current octree. A key observation is that only a subset of spatially proximal nodes in the previous sweep can contribute to better prediction for a given node at timet; moreover, the relative location of each neighbor should deﬁne its relative importance. Inspired by this fact, we employ continuous convolutions[13] to process previous octree features at the current node. A continuous conv. layer aggregates features from neighboring points to a given node in the following manner: hi = ∑ j∈N(i) σ(pj −pi)hj where N(i) is the i-th node’sk-nearest neighboring nodes in 3D space from the (t−1) sweep at the same level as i, pi is the 3D position of each node, and σdenotes a learned MLP. We use a separate MLP with a continuous conv. layer per octree level to process the aggregated features in the previous octree {g(t−1) j }j∈N(i) and produce an embedding feature g(t) i,st. Entropy Header: Finally, the warped feature g(t) i,st and ancestral features h(t) i are aggre- gated through a ﬁnal MLP to output a 256-dimensional softmax of probability values p(x(t) i |X(t) ans(i),P(t−1); w), corresponding to the predicted 8-bit occupancy for node i, time t. 2.4 Intensity Entropy Model The goal of the intensity entropy model is to compress extraneous intensities tied to each spatial point coordinate. We assume these intensities are bounded and discrete, so compression is lossless; if they are continuous, there will be a loss incurred through discretization. The model factorizes as follows: p(R(t)|X(t),P(t−1); w) = ∏ i p(r(t) i |X(t),P(t−1); w) (4) The intent of conditioning on the occupancies X(t) is not to directly use their values per se, but to emphasize that intensity decoding occurs after the point spatial coordinates have already been reconstructed in R3. Therefore, we can directly make use of the spatial position corresponding to each intensity R(t) i in compression. We aim to leverage temporal correlations between point intensities across consecutive timestamps to better model the entropy of r(t) i . Similar to node occupancy predic- tion above, there is the challenge of how to incorporate previous intensity information when there are no direct correspondences between the two point clouds. We again employ continuous convolutions 40 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 0.00 0.05 0.10 0.15Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 70 80 90 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 0.00 0.05 0.10 0.15 0.20Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 0 5 10 15 20 Spatial Bits Per Point 60 70 80 90 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Figure 2: Bitrate vs. reconstruction quality curves on UrbanCity (top) and KITTI (bottom). From left-to-right: F1 with τgeo = 10cmand τint = 0(↑), point-to-point chamfer distance (↓), point-to-plane PSNR (↑). to resolve this challenge. Let RN(i) be the set of nearest neighbor intensities {r(t−1) j }j∈N(i), where nearest neighbor is deﬁned by spatial proximity of previous point jto the current point i. We apply an MLP with a continuous conv. layer that takes the past intensities r(t−1) j as input and outputs an embedding feature for each node i. This feature is then fed through a linear layer and softmax to output intensity probability values. In our setting we assume our intensity value is an 8-bit integer, so the resulting probability vector is 256-dimensional p(r(t) i |X(t),P(t−1); w). 2.5 Entropy Coding Process Encoding: We integrate our entropy model with an entropy coding algorithm (range coding [14]) to produced the ﬁnal compressed bitstream. During the encoding pass, for every octree in the stream, the entropy model is applied across the octree occupancy bytestream, as well as across the intensities per point, to predict the respective probability distributions. We note that encoding only requires one batch GPU pass per sweep for the occupancy and intensity models. The resulting distributions are then passed to range coding which compresses the occupancies and intensities into two bitstreams. Decoding: The same entropy models are used during decoding. First, the occupancy entropy model is run, given the already decoded past octree, to produce distributions that recover the occupancy serialization and spatial coordinates of the current point cloud. Then, the intensity entropy model is run, given the already decoded intensities in the past point cloud, to produce distributions that recover the current point intensities. Note that our model is well-setup for parallel computation during decoding, for both the occupancies and intensities. As mentioned in Sec. 2.3, the dependence on ancestral nodes instead of all past nodes allows us to only run at most O(D) GPU passes for the occupancy model per sweep. Moreover, the assumed independence between intensities in the current sweep, given the past, allows us to only run 1 GPU pass per sweep for the intensity entropy model. 2.6 Learning Both our occupancy and intensity entropy models are trained end-to-end with cross-entropy loss, for every node x(t) i ∈X(t) i and intensity r(t) i ∈R(t) i , for every point cloud in a stream: ℓ= EP∼pdata [ − ∑ t ∑ i log p(x(t) i,gt|X(t) ans(i),P(t−1); w) − ∑ t ∑ i log p(r(t) i,gt|X(t),P(t−1); w) ] (5) 5Oracle (UrbanCity): Bitrate 104  Ours: F1 92.4, Bitrate 9.3  Draco: F1 80.8, Bitrate 9.4  MPEG: F1 59.1, Bitrate 11.4 Oracle (UrbanCity): Bitrate 104  Ours: F1 99.2, Bitrate 13.7  Draco: F1 92.3, Bitrate 13.8  MPEG: F1 81.5, Bitrate 16.2 Oracle (KITTI): Bitrate 104  Ours: F1 90.2, Bitrate 5.6  Draco: F1 87.1, Bitrate 5.7  MPEG: F1 61.0, Bitrate 9.5 Oracle (KITTI): Bitrate 104  Ours: F1 98.6, Bitrate 10.1  Draco: F1 96.9, Bitrate 10.1  MPEG: F1 79.9, Bitrate 12.9 Figure 3: Qualitative results on UrbanCity and KITTI. Points are colored by intensity. Here, x(t) i,gt and r(t) i,gt denote the ground-truth values of the node occupancies/intensities, respectively. As mentioned above, minimizing cross-entropy loss is equivalent to our goal of reducing expected bitrate of the point cloud stream. 2.7 Discussion and Related Works Our approach belongs to a family of point cloud compression algorithms based on tree data struc- tures [15, 2, 16, 17, 18, 19, 20, 1, 20, 21, 22, 23, 24]. Tree-based algorithms are advantageous since they use spatial-partioning data structures that can efﬁciently represent sparse and non-uniformly dense 3D point clouds. Two notable examples are Google’s Draco [2] and the MPEG anchor [25], which use a KD-tree codec [ 15] and an octree codec [ 1] respectively. To exploit temporal redun- dancies, the MPEG anchor encodes successive point clouds as block-based rigid transformations of previous point clouds; this, however, narrows its usefulness to scenes with limited motion. Moreover, these prior works use simple entropy models that do not fully exploit redundant information hidden in LiDAR point clouds; e.g., repetitive local structures, objects with strong shape priors, etc. In contrast, we use a learned entropy model to directly capture these dependencies. Our approach is also related to work in deep point cloud compression [ 4, 5, 26, 27, 28, 29, 6]. In particular, both our approach and the prior state-of-the-art [6] use deep entropy models that operate on octree structures directly. However, they do not model temporal redundancies between successive point clouds and compress LiDAR geometry only. In this work, we propose a uniﬁed framework that aggregates spatio-temporal context to jointly compress both LiDAR geometry and intensity. Finally, our work is inspired by recent successes in deep image compression [30, 31, 32, 33, 34, 35, 36] and video compression [37, 38, 39, 40, 41, 42, 43], many of which use deep entropy models. 3 Experimental Evaluation We evaluate our LiDAR compression method on two large-scale datasets. Holding reconstruction quality equal, our framework for joint geometry and intensity compression achieves a 7–17% and 6– 19% bitrate reduction over OctSqueeze [6], the prior state-of-the-art in deep point cloud compression, on UrbanCity and SemanticKITTI. Holding bitrate equal, our method’s reconstructions also have a smaller realism gap on downstream tasks. 6Spatial Bitrate O T B CC D = 12 D = 14 D = 16 ✓ 2.91 8.12 14.16 ✓ ✓ 2.87 8.04 14.08 ✓ ✓ ✓ 2.72 7.90 13.95 ✓ ✓ ✓ ✓ 2.55 7.64 13.79 Table 1: Abalation study of occupancy entropy model on UrbanCity. O, T, and B stand for using past oc- cupancy bytes, top-down aggregated features, and bottom-up aggregated features. CC indicates using continuous conv. D stands for the octree’s max depth. Intensity Bitrate Encoder P D = 12 D = 14 D = 16 zlib[45] 2.42 4.79 5.23 MLP ✓ 2.31 4.62 5.01 CC ✓ 2.13 4.30 4.68 Table 2: Ablation study of intensity entropy model on SemanticKITTI. zlib is an off-the-shelf library [45]; MLP is our model without continuous conv.; and CC is our ﬁnal model. P stands for using past intensity information. D stands for the octree’s max depth. 3.1 Experimental Details We validate the performance of our approach on two datasets: UrbanCity [7] and SemanticKITTI [8]. UrbanCity: UrbanCity is a large-scale dataset collected by a ﬂeet of self-driving vehicles in several cities across North America [7]. Every sequence consists of 250 Velodyne HDL-64E LiDAR sweeps sampled at 10Hz, each containing a 3D point cloud (as 32-bit ﬂoats) and their intensity values (as 8-bit unsigned integers). The average size of each sweep is 80,156 points. We train our entropy models on 5000 sequences and evaluate on a test set of 500. Every sweep in UrbanCity is annotated with per-point semantic labels for the vehicle, pedestrian, motorbike, road, and background classes, as well as bird’s eye view bounding boxes for the ﬁrst three classes. We use these labels to perform downstream perception experiments on the same train/test split. SemanticKITTI: We also conduct compression and downstream perception experiments on Se- manticKITTI [8], which enhances the KITTI [44] dataset with dense semantic labels for each LiDAR sweep. It consists of 22 driving sequences containing a total of 43,552 Velodyne HDL-64E LiDAR sweeps sampled at 10Hz. The average size of each sweep is 120,402 points. In our experiments, we use the ofﬁcial train/test splits: sequences 00 to 10 (except for 08) for training and sequences 11 to 21 to evaluate reconstruction quality. Since semantic labels for the test split are unavailable, we evaluate downstream tasks on the validation sequence 08. Baselines: We compare against a number of state-of-the-art LiDAR compression algorithms: Huang et al.’s deep octree-based method (OctSqueeze) [6], Google’s KD-tree based method (Draco) [2], Mekuria et al.’s octree-based MPEG anchor (MPEG Anchor) [1]1, and MPEG TMC132. From discussions with the authors, “MPEG Anchor” in [ 6] is a custom implementation that uses an empirical histogram distribution to compress octree occupancy symbols; we report this baseline as Octree. As OctSqueeze and Octree do not compress LiDAR intensities, we augment them with an off-the-shelf lossless compression algorithm [45]. In particular, we ﬁrst assign an intensity to each encoded point based on the intensity of its nearest neighbour in the original point cloud. Then, we compress the resulting bytestream. For MPEG Anchor, we use the built-in PCL color coder in the authors’ implementation, which encodes the average intensity at each leaf node in the octree with range coding. Similarly, for Draco and MPEG TMC13, we use their built-in attributes coders. We also compare against a video compression based algorithm using LiDAR’s range image representation (MPEG Range). As this baseline was uncompetitive, we report its results in the supplementary. Implementation Details: In our experiments, we construct octrees over a 400m ×400m ×400m region of interest centered on the ego-vehicle. By varying the octree’s maximum depth from 11 to 16, we can control our method’s bitrate-distortion tradeoff, with spatial quantization errors ranging from 9.75cm (at depth 11) to 0.3cm (at depth 16). We train and evaluate individual entropy models at each depth from 11 to 16, which we found gave the best results. Our models use Kans = 4 rounds of aggregation and k= 5 nearest neighbors for continuous convolution. Our method is implemented in PyTorch [46] and we use Horovod [47] to distribute training over 16 GPUs. We train our models over 150,000 steps using the Adam optimizer [48] with a learning rate of 1e−4 and a batch size of 16. 1We use the authors’ implementation:https://github.com/cwi-dis/cwi-pcl-codec . 2MPEG TMC13 reference implementation: https://github.com/MPEGGroup/mpeg-pcc-tmc13 75 10 15 20 25 30 Overall Bits Per Point (BPP) 15 20 25 30 35 40Mean IOU Bitrate vs. IOU (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 40 50 60 70 80 90Mean IOU Bitrate vs. IOU (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 83.00 83.25 83.50 83.75 84.00 84.25Vehicle AP@70% IOU Bitrate vs. Vehicle AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 76 77 78Pedestrian AP@50% IOU Bitrate vs. Pedestrian AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle 10 20 30 Overall Bits Per Point (BPP) 67 68 69 70Motorbike AP@50% IOU Bitrate vs. Motorbike AP (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 Oracle Figure 4: Bitrate vs. downstream task performance. Top: mean IoU for semantic segmentation on SemanticKITTI (left) and UrbanCity (right). Bottom: AP for vehicle, pedestrian, and motorbike detection on UrbanCity. Oracle IOU: 37.5, Bitrate: 104.0  Ours IOU: 36.8, Bitrate: 13.1  Oracle AP: 88.5, Bitrate: 104.0  Ours AP: 88.5, Bitrate: 9.9 Figure 5: Left: Semantic segmentation on SemanticKITTI. Right: Object detection on UrbanCity. Even at very low bitrates, our reconstructions have a minimal realism gap for downstream tasks. Metrics: We report reconstruction metrics in terms of F1 score, point-to-point (D1) Chamfer distance [ 6], and point-to-plane (D2) PSNR [ 49]. Point-to-point and point-to-plane errors are standard MPEG metrics [25]. But whereas they measure reconstruction quality in terms of geometry only, F1 measures this in terms of both geometry and intensity. Reconstruction metrics are averaged across sweeps and bitrate is the average number of bits used to store a LiDAR point. Following standard practice, we do not count the one-time transmission of network weights since it is negligible compared to the size of long LiDAR streams; e.g. 1 hour. See our supplementary materials for details. 3.2 Results Quantitative Results: In Fig. 2, we report bitratevs. reconstruction quality curves for all competing methods on UrbanCity and SemanticKITTI. The leftmost ﬁgures show the trade-off between overall bitrate vs. F1. Here, we see that our method outperforms the prior state-of-the-art and, holding reconstruction quality equal, achieves a 7–17% (resp., 6–19%) relative reduction in bitrate versus OctSqueeze on UrbanCity (resp., SemanticKITTI). Our model also outperforms MPEG TMC13— the MPEG point cloud compression standard—especially at lower bitrates. The right two ﬁgures show the trade-off between spatial bitrate vs. Chamfer distance and PSNR respectively. Although our method shares a common octree data structure with OctSqueeze ( resp., Octree), and thus have the same reconstruction quality, we achieve a 5–30% ( resp., 15–45%) reduction in spatial bitrates on UrbanCity by additionally exploiting temporal information; similar results also hold in SemanticKITTI. These results validate our uniﬁed framework for geometry and intensity compression using spatial-temporal information. Qualitative Results: In Fig. 3, we show reconstructions from our method, Draco, and MPEG Anchor on UrbanCity and SemanticKITTI. At similar bitrates, our method yields higher quality reconstructions than the competing methods in terms of both geometry and intensity. For example, from the ﬁrst and third rows of Fig. 3, we can see that our method produces faithful reconstructions even at high compression rates. In contrast, Draco and MPEG Anchor produce apparent artifacts. 8Ablation Studies: We perform two ablation studies on our occupancy and intensity entropy models. In Tab. 1, we ablate how to incorporate past information to lower the entropy of our occupancy model. We start with using the past octree’s occupancy bytes (O) and then progressively add top-down and bottom-up aggregated features (T and B respectively), and ﬁnally continuous convolutions ( CC). We see that, holding reconstruction quality equal, each aspect of our model consistently reduces bitrates. In Tab. 2, we compare three compression methods for the intensity model: the zlib library, a multi-layer perceptron entropy model ( MLP), and our ﬁnal model ( CC). Note that both MLP and CC conditions on context from neighboring points in the past sweep; zlib does not. However, whereas MLP uses context from one neighbor only, CC aggregates context from multiple neighbors via continuous convolutions. Our results show that learning to incorporate past context reduces intensity bitrates by 4–5%, and that this improvement is strengthened to 11–12% by using continuous convolutions to align information across space and time. Please see our supplementary for details. Impact on Downstream Tasks:To study the impact of compression on downstream perception tasks, we ﬁrst train segmentation and detection models on uncompressed LiDAR for SemanticKITTI and UrbanCity. Note that these models use both LiDAR geometry and intensity as input (see supplementary for details). Next, we evaluate the models on LiDAR reconstructions obtained from various compression schemes and report their performance as a function of overall bitrate. For segmentation on SemanticKITTI and UrbanCity, we report mean IOU using voxelized ground truth labels at a 10cm resolution. For detection on UrbanCity, we report AP at 50% IOU for pedestrians and motorbikes and 70% IOU for vehicles. In Fig. 4 and 5, we see that our method’s reconstructions have the smallest realism gap for downstream tasks across all bitrates. This result is especially pronounced for segmentation models, which are more sensitive to ﬁne-grained geometric and intensity details. 4 Conclusion We have presented a novel LiDAR point cloud compression algorithm using a deep entropy model which exploits spatio-temporal redundancies between successive LiDAR point clouds. We showed that we can compress point clouds at identical reconstruction quality to the state-of-the-art while lowering bitrate signiﬁcantly, as well as compress LiDAR intensity values effectively which was not as extensively explored by prior works. Furthermore, we showed our compression can be applied to downstream self-driving perception tasks without hindering performance. Looking forward, we plan to extend our method to jointly compress data streams from entire sensor suites. Broader Impact On an immediate level, our contributions are directly applicable as a data compression algorithm in a novel problem setting: the greater we can maximize the performance of such an algorithm, the more we can reduce the storage cost and space required by point clouds. We hope that this in turn unlocks a milestone towards fulﬁlling our ultimate vision: scaling up the research and deployment of intelligent robots, such as self-driving vehicles, that will revolutionize the safety, efﬁciency, and convenience of our transportation infrastructure. By capturing the 3D geometry of the scene, LiDAR sensors have proven to be crucial in effective and safe prediction/planning of these robots. Currently, LiDAR sensors are not only expensive due to the upfront cost, but also due to the recurring costs of the massive quantities of data they generate. Good point cloud and LiDAR compression algorithms will thus help to democratize the usage of LiDAR by making it more feasible for people to own and operate. Perhaps just as importantly, our responsibility as researchers in a novel problem area led us to carefully consider the downstream impacts of such a compression algorithm—if the primary usage of LiDAR currently is on perception tasks, such as detection and segmentation, then we need to demonstrate how compression bitrate affects perception performance, helping the community determine the acceptable bitrate at which compression can be used for safe vision and robotics applications. We hope that our work inspires the community to further advance sensor compression in addition to the traditional image and video settings. References [1] Rufael Mekuria, Kees Blom, and Pablo Cesar. Design, implementation and evaluation of a point cloud codec for tele-immersive video. In IEEE IEEE Transactions on Circuits and Systems for 9Video Technology, 2016. [2] Google. Draco 3d data compresison. https://github.com/google/draco, 2017. [3] Chenxi Tu, E. Takeuchi, C. Miyajima, and K. Takeda. Compressing continuous point cloud data using image compression methods. In 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), 2016. [4] Chenxi Tu, Eijiro Takeuchi, Alexander Carballo, and Kazuya Takeda. Real-time streaming point cloud compression for 3d lidar sensor using u-net. IEEE Access, 2019. [5] C. Tu, E. Takeuchi, A. Carballo, and K. Takeda. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 International Conference on Robotics and Automation (ICRA), 2019. [6] Lila Huang, Shenlong Wang, Kelvin Wong, Jerry Liu, and Raquel Urtasun. Octsqueeze: Octree- structured entropy model for lidar compression. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2020. [7] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end perception and prediction with tracking in the loop, 2020. [8] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In IEEE International Conference on Computer Vision, ICCV), 2019. [9] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 1948. [10] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009. [11] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Generalized belief propagation. In Proceedings of the 13th International Conference on Neural Information Processing Systems, NIPS’00, page 668–674, Cambridge, MA, USA, 2000. MIT Press. [12] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3391–3401. Curran Associates, Inc., 2017. [13] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep parametric continuous convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2018. [14] G. Nigel Martin. * range encoding: an algorithm for removing redundancy from a digitised message. 1979. [15] O. Devillers and P. . Gandoin. Geometric compression for interactive transmission. In Proceed- ings Visualization 2000. VIS 2000 (Cat. No.00CH37145), 2000. [16] Ruwen Schnabel and Reinhard Klein. Octree-based point-cloud compression. In Proceedings of the 3rd Eurographics / IEEE VGTC Conference on Point-Based Graphics, SPBG’06, page 111–121, Goslar, DEU, 2006. Eurographics Association. [17] Yan Huang, Jingliang Peng, C.-.C. Jay Kuo, and M. Gopi. A generic scheme for progressive point cloud coding. IEEE Transactions on Visualization and Computer Graphics, 2008. [18] Cha Zhang, Dinei Florencio, and Charles Loop. Point cloud attribute compression with graph transform. IEEE - Institute of Electrical and Electronics Engineers, October 2014. [19] Dorina Thanou, Philip Chou, and Pascal Frossard. Graph-based motion estimation and compen- sation for dynamic 3d point cloud compressio,. pages 3235–3239, 09 2015. [20] Ricardo L. de Queiroz and Philip A. Chou. Compression of 3d point clouds using a region- adaptive hierarchical transform. Trans. Img. Proc., 25(8):3947–3956, August 2016. [21] Diogo C. Garcia and Ricardo L. de Queiroz. Context-based octree coding for point-cloud video. In 2017 IEEE International Conference on Image Processing (ICIP), 2017. [22] Yiting Shao, Zhaobin Zhang, Zhu Li, Kui Fan, and Ge Li. Attribute compression of 3d point clouds using laplacian sparsity optimized graph transform. 2017 IEEE Visual Communications and Image Processing (VCIP), pages 1–4, 2017. 10[23] Diogo C. Garcia and Ricardo L. de Queiroz. Intra-frame context-based octree coding for point- cloud geometry. In 2018 25th IEEE International Conference on Image Processing (ICIP), 2018. [24] Diogo C. Garcia, Tiago A. Fonseca, Renan U. Ferreira, and Ricardo L. de Queiroz. Geom- etry coding for dynamic voxelized point clouds using octrees and multiple contexts. IEEE Transactions on Image Processing, 2020. [25] S. Schwarz, M. Preda, V . Baroncini, M. Budagavi, P. Cesar, P. A. Chou, R. A. Cohen, M. Kri- voku´ca, S. Lasserre, Z. Li, J. Llach, K. Mammou, R. Mekuria, O. Nakagami, E. Siahaan, A. Tabatabai, A. M. Tourapis, and V . Zakharchenko. Emerging mpeg standards for point cloud compression. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2019. [26] Maurice Quach, Giuseppe Valenzise, and Frederic Dufaux. Learning convolutional transforms for lossy point cloud geometry compression, 2019. [27] Jianqiang Wang, Hao Zhu, Z. Ma, Tong Chen, Haojie Liu, and Qiu Shen. Learned point cloud geometry compression. ArXiv, abs/1909.12037, 2019. [28] Wei Yan, Yiting Shao, Shan Liu, Thomas H. Li, Zhu Li, and Ge Li. Deep autoencoder-based lossy geometry compression for point clouds. CoRR, abs/1905.03691, 2019. [29] Tianxin Huang and Yong Liu. 3d point cloud geometry compression on deep learning. In Proceedings of the 27th ACM International Conference on Multimedia, MM ’19, page 890–898, New York, NY , USA, 2019. Association for Computing Machinery. [30] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] David Minnen, Johannes Ballé, and George Toderici. Joint autoregressive and hierarchical priors for learned image compression. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 10794–10803, 2018. [32] George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and Michele Covell. Full resolution image compression with recurrent neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5435–5443. IEEE Computer Society, 2017. [33] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 4394–4402. IEEE Computer Society, 2018. [34] Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár. Lossy image compression with compressive autoencoders. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [35] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Practical full resolution learned lossless image compression. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10629–10638. Computer Vision Foundation / IEEE, 2019. [36] James Townsend, Thomas Bird, and David Barber. Practical lossless compression with latent variables using bits back coding. In International Conference on Learning Representations, 2019. [37] Chao-Yuan Wu, Nayan Singhal, and Philipp Krähenbühl. Video compression through image interpolation. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII, volume 11212 of Lecture Notes in Computer Science, pages 425–440. Springer, 2018. [38] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao. DVC: an end-to-end deep video compression framework. In IEEE Conference on Computer Vision and 11Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 11006–11015. Computer Vision Foundation / IEEE, 2019. [39] Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander G. Anderson, and Lubomir D. Bourdev. Learned video compression. In 2019 IEEE/CVF International Conference on Com- puter Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 3453– 3462. IEEE, 2019. [40] AmirHossein Habibian, Ties van Rozendaal, Jakub M. Tomczak, and Taco Cohen. Video compression with rate-distortion autoencoders. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 7032–7041. IEEE, 2019. [41] Abdelaziz Djelouah, Joaquim Campos, Simone Schaub-Meyer, and Christopher Schroers. Neural inter-frame compression for video coding. In The IEEE International Conference on Computer Vision (ICCV), October 2019. [42] Ren Yang, Fabian Mentzer, Luc Van Gool, and Radu Timofte. Learning for video compression with hierarchical quality and recurrent enhancement, 2020. [43] Jianping Lin, Dong Liu, Houqiang Li, and Feng Wu. M-lvc: Multiple frames prediction for learned video compression, 2020. [44] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. [45] Jean loup Gailly and Mark Adler. zlib. https://github.com/madler/zlib, 1995. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, 2019. [47] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799, 2018. [48] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [49] D. Tian, H. Ochimizu, C. Feng, R. Cohen, and A. Vetro. Geometric distortion metrics for point cloud compression. In 2017 IEEE International Conference on Image Processing (ICIP), 2017. [50] Igor Pavlov. Lzma. https://sourceforge.net/p/scoremanager/discussion/457976/ thread/c262da00/, 1998. [51] Julian Seward. bzip2. https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/ source/src/util/compress/bzip2/README, 1996. [52] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A modern library for 3D data processing. arXiv:1801.09847, 2018. [53] Chris Zhang, Wenjie Luo, and Raquel Urtasun. Efﬁcient convolutions for real-time semantic segmentation of 3d point clouds. In 2018 International Conference on 3D Vision, 3DV 2018, Verona, Italy, September 5-8, 2018, pages 399–408. IEEE Computer Society, 2018. [54] Yuxin Wu and Kaiming He. Group normalization. CoRR, abs/1803.08494, 2018. [55] Bin Yang, Wenjie Luo, and Raquel Urtasun. PIXOR: real-time 3d object detection from point clouds. CoRR, abs/1902.06326, 2019. 12A Additional Experiments A.1 Compression of Leaf Offsets We mention in Sec. 2.1 of the main paper that we do not attempt to compress the leaf offsets from the octree. The reason is that we experimented with a few compression baselines and were not able to obtain a bitrate improvement over the uncompressed leaf offsets. We experiment with the zlib [45], LZMA [50], and bzip2 [51] compression algorithms on the leaf offset stream from UrbanCity. The results are shown in Tab. 3; we surprisingly found that in all cases the compressed string was longer than the uncompressed one. Uncompressed zlib [45] LZMA [50] bzip2 [51] Avg. Bytes / Sweep 102429.31 102468.93 102493.84 103242.28 Table 3: Comparison of compression algorithms on leaf offsets from UrbanCity, in terms of average bytes per sweep. There can be room for future work in entropy modeling the leaf offsets, but our current hypothesis is that since the intermediate octree nodes already encode the shared bits between points, the leaf offsets represent residual bits that can be considered “higher-frequency” artifacts (similar to residual frames in video compression), and are therefore harder to compress. A.2 Using a Range Image Representation We mention in Sec. 3.1 of the main paper that we designed a range image-based compression baseline. Towards this goal, we ﬁrst converted point cloud streams in UrbanCity and KITTI into range image representations, which store LiDAR packet data into a 2D matrix. We consider two possible range image representations. The ﬁrst contains dimensions Hlid ×Wazm, where the height dimension represents the separate laser ID’sof the LiDAR sensor, and the width dimension represents the discretized azimuth bins between -180 ◦and 180◦. Each pixel value represents the distance returned by the laser ID at the speciﬁc azimuth angle. Such a representation requires sufﬁcient auxiliary calibration and vehicle information in order to reconstruct the points in Euclidean space— for instance, a separate transform matrix per laser and velocity information to compensate for rolling shutter effects. We use this representation for UrbanCity because we have access to most required information; unfortunately, not every log contains detailed calibration or precise velocity information, requiring us to use approximations. The second representation simply projects the spatial coordinates of the point cloud sweep into the coordinate frame of the sensor, and does not require a map between laser ID and Euclidean space. Such an image contains dimensions Hpitch ×Wazm, where the height dimension now represents discretized pitch angles; each pixel value now represents the distance of a given point from the sensor frame at a given pitch and azimuth bin. We use this representation for our KITTI point clouds, since the dataset does not provide detailed laser calibration information. We explore both geometry-only and geometry + intensity representations. Spatial positions are encoded in the 8-bit R,G channels of the png image (16 bits total). If intensity is encoded, it is encoded in the B channel. We run H.264 on the png image sequence as our compression algorithm. We evaluate on the same reconstruction metrics: point-to-point Chamfer distance and point-to-plane PSNR (geometry), and F1 score (geometry + intensity). We show here in Fig. 6, that the results were uncompetitive—the range image representation under- performs other baselines and our approach on every evaluation metric. We observe that even the “lossless” representation (the right-most point on the curves) does not yield perfect reconstruction metrics. This can be surprising for the laser ID representation in UrbanCity. But we hypothesize that the errors come from approximations of the true calibration values (which are not obtainable for every log), as well as the velocity used in rolling shutter compensation—we found that small perturbations in these calibration values yield a large variance in reconstruction quality and metrics. 130 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 6: Bitrate vs. reconstruction quality curves on UrbanCity (top) and KITTI (bottom). From left-to-right: F1 with τgeo = 10cmand τint = 0(↑), point-to-point chamfer distance (↓), point-to-plane PSNR (↑). B Additional Architecture Details In this section we provide additional architecture details of our octree occupancy and intensity entropy models (Secs. 2.3 and 2.4 in main paper). We also provide architecture details of the models used in the ablation studies of the occupancy and intensity model (Tab. 1, Tab. 2 in main paper). B.1 Occupancy Entropy Model Ancestral Node Dependence: The context feature ci consists of the octree level of the current node (1– 16), spatial location of the node’s octant(x,y,z ), octant index of the node relative to its parent (0–8), and parent occupancy byte (0–255), as well as occupancy byte in the corresponding node in the previous octree (0–255 if exists, 0 otherwise). The initial feature extractor is a 4-layer MLP with fully-connected (fc) layers and intermediate ReLU activations. The hidden layer dimension is 128. Then, every aggregation round consists of a 2-layer fc/ReLU MLP with a 256-dimensional input (concatenating with the ancestor feature), and a hidden dimension of 128. We set the number of aggregation rounds, Kans, to 4. Temporal Octree Dependence: The top-down pass to generate h(t−1) j has essentially the same architecture as the ancestral node dependence module above. The one difference is that each context feature additionally includes the “ground-truth” occupancy byte of each node, since each node in sweep t−1 has already been decoded. Moreover, each hidden dimension is 64 instead of 128. Next, recall that the bottom-up aggregation pass has the following formulation: g(t−1) j = fagg,1(h(t−1) j + ∑ c∈child(j) fagg,2(g(t−1) c )) Here, fagg,2 is a 2-layer fc/ReLU MLP taking a 64-dim input and outputting a 32-dim intermediate embedding. fagg,1 is a 2-layer fc/ReLU MLP taking a (32 + 64)-dim embedding (child embedding + top-down embedding), and outputting a 64-dim embedding for the current node j. The bottom-up pass is run starting from the lowest level D(where there are no children) back up to level 0. Spatio-Temporal Aggregation and Entropy Header: Recall that the continuous convolution layer has the formulation hi = ∑ j∈N(i) σ(pj −pi)hj where N(i) is the i-th node’sk-nearest neighbors in sweep t−1, at the same octree level as node i, and pi is the 3D position of each node. Here, σis a learned kernel function, and it is parameterized by 14an MLP, inspired by [13]. The MLP contains 3 fc/ReLU layers (no ReLU in last layer), with output dimensions 16, 32, and 64 respectively. The continuous conv layer produces the warped feature g(t) i,st. The warped feature g(t) i,st and ancestral feature h(t) i are aggregated through a ﬁnal, 4-layer fc/ReLU MLP with hidden dim 128. The prediction header outputs a softmaxed, 256-dim vector of occupancy predictions. B.2 Intensity Entropy Model The input to the intensity entropy MLP consists of the k-nearest neighbor intensities in sweep t−1: {r(t−1) j }j∈N(i). We set k = 5. In addition to the raw intensity value, we include the following features per r(t−1) j : spatial (x,y,z ) position ∈R3, delta vector to current point ∈R3, and 1-D distance value. Hence each point contains an 8-dimensional feature. Each feature per r(t−1) j is then independently given to a 4-layer MLP, consisting of fc layers and ReLU activations. The dimension of each hidden layer is 128. Then, the koutput features are input to a continuous convolution layer to produce a single 128-dimensional embedding. The kernel function σof the continuous conv. is parameterized with the same MLP as the one used in spatio-temporal aggregation in the occupancy model. The ﬁnal predictor is a fc layer and softmax with a 256-dim. output. B.3 Ablation Study Architectures We ﬁrst describe the architectures of the occupancy ablation in Tab. 1 of the main paper. • O uses the past occupancy byte to model temporal dependence. The byte is taken from the corresponding node in the previous octree if it exists; if it does not, the feature is zeroed out. This past occupancy byte is then appended to the context feature ci (along with parent occupancy byte, octree level, etc.) and fed to the ancestral dependence module. There is no temporal octree dependence module or spatio-temporal aggregation; the ﬁnal prediction header is directly attached to the ancestral feature. • O,T includes the temporal octree dependence module, but removes the bottom-up pass. Hence the ﬁnal feature produced from this module is h(t−1) j (as opposed to g(t−1) j ). There does not exist a spatio-temporal aggregation module using continuous convolutions to produce an embedding for every nodei. Instead, we use a simpler “exact matching” heuristic similar to including the occupancy bytes— h(t−1) j will only be included as a feature for node iin sweep t, if node jcorresponds to the same octant in sweep (t−1) as node iin sweep t. If there is no exact correspondence, the feature is zeroed out. • O,T,B includes the full temporal octree dependence module, including the bottom-up pass to produce g(t−1) j . As with the above, we do not include our spatio-temporal aggregation module but rather use the exact matching heuristic to include g(t−1) j in the corresponding nodes iin sweep tonly if the correspondence exists. • O,T,B,CC includes our full model, including using spatio-temporal aggregation with continuous convolutions to produce an embedding feature for every node i. We now describe the architectures of the intensity ablation in Tab. 2 of the main paper. • MLP only utilizes context from one neighbor in sweep t−1. First, the nearest neighbor to node iis obtained in sweep t−1. We take the neighbor’s corresponding intensity, the delta vector to the current position ∈R3, and 1-D distance value as inputs, and feed it through a 4-layer fc/ReLU MLP and a ﬁnal softmax predictor head to output 256-dim probabilities. • CC contains the full intensity model with continuous convolutions. For architecture details see B.2. 150 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 0) Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 5) Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 5)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 0)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 5)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 10)  Bitrate v. F1 (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 0) Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 5) Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 5cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 5)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 10cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 0)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 5)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 10 20 30 Overall Bits Per Point 0.2 0.4 0.6 0.8 1.0F1 Score ( geo = 15cm, int = 10)  Bitrate v. F1 (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 7: Bitrate vs. F1 curves on UrbanCity (top three rows) and KITTI (bottom three rows). We report F1 across various spatial and intensity thresholds: τgeo ∈{5cm,10cm,15cm}and τint ∈{0,5,10}. 16C Additional Experiment Details C.1 Reconstruction Metrics In Sec. 3.3 of the main text, we report reconstruction quality in terms of three metrics: F1 score, point-to-point Chamfer Distance [6], and point-to-plane PSNR [49]. In the following, we explain each metric in detail. Let P= {(pi,ri)}N i=1 be an input LiDAR point cloud, where each pi ∈R3 denotes a point’s spatial coordinates andri ∈{0,..., 255}its intensity. Furthermore, let ˆP= {(ˆpj,ˆrj)}M j=1 be its reconstruction, where ˆpj and ˆrj are similarly deﬁned. Our ﬁrst metric is an F1 score that measures reconstruction quality in terms of both geometry and intensity: F1(P, ˆP) = 2 ×# true positives 2 ×# true positives + # false positives + # false negatives (6) where a reconstructed point (ˆpj,ˆrj) ∈ ˆPis a true positive if and only if there exists a point (pi,ri) ∈P such that ∥pi −ˆpj∥2 ≤τgeo and |ri −ˆrj|≤ τint. False positivesare the reconstructed points in ˆPthat are not true positives, and false negativesare the original points in Pfor which no reconstructed point is a true positive. In our experiments, we use τgeo = 10cm and τint = 0, and we report F1 as a function of overall bitrates; i.e., the number of bits to store p and r. We further report the F1 score for τgeo ∈{5cm,10cm,15cm}and τint ∈{0,5,10}in Fig. 7. Following the MPEG standards, we also use two standard metrics that measure reconstruction quality in terms of geometry only [ 25]. We report these metrics as a function of spatial bitrates; i.e., the number of bits to store p. The ﬁrst such metric measures the point-to-point error between the original point cloud Pand the reconstructed point cloud ˆP; this metric is often called the D1 error in the MPEG standards. In our paper, we report this metric as a symmetric Chamfer distance: CDsym(P, ˆP) = max { CD(P, ˆP),CD( ˆP,P) } (7) where CD(P, ˆP) = 1 |P| ∑ pi∈P min ˆpj∈ˆP ∥pi −ˆpj∥2 (8) The second metric measures the point-to-place error between the original point cloud Pand the reconstructed point cloud ˆP; this metric is often called the D2 error in the MPEG standards. In our paper, we report this metric in terms of its peak signal-to-noise ratio (PSNR): PSNR(P, ˆP) = 10 log10 3r2 max{MSE(P, ˆP),MSE(P, ˆP)} (9) where MSE(P, ˆP) = 1 |P| ∑ i((pi −ˆpi) ·ˆni)2 is the mean squared point-to-plane distance, ˆni is the normal vector on ˆpi, ˆpi = argminˆp∈ˆP∥pi −ˆp∥2 2 is pi’s nearest neighbor point in ˆP, and r is the peak constant value. We estimate the normal ni at each point pi ∈P using the Open3D function estimate_normals with k= 12 nearest neighbors [52], and we compute the normal ˆni corresponding to each point ˆpi ∈ ˆPby taking the normal of its nearest neighbor in the original point cloud P. Following the MPEG standard, for each dataset, we compute ras the maximum nearest neighbor distance among all point clouds in the dataset: r= max P max pi∈P min j̸=i ∥pi −pj∥2 (10) For UrbanCity, we use r= 98.69 and for SemanticKITTI, we use r= 59.70. For completeness, we also report the point-to-point error in terms of its peak signal-to-noise ratio and the point-to-plane error as a symmetric Chamfer distance in Fig. 8. C.2 Downstream Experiment Details In this section, we provide additional details for our downstream perception experiments. 170 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6 0.8 1.0Point-to-Point Chamfer Distance Bitrate v. D1 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 40 60 80 100Point-to-Point PSNR Bitrate v. D1 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Plane Chamfer Distance Bitrate v. D2 CD (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (UrbanCity) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.2 0.4 0.6Point-to-Point Chamfer Distance Bitrate v. D1 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 40 60 80 100Point-to-Point PSNR Bitrate v. D1 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 0.0 0.1 0.2 0.3Point-to-Plane Chamfer Distance Bitrate v. D2 CD (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range 0 5 10 15 20 Spatial Bits Per Point 60 80 100Point-to-Plane PSNR Bitrate v. D2 PSNR (KITTI) Ours OctSqueeze Octree Draco MPEG Anchor MPEG TMC13 MPEG Range Figure 8: Bitrate vs. reconstruction curves on UrbanCity (top two rows) and KITTI (bottom two rows). We report point-to-point (D1) and point-to-plane (D2) errors in terms of Chamfer distances (left) and PSNR (right). C.2.1 Semantic Segmentation We use a modiﬁed version of the LiDAR semantic segmentation model described in [6]. Input Representation: Our model takes as input T bird’s eye view (“BEV”) occupancy grids of the past T input LiDAR point clouds {P(t−T+1),..., P(t)}, stacked along the height dimension (i.e., the z-axis). By treating the height dimension as multi-dimensional input features, we have a compact input representation on which we can use 2D convolutions [ 53]. Each voxel in the occupancy grids store the average intensity value of the points occupying its volume, or 0 if it contains no points. We use a region of interest of 160m ×160m ×5m centered on the ego-vehicle, T = 5 past LiDAR point clouds, and a voxel resolution of 0.15625cm, yielding an input volume x of size (T ×Z) ×W ×H = 160 ×1024 ×1024. Architecture Details: Our model architecture consists of two components: (1) a backbone feature extractor; and (2) a semantic segmentation head. The backbone feature extractor CNNBEV is a feature pyramid network based on the backbone architecture of [7]: fBEV = CNNBEV(x) (11) where fBEV ∈RCBEV×W/4×H/4 and CBEV = 256. 18The semantic segmentation head CNNsem consists of four 2D convolution blocks with 128 hidden channels 3, followed by a 1 ×1 convolution layer: fsem = CNNsem(fBEV) (12) where fsem ∈R(K×Z)×W/4×H/4 and Kis the number of classes plus an additional ignore class. To extract per-point predictions, we ﬁrst reshape fsem into a K×Z×W/4 ×H/4 logits tensor, then use trilinear interpolation to extract per-point K-dimensional logits, and ﬁnally apply softmax. Training Details: We use the cross-entropy loss to train our semantic segmentation model. For SemanticKITTI, we follow [8] and reweight the loss at each point by the inverse of the frequency of its ground truth class; this helps to counteract the effects of severe class imbalance. Moreover, we use data augmentation by randomly scaling the point cloud by s∼Uniform(0.95,1.05), rotating it by θ∼Uniform(−π/4,π/4), and reﬂecting it along the xand y-axes. We use the Adam optimizer [48] with a learning rate of 4e−4 and a batch size of 12, and we train until convergence. C.2.2 Object Detection We use a modiﬁed version of the LiDAR object detection model described in [6]. It largely follows the same architecture as our semantic segmentation model, with a few modiﬁcations to adapt it for object detection. We describe these modiﬁcations below. Architecture Details: Our object detection model consists of two components: (1) a backbone feature extractor; and (2) an object detection head. The backbone feature extractor here shares an identical architecture to that of the semantic segmentation model. The object detection head consists of four 2D convolution blocks with 128 hidden channels followed by a 1 ×1 convolution layer to predict a bounding box bi,k and detection score αi,k for every BEV pixel iand class k. Each bounding box bi,k is parameterized by (∆x,∆y,log w,log h,sin θ,cos θ), where (∆x,∆y) are the position offsets to the object’s center,(w,h) are the width and height of its bounding box, and θis its heading angle. To remove duplicate bounding boxes predictions, we use non-maximum suppression. Training Details: We use a combination of classiﬁcation and regression losses to train our de- tection model. In particular, for object classiﬁcation, we use a binary cross-entropy loss with online hard negative mining, where positive and negative BEV pixels are determined based on their distance to an object center [ 55]. For bounding box regression, we use a smooth ℓ1 loss on ∆x,∆y,log w,log h,sin θ,cos θ. We use the Adam optimizer [48] with a learning rate of 4e−4 and a batch size of 12, and we train until convergence. D Additional Qualitative Results In Fig. 9 and 10, we compare the reconstruction quality of our method versus Draco [2] and MPEG anchor [1]. Then, in Figs. 11, 12, and 13, we visualize results from semantic segmentation and object detection on SemanticKITTI and UrbanCity. As shown in these ﬁgures, our compression algorithm yields the best reconstruction quality at comparable or lower bitrates than the competing methods. E Change Log ArXiv v2: We updated our reconstruction metrics to use the standard MPEG deﬁnitions [ 25]. Furthermore, we added bitrate vs. F1 curves for a number of spatial and intensity thresholds. We also updated our Draco baseline to use its built-in attributes coder. 3Each 2D convolution block consists of a 3 ×3 convolution, GroupNorm [54], and ReLU. 19Oracle (UrbanCity): Bitrate 104.0  Ours: F1 92.9 Bitrate 10.0 Draco: F1 85.1 Bitrate 10.9  MPEG: F1 53.4 Bitrate 10.4 Figure 9: Qualitative results on UrbanCity. Points are colored by intensity. Oracle (KITTI): Bitrate 104.0  Ours: F1 90.8 Bitrate 5.6 Draco: F1 89.2 Bitrate 5.8  MPEG: F1 69.2 Bitrate 11.0 Figure 10: Qualitative results on SemanticKITTI. Points are colored by intensity. 20Oracle (KITTI): IOU 31.3 Bitrate 104.0  Ours: IOU 29.5 Bitrate 6.7 Draco: IOU 29.0 Bitrate 8.4  MPEG: IOU 26.3 Bitrate 13.0 Figure 11: Semantic segmentation results on SemanticKITTI. IOU is averaged over all classes. Oracle (KITTI): IOU 97.2 Bitrate 104.0  Ours: IOU 94.3 Bitrate 19.8 Draco: IOU 88.3 Bitrate 20.1  MPEG: IOU 85.2 Bitrate 20.5 Figure 12: Semantic segmentation results on UrbanCity. IOU is averaged over all classes. 21Oracle (KITTI): AP 90.6 Bitrate 104.0  Ours: AP 90.4 Bitrate 14.5 Draco: AP 89.4 Bitrate 16.6  MPEG: AP 91.2 Bitrate 17.6 Figure 13: Object detection results on UrbanCity. AP is averaged over the vehicle, pedestrian, and motorbike classes. 22",
      "meta_data": {
        "arxiv_id": "2011.07590v2",
        "authors": [
          "Sourav Biswas",
          "Jerry Liu",
          "Kelvin Wong",
          "Shenlong Wang",
          "Raquel Urtasun"
        ],
        "published_date": "2020-11-15T17:41:14Z",
        "pdf_url": "https://arxiv.org/pdf/2011.07590v2.pdf",
        "github_url": "https://github.com/MPEGGroup/mpeg-pcc-tmc13"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper presents MuSCLE, a novel deep learning-based compression algorithm for LiDAR sensor data streams. Its main contributions are: 1) Exploiting spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values. 2) Proposing a novel conditional entropy model that predicts octree symbol probabilities by considering coarse-level geometry and information from previous sweeps' geometry and intensity. 3) Unifying geometry and attribute compression within the same framework. Experiments show significant bitrate reduction (7–17% on UrbanCity, 6–19% on SemanticKITTI) over prior state-of-the-art methods while maintaining reconstruction quality and superior performance on downstream perception tasks.",
        "methodology": "The proposed method utilizes a comprehensive framework for lossy compression of LiDAR point cloud streams. It first quantizes and encodes point spatial coordinates into an octree representation, where intermediate nodes store 8-bit child occupancies and leaf nodes store point offsets and intensities. A deep conditional entropy model then approximates the joint distribution of point clouds, leveraging a 1st-order Markov assumption (current sweep depends on the previous). This model is factored into two parts: 1) An Occupancy Entropy Model, which factors probabilities of octree symbols based on ancestral nodes of the current sweep and information from the previous sweep. This includes an ancestral node dependence module (recurrent network over ancestral octree path) and a temporal octree dependence module (two-stream feature backbone for previous octree, aggregating top-down and bottom-up features), with spatio-temporal aggregation using continuous convolutions to process previous octree features. 2) An Intensity Entropy Model, which compresses point intensities by leveraging temporal correlations across consecutive timestamps, also employing continuous convolutions to incorporate context from spatially proximal neighbors in the previous sweep. Finally, the learned probability distributions from these models are fed into a lossless entropy coding algorithm (range coding) to produce the compressed bitstream.",
        "experimental_setup": "The method was evaluated on two large-scale datasets: UrbanCity (5000 training, 500 test sequences, 250 Velodyne HDL-64E LiDAR sweeps/sequence, avg. 80,156 points/sweep) and SemanticKITTI (training on sequences 00-10 (excl. 08), test on 11-21, validation on 08, 43,552 sweeps total, avg. 120,402 points/sweep). Baselines included OctSqueeze, Google's Draco, MPEG Anchor, and MPEG TMC13, augmented with zlib for intensity compression where not built-in. A range image-based compression (MPEG Range using H.264) was also tested but found uncompetitive. Reconstruction quality was measured using F1 score (geometry and intensity, with τgeo=10cm, τint=0), point-to-point Chamfer distance (D1), and point-to-plane PSNR (D2). Downstream task performance was evaluated for semantic segmentation (mean IoU) and object detection (AP at 50/70% IoU) using models trained on uncompressed data. Octrees were constructed over a 400m x 400m x 400m region with varying max depths (11-16) to control bitrate-distortion. Models were implemented in PyTorch, trained over 150,000 steps using Adam optimizer with 1e-4 learning rate and batch size 16, distributed with Horovod on 16 GPUs.",
        "limitations": "The paper notes that the compression is lossy due to D-dependent octree quantization for spatial coordinates. While intensity compression is assumed lossless if values are bounded and discrete, a loss would be incurred through discretization if they are continuous. The authors also found that leaf offsets from the octree did not contain meaningful patterns for compression, as evidenced by experiments where standard compression algorithms (zlib, LZMA, bzip2) increased the stream size rather than compressing it, hypothesizing that these offsets represent 'higher-frequency' residual bits that are harder to compress.",
        "future_research_directions": "The authors plan to extend their method to jointly compress data streams from entire sensor suites, moving beyond just LiDAR point clouds."
      }
    },
    {
      "title": "OcTr: Octree-Based Transformer for 3D Object Detection",
      "abstract": "A key challenge for LiDAR-based 3D object detection is to capture sufficient\nfeatures from large scale 3D scenes especially for distant or/and occluded\nobjects. Albeit recent efforts made by Transformers with the long sequence\nmodeling capability, they fail to properly balance the accuracy and efficiency,\nsuffering from inadequate receptive fields or coarse-grained holistic\ncorrelations. In this paper, we propose an Octree-based Transformer, named\nOcTr, to address this issue. It first constructs a dynamic octree on the\nhierarchical feature pyramid through conducting self-attention on the top level\nand then recursively propagates to the level below restricted by the octants,\nwhich captures rich global context in a coarse-to-fine manner while maintaining\nthe computational complexity under control. Furthermore, for enhanced\nforeground perception, we propose a hybrid positional embedding, composed of\nthe semantic-aware positional embedding and attention mask, to fully exploit\nsemantic and geometry clues. Extensive experiments are conducted on the Waymo\nOpen Dataset and KITTI Dataset, and OcTr reaches newly state-of-the-art\nresults.",
      "full_text": "OcTr: Octree-based Transformer for 3D Object Detection Chao Zhou1,2, Yanan Zhang1,2, Jiaxin Chen2, Di Huang1,2,3* 1State Key Laboratory of Software Development Environment, Beihang University, Beijing, China 2School of Computer Science and Engineering, Beihang University, Beijing, China 3Hangzhou Innovation Institute, Beihang University, Hangzhou, China {zhouchaobeing, zhangyanan, jiaxinchen, dhuang}@buaa.edu.cn Abstract A key challenge for LiDAR-based 3D object detection is to capture sufficient features from large scale 3D scenes es- pecially for distant or/and occluded objects. Albeit recent efforts made by Transformers with the long sequence mod- eling capability, they fail to properly balance the accuracy and efficiency, suffering from inadequate receptive fields or coarse-grained holistic correlations. In this paper, we pro- pose an Octree-based Transformer, namedOcTr, to address this issue. It first constructs a dynamic octree on the hier- archical feature pyramid through conducting self-attention on the top level and then recursively propagates to the level below restricted by the octants, which captures rich global context in a coarse-to-fine manner while maintaining the computational complexity under control. Furthermore, for enhanced foreground perception, we propose a hybrid po- sitional embedding, composed of the semantic-aware po- sitional embedding and attention mask, to fully exploit se- mantic and geometry clues. Extensive experiments are con- ducted on the Waymo Open Dataset and KITTI Dataset, and OcTr reaches newly state-of-the-art results. 1. Introduction 3D object detection from point clouds has received ex- tensive attention during the past decade for its ability to pro- vide accurate and stable recognition and localization in au- tonomous driving perception systems. In this task, feature learning plays a very fundamental and crucial role; yet it is rather challenging due to not only the disordered and sparse nature of data sampling, but also to insufficient acquisition under occlusion or at a distance. To address this issue, many methods have been proposed, which can be taxonomized into two major classes, i.e. grid-based and point-based. The former first regularize point clouds into multi-view images or voxels and then apply 2D or 3D CNNs to build shape rep- *indicates the corresponding author. Local Dilated Windows Shifted Windows Global Induced Set (3) Octree Construction Global Receptive Field Fine-grained  Representation (1) Fixed Pattern Limited Receptive Field (2) Set Proxy Limited Representation Figure 1. Illustration of three sparsification strategies of attention matrices. Fixed pattern (1) narrows receptive fields and set proxy (2) discards elaborate correlations. The proposed octree construc- tion (3) keeps the global receptive field in a coarse-grained manner while maintaining fine-grained representations. resentations [4, 53], while the latter directly conduct MLP based networks such as PointNet++ [34] and DGCNN [51] on original points for geometry description [33, 41, 43, 62]. Unfortunately, they fail to capture necessary context infor- mation through the small receptive fields in the deep mod- els, leading to limited results. Witnessing the recent success of Transformers in NLP, many studies have investigated and extended such architec- tures for 3D vision [25, 30, 60, 63]. Transformers are re- puted to model long-range dependencies, delivering global receptive fields, and to be suitable for scattered inputs of ar- bitrary sizes. Meanwhile, in contrast to those static weights that are learned in convolutions, Transformers dynamically aggregate the input features according to the relationships 1 arXiv:2303.12621v1  [cs.CV]  22 Mar 2023between tokens. Regarding the case in 3D object detection, compared to point-based Transformers [12,63], voxel-based ones show the superiority in efficiency. However, they tend to suffer heavy computations when dealing with large scale scenes because of the quadratic complexity of Transform- ers, with the underlying dilemma between the grid size and the grid amount in voxelization. Taking the KITTI dataset as an example, it is unrealistic for Transformers to operate on the feature map with the spatial shape of 200 × 176 × 5, which is commonly adopted in most of the detection heads [39, 47, 53, 57]. More recently, there have appeared an influx of efficient self-attention model variants that attempt to tackle long se- quences as input. They generally sparsify the attention ma- trix by fixed patterns [7, 24, 35], learned patterns [22, 46] or a combination of them [1, 58]. Fixed patterns chunk the input sequence into blocks of local windows or dila- tion windows, whilst learned patterns determine a notion of token relevance and eliminate or cluster outliers. Specific to 3D object detection from point clouds, V oTr [25] modi- fies self-attention with pre-defined patterns including local windows and stride dilation ones in a sparse query manner, and the dilation mechanism enlarges the receptive field by sampling attending tokens in a radius. SST [10] splits input tokens into non-overlapping patterns in a block-wise way and enables window shifting to capture cross-window cor- relation. Despite some improvements reported, they both only achieve bigger local receptive fields rather than the ex- pected global ones, and computations still increase rapidly with the expansion of receptive fields. Another alternative on self-attention is to take advantage of a proxy memory bank which has the access to the entire sequence tokens [1, 2, 58]. By using a small number of in- duced proxies to compress the whole sequence, it diffuses the global context efficiently. V oxSet [13] adapts Set Trans- former [20] to 3D object detection and exploits an induced set to model a set-to-set point cloud translation. With the help of the compressed global proxies and Conv-FFN, it ob- tains a global receptive field; nevertheless, as they admit, it is sub-optimal to set only a few latent codes as proxies for a large 3D scene, prone to impairing the representation of dif- ferent point cloud structures and their correlations. There- fore, there remains space for a stronger solution. In this paper, we present a novel Transformer network, namely Octree-based Transformer ( OcTr), for 3D object detection. We firstly devise an octree-based learnable sparse pattern, i.e. OctAttn, which meticulously and efficiently en- codes point clouds of scenes as shown in Fig. 1. The Oc- tAttn module constructs a feature pyramid by gathering and applies self-attention to the top level of the feature pyramid to select the most relevant tokens, which are deemed as the octants to be divided in the subsequent. When propagating to the level below, the key/value inputs are restricted by the octants from the top. Through recursively conducting this process, OctAttn captures rich global context features by a global receptive field in a coarse-to-fine manner while re- ducing the quadratic complexity of vanilla self-attention to the linear complexity. In addition, for better foreground per- ception, we propose a hybrid positional embedding, which consists of the semantic-aware positional embedding and at- tention mask, to fully exploit geometry and semantic clues. Thanks to the designs above, OcTr delivers a competitive trade-off between accuracy and efficiency. Our contribution is summarized in three-fold: 1. We propose OcTr for voxel-based 3D object detection, which efficiently learns enhanced representations by a global receptive field with rich contexts. 2. We propose an octree-based learnable attention sparsi- fication scheme (OctAttn) and a hybrid positional em- bedding combining geometry and semantics. 3. We carry out experiments on the Waymo Open Dataset (WOD) and the KITTI dataset and report state-of-the- art performance with significant gains on far objects. 2. Related Work 2.1. 3D Object Detection from Point Clouds There exist two prevailing point-cloud representations in 3D object detection, i.e. point-based and voxel-based. The point-based methods [32,41,43,62] directly process raw point clouds in the irregular 3D space. As a pioneering attempt, F-Pointnet [33] employs instance segmentation in frustums to extract proposals. V oteNet [32] clusters objects from the surface in a deep Hough voting manner. PointR- CNN [41] generates 3D RoIs with foreground segmentation and applies an RCNN-style [36] two-stage refinement. Dif- ferent from PointRCNN, some existing methods [3, 55, 61] build a lightweight and efficient single stage 3D object de- tection framework. However, the current point-based meth- ods still suffer from a large computation burden, which is not suitable for large-scale point cloud scenes. The voxel-based ones [4,6,14,53,57,64,65] conduct vox- elization on entire point clouds to construct regular grids. V oxelNet [65] exploits the voxel feature encoding layer with 3D convolutions to extract the feature of each voxel. SEC- OND [53] improves the model with sparse 3D convolutions, significantly increasing the speed of both training and infer- ence. Pointpillars [19] compacts point clouds into vertical columns and encodes them with 2D CNNs. Several recent methods [14, 39, 56] also explore merging point-based and voxel-based networks into one framework for complemen- tary features from different representations of point clouds. Unfortunately, they all use small convolution kernels with limited receptive fields, which are not competent to capture global context that is important to 3D detection. 2RPN Head BEV backbone Voxelization RoI Head One-stage Two-stage Conv Patch Embedding Octree Transformer Block ×𝑵 SubM Spconv Down-Sampling SubM Spconv Down-Sampling OcTr-Top OcTr-Base OcTr-Base Down-Sampling Octree Transformer Block ×𝑵 OcTr-Top OcTr-Base OcTr-Base Figure 2. Framework overview of the proposed Octree-based Transformer (OcTr) model. 2.2. Transformer in 3D Vision Inspired by the great success of the self-attention mech- anism in NLP [48] and CV [8, 24], Transformers have been adapted to 3D vision for their ability to capture long-range dependencies. For instance, Point Transformer [63] brings in vector attention that modulates individual feature chan- nels for point cloud classification and segmentation; PCT [12] presents offset-attention with the implicit Laplace op- erator and normalization refinement which is more suitable for point cloud learning. To address the high latency, some methods [31, 59] adopt voxels or patches for acceleration. For 3D detection, 3DETR [29] treats and predicts bound- ing boxes as sequences in an end-to-end manner. CT3D [38] leverages a channel-wise Transformer architecture to refine the RoI head. To learn context-aware representa- tions, some studies [10, 13, 25, 30] introduce Transform- ers into a point- or voxel-based encoder. Pointformer [30] stacks local, global and local-global Transformers based on the point-based encoder; V oTr [25] exploits dilated atten- tion with fast query to enlarge receptive fields; V oxSet [13] builds an induced point set as proxies of global context and applies point-to-point translation using voxels as medi- ums; and SST [10] embraces single strides without down- sampling and conducts window attention combining with its shifted version. Even though they all expand receptive fields by diverse Transformer variants, global context is not adequately involved or efficiently utilized. In contrast, we propose Octree-based Transformer (OcTr) for voxel-based 3D object detection, achieving a true global receptive field that balances accuracy and efficiency. 3. Method 3.1. Framework This sub-section describes the overall framework of the proposed OcTr model as shown in Fig. 2. Specifically, as inspired by [52], we first voxelize the point cloud into reg- ular grids and adopt the sparse 3D convolution for patch embedding, where the girds are regarded as the “tokens” and are passed through the Octree Transformer Blocks (OTB). Compared with the vanilla Transformer block, the self-attention module is substituted by the proposed octree- attention OctAttn, which encodes global context in a more efficient way. After applying a hybrid semantic embedding on multi-scale features, we sequentially stack two OTBs, tailed by a down-sampling layer. The voxel features are then projected into the BEV view by point-wise convolu- tions and are passed through a multi-scale dense 2D back- bone. Ultimately, an anchor-based or anchor-free RPN head is used for 3D proposal generation, and the RoI head is op- tional for refinement. Note that our OcTr can be adopted for most of the voxel-based detection frameworks by simply al- tering 3D backbones. 3.2. Self-attention Revisit According to [48], Transformer encoder blocks typically include a multi-head self-attention (MHSA) mechanism, a feed-forward network (FFN), a normalization function, and the residual connections [15]. Given an input sequence X, the principle part of MHSA is formulated as MHSA(X ) = HX h=1 Wh[σ(XWqW T k X T √ d ) · XWv ], (1) where h denotes the index of the head, and H, σ, W and d are the amount of heads, softmax function, learnable weight and feature dimension, respectively. The subscripts of q, k and v indicate query, key and value. The inputs and outputs of the MHSA module are connected by residual connectors and normalization layers. The MLP-based FFN connects its inputs/outputs in a similar manner. 3.3. Octree Attention An octree is a multi-scale asymmetric and efficient rep- resentation for unstructured 3D data such as point clouds. To build an octree for the input point cloud, we recursively sub-divide it in the breadth-first order until the pre-defined octree depth is reached. Whether to sub-divide an octant is 3Batch Normalization Softmax Sum & Gumbel-Topk Concat & Linear Cross-Attention Linear  Linear Linear Q Topk Sampling K V OcTr-Base  scattering Topk-region Topk-region Batch Normalization Softmax Sum & Gumbel-Topk Concat & Linear Self-Attention Linear  Linear Linear Q K V OcTr-Top C Concat Add  C OcTr-Base OcTr-Base OcTr-Top LePE Projector BN & FFN SAPE  & SAM F Figure 3. Octree Attention (OctAttn). The OctAttn module constructs a dynamic octree on a hierarchical feature pyramid with a partitive criterion of attention scores. For illustration, an octree representation is individually pruned from the pyramid for each grid in F. For example, the blue grids are the divided octants for the grids with stars. The detailed structures of OcTr-Topand OcTr-Base are shown at the right side. determined by the occupancy [37, 49], the surface approxi- mation [45, 50] or a learning algorithm [27]. Despite of being resource-friendly, the octree represen- tation is discrete and non-differentiable, which motivates us to ameliorate previous cumbersome pre-processing di- vision by introducing a novel octree-based attention mech- anism.As depicted in [22, 46], the attention matrices calcu- lated by self-attention imply the relevance of input tokens and guide feature selection. We thus prune the dense atten- tion matrices of the multi-scale feature pyramid to sparse octree attention in an adaptive and parallel manner, namely OctAttn. As shown in Fig. 3, let the output feature map and coor- dinates of convolutional patch embedding be F0 ∈ RM×d and I0 ∈ RM×3; M and d indicate the amount of the non- empty grids in a batch and feature dimension, respectively. Based on F0, we generate a multi-scale feature pyramid as C = {Fn, In}N , n∈ [0 , N), (2) where In = ⌊ I0 2n ⌋, Fn = BN(Smax(F0, In)), (3) n, N, BN and Smax are the index of the level of the multi- scale feature pyramid, the height of the pyramid, batch nor- malization [17] and the max scatter function, respectively. The pruning begins from the top of the pyramid. The top feature map, i.e. FN−1, is reorganized to dense input tokens ¯FN−1 ∈ RB×mN −1 ×d, where B and mN−1 are the batch size and maximum number of non-empty vox- els per batch, respectively. The voxel is padded with 0 if it is empty. As shown in Eq. (4), the MHSA takes ¯FN−1 as input and outputs the attention score matrices AN−1 ∈ RB×mN −1 ×mN −1 and attentive features ¯F ′ N−1 as AN−1 = HX h=1 σ( ¯FN−1 WqW T k ¯FT N−1√ d ), ¯F ′ N−1 = HX h=1 Wh[ σ( ¯FN−1 WqW T k ¯FT N−1√ d )· ¯FN−1 Wv ] . (4) For each query token, we select the topk attention scores as its most relevant token group in a row-wise way, denoted by ON−1 ∈ ZB×mN −1 ×k. When propagating from top to bottom through the pyra- mid and reaching the n-th level, we uniformly sample lim- ited K attending octants from the selected regions with fea- tures in ¯Fn and topk indices in On+1. We conduct cross- attention instead of self-attention with the dense query se- quence of the n-th level, ¯Qn ∈ RB×mn ×d, and the compact sampled key/value sequence of the n-th level, ¯Kn/ ¯Vn ∈ RB×mn ×K×d , which is formulated as below An = HX h=1 σ( ¯QnWqW T k ¯KT n√ d ), ¯F ′ n = HX h=1 Wh[ σ( ¯QnWqW T k ¯KT n√ d )· ¯VnWv ] . (5) Backing off the sampling, this can be treated as an attention mask on self-attention matrices. The above process is run recursively until reaching the bottom level of the pyramid. Furthermore, as the topk selection is a hard decision that disables gradient back-propagation, we adopt the Gumbel- topk technique [18] to perform a differentiable and contin- uous approximation by replacing the vanilla topk selection. 4The normalized scores used in top k are derived from the distribution in Eq. (6) during training, maintaining the orig- inal ones during inference. g, τ, mn denote the noise sam- pled from the Gumbel distribution, the temperature and the amount of non-empty voxels in layer n, respectively. pi = exp((Ai n + gi)/τ) mnP i exp((Ain + gi)/τ) ∈ [0, 1]. (6) In order to leverage the multi-scale features in distinct spatial shapes, we concatenate them by upsampling, which is implemented by inverse indices of the scatter function, followed by a linear projection layer for aligning the input feature dimension. As the local context is generally critical for object de- tection, inspired by [7], we additionally introduce a Locally enhanced Positional Embedding (LePE) which enables lo- cal neighbor interactions on the value sequence. With sub- manifold sparse convolutions, we replace the residual con- nections in the attention mechanism with LePE. Finally, OTB is formulated as follows ˜F = FC({F ′ N−1|F ′ N−2|...|F ′ 0}) + LePE(F0), ˜F ′ = BN(FFN( ˜F)) + ˜F , (7) where F ′ n ∈ Rm0×d denotes the compact tensor of the up- sampled ¯F ′ n, | indicates concatenation, and FC denotes the fully-connected layer. Besides, we analyze the time complexity of the octree attention as below O(( M ωN−1 )2 + N−2X n=0 KM ωn ) =O(( M ωN−1 )2 + ω ω − 1KM (1 − ω1−N )), (8) where ω is the average down-sampling ratio in the sparse voxel representation. 3.4. Semantic Positional Embedding Due to the large proportion of background grids in point clouds, the attention matrices are dominated by background grid pairs, leading to a sub-optimal solution. To fully lever- age the local 3D shape patterns and original voxel coordi- nates, we propose a hybrid positional embedding to capture both the geometry and semantic clues as displayed in Fig. 4. Specifically, we first segment foreground grids using the supervision from the ground truth. Segmentation scores are predicted by a sub-manifold sparse convolution branch with a sigmoid function; and the focal loss [23] is applied to bal- ance the foreground and background. SubM 𝝈 Segmentation Scores FL Loss Ground Truth Training C ∼ + coordinates Linear +F SAPE SAM 𝝈 C + × SA Sigmoid Concat Add Hadamard  Production Gather . >𝜹𝟏 >𝜹𝟐 Mask . Dot Production Softmax Figure 4. Illustration of semantic positional embedding, where an extra foreground segmentation branch is adopted with supervision and semantic scores are concatenated as absolute positional em- beddings and serve as relative masks on attention matrices. We concatenate the center coordinate ( x, y, z) and the semantic score with the feature f in a grid-wise manner, followed by a linear projection without the bias as below SAPE(X) = FCd+4→d({x, y, z, score|f}) = FCd→d(f) + FC4→d(x, y, z, score). (9) Eq. (9) is equivalent to the absolute positional embedding (APE), thus being denoted as the Semantic APE (SAPE). The scatter function such as the mean, max and batch nor- malization naturally provide position and semantic informa- tion of the downsampled voxel grids, making it applicable for the multi-scale feature pyramid. Besides the semantic clues implicitly used in SAPE, we employ the Semantic Attention Mask (SAM) based on the segmentation scores and we mask the attention matrices to address correlations between inferior queries and superior keys in a simple yet effective way. Given a scalar attention matrix A ∈RNq×Nk before softmax and the segmentation scores of query and key/value Sq ∈ RNq and Sk ∈ RNk , we formulate the attention matrix after softmax as A ′ = σ(−Γ · [1 − (Sq ≥ δq)(Sk ≥ δk)T ] + A), (10) where Nq, δq, Nk, δk, σ and Γ are the length and thresh- old of the query sequence, length and threshold of the key sequence, softmax function and infinite scalar, respectively. Finally, we broadcast the semantic mask A ′ to all heads. 4. Experiments We evaluate the proposed OcTr network on the Waymo Open Dataset (WOD) and KITTI dataset, both of which are popular in 3D object detection. In this section, we introduce the benchmarks and implementation details, make compar- ison to the previous state-of-the-art counterparts, and ablate the key designs of OcTr. 5Model Vehicle (L1) Vehicle (L2) Pedes. (L1) Pedes. (L2) Cyclist (L1) Cyclist (L2) mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH SECOND [53] 70.96/70.34 62.58/62.02 65.23/54.24 57.22/47.49 57.13/55.62 54.97/53.53 PointPillar [19] 70.43/69.83 62.18/61.64 66.21/46.32 58.18/40.64 55.26/51.75 53.18/49.80 PartA2Net [42] 74.82/74.32 65.88/65.42 71.76/63.64 62.53/55.30 67.35/66.15 65.05/63.89 PVRCNN [39] 75.41/74.74 67.44/66.80 71.98/61.24 63.70/53.95 65.88/64.25 63.39/61.82 CenterPoint [57] 71.33/70.76 63.16/62.65 72.09/65.49 64.27/58.23 68.68/67.39 66.11/64.87 LiDAR-RCNN [21] 73.5/73.0 64.7/64.2 71.2/58.7 63.1/51.7 68.6/66.9 66.1/64.4 V oxel-RCNN [6] 75.59/- 66.59/- -/- -/- -/- -/- PVRCNN++ [40] 77.82/77.32 69.07/68.62 77.99/71.36 69.92/63.74 71.80/70.71 69.31/68.26 SST† [10] 76.22/75.79 68.04/67.64 81.39/74.05 72.82/65.93 -/- -/- PDV [16] 76.85/76.33 69.30/68.81 74.19/65.96 65.85/58.28 68.71/67.55 66.49/65.36 Ours 78.12/77.63 69.79/69.34 80.76/74.39 72.48/66.52 72.58/71.50 69.93/68.90 Table 1. Performance on WOD with 202 validation sequences for vehicle (IoU=0.7), pedestrian (IoU=0.5) and cyclist (IoU=0.5), using 20% samples for training. All the results are achieved by the models simultaneously trained for 3 classes on single frames, except the ones of the model marked by †, which is only trained for a single class. Refer to Supp. B for the results trained with 100% samples. Model mAP3D (L1)@Vehicle Overall 0-30m 30m-50m 50m-inf PV-RCNN [39] 70.30 91.92 69.21 42.17 V oxel-RCNN [6] 75.59 92.49 74.09 53.15 V oTR-TSD [25] 74.95 92.28 73.36 51.09 CT3D [38] 76.30 92.51 75.07 55.36 PyramidPV [26] 76.30 92.67 74.91 54.54 PDV [16] 76.85 93.13 75.49 54.75 V oxSeT [13] 77.82 92.78 77.21 54.41 Ours 78.82 92.99 77.66 58.02 Model mAP3D (L2)@Vehicle Overall 0-30m 30-50m 50m-inf PV-RCNN [39] 65.36 91.58 65.13 36.46 V oxel-RCNN [6] 66.59 91.74 67.89 40.80 CT3D [38] 69.04 91.76 68.93 42.60 PDV [16] 69.30 92.41 69.36 42.16 V oxSeT [13] 70.21 92.05 70.10 43.20 Ours 70.50 91.78 71.28 45.46 Table 2. Results on the WOD validation set in different ranges for vehicle detection. 4.1. Datasets and Implementation Details WOD [44] is a large dataset of autonomous driving scenes. It totally contains 798 training sequences with around 160K LiDAR samples and 202 validation sequences with 40K Li- DAR samples, with the mean Average Precision (mAP) and mAP weighted by heading accuracy (mAPH) as evaluation metrics. There are also two levels of difficulty describing the sparsity in each bounding box, and LEVEL 1 (L1) and LEVEL 2 (L2) denote more than 5 points and 1-5 points, respectively. For detection performance along distance, it provides mAP/mAPH on 0-30m, 30m-50m and 50m-inf. KITTI [11] is a widely used benchmark for 3D object de- tection, which includes 3,712, 3,769 and 7,518 frames for training, validation and testing, respectively. mAP is used as the official metric with 11 recall points for theval set and 40 for the test set, and the IoU thresholds are set to 0.7, 0.5, and 0.5 for car, pedestrian and cyclist. We use the official setting in all experiments. Implementation Details The total loss for optimizing the overall two-stage detection is formulated as Eq. (11), where Lrcnn can be omitted if there is no RoI head. Refer toSupp. A.1 for more information. Ldet = Lrpn + Lrcnn + Lseg. (11) 4.2. Results on WOD The results on the validation set are displayed in Tab. 1, and it can be seen that we achieve new state-of-the-art per- formance on all the three classes. In particular, for pedes- trian, we outperform the baseline model PV-RCNN++ [40] by 2.77%/2.56% in terms of both L1 and L2 mAP, which in- dicates the effectiveness of the proposed model in handling hard examples. In comparison with other Transformer-based models, we focus on vehicle since the counterparts only report the per- formance on it. As Tab. 2 shows, our OcTr achieves the best mAP among all these convolution- and Transformer-based backbones. It also outperforms the Transformer-based de- tection head network CT3D [38] by 2.52% and 1.46% in L1 and L2 mAP. Regarding the accuracies at different dis- tances, OcTr ranks the first place in the range of 30m-50m and 50m-inf, which surpasses the previous best by 0.45%, 2.66% in L1 mAP and 1.18%, 2.26% in L2 mAP respec- tively. It clearly illustrates that OcTr has the advantage in capturing long-range fine-grained context, which facilitates dealing with objects far away. Indeed, far objects gener- ally have much more sparse points than near ones and heav- ily rely on context for detection, thus benefiting more from OcTr. Refer to Fig. 5 for visualization. 6Model mAP3D@Car mAP3D@Pedestrian mAP3D@Cyclist Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard SECOND [53] 88.61 78.62 77.22 56.55 52.98 47.73 80.58 67.15 63.10 PointPillars [19] 88.46 77.28 74.65 57.75 52.29 47.90 80.04 62.61 59.52 V oTR [25] 87.86 78.27 76.93 - - - - - - V oxSeT [13] 88.45 78.48 77.07 60.62 54.74 50.39 84.07 68.11 65.14 Ours 88.43 78.57 77.16 61.49 57.17 52.35 85.29 70.44 66.17 Table 3. Results of the single-stage models on the KITTI val set. All the models adopt the same anchor-based region proposal network as the detection head. “Mod.” denotes the moderate difficulty level. Model mAP3D@Car on test mAP3D@Car on val Easy Mod. Hard Mean Easy Mod. Hard Mean SECOND [53] 83.34 72.55 65.82 73.90 88.61 78.62 77.22 81.48 PointPillars [19] 82.58 74.31 68.99 75.29 86.62 76.06 68.91 77.20 STD [56] 87.95 79.71 75.09 80.92 89.70 79.80 79.30 82.93 SA-SSD [14] 88.75 79.79 74.16 80.90 90.15 79.91 78.78 82.95 3DSSD [55] 88.36 79.57 74.55 80.83 89.71 79.45 78.67 82.61 PV-RCNN [39] 90.25 81.43 76.82 82.83 89.35 83.69 78.70 83.91 V oxel-RCNN [6] 90.90 81.62 77.06 83.19 89.41 84.52 78.93 84.29 CT3D [38] 87.83 81.77 77.16 82.25 89.54 86.06 78.99 84.86 V oTR-TSD [25] 89.90 82.09 79.14 83.71 89.04 84.04 78.68 83.92 V oxSeT [13] 88.53 82.06 77.46 82.68 89.21 86.71 78.56 84.83 Focals Conv [4] 90.55 82.28 77.59 83.47 89.52 84.93 79.18 84.54 Ours 90.88 82.64 77.77 83.76 89.80 86.97 79.28 85.35 Table 4. Comparison to the state-of-the-art models on the KITTItest and val sets. “Mod.” and “Mean” denote the moderate difficulty level and the average mAP for the three levels, respectively. The best results are bolded and the second best ones are underlined. Detector Veh. mAP (L1/L2) Pedes. mAP (L1/L2) SECOND [53] 70.96/62.58 65.23/57.22 Ours 73.28/65.05 68.08/60.36 PV-RCNN [39] 75.41/67.44 71.98/63.70 Ours 76.77/68.31 73.22/64.30 PV-RCNN++ [40] 77.82/69.07 77.99/69.92 Ours 78.01/69.60 80.75/72.45 Table 5. Results of extensions to different representative detectors on the WOD validation set. 4.3. Results on KITTI The performance of the single-stage detectors is shown in Tab. 3. We take SECOND [53], a commonly used anchor- based model, as the baseline, and compare OcTr with an- other two advanced Transformer-based variants V oxSet [13] and V oTR [25]. We can see that OcTr achieves comparable results with SECOND and V oxSeT in car, while it signifi- cantly outperforms all the counterparts and reports the best performance both in pedestrian and cyclist. Benefiting from the large receptive field and fine-grained global context, it exceeds SECOND by 4.19% and 3.29% for pedestrians and cyclists respectively, where hard samples often appear. We summarize the performance of the two-stage mod- els on the KITTI test set in Tab. 4. With the help of the Attention Veh. mAP (L1/L2) Pedes. mAP (L1/L2) Ours (OctAttn) 73.3/65.1 68.1/60.4 Performer [5] 71.4/63.6 65.7/57.9 ACT [28] 71.7/63.5 64.3/56.1 V oTr [25] 69.4/61.5 65.0/57.0 NearestK 68.2/59.8 64.9/56.7 Table 6. Ablation on various attention mechanisms and sampling patterns on the WOD validation set. multi-scale backbone features and rich global context, OcTr reaches a leading mAP in car at the moderate level, surpass- ing the state-of-the-art Focals-Conv [4] by 0.36%. We also evaluate OcTr on the KITTIval set, and OcTr again delivers the best performance in the average score, outperforming the second-best by 0.49%. One can observe that we rank the best or the second best in all the cases. 4.4. Ablation study Scalability on various detectors As summarized in Tab. 5, we conduct experiments on three different and representa- tive detectors, SECOND (single-stage, anchor-based), PV- RCNN (two-stage, anchor-based) and PV-RCNN++ (two- stage, anchor-free). Regardless of the number of stages or region proposal network, we acquire sound improvements 7LEPE SAPE SAM Veh. mAP (L1/L2) Pedes. mAP (L1/L2) 71.35/63.30 65.75/57.89 ✓ 72.34/64.32 66.56/58.62 ✓ ✓ 72.64/64.46 66.62/58.83 ✓ ✓ 72.86/64.40 67.79/59.90 ✓ ✓ ✓ 73.28/65.05 68.08/60.36 Table 7. Ablation on semantic positional embedding on the WOD validation set. topk number Veh. mAP (L1/L2) Pedes. mAP (L1/L2) 1 70.38/62.20 64.19/56.43 4 72.58/64.42 66.21/58.42 8 73.28/65.05 68.08/60.36 16 73.25/65.01 67.89/60.10 Table 8. Results of different k values on the WOD validation set. Method #Param. (M) Latency (ms) Memory (GB) SECOND [53] 5.3 48 2.3 V oTR-SSD [25] 4.8 67 3.0 V oxSeT-SSD [13] 3.0 37 3.6 OcTr-SSD 2.9 64 2.5 Table 9. Resource costs of different backbones with single-stage detectors on the KITTI dataset, test on GTX2080Ti. compared to the baselines of the sparse convolution back- bones, highlighting its scalability. Ablation on OctAttn We carry out additional experiments to make apple-to-apple comparison with several represen- tative linear Transformer methods, including Performer [5] (kernel-based linear attention), ACT [28] (cluster-based lin- ear attention), V oTr [25] (fixed patterns) and the Nearest-K strategy. Tab. 6 lists the results, and ourOctAttn clearly per- forms the best, showing its ability. Ablation on semantic positional embedding We individ- ually evaluate the contributions of LePE, SAPE and SAM with SECOND as the baseline detector on WOD in Tab. 7. By incorporating LePE, the L1/L2 performance is boosted by 0.99%/1.02% and 0.81%/0.73% on vehicle and pedes- trian, illustrating its necessity. Furthermore, we separately verify the validity of SAPE and SAM. With semantic clues, we observe that SAPE increases by 0.3% L1 mAP on ve- hicle, while SAM provides an L1/L2 mAP improvement of 0.52%/0.08% and 1.23%/1.28% on vehicle and pedestrian. Finally, we simultaneously apply SAPE and SAM and con- struct the full model of OcTr, which gains 0.94%/0.73% and 1.52%/1.74% L1/L2 mAP on vehicle and pedestrian, show- ing its impact. Influence by top k Tab. 8 shows the performance of OcTr with various values of k. As we can see, the performance improves when k becomes larger but quickly saturates. Due to the redundancy of the scanned scenes, we argue that only a few tokens with high relevance need to be subdivided to embrace fine-grained features. Analysis on model complexityWe compare OcTr with two recent Transformer-based models in terms of resource cost by keeping the same detection head. Tab. 9 shows that with the learnable octree attention mechanism, OcTr consistently maintains less model parameters and less memory occupan- cies than the counterparts. Regarding the inference speed, V oxSeT runs faster, but it should be noted that V oxSeT in- puts with pillars which discard the height dimension, lead- ing to inferior results. As for V oTR, we deliver a mild im- provement in efficiency while bringing a large gain in per- formance. Figure 5. Visualization of results by OcTr on the WOD validation split. Blue/red indicates predicted/ground-truth bounding boxes 5. Conclusion This paper proposes a novel voxel-based approach to 3D object detection, namely OcTr. It aims to balance the fine- grained global representation and efficiency with acceptable resource costs. To this end, we propose a learned sparsifica- tion attention mechanism, OctAttn, which adaptively prunes the octants from the multi-scale feature pyramid in a top-to- bottom manner. Furthermore, we adopt a hybrid semantic- aware positional embedding based on foreground segmen- tation. Extensive experiments are conducted on WOD and KITTI, and OcTr reaches the state-of-the-art performance, validating its effectiveness. Acknowledgment This work is partly supported by the National Natu- ral Science Foundation of China (No. 62022011 and No. 62202034), the Research Program of State Key Labora- tory of Software Development Environment (SKLSDE- 2021ZX-04), and the Fundamental Research Funds for the Central Universities. 8References [1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured inputs in transformers. InEMNLP, 2020. 2 [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. 2 [3] Chen Chen, Zhe Chen, Jing Zhang, and Dacheng Tao. Sasa: Semantics-augmented set abstraction for point-based 3d ob- ject detection. In AAAI, volume 1, 2022. 2 [4] Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Ji- aya Jia. Focal sparse convolutional networks for 3d object detection. In CVPR, pages 5428–5437, 2022. 1, 2, 7 [5] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam´as Sarl´os, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In ICLR, 2021. 7, 8 [6] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. V oxel r-cnn: Towards high performance voxel-based 3d object detection. In AAAI, volume 35, pages 1201–1209, 2021. 2, 6, 7 [7] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR, pages 12124–12134, 2022. 2, 5 [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. In ICLR, 2020. 3 [9] Lue Fan, Wang Feng, Wang Naiyan, and Zhang Zhaoxiang. Fully sparse 3d object detection. In NIPS, 2022. 13, 14 [10] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Embracing single stride 3d object detector with sparse trans- former. In CVPR, pages 8458–8468, 2022. 2, 3, 6 [11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, pages 3354–3361. IEEE, 2012. 6, 13 [12] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud transformer. CVM, 7(2):187–199, 2021. 2, 3 [13] Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang. V oxel set transformer: A set-to-set approach to 3d object detection from point clouds. In CVPR, pages 8417–8427, 2022. 2, 3, 6, 7, 8 [14] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detec- tion from point cloud. In CVPR, pages 11873–11882, 2020. 2, 7 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 3 [16] Jordan SK Hu. Point density-aware voxels for lidar 3d object detection. In CVPR, pages 8469–8478, 2022. 6 [17] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456. PMLR, 2015. 4 [18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa- rameterization with gumbel-softmax. In ICLR, 2016. 4 [19] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, pages 12697–12705, 2019. 2, 6, 7 [20] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se- ungjin Choi, and Yee Whye Teh. Set transformer: A frame- work for attention-based permutation-invariant neural net- works. In ICML, pages 3744–3753. PMLR, 2019. 2 [21] Zhichao Li, Feng Wang, and Naiyan Wang. Lidar r-cnn: An efficient and universal 3d object detector. In CVPR, pages 7546–7555, 2021. 6 [22] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. In ICLR, 2022. 2, 4 [23] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In ICCV, pages 2980–2988, 2017. 5 [24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012–10022, 2021. 2, 3 [25] Jiageng Mao. V oxel transformer for 3d object detection. In ICCV, pages 3164–3173, 2021. 1, 2, 3, 6, 7, 8 [26] Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu. Pyramid r-cnn: Towards bet- ter performance and adaptability for 3d object detection. In ICCV, pages 2723–2732, 2021. 6 [27] Julien NP Martel, David B Lindell, Connor Z Lin, Eric R Chan, Marco Monteiro, and Gordon Wetzstein. Acorn: Adaptive coordinate networks for neural scene representa- tion. In ACM SIGGRAPH, 2021. 4 [28] Zheng Minghang, Gao Peng, Zhang Renrui, Li Kunchang, Li Hongsheng, and Dong Hao. End-to-end object detection with adaptive clustering transformer. In BMVC, page 226, 2021. 7, 8 [29] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end- to-end transformer model for 3d object detection. In ICCV, pages 2906–2917, 2021. 3 [30] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao Huang. 3d object detection with pointformer. In CVPR, pages 7463–7472, 2021. 1, 3 [31] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. Fast point transformer. In CVPR, pages 16949–16958, 2022. 3 [32] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, pages 9277–9286, 2019. 2 9[33] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object detection from rgb-d data. In CVPR, pages 918–927, 2018. 1, 2 [34] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, volume 30, 2017. 1 [35] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self- attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019. 2 [36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, volume 28, 2015. 2 [37] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In CVPR, pages 3577–3586, 2017. 4 [38] Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, and Min-Jian Zhao. Improving 3d object detection with channel-wise transformer. In ICCV, pages 2743–2752, 2021. 3, 6, 7 [39] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point- voxel feature set abstraction for 3d object detection. In CVPR, pages 10529–10538, 2020. 2, 6, 7 [40] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv- rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. IEEE TPAMI, 2021. 6, 7, 14 [41] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr- cnn: 3d object proposal generation and detection from point cloud. In CVPR, pages 770–779, 2019. 1, 2 [42] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detec- tion from point cloud with part-aware and part-aggregation network. IEEE TPAMI, 43(8):2647–2664, 2020. 6 [43] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net- work for 3d object detection in a point cloud. InCVPR, pages 1711–1719, 2020. 1, 2 [44] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, pages 2446–2454, 2020. 6, 14 [45] Jia-Heng Tang, Weikai Chen, Jie Yang, Bo Wang, Songrun Liu, Bo Yang, and Lin Gao. Octfield: Hierarchical implicit functions for 3d modeling. In NeurIPS, 2021. 4 [46] Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan. Quadtree attention for vision transformers. In ICLR, 2022. 2, 4 [47] OpenPCDet Development Team. Openpcdet: An open- source toolbox for 3d object detection from point clouds. https://github.com/open-mmlab/OpenPCDet , 2020. 2, 12 [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Il- lia Polosukhin. Attention is all you need. In NeurIPS, vol- ume 30, 2017. 3 [49] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octree-based convolutional neural networks for 3d shape analysis. ACM TOG , 36(4):1–11, 2017. 4 [50] Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, and Xin Tong. Adaptive o-cnn: A patch-based deep representation of 3d shapes. ACM TOG, 37(6):1–11, 2018. 4 [51] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM TOG, 38(5):1– 12, 2019. 1 [52] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll´ar, and Ross Girshick. Early convolutions help trans- formers see better. In NeurIPS, volume 34, pages 30392– 30400, 2021. 3 [53] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed- ded convolutional detection. Sensors, 18(10):3337, 2018. 1, 2, 6, 7, 8, 14 [54] Honghui Yang, Liu Zili, Wu Xiaopei, Wang Wenxiao, Qian Wei, He Xiaofei, and Cai Deng. Graph r-cnn: Towards accu- rate 3d object detection with semantic-decorated local graph. In ECCV, 2022. 13, 14 [55] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage object detector. In CVPR, pages 11040–11048, 2020. 2, 7 [56] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Std: Sparse-to-dense 3d object detector for point cloud. In CVPR, pages 1951–1960, 2019. 2, 7 [57] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center- based 3d object detection and tracking. In CVPR, pages 11784–11793, 2021. 2, 6 [58] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. In NeurIPS, volume 33, pages 17283–17297, 2020. 2 [59] Cheng Zhang, Haocheng Wan, Xinyi Shen, and Zizhao Wu. Patchformer: An efficient point transformer with patch at- tention. In CVPR, pages 11799–11808, 2022. 3 [60] Yanan Zhang, Jiaxin Chen, and Di Huang. Cat-det: Con- trastively augmented transformer for multi-modal 3d object detection. In CVPR, pages 908–917, 2022. 1 [61] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jian- wei Wan, and Yulan Guo. Not all points are equal: Learn- ing highly efficient point-based detectors for 3d lidar point clouds. In CVPR, pages 18953–18962, 2022. 2 [62] Yanan Zhang, Di Huang, and Yunhong Wang. Pc-rgnn: Point cloud completion and graph neural network for 3d object de- tection. In AAAI, volume 35, pages 3430–3437, 2021. 1, 2 [63] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In ICCV, pages 16259– 16268, 2021. 1, 2, 3 [64] Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, and Chi- Wing Fu. Cia-ssd: Confident iou-aware single-stage object detector from point cloud. In AAAI, volume 35, pages 3555– 3562, 2021. 2 10[65] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning for point cloud based 3d object detection. In CVPR, pages 4490–4499, 2018. 2 [66] Zixiang Zhou, Zhao Xiangchen, Wang Yu, Wang Panqu, and Hassan Foroosh. Centerformer: Center-based transformer for 3d object detection. In ECCV, 2022. 13, 14 11Supplementary Material This supplementary material provides more implemen- tation details on OcTr in Sec. A, more experiments results in Sec. B and additional visualization in Sec. C. A. More Implementation Details A.1. Detailed Implementation The voxel size in WOD and KITTI is set as [0.1m, 0.1m, 0.1875m] and [0.05m, 0.05m, 0.125m], respectively. The convolution patch embedding module outputs the feature map with a downsampling ratio of 4 and the dimension of the feature map is set as 64 × 8 × 376 × 376 in WOD and 64 × 8 × 400 × 352 in KITTI. There are two stacked octree Transformer layers, with two Octree Transformer Blocks (OTBs) in each layer. In the first layer, the pyramid height, the attention dimension, the number of heads, the dimen- sion of heads, and the value of top k are set to 4, 64, 2, 32 and 8, respectively. In the second layer, they are set to 3, 64, 2, 32 and 8. τ in Eq. (6) is set to 1 and Γ in Eq. (10) is 10000 in practice. During the training procedure, we adopt the Adam optimizer with a batch size of 16, and the co- sine annealing learning rate scheduler with an initial value of 0.01 for the two-stage model and 0.003 for the single- stage model. Other hyper-parameters in detection heads, data augmentation and post-processing are set the same as the default values in OpenPCDet [47]. Our code is implemented based on OpenPCDet [47]. All the experiments are conducted on 4 RTX 3090 GPUs except the ones on complexity analysis shown in Tab. 9. A.2. Detailed Architecture The detailed architecture of OcTr is demonstrated in Fig. A. The convolutional patch embedding is composed of sparse convolutions with the kernel size of3×3×3, and 4× downsampling is conducted on input feature maps. A reg- ular sparse convolution layer is applied for downsampling between OTBs, and the successive height compression op- eration is replaced with a pixel-wise sub-manifold sparse convolution on BEV features. A.3. Topk Sampling Topk sampling is an important component in OcTr (in Sec. 3.3 of the main body). To generate the selected sparse octants for subdivision, we first record the indices between the child and parent octants as a pre-calculated index bank. Fig. B depicts the entire procedure. In level n, by query- ing about topk parent octants and the pre-calculated index, we densify the sampling outputs ⃗K ∈ RB×mn+1×8·k×d and flatten the tensors of the key/value in an 8 · k → [k, 8] man- ner. We then compact the tensor and truncate the top K children, resulting in ⃗K ∈ RB×mn+1×K×d. By using the Submanifold Sparse Conv kernel=3×3×3stride=1 𝟒×32×1408×1600 𝟏𝟔×32×1408×1600 Regular Sparse Conv kernel=3×3×3stride=2 Submanifold Sparse Conv kernel=3×3×3stride=1 𝟑𝟐×16×704×800 Regular Sparse Conv kernel=3×3×3stride=2 𝟔𝟒×8×352×400 Octree Attention Module tree_depth=4 topk=8 attending_grids=32 dim=64 head_dim=32 heads=2  ffn_dim=128 dropout=0 Octree Attention Module tree_depth=4 topk=8 attending_grids=32 dim=64 head_dim=32 heads=2  ffn_dim=128 dropout=0 Regular Sparse Conv kernel=3×3×3stride=2 𝟔𝟒×8×352×400 𝟔𝟒×4×176×200 Octree Attention Module tree_depth=3 topk=8 attending_grids=32 dim=64 head_dim=32 heads=2  ffn_dim=128 dropout=0 Octree Attention Module tree_depth=3 topk=8 attending_grids=32 dim=64 head_dim=32 heads=2  ffn_dim=128 dropout=0 𝟔𝟒×4×176×200 Submanifold Sparse Conv kernel=4×1×1stride=1 𝟐𝟓𝟔×1×176×200 Octree Transformer Block Octree Transformer Block Convolutional Patch Embedding Figure A. Detailed architecture of the proposed OcTr network. pre-defined index bank, we broadcast the sampled tensors to align the features in layer n, generating a tensor with the shape of RB×mn×K×d. According to statistics, the downsampling ratio in feature pyramid construction is fixed as 3.2, i.e. mn/mn+1 ≈ 3.2. Empirically, to adequately query, we set K = 4 × k in our implementation. A.4. Semantic Attention Mask Following Eq. (10) in the main body, we further show the details of SAM in Fig. C. To obtain a mask for inferior foreground grids, we define a boolean tensorMq = ISq≥δq , where Sq is calculated by the mean scatter function 1. Sim- 1https://pytorch-scatter.readthedocs.io/ 12Pre-calculated Index Bank Parents Children querying flattening compacting truncating 𝑂𝑛+1 𝐾𝑘 Figure B. Illustration of top k sampling (the white/colored square denotes empty/non-empty grid, respectively). 𝑆𝑞 ≥ 𝛿𝑞 𝑆𝑘 ≥ 𝛿𝑘 − Γ 1 − 𝑀 𝐴 )+𝜎( = 𝐴′ back fore -Γ zero unchanged masked Figure C. In SAM, the attention scores of background grids main- tain unchanged, while those of foreground ones are masked. ilarly, we have Mk = ISk≥δk and obtain the boolean se- mantic mask on the attention matrices, which is measured by M = Mq · Mk. Though segmentation scores indicate the significance of grids, they suffer from inaccurate predic- tions. Considering that the mask M tends to suppress the attention scores of the background grids to 0 and thus de- teriorate representations, we simply maintain the attention scores of the background unchanged. The hyper-parameters δq and δk are set to 0.05 and 0.2, respectively. B. More Experiments Results We add experiments with 100% training data and com- pare OcTr with Graph-RCNN [54], FSD [9] and Center- Former (CF) [66]. Note that PVRCNN++ in Table 1 is the same as PVRCNN++ (center). As in Table A, Graph- RCNN and CF (8 frames) achieve higher results, but either with multi-modal or multi-frame data for prediction. When using single frames, OcTr clearly outperforms CF. As for FSD, the performance of OcTr is comparable or even better than that of FSD on vehicle and pedestrian, but is moder- ately lower on cyclist. However, FSD builds a strong de- tection head, which tends to be complementary to our OcTr backbone. We believe that OcTr can be further promoted by Figure D. Visualization on the KITTIval set. The blue/red bound- ing boxes indicate the predicted/ground-truth results, respectively. combining FSD. We also conduct experiments on Waymo test in Table B using the representative single-stage SECOND and two- stage PVRCNN++, and the results confirm the effectiveness of our method. It should be noted that the common testing tricks, e.g. TTA and WBF, are not applied. C. More Visualization Results We additionally visualize some detection results by using the proposed OcTr network on KITTI [11] and 13Model Vehicle (L1) Vehicle (L2) Pedes. (L1) Pedes. (L2) Cyclist (L1) Cyclist (L2) mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH CF (1 frame) [66] 75.2/74.7 70.2/69.7 78.6/73.0 73.6/68.3 72.3/71.3 69.8/68.8 CF (8 frames) [66] 78.8/78.3 74.3/73.8 82.1/79.3 77.8/75.0 75.2/74.4 73.2/72.3 FSD [9] 79.2/78.8 70.5/70.1 82.6/77.3 73.9/69.1 77.1/76.0 74.4/73.3 Graph-RCNN [54] 80.8/80.3 72.6/72.1 82.4/76.6 74.4/69.0 75.3/74.2 72.5/71.5 OcTr 79.2/78.7 70.8/70.4 82.2/76.3 74.0/68.5 73.9/72.8 71.1/69.2 Table A. Performance on WOD validation with 100% training data. Model Vehicle (L1) Vehicle (L2) Pedes. (L1) Pedes. (L2) Cyclist (L1) Cyclist (L2) mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH mAP/mAPH SECOND [53] 76.2/75.7 68.3/67.8 68.6/55.3 62.8/50.5 62.4/56.6 60.1/54.6 Ours (SECOND) 77.9/77.4 70.2/69.7 71.5/61.1 65.7/56.1 70.7/69.3 68.1/66.8 PV-RCNN++ [40] 81.6/81.2 73.9/73.5 80.4/75.0 74.1/69.0 71.9/70.8 69.3/68.2 Ours (PV-RCNN++) 81.7/81.4 74.0/73.6 81.2/75.2 75.0/69.3 73.0/71.8 70.4/69.4 Table B. Performance on WOD test with 100% training data. WOD [44] in Fig. D and Fig. E, respectively. We use the two-stage detector PVRCNN++ [40] as our baseline model and only predict cars on KITTI. As displayed, we can ob- serve that OcTr delivers accurate localization and classifica- tion for distant and sparse samples, even in crowded scenes. 14Figure E. Visualization on the WOD validation set in crowded scenes. The blue/red bounding boxes indicate the predicted/ground-truth results, respectively. 15",
      "meta_data": {
        "arxiv_id": "2303.12621v1",
        "authors": [
          "Chao Zhou",
          "Yanan Zhang",
          "Jiaxin Chen",
          "Di Huang"
        ],
        "published_date": "2023-03-22T15:01:20Z",
        "pdf_url": "https://arxiv.org/pdf/2303.12621v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes OcTr, an Octree-based Transformer, to address the challenge of capturing sufficient features from large-scale 3D LiDAR scenes, especially for distant or occluded objects, while balancing accuracy and computational efficiency. OcTr introduces OctAttn, an octree-based learnable attention sparsification scheme that builds a dynamic octree on a hierarchical feature pyramid to capture rich global context in a coarse-to-fine manner with linear complexity. Additionally, a hybrid positional embedding (Semantic-Aware Positional Embedding and Semantic Attention Mask) is introduced for enhanced foreground perception, leveraging both geometry and semantic clues. OcTr achieves state-of-the-art results on Waymo Open Dataset and KITTI Dataset, showing significant improvements for far-away objects.",
        "methodology": "The OcTr framework voxelizes point clouds into regular grids, which are then processed by sparse 3D convolutions for patch embedding. These 'tokens' are fed into Octree Transformer Blocks (OTB), where the self-attention module is replaced by the proposed OctAttn. OctAttn constructs a multi-scale feature pyramid, applies self-attention to the top level to select relevant tokens (octants), and recursively propagates this attention to lower levels via cross-attention, restricting key/value inputs by the octants from the level above. Gumbel-topk is used for differentiable approximation of topk selection. A Locally enhanced Positional Embedding (LePE) is incorporated for local neighbor interactions. For foreground perception, a hybrid positional embedding is used, comprising Semantic-Aware Positional Embedding (SAPE) which concatenates coordinates and foreground segmentation scores with features, and a Semantic Attention Mask (SAM) which masks attention matrices based on segmentation scores to emphasize foreground correlations. The processed features are projected to a BEV view, passed through a 2D backbone, and finally an RPN head for 3D proposal generation, with an optional RoI head for refinement.",
        "experimental_setup": "OcTr was evaluated on the Waymo Open Dataset (WOD) and KITTI Dataset. WOD consists of 798 training sequences (approx. 160K LiDAR samples) and 202 validation sequences (40K LiDAR samples), using mAP and mAPH (mAP weighted by heading accuracy) as evaluation metrics for different difficulty levels (L1, L2) and distance ranges (0-30m, 30-50m, 50m-inf). KITTI includes 3,712 training, 3,769 validation, and 7,518 testing frames, using mAP with 11/40 recall points and IoU thresholds of 0.7 for car and 0.5 for pedestrian/cyclist. The implementation details include specific voxel sizes, a two-stage detection framework with Adam optimizer, cosine annealing learning rate scheduler, and hyper-parameters for the Octree Transformer layers. Ablation studies were conducted on various attention mechanisms, sampling patterns, the components of semantic positional embedding (LEPE, SAPE, SAM), the influence of the 'topk' value, and model complexity (parameters, latency, memory) compared to state-of-the-art baselines like SECOND, PV-RCNN, PV-RCNN++, VoTR-SSD, and VoxSeT-SSD.",
        "limitations": "The paper notes that while segmentation scores are used in the hybrid positional embedding to indicate grid significance, these scores can suffer from inaccurate predictions, which could impact the effectiveness of the Semantic Attention Mask (SAM).",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Efficient Hierarchical Entropy Model for Learned Point Cloud Compression"
    },
    {
      "title": "A new near-linear time algorithm for k-nearest neighbor search using a compressed cover tree",
      "abstract": "Given a reference set $R$ of $n$ points and a query set $Q$ of $m$ points in\na metric space, this paper studies an important problem of finding $k$-nearest\nneighbors of every point $q \\in Q$ in the set $R$ in a near-linear time. In the\npaper at ICML 2006, Beygelzimer, Kakade, and Langford introduced a cover tree\non $R$ and attempted to prove that this tree can be built in $O(n\\log n)$ time\nwhile the nearest neighbor search can be done in $O(n\\log m)$ time with a\nhidden dimensionality factor. This paper fills a substantial gap in the past\nproofs of time complexity by defining a simpler compressed cover tree on the\nreference set $R$. The first new algorithm constructs a compressed cover tree\nin $O(n \\log n)$ time. The second new algorithm finds all $k$-nearest neighbors\nof all points from $Q$ using a compressed cover tree in time $O(m(k+\\log n)\\log\nk)$ with a hidden dimensionality factor depending on point distributions of the\ngiven sets $R,Q$ but not on their sizes.",
      "full_text": "arXiv:2111.15478v5  [cs.CG]  4 Mar 2024 A New Near-linear Time Algorithm For k-Nearest Neighbor Search Using a Compressed Cover T ree Y ury Elkin 1 Vitaliy Kurlin 1 Abstract Given a reference set Rof npoints and a query set Q of m points in a metric space, this pa- per studies an important problem of ﬁnding k- nearest neighbors of every point q ∈ Q in the set Rin a near-linear time. In the paper at ICML 2006, Beygelzimer, Kakade, and Langford intro- duced a cover tree onRand attempted to prove that this tree can be built in O(nlog n) time while the nearest neighbor search can be done in O(nlog m) time with a hidden dimensional- ity factor. This paper ﬁlls a substantial gap in the past proofs of time complexity by deﬁning a simpler compressed cover tree on the reference setR. The ﬁrst new algorithm constructs a com- pressed cover tree in O(nlog n) time. The sec- ond new algorithm ﬁnds all k-nearest neighbors of all points from Q using a compressed cover tree in time O(m(k+ log n) log k) with a hidden dimensionality factor depending on point distri- butions of the given setsR,Q but not on their sizes. 1. The Neighbor Search, Overview Of Results In the modern formulation, thek-nearest neighbor problem is to ﬁnd all k ≥1 nearest neighbors in a given reference set Rfor all points from another given query set Q. Both sets belong to a common ambient space Xwith a dis- tance metric dsatisfying all metric axioms. The simplest example of Xis Rn with the Euclidean metric. A query set Qcan be a point or a ﬁnite subset of a reference set R. The exact k-nearest neighbor problem asks for all true (ex- act) k-nearest neighbors in Rfor every point q∈Q. 1 Department of Computer Science, University of Liverpool, Liverpool, United Kingdom. Correspondence to: Vitaliy Kur lin <vitaliy .kurlin@gmail.com>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). Another (probabilistic) version of the k-nearest neighbor search Har-Peled & Mendel (2006); Manocha & Girolami (2007) aims to ﬁnd exact k-nearest neighbors with a given probability. The approximate version Arya & Mount (1993); Krauthgamer & Lee (2004); Andoni et al. (2018); W ang et al. (2021) of the nearest neighbor search looks for an ǫ-approximate neighbor r ∈ R of every query point q ∈ Q such that d(q,r) ≤ (1 + ǫ)d(q,NN(q)) , where ǫ> 0 is ﬁxed and NN(q) is the exact ﬁrst nearest neighbor of q. Deﬁnition 1.1 (diameter and aspect ratio) . F or any ﬁ- nite set R with a metric d, the diameter is diam(R) = max p∈R max q∈R d(p,q). The aspect ratio is ∆( R) = diam(R) dmin(R) , where dmin(R) is the shortest distance between points of R. Deﬁnition 1.2 (k-nearest neighbor set NNk). F or any point q ∈Q, let d1 ≤···≤ d|R| be ordered distances from qto all points of a reference set Rwhose size (number of points) is denoted by |R|. F or any k ≥1, the k-nearest neighbor set NNk(q; R) consists of all u∈Rwith d(q,u) ≤dk. For Q = R = {0,1,2,3}, the point q = 1 has ordered distances d1 = 0 < d2 = 1 = d3 < d4 = 2 . The nearest neighbor sets are NN1(1; R) = {1}, NN2(1; R) = {0,1,2}= NN 3(1; R), NN4(1; R) = R. So 0 can be a 2nd neighbor of 1, then 2 becomes a 3rd neighbor of 1, or these neighbors of0 can be found in a different order. Problem 1.3 (all k-nearest neighbors search) . Let Q,R be ﬁnite subsets of query and reference sets in a metric space (X,d). F or any ﬁxed k≥1, design an algorithm to exactly ﬁnd k distinct points from NNk(q; R) for all q ∈ Q so that the parametrized worst-case time complexity is near- linear in timemax{|Q|,|R|}, where hidden constants may depend on structures of Q,R but not on their sizes |Q|,|R|. In a metric space, let ¯B(p,t) be the closed ball with a cen- ter pand a radius t≥0. The notation |¯B(p,t)|denotes the number (if ﬁnite) of points in the closed ball. Deﬁnition 1.4 recalls the expansion constantc from Beygelzimer et al. (2006a) and introduces the new minimized expansion con- stant cm, which is a discrete analog of the doubling dimen- sion Cole & Gottlieb (2006). 1A new compressed cover tree for k-nearest neighbors Deﬁnition 1.4 (expansion constants cand cm). A subset R of a metric space (X,d) is called locally ﬁnite if the set ¯B(p,t) ∩Ris ﬁnite for all p∈X and t∈R+. Let Rbe a locally ﬁnite set in a metric space X. The expansion constant c(R) is the smallest c(R) ≥2 such that |¯B(p,2t)|≤ c(R) ·|¯B(p,t)|for any point p∈Rand t≥0, see Beygelzimer et al. (2006a). Introduce the new minimized expansion constant cm(R) = lim ξ→0+ inf R⊆A⊆X sup p∈A,t>ξ |¯B(p,2t) ∩A| |¯B(p,t) ∩A|, where Ais a locally ﬁnite set which covers R. Lemma 1.5. F or any ﬁnite sets R ⊆U in a metric space, we have that cm(R) ≤cm(U) and cm(R) ≤c(R). Note that both c(R),cm(R) are always deﬁned when Ris ﬁnite. W e show below that a single outlier can make the expansion constantc(R) as large as O(|R|). In the Euclidean line R, The set R= {1,2,...,n, 2n+ 1} of |R|= n+ 1 points has c(R) = n+ 1 because ¯B(2n+ 1; n) = {2n+ 1}is a single point, while ¯B(2n+ 1; 2 n) = R is the full set of n+ 1 points. On the other hand, the same set R can be extended to a larger uniform set A = {1,2,..., 2n−1,2n}whose expansion constant is c(A) = 2. So the minimized constant of the original set Ris much smaller: cm(R) ≤c(A) = 2 <c(R) = n+ 1. The constant c from Beygelzimer et al. (2006a) equals 2dimKR from Krauthgamer & Lee (2004, Section 2.1). In Krauthgamer & Lee (2004, Section 1.1) the doubling di- mension2dim is deﬁned as a minimum value ρsuch that any set X can be covered by 2ρ sets whose diameters are half of the diameter of X. The past work Krauthgamer & Lee (2004) proves that 2dim ≤2n for any subset of Rn. Theorem C.15 in appendix C will prove that cm(R) ≤2n for any a ﬁnite subset R⊂Rn, so cm(R) mimics 2dim. Navigating nets . In 2004, Krauthgamer & Lee (2004, The- orem 2.7) claimed that a navigating net can be constructed in timeO ( 2O(dimKR (R)|R|(log |R|) log(log |R|) ) and all k-nearest neighbors of a query point q can be found in time O(2O(dimKR (R∪{q})(k+ log |R|), where dim KR (R∪ {q}) is the expansion constant deﬁned above. The paper above sketched a proof of Krauthgamer & Lee (2004, The- orem 2.7) in one sentence and skipped pseudo-codes. Un- fortunately, the authors didn’t reply to our request for these details. Modiﬁed navigating netsCole & Gottlieb (2006) were used in 2006 to claim the time O(log(n) + (1 /ǫ)O(1)) to ﬁnd (1 + ǫ)-approximate neighbors. The proof and pseudo- code were skipped for this claim and for the construc- tion of the modiﬁed navigating net for the claimed time O(|R|·log(|R|)). Cover trees . In 2006, Beygelzimer et al. (2006a) in- troduced a cover tree inspired by the navigating nets Krauthgamer & Lee (2004). This cover tree was designed to prove a worst-case time for the nearest neighbor search in terms of the size|R|of a reference set R and the ex- pansion constant c(R) from Deﬁnition 1.4. Assume that a cover tree is already constructed on the set R. Then Beygelzimer et al. (2006a, Theorem 5) claimed that a near- est neighbor of any query pointq ∈Qcould be found in time O(c(R)12 ·log |R|). In 2015, Curtin (2015, Section 5.3) pointed out that the proof of Beygelzimer et al. (2006a, Theorem 5) contains a crucial gap, now also conﬁrmed by a speciﬁc example in Elkin & Kurlin (2022a, Counterexample 5.2). The time complexity result of the cover tree construction algorithm Beygelzimer et al. (2006a, Theorem 6) had a similar issue, the gap of which is exposed rigorously in Elkin & Kurlin (2022a, Counterexample 4.2). Further studies in cover trees.A noteworthy paper on cover trees Kollar (2006) introduced a new probabilistic algorithm for the nearest neighbor search, as well as cor- rected the pseudo-code of the cover tree construction algo- rithm of Beygelzimer et al. (2006a, Algorithm 2). Later in 2015, a new , more efﬁcient implementation of cover tree was introduced in Izbicki & Shelton (2015). However, no new time-complexity results were proven. Another study Jahanseir & Sheehy (2016) explored con- nections between modiﬁed navigating nets Cole & Gottlieb (2006) and cover trees Beygelzimer et al. (2006a). Several papers Beygelzimer et al. (2006b); Ram et al. (2009); Curtin et al. (2015) studied the possibility of solv - ing k-nearest neighbor Problem 1.3 by using cover trees on both sets Q,R, see Elkin & Kurlin (2022a, Section 6). New contributions . This work corrects the past gaps of the single-tree approach Beygelzimer et al. (2006a), which were discovered in Elkin & Kurlin (2022a) by using a new compressed cover tree T(R) from Deﬁnition 2.1. • Theorem 3.7 and Corollary 3.11 estimate the time to build a compressed cover tree, which corrects the proof of Beygelzimer et al. (2006a, Theorem 6). • Theorem 4.9 and Corollary 4.7 estimate the time to ﬁnd all k-nearest neighbors as in Problem 1.3. These advances correct and generalize Beygelzimer et al. (2006a, Theorem 5). 2A new compressed cover tree for k-nearest neighbors T able 1. Building data structures with hidden cm(R) or dimensionality constant 2dim Krauthgamer & Lee (2004, Section 1.1). Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dim) ·|R|·log(∆) ·log(log((∆)) ) O(2O(dim)|R|) Krauthgamer & Lee (2004, Theorem 2.5) Compressed cover tree [Deﬁnition 2.1] O ( cm(R)O(1) ·|R|log(∆( R)) ) O(|R|) Lemma B.1 Theorem 3.7 T able 2. Results for building data structures with the hidden classi cal expansion constant c(R) of Deﬁnition 1.4 or KR-type constant 2dimKR Krauthgamer & Lee (2004, Section 2.1). Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dimKR ) ·|R|log(|R|) log(log |R|) ) , Krauthgamer & Lee (2004, Theorem 2.6) O(2O(dim)|R|) Not available Cover tree Beygelzimer et al. (2006a) O(c(R)O(1) ·|R|· log |R|), Beygelzimer et al. (2006a, Theorem 6) O(|R|) Elkin & Kurlin (2022a, Counterexample 4.2) shows that the past proof is incorrect Compressed cover tree [Deﬁnition 2.1] O ( c(R)O(1) ·|R|·log |R| ) O(|R|) Lemma B.1 Corollary 3.11 T able 3. Results for exact k-nearest neighbors of one query point q ∈ X using the hidden classical expansion constant c(R) of Deﬁnition 1.4 or KR-type constant 2dimKR Krauthgamer & Lee (2004, Section 2.1) and assuming that all d ata structures are already built. Note that the dimensionality factor 2dimKR is equivalent to c(R)O(1). Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dimKR )(log(|R|) + k) ) for k≥1 Krauthgamer & Lee (2004, Theorem 2.7) O(2O(dim)|R|) Not available Cover tree Beygelzimer et al. (2006a) O ( c(R)O(1) log |R| ) for k= 1 Beygelzimer et al. (2006a, Theorem 5) O(|R|) Elkin & Kurlin (2022a, Coun- terexample 5.2) shows that the past proof is incorrect Compressed cover tree, Deﬁnition 2.1 O ( c(R∪{q})O(1) ·log(k) · (log(|R|) + k) ) O(|R|), Lemma B.1 Theorem 4.9 T able 4. Results for exact k-nearest neighbors of one point q using hidden cm(R) or dimensionality constant 2dim Krauthgamer & Lee (2004, Section 1.1) assuming that all structures are built. Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dim) ·log(∆) + |¯B(q,O(d(q,R))| ) for k= 1 O(2O(dim)|R|) a proof outline in Krauthgamer & Lee (2004, Theorem 2.3) Compressed cover tree, Deﬁnition 2.1 O ( log(k) ·(cm(R)O(1) log(|∆ |) + |¯B(q,O(dk(q,R))|) ) O(|R|), Lemma B.1 Corollary 4.7 3A new compressed cover tree for k-nearest neighbors l= ∞ l= 5 l= 4 l= 3 l= 2 l= 1 l= −∞ r r r r r r r p4 p4 p4 p4 p4 p3 p3 p3 p3 p2 p2 p2 p1 p1 r r r p4 p4 p3 p2 p2p1 r p4 p2 p1 p3 Figure 1. A comparison of past cover trees and a new compressed cover tr ee in Example B.3. Left: an implicit cover tree contains inﬁnite repetitions. Middle: an explicit cover tree. Right: a compressed cover tree from Deﬁnition 2.1 includes each giv en point exactly once. 2. A New Compressed Cover T ree This section introduces in Deﬁnition 2.1 a new compressed cover tree to solve Problem 1.3. W e also prove relevant properties of the expansion constantc(R) and minimized expansion constant cm(R) of Deﬁnition 1.4. All extra de- tails and proofs of this section are in Appendices B,D. A compressed cover tree in Deﬁnition 2.1 will be signif- icantly simpler than an explicit cover tree Elkin & Kurlin (2022a, Deﬁnition 2.2), where any given point pcan appear in many different nodes simultaneously. T o regain the functionality of the explicit cover tree, we introduce the new concept of a distinctive descendant set Si(p,T(R)) in Deﬁnition 2.8. See Figure 1 for a compari- son between implicit, explicit, and compressed cover trees . Deﬁnition 2.1 (a compressed cover tree T(R)). Let Rbe a ﬁnite set in a metric space (X,d). A compressed cover tree T(R) has the vertex set Rwith a root r ∈Rand a level function l: R→Z satisfying the conditions below . (2.1a) Root condition : the level of the root node rsatisﬁes l(r) ≥1 + max p∈R\\{r} l(p). (2.1b) Cover condition : for every node q ∈R\\{r}, we select a unique parent pand a level l(q) such that d(q,p) ≤ 2l(q)+1 and l(q) <l(p); this parent node phas a single link to its child node q. (2.1c) Separation condition : for i ∈ Z, the cover set Ci = {p ∈ R | l(p) ≥ i} has dmin(Ci) = min p∈Ci min q∈Ci\\{p} d(p,q) >2i. Since there is a 1-1 map between Rand all nodes of T(R), the same notation pcan refer to a point in the set Ror to a node of the tree T(R). Set lmax = 1 + max p∈R\\{r} l(p) and lmin = min p∈R l(p). F or any node p∈T (R), Children(p) de- notes the set of all children of p, including pitself. F or any node p ∈T (R), deﬁne the node-to-root path as a unique sequence of nodes w0,...,w m such that w0 = p, wm is the root and wj+1 is the parent of wj for j = 0 ,...,m −1. A node q ∈T (R) is a descendant of a node pif pis in the node-to-root path of q. A node pis an ancestor of qif qis in the node-to-root path of p. Let Descendants(p) be the set of all descendants of p, including itself p. Lemma 2.2 links the minimized expansion constant with the doubling dimension. This result is used in the proofs of the width bound of a compressed cover tree in Lemma 2.3, also for the time complexity of a compressed cover tree construction in Lemma 3.3, and for thek-nearest neighbor search in Lemma 4.5. All hyperlinks are clickable. Lemma 2.2(packing). Let S be a ﬁnite δ-sparse set in a metric space (X,d), so d(a,b) > δ for all a,b ∈ S. Then, for any point p ∈X and any radius t > δ, we have |¯B(p,t) ∩S|≤ (cm(S))µ, where µ= ⌈log2(4t δ + 1)⌉. Proof of Lemma 2.2 is in Appendix B. Lemma 2.3 shows that the number of children of any node of a compressed cover tree on any speciﬁc level can be 4A new compressed cover tree for k-nearest neighbors bounded by using minimized expansion constant cm(R). Lemma 2.3 (width bound) . Let R be a ﬁnite subset of a metric space (X,d). F or any compressed cover tree T(R), any node pand any level i≤l(p) we have {q∈Children(p) |l(q) = i}∪{p}≤ (cm(R))4, where cm(R) is the minimized expansion constant of R. Lemma 2.4 is an important property of the expansion con- stant, which allows us to calculate the low-bound of the number of points in the larger ball¯B(q,4r) for any node q∈Rand radius r∈R+ using the smaller ball ¯B(q,r) and the expansion constant c(R), if there exists a point p ∈R which is located in annulus 2r<d (p,q) ≤3r. Lemma 2.4 (growth bound) . Let (A,d) be a ﬁnite metric space, let q ∈A be an arbitrary point and let r ∈R be a real number . Let c(A) be the expansion constant from Deﬁnition 1.4. If there exists a point p∈Asuch that 2r < d(p,q) ≤3r, then |¯B(q,4r)|≥ (1 + 1 c(A)2 ) ·|¯B(q,r)|. Lemma 2.5 is a generalization of Lemma 2.4 and will be used to estimate the number of iterations in compressed cover tree construction algorithm, Lemma 3.9 and in the k-nearest neighbors algorithm, Lemma 4.8. Lemma 2.5 (extended growth bound) . Let (A,d) be a ﬁnite metric space, let q∈Abe an arbitrary point. Let p1,...,p n be a sequence of distinct points in R, in such a way that for all i∈{2,...,n }we have 4 ·d(pi,q) ≤d(pi+1,q). Then |¯B(q,4 3 ·d(q,pn))|≥ (1 + 1 c(A)2 )n ·|¯B(q,1 3 ·d(q,p1))|. Deﬁnition 2.6 (the height of a compressed cover tree) . F or a compressed cover tree T(R) on a ﬁnite set R, the height set is H(T(R)) = {i |Ci−1 ̸= Ci}∪{ lmax,lmin}. The size |H(T(R))|of this set is called the height of T(R). Lemma 2.7. Any ﬁnite set R has the upper bound |H(T(R))|≤ 1 + log 2(∆( R)). Intuitively Si(p,T(R)) denotes all the descendants of pair (p,i) in the explicit or implicit cover tree. Deﬁnition 2.8 (Distinctive descendant sets) . Let R⊆Xbe a ﬁnite reference set with a compressed cover tree T(R). F or any node p ∈ T(R) and level i ≤ l(p) −1, set Vi(p) = {u∈Descendants(p) |i ≤l(u) ≤l(p) −1}.If i≥l(p), then set Vi(p) = ∅. F or any level i≤l(p), the dis- tinctive descendant set is Si(p,T(R)) = Descendants( p)\\⋃ u∈Vi(p) Descendants(u) and has the size |Si(p,T(R))|. Lemma 2.9 shows that if q ∈Si(p,T(R)) then there is a node-to-node path q = a0,...,a m = p, so that l(am−1) ≤ i−1. Lemma 2.9. Let R ⊆X be a ﬁnite reference set with a cover tree T(R). In the notations of Deﬁnition 2.8, let p∈ T(R) be any node. If w ∈Si(p,T(R)) then either w = p or there exists a ∈Children(p) \\{p}such that l(a) < i and w∈Descendants(a). Deﬁnition 2.10 explains the concrete implementation of a compressed cover tree. Deﬁnition 2.10(Children(p,i) and Next(p,i, T(R))). In a compressed cover tree T(R) on a set R, for any level i and a node p ∈ R, set Children(p,i) = {a ∈ Children(p) |l(a) = i}. Let Next(p,i, T(R)) be the max- imal level j satisfying j < iand Children(p,i) ̸= ∅. If such level does not exist, we set j = lmin(T(R)) −1. F or every node p, we store its set of children in a linked hash map so that (a) any key igives access to Children(p,i), (b) Children(p,i) →Children(p,Next(p,i, T(R))), (c) we can directly access max{j|Children(p,j) ̸= ∅}. 3. Construction Of a Compressed Cover T ree This section discusses a construction of a compressed cover tree. New Algorithm 3.4 builds a compressed cover tree by using the Insert() method from Beygelzimer et al. (2006a, Algorithm 2), which was speciﬁcally adapted for a com- pressed cover tree, see details in Appendix E. The proof of Beygelzimer et al. (2006a, Theorem 6), which estimated the time complexity of Beygelzimer et al. (2006a, Algorithm 2), was shown to be incorrect by Elkin & Kurlin (2022a, Counterexample 4.2). The main contribution of this section estimate the time complexity of Algorithm 3.4: • Theorem 3.7 bounds the time complexity as O(cm(R)10 · log2(∆( R)) · |R|) via the mini- mized expansion constant cm(R) and the aspect ratio ∆( R). • Theorem 3.10 bounds the time complexity as O(c(R)12 ·log2 |R|·|R|) via the expansion constant c(R). Deﬁnition 3.1 (construction iteration set L(T(W),p)). Let W be a ﬁnite subset of a metric space (X,d). Let T(W) be a cover tree of Deﬁnition 2.1 built on W and let p∈X\\W be an arbitrary point. Let L(T(W),p) be the set of all levels i during iterations 6-16 of Algorithm 3.5 launched with the inputs T(W),p. W e set η(i) = min {t∈L(T(W),p) |t>i }. 5A new compressed cover tree for k-nearest neighbors Level 2 Level 1 Level 0 Level -1 1 5 3 2 4 7 8 Figure 2. Consider a compressed cover tree T (R) that was built on set R = {1,2,3,4,5,7,8}. Let Si(p,T (R)) be a distinctive descendant set of Deﬁnition 2.8. Then V2(1) = ∅, V1(1) = {5} and V0(1) = {3,5,7}. And also S2(1,T (R)) = {1,2,3,4,5,7,8}, S1(1,T (R)) = {1,2,3,4} and S0(1,T (R)) = {1}. Theorem 3.2 (correctness of Algorithm 3.4) . Algo- rithm 3.4 builds a compressed cover tree in Deﬁnition 2.1. Lemma 3.3(time complexity of a key step for T(R)). Arbitrarily order all points of a ﬁnite reference set R in a metric space (X,d) starting from the root: r = p1, p2,...,p |R|. Set W1 = {r}and Wy+1 = Wy ∪{py}for y= 1 ,..., |R|−1. Then Algorithm 3.4 builds a compressed cover tree T(R) in time O ( (cm(R))8 · max y=1,...,|R|−1 L(T(Wy ),py) ·|R| ) , where cm(R) is the minimized expansion constant from Deﬁnition 1.4. Algorithm 3.4 Building a compressed cover tree T(R) from Deﬁnition 2.1. 1: Input : a ﬁnite subset Rof (X,d), root r∈R 2: Output : a compressed cover tree T(R). 3: Build the initial compressed cover tree T = T({r}) consisting of the root node rby setting l(r) = + ∞. 4: for p∈R\\{r}do 5: T ←run AddPoint (T,p), Algorithm 3.5. 6: end for 7: For the root rof T set l(r) = 1 + max p∈R\\{r} l(p) Theorem 3.7 (time complexity of T(R) via aspect ratio) . Let R be a ﬁnite subset of a metric space (X,d) having the aspect ratio ∆( R). Algorithm 3.4 builds a compressed cover tree T(R) in time O((cm(R))8 ·log2(∆( R)) ·|R|), where cm(R) is the minimized expansion constant from Deﬁnition 1.4. Proof. In Lemma 3.3, use the bounds from Lemma 2.7: max y=2,...,|R| |L(T(Wy−1),py)|≤ H(T(R)) ≤1+log2 ∆( R). Algorithm 3.5 Building T(W ∪{p}) in lines 4-6 of Algo- rithm 3.4. 1: Function AddPoint(a compressed cover tree T(W) with a root r, a point p∈X) 2: Output : compressed cover tree T(W ∪{p}). 3: Set i←lmax(T(W)) −1 and η(lmax −1) = lmax {If the root rhas no children then i←−∞} 4: Set Rlmax ←{r} 5: initialize sorted dictionary M with M[lmax] = {r} 6: while i≥lmin do 7: V = ∪q∈Rη(i) {a∈Children(q) |l(a) = i}. 8: Assign Ci(Rη(i)) ←Rη(i) ∪V. 9: Set Ri = {a∈Ci(Rη(i)) |d(p,a) ≤2i+1} 10: Assign M[i] = Ri. 11: if Ri is empty then 12: Launch Algorithm 3.6 with parameters (p,M) and exit this algorithm . 13: end if 14: t= max a∈Ri Next(a,i, T(W)) {If Ri has no children we set t= lmin −1} 15: η(i) ←iand i←t 16: end while 17: Launch Algorithm 3.6 with parameters (p,M). Algorithm 3.6 Assign node subprocedure 1: Function AssignParent(Point p, dictionary M) 2: Output: Compressed cover tree T(W ∪{p}) 3: Set ito be the lowest key of M. 4: while i≤lmax do 5: if d(p,Ri) ≤2i then 6: Let q ∈Ri such that d(q,p) = d(Ri,p), let x maximal integer for which d(p,q) >2x. 7: Set l(p) = xand qto be the parent of p. 8: end if 9: Find next key j >iof M and set i= j 10: end while 6A new compressed cover tree for k-nearest neighbors Lemma 3.8. Let (X,d) be a metric space and let W ⊆X be its ﬁnite subset. Let q ∈X \\W be an arbitrary point. Let i ∈L(T(W),q) be arbitrarily iteration of Deﬁnition 3.1. Assume that t = η(η(i+ 1)) is deﬁned. Then there exists p∈W satisfying 2i+1 <d(p,q) ≤2t+1. Lemma 3.9 (Construction iteration bound) . Let A,W be ﬁnite subsets of a metric space X satisfying W ⊆A⊆X. T ake a point q ∈A\\W. Given a compressed cover tree T(W) on W, Algorithm 3.5 runs lines 6-16 this number of times: |L(T(W),q)|= O ( c(A)2 ·log2(|A|) ) . Outline Proof. Assume that Algorithm 3.5 was launched with parameters (q,T(W)) Lemma 3.8 showed that for any iterations i ∈L(T(W),q), if t = η(η(i+ 1)) exists, then there exists p∈W which belongs to annulus ¯B(q,2t+1) \\ ¯B(q,2i+1). W e can select a subsequence S of iterations L(T(W),q), in such a way that for every i∈Sthere exists point pi ∈ ¯B(q,2t+1) \\ ¯B(q,2i+1). It can be shown that the size of Sselected this way is 12 ·|S|≥| L(T(W),q)| Denote by P = ( p1,...,p n) the sequence of points pi ob- tained from S. Using Lemma 2.5 we obtain |¯B(q,4 3 d(q,pn))|≥ (1 + 1 c(R)2 )n ·|¯B(q,1 3d(q,p1))| which can be written as |A|≥ |¯B(q,4 3 ·d(q,pn))| |¯B(q,1 3 ·d(q,p1))|≥(1 + 1 c(A)2 )|S| Lemma B.7 gives c(A)2 log(A) ≥ |S|. Combining this with the fact that 12 ·|S|≥| L(T(W),q)|we ﬁnally con- clude that |L(T(W),q)|≤ 12 ·c(A)2 ·log2(|A|). Theorem 3.10 (time for T(R) via expansion constants) . Let R be a ﬁnite subset of a metric space (X,d). Let A be a ﬁnite subset of X satisfying R ⊆A ⊆X. Then Al- gorithm 3.4 builds a compressed cover tree T(R) in time O((cm(R))8 ·c(A)2 ·log2(|A|) ·|R|), see the expansion constants c(A),cm(R) in Deﬁnition 1.4. Proof. It follows from Lemmas 3.9 and 3.3. Corollary 3.11. Let Rbe a ﬁnite subset of a metric space (X,d). Then Algorithm 3.4 builds a compressed cover tree T(R) in time O((cm(R))8 ·c(R)2 ·log2(|R|))·|R|),where the constants c(R),cm(R) appeared in Deﬁnition 1.4. Proof. In Theorem 3.10 set A= R. 4. New k-Nearest Neighbor Search Algorithm This section is motivated by Elkin & Kurlin (2022a, Coun- terexample 5.2), which showed that the proof of past time complexity claim in Beygelzimer et al. (2006a, Theorem 5) for the nearest neighbor search algorithm contained gaps. For extra details and all proofs, see Appendix F. The gaps are ﬁlled by new Algorithm 4.3 for all k-nearest neighbors, which generalizes and improves the original method in Beygelzimer et al. (2006a, Algorithm 1). The ﬁrst improvement is the λ-point in line 7, which helps ﬁnd all k-nearest neighbors of a given query point for any k ≥ 1. The second improvement is a new break condi- tion for the loop in line 9. This condition is used in the proof of Lemma 4.8 to conclude that the total number of performed iterations is bounded byO(c(R)2 log(|R|)) dur- ing the whole run-time of the algorithm. The latter improvement corrects the past gap in proof of Beygelzimer et al. (2006a, Theorem 5) by bounding the number of iterations independently from the explicit depth Elkin & Kurlin (2022a, Deﬁnition 3.2). Assuming that we have already constructed a compressed cover tree on a reference set R, the two main results es- timate the time complexity of a new k-nearest neighbor method in Algorithm 4.3m which ﬁnds all k-nearest neigh- bors of any query point q∈Xin a reference set R⊆X as follows: • Corollary 4.7 bounds the time complexity as O ( log2(k) · (log2(∆( R)) + |¯B(q,5dk(q,R))|) ) , where ∆( R) is the aspect ratio and cm(R) is consid- ered ﬁxed (hence hidden). • Theorem 4.9 bounds the time complexity as O ( log2(k) · ( log2(|R|) + k ) ) , where the ex- pansion constant c(R ∪ {q}) is considered ﬁxed (hence hidden). Deﬁnition 4.1 (λ-point). Fix a query point q in a metric space (X,d) and ﬁx any level i∈Z. Let T(R) be its com- pressed cover tree on a ﬁnite reference set R ⊆X. Let C be a subset of a cover set Ci from Deﬁnition 2.1 satisfying∑ p∈C |Si(p,T(R))|≥ k, where Si(p,T(R)) is the distinc- tive descendant set from Deﬁnition 2.8. F or any k ≥ 1, deﬁne λk(q,C) as a point λ ∈C that minimizes d(q,λ) subject to ∑ p∈N(q;λ) |Si(p,T(R))|≥ k. Deﬁnition 4.2. Let Rbe a ﬁnite subset of a metric space (X,d). Let T(R) be a cover tree of Deﬁnition 2.1 built on R and let q ∈X be arbitrary point. Let L(T(R),q) be 7A new compressed cover tree for k-nearest neighbors the set of all levels iduring iterations of lines 4-15 of Algo- rithm 4.3 launched with inputs T(R),q. If Algorithm 4.3 reaches line 11 at a level ̺∈L(T(R),q), then we say that ̺is special. Set η(i) = min {t∈L(T(R),q) |t>i }. Algorithm 4.3 k-nearest neighbor search by a compressed cover tree 1: Input : compressed cover tree T(R), a query point q∈X, an integer k∈Z+ 2: Set i←lmax(T(R)) −1 and η(lmax −1) = lmax 3: Let rbe the root node of T(R). Set Rlmax = {r}. 4: while i≥lmin do 5: V = ∪q∈Rη(i) {a∈Children(q) |l(a) = i}. 6: Assign Ci(Rη(i)) ←Rη(i) ∪V. 7: Compute λ= λk(q,Ci(Rη(i))) by Algorithm D.8. 8: Ri = {p∈Ci(Rη(i)) |d(q,p) ≤d(q,λ) + 2 i+2} 9: if d(q,λ) >2i+2 then 10: Collect the distinctive descendants Si(p,T(R)) of all points p∈Rin set S, see Algorithm F .3. 11: Compute and output k-nearest neighbors of the query point qfrom set S. 12: end if 13: Set j ←maxa∈Ri Next(a,i, T(R)) {If such jis undeﬁned, we set j = lmin −1} 14: Set η(j) ←iand i←j. 15: end while 16: Compute and output k-nearest neighbors of query point qfrom the set Rlmin . Theorem 4.4 (correctness of Algorithm 4.3) . Algo- rithm 4.3 correctly ﬁnds all k-nearest neighbors of query point qwithin the reference set R. Lemma 4.5. Algorithm 4.3 has the following time complex- ities of its lines (a) max{#Line[4 −9],#Line[12 −15],#Line[16]} = O ( cm(R)10 ·log2(k) ) ; (b) #Line[8 −14] = O ( |¯B(q,5dk(q,R))|·log2(k) ) . Theorem 4.6. Let R be a ﬁnite set in a metric space (X,d), cm(R) be the minimized constant from Deﬁnition 1.4. Given a compressed cover tree T(R), Algorithm 4.3 ﬁnds all k-nearest neighbors of a query point q ∈X in time O ( log2(k)·((cm(R))10·|L(q,T(R))|+|¯B(q,5dk(q,R))|) ) , where L(T(R),q) is the set of all performer iterations (lines 4-15 ) of Algorithm 4.3. Proof. Apply Lemma 4.5 to estimate the time complexity of Algorithm 4.3: O ( |L(T(R),q)|· (#Line[4 −9] + #Line[12 −15] + #Line[16]) + #Line[9 −12] ) . Corollary 4.7 gives a run-time bound using only mini- mized expansion constantcm(R), where if R ⊂Rm, then cm(R) ≤2m. Recall that ∆( R) is the aspect ratio of R introduced in Deﬁnition 1.1. Corollary 4.7.Let R be a ﬁnite set in a metric space (X,d). Given a compressed cover tree T(R), Algo- rithm 4.3 ﬁnds all k-nearest neighbors of q in time O ( (cm(R))10 ·log2(k) ·log2(∆( R)) + |¯B(q,5dk(q,R))|· log2(k) ) . Proof. Replace |L(q,T(R))| in the time complexity of Theorem 4.6 by its upper bound in Lemma 2.7: |L(q,T(R))|≤| H(T(R))|≤ log2(∆( R)). Lemma 4.8 is proved similarly to Lemma 3.9. For full de- tails see Appendix G. Lemma 4.8.Algorithm 4.3 executes lines 4-15 the follow- ing number of times: |L(T(R),q)| = O(c(R ∪{q})2 · log2(|R|)). Theorem 4.9. Let R be a ﬁnite reference set in a metric space (X,d). Let q ∈ X be a query point, c(R∪{q}) be the expansion constant of R∪{q}and cm(R) be the minimized expansion constant from Deﬁnition 1.4. Given a compressed cover treeT(R), Algorithm 4.3 ﬁnds all k- nearest neighbors of q in time O ( c(R∪{q})2 ·log2(k) · ( (cm(R))10 ·log2(|R|) + c(R∪{q}) ·k ) ) . Proof. By Theorem 4.6 the required time complexity is O ( (cm(R))10 ·log2(k) ·|L(q,T(R))|+ |¯B(q,5d(q,β))|· log2(k) ) , for some point βamong the ﬁrst k-nearest neigh- bors of q. Apply Deﬁnition 1.4 to get the upper bound |B(q,5d(q,β))|≤ (c(R∪{q}))3 ·|B(q,5 8 d(q,β))| (1) Since |B(q, 5 8 d(q,β))| ≤k, we have |B(q,5d(q,β))| ≤ (c(R ∪{q}))3 ·k. It remains to apply Lemma 4.8: |L(q,T(R))|= O(c(R∪{q})2 ·log2 |R|). 5. Discussion Of Contributions and Next Steps This paper rigorously proved the time complexity of the exact k-nearest neighbor search. The submission to ICML is strongly motivated by the past gaps in the proofs of time complexities in the highly cited Beygelzimer et al. (2006a, Theorem 5) at ICML, Ram et al. (2009, Theorem 3.1) at NIPS, and March et al. (2010, Theorem 5.1) at KDD. 8A new compressed cover tree for k-nearest neighbors Though Elkin & Kurlin (2022a) provided concrete coun- terexamples, no corrections were published. Main Theo- rem 4.9 and Corollary 3.11 ﬁnally ﬁlled all the gaps. Since the past obstacles were caused by unclear descrip- tions and missed proofs, often without pseudo-codes, this paper necessarily ﬁlls in all technical details. Otherwise, future generations would continue citing unreliable resul ts. T o overcome the discovered challenges, ﬁrst Deﬁnition 1.2 and Problem 1.3 rigorously dealt with a potential ambigu- ity ofk-nearest neighbors at equal distances. This singular case was unfortunately not discussed in the past work at all. A new compressed cover tree in Deﬁnition 2.1 substantially simpliﬁed the navigating net Krauthgamer & Lee (2004) and original cover tree Beygelzimer et al. (2006a) by avoid- ing repetitions of given data points. This compression clar- iﬁed the construction and search in Algorithms 3.4 and 4.3. Sections 3 and 4 corrected the approach of Beygelzimer et al. (2006a) as follows. Assuming that the expansion constants and aspect ratio of a reference set Rare ﬁxed, Corollaries 3.11 and 4.9 rigorously showed that the time complexities are linear in the maximum size ofR,Q and near-linear O(klog k) in the number k of neighbors. The library MLpack (Curtin et al., 2013) implemented a version of an explicit cover tree, which was later deﬁned in Elkin & Kurlin (2022a, Counterexample 4.2). The im- plementation of a compressed cover tree is similar but con- ceptually simpler due to its easier structure in Fig. 1. The new results justify that the MLpack implementations of the k-nearest neighbors search now have proved theo- retical guarantees for a near-linear time complexity, whic h was practically important for the recent advances below . Main Theorem 4.9 helped justify a near-linear time com- plexity for several invariants based on computingk-nearest neighbors in a new area of Geometric Data Science , whose aim is to build continuous geographic-style maps for mod- uli space of real data objects parametrized by complete in- variants under practically important equivalence relatio ns. The key example is a ﬁnite cloud of unlabeled points up to isometry maintaining all inter-point distances. The mos t general isometry invariant SDD ( Simplexwise Distance Dis- tribution (Kurlin, 2023a)) is conjectured to be complete for any ﬁntie point clouds in any metric space. In a Euclidean spaceRn, the SDD was adapted to the stronger invariant SCD ( Simplexwise Centered Distribution (Widdowson & Kurlin, 2023)), whose completeness and polynomial complexity (in the number m of points for a ﬁxed dimension n) was proved in (Kurlin, 2023b). The related and much harder problem is for periodic sets of unlabeled points, which model all solid crystalline materi - als (periodic crystals). The ﬁrst generically complete inv ari- ant using k-nearest neighbors was the sequence of density functions ψk(t) measuring a fractional volume of k-fold in- tersections of balls with a variable radius t and centers at all atoms of a crystal (Edelsbrunner et al., 2021). These density functions have efﬁcient algorithms in the low dimensions n = 2 ,3 through higher-degree V oronoi do- mains (Smith & Kurlin, 2022) of periodic point sets. The ﬁrst continuous and complete invariant for periodic point sets in Rn is the isoset of local atomic environments up to a justiﬁed stable radius (Anosova & Kurlin, 2021). The ﬁrst continuous metric on isosets was introduced in (Anosova & Kurlin, 2022) with an approximate algorithm that has a polynomial time complexity (for a ﬁxed dimen- sionn) and a small approximation factor (about 4 in R3). The much faster generically complete isometry invariant for both ﬁnite and periodic sets of points is the PDD (Point- wise Distance Distribution (Widdowson & Kurlin, 2021)) consisting of distances to knearest neighbors per point. The implemented search for atomic neighbors was so fast that all (more than 660 thousand) periodic crystals in the world’s largest database of real materials were hierarchi- cally compared by the PDD and its simpliﬁed version AMD (A verage Minimum Distance (Widdowson et al., 2022)). Due to the ultra-fast running time, more than 200 billion pairwise comparisons were completed over two days on a modest desktop while past tools were estimated to require over 34 thousand years (Widdowson & Kurlin, 2022). The most important conclusion from the search results is the Crystal Isometry Principle saying that any real periodic crystal has a uniquely deﬁned location in a single contin- uous space of all isometry classes of periodic point sets (Widdowson & Kurlin, 2022). ThisCrystal Isometry Space contains all known and not yet discovered crystals similar to the much simpler and discret e Mendeleev’s table of chemical elements. The next step is to improve the complexity of the k-nearest neighbor search to a purely linear time O(c(R)O(1)|R|) with no other extra hidden parameters by using a new com- pressed cover tree on both sets Q,R. Since a similar approach Ram et al. (2009) was shown to have incorrect proof in Elkin & Kurlin (2022a, Counterex- ample 6.5) and Curtin et al. (2015) used some additional 9A new compressed cover tree for k-nearest neighbors parameters I,θ, this goal will require signiﬁcantly more ef- fort to understand if O(c(R)O(1)|R|) is achievable by using a compressed cover tree. W e thank all reviewers for their time and helpful sugges- tions. This work was supported by the EPSRC grants EP/R018472/1, EP/X018474/1, and the Royal Academy Engineering fellowship IF2122/186 of the second author. References Andoni, A., Indyk, P ., and Razenshteyn, I. Approximate nearest neighbor search in high dimensions. In Proceed- ings of the International Congress of Mathematicians: Rio de Janeiro 2018 , pp. 3287–3318, 2018. Anosova, O. and Kurlin, V . An isometry classiﬁcation of periodic point sets. In Lecture Notes in Computer Sci- ence (Proceedings of DGMM) , volume 12708, pp. 229– 241, 2021. Anosova, O. and Kurlin, V . Algorithms for continuous met- rics on periodic crystals. arxiv:2205.15298 , 2022. Arya, S. and Mount, D. M. Approximate nearest neighbor queries in ﬁxed dimensions. In SODA, volume 93, pp. 271–280, 1993. Beckmann, N., Kriegel, H.-P ., Schneider, R., and Seeger, B. The r*-tree: An efﬁcient and robust access method for points and rectangles. InProceedings of the ACM SIGMOD International Conf. on Management of Data , pp. 322–331, 1990. Bentley, J. and Friedman, J. Fast algorithms for construct- ing minimal spanning trees in coordinate spaces. IEEE T ransactions on Computers , 27(02):97–105, 1978. Bentley, J. L. Multidimensional binary search trees used for associative searching. Communications of the ACM , 18(9):509–517, 1975. Berchtold, S., Keim, D., and Kriegel, H. The x-tree: An index structure for high-dimensional data. In V ery Large Data-Bases, pp. 28–39, 1996. Beygelzimer, A., Kakade, S., and Langford, J. Cover trees for nearest neighbor. In Proceedings of ICML , pp. 97– 104, 2006a. Beygelzimer, A., Kakade, S., and Lang- ford, J. Extended version of ”cover trees for nearest neighbor”. 2006b. URL https://hunch.net/˜jl/projects/cover_tree/paper/paper.pdf. Cole, R. and Gottlieb, L.-A. Searching dynamic point sets in spaces with bounded doubling dimension. In Proceed- ings of the thirty-eighth annual ACM symposium on The- ory of computing, pp. 574–583, 2006. Cormen, T . Introduction to algorithms . MIT Press McGraw-Hill, Cambridge, Mass. New Y ork, 1990. Curtin, R. R. Improving dual-tree algorithms . PhD thesis, Georgia Institute of T echnology, 2015. Curtin, R. R., Cline, J. R., Slagle, N. P ., March, W . B., Ram, P ., Mehta, N. A., and Gray, A. G. Mlpack: A scalable c++ machine learning library.Journal of Machine Learn- ing Research , 14(Mar):801–805, 2013. Curtin, R. R., Lee, D., March, W . B., and Ram, P . Plug- and-play dual-tree algorithm runtime analysis. J. Mach. Learn. Res. , 16:3269–3297, 2015. Edelsbrunner, H., Heiss, T ., Kurlin, V ., Smith, P ., and Win- traecken, M. The density ﬁngerprint of a periodic point set. InProceedings of Symposium on Computational Ge- ometry, pp. 32:1–32:16, 2021. Elkin, Y . and Kurlin, V . Counterexamples expose gaps in the proof of time complexity for cover trees introduced in 2006. InT opological Data Analysis and V isualization (T opoInV is, arxiv:2208.09447), 2022a. Elkin, Y . and Kurlin, V . Paired compressed cover trees guarantee a near linear parametrized complexity for all k-nearest neighbors search in an arbitrary metric space. arXiv:2201.06553 , 2022b. Finkel, R. A. and Bentley, J. L. Quad trees a data structure for retrieval on composite keys. Acta informatica , 4(1): 1–9, 1974. Fukunaga, K. and Narendra, P . M. A branch and bound al- gorithm for computing k-nearest neighbors. IEEE trans- actions on computers , 100(7):750–753, 1975. Fussell, D. and Subramanian, K. R. F ast ray tracing using kd trees . University of T exas at Austin, Department of Computer Sciences, 1988. Har-Peled, S. and Mendel, M. Fast construction of nets in low-dimensional metrics and their applications. SIAM Journal on Computing , 35(5):1148–1184, 2006. Holmes, M. P ., Gray, A. G., and Isbell Jr, C. L. Quic-svd: Fast svd using cosine trees. In NIPS, pp. 673–680, 2008. Izbicki, M. and Shelton, C. Faster cover trees. In Interna- tional Conference on Machine Learning , pp. 1162–1170. PMLR, 2015. Jahanseir, M. and Sheehy, D. Transforming hierarchical trees on metric spaces. In Canadian Conference on Com- putational Geometry , pp. 107–113, 2016. Jones, F . Lebesgue integration on Euclidean space . Jones and Bartlett, Sudbury, MA, November 2000. 10A new compressed cover tree for k-nearest neighbors Kollar, T . F ast nearest neighbors . T echnical report, Com- puter Science and Artiﬁcial Intelligence Lab, 2006. Krauthgamer, R. and Lee, J. R. Navigating nets: Simple algorithms for proximity search. In Proceedings of the ﬁfteenth annual ACM-SIAM symposium on Discrete al- gorithms, pp. 798–807, 2004. Kurlin, V . Simplexwise distance distributions for ﬁnite spaces with metrics and measures. arXiv:2303.14161 , 2023a. Kurlin, V . The strength of a simplex is the key to a contin- uous isometry classiﬁcation of Euclidean clouds of unla- belled points.arXiv:2303.13486 , 2023b. Lin, K., Jagadish, H., and Faloutsos, C. The tv-tree: An index structure for high-dimensional data. The VLDB Journal, 3(4):517–542, 1994. Liu, T ., Moore, A. W ., Gray, A. G., and Y ang, K. An inves- tigation of practical approximate nearest neighbor algo- rithms. InNIPS, volume 12, pp. 2004, 2004. Manocha, S. and Girolami, M. A. An empirical analysis of the probabilistic k-nearest neighbour classiﬁer. P attern Recognition Letters , 28(13):1818–1824, 2007. March, W . B., Ram, P ., and Gray, A. G. Fast euclidean minimum spanning tree: algorithm, analysis, and appli- cations. InProceedings of SIG KDD: Knowledge discov- ery and data mining , pp. 603–612, 2010. McNames, J. A fast nearest-neighbor algorithm based on a principal axis search tree. IEEE T ransactions on pattern analysis and machine intelligence , 23(9):964–976, 2001. Omohundro, S. M. Five balltree construction algorithms . International Computer Science Institute Berkeley, 1989. Pelleg, D. and Moore, A. Accelerating exact k-means algo- rithms with geometric reasoning. In Proceedings of the SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, pp. 277–281, 1999. Ram, P . and Gray, A. G. Maximum inner-product search us- ing cone trees. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 931–939, 2012. Ram, P ., Lee, D., March, W ., and Gray, A. Linear-time algorithms for pairwise statistical problems. Advances in Neural Information Processing Systems , 22:1527–1535, 2009. Ram, P ., Lee, D., and Gray, A. G. Nearest-neighbor search on a time budget via max-margin trees. In Proceedings of the 2012 SIAM International Conference on Data Min- ing, pp. 1011–1022. SIAM, 2012. Rudin, W . Functional Analysis . International Series in Pure & Applied Mathematics. McGraw Hill Higher Ed- ucation, Maidenhead, England, 2 edition, October 1990. Smith, P . and Kurlin, V . A practical algorithm for degree- k voronoi domains of three-dimensional periodic point sets. InLecture Notes in Computer Science (Proceedings of ISVC) , volume 13599, pp. 377–391, 2022. W ang, M., Xu, X., Y ue, Q., and W ang, Y . A comprehen- sive survey and experimental comparison of graph-based approximate nearest neighbor search.arXiv preprint arXiv:2101.12631 , 2021. Widdowson, D. and Kurlin, V . Pointwise distance distribu- tions of periodic sets. arXiv:2108.04798 , 2021. Widdowson, D. and Kurlin, V . Resolving the data ambigu- ity for periodic crystals. Advances in Neural Information Processing Systems (Proceedings of NeurIPS 2022) , 35, 2022. Widdowson, D. and Kurlin, V . Recognizing rigid patterns of unlabeled point clouds by complete and continuous isometry invariants with no false negatives and no false positives. InProceedings of CVPR (arxiv:2303.15385) , 2023. Widdowson, D., Mosca, M., Pulido, A., Kurlin, V ., and Cooper, A. A verage minimum distances of periodic point sets.MATCH Communications in Mathematical and in Computer Chemistry , 87:529–559, 2022. Y ianilos, P . N. Data structures and algorithms for nearest neighbor search in general metric spaces. In Symposium on Discrete Algorithms , volume 93, pp. 311–21, 1993. 11A new compressed cover tree for k-nearest neighbors The appendices below contain the full version of the paper wi th detailed proofs and pseudo codes A. The k-nearest neighbor search and overview of results In the modern formulation, k-nearest neighbors problem intends to discover all k≥1 nearest neighbors in a given reference set Rfor all points from another given query set Q. Both sets belong to a common ambient space X with a distance d satisfying all metric axioms. The simplest example of X is Rn with the Euclidean metric. A query set Qcan be a single point or a subset of a larger reference set R. The exact k-nearest neighbor problem asks for all true (non-approxima te) k-nearest neighbors in Rfor every query point q ∈Q. Another probabilistic version of the k-nearest neighbor search Har-Peled & Mendel (2006); Manoch a & Girolami (2007) aims to ﬁnd exact k-nearest neighbors with a given probability. The probabili stic k-nearest neighbor problem can be simpliﬁed to kinstances of 1-nearest-neighbors problem by splitting Rinto ksubsets R1,...,R k and searching for near- est neighbors in each subset. The approximate version Arya & Mount (1993); Krauthgamer & Lee (2004); Andoni et al. (2018); W ang et al. (2021) of the nearest neighbor search loo ks for an ǫ-approximate neighbor r∈Rof every query point q∈Qsuch that d(q,r) ≤(1 + ǫ)d(q,NN(q)) , where ǫ> 0 is ﬁxed and NN(q) is the exact ﬁrst nearest neighbor of q. Spacial data structures. It is well known that the time complexity of a brute-force app roach of ﬁnding all 1st nearest neighbors of points from Qwithin Ris proportional to the product |Q|·| R|of the sizes of Q,R. Already in the 1970s real data was big enough to motivate faster algorithms and so phisticated data structures. One of the ﬁrst spacial data structures, a quadtree Finkel & Bentley (1974), hierarchically splits a reference set R ⊂R2 by subdividing its bounding box (a root) into four smaller boxes (children), which are re cursively subdivided until ﬁnal boxes (leaf nodes) contain only a small number of reference points. A generalization of the quadtree to Rn exposes an exponential dependence of its computational complexity on the dimension n, because the n-dimensional box is subdivided into 2n smaller boxes. The ﬁrst attempt to overcome this curse of dimensionality wa s the kd-tree Bentley (1975) that subdivides a subset of Rat every step into two subsets instead of 2n subsets. Many more advanced algorithms utilizing spatial d ata structures have positively impacted various related research areas such as a minimum spanning tree Bentley & Friedman (1978), range search Pelleg & Moore (1999), k-means clustering Pelleg & Moore (1999), and ray tracing Fus sell & Subramanian (1988). The spacial data structures for ﬁnding nearest neighbors in the chronological order are k-means tree Fukunaga & Narendra (1975), Rtree Beckmann et al. (1990), ball tree Omohundro (1989), R∗ tree Beckmann et al. (1990), vantage-point tree Y ianilos (1993), TV trees Lin et al. (1994), X trees Berchtol d et al. (1996), principal axis tree McNames (2001), spill tree Liu et al. (2004), cover tree Beygelzimer et al. (2006a) , cosine tree Holmes et al. (2008), max-margin tree Ram et al. (2012), cone tree Ram & Gray (2012) and others. Deﬁnition 1.1 (diameter and aspect ratio) . F or any ﬁnite set R with a metric d, the diameter is diam(R) = max p∈R max q∈R d(p,q). The aspect ratio is ∆( R) = diam(R) dmin(R) , where dmin(R) is the shortest distance between points of R. Deﬁnition 1.2 (k-nearest neighbor set NNk). F or any point q ∈Q, let d1 ≤···≤ d|R| be ordered distances from q to all points of a reference set Rwhose size (number of points) is denoted by |R|. F or any k ≥1, the k-nearest neighbor set NNk(q; R) consists of all u∈Rwith d(q,u) ≤dk. For Q= R= {0,1,2,3}, the point q = 1 has ordered distances d1 = 0 <d2 = 1 = d3 <d4 = 2 . The nearest neighbor sets are NN1(1; R) = {1}, NN2(1; R) = {0,1,2}= NN 3(1; R), NN4(1; R) = R. So 0 can be a 2nd neighbor of 1, then 2 becomes a 3rd neighbor of 1, or these neighbors of 0 can be found in a different order. Problem 1.3 (all k-nearest neighbors search) . Let Q,R be ﬁnite subsets of query and reference sets in a metric space (X,d). F or any ﬁxed k ≥1, design an algorithm to exactly ﬁnd k distinct points from NNk(q; R) for all q ∈Qso that the parametrized worst-case time complexity is near-linea r in time max{|Q|,|R|}, where hidden constants may depend on structures of Q,R but not on their sizes |Q|,|R|. In a metric space, let ¯B(p,t) be the closed ball with a center p and a radius t ≥0. The notation |¯B(p,t)|denotes the number (if ﬁnite) of points in the closed ball. Deﬁnition 1.4 recalls the expansion constant c from Beygelzimer et al. (2006a) and introduces the new minimized expansion constan t cm, which is a discrete analog of the doubling dimension Cole & Gottlieb (2006). 12A new compressed cover tree for k-nearest neighbors Deﬁnition 1.4 (expansion constants c and cm). A subset R of a metric space (X,d) is called locally ﬁnite if the set ¯B(p,t) ∩Ris ﬁnite for all p∈X and t∈R+. Let Rbe a locally ﬁnite set in a metric space X. The expansion constant c(R) is the smallest c(R) ≥2 such that |¯B(p,2t)|≤ c(R) ·| ¯B(p,t)|for any point p ∈Rand t≥0, see Beygelzimer et al. (2006a). Introduce the new minimized expansion constant cm(R) = lim ξ→0+ inf R⊆A⊆X sup p∈A,t>ξ |¯B(p,2t) ∩A| |¯B(p,t) ∩A|, where A is a locally ﬁnite set which covers R. Lemma 1.5. F or any ﬁnite sets R⊆U in a metric space, we have that cm(R) ≤cm(U) and cm(R) ≤c(R). Proof. Let us ﬁrst prove that cm(R) ≤cm(U). Let ǫ> 0 be arbitrary real number. By deﬁnition of cm(U) there exists set ξ >0 and set Asatisfying U ⊆Afor which sup p∈A,t>ξ ||¯B(p,2t) ∩A| |¯B(p,t) ∩A| −cm(U)|≤ ǫ (2) Since R ⊆U we have R ⊆Atherefore we can choose the same ξ and set U which satisfy inequality (2). Therefore it follows cm(R) ≤cm(U) + ǫ. Since ǫwas chosen arbitrarily it follows that cm(R) ≤cm(U). T o prove that cm(R) ≤c(R), note that sup p∈A,t>ξ ||¯B(p,2t) ∩A| |¯B(p,t) ∩A| ≤ sup p∈A,t>0 ||¯B(p,2t) ∩A| |¯B(p,t) ∩A|. Then by choosing ξ= dmin(R) 4 and A= Rwe have: cm(R) ≤ sup p∈R,t>0 ||¯B(p,2t) ∩R| |¯B(p,t) ∩R| −cm(U)|= c(R) Note that both c(R),cm(R) are always deﬁned when Ris ﬁnite. W e will show that a single outlier can make the expan sion constant c(R) as large as O(|R|). The set R = {1,2,...,n, 2n+ 1 }of |R|= n+ 1 points has c(R) = n+ 1 because ¯B(2n+ 1; n) = {2n+ 1 }is a single point, while ¯B(2n+ 1; 2 n) = Ris the full set of n+ 1 points. On the other hand the same set Rcan be extended to a larger uniform set A = {1,2,..., 2n−1,2n}whose expansion constant c(A) = 2 , therefore the minimized constant of the original set Rbecomes much smaller: cm(R) ≤c(A) = 2 <c(R) = n+ 1. The constant c from Beygelzimer et al. (2006a) equals to 2dimKR from Krauthgamer & Lee (2004, Section 2.1). In Krauthgamer & Lee (2004, Section 1.1) the doubling dimensio n 2dim is deﬁned as a minimum value ρsuch that any set X can be covered by 2ρ sets whose diameters are half of the diameter of X. The past work Krauthgamer & Lee (2004) proves that 2dim ≤2n for any subset of Rn. Theorem C.15 will prove that cm(R) ≤2n for any a ﬁnite subset R⊂Rn, so cm(R) mimics 2dim. Navigating nets . In 2004, Krauthgamer & Lee (2004, Theorem 2.7) claimed that a navigating net can be constructed in time O ( 2O(dimKR (R)|R|(log |R|) log(log |R|) ) and all k-nearest neighbors of a query point q can be found in time O(2O(dimKR (R∪{q})(k+ log |R|), where dim KR (R∪{q}) is the expansion constant deﬁned above. All proofs and pseud o- codes were omitted. The authors didn’t reply to our request f or details. Modiﬁed navigating nets Cole & Gottlieb (2006) were used in 2006 to claim the time O(log(n) + (1 /ǫ)O(1)) for the (1 + ǫ)-approximate neighbors. All proofs and pseudo-codes were l eft out, also for the construction of the modiﬁed navigating net for the claimed time O(|R|·log(|R|)). Cover trees . In 2006, Beygelzimer et al. (2006a) introduced a cover tree inspired by the navigating nets Krauthgamer & Lee (2004). This cover tree was designed to pro ve a worst-case bound for the nearest neighbor search in terms of the size |R|of a reference set R and the expansion constant c(R) of Deﬁnition 1.4. Assume that a cover tree is already constructed on set R. Then Beygelzimer et al. (2006a, Theorem 5) claims that near est neighbor of any query point q ∈Qcould be found in time O(c(R)12 ·log |R|). In 2015, Curtin (2015, Section 5.3) pointed out that the 13A new compressed cover tree for k-nearest neighbors C−∞ 1 2 3 4 5 C−1 1 2 3 4 5 C0 1 3 5 C1 1 5 C2 1 C∞ 1 Level 2 Level 1 Level 0 Level -1 1 5 3 2 4 Figure 3. Left: an implicit cover tree from Beygelzimer et al. (2006a, Secti on 2) at ICML 2006 for a ﬁnite set of reference points R = {1,2,3,4,5} with the Euclidean distance d(x, y) = |x − y|. Right: a new compressed cover tree in Deﬁnition 2.1 corrects the past worst-case complexity for k-nearest neighbors search in R. proof of Beygelzimer et al. (2006a, Theorem 5) contains a cru cial gap, now have been conﬁrmed by a speciﬁc dataset in Elkin & Kurlin (2022a, Counterexample 5.2). The time comp lexity result of the cover tree construction algorithm Beygelzimer et al. (2006a, Theorem 6) had a similar issue, th e gap of which is exposed rigorously in Elkin & Kurlin (2022a, Counterexample 4.2). Further studies in cover trees. A noteworthy paper on cover trees Kollar (2006) introduced a new probabilistic algo- rithm for the nearest neighbor search, as well as corrected t he pseudo-code of the cover tree construction algorithm of Beygelzimer et al. (2006a, Algorithm 2). Later in 2015, a new , more efﬁcient implementation of cover tree was introduced in Izbicki & Shelton (2015). However, no new time-complexit y results were proven. A study Jahanseir & Sheehy (2016) explored connections between modiﬁed navigating nets Cole & Gottlieb (2006) and cover trees Beygelzimer et al. (2006a) . Multiple papers Beygelzimer et al. (2006b); Ram et al. (2009 ); Curtin et al. (2015) studied possibility of solving k-nearest neighbor problem ( Problem 1.3 ) by using cover tree on both, t he query set and the reference set, for further details see Elkin & Kurlin (2022a, Section 6). The main contributions are the following. • Deﬁnition 2.1 introduces a compressed cover tree. • Theorem 3.7 and Corollary 3.11 estimate the time to build a compressed cover tree. • Theorem 4.9 and Corollary 4.7 estimate the time to ﬁnd all k-nearest neighbors as in Problem 1.3. • Theorem G.6 estimates the time complexity of approximate k-nearest neighbor search. This work corrects the past gaps of the single-tree approach via an original cover tree Beygelzimer et al. (2006a) by usin g a new compressed cover tree T(R) from Deﬁnition 2.1, which can be constructed on any ﬁnite ref erence set Rwith a metric d. Theorem 3.10 will prove that a compressed cover tree T(R) can be built in time O(cm(R)8 ·c(R)2 ·log2(|R|) ·|R|). The past gap in the proof of the time complexity Beygelzimer e t al. (2006a, Theorem 1) for nearest neighbor search is tack- led by new Algorithm F .2, which add an essential block to the o riginal code in Beygelzimer et al. (2006a, Algorithm 1). The extra block eliminates the issue of having too many successi ve iterations when a query point qis disproportionately far away from the remaining candidate set Ri on some level i. Then Lemma 4.8 shows that the number of iterations of Algori thm F .2 is bounded by O(c(R)2 log2(|R|)). This new lemma replaces the old result Beygelzimer et al. (2 006a, Lemma 4.3), which had a similar bound for the number of explicit levels of a cover tree, for further information see Elkin & Kurlin (2022a, Deﬁnition 3.2) The old result cannot be used to estim ate the number of iterations of Beygelzimer et al. (2006a, Algorithm 1) due to Elkin & Kurlin (2022a, Counterexample 5. 2). 14A new compressed cover tree for k-nearest neighbors Assume that a compressed cover tree T(R) is already constructed on a reference set R. Our ﬁrst main Theorem 4.9 shows that k-nearest neighbors of a query node qcan be found in time of O ( c(R∪{q})2 ·log2(k) · ( (cm(R))10 ·log2(|R|) + c(R∪{q}) ·k ) ) . Recall that c(R) can potentially become as large as O(|R|) when Ris not uniformly distributed. Our second main Corol- lary 4.7 estimates the time complexity of the new k-nearest neighbor search by using only the minimized expans ion constant cm(R) of Deﬁnition 1.4 and the aspect ratio ∆( R) of Deﬁnition 1.1 as parameters. These parameters are less de pendent on the point distribution (or noise) in the sets R,Q. In many cases, ∆( R) is relatively small and cm(R) depends mostly on the dimension of the ambient space X. It is shown that k-nearest neighbors of qin a reference set Rcan be found in time O ( (cm(R))10 ·log2(k) ·log2(∆( R)) + |¯B(q,5dk(q,R))|·log2(k) ) , where dk(q,R) is the distance from qto its kth nearest neighbor. T ables 7-8 summarize past and new resul ts. T able 5. Results for building data structures with hidden classic ex pansion constant c(R) of Deﬁnition 1.4 or KR-type constant 2dimKR Krauthgamer & Lee (2004, Section 2.1) Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dimKR ) ·|R|log(|R|) log(log |R|) ) , Krauthgamer & Lee (2004, Theorem 2.6) O(2O(dim)|R|) Not available Cover tree Beygelzimer et al. (2006a) O(c(R)O(1) ·|R|· log |R|), Beygelzimer et al. (2006a, Theorem 6) O(|R|) Elkin & Kurlin (2022a, Counterexample 4.2) shows that the past proof is incorrect Compressed cover tree [dfn 2.1] O ( c(R)O(1) ·|R|·log(R) ) O(|R|) Lemma B.1 Corollary 3.11 T able 6. Results for exact k-nearest neighbors of one query point q ∈ X using hidden classic expansion constant c(R) of Deﬁnition 1.4 or KR-type constant 2dimKR Krauthgamer & Lee (2004, Section 2.1) and assuming that all d ata structures are already built. Note that the dimensionality factor 2dimKR is equivalent to c(R)O(1). Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dimKR )(log(|R|) + k) ) for k≥1 Krauthgamer & Lee (2004, Theorem 2.7) O(2O(dim)|R|) Not available Cover tree Beygelzimer et al. (2006a) O ( c(R)O(1) log |R| ) for k= 1 Beygelzimer et al. (2006a, Theorem 5) O(|R|) Elkin & Kurlin (2022a, Counterexample 5.2) shows that the past proof is incorrect Compressed cover tree, Deﬁnition 2.1 O ( c(R)O(1)·log(k)·(log(|R|)+k) ) O(|R|), Lemma B.1 Theorem 4.9 15A new compressed cover tree for k-nearest neighbors T able 7. Building data structures with hidden cm(R) or dimensionality constant 2dim Krauthgamer & Lee (2004, Section 1.1) Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dim) ·|R|·log(∆) ·log(log((∆)) ) O(2O(dim)|R|) Krauthgamer & Lee (2004, Theorem 2.5) Compressed cover tree [dfn 2.1] O ( cm(R)O(1) ·|R|log(∆( |R|)) ) O(|R|) Lemma B.1 Theorem 3.7 T able 8. Results for exact k-nearest neighbors of one point q using hidden cm(R) or dimensionality constant 2dim Krauthgamer & Lee (2004, Section 1.1) assuming that all structures are built. Data structure claimed time complexity space proofs Navigating nets Krauthgamer & Lee (2004) O ( 2O(dim) ·log(∆) + |¯B(q,O(d(q,R))| ) for k= 1 O(2O(dim)|R|) a proof outline in Krauthgamer & Lee (2004, Theorem 2.3) Compressed cover tree, Deﬁnition 2.1 O ( cm(R)O(1) ·log(k) ·(log(|∆ |) + |¯B(q,O(dk(q,R))|) ) O(|R|), Lemma B.1 Corollary 4.7 B. Compressed cover tree This section introduces in Deﬁnition 2.1 a new compressed co ver tree, which will be used to solve Problem 1.3. Other important results are Lemmas 2.2 and 2.4. Given a δ-sparse ﬁnite metric space R, Lemma 2.2 shows that the number of points of R in the closed ball ¯B(p,t) has the upper bound cm(S)µ, where µ depends on t δ . Lemma 2.4 will imply that if there are points p,q in a ﬁnite metric space Rsatisfying 2r < d(p,q) ≤3r for some r ∈R, then |¯B(q,4r)|≥ (1 + 1 c(R)2 )|¯B(q,r)|. Deﬁnition 2.1 (a compressed cover tree T(R)). Let Rbe a ﬁnite set in a metric space (X,d). A compressed cover tree T(R) has the vertex set Rwith a root r∈Rand a level function l: R→Z satisfying the conditions below . (2.1a) Root condition : the level of the root node rsatisﬁes l(r) ≥1 + max p∈R\\{r} l(p). (2.1b) Cover condition : for every node q∈R\\{r}, we select a unique parent pand a level l(q) such that d(q,p) ≤2l(q)+1 and l(q) <l(p); this parent node phas a single link to its child node q. (2.1c) Separation condition : for i∈Z, the cover set Ci = {p∈R|l(p) ≥i}has dmin(Ci) = min p∈Ci min q∈Ci\\{p} d(p,q) >2i. Since there is a 1-1 map between Rand all nodes of T(R), the same notation pcan refer to a point in the set Ror to a node of the tree T(R). Set lmax = 1 + max p∈R\\{r} l(p) and lmin = min p∈R l(p). F or any node p∈T (R), Children(p) denotes the set of all children of p, including pitself. F or any node p∈T (R), deﬁne the node-to-root path as a unique sequence of nodes w0,...,w m such that w0 = p, wm is the root and wj+1 is the parent of wj for j = 0 ,...,m −1. A node q∈T (R) is a descendant of a node pif pis in the node-to-root path of q. A node pis an ancestor of qif qis in the node-to-root path of p. Let Descendants(p) be the set of all descendants of p, including itself p. Lemma B.1 (Linear space of T(R)). Let (R,d) be a ﬁnite metric space. Then any cover tree T(R) from Deﬁnition 2.1 takes O(|R|) space. Proof. Since T(R) is a tree , both its vertex set and its edge set contain at most |R|nodes. Therefore T(R) takes at most O(|R|) space. 16A new compressed cover tree for k-nearest neighbors Level i Level i−1 Level -1 2i 1 0 Level i Level i−1 Level -1 2i 0 1 Level i Level i−1 Level -1 0 2i 1 Figure 4. Compressed cover trees T (R) from Deﬁnition 2.1 for R = {0,1,2i}. Level 2 Level 1 Level 0 Level −1 1 3 5 7 9 11 13 15 2 6 10 14 4 12 8 Figure 5. Compressed cover tree T (R) on the set R in Example B.2 with root 16. Example B.2 (T(R) in Fig. 5) . Let (R,d = |x−y|) be the real line with euclidean metric. Let R = {1,2,3,..., 15} be its ﬁnite subset. Fig. 5 shows a compressed cover tree on th e set R with the root r = 8 . The cover sets of T(R) are C−1 = {1,2,3,..., 15}, C0 = {2,4,6,8,10,12,14}, C1 = {4,8,12}and C2 = {8}. W e check the conditions of Deﬁnition 2.1. • Root condition (2.1a): since maxp∈R\\{8} d(p,8) = 7 and ⌈log2(7)⌉−1 = 2 , the root can have the level l(8) = 2 . • Covering condition (2.1b) : for any i ∈ −1,0,1,2, let pi be arbitrary point having l(pi) = i. Then we have d(p−1,p0) = 1 ≤20, d(p0,p1) = 2 ≤21 and d(p1,p2) = 4 ≤22. • Separation condition (2.1c) : dmin(C−1) = 1 > 1 2 = 2 −1, dmin(C0) = 2 >1 = 2 0,dmin(C1) = 4 >2 = 2 1. ■ A cover tree was deﬁned in Beygelzimer et al. (2006a, Section 2) as a tree version of a navigating net from Krauthgamer & Lee (2004, Section 2.1). For any index i ∈ Z ∪{±∞}, the level i set of this cover tree coincides with the cover set Ci above, which can have nodes at different levels in Deﬁnition 2.1. Any point p ∈Ci has a single parent in the set Ci+1, which satisﬁed conditions (2.1b,c). Beygelzimer et al. (2 006a, Section 2) referred to this original tree as an implicit representation of a cover tree. Such a tre e in Figure 6 (left) contains inﬁnitely many repetitions of e very point p∈Rin long branches and will be called an implicit cover tree . Since an implicit cover tree is formally inﬁnite, for practi cal implementations, the authors of Beygelzimer et al. (200 6a) had to use another version that they named an explicit repres entation of a cover tree. W e call this version an explicit cover tree. Here is the full deﬁning quote at the end of Beygelzimer et al . (2006a, Section 2): ”The explicit representation of the tree coalesces all nodes in which the only child is a self-chi ld”. In an explicit cover tree, if a subpath of every node-to- root path consists of all identical nodes without other children , all these identical nodes collapse to a single node, see Fig ure 6 (middle). Since an explicit cover tree still contains repeated points , Deﬁnition 2.1 is well-motivated by the aim to include every point only once, which saves memory and simpliﬁes all subsequent a lgorithms, see Fig. 6 (right). Example B.3 (a short train line tree) . Let Gbe the unoriented metric graph consisting of two vertices r,q connected by three different edges e,h,g of lengths |e|= 2 6 , |h|= 2 3 , |g|= 1 . Let p4 be the middle point of the edge e. Let p3 be the middle point of the subedge (p4,q). Let p2 be the middle point of the edge h. Let p1 be the middle point of the subedge (p2,q). Let R = {p1,p2,p3,p4,r}. W e construct a compressed cover tree T(R) by choosing the level l(pi) = iand by setting the root rto be the parent of both p2 and p4, p4 to be the parent of p3, and p2 to be the parent of p1. Then T(R) satisﬁes all the conditions of Deﬁnition 2.1, see a comparis on of the three cover trees in Fig. 6. ■ 17A new compressed cover tree for k-nearest neighbors l= ∞ l= 5 l= 4 l= 3 l= 2 l= 1 l= −∞ r r r r r r r p4 p4 p4 p4 p4 p3 p3 p3 p3 p2 p2 p2 p1 p1 r r r p4 p4 p3 p2 p2p1 r p4 p2 p1 p3 Figure 6. A comparison of past cover trees and a new tree in Example B.3. Left: an implicit cover tree contains inﬁnite repetitions. Middle: an explicit cover tree. Right: a compressed cover tree from Deﬁnition 2.1 includes each poi nt once. p R\\{p} Figure 7. Example B.4 describes a set R with a big expansion constant c(R). Let R \\ { p} be a ﬁnite subset of a unit square lattice in R2, but a point p is located far away from R \\ { p} at a distance larger than diam(R \\ { p}). Deﬁnition 1.4 implies that c(R) = |R|. Even a single outlier point can make the expansion constant b ig. Consider set R= {1,2,...,n −1,2n}for some n∈Z+. Since |¯B(2n,n)|= 1 and |¯B(2n,n)|= |R|, we have c(R) = |R|. Example B.4 shows that expansion constant of a set R can be as big as |R|. Example B.4 (one outlier can make the expansion constant big) . Let Rbe a ﬁnite metric space and p∈Rsatisfy d(p,R \\ {t}) >diam(R\\{p}). Since ¯B(p,2d(p,R \\{t}) = R, ¯B(p,d(p,R \\{t}) = {p}, we get c(R) = N, see Fig. 7. ■ Example B.5 shows that the minimized expansion can be signiﬁ cantly smaller than the original expansion constant. Example B.5 (minimized expansion constants) . Let (R,d) be the Euclidean line. For an integer n >10, consider the ﬁnite sets R = {2i |i ∈[1,n]}and let Q = {i |i ∈[1,2n]}. If 0 < ǫ <10−9, then ¯B(2n,2n−1 −ǫ) = {2n}and ¯B(2n,2(2n−1 −ǫ)) = R, so c(R) = n. For any q ∈Qand any t ∈R, we have that ¯B(q,t) = Z ∩[q−t,q + t] and ¯B(q,2t) = Z ∩[q−2t,q + 2t], so c(Q) ≤4. Then cm(R) ≤cm(Q) ≤c(Q) ≤4 by Lemma 1.5. ■ Lemma B.6 provides an upper bound for a distance between a nod e and its descendants. Lemma B.6 (a distance bound on descendants) . Let R be a ﬁnite subset of an ambient space X with a metric d. In a compressed cover tree T(R), let q be any descendant of a node p. Let the node-to-root path S of q contain a node u satisfying u∈Children(p) \\{p}. Then d(p,q) ≤2l(u)+2 ≤2l(p)+1. ■ Proof. Let (w0,...,w m) be a subpath of the node-to-root path for w0 = q , wm−1 = u, wm = p. Then d(wi,wi+1) ≤ 18A new compressed cover tree for k-nearest neighbors p δ/ 2 tS Figure 8. This volume argument proves Lemma 2.2. By using an expansion constant, we can ﬁnd an upper bound for the number of smaller balls of radius δ 2 that can ﬁt inside a larger ¯B(p, t). 2l(wi)+1 for any i. The ﬁrst required inequality follows from the triangle ine quality below: d(p,q) ≤ m−1∑ j=0 d(wj ,wj+1) ≤ m−1∑ j=0 2l(wj )+1 ≤ l(u)+1∑ t=lmin 2t ≤2l(u)+2 Finally, l(u) ≤l(p) −1 implies that d(p,q) ≤2l(p)+1. Lemma 2.2 uses the idea of Curtin et al. (2015, Lemma 1) to show that if Sis a δ-sparse subset of a metric space X, then S has at most (cm(S))µ points in the ball ¯B(p,r), where cm(S) is the minimized expansion constant of S, while µdepends on δ,r. Lemma 2.2 (packing). Let S be a ﬁnite δ-sparse set in a metric space (X,d), so d(a,b) > δfor all a,b ∈S. Then, for any point p∈X and any radius t>δ , we have |¯B(p,t) ∩S|≤ (cm(S))µ, where µ= ⌈log2(4t δ + 1)⌉. Proof. Assume that d(p,q) > tfor any point q ∈S. Then ¯B(p,t) ∩S = ∅and the lemma holds trivially. Otherwise ¯B(p,t)∩Sis non-empty. By Deﬁnition 1.4 of a minimized expansion cons tant, for any small enough ǫ> 0, we can always ﬁnd ξ≤2t+ δ 2 2µ and a set Asuch that S ⊆A⊆X for which: |B(q,2s) ∩A|≤ (cm(S) + ǫ) ·|B(q,s) ∩A| (3) for any q ∈Aand s > ξ. Note that for any u ∈ ¯B(p,t) ∩S we have ¯B(u,δ 2 ) ⊆ ¯B(p,t + δ 2 ). Therefore, for any point q∈ ¯B(p,t) ∩S, we get ⋃ u∈ ¯B(p,t)∩S ¯B(u,δ 2 ) ⊆ ¯B(p,t + δ 2 ) ⊆ ¯B(q,2t+ δ 2 ) Since all the points of Swere separated by δ, we have |¯B(p,t) ∩S|· min u∈ ¯B(p,t)∩S |¯B(u,δ 2 ) ∩A|≤ ∑ u∈ ¯B(p,t)∩S |¯B(u,δ 2) ∩A|≤| ¯B(q,2t+ δ 2) ∩A| In particular, by setting q= argmin a∈S∩ ¯B(p,t)|¯B(a,δ 2 )|, we get: |¯B(p,t) ∩S|·| ¯B(q, δ 2 ) ∩A|≤| ¯B(q,2t+ δ 2 ) ∩A| (4) Inequality (3) applied µtimes for the radii si = 2t+ δ 2 2i , i= 1 ,...,µ , implies that: |¯B(q,2t+ δ 2) ∩A|≤ (cm(S) + ǫ)µ|¯B(q,2t+ δ 2 2µ ) ∩A|≤ (cm(S) + ǫ)µ|¯B(q, δ 2 ) ∩A|. (5) 19A new compressed cover tree for k-nearest neighbors By combining inequalities (4) and (5), we get |¯B(p,t) ∩S|≤ |¯B(q,2t+ δ 2 ) ∩A| |¯B(q, δ 2 ) ∩A| ≤(cm(S) + ǫ)µ. The required inequality is obtained by letting ǫ→0. Krauthgamer & Lee (2004, Section 1.1) deﬁned dim( X) of a space (X,d) as the minimum number msuch that every set U ⊆X can be covered by 2m sets whose diameter is a half of the diameter of U. If U is ﬁnite, an easy application of Lemma 2.2 for δ = r 2 shows that dim (X) ≤supA⊆X (cm(A))4 ≤supA⊆X infA⊆B⊆X (c(B))4,where Aand Bare ﬁnite subsets of X. Let T(R) be an implicit cover tree of Beygelzimer et al. (2006a) on a ﬁn ite set R. Beygelzimer et al. (2006a, Lemma 4.1) showed that the number of children of any node p ∈ T(R) has the upper bound (c(R))4. Lemma 2.3 generalizes Beygelzimer et al. (2006a, Lemma 4.1) for a compressed cover tree. Lemma 2.3 (width bound) . Let Rbe a ﬁnite subset of a metric space (X,d). F or any compressed cover tree T(R), any node pand any level i≤l(p) we have {q∈Children(p) |l(q) = i}∪{p}≤ (cm(R))4, where cm(R) is the minimized expansion constant of R. Proof. By the covering condition of T(R), any child q of plocated on the level ihas d(q,p) ≤2i+1. Then the number of children of the node pat level iat most |¯B(p,2i+1)|. The separation condition in Deﬁnition 2.1 implies that the set Ci is a 2i-sparse subset of X. W e apply Lemma 2.2 for t = 2 i+1 and δ = 2 i. Since 4 ·t δ + 1 ≤4 ·2 + 1 ≤24, we get |¯B(q,2i+1) ∩Ci|≤ (cm(Ci))4. Lemma 1.5 implies that (cm(Ci))4 ≤(cm(R))4, so the upper bound is proved. Lemma 2.4 (growth bound) . Let (A,d) be a ﬁnite metric space, let q ∈Abe an arbitrary point and let r ∈R be a real number . Let c(A) be the expansion constant from Deﬁnition 1.4. If there exist s a point p∈Asuch that 2r<d (p,q) ≤3r, then |¯B(q,4r)|≥ (1 + 1 c(A)2 ) ·|¯B(q,r)|. Proof. Since ¯B(q,r) ⊂ ¯B(p,3r+ r), we have |¯B(q,r)|≤| ¯B(q,4r)|≤ c(A)2 ·|¯B(p,r)|. And since ¯B(p,r) and ¯B(q,r) are disjoint and are subsets of ¯B(q,4r), we have |¯B(q,4r)|≥| ¯B(q,r)|+ |¯B(p,r)|≥| ¯B(q,r)|+ |¯B(q,r)| c(A)2 ≥(1 + 1 c(A)2 ) ·|¯B(q,r)|, which proves the claim. Lemma 2.5 (extended growth bound) . Let (A,d) be a ﬁnite metric space, let q ∈Abe an arbitrary point. Let p1,...,p n be a sequence of distinct points in R, in such a way that for all i∈{2,...,n }we have 4 ·d(pi,q) ≤d(pi+1,q). Then |¯B(q,4 3 ·d(q,pn))|≥ (1 + 1 c(A)2 )n ·|¯B(q, 1 3 ·d(q,p1))|. Proof. Let us prove this by induction. In basecase n= 1 deﬁne r= d(q,pm) 3 . Now by Lemma 2.4 we have |¯B(q,4 3d(q,p1))|= |¯B(q,4r)|≥ (1 + 1 c(A)2 ) ·|¯B(q,r)|= |¯B(q,1 3d(q,p1))|. Assume now that the claim holds for some i = mand let p1,...,p m+1 be a sequence satisfying the condition of Lemma 2.5. By induction assumption we have |¯B(q, 4 3 d(q,pm))|≥ (1 + 1 c(A)2 )m ·|¯B(q,1 3 d(q,p1))|. Let us pick r = d(q,pm+1) 3 . 20A new compressed cover tree for k-nearest neighbors Then we have: |¯B(q,4 3 ·d(q,pm+1))|≥ (1 + 1 c(A)2 ) ·|¯B(q,1 3 ·d(q,pm+1))| ≥(1 + 1 c(A)2 ) ·|¯B(q,4 3 ·d(q,pm))| ≥(1 + 1 c(A)2 ) ·(1 + 1 c(A)2 )m ·|¯B(q, 1 3 ·d(q,p1))| ≥(1 + 1 c(A)2 )m+1 ·|¯B(q,1 3 ·d(q,p1))| which proves the claim. Lemma B.7. F or every x∈R satisfying x≥2, the following inequality holds: x2 ≥ 1 log2(1 + 1 x2 ). Proof. Let ln be natural logarithm. Note ﬁrst that for any u> 0 we have: u u+ 1 = ∫ u 0 dt u+ 1 ≤ ∫ u 0 dt t+ 1 = ln( u+ 1). By setting u= 1 x2 >0 we get: log2(1 + 1 x2 ) = ln( 1 x2 ) ln(2) ≥ 1 ln(2) · 1 x2 1 x2 + 1 = 1 ln(2) · 1 x2 + 1 . Let us now show that for x≥2 we have: 1 ln(2) · 1 x2+1 ≥ 1 x2 . Note ﬁrst that 4 ≥ ln(2) 1−ln(2) . Since x≥2 we have x2 ≥ ln(2) 1−ln(2) . Therefore x2 −ln(2) ·x2 ≥ln(2) and x2 ≥ln(2) ·(1 + x2). It follows that 1 ln(2) 1 1+x2 ≥ 1 x2 , which proves the claim. Deﬁnition 2.6 (the height of a compressed cover tree) . F or a compressed cover tree T(R) on a ﬁnite set R, the height set is H(T(R)) = {i|Ci−1 ̸= Ci}∪{lmax,lmin}. The size |H(T(R))|of this set is called the height of T(R). The new concept of the height |H(T)|will justify a near-linear parameterized worst-case compl exity in Theorem 4.9. By condition (2.1b), the height |H(T(R))|counts the number of levels iwhose cover sets Ci include new points that were absent on higher levels. Then |H(T)|≤| R|as any point can be alone at its own level. Lemma B.8. Any ﬁnite set Rhas the bound |H(T(R))|≤ 1 + log 2(∆( R)). ■ Proof. W e have |H(T(R))|≤ lmax −lmin + 1 by Deﬁnition 2.6. W e estimate lmax −lmin as follows. Let p∈Rbe a point such that diam(R) = max q∈R d(p,q). Then Ris covered by the closed ball ¯B(p; diam(R)). Hence the cover set Ci at the level i = log 2(diam(R)) consists of a single point p. The separation condition in Deﬁnition 2.1 implies that lmax ≤log2(dmax(R)). Since any distinct points p,q ∈Rhave d(p,q) ≥dmin(R), the covering condition implies that no new points can enter the cover set Ci at the level i = [log 2(dmin(R))], so lmin ≥log2(dmin(R)). Then |H(T(R))|≤ 1 + lmax −lmin ≤1 + log 2(diam(R) dmin(R) ). If the aspect ratio ∆( R) = O(Poly(|R|)) polynomially depends on the size |R|, then |H(T(R))|≤ O(log(|R|)). Lemma 2.4 corresponds Beygelzimer et al. (2006a, Lemma 4.2) with s lightly modiﬁed notation. 21A new compressed cover tree for k-nearest neighbors C. The minimized expansion constant in a normed vector space on R In this section, main Theorem C.15 will show that, for any ﬁni te subset R of a normed vector space (Rn,∥·∥ ), the minimized expansion constant from Deﬁnition 1.4 has the upp er bound 2n, so cm(R) = inf 0<ξ inf R⊆A⊆Rn sup p∈A,t>ξ |¯B(p,2t) ∩A| |¯B(p,t) ∩A| ≤2n. The proof of Theorem C.15 is based on the volume argument. W e b rieﬂy explain the idea before giving the proof later. For this purpose, we recall the deﬁnition of the Lebesgue measur e in Deﬁnition C.2. In Deﬁnition C.5 we deﬁne concepts of grid G(δ) = δ·Zn and cubic regions ¯V(p,δ) = p+ [−δ 2 ,δ 2 ]n. For every δ >0 we deﬁne grid extension U(δ) of Ras set U(δ) = ( G(δ) \\f(R)) ∪R, where f : R→G(δ) is used to replace points of R with their nearest neighbors in grid G(δ). Note that ξ in the deﬁnition of cm(R) acts as a low bound for radius t > ξ. Let γ >0 be a constant, that depends on dimension nand norm ∥·∥. In Lemma C.13 it is shown that if δsatisﬁes 0 <δ < ξ γ , then for any p∈U(δ) and t>ξ we can bound |¯B(p,t) ∩U(δ)|as follows: µ( ¯B(p,t −δ·γ)) δn ≤| ¯B(p,t) ∩U(δ)|≤ µ( ¯B(p,t + δ·γ)) δn . Therefore |¯B(p,2t) ∩U(δ)| |¯B(p,t) ∩U(δ)| ≤µ( ¯B(p,2t+ δ·γ)) µ( ¯B(p,t −δ·γ)) . Now since this inequality is satisﬁed for any δ >0, we can choose arbitrary dense grid extension U(δ). It will be shown that when δ→0 , then µ( ¯B(p,2t+ δ·γ)) µ( ¯B(p,t −δ·γ)) →2n. Then we can conclude that cm(R) ≤2n. Deﬁnition C.1 (Normed vector space (Rn,∥·∥ ) on real numbers R Rudin (1990)) . Consider Rn as a vector space. A norm is a function ∥·∥ : Rn →R satisfying the properties below . 1. Non-negativity : ∥x∥≥ 0. 2. The norm is positive on nonzero vectors, so ∥x∥= 0 implies that x= 0 . 3. F or every vector x∈Rn, and every scalar a∈R: ∥a·x∥= |a|·∥x∥. 4. The triangle inequality holds for every x∈Rn and y∈Rn, ∥x+ y∥≤∥ x∥+ ∥y∥. A norm induces a metric by the formula d(x,y) = ∥x−y∥. F or every i ∈{1,...,n }let ei be a unit vector of Rn i.e. ei(i) = 1 and ei(j) = 0 for all j ∈{1,...,n }\\{i}. Deﬁne ρ= max i∈{1,...,n} ∥ei∥. ■ Deﬁnition C.2 (Lebesgue outer measure, Jones (2000, Section 2.A)) . Let Rn be an n-dimensional space. Deﬁne n- dimensional interval as I = {x∈Rn |ai ≤xi ≤bi,i = 1 ,...,n }= [ a1,b1] ×...×[an,bn], with sides parallel to the coordinate axes. Deﬁne Lebesgue o uter measure µ∗ : {A|A⊆Rn}→ [0,∞)∪{∞}of interval I as µ∗(I) = ( b1 −a1) ·...·(bn −an). The Lebesgue µmeasure of a set A⊆Rn is deﬁned as: µ∗(A) = inf A { ∞∑ i=0 µ∗(Ii) |A⊆∪∞ i=0Ii}, where the inﬁnium is taken over all covering of A by countably many intervals Ii,i = 1 ,2.... If set E ⊆Rn satisﬁes µ∗(A) = µ∗(A∩E) + µ∗(A\\E) for all A⊆Rn, then Eis lebesgue-measurable and we set µ(E) = µ∗(E). ■ 22A new compressed cover tree for k-nearest neighbors It should be noted that all open sets and closed sets , as well a s compact sets are Lebesgue-measurable. Lemma C.3 (Basic properties of Lebesgue measure, Jones (2000, Sectio n 2.A)) . A Lebesgue outer measure µ∗ of Deﬁni- tion C.2 satisﬁes the following conditions: 1. µ∗(∅) = 0 , 2. µ∗(A) ≤µ∗(B) whenever A⊆B ⊆Rn and 3. µ∗(∪∞ i=1µ∗(Ai)) ≤∑ ∞ i=1 µ∗(Ai). Lemma C.4 (Lebesgue measure scale property, Jones (2000, Section 3.B )). Let µ be Lebesgue measure on a normed vector space (Rn,∥·∥). Then, for any p∈Rn and t∈R+, we have: µ( ¯B(p,t)) = tn ·µ( ¯B(p,1)). Deﬁnition C.5 (Grid and Cubic region) . Let Rn be a normed vector space and let δ ∈R. Deﬁne δ-grid on Rn as the set G(δ) = {t·δ |t ∈Zn}. F or any p ∈Rn deﬁne its open cubic region V(p,δ) ⊆Rn as the set {p+ u |u ∈(−δ 2 ,δ 2 )n} and closed cubic region ¯V(p,δ) ⊆Rn as {p+ u|u∈[−δ 2 ,δ 2 ]n}. Note that the union ∪p∈G(δ)V(p,δ) covers whole set Rn. Lemma C.6 (Cubic regions are separate) . In conditions of Deﬁnition C.5 let p,q ∈G(δ) be distinct points. Then their cubic regions are separate i.e. V(p,δ) ∩V(q,δ) = ∅. Proof. Assume contrary that there exists a ∈V(p,δ) ∩V(q,δ), then |a(i) −p(i)|< δ 2 and |a(i) −q(i)|< δ 2 for all i ∈ {1,...,n }. Since p ̸= q, there exists index j, such that p(j) ̸= q(j). By deﬁnition of grid G(δ) it follows that |p(j) −q(j)|≥ δ. However, by triangle inequality we have |p(j) −q(j)|≤| p(j) −a(j)|+ |q(j) −a(j)|< δ 2 + δ 2 = δ, which is a contradiction. Therefore V(p,δ) ∩V(q,δ) = ∅. Lemma C.7. Let Rn be a normed vector space of Deﬁnition C.1. Let δ ∈R and let G(δ) be a grid of Deﬁnition C.5. Let p∈G(δ) and let q∈V(p,δ), then d(p,q) ≤n·δ·ρ 2 Proof. Let γ ∈R be such that q = p+ γ. By condition (3) of Deﬁnition C.1 we have ∥γ(i)∥≤∥ ei∥· δ 2 ≤δ·ρ 2 for all i∈{1,...,n }. By the deﬁnition of norm and triangle inequality we have: d(p,q) = ∥p−q∥= ∥γ∥≤ n∑ i=1 ∥γ(i)∥≤ n·δ·ρ 2 . Any normed vector space (Rn,∥·∥) has the metric d(x,y) = ∥x−y∥. Lemma C.8 (Existence of covering grid ) . Let Rbe a ﬁnite subset of a normed vector space (Rn,∥·∥ ). Then for any δ ∈R having δ <dmin(R) n·ρ , then any map f : R →G(δ) which maps p ∈Rto one of its nearest neighbor in G(δ) is a well-deﬁned injection. Proof.Let f be an arbitrary map f : R→G(δ) mapping point p∈Rto one of its nearest neighbors. This map is clearly well-deﬁned. Let us now show that it is injective. Assume tha t x,y ∈Rare such that f(x) = f(y). Then by triangle inequality and Lemma C.7 we have: d(x,y) ≤d(x,p) + d(p,y) ≤n·δ·ρ<d min(R), it follows that x= y. Therefore map f is injective. 23A new compressed cover tree for k-nearest neighbors Lemma C.9. Let Rbe a ﬁnite subset of normed space (Rn,d), let ρbe as in Deﬁnition C.1 and let δ ∈R be such that 0 <δ < dmin(R) n·ρ . Let p∈Rbe arbitrary point and let t> n·δ·ρ 2 be a real number . Then there exists a set U(δ) satisfying R⊆U(δ) and |G(δ) ∩¯B(p,t −n·δ·ρ 2 )|≤| U(δ) ∩¯B(p,t)|≤| G(δ) ∩¯B(p,t + n·δ·ρ 2 )| Proof. Let f : R →G(δ) be an injection from Lemma C.8, which maps every q ∈Rto one of its nearest neighbors in G(δ). Deﬁne U(δ) = ( G(δ) \\f(R)) ∪R. Let us ﬁrst show that g: U(δ) ∩¯B(p,t) →G(δ) ∩¯B(p,t + n·δ·ρ 2 ), deﬁned by g(q) = f(q), if q ∈Rand g(q) = q, if q /∈R, is an injection. Let us show ﬁrst that the map gis well-deﬁned, if q /∈R, the claim is trivial. Let q /∈R, then by triangle inequality d(g(q),p) ≤d(q,p) + d(g(q),q) ≤t+ n·δ·ρ 2 . Assume now that g(a) = g(b) for some a,b ∈U(δ) ∩¯B(p,t). Let us ﬁrst show that either a,b both belong to Ror neither of a,b belong to R. Assume contrary that a ∈Rand b /∈R. Since b /∈Rwe have b ∈G(δ) \\f(R). On the other hand since h(a) = h(b) we have f(a) = b, therefore b∈f(R), which is a contradiction. If both, aand bbelong to Rwe have a= b, similarly if a,b /∈R we have a = b by injectivity of function f. Therefore we have now shown that g is well-deﬁned injection. It follows |U(δ) ∩¯B(p,t)|≤| G(δ) ∩¯B(p,t + n·δ·ρ 2 )|. Let us now show that map h: G(δ) ∩¯B(p,t −n·δ·ρ 2 ) →U(δ) ∩¯B(p,t), deﬁned by h(q) = f−1(q), if q∈f(R) and h(q) = q, if q /∈f(R) is well-deﬁned injection. Let us ﬁrst show that the map is well-deﬁned. Let q∈G(δ) ∩¯B(p,t −n·δ·ρ 2 ), if q /∈f(R) the claim is satisﬁed trivially. If q∈f(R), then by deﬁnition d(h(q),q) ≤n·δ·ρ 2 . By using triangle inequality we obtain: d(p,h(q)) ≤d(p,q) + d(q,h(q)) ≤t−n·δ·ρ 2 + n·δ·ρ 2 ≤t. Therefore h(q) ∈U(δ) ∩¯B(p,t). Let us now show that his an injection. Let a,b ∈G(δ) ∩¯B(p,t −n·δ 2 ) assume that h(a) = h(b), let us show that a= b. Let us ﬁrst show that either a,b ∈f(R) or neither of a,b belong to f(R). Assume contrary that a∈f(R) and b /∈f(R). Then h(a) = h(b) implies that f−1(a) = b. Since f−1(a) ∈R, we have b∈R. Since b∈G(δ), it follows that f(b) = b, which is a contradiction since b /∈f(R). Assume now that a,b ∈f(R), then the claim follows by noting that f−1 is injection. If a,b /∈f(R) , then claim follows by noting that h(a) = aand h(b) = b. Therefore map his injection. It follows that |G(δ) ∩¯B(p,t −n·δ·ρ 2 )|≤| U(δ) ∩¯B(p,t)|. Lemma C.10. Let Rbe a ﬁnite subset of normed vector space Rn and let δ ∈R. F or any p∈G(δ) recall that V(p,δ) is Minkowski sum p+ (−δ 2 ,δ 2 )n. Deﬁne ¯W(p,t,δ ) = ⋃ q∈ ¯B(p,t)∩G(δ) ¯V(q,δ). Then for any p∈Rand t> n·δ·ρ 2 we have: ¯B(p,t −n·δ·ρ 2 ) ⊆ ¯W(p,t,δ ) ⊆ ¯B(p,t + n·δ·ρ 2 ). Proof. Let u ∈ ¯B(p,t −n·δ·ρ 2 ) be an arbitrary point. Since {¯V(q,δ) |q ∈G(δ)}covers Rit follows that there exists a∈G(δ) such that u∈¯V(a,δ). By triangle inequality we obtain: d(a,p) ≤d(a,u) + d(u,p) ≤n·δ·ρ n + t−n·δ·ρ n ≤t. 24A new compressed cover tree for k-nearest neighbors It follows that ¯V(w,δ) ∈ ¯W(p,t), therefore p ∈ ¯W(p,t,δ ). W e have ¯B(p,t −n·δ·ρ 2 ) ⊆ ¯W(p,t,δ ).Let u ∈ ¯W(p,t,δ ), then there exists a∈G(δ) such that u∈¯V(a,δ) and ¯V(a,δ) ∈ ¯W(p,t). By triangle inequality we obtain: d(u,p) ≤d(u,a) + d(a,p) ≤n·δ·ρ n + t. It follows that u∈ ¯B(p,t + n·δ·ρ 2 ). Therefore ¯W(p,t,δ ) ⊆ ¯B(p,t + n·δ·ρ 2 ) which proves the claim. Lemma C.11 (Countable additivity, Jones (2000, Section 2.A)) . Assume that Ai ⊆Rn, i= 1 ,2,..., are pairwise disjoint i.e. Ai ∩Aj = ∅for all i̸= jLebesgue-measurable sets. Then µ( ∞⋃ i=0 Ai) = ∞∑ i=1 µ(Ai). Lemma C.12 (Lebesgue measure of ¯W(p,t,δ )). In notations of Lemma C.10 let µ be a Lebesgue measure on R from Deﬁnition C.2, then µ( ¯W(p,t,δ )) = δn ·|¯B(p,t) ∩G(δ)|. Proof. Deﬁne W(p,t,δ ) = ⋃ q∈ ¯B(p,t)∩G(δ) V(q,δ). Recall that for all p ∈ Rn and δ > 0 set ¯V(p,t) is a closed n−dimensional interval and V(p,t) is an open n-dimensional interval. Therefore we have µ( ¯V(p,t)) = µ(V(p,t)). Since ¯V(p,t) is a closed interval, it follows that µ( ¯V(p,t)) = δn. Since all the sets of W are separate we can use Lemma C.11 to obtain: µ(W(p,t,δ )) = ∑ A∈W (p,t) µ(A) = ∑ A∈ ¯W (p,t) µ(A) = δn ·|¯B(p,t) ∩G(δ)| By Lemma C.3 (2), since W(p,t,δ ) ⊆ ¯W(p,t,δ ) we obtain µ(∪¯W(p,t)) ≥δn ·|¯B(p,t) ∩G(δ)|. On the other hand, by Lemma C.3 (3) we obtain µ( ¯W(p,t,δ )) ≤ ∑ A∈ ¯W (p,t) µ(A) = δn ·|¯B(p,t) ∩G(δ)| Therefore we have shown that µ( ¯W(p,t,δ )) = δn ·|¯B(p,t) ∩G(δ)|. Lemma C.13 (Set U(δ) bounds). Let Rn be a normed vector space. Let R⊆Rn be its ﬁnite subset. Then any set U(δ) of Lemma C.9 satisﬁes the following inequalities: µ( ¯B(p,t −δ·n·ρ)) δn ≤| ¯B(p,t) ∩U(δ)|≤ µ( ¯B(p,t + δ·γ·n·ρ)) δn , for all p∈Rand t>n ·δ·ρ. Proof. Let p∈Rn be an arbitrary point and let t>n ·δ·ρbe an arbitrary real number. By Lemma C.9 it follows: |G(δ) ∩¯B(p,t + n·δ·ρ 2 )|≤| ¯B(p,t) ∩U(δ)|≤| G(δ) ∩¯B(p,t + n·δ·ρ 2 )|. Let ¯W(p,t + n·δ·ρ 2 ,δ) = ∪q{¯V(q,δ) |q∈ ¯B(p,t + n·δ·ρ 2 )}.By Lemma C.10 we have: ¯B(p,t −n·δ·ρ) ⊆ ¯W(p,t −n·δ·ρ 2 ,δ) and ¯W(p,t + n·δ·ρ 2 ,δ) ⊆ ¯B(p,t + n·δ·ρ) By Lemma C.3 we have µ( ¯W(p,t + n·δ·ρ 2 ,δ)) ≤µ( ¯B(p,t + n·δ·ρ)). By Lemma C.12 we have: µ( ¯W(p,t + n·δ·ρ 2 ,δ)) = δn ·|¯B(p,t + n·δ·ρ 2 ) ∩G(δ)|. By combining the facts we obtain: |¯B(p,t) ∩U(δ)|≤| G(δ) ∩¯B(p,t + n·δ·ρ 2 )|≤ µ( ¯W(p,t + n·δ·ρ 2 ,δ)) δn ≤µ( ¯B(p,t + n·δ·ρ)) δn 25A new compressed cover tree for k-nearest neighbors |¯B(p,t) ∩U(δ)|≥| G(δ) ∩¯B(p,t −n·δ·ρ 2 )|≥ µ( ¯W(p,t −n·δ·ρ 2 ,δ)) δn ≥µ( ¯B(p,t −n·δ·ρ)) δn which concludes the proofs. Lemma C.14 (Set U(δ) is locally ﬁnite) . Let Rn be a normed vector space. Let R⊆Rn be its ﬁnite subset Then any set U(δ) from Lemma C.9 is locally ﬁnite. Proof. With the exact same proof of Lemma C.13 it can be shown that |¯B(p,t) ∩U(δ)|≤ µ( ¯B(p,t + δ·n·ρ)) δn is satisﬁed for all p∈Rand t> 0. Therefore |¯B(p,t) ∩U(δ)|is ﬁnite as well. Recall that minimized expansion constant of Deﬁnition 1.4 of a ﬁnite subset Rof a metric space (X,d) was deﬁned as cm(R) = lim ξ→0+ inf R⊆A⊆X sup p∈A,t>ξ |¯B(p,2t) ∩A| |¯B(p,t) ∩A| where Ais a locally ﬁnite set which covers R. Theorem C.15 (The minimized expansion constant of a ﬁnite subset Rof Rn is at most 2n). Let Rbe a ﬁnite subset of a normed Euclidean space Rn. Let cm(R) be the minimized expansion constant of Deﬁnition 1.4, then cm(R) ≤2n. Proof. Let 0 <ξ < dmin(R) 2 be an arbitrary real number. Let 0 <δ < ξ n·ρ be a real number. Since δ <dmin(R) 2·n·ρ by Lemma C.13 we have: µ( ¯B(p,t −δ·n·ρ)) δn ≤| ¯B(p,t) ∩U(δ)|≤ µ( ¯B(p,t + δ·γ·n·ρ)) δn Note that by Lemma C.4 we have: µ( ¯B(q,y)) = yn ·µ( ¯B(q,1)) for any q∈Rn and y∈R+. Therefore |¯B(p,2t) ∩U(δ)| |¯B(p,t) ∩U(δ)| ≤µ( ¯B(p,2t+ nδρ)) ·δ2 µ( ¯B(p,t −nδρ)) ·δ2 = (2t+ nδρ)n ·µ( ¯B(p,1)) (t−nδρ)n ·µ( ¯B(p,1)) = (2t+ nδρ)n (t−nδρ)n , is satisﬁed for for all t>ξ . Since 0 <ξ < dmin(R) 2 was chosen arbitrarily, we conclude that: cm(R) = lim ξ→0+ inf R⊆A⊆X sup p∈A,t>ξ |¯B(p,2t)|∩A |¯B(p,t)|∩A ≤lim δ→0 |¯B(p,2t) ∩U(δ)| |¯B(p,t) ∩U(δ)| = lim δ→0 (2t+ δ·nρ)n (t−δ·nρ)n = 2n ·tn tn = 2 n. D. Distinctive descendant sets This section introduces auxiliary concepts for future proo fs. The main concept is a distinctive descendant set in Deﬁni tion 2.8. The distinctive descendant set at a level iof a node p ∈T (R) in a compressed cover tree corresponds to the set of descendants of a copy of node pat level iin the original implicit cover tree T(R). Other important concepts are λ-point of Deﬁnition D.6 that is used in Algorithm F .2 as an approxima tion for k-nearest neighboring point. The β-point property of λ-point deﬁned in Lemma D.15 plays a major role in the proof of t he main worst-case time complexity result Theorem 4.9. Deﬁnition 2.8 (Distinctive descendant sets) . Let R ⊆X be a ﬁnite reference set with a compressed cover tree T(R). F or any node p ∈ T(R) and level i ≤ l(p) −1, set Vi(p) = {u ∈ Descendants(p) | i ≤ l(u) ≤ l(p) −1}. If i ≥l(p), then set Vi(p) = ∅. F or any level i ≤l(p), the distinctive descendant set is Si(p,T(R)) = Descendants( p) \\⋃ u∈Vi(p) Descendants(u) and has the size |Si(p,T(R))|. Lemma D.1 (Distinctive descendant set inclusion property) . In conditions of Deﬁnition 2.8 let p ∈ R and let i,j be integers satisfying lmin(T(R)) ≤i≤j ≤l(p) −1. Then Si(p,T(R)) ⊆Sj (p,T(R)). 26A new compressed cover tree for k-nearest neighbors Level 2 Level 1 Level 0 Level -1 1 5 3 2 4 7 8 Figure 9. Consider a compressed cover tree T (R) that was built on set R = {1,2,3,4,5,7,8}. Let Si(p,T (R)) be a distinctive descendant set of Deﬁnition 2.8. Then V2(1) = ∅, V1(1) = {5} and V0(1) = {3,5,7}. And also S2(1,T (R)) = {1,2,3,4,5,7,8}, S1(1,T (R)) = {1,2,3,4} and S0(1,T (R)) = {1}. Essential levels of a node p∈T (R) have 1-1 correspondence to the set consisting of all nodes co ntaining pin the explicit representation of cover tree in (Beygelzimer et al., 2006a) , see Figure 6 middle. Deﬁnition D.2 (Essential levels of a node) . Let R⊆X be a ﬁnite reference set with a cover tree T(R). Let q∈T (R) be a node. Let (ti) for i∈{0,1,...,n }be a sequence of H(T(R)) in such a way that t0 = l(q), tn = lmin(T(R)) and for all iwe have ti+1 = Next( q,ti,T(R)). Deﬁne the set of essential indices E(q,T(R)) = {ti |i∈{0,...,n }}. ■ Lemma D.3 (Number of essential levels) . Let R ⊆ X be a ﬁnite reference set with a cover tree T(R). Then∑ p∈R |E(p,T(R))|≤ 2 ·|R|,where E(p,T(R)) appears in Deﬁnition D.2. ■ Proof. Let us prove this claim by induction on size |R|. In basecase R = {r}and therefore |E(r,T(R))|= 1 . Assume now that the claim holds for any tree T(R), where |R|= mand let us prove that if we add any node v ∈X \\Rto tree T(R), then ∑ p∈R |E(p,T(R∪{v}))|≤ 2 ·|R|+ 2 . Assume that we have added uto T(R), in such a way that vis its new parent. Then |E(p,T(R∪{v}))|= |E(p,T(R))|+ 1 and |E(v,T(R∪{v}))|= 1 . W e have: ∑ p∈R∪{u} |E(p,T(R))|= ∑ p∈R |E(p,T(R))|+ 1 + |E(v,T(R∪{v}))|≤ 2 ·|R|+ 2 ≤2(|R∪{v}|) which completes the induction step. Algorithm D.4 This algorithm returns sizes of distinctive descendant set Si(p,T(R)) for all essential levels i∈E(p,T(R)) 1: Function : CountDistinctiveDescendants(Node p, a level iof T(R)) 2: Output : an integer 3: if i>l min(T(Q)) then 4: for q∈Children(p) having l(p) = i−1 or q= pdo 5: Set s= 0 6: j ←1 + Next( q,i −1,T(R)) 7: s←s+CountDistinctiveDescendants(q, j) 8: end for 9: else 10: Set s= 1 11: end if 12: Set |Si(p)|= sand return s Lemma D.5. Let Rbe a ﬁnite subset of a metric space. Let T(R) be a compressed cover tree on R. Then, Algorithm D.4 computes the sizes |Si(p,T(R))|for all p∈Rand essential levels i∈E(p,T(R)) in time O(|R|). ■ Proof. By Lemma D.3 we have ∑ p∈R |E(p,T(R))|≤ 2 ·|R|.Since CountDistinctiveDescendants is called once for ever y any combination p∈Rand i∈E(p,T(R)) it follows that the time complexity of Algorithm D.4 is O(R). 27A new compressed cover tree for k-nearest neighbors Recall that the neighbor set N(q; r) = {p∈C |d(q,p) ≤d(q,r)}was introduced in Deﬁnition 1.2. Deﬁnition D.6 (λ-point). Fix a query point q in a metric space (X,d) and ﬁx any level i ∈Z. Let T(R) be its com- pressed cover tree on a ﬁnite reference set R ⊆X. Let C be a subset of a cover set Ci from Deﬁnition 2.1 satisfying∑ p∈C |Si(p,T(R))|≥ k, where Si(p,T(R)) is the distinctive descendant set from Deﬁnition 2.8. F or an y k ≥1, deﬁne λk(q,C) as a point λ∈Cthat minimizes d(q,λ) subject to ∑ p∈N(q;λ) |Si(p,T(R))|≥ k. ■ Algorithm D.7 Finding k-lowest element of a ﬁnite subset A⊆Rwith priority function f : A→R 1: Input: Ordered subset A⊆R, priority function f : A→R, an integer k∈Z 2: Initialize an empty max-binary heap Band an empty array Don points A. 3: for p∈Ado 4: add pto Bwith priority f(p) 5: if |H|≥ kthen 6: remove the point with a maximal value from B 7: end if 8: end for 9: Transfer points from the binary heap Bto the array Din reverse order. 10: return D. Algorithm D.8 Computation of a λ-point of Deﬁnition D.6 in line 6 of Algorithm F .2 1: Input: A point q∈X, a subset Cof a level set Ci of a compressed cover tree T(R), an integer k∈Z 2: Deﬁne f : C →R by setting f(p) = d(p,q). 3: Run Algorithm D.7 on inputs (C,f,k ) and retrieve array D. 4: Find the smallest index jsuch that ∑ j t=0 |Si(D[t],T(R))|≥ k. 5: return λ= D[j]. Lemma D.9. Let A⊆Rbe a ﬁnite subset and let f : A→R be a priority function and let k∈Z+. Then Algorithm D.7 ﬁnds k-smallest elements of Ain time |A|·log2(k) Proof. Adding and removing element from binary heap data structure Cormen (1990, section 6.5) takes at most O(log(n)) time, where nis the size of binary heap. Since the size of our binary heap is capped at kand we add/remove at most |A| elements, the total time complexity is O(|A|·log2(k)). Lemma D.10 (time complexity of a λ-point). In the conditions of Deﬁnition D.6, the time complexity of Al gorithm D.8 is O(|C|·log2(k)). Proof. Note that in line 4 we have |Si(D[t],T(R))|≥ 1 for all t = 0 ,...,j . Therefore the time complexity of line 4 is O(k). By Lemma D.9 The time complexity of line 3 is O(|C|·log2(k)), which proves the claim. Lemma D.11 (separation). In the conditions of Deﬁnition 2.8, let p ̸= qbe nodes of T(R) with l(p) ≥i, l(q) ≥i. Then Si(p,T(R)) ∩Si(q,T(R)) = ∅. ■ Proof. Without loss of generality assume that l(p) ≥l(q). If qis not a descendant of p, the lemma holds trivially due to Descendants(q) ∩Descendants(p) = ∅. If qis a descendant of p, then l(q) ≤l(p) −1 and therefore q∈Vi(p). It follows that Si(p,T(R)) ∩Descendants(q) = ∅and therefore Si(p,T(R)) ∩Si(q,T(R)) ⊆Si(p,T(R)) ∩Descendants(q) = ∅. Lemma D.12 (Sum lemma) . In the notations of Deﬁnition 2.8 assume that i is arbitrarily index and a subset V ⊆R satisﬁes l(p) ≥ifor all p∈V. Then | ⋃ p∈V Si(p,T(R))|= ∑ p∈V |Si(p,T(R))|. 28A new compressed cover tree for k-nearest neighbors Proof. Proof follows from Lemma D.11. By Lemma D.12 in Deﬁnition D.6 one can assume that |⋃ p∈C Si(p,T(R))|≥ k. Lemma 2.9. Let R⊆X be a ﬁnite reference set with a cover tree T(R). In the notations of Deﬁnition 2.8, let p∈T (R) be any node. If w ∈ Si(p,T(R)) then either w = p or there exists a ∈Children(p) \\{p}such that l(a) < iand w∈Descendants(a). Proof. Let w ∈ Si(p) be an arbitrary node satisfying w ̸= p. Let s be the node-to-root path of w. The inclusion Si(p) ⊆Descendants(p) implies that w ∈Descendants(p). Let a ∈Children(p) \\{p}be a child on the path s. If l(a) ≥ i then a ∈ Vi(p). Note that w ∈ Descendants(a). Therefore w /∈ Si(p), which is a contradiction. Hence l(a) <i. Lemma D.13. In the notations of Deﬁnition 2.8, let p∈T (R) be any node. If w∈Si(p,T(R)) then d(w,p) ≤2i+1. ■ Proof. By Lemma 2.9 either w = γ or w ∈Descendants(a) for some a ∈Children(γ) \\{γ}for which l(a) < i. If w= γ, then trivially d(γ,w) ≤2i. Else wis a descendant of a, which is a child of node γon level i−1 or below , therefore by Lemma B.6 we have d(γ,w) ≤2i anyway. Lemma D.14. Let Rbe a ﬁnite subset of a metric pace. Let T(R) be a compressed cover tree on R. Let Rj ⊆Cj , where Cj is the ith cover set of T(R). Let i= max p∈Rj Next(p,j, T(R)). Set Ci(Rj ) = Rj ∪{a∈Children(p) for some p∈ Ri |l(a) = i}. Then ⋃ p∈Ci(Rj ) Si(p,T(R)) = ⋃ p∈Rj Sj (p,T(R)). Proof. Let a ∈⋃ p∈Ci(Rj ) Si(p,T(R)) be an arbitrary node. Then there exits u ∈Ci(Rj ) having a ∈Si(u,T(R)). By deﬁnition of index i, either u∈Rj or uhas a parent in Rj . If u∈Rj then we note that Vj (u) ⊆Vi(u). Since a /∈Vi(u), we also have a /∈Vj (u). Otherwise let wbe a parent of u. Therefore there are no descendants of win having level in interval [l(u) + 1 ,l(p) −1]. Since l(u) = iand j > iit follows that Vj (w) = ∅. Denote w to be the lowest level ancestor of uon level j. By cases above we have a /∈Vj (w). Therefore it follows that a∈Sj (w,T(R)) ⊆ ⋃ p∈Rj Sj (p,T(R)). T o prove the converse inclusion assume now that a ∈ ⋃ p∈Rj Sj (p,T(R)). Then a ∈ Sj (v,T(R)) for some w ∈ Rj . Assume that whas no children at the level i. Then Vj (w) = Vi(w) and a∈Si(w,T(R)) ⊆ ⋃ p∈Ci(Rj ) Si(p,T(R)). Assume now that w has children at the level i. If there exists b ∈Children(w) for which a ∈Descendants(b). Since Vi(b) = ∅, we conclude that a∈Si(b,T(R)) ⊆ ⋃ p∈Ci(Rj ) Si(p,T(R)). Assume that a /∈ Descendants(b) for all b ∈ Children(w) with l(b) = i. Then a ∈ Descendants(w) and a /∈Descendants(b′) for any b′ ∈Vj (w). Then a∈Si(w,T(R)) and the proof ﬁnishes: ⋃ p∈Rj Sj (p,T(R)) ⊆ ⋃ p∈Ci(Rj ) Si(p,T(R)). 29A new compressed cover tree for k-nearest neighbors Lemma D.15 (β-point). In the notations of Deﬁnition D.6, let C ⊆Ci so that ∪p∈C Si(p,T(R)) contains all k-nearest neighbors of q. Set λ = λk(q,C). Then Rhas a point β among the ﬁrst k nearest neighbors of q such that d(q,λ) ≤ d(q,β) + 2 i+1. ■ Proof. W e show that Rhas a point βamong the ﬁrst knearest neighbors of qsuch that β ∈ ⋃ p∈C Si(p,T(R)) \\ ⋃ p∈N(q,λ)\\{λ} Si(p,T(R)). Lemma D.12 and Deﬁnition D.6 imply that | ⋃ p∈N(q,λ)\\{λ} Si(p,T(R))|= ∑ p∈N(q,λ)\\{λ} |Si(p,T(R))|<k. Since ∪p∈C Si(p,T(R)) contains all k-nearest neighbors of q, a required point β ∈Rexists. Let us now show that β satisﬁes d(q,λ) ≤d(q,β) + 2 i+1. Let γ ∈C \\N(q,λ) ∪{λ}be such that β ∈Si(γ,T(R)). Since γ /∈N(q,λ) \\{λ}, we get d(γ,q) ≥d(q,λ). The triangle inequality says that d(q,γ) ≤d(q,β) + d(γ,β). Finally Lemma D.13 implies that d(γ,β) ≤2i+1. Then d(q,λ) ≤d(q,γ) ≤d(q,β) + d(γ,β) ≤d(q,β) + 2 i+1 So βis a desired k-nearest neighbor satisfying d(q,λ) ≤d(q,β) + 2 i+1. E. Construction of a compressed cover tree This section introduces a new method Algorithm E.2 for const ruction of a compressed cover tree, which is based on Insert( ) method Beygelzimer et al. (2006a, Algorithm 2) that was spec iﬁcally adapted for compressed cover tree. The proof of Beygelzimer et al. (2006a, Theorem 6), which estimated the t ime complexity of Beygelzimer et al. (2006a, Algorithm 2) was shown to be incorrect Elkin & Kurlin (2022a, Counterexam ple 4.2). The main contribution of this section are two new time complexity results that bound the time complexity of Al gorithm E.2: • Theorem 3.7 bounds the time complexity as O(cm(R)10 ·log2(∆( R)) ·|R|) by using minimized expansion constant cm(R) and aspect ratio ∆( R) as parameters. • Theorem 3.10 bounds the time complexity as O(c(R)12 ·log2 |R|·|R|) by using expansion constant c(R) as parameter. Deﬁnition 2.10 explains the concrete implementation of com pressed cover tree. Deﬁnition 2.10 (Children(p,i) and Next(p,i, T(R))). In a compressed cover tree T(R) on a set R, for any level iand a node p ∈R, set Children(p,i) = {a∈Children(p) |l(a) = i}. Let Next(p,i, T(R)) be the maximal level j satisfying j < iand Children(p,i) ̸= ∅. If such level does not exist, we set j = lmin(T(R)) −1. F or every node p, we store its set of children in a linked hash map so that (a) any key igives access to Children(p,i), (b) Children(p,i) →Children(p,Next(p,i, T(R))), (c) we can directly access max{j|Children(p,j) ̸= ∅}. Deﬁnition E.1 (construction iteration set L(T(W),p)). Let W be a ﬁnite subset of a metric space (X,d). Let T(W) be a cover tree of Deﬁnition 2.1 built on W and let p∈X\\W be an arbitrary point. Let L(T(W),p) ⊆H(T(R)) be the set of all levels iduring iterations 5-13 of Algorithm E.3 launched with input s T(W),p. Set η(i) = min t{t∈L(T(W),p) | t>i }. 30A new compressed cover tree for k-nearest neighbors Algorithm E.2 Building a compressed cover tree T(R) from Deﬁnition 2.1. 1: Input : a ﬁnite subset Rof a metric space (X,d) 2: Output : a compressed cover tree T(R). 3: Choose a random point r∈Rto be a root of T(R) 4: Build the initial compressed cover tree T = T({r}) by making l(r) = + ∞. 5: for p∈R\\{r}do 6: T ←run AddPoint (T,p) described in Algorithm E.3. 7: end for 8: For root rof T set l(r) = 1 + max p∈R\\{r} l(p) Algorithm E.3 Building T(W ∪{p}) in lines 5-7 of Algorithm E.2. 1: Function AddPoint(a compressed cover tree T(W) with a root r, a point p∈X) 2: Output : compressed cover tree T(W ∪{p}). 3: Set i←lmax(T(W)) −1 and η(lmax −1) = lmax {If the root rhas no children then i←−∞} 4: Set Rlmax ←{r}and initialize sorted dictionary M with M[lmax] = {r} 5: while i≥lmin do 6: Assign Ci(Rη(i)) ←Rη(i) ∪{a∈Children(q) for some q∈Rη(i) |l(a) = i} 7: Set Ri = {a∈Ci(Rη(i)) |d(p,a) ≤2i+1}and M[i] = Ri. 8: if Ri is empty then 9: Launch Algorithm E.4 with parameters (p,M) and exit this algorithm . 10: end if 11: t= max a∈Ri Next(a,i, T(W)) {If Ri has no children we set t= lmin −1} 12: η(i) ←iand i←t 13: end while 14: Launch Algorithm E.4 with parameters (p,M). Algorithm E.4 Assign node subprocedure 1: Function AssignParent(Point p, dictionary M) 2: Output: Compressed cover tree T(W ∪{p}) 3: Set ito be the lowest key of M. 4: while i≤lmax do 5: if d(p,Ri) ≤2i then 6: Let q∈Ri such that d(q,p) = d(Ri,p), let xmaximal integer for which d(p,q) >2x. 7: Set l(p) = xand qto be the parent of p. 8: end if 9: Find next key j >iof M and set i= j 10: end while 31A new compressed cover tree for k-nearest neighbors Let R be a ﬁnite subset of a metric space (X,d). A compressed cover tree T(R) will be incrementally constructed by adding points one by one as summarized in Algorithm E.2. Firs t we select a root node r∈Rand form a tree T({r}) of a single node rat the level lmax = lmin = + ∞. Assume that we have a compressed cover tree T(W) for a subset W ⊂R. For any point p∈R\\W, Algorithm E.3 builds a larger compressed cover tree T(W ∪{p}) from T(W). Note that during the construction of the compressed cover tr ee in Algorithm E.3 we write down additional information for every node p, which includes the number of descendants of node pand the maximal level of nodes in set Children(p). Lemma E.5. Let T(R) be a cover tree and let p ∈X be a point and let i ∈Z. Assume that for some q ∈T (R) we have d(p,q) > 2i+1. Let Si(q,T(R)) be as deﬁned in Deﬁnition 2.8. Then for any θ ∈Si(q,T(R)) \\{q}we have d(θ,p) >2l(θ). Proof. Let S = ( θ = a0,...,a m = q) be a node to node path. Since θ ∈Si(q,T(R)) \\{q}by Lemma 2.9 we have l(am−1) ≤i−1. Therefore l(θ) = l(a0) ≤...≤l(am−1) ≤i−1. W e have the following inequality: d(q,θ) ≤ h−1∑ z=0 d(az,az+1) ≤ j∑ x=l(θ)+1 2x = (2 j+1 −2l(θ)+1). By triangle inequality we have: d(p,θ) ≥d(p,γ) −d(γ,θ) >2j+1 −(2j+1 −2l(θ)+1) >2l(θ). Therefore d(p,θ) >2l(θ) which proves the claim. Theorem 3.2 (correctness of Algorithm 3.4) . Algorithm 3.4 builds a compressed cover tree in Deﬁnition 2. 1. Proof. It sufﬁces to prove that Algorithm E.3 correctly extends a co mpressed cover tree T(W) for any ﬁnite subset W ⊆X by adding a point p. Let us prove that T(W ∪{p}) satisﬁes Deﬁnition 2.1. W e ﬁrst note that the parent qof pis always assigned in Algorithm E.4 by choosing q ∈Ri which minimizes d(Ri,p) as the parent of pand by setting level of pto be maximal integer which satisﬁes d(p,q) >2x. Since d(Ri,p) ≤2i we have l(p)x<i ≤l(q). Therefore l(p) <l(q). W e also have d(q,p) ≤2x+1 ≤2l(p)+1. Therefore both conditions of (2.1b) are satisﬁed. T o check (2.1c) Consider arbitrary cover set Ch = {q ∈T (W ∪{p}) |l(q) ≥h}. Since we have assumed that T(W) is a valid cover tree, all the cover sets Ch for h > l(p) satisfy the condition. Let us consider cover sets having h ≤l(p). Consider a sequence of iterations lmin(T(W)) ≤s(0) <s(1) <...<s (t) = lmax(T(W)) that were saved as keys of the dictionary M deﬁned in Line 7 of Algorithm E.3. Let θ∈Ch be an arbitrary node. Assume ﬁrst that θ /∈Rs(j) for all j ∈{0,...,s (t)}. Therefore θ is a non-trivial descendant of some node γ that was eliminated in line 7 in some level s(m). W e have θ ∈Ss(m)(γ,T(R)) \\{γ}and by line 7 also d(p,γ) > 2s(m)+1. By Lemma E.5 it follows that d(p,θ) >2l(θ). Since θ∈Ch it follows that d(p,θ) >2h. Assume then that θ ∈Rs(j) for all j ∈[n,m]. Assume ﬁrst that s(n−1) ≥h. Since θ ∈Cs(n−1)(Rs(n)) \\Rs(n−1) we have d(p,θ) > 2s(n−1)+1 ≥2h. W e then assume that s(n−1) < h. Since θ ∈Ch it follows that s(m) ≥h. Pick minimal u≥nsuch that h≤s(u). Assume ﬁrst that h≤s(u) ≤l(p), then by line 5 in Algorithm E.4 we have d(θ,p) ≥ d(Rs(u),p) > 2s(u) ≥2h. In ﬁnal case assume that h ≤l(p) < s(u). Since s(u−1) < hit follows that the parent of p was selected from Rs(u) in Algorithm E.4. By line 6 we have d(Rs(u),p) >2l(p). Therefore d(θ,p) >2l(p) ≥2h. Lemma 3.3 (time complexity of a key step for T(R)). Arbitrarily order all points of a ﬁnite reference set Rin a metric space (X,d) starting from the root: r = p1, p2,...,p |R|. Set W1 = {r}and Wy+1 = Wy ∪{py}for y = 1 ,..., |R|−1. Then Algorithm 3.4 builds a compressed cover tree T(R) in time O ( (cm(R))8 · max y=1,...,|R|−1 L(T(Wy ),py) ·|R| ) , 32A new compressed cover tree for k-nearest neighbors where cm(R) is the minimized expansion constant from Deﬁnition 1.4. Proof. The worst-case time complexity of Algorithm E.2 is dominate d by lines 5-7 which call Algorithm E.3 O(|R|) times in total. Assume that we have already constructed a cover tree on set T(Wy ), the goal Algorithm E.3 is to construct tree T(Wy ∪ {py+1}). By Deﬁnition E.1 loop on lines 5-13 is performed L(T(Wy ),py+1) times. Let R∗ be the maximal size of set Ri during all iterations i∈L(T(Wy ),py+1). By Lemma 2.3 since Wy+1 ⊆R⊆X we have |Ci(Rη(i))|≤ cm(Wy+1)4 ·|R∗|≤ cm(R)4 ·|R∗| nodes, where Cη(i)(Rη(i)) is deﬁned in line 6. Therefore both, lines 7 and 6 take at most cm(R)4|R∗|time. In line 11 we handle |R∗|elements, for each of them we can retrieve index Next(a,i, T(Wy)) in O(1) time, since for every a∈T (R) we can update the last index j, when ahad children on level jin line 6. Therefore line 11 takes at most O(|R∗|) time. Algorithm E.4 is called once during whole run-time of t he algorithm takes at most O(L(T(Wy ),py+1) ∗|R∗|) time. Therefore line 9 and line 14 take at most O(L(T(Wy ),py+1) ∗|R∗|) time. Let us now bound |R∗|during the whole run-time of the algorithm. Letibe an arbitrary level. Note that Ri ⊆B(p,2i+1) ∩Ci where Ci is a ith cover set of T(R). Since Ci is 2i-spares subset of Rwe can apply packing Lemma 2.2 with r= 2 i+1 and δ= 2 i to obtain |B(p,2i+1) ∩Ci|≤ (cm(W))4. Lemma 1.5 implies (cm(W))4 ≤(cm(R))4, therefore |B(p,2i) ∩Ci|≤ (cm(R))4. The time complexity of loop 5 - 13 in Algorithm E.3 is dominate d by line 6 that has time O(|C(Ri)|) ≤O((cm(R))4 · |R∗|) ≤O((cm(R))8). Then the whole Algorithm E.2 has time O((cm(R))8 · max y=2,...,|R| L(T(Wy−1),py) ·|R|) as desired. Theorem 3.7 (time complexity of T(R) via aspect ratio) . Let R be a ﬁnite subset of a metric space (X,d) having the aspect ratio ∆( R). Algorithm 3.4 builds a compressed cover tree T(R) in time O((cm(R))8 ·log2(∆( R)) ·|R|),where cm(R) is the minimized expansion constant from Deﬁnition 1.4. Proof. In Lemma 3.3 use the upper bounds due to Lemma B.8 as follows: max y∈2,...,|R| |L(T(Wy−1),py)|≤ H(T(R)) ≤1 + log 2(∆( R)). Lemma 3.8. Let (X,d) be a metric space and let W ⊆X be its ﬁnite subset. Let q ∈X\\W be an arbitrary point. Let i∈L(T(W),q) be arbitrarily iteration of Deﬁnition 3.1. Assume that t= η(η(i+ 1)) is deﬁned. Then there exists p∈W satisfying 2i+1 <d(p,q) ≤2t+1. Proof. Note ﬁrst that since η(i+ 3) ∈L(T(R),q), there exists distinct u ∈Rη(η(i+3)) and v ∈Cη(i+1)(Rη(η(i+1))), in such a way that u is the parent of v. Let us show that both of u,v cant belong to set Ri. Assume contrary that both u,v ∈Ri. Then by line 7 of Algorithm E.3 we have d(v,q) ≤2i+1 and d(u,q) ≤2i+1. By triangle inequality d(u,v) ≤d(u,q) + d(q,v) ≤2i+2 ≤2η(i+1). Recall that we denote a level of a node by l. On the other hand we have l(u) ≥η(i+ 1) and l(v) ≥η(i+ 1) , by separation condition of Deﬁnition 2.1 we have d(u,v) > 2η(i+1), which is a contradiction. Therefore only one of {u,v}can belong to Ri. It sufﬁcies two consider the two cases below: Assume that v /∈ Ri. Since v is children of u we have d(u,v) ≤ 2η(i+1)+1. By line 7 of Algorithm E.3 we have d(u,q) ≤2η(i+1)+1. By triangle inequality d(v,q) ≤d(v,u) + d(u,q) ≤2η(i+1)+1 + 2η(i+1)+1 ≤2η(i+1)+2 ≤2η(η(i+1))+1 Since v /∈Ri there exists level thaving η(i+ 1) ≥t ≥iand v ∈Ct(Rη(t)) \\Rt. Therefore by line 7 of Algorithm E.3 we have d(q,v) >2t+1 ≥2i+1. It follows that we have found point v∈Rsatisfying 2i+1 <v ≤2η(η(i+1))+1. Therefore p= v, is the desired point. 33A new compressed cover tree for k-nearest neighbors Assume that u /∈Ri. Since u∈Rη(η(i+1)), by line 7 of Algorithm E.3 we have d(u,q) ≤2η(η(i+1))+1. On the other hand since u /∈Ri, there exists level thaving η(i+ 3) ≥t ≥iand u ∈Ct(Rη(t)) \\Rt. Therefore by line 7 of Algorithm E.3 we have d(q,u) >2t+2 ≥2i+2. It follows that we have found point u∈Rsatisfying 2i+1 <u ≤2η(η(i+1))+1. Therefore p= u, is the desired point. Lemma 3.9 (Construction iteration bound) . Let A,W be ﬁnite subsets of a metric space X satisfying W ⊆A⊆X. T ake a point q ∈A\\W. Given a compressed cover tree T(W) on W, Algorithm 3.5 runs lines 6-16 this number of times: |L(T(W),q)|= O ( c(A)2 ·log2(|A|) ) . Proof. Let x∈L(T(R),q) be the lowest level of L(T(R),q). Deﬁne s1 = η(η(x)+1) and let si = η(η(η(si−1 +1))+1) , if it exists. Assume that sn+1 is the last sequence element for which η(η(η(sn−1 + 1)) + 1) is deﬁned. Deﬁne S = {s1,...,s n}. For every i∈{1,...,n }let pi be the point provided by Lemma 3.8 that satisﬁes 2si+1 <d(pi,q) ≤2η(η(si+1))+1. Let P be the sequence of points pi. Denote n= |P|= |S|. Let us show that Ssatisﬁes the conditions of Lemma 2.5. Note that: 4 ·d(pi,q) ≤4 ·2η(η(si+1))+1 ≤2η(η(si+1))+3 ≤2η(η(η(si+1))+1)+1 ≤2si+1+1 ≤d(pi+1,q) By Lemma 2.5 applied for set Aand sequence P we get: |¯B(q,4 3 d(q,pn))|≥ (1 + 1 c(R)2 )n ·|¯B(q,1 3d(q,p1))| Since η(x) ∈L(T(R),q) , there exists some point u∈Rη(x). By deﬁnition of Ri we have d(u,q) ≤2η(x)+1. Also 2η(η(x)+1)−1 ≤2η(η(x)+1)+1 3 < d(q,p1) 3 It follows that: 1 ≤| ¯B(q,2η(x)+1)|≤| ¯B(q,2η(η(x)+1)−1|≤| ¯B(q,d(q,p1) 3 )| Therefore we have |A|≥ |¯B(q, 4 3 ·d(q,pn))| |¯B(q,1 3 ·d(q,p1))|≥(1 + 1 c(A)2 )n Note that c(A) ≥ 2 by deﬁnition of expansion constant. Then by applying log and by using Lemma B.7 we obtain: c(A)2 log(A) ≥n= |S|. Let xbe minimal level of L(T(W),q) and let ybe the maximal level of L(T(W),q) Note that Sis a sub sequence of Lin such a way that: • [x,s1] ∩L(T(R),q) ≤3, • for all i∈1,...,n we have [si,si+1] ∩L(T(R),q) ≤6 • [sn,y] ∩L(T(R),q) <12 Since segments [x,s1],[s1,s2],..., [s2,sn],[sn,y] cover |L(T(R),q)|, it follows that |S|≥ |L(T (R),q)| 12 . W e obtain that |L(T(R),q)|≤ 12 ·c(A)2 ·log2(|A|), which proves the claim. Theorem 3.10 (time for T(R) via expansion constants) . Let Rbe a ﬁnite subset of a metric space (X,d). Let Abe a ﬁnite subset of X satisfying R⊆A⊆X. Then Algorithm 3.4 builds a compressed cover tree T(R) in time O((cm(R))8 · c(A)2 ·log2(|A|) ·|R|), see the expansion constants c(A),cm(R) in Deﬁnition 1.4. Proof. It follows from Lemmas 3.9 and 3.3. 34A new compressed cover tree for k-nearest neighbors Corollary 3.11. Let R be a ﬁnite subset of a metric space (X,d). Then Algorithm 3.4 builds a compressed cover tree T(R) in time O((cm(R))8 ·c(R)2 ·log2(|R|)) ·|R|),where the constants c(R),cm(R) appeared in Deﬁnition 1.4. Proof. The proof follows from Theorem 3.10 by setting A= R. F . k-nearest neighbor search algorithm This section is motivated by Elkin & Kurlin (2022a, Countere xample 5.2), which showed that the proof of past time com- plexity claim in Beygelzimer et al. (2006a, Theorem 5) for th e nearest neighbor search algorithm contained gaps. The two main results of this sections are Corollary 4.7 and Theor em 4.9 which provide new time complexity results for k- nearest neighbor problem, assuming that a compressed cover tree was already constructed for the reference set R. For the construction algorithm of compressed cover tree and its tim e complexity, we refer to Section E. The past mistakes are resolved by introducing a new Algorith m F .2 for ﬁnding k-nearest neighbors that generalize and im- proves the original method for ﬁnding nearest neighbors usi ng an implicit cover. Beygelzimer et al. (2006a, Algorithm 1 ). The ﬁrst improvement is λ-point of line 6 which allows us to search for all k-nearest neighbors of a given query point for any k ≥1. The second improvement is a new loop break condition on line 8. The new loop break condition is utilized in the proof of Lemma 4.8 to conclude that the total number of per formed iterations is bounded by O(c(R)2 log(|R|)) during whole run-time of the algorithm. The latter improvement clo ses the past gap in proof of Beygelzimer et al. (2006a, Theo- rem 5) by bounding the number of iterations independently fr om the explicit depth Elkin & Kurlin (2022a, Deﬁnition 3.2), that generated the past confusion. Recall from Deﬁnition D.2 that an essential setE(p,T(R)) ⊆H(T(R) consists of all levels i ∈H(T(R)) for which p has non-trivial children in T(R) at level i. By Lemma D.5 the sizes of distinctive descendants |Si(p,T(R))|can be precomputed in a linear time O(|R|) for all p ∈R and i ∈ E(p,T(R)). Since the size of distinctive descendant set |Si(p,T(R))|can only change at indices i∈E(p,T(R)), we assume that the sizes of |Si(p,T(R))|can be retrieved in a constant time O(1) for any p∈Rand i∈H(T(R)) during the run-time of Algorithm F .2. Deﬁnition F .1. Let Rbe a ﬁnite subset of a metric space (X,d). Let T(R) be a cover tree of Deﬁnition 2.1 built on R and let q ∈X be arbitrary point. Let L(T(R),q) ⊆H(T(R)) be the set of all levels iduring iterations of lines 4-17 of Algorithm F .2 launched with inputs T(R),q. If Algorithm F .2 reaches line 13 at level ̺∈L(T(R),q), then we say that is special. W e denote η(i) = min t{t∈L(T(R),q) |t>i }. ■ Note that η(i) of Deﬁnition F .1 may be undeﬁned. If η(i) is deﬁned, then by deﬁnition we have η(i) ≥i+ 1. Let dk(q,R) be the distance of qto its kth nearest neighbor in R. Example F .4 (Simulated run of Algorithm F .2) . Let Rand T(R) be as in Example B.2. Let q= 0 and k= 5 . Figures 10, 11, 12 and 13 illustrate simulated run of Algorithm F .2 on inp ut (T(R),q,k ). Recall that lmax = 2 and lmin = −1. During the iteration iof Algorithm F .2 we maintain the following coloring: Points in Ri are colored orange. Points Cη(i)(Rη(i)) (of line 5) that are not contained in Ri are colored yellow . The λ-point of line 6 is denoted by using purple color. All the nodes that were present in Rη(i) , but are no longer included in Ri will be colored red. Finally all the points that are selected as k-nearest neighbors of qare colored green in the ﬁnal iteration. Nodes that haven’t b een yet visited or that will never be visited are colored white. Let R2 = {8}. Consider the following steps: Iteration i = 1 : Figure 10 illustrates iteration i = 1 of the Algorithm F .2. In line 5 we ﬁnd C1(R2) = {4,8,12}. Since node 4 minimizes distance d(C1(R2),0) and distinctive descendant set S2(4,T(R)) consists of 7 elements we get λ = 4 and therefore d(q,λ) = 4 ≤2i+2 = 8 . In line 7 we ﬁnd R1 = {r∈C |d(0,r) ≤d(q,λ) + 2 3 = 12 }= {4,8,12}. Iteration i = 0 : Figure 11 illustrates iteration i = 0 of the Algorithm F .2. In line 5 we ﬁnd C0(R1) = {2,4,6,8,10,12,14}.Since |S1(2,T(R))| = 3 , |S1(4,T(R))| = 1 and |T1(6)| = 3 and 6 is the node with small- est to distance 0 satisfying ∑ p∈N(0,6)={2,4,6} |S1(p,T(R))| ≥5 = k. It follows that λ = 6 . In line 7 we ﬁnd R0 = {r ∈C (R1) |d(0,r) ≤d(q,λ) + 2 2 = 10 }= {2,4,6,8,10}. Since d(q,λ) > 2i+2 = 4 . W e proceed into lines 8 - 14 Final blocklines 8 - 14 for i= 0 : Figure 12 marks all the points Sdiscovered by line 11 as orange. Figure 13 illustrates the ﬁnal selection of kpoints from set Sthat are selected as the ﬁnal output {1,2,3,4,5}. 35A new compressed cover tree for k-nearest neighbors Algorithm F .2 k-nearest neighbor search by a compressed cover tree 1: Input : compressed cover tree T(R), a query point q∈X, an integer k∈Z+ 2: Set i←lmax(T(R)) −1 and η(lmax −1) = lmax 3: Let rbe the root node of T(R). Set Rlmax = {r}. 4: while i≥lmin do 5: Assign Ci(Rη(i)) ←Rη(i) ∪{a∈Children(p) for some p∈Rη(i) |l(a) = i} {Recall that Children(p) contains node p} 6: Compute λ= λk(q,Ci(Rη(i))) from Deﬁnition D.6 by Algorithm D.8. 7: Find Ri = {p∈Ci(Rη(i)) |d(q,p) ≤d(q,λ) + 2 i+2} 8: if d(q,λ) >2i+2 then 9: Deﬁne list S = ∅ 10: for p∈Ri do 11: Update Sby running Algorithm F .3 on (p,i) 12: end for 13: Compute and output k-nearest neighbors of the query point qfrom set S. 14: end if 15: Set j ←maxa∈Ri Next(a,i, T(R)) {If such jis undeﬁned, we set j = lmin −1} 16: Set η(j) ←iand i←j. 17: end while 18: Compute and output k-nearest neighbors of query point qfrom the set Rlmin . Algorithm F .3 The node collector called in line 11 of Algorithm F .2. 1: Input: p∈R, index i. 2: Output: a list S ⊆Rcontaining all nodes of Si(p,T(R)). 3: Add pto list S. 4: if i>l min(T(R)) then 5: Set j = Next( p,i, T(R)) 6: Set C = {a∈Children(p) |l(a) = j} 7: for u∈Cdo 8: Call Algorithm F .3 with (u,j). 9: end for 10: end if 36A new compressed cover tree for k-nearest neighbors Level 2 Level 1 Level 0 Level −1 1 3 5 7 9 11 13 15 2 6 10 14 4 12 8 Figure 10. Iteration i = 1 of simulation in Example F .4 of Algorithm F .2 Level 2 Level 1 Level 0 Level −1 1 3 5 7 9 11 13 15 2 6 10 14 4 12 8 Figure 11. Iteration i = 0 of simulation in Example F .4 of Algorithm F .2 Level 2 Level 1 Level 0 Level −1 1 3 5 7 9 11 13 15 2 6 10 14 4 12 8 Figure 12. Line 11 of Iteration i = 0 of simulation in Example F .4 of Algorithm F .2 Level 2 Level 1 Level 0 Level −1 1 3 5 7 9 11 13 15 2 6 10 14 4 12 8 Figure 13. Line 13 of iteration i = 0 of simulation in Example F .4 of Algorithm F .2 Note that ⋃ p∈Ri Si(p,T(R)) is decreasing set for which ⋃ p∈Rlmax Slmax (p,T(R)) = Rand ⋃ p∈Rlmin Slmin (p,T(R)) = Rlmin . 37A new compressed cover tree for k-nearest neighbors Lemma F .5 (k-nearest neighbors in the candidate set for all i). Let Rbe a ﬁnite subset of an ambient metric space (X,d), let q ∈X be a query point and let k ∈Z ∩[1,∞) be a parameter . Let T(R) be a compressed cover tree of R. Assume that |R|≥ k. Then for any iteration i ∈L(T(R),q) of Deﬁnition F .1 the candidate set ⋃ p∈Ri Si(p,T(R)) contains all k-nearest neighbors of q. ■ Proof. Since Rlmax = {r}, where r is the root T(R) we have Slmax (r,T(R)) = R and therefore any point among k- nearest neighbor of q is contained in Rlmax . Let i be the largest index for which there exists a point among k-nearest neighbor of qthat doesn’t belong to ⋃ p∈Ri Si(p,T(R)). Let us denote such point by β, then: β ∈ ⋃ p∈Rη(i) Sη(i)(p,T(R)) \\ ⋃ p∈Ri Si(p,T(R)). By Lemma D.14 we have ⋃ p∈Cη(i)(Rη(i) ) Si(p,T(R)) = ⋃ p∈Rη(i) Sη(i)(p,T(R)) (6) Let λbe as in line 6 of Algorithm F .2. By Equation (6) we have | ⋃ p∈Cη(i) (Rη(i)) Si(p,T(R))|≥ k, therefore by Deﬁnition D.6 such λexists. Since β ∈⋃ p∈Cη(i) (Rη(i) ) Si(p,T(R)), there exists α∈Cη(i)(Rη(i)) satisfying β ∈Si(α,T(R)). By assumption it follows α /∈Ri. By line 7 of the algorithm we have d(α,q) >d(q,λ) + 2 i+2. (7) Let w be arbitrary point in set ⋃ p∈N(q;λ) Si(p,T(R)). Therefore w ∈Si(γ,T(R)) for some γ ∈N(q; λ). By Lemma D.13 applied on iwe have d(γ,w) ≤2i+1. By Deﬁnition D.6 since γ ∈N(q; λ) we have d(q,γ) ≤d(q,λ). By (7) and the triangle inequality we obtain: d(q,w) ≤d(q,γ) + d(γ,w) ≤d(q,λ) + 2 i+1 <d(α,q) −2i+1 (8) On the other hand βis a descendant of αthus we can estimate: d(q,β) ≥d(q,α) −d(α,β) ≥d(α,q) −2i+1 (9) By combining Inequality (8) with Inequality (9) we obtain d(q,w) < d(q,β). Since w was arbitrary point from⋃ p∈N(q;λ) Si(p,T(R)), that contains at least k points, β cannot be any k-nearest neighbor of q, which is a contradic- tion. Theorem 4.4 (correctness of Algorithm 4.3) . Algorithm 4.3 correctly ﬁnds all k-nearest neighbors of query point qwithin the reference set R. Proof. Note that Algorithm F .2 is terminated by either reaching lin e 18 or by going inside block 10 - 12. Assume ﬁrst that Algorithm F .2 is terminated by reaching lin e 18. Claim follows directly from Lemma F .5 by noting that since i = lmin all the nodes p ∈Rlmin do not have any children. Therefore it follows ⋃ p∈Rlmin Si(p,T(R)) = Rlmin . Thus all the k-nearest neighbors of qare contained in the set Rlmin . Assume then that block 10 - 12 is reached during some iteratio n i ∈L(T(R),q). By Lemma F .5 set ⋃ p∈Ri Si(p,T(R)) contains all k-nearest neighbors of q. Note that in line 11 we collect all nodes of ⋃ p∈Ri Si(p,T(R)) into single array S. Therefore in line 13 we correctly select knearest neighbors of qfrom array S, which proves the claim. 38A new compressed cover tree for k-nearest neighbors Lemma 4.5. Algorithm 4.3 has the following time complexities of its lin es (a) max{#Line[4 −9],#Line[12 −15],#Line[16]}= O ( cm(R)10 ·log2(k) ) ; (b) #Line[8 −14] = O ( |¯B(q,5dk(q,R))|·log2(k) ) . Proof. (a) Let ̺∈L(T(R),q) be as in Deﬁnition F .1. Note that if iteration ̺is encountered, it becomes the last iteration of L(T(R),q). The total number of children encountered in line 5 during si ngle iteration (4-17) is at most is at most (cm(R))4 · max i∈L(T (R),q)\\̺ |Ri|by Lemma 2.3. From Lemma D.10 we obtain that line 6, which laun ches Algorithm D.8 takes at most |C(Ri)|·log2(k) = ( cm(R))4 · max L(q,T (R))\\̺ |Ri|·log2(k) time. Line 7 never does more work than line 5, since in the wors t case scenario Rη(i) is copied to Ri in its current form. Line 15 handles |Ri|nodes, since we can keep track of value of Next(a,i, T(R)) of Deﬁnition 2.10 by updating it when necessary in line 5 we can retrieve its value in O(1) time. Therefore maximal run-time of line 15 is max i∈L(q,T (R))\\̺ |Ri|. Final line 18 picks lowest k-elements Rη(i) ranked by function f(p) = d(p,q). By Lemma D.9 it can be computed in time O(log2(k) · max L(q,T (R))\\̺ |Ri|). It follows that max(#Line[4,8],#Line[14,17],#Line[18]) = O ( cm(R)4 · max i∈L(q,T (R))\\̺ |Ri|·log2(k) ) (10) Let us now bound maxi∈L(q,T (R))\\̺ |Ri|, by showing |Ri|≤ cm(R)6. Let Ci be the ith level of T(R) as in Deﬁnition 2.1. For all i∈L(T(R),q) \\̺we have: Ri = {r∈Ci(Rη(i)) |d(p,q) ≤d(q,λ) + 2 i+2} (11) = B(q,d(q,λ) + 2 i+2) ∩Ci(Ri) (12) ⊆B(q,2i+3) ∩Ci (13) From cover-tree condition we know that all the points in Ci are separated by 2i. W e will now apply Lemma 2.2 with t= 2 i+3 and δ= 2 i. Since 4 t δ + 1 = 2 5 + 1 ≤26 we obtain max i∈L(q,T (R))\\̺ |Ri|≤| B(q,2i+2) ∩Ci|≤ cm(R)6. The claim follows by replacing max i∈L(q,T (R))\\̺ |Ri|with cm(R)6 in (10). (b) Let us now bound the run-time of #Line[8,17]. which runs Algorithm F .3 for all (p,i), where p ∈Ri. Let Sbe a distinctive descendant set from Deﬁnition 2.8. Algorithm F .3 visits every node u∈∪p∈Ri Si(p,T(R)) once, therefore its running time is O(∪p∈Ri |Si(p,T(R))|). Let us now show that ∪p∈Ri Si(p,T(R)) ⊆ ¯B(q,5dk(q,R)) Note ﬁrst that by Lemma F .5 set ∪p∈Ri Si(p,T(R)) contains all k-nearest neighbors of q. Using Lemma D.15 we ﬁnd β among k-nearest neighbors of qsatisfying d(q,λ) ≤d(q,β) + 2 i+1. From assumption It follows 2i+1 ≤d(q,β) . By line 8 we have d(q,λ) ≤2i+1. By line 13 we perform depth-ﬁrst traversal on A= ∪p∈Ri Si(p,T(R)). Let u ∈∪p∈Ri Si(p,T(R)) be arbitrary node and let v ∈Ri be such that u ∈Si(v,T(R)). By Lemma D.13 we have d(u,v) ≤2i+1. Since v∈Ri we have d(q,v) ≤d(λ,q) + 2 i+2. By triangle inequality d(u,q) ≤d(u,v) + d(v,q) ≤2i+1 + d(λ,v) + 2 i+2 ≤2i+1 + 2i+1 + d(q,β) + 2 i+2 ≤5 ·d(q,β) It follows that ∪p∈Ri Si(p,T(R)) ⊆ ¯B(q,5 ·d(q,β)). Let us now bound the time complexity of line 13. By Lemma D.9 for any set Ais takes log(k) ·|A|time to select k-lowest elements. W e have: #Line[8,17] = O(|¯B(q,5 ·dk(q,R))|·log(k)). 39A new compressed cover tree for k-nearest neighbors Theorem 4.6. Let Rbe a ﬁnite set in a metric space (X,d), cm(R) be the minimized constant from Deﬁnition 1.4. Given a compressed cover tree T(R), Algorithm 4.3 ﬁnds all k-nearest neighbors of a query point q∈X in time O ( log2(k) ·((cm(R))10 ·|L(q,T(R))|+ |¯B(q,5dk(q,R))|) ) , where L(T(R),q) is the set of all performer iterations (lines 4-15 ) of Algori thm 4.3. Proof. Apply Lemma 4.5 to estimate the time complexity of Algorithm F .2: O ( |L(T(R),q)|·(#Line[4 −8] + #Line[14 −17] + #Line[18]) + #Line[8 −14] ) . Corollary 4.7 gives a run-time bound using only minimized ex pansion constant cm(R), where if R⊂Rm, then cm(R) ≤ 2m. Recall that ∆( R) is aspect ratio of Rintroduced in Deﬁnition 1.1. Corollary 4.7. Let Rbe a ﬁnite set in a metric space (X,d). Given a compressed cover tree T(R), Algorithm 4.3 ﬁnds all k-nearest neighbors of qin time O ( (cm(R))10 ·log2(k) ·log2(∆( R)) + |¯B(q,5dk(q,R))|·log2(k) ) . Proof. Replace |L(q,T(R))|in the time complexity of Theorem 4.6 by its upper bound from L emma B.8: |L(q,T(R))|≤ |H(T(R))|≤ log2(∆( R)). If we are allowed to use the standard expansion constant, tha t corresponds to KR-dimension of Krauthgamer & Lee (2004), then we obtain a stronger result, Theorem 4.9. Lemma F .6.Let Rbe a ﬁnite reference set in a metric space (X,d) and let q ∈X be a query point. Let ̺be the special level of L(T(R),q). Let i∈L(T(R),q) \\̺be any level. Then if p∈Ri we have d(p,q) ≤2i+3. Proof. By assumption in this part of the algorithm we have d(q,λ) ≤2i+2. By line 7 of Algorithm F .2, since p∈Ri we have d(p,q) ≤d(q,λ) + 2 i+2 ≤2i+2 + 2i+2 ≤2i+3, which proves the claim. Lemma F .7. Let Rbe a ﬁnite reference set in a metric space (X,d) and let q ∈X be a query point. Let ̺be the special level of L(T(R),q). Let i∈L(T(R),q) \\̺be any level. Then if p∈Ci(Rη(i)) \\Ri, we have d(p,q) >2i+2. Proof. By assumption p ∈Ci(Rη(i)) \\Ri. By line 7 of Algorithm F .2 it follows that d(q,p) > 2i+2 + d(q,λ) ≥2i+2. Therefore d(q,p) >2i+2, which proves the claim. Lemma F .8. Let ibe a non-minimal level of L(T(R),q) of Deﬁnition F .1. Assume that t = η(η(i+ 3)) is deﬁned. Then there exists p∈Rsatisfying 2i+2 <d(p,q) ≤2t+4. Proof. Note ﬁrst that since η(i+ 3) ∈L(T(R),q), there exists distinct u ∈Rη(η(i+3)) and v ∈Cη(i+3)(Rη(η(i+3))), in such a way that uis the parent of v. Let us show that both of u,v cant belong to set Ri. Assume contrary that both u,v ∈Ri. Then by Lemma F .6 we have d(v,q) ≤2i+3 and d(u,q) ≤2i+3. By triangle inequality d(u,v) ≤d(u,q) + d(q,v) ≤ 2i+4 ≤2η(i+3). Recall that we denote a level of a node by l. On the other hand we have l(u) ≥η(i+3) and l(v) ≥η(i+3), by separation condition of Deﬁnition 2.1 we have d(u,v) >2η(i+3), which is a contradiction. Therefore only one of {u,v} can belong to Ri. It sufﬁcies two consider the two cases below: Assume that v /∈Ri. Since vis children of uwe have d(u,v) ≤2η(i+3)+1. By Lemma F .6 we have d(u,q) ≤2η(η(i+3))+3. By triangle inequality d(v,q) ≤d(v,u) + d(u,q) ≤2η(η(i+3))+3 + 2η(i+3)+1 ≤2η(η(i+3))+4 Since v /∈Ri there exists level thaving η(i+ 3) ≥t ≥iand v ∈Ct(Rη(t)) \\Rt. Therefore by Lemma F .7 we have d(q,v) >2t+2 ≥2i+2. It follows that we have found point v ∈Rsatisfying 2i+2 <v ≤2η(η(i+3))+4. Therefore p= v, is the desired point. Assume that u /∈Ri. Since u ∈Rη(η(i+3)), by Lemma F .6 we have d(u,q) ≤2η(η(i+3))+3. On the other hand since u /∈ Ri, there exists level t having η(i + 3) ≥ t ≥ i and u ∈ Ct(Rη(t)) \\Rt. Therefore by Lemma F .7 we have 40A new compressed cover tree for k-nearest neighbors d(q,u) >2t+2 ≥2i+2. It follows that we have found point u∈Rsatisfying 2i+2 <u ≤2η(η(i+3))+4. Therefore p= u, is the desired point. Lemma 4.8. Algorithm 4.3 executes lines 4-15 the following number of ti mes: |L(T(R),q)|= O(c(R∪{q})2 ·log2(|R|)). Proof. Let x∈L(T(R),q) be the lowest level of L(T(R),q). Deﬁne s1 = η(η(x)+1) and let si = η(η(η(si−1 +3))+3) , if it exists. Assume that sn+1 is the last sequence element for which η(η(η(sn−1 + 3)) + 3) is deﬁned. Deﬁne S = {s1,...,s n}. For every i∈{1,...,n }let pi be the point provided by Lemma F .8 that satisﬁes 2si+2 <d(pi,q) ≤2η(η(si+3))+4. Let P be the sequence of points pi. Denote n= |P|= |S|. Let us show that Ssatisﬁes the conditions of Lemma 2.5. Note that: 4 ·d(pi,q) ≤4 ·2η(η(si+3))+4 ≤2η(η(si+3))+6 ≤2η(η(η(si+3))+3)+2 ≤2si+1+2 ≤d(pi+1,q) By Lemma 2.5 applied for A= R∪qand sequence P we get: |¯B(q,4 3 d(q,pn))|≥ (1 + 1 c(R)2 )n ·|¯B(q,1 3d(q,p1))| Since η(x) ∈ L(T(R),q) , there exists some point u ∈ Rη(x). By Lemma F .6 we have d(u,q) ≤ 2η(x)+3. Also 2η(η(x)+1)+1 ≤2η(η(x)+1)+2 3 < d(q,p1) 3 It follows that: 1 ≤| ¯B(q,2η(x)+3)|≤| ¯B(q,2η(η(x)+1) + 1)|≤| ¯B(q,d(q,p1) 3 )| Therefore we have |R|≥ |¯B(q,4 3 ·d(q,pn))| |¯B(q,1 3 ·d(q,p1))|≥(1 + 1 c(R∪{q})2 )n Note that c(R∪{q}) ≥2 by deﬁnition of expansion constant. Then by applying log and by using Lemma B.7 we obtain: c(R∪{q})2 log(|R|) ≥n= |S|. Let xbe minimal level of L(T(R),q) and let ybe the maximal level of L(T(R),q) Note that Sis a sub sequence of Lin such a way that: • [x,s1] ∩L(T(R),q) ≤3, • for all i∈1,...,n we have [si,si+1] ∩L(T(R),q) ≤10 • [sn,y] ∩L(T(R),q) <20 Since segments [x,s1],[s1,s2],..., [s2,sn],[sn,y] cover |L(T(R),q)|, it follows that |S|≥ |L(T (R),q)| 20 . W e obtain that |L(T(R),q)|≤ 20 ·c(R∪{q})2 ·log2(|R|), which proves the claim. Theorem 4.9. Let Rbe a ﬁnite reference set in a metric space (X,d). Let q ∈X be a query point, c(R∪{q}) be the expansion constant of R∪{q}and cm(R) be the minimized expansion constant from Deﬁnition 1.4. Giv en a compressed cover tree T(R), Algorithm 4.3 ﬁnds all k-nearest neighbors of qin time O ( c(R∪{q})2 ·log2(k)· ( (cm(R))10 ·log2(|R|)+ c(R∪{q}) ·k ) ) . Proof. By Theorem 4.6 the required time complexity is O ( (cm(R))10 ·log2(k) ·|L(q,T(R))|+ |¯B(q,5d(q,β))|·log2(k) ) 41A new compressed cover tree for k-nearest neighbors for some point βamong the ﬁrst k-nearest neighbors of q. Apply Deﬁnition 1.4: |B(q,5d(q,β))|≤ (c(R∪{q}))3 ·|B(q,5 8 d(q,β))| (14) Since |B(q,5 8 d(q,β))|≤ k, we have |B(q,5d(q,β))|≤ (c(R∪{q}))3 ·k. It remains to apply Lemma 4.8: |L(q,T(R))|= O(c(R∪{q})2 ·log2 |R|). Corollary F .9 combines Theorem 3.10 with Theorem 4.9, to sho w that Problem 1.3 can be solved in O(cO(1) ·log(k) · max{|Q|,|R|}·(log |R|) + k) time. Corollary F .9 (solution to Problem 1.3) . In the notations of Theorem 4.9, set c = max q∈Q c(R∪{q}). Algorithms E.2 and F .2 solve Problem 1.3 in time O ( max(|Q|,|R|) ·c2 ·log2(k) · ( (cm(R))10 ·log2(|R|) + c·k ) ) . Proof. For any q∈Q, since log2 |R∪{q}|≤ 2 log2 |R|, a tree T(R) can be built in time O(c2 ·cm(R)8 ·log |R|) by Theorem 3.10. Therefore the time complexity is dominated by running Algorithm F .2 on all points q ∈Q. The ﬁnal complexity is obtained by multiplying the time from Theorem 4.9 by |Q|. G. Approximate k-nearest neighbor search The original navigating nets and cover trees were used in Kra uthgamer & Lee (2004, Theorem 2.2) and Beygelzimer et al. (2006a, Section 3.2) to solve the (1 + ǫ)-approximate nearest neighbor problem for k= 1 . The main result, Theorem G.6 justiﬁes a near linear parameterized complexity to ﬁnd appr oximate a k-nearest neighbor set Pformalized in Deﬁnition G.1. Deﬁnition G.1(approximate k-nearest neighbor set P). Let R be a ﬁnite reference set and let Q be a ﬁnite query set of a metric space (X,d). Let q ∈Q ⊆X be a query point, k ≥1 be an integer and ǫ >0 be a real number . Let Nk = ∪k i=1NNi(q) be the union of neighbor sets from Deﬁnition 1.2. A set P⊆ Ris called an approximate k-nearest neighbors set , if |P|= kand there is an injection f : P→N k satisfying d(q,p) ≤(1 + ǫ) ·d(q,f(p)) for all p∈P. ■ Deﬁnition G.3 is analog of Deﬁnition F .1 for (1 + ǫ)-approximate k-nearest neighbor search. Deﬁnition G.3 (Iteration set of approximate k-nearest neighbor search) . Let Rbe a ﬁnite subset of a metric space (X,d). Let T(R) be a cover tree of Deﬁnition 2.1 built on Rand let q ∈X be an arbitrary point. Let L(T(R),q) ⊆H(T(R)) be the set of all levels i during iterations of lines 3-19 of Algorithm G.2 launched wi th inputs (T(R),q). W e denote η(i) = min t{t∈L(T(R),q) |t>i }. ■ Lemma G.4 (k-nearest neighbors in the candidate set for all i). Let Rbe a ﬁnite subset of an ambient metric space (X,d), let q ∈X be a query point , let k ∈Z ∩[1,∞) and ǫ ∈R+ be parameters. Let T(R) be a compressed cover tree of R. Assume that |R|≥ k. Then for any iteration i ∈L(T(R),q) of Algorithm G.2 the candidate set ⋃ p∈Ri Si(p,T(R)) contains all k-nearest neighbors of q. ■ Proof. Proof of this lemma is similar to Lemma G.4 and is therefore om itted. Lemma G.5 shows that Algorithm G.2 correctly returns an Appr oximate k-nearest neighbor set of Deﬁnition G.1. Lemma G.5 (Correctness of Algorithm G.2) . Algorithm G.2 ﬁnds an approximate k-nearest neighbors set of any query point q∈X. ■ Proof. Assume ﬁrst that condition on line 7 of Algorithm G.2 is satis ﬁed during some iteration i∈H(T(R)) of Algorithm G.2. Let us denote A= ⋃ p∈Ci(Rη(i)) {Si(p,T(R)) |d(p,q) <d(q,λ)},B= ⋃ p∈Ci(Rη(i)) {Si(p,T(R)) |d(p,q) = d(q,λ)}. 42A new compressed cover tree for k-nearest neighbors Algorithm G.2 This algorithm ﬁnds approximate k-nearest neighbor of Deﬁnition G.1. 1: Input : compressed cover tree T(R), a query point q∈X, an integer k∈Z+, real ǫ∈R+. 2: Set i←lmax(T(R)) −1 and η(lmax −1) = lmax. Set Rlmax = {root(T(R))}. 3: while i≥lmin do 4: Assign Ci(Rη(i)) ←Rη(i) ∪{a∈Children(p) for some p∈Rη(i) |l(a) = i}. 5: Compute λ= λk(q,Ci(Rη(i))) from Deﬁnition D.6 by Algorithm D.8. 6: Find Ri = {p∈Ci(Rη(i)) |d(q,p) ≤d(q,λ) + 2 i+2}. 7: if 2i+2 ǫ + 2i+1 ≤d(q,λ) then 8: Let P= ∅. 9: for p∈Ci(Rη(i)) do 10: if d(p,q) <d(q,λ) then 11: P= P∪S i(p,T(R)) 12: end if 13: end for 14: Fill Puntil it has kpoints by adding points from sets Si(p,T(R)), where d(p,q) = d(q,λ). 15: return P. 16: end if 17: Set j ←maxa∈Ri Next(a,i, T(R)). {If such jis undeﬁned, we set j = lmin −1} 18: Set η(j) ←iand i←j. 19: end while 20: Compute and output k-nearest neighbors of query point qfrom the set Rlmin . By Algorithm G.2 set Pcontains all points of Aand rest of the points are ﬁlled form B. W e will now form f : P→N k by mapping every point p∈A∩P into itself and then by extending f to be injective map on whole set P. The claim holds trivially for all points p∈A∩P . Let us now consider points p∈P\\A . Let γ ∈Ci(Rη(i)) be such that p∈Si(γ,T(R)) and let ψ ∈ Ci(Rη(i)) be such that f(p) ∈ Si(ψ,T(R)). By using triangle inequality, Lemma B.6 and the fact that p∈A∪B we obtain: d(q,p) ≤d(q,γ) + d(γ,p) ≤d(q,λ) + 2 i+1 (15) On the other hand since f(p) /∈A we have (1 + ǫ) ·d(q,f(p)) ≥(1 + ǫ) ·(d(q,ψ) −d(ψ,f(p))) ≥(1 + ǫ) ·(d(q,λ) −2i+1) (16) Note that by line 7 we have 2i+2 ǫ + 2i+1 ≤d(q,λ). It follows that 2i+2 ≤ǫ·d(q,λ) −ǫ·2i+1. Therefore we have: d(q,λ) + 2 i+1 ≤d(q,λ) + 2 i+2 −2i+1 ≤(1 + ǫ) ·(d(q,λ) −2i+1) (17) By combining Equations (15) - (17) we obtain d(q,p) ≤(1 + ǫ) ·d(q,f(p)). If the condition on line 7 of Algorithm G.2 is never satisﬁed, then the Algorithm ﬁnds real k-nearest neighbors of point qin the end of the algorithm and therefore the claim holds. Theorem G.6 (Time complexity of Algorithm G.2 ) . In the notations of Deﬁnition G.1, the complexity of Algorit hm G.2 is O ( (cm(R))8+⌈log(2+ 1 ǫ )⌉ ·log2(k) ·log2(∆( R)) + k ) . ■ Proof. Similarly to Lemma 4.5 it can be shown that Algorithm G.2 is bo unded by: O((cm(R))4 ·log2(k) ·max i |Ri|·|H(T(R))|+ #Line[7 −16]) (18) Note ﬁrst that in lines 7 - 16 we loop over set Ci(Rη(i)) and select k points from it. Therefore #Line[7 −16] = k+ |Ci(Rη(i))|. 43A new compressed cover tree for k-nearest neighbors Let us now bound the size of Ri. By line 7 of Algorithm G.2 either Algorithm G.2 is launched t hat terminates the program or 2i+2 ǫ + 2 i+1 > d(q,λ). Let Ci be the ith cover set of T(R). T o bound |Ri|we can assume the latter. Similarly to Theorem 4.9 we have: Ri = {r∈Ci(Rη(i)) |d(p,q) ≤d(q,λ) + 2 i+2} (19) = ¯B(q,d(q,λ) + 2 i+2) ∩Ci(Rη(i)) (20) ⊆ ¯B(q,d(q,λ) + 2 i+2) ∩Ci (21) ⊆ ¯B(q,2i+2(3 2 + 1 ǫ)) ∩Ci (22) Since the cover set Ci is a 2i-sparse subset of the ambient metric space X, we can apply Lemma 2.2 with t= 2 i+2(3 2 + 1 ǫ ) and δ= 2 i. Since 4 t δ + 1 = 2 4(3 2 + 1 ǫ ) + 1 ≤24(2 + 1 ǫ ), we get max |Ri|≤ (cm(R))4+⌈log2(2+ 1 ǫ )⌉. The ﬁnal complexity is obtained by plugging the upper bound of |Ri|above into (18). Corollary G.7 (complexity for approximate k-nearest neighbors set P). In the notations of Deﬁnition G.1, an approximate k-nearest neighbors set is found for all q∈Qin time O ( |Q|·(cm(R))8+⌈log(2+ 1 ǫ )⌉ ·log(k) ·log2(∆( R)) + |Q|·k ) . ■ Proof. This corollary follows directly from Theorem G.6 . H. Discussions: current contributions and future steps This paper rigorously proved the time complexity of the exac t k-nearest neighbor search. The motivations were the past gaps in the proofs of time complexities in Beygelzimer e t al. (2006a, Theorem 5), Ram et al. (2009, Theorem 3.1), March et al. (2010, Theorem 5.1). Though Elkin & Kurlin (2022 a) provided concrete counterexamples, no corrections were published. Main Theorem 4.9 and Corollary 3.11 have ﬁna lly ﬁlled the above gaps. T o overcome all past obstacles, ﬁrst Deﬁnition 1.2 and Probl em 1.3 rigorously dealt with a potential ambiguity of k-nearest neighbors at equal distances, which was not discussed in the past work. A new compressed cover tree in Deﬁnition 2.1 substantially s impliﬁed the navigating nets Krauthgamer & Lee (2004) and original cover trees Beygelzimer et al. (2006a) by avoiding any repetitions of given data points. This compression has substantially clariﬁed the construction and search Algori thms E.2 and F .2. Second, section C showed that the new minimized expansion co nstant cm of any ﬁnite subset Rof a normed vector space Rn has the upper bound 2m. In the future, it can be similarly shown that if R is uniformly distributed then classical expansion constant c(R) is 2m as well. Third, sections E and F corrected the approach of Beygelzime r et al. (2006a) as follows. Assuming that expansion constan ts and aspect ratio of a reference set Rare ﬁxed, Corollaries 3.11 and F .9 rigorously showed that th e times are linear in the maximum size of R,Q and near-linear O(klog k) in the number kof neighbors. The future problem is to improve the complexity of k-nearest neighbor search to a pure linear time O(c(R)O(1)|R|by using cover trees on both sets Q,R. Since a similar approach Ram et al. (2009) was shown to have i ncorrect proof in Elkin & Kurlin (2022a, Counterexample 6.5) and Curtin et al. (2015); Elkin & Kurlin (2022b) used additional parameters I,θ, this goal will require signiﬁcantly more effort to underst and if O(c(R)O(1)|R|) is achievable by using a compressed cover tree. Corollary F .9 allowed us to justify the near-linear time of generically complete PDD Widdowson & Kurlin (2021) invarian ts (Pointwise Distance Distributions), which recently disti nguished all (more than 660 thousand) periodic crystals in t he world’s largest database of real materials Widdowson et al. (2022). Due to these ultra-fast invariants, more than 200 bi llion pairwise comparisons were completed over two days on a modes t desktop while past tools were estimated to require over 34 thousand years Widdowson & Kurlin (2022). The huge sp eed of PDD is complemented by slower but provably complete invariant isosets Anosova & Kurlin (2021) with con tinuous metrics that allow polynomial-time approximation s Anosova & Kurlin (2022). 44A new compressed cover tree for k-nearest neighbors For the purpose of open access, the authors applied a Creativ e Commons Attribution (CC BY) licence to any accepted version. 45",
      "meta_data": {
        "arxiv_id": "2111.15478v5",
        "authors": [
          "Yury Elkin",
          "Vitaliy Kurlin"
        ],
        "published_date": "2021-11-30T15:14:07Z",
        "pdf_url": "https://arxiv.org/pdf/2111.15478v5.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the problem of finding k-nearest neighbors (kNN) for all query points in a given reference set within a metric space in near-linear time. The main contribution is the introduction of a new, simpler compressed cover tree and the rigorous correction and generalization of existing (and previously unproven or incorrectly proven) time complexity bounds for both constructing this tree and performing kNN searches. Specifically, it provides new theoretical guarantees for building a compressed cover tree in O((cm(R))^8 * log^2(Delta(R)) * |R|) or O((cm(R))^8 * c(R)^2 * log^2(|R|)) * |R|) time, and for kNN search in O(log^2(k) * ((cm(R))^10 * log^2(Delta(R)) + |B(q, 5dk(q,R))|) * log^2(k))) or O(c(R union {q})^2 * log^2(k) * ((cm(R))^10 * log^2(|R|) + c(R union {q}) * k))) time. It also defines a minimized expansion constant cm(R) and proves it is at most 2^n in R^n, and clarifies the ambiguity of kNN at equal distances.",
        "methodology": "The core methodology involves defining and utilizing a new data structure called a 'compressed cover tree' (Definition 2.1), which simplifies previous 'implicit' and 'explicit' cover trees by ensuring each data point appears only once, improving memory efficiency. The paper introduces two new algorithms: Algorithm 3.4 for constructing the compressed cover tree and Algorithm 4.3 for performing k-nearest neighbor searches. Algorithm 3.4 builds the tree incrementally using an `AddPoint` procedure. Algorithm 4.3 improves upon prior work by introducing a 'lambda-point' (Definition 4.1) for generalized kNN search and a new loop-break condition to accurately bound the number of iterations. The theoretical analysis relies on concepts such as expansion constants (c(R), cm(R)), aspect ratio (Delta(R)), packing lemmas, and extended growth bounds.",
        "experimental_setup": "The paper is primarily theoretical, focusing on mathematical proofs and corrections of time complexity bounds. It does not present new experimental evaluations, benchmarks, or specific datasets for empirical validation. It mentions that the MLpack library implemented a version of an explicit cover tree and states that the new theoretical results justify the near-linear time complexity of these existing implementations.",
        "limitations": "The primary 'limitation' addressed by this paper is the existence of substantial gaps and incorrect proofs in the time complexity analysis of highly cited previous works on cover trees (Beygelzimer et al. 2006a, Ram et al. 2009, March et al. 2010). While the proposed algorithms achieve near-linear time complexity in the sizes of input sets, the hidden constants in the complexity bounds can still depend on the dimensionality or point distributions of the given sets R and Q via expansion constants (c(R) and cm(R)) and aspect ratio (Delta(R)). The paper's scope is strictly theoretical, lacking new empirical validation of the proposed algorithms.",
        "future_research_directions": "The main future research direction is to further improve the complexity of the k-nearest neighbor search to a purely linear time, O(c(R)^O(1) * |R|), without any other extra hidden parameters. This would involve using a new compressed cover tree on both the query set (Q) and the reference set (R). The authors acknowledge that achieving this goal will require significantly more effort, given that similar past approaches have been shown to have incorrect proofs or relied on additional parameters."
      }
    },
    {
      "title": "Coresets for Decision Trees of Signals",
      "abstract": "A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix\n(2D-signal) into $k\\geq 1$ block matrices (axis-parallel rectangles, leaves)\nwhere each rectangle is assigned a real label. Its regression or classification\nloss to a given matrix $D$ of $N$ entries (labels) is the sum of squared\ndifferences over every label in $D$ and its assigned label by $t$. Given an\nerror parameter $\\varepsilon\\in(0,1)$, a $(k,\\varepsilon)$-coreset $C$ of $D$\nis a small summarization that provably approximates this loss to \\emph{every}\nsuch tree, up to a multiplicative factor of $1\\pm\\varepsilon$. In particular,\nthe optimal $k$-tree of $C$ is a $(1+\\varepsilon)$-approximation to the optimal\n$k$-tree of $D$.\n  We provide the first algorithm that outputs such a $(k,\\varepsilon)$-coreset\nfor \\emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial\nin $k\\log(N)/\\varepsilon$, and its construction takes $O(Nk)$ time. This is by\nforging a link between decision trees from machine learning -- to partition\ntrees in computational geometry.\n  Experimental results on \\texttt{sklearn} and \\texttt{lightGBM} show that\napplying our coresets on real-world data-sets boosts the computation time of\nrandom forests and their parameter tuning by up to x$10$, while keeping similar\naccuracy. Full open source code is provided.",
      "full_text": "Coresets for Decision Trees of Signals Ibrahim Jubran, Ernesto Sanches, Ilan Newman, Dan Feldman University of Haifa, Israel {ibrahim.jub, ernestosanches, dannyf.post}@gmail.com {ilan}@cs.haifa.ac.il Abstract A k-decision tree t(or k-tree) is a recursive partition of a matrix (2D-signal) into k ≥1 block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classiﬁcation loss to a given matrix Dof N entries (labels) is the sum of squared differences over every label inDand its assigned label by t. Given an error parameter ε∈(0,1), a (k,ε)-coreset Cof D is a small summarization that provably approximates this loss to every such tree, up to a multiplicative factor of 1 ±ε. In particular, the optimal k-tree of C is a (1 + ε)-approximation to the optimal k-tree of D. We provide the ﬁrst algorithm that outputs such a (k,ε)-coreset for every such matrix D. The size |C|of the coreset is polynomial in klog(N)/ε, and its con- struction takes O(Nk) time. This is by forging a link between decision trees from machine learning – to partition trees in computational geometry. Experimental results on sklearn and lightGBM show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x10, while keeping similar accuracy. Full open source code is provided. 1 Introduction Decision trees are one of the most common algorithms used in machine learning today, both in the academy and industry, for classiﬁcation and regression problems [ 52]. Informally, a decision tree is a recursive binary partition of the input feature space into hyper-rectangles, where each such hyper-rectangle is assigned a label. If the labels are given from a discrete set, the trees are usually called classiﬁcation trees, and otherwise they are usually called regression trees. Variants include non-binary partitions and forests [29]. Why decision trees? Advantages of decision trees, especially compared to deep learning, include: (i) Interpretability. They are among the most popular algorithms for interpretable (transparent) machine learning [31]. (ii) Usually require small memory space, which also implies fast classiﬁcation time. (iii) Accuracy. Decision trees are considered as one of the few competitors of deep networks. In competitions, such as the ones in Kaggle [36], they are one of the favorite classiﬁers [10], especially on small or traditional tabular data. (iv) May learn from small training data. The goal is usually to compute the optimal k-tree t∗for a given dataset Dand a given number kof leaves, according to some given loss function. In practice, researchers usually use ensemble of trees called forests, e.g., a Random Forest [12], which are usually learned from different subsets of the training set. The ﬁnal classiﬁcation is then based on a combination rule, such as majority or average vote. Since both the training and classiﬁcation of each tree are computed independently and possibly in parallel, we focus on the construction of a single tree. A dataset Din this paper is a set D= {(x1,y1),··· ,(xN,yN)}⊆ A×R of pairs, where Ais the feature space. Each pair (x,y) ∈Dconsists of a database record (vector / sample) x∈Aand its real label y ∈R. As common, we assume that non-real features, such as categorical features, are arXiv:2110.03195v1  [cs.LG]  7 Oct 2021Figure 1: (Left): A one dimensional signal (orange points) and its segmentation into 25 “smooth” segments / leaves (green lines). Image taken from Section 1.10 (“Decision Trees”) of the sklearn’s User Guide [56]. The vector von top represents a subset of the signal’s values. The bottom vectors represent a 4-segmentation of v, similar to the horizontal green line segments. Each segment contains the average value of its corresponding segment from v. (Middle): A matrix that represents the 4 ×5 signal D = {((1,1),3),((1,2),4),((1,3),5),···} (in black) and a 3 ×2 matrix that represents a 3 ×2 sub-signal B = {((1,2),4),((1,3),4),((2,2),3),···} (in purple). (Right): A matrix that represents a 5-segmentation sof A = [4] ×[5]; see Deﬁnition 1. Since sis a 5-segmentation, it partitions the [4] ×[5] matrix into 5 distinct block matrices B1 (red), B2 (blue), B3 (yellow), B4 (green), and B5 (black) such that sassigns the same number for all the entries in the same block. The SSE ﬁtting loss ℓ(D,s) is the sum of squared differences over every entry in the left matrix to its corresponding entry on the right matrix. Also, there is no k-tree that can obtain the same partition. converted to real numbers; see e.g. [27]. For example, in common classiﬁcation problems A= Rd for some d≥1 and y∈{0,1}is a binary number. The resulting model may be used for prediction on another test dataset, completion of missing values, or efﬁcient storage of the original dataset by replacing the label yof each pair (x,y) ∈Dwith the label t(x) that was assigned to it by the tree t. The last technique is used e.g. in the MPEG4 encoder [ 46], where decision trees of a speciﬁc structure (quad-trees) are used to compress an image Dthat consists of pixel-grayscale pairs (x,y). Challenges. The motivation for this paper originated from the following challenges: (i) Sub-optimality. Hardness of decision tree optimization is both a theoretical and practical obsta- cle [31]. It is NP-hard to compute the optimal k-tree, or its approximation, when the number kis not ﬁxed [14, 39]. There were several attempts to improve the optimality of decision tree algorithms, from binary-split decision trees as in [6, 8], in a line of work of e.g. [9, 61]. Nevertheless, greedy implementations e.g., CART [40] and C4.5 [51] have remained the dominant methods in practice. (ii) Computation time. Due to this lack of optimality, ﬁnding a decision tree that provides a good accuracy usually requires many runs, since each of them returns only a local minimum that might be arbitrarily far from the global optimum. The ﬁnal model usually consists of a forest containing many trees. Popular forest implementations include the famous Sklearn, XGBoost, LightGBM, and CatBoost libraries [49, 15, 37, 19], which all utilize (as default) an ensemble of at least 100 trees. Moreover, there is a list of dozen parameters to calibrate including: number of trees, depth of each tree, pruning/splitting strategies on each tree and between them, and many others. To this end, the running time for obtaining reasonable results even on moderate size datasets might be impractical. (iii) Scalability. Existing techniques tend not to scale to realistically-sized problems unless simpliﬁed to trees of a speciﬁc form as stated in [31]. (iv) Streaming, parallel, and dynamic updates. The common algorithms mentioned above do not support continuous learning or updating of the modeled tree when an input sample is either added or removed from the dataset, e.g., when the dataset does not ﬁt into memory or arrives on-the-ﬂy. Similarly, we do not know techniques to train a single tree in parallel on multiple machines. 1.1 Coresets “Coresets are one of the central methods to facilitate the analysis of large data sets.” [47]. Informally, for an input dataset D, a set T of models, an approximation error ε ∈(0,1), and a loss function ℓ, a coreset C is a data structure that approximates the loss ℓ(D,t) for every model t ∈T, up to a multiplicative factor of 1 ±ε, in time that depends only on |C|. Hence, ideally, C is also much smaller than ther original input D. Why coresets? The main motivation for constructing a coreset is to compute the optimal model or its approximation, much faster, while sacriﬁcing little accuracy. Furthermore, a coreset for a family of classiﬁers is many times a “silver bullet” that provides a uniﬁed solution to all Challenges (i)-(iv) above. Combining the two main coreset properties: merge and reduce [32, 7, 26, 1], which 2are usually satisﬁed, with the fact that a coreset approximates every model, and not just the optimal model, enables it to support streaming and distributed data [11, 41], parallel computation [21], handle constrained versions of the problem [25], model compression [20], parameter tuning [43] and more. Coreset construction techniques. There are many different techniques for coreset construction, ranging from loss-less to lossy, from deterministic to randomized, and from greedy to non-greedy con- structions. Examples include accurate coresets [33] and non-accurate coresets [23] via computational geometry, random sampling-based coresets [22, 17, 44], and greedy deterministic coresets via the Frank-Wolfe algorithm [16]. Recently, many works focus on developing frameworks for general fam- ilies of loss functions, e.g., [22, 57]. We refer the interested reader to the surveys [2, 1, 50, 11, 4, 21] with references therein. Practical usage. Since a coreset is not just another solver that competes with existing solutions, but a data structure for approximating any given model in the family, we can apply existing approximation algorithms or heuristics on the coreset to obtain similar results compared to the original (full) data. Since the coreset is small, we may run these heuristics multiple times, or apply the hyperparameter tuning using the coreset [43], thus reducing the computational burden by orders of magnitude. Main challenges: (i) No coreset. Unfortunately, it is not clear at all that a small coreset exists for a given family of models. In fact, we can conclude from [54] that a coreset for decision trees does not exist in general; see details below. In this case, we can either give up on the coreset paradigm and develop a new solver, or add assumption on the input dataset, instead of targeting every possible dataset that may be very artiﬁcial and unrealistic, as the counter example in [54]. In this paper, we choose the latter option. (ii) Unlike, say, uniform sampling, every problem formulation requires a different coreset construction, which may take years of research to design. 1.2 First coreset for decision trees and their generalization In this paper, we tackle a generalized and more complex set of models than decision trees, where, rather than a recursive binary partition, we allow the input feature space Rd to be partitioned into any kdisjoint axis-parallel hyper-rectangles; see Fig. 1. This generalization is essential in order to support future non-recursive and not necessarily binary classiﬁcation models, e.g., ID3 and C4.5 [30, 51]. To our knowledge, this is the ﬁrst coreset with provable guarantees for constructing decision trees. Deﬁnition 1 (k-segmentation). For an integerd≥1 that denotes the dimension of the feature space A= Rd, and an integer k≥1 that denotes the size of the partition (number of leaves), a function s : A →R is a k-segmentation if there is a partition B= {B1,··· ,Bk}of A into k disjoint axis-parallel hyper-rectangles (blocks), such that |{s(b) |b∈B}|= 1 for every block B ∈B, i.e., sassigns a unique value for all the entries in each of its krectangles; see Fig. 1. We deﬁne the union over all possible such k-segmentations by SEG(k,d). We now deﬁne our loss function, and an optimal k-segmentation model over some set A⊆Rd. Deﬁnition 2 (Loss function). For a dataset D = {(x1,y1),··· ,(xN,yN)}⊆ A×R, an integer k≥1, and a k-segmentation s∈SEG(k,d), we deﬁne the sum of squared error (SSE) loss ℓ(D,s) := ∑ (x,y)∈D (s(x) −y)2 as the loss of ﬁtting sto D. A k-segmentation s∗is an optimal k-segmentation of Dif it minimizes ℓ(D,s) over every k-segmentation s∈SEG(k,d) i.e., s∗∈arg mins∈SEG(k,d) ℓ(D,s). The optimal SSE loss is denoted by optk(D) := ℓ(D,s∗). For example, the optimal 1-segmentation s∗of Dis the constant function s∗≡ 1 |D| ∑ (x,y)∈Dysince the mean of a set of numbers minimizes the sum of squared distances to the elements of the set. Also, opt|D|(D) = 0 for every dataset D. We are now ready to formally deﬁne a coreset for the k-segmentation problem (and k-decision trees of at most kleaves, in particular). Deﬁnition 3 (Coreset). Let D= {(x1,y1),··· ,(xn,yn)}⊆ A×R be an input dataset. Let k≥1 be an integer and ε ∈(0,1) be the desired approximation error. A (k,ε)-coreset for Dis a data structure (C,u) where C ⊆A×R is an ordered set, and u : C →[0,∞) is called a weight 3function, such that (C,u) sufﬁces to approximate the loss ℓ(D,s) of the original dataset D, up to a multiplicative factor of 1 ±ε, in time that depends only on |C|and k, for any k-segmentation s. Practical usage. As deﬁned above and discussed in Section 1.1, a coreset approximates every model in our set of models SEG(k,d). Hence, a coreset for decision trees is clearly also a coreset for forests with an appropriate tuning for k, since every tree in the forest is approximated independently by the coreset. We expect that applying existing heuristics (not necessarily with provable guarantees) such as sklearn [49] or LightGBM [37] on the coreset, would yield similar results compared to the original data. Indeed, our experimental results in Section 5 validate those claims. No coreset for general datasets. Unfortunately, even for the case of k= 4 and A⊆R, i.e., when the input is simply a one dimensional dataset D= {(x1,y1),··· ,(xn,yn)}where x1,··· ,xn are real numbers, and the labels y1,··· ,yn ∈{0,1}have only binary values, it is easy to construct datasets which have provably no k-segmentation (or even k-tree) coreset of size smaller than n; see e.g. [54]. Hence, there is no non-trivial decision tree coreset for general datasets of nvectors in any dimension. However, as we prove in the rest of the paper, a coreset does exist for datasets where the input is a matrix, i.e., a discrite signal where every coordinate in the domain is assigned a label (value), rather than a random set of nvectors. The ﬁrst coreset for n×m-signals. To overcome the above problem, while still obtaining a small coreset, we assume a discretization of the dataset so that every coordinate has a label. We also assume, mainly for simplicity and lack of space, that the input feature space is A= [n] ×[m] ⊆R2. That is, the input can be represented by an n×mmatrix. The output coreset may contain fraction of entries, as in Fig. 3, which is called an n×msignal; see Section 1.5. Our assumption on the input data seems to be the weakest assumption that can enable us to have a provably small coreset for any input. Furthermore, it seems natural for e.g. images, matrices, or any input data from sensors (such as GPS) that has a value in every cell or continuous in some other sense. Previous work. The prior works [ 54, 24, 62], which only handle the case of segmenting a 1- dimensional signal, use relaxations similar to our relaxation above to obtain a coreset of sizeO(k/ε2). However, our results apply easily for the case of vectors ( 1-dimensional signals) as in [ 54] and generalize for tensors if d≥3. We also give further applications, and provide extensive experiments with popular state of the art software. A special case for d = 2 includes image compression, where quadtrees are usually used in e.g. MPEG4 to replace the image by smooth blocks of different sizes [55], or for completion of missing values [58] Using dynamic programming, it is easy to compute the optimal tree of a 2D-signal Din O(k2n5) time [5], which is impractical even for small datasets,unless applied on a small coreset ofD. However we do not know of any such coreset construction, ford≥2, with provable guarantees on its size. To this end, the following questions are the motivation for this paper:(i): Is there a small coreset for any n×msignal (e.g. of sub-linear size)? (ii): If so, can it be computed efﬁciently? (iii): Can it be used on real-world datasets to boost the performance of existing random forest implementations? Extensions. For simplicity, we focus on the classic sum of squared distances (SSE) or the risk minimization model [ 59]. However, our suggested techniques mainly assume that a coreset for the case k = 1 is known, which is trivial for SSE, but exists for many other loss functions e.g., non-squared distances; see Section 6. 1.3 Our Contribution For any given error parameter ε∈(0,1), and an integer k≥1, this paper answers afﬁrmatively the above three questions. More formally, in this paper we provide: (i): A proof that every n ×m signal D has a (k,ε)-coreset (C,u) of size |C|polynomial in klog(nm)/ε. To our knowledge, this is the ﬁrst coreset for decision trees whose size is smaller than the input; see Theorem 8. Due to lack of space, our full proofs are given in the appendix. (ii): A novel coreset construction algorithm that outputs such a coreset (C,u) with the above guarantees, for every given input signal D. Its running time is O(nmk), i.e., linear in the input size |D|. Unlike common coreset constructions, our algorithm is deterministic; see Algorithm 3. (iii): Experimental results that apply modern solvers, such as the sklearn and LightGBM libraries, on this coreset for real-world public datasets. We measured the trade-off between the empirical accuracy of the resulting forests, and the coreset size; see Section 5. (iv): AutoML for decision trees: since the suggested coreset approximates every tree of at most k 4leaves, we may use the same coreset for hyperparameter tuning. We demonstrate this in Section 5, by calibrating the parameter kusing only the coreset, as compared to using the original (big) data. (v): Open source code for our algorithms [35]. We expect that it will be used by both the academic and industry communities, not only to improve the running time of existing projects, but also to extend the algorithms and experimental results to other libraries and cost functions; see Section 6. 1.4 Novel technique: partition trees meet decision trees In a seminal paper [28] during the 80’s of the previous century, Haussler and Welz introduced the importance of VC-dimension by Vapnik–Chervonenkis [60]. Their main application was partition trees for answering range queries. Informally, a partition tree of a given set of points on the plane is the result of computing recursively a simplicial partition which is deﬁned as follows. For a set Dof N points on the plane, a (1/ε)- simplicial partition is the partition of D into O(1/ε) subsets, such that: (i) each subset has at most 2εN points, and (ii) Every line in the plane intersects the convex hull of at most √ 1/εsets. Answering range queries of the form “how many points in Dare in a given rectangular” in sub-linear time, using partition trees, is straightforward: We can sum in O(1/ε) time the number of points in the subsets of the above simplicial partition that are not intersected by the query rectangular. We then continue recursively to count the points on each of the √ 1/εintersected sets. In other words, the main idea behind the above work is to partition the input into a (relatively small) number of subsets, each containing a fraction of the input, such that each query (in this case, a rectangular shape) might intersect only a small fraction of those subsets. Such a partition is termed a simplicial partition. The number of points contained in non-intersected subsets can be easily computed, while the sum of points in intersected subsets require a more involved solution. The novelty in that work is how to achieve such a partition of the input. Our paper closes a loop in the sense that it forges links between decision trees in machine learning – to partition trees from computational geometry. We aim to generalize the above technique from covering problems to regression and classiﬁcation problems. This is by devising an algorithm which achieves the above requirements, but where the query is a decision tree (and not a rectangular shape), and the cost function is the sum of squared distances to the query and not the number of points. More precisely, We partition the input datasetDinto a relatively small number of subsets, such that every possible decision tree (query) intersects at most few of these subsets. We then independently compress every subset via another novel algorithm such that the cost (sum of squared distances, in this case) of points contained in non-intersected subsets can be easily and accurately estimated, while the cost of points in intersected subsets can be provably approximated via a more involved calculation. This is very different from existing coreset techniques that are sampling-based [38], or utilize convex optimization greedy algorithms [16]. Our main challenge was to deﬁne and design such a “simplicial partition for sum of squared distances”, and the coreset to be computed for each subset in this partition. 1.5 Preliminaries In this section we deﬁne the notation that will be used in the next sections. Let n ≥ 1 and denote [n] = {1,··· ,n}. An n-signal is a set {(x,f(x)) | x ∈ [n]}that is deﬁned by a function f : [n] →R (known as the graph of f). For an additional integer m ≥1, an n×m signal D = {(x,g(x)) | x ∈ [n] ×[m]}is the set that corresponds to a function g : [n] ×[m] →R. That is, D represents an n×mreal matrix whose size is |D|= N = nm. For integers i1,i2,j1,j2 such that 1 ≤i1 ≤i2 ≤nand 1 ≤j1 ≤j2 ≤m, an n×msub-signal is the set B = { (x,g(x)) |x ∈{i1,··· ,i2}×{ j1,··· ,j2} } ⊆D; see Fig. 1. A sub-signal B is called a row (respectively, column) if i1 = i2 (respectively, j1 = j2). For a sub-signal B, we denote by BT = {((j,i),y) |((i,j),y) ∈B}the transposed sub-signal. A k-segmentation s is said to intersect an n×msub-signal Bif sassigns at least two distinct values to the entries of B, i.e., |{s(x) |(x,y) ∈B}|≥ 2. Furthermore, by deﬁnition, a k-segmentation sinduces a partition of an n×m sub-signal B into at most k n×m sub-signals. For two functions f,g : R →R we use the big Onotation f(x) ∈O(g(x)), thinking of O(g(x)) as the class of all functions h(x) 5such that |h(x)| ≤c|g(x)|for every x > x0, for some constants c and x0. Lastly, we denote SEGk := SEG(k,2) for brevity. Paper organization. Section 2 provides a rough approximation to the k-segmentation problem. Section 3 provides an algorithm for computing a simplicial partition for the k-segmentation problem. Each region in this partition will be then compressed individually in Section 4 to obtain our desired coreset. Experimental results and discussions are given in Section 5, and a conclusion in Section 6. 2 Bi-criteria Approximation A coreset construction usually requires some rough approximation to the optimal solution as its input. Unfortunately, we do not know how toefﬁciently compute even a constant factor approximation to the optimal k-segmentation problem in Deﬁnition 2, as explained in Section 1. Instead, we provide an (α,β)k or bi-criteria approximation [22], where the approximation is with respect to a pair of parameters: the number of segments in the partition may be up to βk instead of k, and the loss may be up to α·optk(D) instead of optk(D). Deﬁnition 4 ((α,β)k-approximation.). Let D be an n×msub-signal, k ≥1 be an integer and let α,β > 1. A function s : [ n] ×[m] → R is an (α,β)k-approximation of D, if s is a βk- segmentation whose ﬁtting loss to Dis at most αtimes the loss of the optimal k-segmentation of D, i.e., s∈SEG(βk) and ℓ(D,s) = ∑ (x,y)∈D(s(x) −y)2 ≤α·optk(D). We now describe an algorithm that computes such an approximation, in time only linear in the input’s size |D|= nm. The following lemma gives the formal statement. A suggested implementation for the algorithm is given in the appendix, as well as the full proof of the lemma; see Section B. Lemma 5. Let D= {(x1,y1),··· ,(xnm,ynm)}be an n×msub-signal and k≥1 be an integer. Then, there is an algorithm that can compute, in O(knm) time, an (α,β)k-approximation for D, where α∈O(klog(nm)) and β ∈kO(1) log2 (nm). Overview of the bicriteria algorithm from Lemma 5: The algorithm is iterative and works as follows. At the ith iteration, we ﬁnd a collection Bi of at most tdisjoint sub-signals in Di (where D0 = Dis the input), for which: (i) ∑ B∈Bi opt1(B) ≤optk(Di) ≤optk(D), and (ii) ∪B∈BiB has size |∪B∈BiB|≥| Di|/cfor some parameter cthat depends on k, i.e., those sub-signals contain at least a 1/cfraction of Di. We then deﬁne Di+1 = Di \\∪B∈BiB. After repeating this for at most ψ∈O(clog(nm)) iterations, we end up covering all entries of Dwith sub-signals where the overall loss of the sub-signals in each iteration is at most optk(D). This deﬁnes a partition of D into a collection of at most tψdisjoint sets B′, which, in turn, deﬁne a set of at most (tψ)2 distinct sub-signals. The output is now simply the function sthat assigns, for every B ∈B′and b∈B, the mean value s(b) = 1 |B| ∑ ((i,j),y)∈Byof B. See Pseudo-code in Algorithm 4 at the appendix. 3 Balanced Partition In this section we present Algorithm 2, which computes a partition similar to the simplicial partition described in Section 1.4; It computes, in O(|D|) time, a partition Bof the input D that satisﬁes the following properties: (i) |B|depends on k/ε but independent of |D|, (ii) the loss opt1(B) of every B ∈B is small, and (iii) every k-segmentation sintersects only few sub-signals B ∈B; see Deﬁnition 6, Fig. 2, and Lemma 7. A full proof is given at the appendix; see Section C. Deﬁnition 6 (Balanced Partition). Let Dbe an n×msignal, k≥1 be an integer, andc1,c2,c3 >0. A (c1,c2,c3)k-balanced partition of Dis a partition Bof Dsuch that: (i) Bcontains |B|≤ c1 n×m sub-signals, (ii) opt1(B) ≤c2 for every B ∈B, and (iii) every k-segmentation ˆsintersects at most c3 sub-signals B ∈B (i.e., assigns more than one unique number to those sub-signals). Lemma 7. Let D be an n×msignal, k ≥1 be an integer, ε ∈(0,1/4) be an error parameter, and s : [n] ×[m] →R be an (α,β)k-approximation of D, where α,β >1. Deﬁne σ := ℓ(D,s) α and γ := ε2 βk. Let Bbe an output of a call to PARTITION (D,γ,σ ); see Algorithm 2. Then Ban( O ( α γ2 ) ,γ2σ,O ( kα γ )) k -balanced partition of D. Moreover, Bcan be computed in O(nm) time. 6Figure 2: (Left): A 6 ×5 signal D consisting of 6 rows R1,··· ,R6. (Middle): A step by step illustration of the call to B := PARTITION (D,1/4,64) which partitions D into |B| = 13 sub- signals (sub-matrices) as follows. (1) B′ := SLICE PARTITION ({R1},4) (top green row). (2) B′:= SLICE PARTITION (R1 ∪R2,4) (middle green matrix), and so on as long as the output contains at most |B′|≤ 1/γ = 4 sub-signals (as in the bottom green matrix (3)). We then append B′to the output B, and repeat with the remaining {R4,R5,R6}. (4) B′:= SLICE PARTITION ({R4},4) which already returns |B′|= 5 >1/γsignals (yellow matrix). We append them to Band repeat. (Right): The ﬁnal partition B, where opt1(B) ≤γ2σ= 1/42 ·64 = 4 for every B ∈B. Overview of Algorithms 1 and 2. Algorithm 2 gets as input an n×m-signal Dand two parameters σ,γ. Algorithm 2 aims to compute a balanced partition of D; see Fig. 2. In turn, it calls Algorithm 1, which takes as input an n×msub-signal Rthat is deﬁned by several contiguous rows of the original dataset D, and a parameter σ >0, and aims to compute a partition Bof R. To do so, Algorithm 1 partitions Ralong the vertical dimension (e.g., into vertical slices), in a greedy fashion, such that for every B ∈B, opt1(B) is as large as possible, while still upper bounded by σ. This will ensure that the partition is into a relatively small number of slices. In the case where one of the sub-signals Bin this vertical partition of Rcontains only one column, and already exceeds the maximum tolerance opt1(B) >σ, we recursively apply Algorithm 1 to BT in order to partition Bhorizontally. As long as the total number of slices returned by Algorithm 1 is smaller than 1/γ, Algorithm 2 adds yet another row to the previous set of rows, and repeats the above process. At this point, the partition of the current horizontal slice (collection of rows) Ris ﬁnal, and is added to the output partition of Algorithm 2. In turn, a new horizontal slice Rof just one row, the ﬁrst row of Dthat is not included in the previous R, is initiated on which we again call Algorithm 1. Algorithm 1: SLICE PARTITION (D,σ) Input : A parameter σ >0 and an n×m signal D= {(xi,yi)}N i=1. Output :A partition Bof D. 1 B:= ∅and cbegin := 1 2 while cbegin ≤mdo 3 B := {((i,j),y) ∈D|j = cbegin} // extract first column 4 if opt1(B) >σ then 5 B′:= SLICE PARTITION (BT,σ) 6 B:= B∪ { B′T |B′∈B′ } cbegin := cbegin + 1 7 else 8 cend := cbegin 9 while opt1(B) ≤σand cend <m do 10 cend := cend + 1 and lastB := B 11 B := {((i,j),y) ∈D|i∈[cbegin,cend]} 12 B:= B∪{lastB} 13 cbegin := cend 14 return B Algorithm 2: PARTITION (D,γ,σ ) Input : An n×msignal D, a parameter γ ∈(0,1), and a lower bound σ∈[0,optk(D)]. Output : A partition Bof D; see Lemma 7 1 B:= ∅and rbegin := 1 2 while rbegin ≤ndo 3 R:= {((i,j),y) ∈D|i= rbegin} // extract first row 4 B′:= SLICE PARTITION (R,γ2σ) 5 rend := rbegin 6 lastB′:= B′ 7 while |B′|≤ 1/γand rend <n do 8 rend := rend + 1 9 lastB′:= B′ 10 S := {((i,j),y) ∈D|i∈[rbegin,rend]} // extract a slice 11 B′= SLICE PARTITION (S,γ2σ) 12 B:= B∪lastB′ 13 rbegin := rend 14 return B 74 Coreset Construction In this section, we present our main algorithm (Algorithm 3), which outputs a (k,ε)-coreset for a given n×msignal D, the number of leaves k≥1, and an approximation error ε∈(0,1). Overview of Algorithm 3: The algorithm ﬁrst utilizes the (α,β)k-approximation from Section 2 to obtain a lower bound σ≤optk(D) for the optimal k-segmentation. It then computes, as described in Section 3, a balanced partition Bof D, where opt1(B) is small and depends on σ, for every B ∈B. Finally, it computes a small representation (CB,uB) for every B ∈B, and returns the union of those representations. Each such pair (CB,uB) satisﬁes: (i) |CB|= 4, and (ii) has the same weighted sum of values, weighted sum of squared values, and sum of weights, as B, i.e.,∑ (a,b)∈CB uB((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B(y|y2 |1); see Fig. 3. Such a representation can be computed using Caratheodory’s theorem, as explained in Section E of the supplementary material. Figure 3: (Left): A matrix representing of a 5 ×5 sub-signal Bwhere yis mapped into unique colors for every (x,y) ∈B. (Middle): A representative (coreset) pair (CB,uB) for Bwhere CB ⊆Bis a (small) subset and uB : CB →[0,∞) is a weight function. That is, the pair (CB,uB) satisﬁes∑ (a,b)∈CB uB((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B(y |y2 |1). (Right): A duplication of the coreset points according to their weight. We call the resulting pair a “smoothed” version of(CB,uB); see more details and formal deﬁnition in Section D of the supplementary material. Some intuition behind Algorithm 3: Consider some k-segmentation s. By the properties of the balanced partition Bof D, only a small number of sub-signals B ∈B are intersected by s, i.e., assigned at least 2 distinct values. For every non-intersected sub-signal B ∈B, the loss ℓ(B,s) is accurately estimated by the (coreset) pair (CB,uB). On the other hand, for every sub-signal B ∈B which is intersected by s, by the guarantees of the representation (CB,uB), the loss ℓ(B,s) will be approximated, using only (CB,uB), up to some small error that depends on opt1(B). However, again by the properties of B, we have that opt1(B) is small. Hence, using the union (C,u) of the representations we can approximate ℓ(D,s) as required. Furthermore, combining that |CB|∈ O(1) for every B ∈B with the fact that |B|is small yields that |C|is indeed small; see Theorem 8. Algorithm 3: SIGNAL -CORESET (D,k,ε ); see Theorem 8 Input : An n×msignal D, an integer k≥1, and an error parameter ε∈(0,1/4). Output :A (k,ε)-coreset (C,u) for D. 1 s:= an (α,β)k approximation of Dfor α∈O(klog(nm)) and β ∈kO(1) log2 (nm) ;see Lemma 5 for suggested implementation. 2 γ := ε2/(βk), σ:= ℓ(D,s) α and C := ∅ 3 B:= PARTITION (D,γ,σ ) // see Algorithm 2. 4 for every set B ∈B do 5 (CB,uB) := a (1,0)-coreset for B, (a zero error coreset for k= 1), such that CB ⊆B, |CB|= 4, and ∑ (a,b)∈CB uB((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B(y|y2 |1) this is done using Caratheodory’s theorem; see Corollary 17 in the appendix. 6 Replace each of the coordinates aof the 4 pairs (a,b) ∈Cwith one of the 4 corner coordinates of the pairs in B; see detailed explanation if the proof of Theorem 8. 7 C := C∪CB and u((a,b)) := uB((a,b)) for every (a,b) ∈CB. 8 return (C,u) Theorem 8 (Coreset). Let D = {(x1,y1),··· ,(xN,yN)}be an n×m signal i.e., N := nm. Let k ≥1 be an integer (that corresponds to the number of leaves/rectangles), and ε ∈(0,1/4) be an error parameter. Let (C,u) be the output of a call to SIGNAL -CORESET (D,k,ε/ ∆) for a 8sufﬁciently large constant ∆ ≥1; see Algorithm 3. Then, (C,u) is a (k,ε)-coreset for Dof size |C|∈ (klog(N))O(1) ε4 ; see Deﬁnition 3. Moreover, (C,u) can be computed in O(kN) time. Coreset size. While Theorem 8 gives a worst-case theoretical upper bound, this bound is too pessimistic in practice, as common in coreset papers [42, 34]. This phenomenon is well known for coresets; see discussion e.g., in [21, 53]. The reasons might include: worst-case artiﬁcial examples vs. average behaviour on structured real-world data, noise removing/smoothing by coresets, the fact that in practice we run heuristics that output a local minima (and not optimal solutions with global minimum), non-tight analysis (especially when it comes to constants), etc. Our experiments in Section 5 show that, empirically, the constructed coresets are signiﬁcantly smaller: for N := nm∼140,000, k= 1000, and ε= 0.2, Theorem 8 predicts, in the worst case, a coreset of size larger than the full dataset size N. However, such an εerror is obtained with a coreset of size at most 1% of the input; see Fig. 4. 5 Experimental Results We implemented our coreset construction from Algorithm 3 in Python 3.7, and in this section we evaluate its empirical results, both on synthetic and real-world datasets. More results are placed in the supplementary material; see Section A. Open-source code can be found in [35]. The hardware used was a standard MSI Prestige 14 laptop with an Intel Core i7-10710U and 16GB of RAM. Since our coreset construction algorithm does not compete with existing solvers, but improves them by reducing their input as a pre-processing step, we apply existing solvers as a black box on the small coreset returned by Algorithm 3. The results show that our coreset can boost, by up to x10 times, the running time and storage cost of common random forest implementations. Implementations for forests. We used the following common implementations: (i) the func- tion RandomForestRegressor from the sklearn.ensemble package, and (ii) the function LGBMRegressor from the lightGBM package that implements a forest of gradient boosted trees. Both functions were used with their default hyperparameters, unless states otherwise. Data summarizations. We consider the following compression schemes: (i): DT-coreset(D,k,ε ) - The implementation based on Algorithm 3. In all experiments we used a constant k= 2000 for computing the coreset, regardless of the (larger) actual kvalue in each test, since k= 2000 was sufﬁcient to obtain a sufﬁciently small empirical approximation error. Hence, the parameter εcontrols the trade-off between size and accuracy. (ii): RandomSample(D,τ) - returns a uniform random sample of size τ from D. In all tests τ was set to the size of the coreset DT-coreset(D,k,ε ) for fair comparison. Datasets. We used the following pair of datasets from the public UCI Machine Learning Reposi- tory [3], each of which was normalized to have zero mean and unit variance for every feature: (i): Air Quality Dataset [18] - contains n= 9358 instances and m= 15 features. (ii) Gesture Phase Segmentation Dataset [45] - contains n= 9900 instances and m= 18 features. The experiment. The goal was to predict missing entries in every given dataset, by training random forests on the available data. The test set (missing values) consists of 30% of the dataset, and was extracted from the input dataset matrix by randomly and uniformly choosing a sufﬁcient number of 5 ×5 patches in the input dataset, and deﬁning them as missing values. The ﬁnal loss of a trained forest is the sum of squared distances between the forest predictions for the missing values, and the ground truth values. To tune the hyperparameter k, we randomly generate a set Kof possible values for kon a logarithmic scale. Then, we either: (i) apply the standard tuning (train the forest on the full data, for each value in K, and pick the one with the smallest test set error), or (ii) compress the input (only once) into a small representative set, and then apply the standard tuning on the small, rather than the full, data. The experiment was repeated 10 times. All the results are averaged over all 10 tests; see Fig. 4. Discussion. While the size and accuracy of our coreset are independent of our exact implementation of Algorithm 3, the running time is heavily based on our naive implementation, as compared to the very efﬁcient professional Python libraries. This explains why most of the running time is still devoted to the coreset construction rather than the forest training. Nevertheless, even our simple implementation yielded improvements of up to x10 in both computational time and storage, for a relatively small accuracy drop of 0.03 in the SSE. Tuning more than one hyperparameter will result in a bigger improvement. Furthermore, Fig. 4 empirically shows that tuning a hyperparameter on the 9Figure 4: Experimental results. (Top): The X-axis is the compression size. For every compression size γ, hyperparameter tuning is applied on both the coreset and the uniform sample (which are both of size γ). A random forest is then trained, on the full data, using those tuned parameters. The Y-axis presents the test set SSE loss of the trained forests. (Bottom left): Hyperparameter tuning. For every different value of k(X-axis), a forest is trained using this parameter value either on the compression (of two different sizes) or on the full data. The Y-axis presents ℓ+ k/105, where ℓis the normal SSE loss of the trained forest on the test set. (Bottom right): Time comparison. The Y-axis presents the total running time of both to compute the compression and to tune the parameter kon the compression (out of 50 different values). Note that the bottom right ﬁgures measure the total time to tune the parameter kin the bottom left ﬁgures, but using many more compression sizes. The optimal obtained parameter was then used to train the random forest in the top ﬁgures. coreset yields a loss curve very similar to the loss curve of tuning on the full data. Lastly, we observe that, in practice, our coresets have size much smaller than predicted in the pessimistic theory. 6 Conclusions and Future Work While coresets for k-trees do not exist in general, we provided an algorithm that computes such a coreset for every input n×msignal. The coreset size depends polynomialy on klog(nm)/εand can be computed in O(nmk) time. Our experimental results on real and synthetic datasets demonstrates how to apply existing forest implementations and tune their hyperparameters on our coreset to boost their running time and storage cost by up to x 10. In practice our coreset works very well also on non-signal datasets, probably since they have “real-world” properties that do not exist in the artiﬁcial worst-case example from Section 1.2. An open problem is to deﬁne these properties. Moreover, while this paper focuses on the sum of squares distances loss, we expect that the results can be generalized to support other loss functions; see Section 1.2. Lastly, supporting high-dimensional data (tensors), instead of matrices, is a straightforward generalization that can be achieved via minor modiﬁcations to our algorithms. Due to space limitation we also leave this to future work. References [1] Pankaj K Agarwal, Graham Cormode, Zengfeng Huang, Jeff M Phillips, Zhewei Wei, and Ke Yi. Mergeable summaries. ACM Transactions on Database Systems (TODS), 38(4):1–28, 2013. [2] Pankaj K Agarwal, Sariel Har-Peled, Kasturi R Varadarajan, et al. Geometric approximation via coresets. Combinatorial and computational geometry, 52:1–30, 2005. [3] Arthur Asuncion and David Newman. Uci machine learning repository, 2007. [4] Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learning. arXiv preprint arXiv:1703.06476, 2017. [5] Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966. 10[6] Kristin P Bennett. Decision tree construction via linear programming. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 1992. [7] Jon Louis Bentley and James B Saxe. Decomposable searching problems i. static-to-dynamic transformation. Journal of Algorithms, 1(4):301–358, 1980. [8] Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees.Machine Learning, 106(7):1039– 1082, 2017. [9] Rafael Blanquero, Emilio Carrizosa, Cristina Molero-Rıo, and Dolores Romero Morales. Opti- mal randomized classiﬁcation trees. In Technical Report, 2018. [10] Casper Solheim Bojer and Jens Peder Meldgaard. Kaggle forecasting competitions: An overlooked learning opportunity. International Journal of Forecasting, 37(2):587–603, 2021. [11] Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for ofﬂine and streaming coreset constructions. arXiv preprint arXiv:1612.00889, 2016. [12] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001. [13] Constantin Carathéodory. Über den variabilitätsbereich der koefﬁzienten von potenzreihen, die gegebene werte nicht annehmen. Mathematische Annalen, 64(1):95–115, 1907. [14] Venkatesan T Chakaravarthy, Vinayaka Pandit, Sambuddha Roy, Pranjal Awasthi, and Mukesh Mohania. Decision trees for entity identiﬁcation: Approximation algorithms and hardness results. In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 53–62, 2007. [15] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794, 2016. [16] Kenneth L Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):1–30, 2010. [17] Michael B Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, pages 181–190, 2015. [18] Saverio De Vito, Ettore Massera, Marco Piga, Luca Martinotto, and Girolamo Di Francia. On ﬁeld calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario. Sensors and Actuators B: Chemical, 129(2):750–757, 2008. [19] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. Catboost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363, 2018. [20] Abhimanyu Dubey, Moitreya Chatterjee, and Narendra Ahuja. Coreset-based neural network compression. In Proceedings of the European Conference on Computer Vision (ECCV), pages 454–470, 2018. [21] Dan Feldman. Core-sets: Updated survey. Sampling Techniques for Supervised or Unsupervised Tasks, pages 23–44, 2020. [22] Dan Feldman and Michael Langberg. A uniﬁed framework for approximating and clustering data. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 569–578, 2011. [23] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant- size coresets for k-means, pca, and projective clustering. SIAM Journal on Computing , 49(3):601–657, 2020. [24] Dan Feldman, Cynthia Sung, and Daniela Rus. The single pixel gps: learning big data signals from tiny coresets. In Proceedings of the 20th international conference on advances in geographic information systems, pages 23–32, 2012. [25] Dan Feldman and Tamir Tassa. More constraints, smaller coresets: Constrained matrix approxi- mation of sparse big data. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 249–258, 2015. [26] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291–300, 2004. 11[27] Melissa A Hardy. Regression with dummy variables, volume 93. Sage, 1993. [28] D Haussler and E Welzl. Epsilon-nets and simplex range queries. In Proceedings of the Second Annual Symposium on Computational Geometry, SCG ’86, page 61–71, New York, NY , USA, 1986. Association for Computing Machinery. [29] Tin Kam Ho. Random decision forests. In Proceedings of 3rd international conference on document analysis and recognition, volume 1, pages 278–282. IEEE, 1995. [30] Badr Hssina, Abdelkarim Merbouha, Hanane Ezzikouri, and Mohammed Erritali. A comparative study of decision tree id3 and c4. 5. International Journal of Advanced Computer Science and Applications, 4(2):13–19, 2014. [31] Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. Advances in Neural Information Processing Systems (NeurIPS), 2019. [32] Piotr Indyk, Sepideh Mahabadi, Mohammad Mahdian, and Vahab S Mirrokni. Composable core-sets for diversity and coverage maximization. In Proceedings of the 33rd ACM SIGMOD- SIGACT-SIGART symposium on Principles of database systems, pages 100–108, 2014. [33] Ibrahim Jubran, Alaa Maalouf, and Dan Feldman. Overview of accurate coresets. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, page e1429, 2021. [34] Ibrahim Jubran, Murad Tukan, Alaa Maalouf, and Dan Feldman. Sets clustering. InInternational Conference on Machine Learning, pages 4994–5005. PMLR, 2020. [35] Jubran, Ibrahim and Sanches, Ernesto and Newman, Ilan and Feldman, Dan. Open source code for the algorithms presented in this paper, 2021. Link for open-source code. [36] Kaggle. Kaggle website. https://www.kaggle.com/. [37] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efﬁcient gradient boosting decision tree. Advances in neural information processing systems, 30:3146–3154, 2017. [38] Michael Langberg and Leonard J Schulman. Universal ε-approximators for integrals. In Proceedings of the twenty-ﬁrst annual ACM-SIAM symposium on Discrete Algorithms, pages 598–607. SIAM, 2010. [39] Hyaﬁl Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete. Information processing letters, 5(1):15–17, 1976. [40] Wei-Yin Loh. Classiﬁcation and regression trees. Wiley interdisciplinary reviews: data mining and knowledge discovery, 1(1):14–23, 2011. [41] Hanlin Lu, Ming-Ju Li, Ting He, Shiqiang Wang, Vijaykrishnan Narayanan, and Kevin S Chan. Robust coreset construction for distributed machine learning. IEEE Journal on Selected Areas in Communications, 38(10):2400–2417, 2020. [42] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture models at scale via coresets. The Journal of Machine Learning Research, 18(1):5885–5909, 2017. [43] Alaa Maalouf, Ibrahim Jubran, and Dan Feldman. Fast and accurate least-mean-squares solvers. In Advances in Neural Information Processing Systems, pages 8305–8316, 2019. [44] Alaa Maalouf, Ibrahim Jubran, Murad Tukan, and Dan Feldman. Faster pac learning and smaller coresets via smoothed analysis. arXiv preprint arXiv:2006.05441, 2020. [45] Renata CB Madeo, Clodoaldo AM Lima, and Sarajane M Peres. Gesture unit segmentation using support vector machines: segmenting gestures from rest positions. In Proceedings of the 28th Annual ACM Symposium on Applied Computing, pages 46–52, 2013. [46] Detlev Marpe, Thomas Wiegand, and Gary J Sullivan. The h. 264/mpeg4 advanced video coding standard and its applications. IEEE communications magazine, 44(8):134–143, 2006. [47] Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff. On coresets for logistic regression. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [48] Soliman Nasser, Ibrahim Jubran, and Dan Feldman. Autonomous toy drone via coresets for pose estimation. Sensors, 20(11):3042, 2020. 12[49] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit- learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. [50] Jeff M Phillips. Coresets and sketches. arXiv preprint arXiv:1601.00617, 2016. [51] J Ross Quinlan. C4. 5: programs for machine learning. Elsevier, 2014. [52] Lior Rokach and Oded Maimon. Decision trees. In Data mining and knowledge discovery handbook, pages 165–192. Springer, 2005. [53] Frédéric Ros and Serge Guillaume. Sampling techniques for supervised or unsupervised tasks. Springer, 2020. [54] Guy Rosman, Mikhail V olkov, Danny Feldman, John W Fisher III, and Daniela Rus. Coresets for k-segmentation of streaming data. In Neural Information Processing Systems Foundation, 2014. [55] Eli Shusterman and Meir Feder. Image compression via improved quadtree decomposition algorithms. IEEE Transactions on Image Processing, 3(2):207–215, 1994. [56] sklearn. sklearn user guide - decision trees. https://scikit-learn.org/stable/ modules/tree.html. [57] Morad Tukan, Alaa Maalouf, and Dan Feldman. Coresets for near-convex functions. Advances in Neural Information Processing Systems, 33, 2020. [58] Bhekisipho Twala. An empirical comparison of techniques for handling incomplete data using decision trees. Applied Artiﬁcial Intelligence, 23(5):373–405, 2009. [59] Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural information processing systems, pages 831–838, 1992. [60] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of complexity, pages 11–30. Springer, 2015. [61] Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees using a binary lin- ear program formulation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 33, pages 1625–1632, 2019. [62] Mikhail V olkov, Guy Rosman, Dan Feldman, John W Fisher, and Daniela Rus. Coresets for visual summarization with applications to loop closure. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 3638–3645. IEEE, 2015. 13Figure 5: The blobs dataset. The dataset D was generated using the function sklearn.datasets.make_blobs, and contains n= 17,000 points clustered into 3 clusters (con- taining 8500,5800, and 2700 points), each with a different label. A coreset (C,u) was constructed using Algorithm 3. From top down, the rows illustrate: (i) The input dataset D, (ii) The balanced partition of D, including the number of sets in the partition (iii) The weighted coreset points. Each point (x,y) ∈Cis plotted at location x, colored according to its label y, and its radius is proportional to its weight u(x,y). The percentage of the coreset size relative to the full data is presented. (iv) The unweighted coreset points. Each point (x,y) ∈C is plotted at location x, colored according to y, and has a ﬁxed radius. The percentage of the coreset size relative to the full data is presented. (v) The partition of the space via a decision tree computed using a call to DecisionTreeRegressor from the sklearn.tree package, where the input was the weighted coreset points only. Each region is assigned a color according to the label assigned to it by the computed tree. (vi) Similar to Row (v), but where the decision tree is trained on the full data. Algorithm 5 was used during training of the decision tree to evaluate the loss of each model. A Additional Experiments In this section, we present some additional experiments conducted using our algorithm from Sec- tions 3-4. We give visual illustration both of our coreset itself and of the result of applying a very common decision tree implementation on the coreset, as compared to running the same function on the original (full) data; see Fig. 5-6. Discussion. Visual representation. As seen in Fig. 5-6, the balanced partition in the second row partitions the input data into multiple subsets, where, as expected, ﬂat and relatively smooth regions are partitioned into a smaller number of large cells, while more complex regions are partitioned into a 14Figure 6: The moons dataset. The dataset was generated using the function sklearn.datasets.make_moons. The dataset contains n = 24 ,000 points spread across two interleaving half circles (12,000 points for each half circle), each with a different label. See caption of Fig. 5 for a detailed explanation about the rows. larger number of ﬁner cells. This is expected since the balanced partition insures a small variance inside each cell. Furthermore, as seen in the third row of the above ﬁgures, the weighted coreset contains a small number of “large” circles (points with large weight) in the ﬂat and relatively smooth regions, while it contains large number of “small” circles (points with small weight) in the more complex regions of the input. Accuracy. As seen in the last two rows of Fig. 5-6, the decision tree trained only on the coreset points resembles the decision tree trained on the full data, even for coresets of size only 6%, 8%, and 14% of the input data, as seen in Fig. 5,6, and 7 respectively. This implies a x10 faster training time of a decision tree (or, similarly, a forest) on a given coreset, compared to training it on the full data, with almost no compromises to the accuracy. The difference in the coreset size required in order to accurately represent the full data depends on the complexity of the input dataset. Indeed, the dataset in Fig. 5 is a much simpler dataset for a decision tree to classify, compared to the dataset in Fig. 7. Hence, the coreset sizes required in Fig. 5 are smaller than the ones in Fig. 7. 15Figure 7: The circles dataset. The dataset was generated using the function sklearn.datasets.make_circles. The dataset contains n= 26,000 points spread across a big circle (14,000 points) and a small circle (12,000 points), each with a different label. See caption of Fig. 5 for a detailed explanation about the rows. B Bi-criteria Approximation Notations. A sub-signal B is said to be horizontally intersected by a k-segmentation function s if there are ((i1,j1),y1),((i2,j2),y2) ∈Bwhere i1 ̸= i2 such that s(i1,j1) ̸= s(i2,j2). Similarly, a block Bof Dis said to be vertically intersected by sif there are ((i1,j1),y1),((i2,j2),y2) ∈B where j1 ̸= j2 such that s(i1,j1) ̸= s(i2,j2). B is said to be intersected by s if B is either horizontally or vertically intersected, i.e., |{s(x) |(x,y) ∈B}|>1. A set of sub-signals Bis said to be horizontally (vertically) intersected by sif it contains a sub-signal B ∈B that is horizontally (vertically) intersected by s. Also, we might abuse notation and denote by signal (sub-signal) an n×msignal (sub-signal) and by k-segmentation an n×mk-segmentation. In this section we give a constructive proof for Lemma 5. A suggested implementation for this constructive proof is given in Algorithm 4. We ﬁrst prove a small technical observation (see Observation 9), and then we prove Lemma 10, which will be used throughout the proof of Lemma 5. Observation 9. Let Aand Bbe two n×msub-signals. Then it holds that opt1(A∪B) ≥opt1(A) + opt1(B). 16Algorithm 4: BICRITERIA (D,k); Lemma 5 Input : An n×msub-signal D= {(xi,yi)}N i=1 and an integer k≥1. Output :An (α,β)k-approximation for D. 1 B:= ∅ 2 ν,γ := sufﬁciently large constants // see proof of Lemma 5 3 while |D|>k log N do 4 if Dcontains a row Rwith |R|≥ |D| νk then 5 Partition [m] into t′= γk intervals [m] = ∪t′ j=1Ij such that every j ∈[t′], the size of each corresponding sub-signal Rj = {((x1,x2),y) ∈R|x2 ∈Ij}is |Rj|∈ { |R| t′ −1,|R| t′ + 1 } . // e.g., by a greedy pass over [m]. 6 B:= the set of t′−2ksignals Rj with the smallest opt1(Rj). 7 else 8 Partition [n] into ψintervals [n] = ∪ψ j=1Ij such that for every j ∈[ψ], the size of each corresponding sub-signal Dj = {((x1,x2),y) ∈D|x1 ∈Ij}is |D| νk ≤|Dj|≤ 2|D| νk . // e.g., by a greedy pass over [n]. 9 if at least ψ/2 of the sub-signals Dj do not contain a column colof size |col|≥ |Dj| 2(νk)2 then 10 Vertically partition each of the (at leastψ/2) sub-signals Dj into ψj sub-signals, each such sub-signal Bof size |Dj| 2(νk)2 ≤|B|≤ |Dj| (νk)2 , and let B′contain the union of all those sub-signals. e.g., via a greedy algorithm. 11 B:= the set of |B′|−4ν2k3 −2kψsignals B ∈|B′|with the smallest opt1(B). 12 else 13 B:= { C |Cis a column of Dj,|C|≥ |Dj| 2(νk)2 and j ∈[ψ] } 14 D:= D\\∪B∈B′ Band B:= B∪B 15 s(b) := 1/|B|∑ (x,y)∈Byfor every b∈Band B ∈B. 16 return s Proof. Let C = A∪B. Let µ= 1 |C| ∑ (x,y)∈C ybe the weighted mean of A∪B. By Deﬁnition of opt we have that opt1(A∪B) = ∑ (x,y)∈C (y−µ)2 = ∑ (x,y)∈A (y−µ)2 + ∑ (x,y)∈B (y−µ)2 ≥opt1(A) + opt1(B), where the ﬁrst derivation holds since the mean of a points minimizes the sum of squared distances to those points, and the last derivation is by the deﬁnition of opt1. Lemma 10. Let D= {(x1,y1),··· ,(xN,yN)}be an n×msub-signal and let k≥1 be an integer. Then, in O(N) time we can ﬁnd a set Bof |B|= t∈O(k3) mutually disjoint blocks with respect to D, for which (i) ∑ B∈Bopt1(B) ≤optk(D). (ii) |∪B∈BB|∈ Ω (N k ) . Proof. Let ν >50 be an arbitrary parameter and let γ ≥8 be a parameter that will be deﬁned later. We will prove Lemma 10 for t≤2ν3k3 and for |∪B∈BB|≥ N 8νk. We start with the simple 1-dimensional case, namely – we assume that m= 1. In this case, we just partition [n] into t′= γk consecutive intervals [n] = ∪t′ 1 Ej, such that each corresponding sub-signal Dj = {((a,b),y) ∈D|a∈Ej}, j ∈[t′] of Dhas equal share of elements in D(up to ±1), i.e., 17|Dj|∈{| D|/t′−1,|D|,|D|/t′+ 1}. This can be done by moving point by point along the elements of D, which are assumed to be sorted in ascending order according toafor every ((a,b),y) ∈D, and deﬁning a new interval at the ﬁrst moment the current interval contains more than |D|/t′elements of Dor just at the prior point. We then compute for each Dj, j∈[t′] the optimal opt1(Dj),and return as output the set Bcontaining the t= t′−2ksub-signals Dj with the smallest loss opt1(Dj), among all the t′sub-signals {D1,··· ,Dt′ }. To see what guarantees we get, note that the computation takes O(nm) = O(N) time to sort the elements ((a,b),y) of Daccording to a, since a∈[nm] is a bounded integer. Afterwards, the above greedy partition also takes O(nm) = O(N) time. Furthermore, |∪B∈BB|≥ (γ−2)k·|D| γk ≥γ−2 γ |D|≥ N 8νk. For γ ≥8 this proves Property (ii) above. Finally, any n×1 k-segmentation function intersects at most 2ksub-signals from {Dj}t′ j=1 (i.e., at most 2k sub-signals are assigned more than 1 distinct value via the k-segmentation function). Hence, the optimal k-segmentation s∗of Dintersects at most 2kof those sub-signals as well. This implies that at least t′−2k≥(γ−2)kof the intervals {Dj}t′ j=1 which are assigned 1 distinct value |{s(x) |(x,y) ∈Dj}|= 1. Hence, by Observation 9, we conclude that optk(D) ≥∑ B∈Bopt1(D) which veriﬁes the 1st item above. We remark that for the 1-dimensional case we can do much better (there is an overall (1 + ε)- approximation of the k-segmentation using logarithmic number of blocks), but this will not be used here. Another remark is that we have ignored the ±1 slack in the sizes above, making the actual part of Dthat is removed at least γ−2 γ |D|−t′. This is insigniﬁcant in the 1-dimensional case above, as for t= O(1) and for |D|≥ log nthis would be an insigniﬁcant fraction, while for smaller D, we can just use single point sub-signals. The 2-dim case : Consider a row R of D, say the i′th row R = {((a,b),y) ∈D|a= i}. We call a row Rof D r-heavy if |R|≥| D|/r, namely – Rcontains at least |D|/relements from D. Analogously, we deﬁne a column to be r-heavy. Assume ﬁrst that our Dcontains a νk-heavy row R. We choose Rand use it as in the 1-dimensional case. As explained above we can ﬁnd in Ra set of disjoint sub-signals Bcontaining γkblocks and for which the ﬁrst item above holds for optk(R) and in particular for optk(D), i.e., ∑ B∈Bopt1(B) ≤ optk(R) ≤optk(D). Further, using the above guarantees for the1-dim case, |∪B∈BB|≥ γ−2 γ ·|R|≥ γ−2 γ ·|D| νk . This proves 2nd item for this case (with γ ≥3). Otherwise, let ei = |Ri|where Ri = {((a,b),y) ∈D|a= i}. By our assumption ei ≤|D|/νk for every i ∈[n]. Our algorithm is essentially identical to the 1-dimensional case, on the 1-dim array L = ( e1,...,e n1 ) where we weight the ith element by ei. Namely, we ﬁnd a partition of Linto ψ contiguous subintervals E= {E1,··· ,Eψ}such that the corresponding sub-signals Dj = {((a,b),y) ∈D|a∈Ej}of Dare as equal as possible. By our assumption this could be done so that for any j ∈ψ, the number of elements in Dj is between |D| νk and at most 2|D| νk . This is since adding a new ‘point’ from the list to an existing interval may increase the sum by at most |D|/νk. this implies that νk/2 ≤ψ≤νk. Next we perform the above algorithm again onD1,··· ,Dψ with the intention to “vertically partition” each such Dj, i.e., split each Dj into sets according to the value bfor every ((a,b),y) ∈Dj (rather than considering the value aabove). Let r= 2ν2k2, we continue with the following case analysis: (i) at least a 1 2 -fraction of {D1,··· ,Dψ}contain no a r-heavy column, and (ii) at most a ψ/2 of {D1,··· ,Dψ}contain no a r-heavy column. Case (i): At least a 1 2 -fraction of {D1,··· ,Dψ}contain no a r-heavy column. Then we partition each set Dj with no r-heavy column into ψi sub-signals { D(1) j ,··· ,D(ψi) j } of nearly equal number of points, where the partition is applied onto the values bof every ((a,b),y) ∈Dj. By a reasoning similar to that above, each r/2 ≤ψi ≤r, and each such block B ∈ { D(1) j ,··· ,D(ψi) j } contains |Di|/r≤|B|≤ 2|Di|/r. Using the bounds on |Di|and rwe get |D| 2ν3k3 ≤|B|≤ 2|D| ν3k3 . In particular 18we conclude that the total number of such sub-signals is at most t′, and t′≤2ν3k3. On the other hand, t′≥ψ 2 |D| νk ·ν3k3 2|D| ≥ν3k3/8. Let B2 be the collection of these sub-signals. We choose for our output collection, B, the set of t′−z sub-signals B ∈B2 with the smallest opt1(B), for z= 2k(r+ ψ) ≤6k3ν2. We note that Bcontains tsub-signals, where t′−z ≤t ≤t′. Further, by the lower bound on tit follows that |∪B∈BB|≥ (t′−z) · |D| 2ν3k3 ≥(ν3k3/8 −6k3ν2) |D| 2ν3k3 ≥|D| 8 (1 −48/ν). This veriﬁes the 2nd item for this case. Finally, we note that any row of D is shared by at most r sub-signals of B, and each column of D is shared by at most ψ sub-signals of B. Hence, any k-segmentation function may intersect at most z = 2k(r+ ψ) sub-signals from B. Therefore, for any k-segmentation sthere are at least t′−zsub-signals in B2 which are not-intersected by s. By our deﬁnition of Bto be the set of t′−zsub-signals in B2 with the smallest loss, we obtain that the loss ℓ(D,s) is at least ∑ B∈Bℓ(B) proving the 1st item of the lemma for this case. Case (ii): At most ψ/2 of {D1,··· ,Dψ}contain no a r-heavy column, namely – at least ψ/2 of the Di have a r-heavy column. In this case we take the heavy column from each Di as its own sub-signal. We get a collection B1 of ψ1 ≥ψ/2 blocks. We now return as output the set Bof the ψ1 −2ksub-signals B ∈B1 with the smallest opt1(B). We note that number of blocks we output in this case is at most ψ≤νk. Note also that Bcontains at least ψ/2 blocks, it follows that |∪B∈BB|) ≥ψ 2 · |D| 2ν2k2 ≥|D| 8νk which proves the 2nd item in the lemma. Finally, note that any k-segmentation scan intersect at most 2kintervals from B1 (similarly to the 1-dim case). Hence, there are at least |B1|−2k= ψ1 −2ksub-signals in B1 that are not-intersected by s, which implies that its loss ℓ(D,s) is at least the sum ∑ B∈Bopt1(B), which proves the 1st item of this lemma. Remark: we did not optimise the parameter. A slightly better partition can be obtains (less blocks), but this is good enough for our purposes. Computational time: Note that the elements of the input Dcan be sorted in lexicographic order in O(nm) = O(N) time since the coordinates aand bfor every ((a,b),y) ∈Dare bounded integers. Then, a linear-time preprocessing can be applied to the input D to store some statistics, e.g., the number of elements in each non-empty row or column, and the index of the next non-empty row or column for every element in D. Afterwards, the above greedy partition also takes O(nm) = O(N) time. We now restate and prove Lemma 5 from Section 2. Lemma 11 (Lemma 5). Let D = {(x1,y1),··· ,(xN,yN)}be an n×m signal and k ≥1 be an integer. Then, in O(kN) time we can compute an (α,β)k-approximation for D, where α ∈ O(klog N) and β ∈O(kO(1) log2 N). The proof of the above claim is a constructive proof. A suggested implementation is provided in Algorithm 4. Proof. The top level idea of the algorithm is as follows. We suggest an iterative algorithm. We start the ﬁrst iteration with D1 = Dand, using Lemma 10, ﬁnd a collection of disjoint sub-signals B1 = {B1,··· ,Bt}in D1 (which will not necessarily cover the entire signal D1), such that: (i) the sum of their 1-segmentation loss satisﬁes, ∑t i=1 opt1(Bi) ≤optk(D1) = opt k(D), and (ii) ∪B∈B1 Bhas size |∪B∈B1 B|≥| D1|/cfor some c(e.g., c∈O(k) in the lemma). Let B1 be such a collection. We then ‘delete’ fromD1 the elements of ∪B∈B1 B, and set D2 = D1 \\∪B∈B1 B. In the ith iteration, we repeat the same process with respect to the current set Di. Namely, we ﬁnd a collection Bi of at most tdisjoint sub-signals in D, for which: (i) ∑ B∈Bi opt1(B) ≤optk(Di) ≤ optk(D), and (ii) ∪B∈BiBhas size |∪B∈BiB|≥| Di|/c, i.e., those blocks cover at least a constant fraction of Di. Repeating these iterative procedure for at most ψ = O(clog(nm)) times, we end up covering all entries of Dwith sub-signals where the overall loss of the sub-signals in each iteration is at most 19optk(D). This deﬁnes a collection of at most tψsub-signals Bthat cover the entire original set D. Hence, the total overall loss over those sub-signals is ∑ B∈B′ OPT1(B) ≤ψoptk(D). By deﬁning the output function ssuch that for every B ∈B and b ∈B, sassigns to bthe mean value of B, i.e., s(b) = 1/|B|∑ ((i,j),y)∈By, we obtain that ℓ(B,s) = opt1(B) for every B ∈B, and that the ℓ(D,s) ≤∑ B∈Bℓ(B,s) ≤ψoptk(D). While for every B ∈B sassigns the same value for every element b∈B, there is not necessarily a partition of n×minto |B|∈ O(tψ) distinct axis-parallel blocks that correspond to the sub-signals of B. Therefore, sis not necessarily a |B|-segmentation function. However, looking at all possible intersections of the sub-signals in B, it is easy to realize that the tψsub-signals in Bdeﬁne a partition of Dinto at most O(t2ψ2) sub-signals B′that indeed correspond to a distinct partition of n×minto |B′|distinct axis-parallel blocks. Hence, sis guaranteed to be a |B′|-segmentation function. The parameters that are guaranteed by the lemma are c ∈O(k), t ∈O(k3). This implies that β = |B′|∈ O(t2ψ2) = O(k8 log2 nm) and α= ψ∈O(klog nm). Computational time: By Lemma 10, each of the ψiterations above takes time linear in the input size. The input size in the i’th iteration isO(N((k−1)/k)i) since at each iteration we remove at least a 1/kfraction of the input. Hence, the total running time is the sum of the geometric series N ·∑ i∈ψ((k−1)/k)i ∈O(kN). C Balanced Partition In this section we give our full proof for Lemma 7. We ﬁrst prove the following lemma regarding the output of Algorithm 1. Lemma 12. Let Dbe an n×msub-signal, and σ >0 be a parameter. Let B= { B1,··· ,B|B| } be the output of a call to SLICE PARTITION (D,σ), where the sub-signals in Bare numbered according to the order in which each of them was added to B; see Algorithm 1. Then the following properties hold: (i) Bis a partition of D. (ii) opt1(B) ≤σfor every sub-signal B ∈B. (iii) If |B|>8kthen for any k-segmentation sthat does not horizontally intersect Dwe have that ℓ(D,s) ≥ ( |B| 4 −2k ) σ. (iv) Bcan be computed in O(|D|) time. Proof. We consider the variables deﬁned in Algorithm 1. Proof of (i): By construction it immediately follows that Bis a partition of D. Proof of (ii): Consider a sub-signal B ∈B. We prove (ii) for each of the following cases: Case (a): Bwas added to Bat Line 12, and Case (b): Bwas either added to Bat Line 6 Case (a): In this case, by the condition at Line 9, Bmust satisfy that opt1(B) ≤σ. Case (b): In this case, Bwas returned via a recursive call. Hence, this case holds trivially by Case (a) above. Therefore, (ii) above holds by combining Cases (a)–(b). Proof of (iii): Let t= |B|and assume for simplicity that tis an even number. Recall that the index of each sub-signal in Bindicates its order of insertion to B, i.e., B1 is the ﬁrst sub-signal that was inserted to Band Bt was the last such sub-signal to be inserted to B. Observe that each recursive call B′:= SLICE PARTITION (BT,σ) at Line 5 returns at least |B′|≥ 2 sub-signals. This is because the recursive call happens only when opt1(BT) = opt1(B) >σ, which can only happen if |{(i,j) |((i,j),y) ∈B}|> 1, i.e., BT exceeds the maximum tolerance, and can indeed be partitioned into sub-signals. Hence, there are at least t/4 distinct pairs of consecutive sub-signals Bi and Bi+1 that were both either computed via the recursive call or both were not computed via the recursive call. We now show that each such pair satisﬁes opt1(Bi ∪Bi+1) >σ. 20Consider a pair of consecutive sub-signalsBi and Bi+1 that were both not computed via the recursive call at Line 5. Let B′⊆Bi+1 contain the elements ((i,j),y) ∈Bi+1 with the smallest value of i over all elements of Bi+1. By the greedy partition loop at Line 9 we obtain that opt1(Bi ∪B′) >σ. We now have that opt1(Bi ∪Bi+1) ≥opt1(Bi ∪B′) >σ, where the ﬁrst inequality is by Claim 9. Consider a pair of consecutive sub-signals Bi and Bi+1 that were both computed via the recursive call at Line 5. Then, similarly to the previous argument, we obtain that opt1(Bi ∪Bi+1) >σ. Now, let sbe a k-segmentation that does not horizontally intersect B, i.e., it does not horizontally intersect any B ∈B. By the deﬁnition of s, there might be at most 2ksub-signals in Bwhich are vertically intersected by s. Hence, among the t/4 distinct consecutive pairs of sub-signals discussed above there are at least t/4 −2ksuch pairs that are not intersected by s. Since Bis a partition of D, we have that ℓ(D,s) is at least the sum of opt1(Bi ∪Bi+1) ≥σ, over the above t/4 −2knon-intersected pairs of sub-signals. Hence, ℓ(D,s) ≥ (t 4 −2k ) σ= (|B| 4 −2k ) σ. Proof of (iv): The greedy Algorithm 1 can be implemented so that it computes only O(|D|) oper- ations. The most costly operation is the computation of opt1(B) for some sub-signal B. We now argue that this can be computed in O(1) time. Let Bbe a sub-signal and let µB = 1/|B|∑ (x,y)∈By be its mean value. Observe that opt1(B) = ∑ (x,y)∈B (y−µB)2 = ∑ (x,y)∈B y2 + |B|·µB −2µB ∑ (x,y)∈B y. (1) By precomputing and storing some statistics at each of the signal’s elements, then the three terms on the right hand side of (1) can all be evaluated in O(1) time for any sub-signal B. Hence, the total running time of Algorithm 1 is linear in the input size. We now restate and prove Lemma 7 from Section 3. Lemma 13. Let Dbe an n×msignal, k ≥1 be an integer, ε ∈(0,1/4) be an error parameter, and s: [n] ×[m] →R be an (α,β)k-approximation of D. Deﬁne σ := ℓ(D,s) α and γ := ε2 βk. Then algorithm PARTITION (D,γ,σ ) outputs a partition Bof D that is an ( O ( α γ2 ) ,γ2σ,O ( kα γ )) k - balanced partition in O(|D|) time. Proof. To prove thatBis a ( O ( α γ2 ) ,γ2σ,O ( kα γ )) k -balanced partition as in Deﬁnition 6, we need to prove the following properties: (i) opt1 (B) ≤γ2σfor every B ∈B. (ii) Bis a partition of Dwhose size is |B|∈ O ( α γ2 ) . (iii) For every k-segmentation ˆsthere are O ( kα γ ) sub-signals B ∈B for which ˆsassigns at least 2 distinct values, i.e., |{ˆs(x) |(x,y) ∈B}|≥ 2. Proof of (i): Observe that the output set Bcontains the the union of multiple output sets B′ := SLICE PARTITION (·,γ2σ) computed via calls to Algorithm 1. By Property (ii) of Lemma 12, every sub-signal B ∈B′in such output set B′satisﬁes that opt1 (B) ≤γ2σ. Hence, Property (i) of Lemma 7 immediately holds. 21Proof of (ii): By the greedy construction it holds that Bis a partition of D. We now prove that the number of times that Line 12 was executed is t∈O(α/γ), i.e., the number of times we append a set of signals lastB′to Bis at most O(α/γ). Let B1,··· ,Bt denote the set of sub-signals lastB′in each of the texecutions of Line 12, i.e., B1 := lastB′at the ﬁrst time Line 12 was executed. Each such set is called a horizontal set. Recall that s ∈SEGβk is a βk-segmentation. First of all, by the deﬁnition of a βk-segmentation function there are at most 2βk sets among the horizontal sets B1,··· ,Bt which can be horizontally intersected by s. Consider two consecutive horizontal sets Bi,Bi+1 that are not horizontally intersected by s, and let H = ⋃ B∈Bi∪Bi+1 B be the union of all the sub-signals in Bi ∪Bi+1. We now argue that the loss ℓ(H,s) is at least O(γσ). Since Bi and Bi+1 are two different horizontal sets, by the greedy construction we know that their union H could have been partitioned via a call to E := SLICE PARTITION (H,γ2σ) into a setE= { E1,··· ,E|E| } of at least|E|≥ 1 γ blocks. By substituting D= H,k = βk,B= Eand σ = γ2σin Property (iii) of Lemma 12, for a βk-segmentation s, we have that ℓ(H,s) ≥ (|E| 4 −2βk ) γ2σ≥ ( 1 4γ −2βk ) γ2σ ≥ ( 1 4γ −βk 9ε2 ) γ2σ= ( 1 4γ − 1 9γ ) γ2σ ≥γσ/2, where the second derivation holds for ε∈(0,1/3), and the third derivation is by the deﬁnition of γ. Assume by contradiction that there are more than 2α γ such pairs of consecutive horizontal sets Bi,Bi+1, which are not horizontally intersected by s. The loss of those slices to swould be bigger than 2α γ ·γσ 2 = ασ= ℓ(D,s), which is a contradiction. Therefore, the number of pairs of consecutive horizontal sets, which are not horizontally intersected by s, cannot exceed O ( α γ ) . Observe that the total number of horizontal sets that can be intersected by sis at most 2βk. Hence, the total number of horizontal sets is at most m∈O (α γ + 2βk ) ∈O (α γ ) . (2) We now prove that the number of output cells is at most|B|∈ O ( α γ2 ) in two steps. In step (i) we consider the horizontal sets that contain more than one row of Dand show that they contain a total of O ( α γ2 ) sub-signals. In step (ii) we consider the horizontal sets that contain exactly one row of D and prove that they also contain a total of O ( α γ2 ) sub-signals. Step (i): By (2), the total number of horizontal sets is at most m ∈O(α γ). Therefore, the total number of horizontal slices that contain more than one row of Ais also at most O ( α γ ) . By the construction in Algorithm 2, each such horizontal set Bi with more than 1 row of Ais partitioned into at most 1 γ sub-signals. Hence, the total number of blocks in horizontal sets than contain more than one row of Dis at most O ( α γ ·2 γ ) = O ( α γ2 ) . Step (ii): Consider all the horizontal sets Bi which contain one row of D, and which have been partitioned into |Bi|≤ 2βk ≤1/γblocks. The total number of blocks in such horizontal slices is thus bounded by the maximum number of horizontal slices m∈O(α/γ) times 1/γfor a total of at most O(α/γ2) blocks. For the rest of this step, we assume that all horizontal sets Bi have |Bi|≥ 2βk. Let G⊆[t] contain the indices of the horizontal sets which contain exactly one row of D, and let i∈G. Observe that Bi was computed, at some point, via a call Bi := SLICE PARTITION (∪B∈BiB,γ2σ). Also, since the points ∪B∈BiBof Bi all have the same row index, observe that Bi cannot be horizontally intersected by s. Therefore, by substituting D= ∪B∈BiB,k = βk and σ= γ2σin Property (iii) of Lemma 12 22we obtain that ℓ(Bi,s) ≥ (|Bi| 4 −2βk ) ·γ2σ. (3) Furthermore, we have that α·σ= ℓ(D,s) ≥ ∑ i∈G ℓ(Bi,s) ≥ ∑ i∈G (|Bi| 4 −2βk ) ·γ2σ, (4) where the ﬁrst derivation is by the deﬁnition of σ, the second derivation holds since {(x,y) ∈B |B ∈Bi,i ∈G}⊆ D, and the third derivation is by (3). Rearranging terms in (4) concludes Step (ii) as ∑ i∈G |Bi|≤ 4α γ2 + 8 ∑ i∈G βk ≤4α γ2 + 8tβk ≤4α γ2 + 8αβk γ ∈O (α γ2 ) . Therefore, the total number of blocks in horizontal sets that contain exactly one row of Dis at most O ( α γ2 ) . Proof of (iii): By the properties above we have that: (i) there are at mostO(α/γ) horizontal sets, and (ii) each horizontal set Bi either contains at most O(1/γ) sub-signals, or all the points ((i,j),y) ∈B of all the blocks B ∈Bi have the same row index i. Let ˆsbe a k-segmentation. ˆscan horizontally intersect all the (at most) 1/γsub-signals of at most khorizontal sets, and can vertically intersect at most 1 block from each of the O(α/γ) horizontal sets. Hence, the total number of intersected sub-signals is O(kα/γ). Computational time: We now prove that Bcan be computed in O(|D|) time. The computational time of Algorithm 2 is dominated by the computational time of Line 11 where we partition a slice S. Using Algorithm 1 we can partition each such slice S in linear O(|S|) time; see Lemma 12. Therefore, the naive implementation, i.e. by calling Algorithm 1 for every slice S, will result in O(|D|2) overall time, since many rows of Dparticipate many times in such a call to Algorithm 1. However, we can implement Line 11 inO(m) time, rather than O(|S|) time, by preprocessing the input signal D, in linear time O(|D|), and storing some statistics for every element ((i,j),y) ∈D. For example, one can store the sum of values and squared values over all elements ((i′,j′),y′) where i′<i or j′<j . Using those values we can computeopt(B) in O(1) time for every sub-signalBof D. Now, using such statistics (and possibly more statistics), Line 11 can be implemented in O(m) time via a greedy algorithm that iterates over the points of the last row R= {((i,j),y) ∈D|i= rend} added to S(i.e. with no need to iterate over other elements of S). We leave the small details to the reader. D Coreset Construction In this section, we provide the proof of correctness for our main coreset construction algorithm presented in Algorithm 3; see Theorem 15. Furthermore, we provide an algorithm than gets as input a k-segmentation s, as well as a (k,ε)-coreset for some input dataset D, which was computed using Algorithm 3. The algorithm returns a (1 + ε)-approximation to the loss ℓ(D,s), in O(k|C|) time; see Algorithm 5 and full details in Lemma 14. In what follows, for an n×msub-signal Band a weight function u: B →[0,∞), we abuse notation and denote u((a,b)) by simply u(a,b) for (a,b) ∈B. Some intuition behind Algorithm 5. Given a (k,ε)-coreset (C,u) for an input dataset D = {(x1,y1),··· ,(xN,yN)}, and a k-segmentation s, the algorithm outputs a (1 +ε)-approximation to ℓ(D,s) in time that depends only on kand |C|. During the computation of (C,u) in Algorithm 3, a partition Bof Dwas computed. Then, for every set Bin the partition B, a representative pair (CB,uB) for Bwas computed and added to C. 23To approximate the loss ℓ(D,s), we will approximate individually ℓ(B,s) for every B ∈B, and return the sum of those losses. Therefore, we now consider a single set B ∈B, and consider the following two cases. Case (i) : sassigns the same value for all the elements of B. Then, by construction, it is guaranteed that ℓ(B,s) = ∑ (x,y)∈CB uB(x,y)(s(y) −y)2. Therefore, in this case, ℓ(B,s) will be accurately estimated using (CB,uB). Case (ii) : s assigns more than one unique value to the elements of B. In this case, if we ig- nore the computational time, we would ideally want to compute a “smoothed version” (S,w) of (CB,uB), as shown in Fig. 3 (see (9)- (11) below for formal details). Then, we would return the loss ∑ (x,y)∈Sw(x,y)(s(y) −y)2. However, computing (S,w) is not necessary, since there are many subsets of B in which all the elements x∈B have simultaneously the same label in S and are assigned the same value by s. Combining this with the fact that those subsets are of rectangular (simple) shape, we obtain that the loss over those subsets can be evaluated efﬁciently, as computed in Algorithm 5. Algorithm 5: FITTING -LOSS ((C,u),s); see Lemma 14 Input : A (k,ε)-coreset (C,u) which was returned from a call to SIGNAL -CORESET (D,k,ε/ ∆) in Algorithm 3, for some n×m-signal D, k≥1, ε∈(0,1) and a sufﬁciently large ∆ ≥1. A k-segmentation (or k-tree) s. Output :A (1 + ε)-approximation to the loss ℓ(D,s). 1 loss:= 0 2 for every 4 consecutive elements ˆC = {(ai,bi)}4 i=1 in Cdo 3 Denote by Bthe sub-signal that corresponds to C′. // By construction in Algorithm 3, the coordinates a of the 4 elements (a,b) ∈ ˆC are the corners of B. 4 z:= |{s(x) |(x,y) ∈B}| 5 if z= 1 // i.e., s does not intersect B 6 then 7 lossˆC := ∑ (x,y)∈CB uB(x,y)(s(x) −y)2. // note that s(x1) = s(x2) for every x1,x2 ∈B 8 else // In this case, s intersects B 9 Denote by Sthe partition that sinduces onto [n] ×[m]. // S contains |S|≤ k subsets of [n] ×[m]. 10 i:= 1 11 for every S′∈S do 12 Denote by ℓthe label that sassigns to the elements of S′i.e., s(x,y) = ℓfor every (x,y) ∈S′ 13 z:= |B∩S′|// The number of element in the intersection of the S′ and the subset of [n] ×[m] that is represented by C′. 14 lossˆC := 0 15 while z≥1 do 16 if u(ai,bi) ≤zthen 17 lossˆC := lossˆC + u(ai,bi) ·(ℓ−bi)2 18 u(ai,bi) := 0 19 z:= z−u(ai,bi) 20 i:= i+ 1 21 else 22 lossˆC := lossˆC + z·(ℓ−bi)2 23 u(ai,bi) := u(ai,bi) −z 24 z:= 0 25 loss:= loss+ lossˆC. 26 return loss 24Lemma 14. Let D = {(x1,y1),··· ,(xN,yN)}be an n ×m signal i.e., N := nm. Let k ≥1 be an integer, ε ∈(0,1/4) be an error parameter, and (C,u) be the output of a call to SIGNAL -CORESET (D,k,ε ) (see Algorithm 3). Let sbe a k-segmentation (in particular, a k-tree). Finally, let lossbe the output of a call to FITTING -LOSS ((C,u),s); see Algorithm 5. Then there is a sufﬁciently large constant ∆ ≥1 such that |ℓ(D,s) −loss|≤ ∆ε·ℓ(D,s). Moreover,losscan be computed in O(k|C|) time. Proof. We consider the variables deﬁned in Algorithm 5. First, consider a subset ˆCof Cfrom some iteration of the For loop at Line 2 of Algorithm 5, and let Bbe the sub-signal that corresponds to ˆC, as in Line 3. We now prove that the loss lossˆC computed in the same iteration of the For loop (i.e., at Lines 3- 25) satisﬁes the following claim. Claim 14.1. Let z = |{s(x) |(x,y) ∈B}| be the number of distinct values s assigns to the coordinates of B(as computed in Line 4). Then, lossˆC satisﬁes that { ℓ(B,s) = lossˆC if z = 1⏐⏐ℓ(B,s) −lossˆC ⏐⏐≤ε·ℓ(B,s) + O ( opt1(B) ε ) otherwise Proof. We prove Claim 14.1 using the following case analysis: (i) z= 1 and (ii) z≥2. Case (i): z= 1. We prove that ℓ(B,s) = lossˆC. Since the input coreset (C,u) was computed using Algorithm 3, we know that the set ˆC was computed at Line 5 of Algorithm 3, along with a weight function ˆu. Hence, the pair ( ˆC,ˆu) satisfy, by construction, the following property: ∑ (a,b)∈ˆC ˆu((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B (y|y2 |1). (5) Now, for any constant ˆs∈R, we have that ∑ (a,b)∈ˆC ˆu(a,b)(b−ˆs)2 = ∑ (a,b)∈ˆC ˆu(a,b) ·b2 + ˆs2 ∑ (a,b)∈ˆC ˆu(a,b) −2ˆs ∑ (a,b)∈ˆC ˆu(a,b)b = ∑ (x,y)∈B y2 + ˆs2 ∑ (x,y)∈B 1 −2ˆs ∑ (x,y)∈B y = ∑ (x,y)∈B (y−ˆs)2, (6) where the second equality is by (5). Since z= |{s(x) |(x,y) ∈B}|= 1, there is a constant ˆs∈R such that ℓ(B,s) = ∑ (x,y)∈B (y−ˆs)2. (7) Hence, we have that ℓ(B,s) = ∑ (x,y)∈B (y−ˆs)2 = ∑ (a,b)∈ˆC ˆu(a,b)(b−ˆs)2 = lossˆC, where the ﬁrst derivation is by (7), the second derivation is by (6), and the last derivation is by the deﬁnition of lossˆC at Line 7 of Algorithm 5. 25Case (ii): z≥2. We prove that ⏐⏐ℓ(B,s) −lossˆC ⏐⏐≤ε·ℓ(B,s) + O (opt1(B) ε ) . We ﬁrst observe that, by the triangle inequality, for anya,b,c ∈R we have that ||a−c|2 −|b−c|2|= ||a−c|−|b−c||·(|a−c|+ |b−c|) ≤|a−b|·(2|a−c|+ |a−b|) = |a−b|2 + 2|a−c|·|a−b| = |a−b|2 + 2√ε|a−c|· |a−b|√ε ≤|a−b|2 + ε·|a−c|2 + |a−b|2 ε = ε·|a−c|2 + ( 1 + 1 ε ) ·(a−b)2, (8) where the second inequality holds since 2xy≤x2 + y2 for every x,y ∈R. Smoothed coreset. In Algorithm 3 we computed some small compression CB, along with a weights function uB, for every subset Bin the partition of the input. The size |CB|of this compression is a small constant, independent of the (potentially large) size of B. The pair (CB,uB) satisfy a set of properties, which we visually demonstrate via this “smoothed coreset” notion; see Fig. 3. Informally, the “smoothed version” of(CB,uB) is another pair (C′ B,u′ B), such that C′ B contains a duplication of the elements of CB. The number of duplications of every element cfrom CB is according to its weight uB(c). We now formally deﬁne a “smoothed version” of a pair( ˆC,ˆu). A pair (S,w) is said to be a smoothed version of the pair ( ˆC,ˆu) if it satisﬁes the following properties: (i) (S,w) has the same sum of weights, sum of labels, and sum of squared labels as( ˆC,ˆu), (ii) The set of coordinates {a|(a,b) ∈S} in Scovers the entire set of coordinates{x|(x,y) ∈B}of the original set B, with possible duplicates, and (iii) The sum of weights over all elements in Swith the same coordinate is 1. Formally, ∑ (a,b)∈S w((a,b)) ·(b|b2 |1) = ∑ (a,b)∈ˆC ˆu((a,b))(b|b2 |1), (9) {x|(x,y) ∈B}= {a|(a,b) ∈S}, (10) and ∑ (a,b)∈S:a=x w((a,b)) = 1 for every (x,y) ∈B. (11) In what follows, for every pair (S,w) which is a smoothed version of ( ˆC,ˆu), we prove the following two properties: We now prove the following two properties: ﬁrst, that |ℓ(B,s) −ℓ((S,w),s)|≤ ε·ℓ(B,s) + O (opt1(B) ε ) , (12) for every every pair (S,w) which is a smoothed version of ( ˆC,ˆu). Second, we need to prove there is some pair ( ˆS, ˆw) which is a smoothed version of ( ˆC,ˆu) that satisﬁes lossˆC = ℓ(( ˆS, ˆw),s), (13) where lossˆC is the loss computed at Lines 3- 25, using only the pair ( ˆC,ˆu) (i.e., at the current iteration of the outer-most For loop of Algorithm 5), without actually computing ( ˆS, ˆw)). Case (ii) then immediately holds by combining (12) and (13) above. A proof of (12). Let (S,w) be a pair which is a smoothed version of ( ˆC,ˆu). By deﬁnition of (S,w), we have that wsums to 1 over all (a,b) ∈Swith the same a, as in (11). Therefore, for every 26(x,y) ∈Bwe can rewrite the term (y−s(x))2 as ∑ (a,b)∈S:a=xw(a,b)(y−s(x))2. Now, deﬁne yB(x) = yfor every (x,y) ∈B. We therefore have that ℓ(B,s) = ∑ (x,y)∈B (y−s(x))2 = ∑ (x,y)∈B   ∑ (a,b)∈S:a=x w(a,b)  ·(y−s(x))2 = ∑ (x,y)∈S w(x,y)(yB(x) −s(x))2, (14) where the last equality holds by (10) and by simply combining the two sums. We now have that |ℓ(B,s) −ℓ((S,u),s)| = ⏐⏐⏐⏐⏐ ∑ (x,y)∈S w(x,y)(yB(x) −s(x))2 − ∑ (x,y)∈S w(x,y)(y−s(x))2 ⏐⏐⏐⏐⏐ (15) = ⏐⏐⏐⏐⏐⏐ ∑ (x,y)∈S w(x,y) · ( (yB(x) −s(x))2 −(y−s(x))2) ⏐⏐⏐⏐⏐⏐ ≤ ∑ (x,y)∈S w(x,y) ⏐⏐(yB(x) −s(x))2 −(y−s(x))2⏐⏐ (16) ≤ ∑ (x,y)∈S w(x,y) ( ε·(yB(x) −s(x))2 + ( 1 + 1 ε ) (yB(x) −y)2 ) (17) = ε· ∑ (x,y)∈S u(x,y) ·(yB(x) −s(x))2 + ( 1 + 1 ε ) ∑ (x,y)∈S w(x,y) ·(yB(x) −y)2 = ε·ℓ(B,s) + ( 1 + 1 ε ) ∑ (x,y)∈S u(x,y) ·(yB(x) −y)2, (18) where (15) is by combining (14) and the deﬁnition of ℓ, (16) holds since the sum of absolute values is greater or equal than the absolute value of a sum, (17) holds by substituting in (8) every term in the sum, and (18) is by (14). We now bound the rightmost term of (18). Let ˆs≡1/|B|∑ (x,y)∈Bybe a 1-segmentation function that returns the label mean of B. We have that∑ (x,y)∈S w(x,y) ·(yB(x) −y)2 ≤2 · ∑ (x,y)∈S w(x,y) · ( (yB(x) −ˆs(x))2 + (y−ˆs(x))2) (19) = 2 ∑ (x,y)∈S w(x,y) ·(yB(x) −ˆs(x))2 + 2 ∑ (x,y)∈S w(x,y) ·(y−ˆs(x))2 = 2 ·(ℓ(B,ˆs) + ℓ((S,u),ˆs)) (20) = 2 ·(ℓ(B,ˆs) + ℓ(B,ˆs)) (21) = 4 ·ℓ(B,ˆs) = 4 ·opt1(B) (22) where (19) is by the weak triangle inequality, (20) is by combining the deﬁnition of ℓwith (14), (21) holds by Case (i) above, and (22) holds since the label means minimizes its sum of squared differences to the labels. 27Figure 8: (Left): The pair ( ˆC,ˆu). (Middle): A 4-segmentation s, which in- duces a partition of [5] × [5] into 4 sets B = B1 ∪ B2 ∪ B3 ∪ B4 where B1 = {(1,1),(2,1),(3,1),(4,2),··· ,}, B2 = {(1,3),(2,3),(1,4),(2,4),··· ,}, B3 = {(3,3),(4,3),(5,3)}, B4 = {(3,4),(4,4),(4,5),(3,5),···}. (Right): A smoothed version ( ˆS, ˆw) of ( ˆC,ˆu). There can be more than one unique smoothed version for the same pair ( ˆC,ˆu); see Properties in (9)-(11). The pair ( ˆS, ˆw) is constructed by iterating over every set B ∈B. Every element in Bis assigned to a label from the labels of the 4-coreset points (8,17,21,22) as follows: If |B|> ˆu(li), then ˆu(li) elements of Bare assigned to l1 and |B|− ˆu(li) are assigned to li+1. If |B|≤ ˆu(li) then all the elements of Bare assigned to ˆu(li), and |B|is subtracted from ˆu(li), and so on. If ˆuassigns fractional weights, then some elements of Bmight be assigned to more than one label, as long as the sum of weights over every element in Bis 1. By construction, ( ˆS, ˆw) satisﬁes Properties (10)-(11). Hence, ( ˆS, ˆw) is a smoothed version of ˆC,ˆu). Computing ℓ(( ˆS, ˆw,s) can be trivially computed in time only O(k|ˆC|) (rather than O(n) where nis the size of the original data), since the sets in the partition Bcontain a duplication of a constant number of labels. Equation (12) now holds by combining (18) and (22). A proof of (13). To prove (13), in Fig. 8 we construct a smoothed version ( ˆS, ˆw) of ( ˆC,ˆu) which satisﬁes (13). Furthermore, by combining the construction of ( ˆS, ˆw) with the computation of lossˆC in Lines 9-25 of Algorithm 5, we obtain, as desired, that lossˆC = ℓ(( ˆS, ˆw),s). Claim 14.1 now holds by combining cases (i) and (ii) above. We now prove Lemma 14. Consider the construction of (C,u) in Algorithm 3. By deﬁnition and by Lemma 5, the function s′computed at Line 5 of Algorithm 3 is an O(k8 log2 nm)-segmentation and satisﬁes that ℓ(D,s′) ∈O(klog n·optk(D)) . By the last derivation, let cα be the smallest constant such that ℓ(D,s′) ≤cα ·klog n·optk(D). By the last inequality and deﬁnitions of σand αwe obtain that σ:= ℓ(D,s′) α ≤cαklog nm·optk(D) α = cαklog nm·optk(D) cαklog nm = optk(D). (23) Now consider the partition Bcomputed at Line 3 of Algorithm 3 via a call to PARTITION (D,γ,σ ). By Lemma 7, Bsatisﬁes that (i) opt1 (B) ≤γ2σfor every B ∈B. (ii) Bis a partition of Dwhose size is |B|∈ O ( α γ2 ) . 28(iii) There are O ( kα γ ) sub-signals B ∈B where |{s(x) |(x,y) ∈B}|>1. (iv) Bcan be computed in O(|D|) time. Consider the pair (CB,uB) computed at Line 5 of Algorithm 3 for some B ∈B. Now, consider the For loop at Line 2 of Algorithm 5. For every pair (CB,uB), there is an iteration of this For loop for which C′= CB. In this iteration, Algorithm 5 computes a loss lossCB that corresponds to (CB,uB). We can plug B, (CB,uB), and lossCB in Claim 14.1 to obtain that:{ ℓ(B,s) = lossCB if z = 1 |ℓ(B,s) −lossCB |≤ ε·ℓ(B,s) + O ( opt1(B) ε ) otherwise (24) Let B1 ⊆ Bcontain the set of sub-signals in B that are not intersected by s, i.e., B1 = {B ∈B|| s(B)|= 1|}, and let B2 = B\\B 1 be the set of sub-signals which are partially inter- sected by s. By Property (iii) above, |B2|∈ O (kα γ ) . (25) Furthermore, by combining (24) with Property (i) above, for every B ∈B2 we have that |ℓ(B,s) −lossCB |≤ ε·ℓ(B,s) + O (opt1(B) ε ) ≤ε·ℓ(B,s) + O (γ2σ ε ) . (26) In other words, the loss ℓ(B,s) of every sub-signal B ∈B2 is approximated by lossCB up to some small error. Hence, by summing over all B ∈B2 we obtain that∑ B∈B2 |ℓ(B,s) −lossCB | ∈ ∑ B∈B2 ( ε·ℓ(B,s) + O (γ2σ ε )) (27) ≤ε·ℓ(D,s) + O ( |B2|· γ2σ ε ) (28) ≤ε·ℓ(D,s) + O (kα γ ·γ2σ ε ) ≤ε·ℓ(D,s) + O (kαγ ε ·optk(D) ) (29) ≤ε·ℓ(D,s) + O(ε·optk(D)) (30) ∈O(ε·ℓ(D,s)), (31) where (27) follows from (26), (28) is by (25), (29) is by (23), (30) holds since kαγ ≤ε2, and (31) holds since optk(D) ≤ℓ(D,s) for every k-segmentation s. Furthermore, for every B ∈B1, by (24) we have that ℓ(B,s) = lossCB . Hence, by summing over every B ∈B1 we obtain that ∑ B∈B1 ℓ(B,s) = ∑ B∈B1 lossCB . (32) In other words, the loss ℓ(B,s) of ever sub-signal B ∈B1 is accurately estimated by lossCB . Algorithm 5 then outputs the sum of losses loss:= ∑ B ∈BlossCB . (33) We hence obtain that |ℓ(D,s) −loss|= ⏐⏐⏐⏐⏐ ∑ B∈B1 (ℓ(B,s) −lossCB ) + ∑ B∈B2 (ℓ(B,s) −lossCB ) ⏐⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐⏐ ∑ B∈B1 (ℓ(B,s) −lossCB ) ⏐⏐⏐⏐⏐+ ⏐⏐⏐⏐⏐ ∑ B∈B2 (ℓ(B,s) −lossCB ) ⏐⏐⏐⏐⏐∈O(ε·ℓ(D,s)), (34) 29where the ﬁrst derivation is by 33, second derivation is by the triangle inequality, and the last is by combining (32) and (31). By (34), there is a constant ∆ ≥1 such that |ℓ(D,s) −loss|≤ ∆εℓ(D,s). This concludes the proof of the claim in Lemma 14. Computational time: Line 2 of the Algorithm 5 is a loop with |C| |ˆC|iterations. Inside this loop: if z >1 line 7 is computed in O(|ˆC|), else line 11 is another loop with O(k) iterations, inside which the line 20 is executed at most |ˆC|times and line 24 can be executed only once because it results in z= 0 and in exiting from the while loop. In total the complexity of line 15 is O(|ˆC|), of line 11 is O(k|ˆC|) and of line 2 and the whole algorithm: O(k|C|) Space complexity: Algorithm 5 uses only constant amount of additional storage space because in each line of the algorithm only numeric variables are created and variables are reused inside the loops. Theorem 15 (Coreset). Let D = {(x1,y1),··· ,(xN,yN)}be an n×m signal i.e., N := nm. Let k ≥1 be an integer (that corresponds to the number of leaves/rectangles), and ε ∈(0,1/4) be an error parameter. Let (C,u) be the output of a call to SIGNAL -CORESET (D,k,ε/ ∆) for a sufﬁciently large constant ∆ ≥1; see Algorithm 3. Then, (C,u) is a (k,ε)-coreset for Dof size |C|∈ (klog(N))O(1) ε4 ; see Deﬁnition 3. Moreover, (C,u) can be computed in O(kN) time. Proof. To prove that (C,u) is a (k,ε)-coreset for D, we need to prove that for every k-segmentation s, (C,u) sufﬁces to approximate the loss ℓ(D,s), up to a multiplicative factor of 1 + ε, in time that depends only on |C|and k. Let sbe a k-segmentation and let loss≥0 be an output of a call to FITTING -LOSS ((C,u),s); see Algorithm 5. Then, by Lemma 14, losscan be computed in O(k|C|) time (i.e., in time that depends only on |C|and k), and provides, as required, a (1 + ε)-approximation to ℓ(D,s) as |ℓ(D,s) −loss|≤ ∆ · ε ∆ ·ℓ(D,s) = ε·ℓ(D,s). Hence, (C,u) is a (k,ε)-coreset for D. Line 1 of Algorithm 3 can be computed in O(k·|D|) time by Lemma 5. Line 3 of Algorithm 3 can be computed in O(|D|) time by Lemma 7. The loop at Line 4 can be computed in ∑ B∈BO(|B|) = O(|D|) time by Section E. Hence, the call SIGNAL -CORESET (D,k,ε ) can be implemented in O(k|D|) = O(kmn) time. Proof behind Line 6. We now prove that the replacements of the coordinates applied at Line 6 does not violate the correctness of the algorithm. Observe that replacing the coordinates of entries inside each cell, while keeping the same labels, does not affect the variance of this subset. Therefore, the cost of this cell, which is computed in Algorithm 5) and depends only on the labels, remains exactly the same. Space complexity: By construction, each pair (CB,uB) computed at Line 5 can be stored using only O(1) space. Hence, the concatenation (C,u) of the |B|pairs {(CB,uB) |B ∈B} can be stored using O(|B|) ∈O(α/γ2) = O(α(βk)2/ε4) = O ( kO(1) logO(1) nm ε4 ) space. E The Caratheodory Theorem Given a point p ∈Rd inside the convex hull of a set of points P ⊆Rd, Caratheodory’s Theorem proves that there is a subset of at most d+ 1 points in P whose convex hull also contains p. Theorem 16 (Caratheodory’s Theorem [13, 48]). Let P ⊆Rd be a (multi)set of npoints. Then in O(nd3) time we can compute a subset Q⊆P and a weights functions u: Q→[0,∞) such that: (i) Q⊆P, (ii) |Q|= d+ 1, (iii) ∑ q∈Qu(q) ·q= 1 n ∑ p∈P p, and (iv) ∑ q∈Qu(q) = n. 30Corollary 17. Let Dbe an n×msub-signal. Then, in O(|D|) time we can compute a weighted n×msub-signal (A,w) such that: (i) A ⊆D, (ii) |A|= 4, (iii) ∑ (a,b)∈A w(a,b) ·(b |b2 |1) = ∑ (x,y)∈D (y|y2 |1), and ∑ (a,b)∈A w(a,b) = |D|. Proof. Deﬁne the multi-set P = { (y|y2 |1) ∈R3 |(x,y) ∈D } . Now, substituting P in The- orem 16 yields that in O(n) time we can compute a subset Q ⊆ P and a weights functions u : Q →[0,∞) such that: (i) Q ⊆P, (ii) |Q|= 4 , (iii) ∑ q∈Qu(q) ·q = 1 |D| ∑ p∈P p, and (iv) ∑ q∈Qu(q) = |D|. Now, add to Aa single element (x,y) ∈Dfor every (y |y2 |1) ∈Q. In other words, for every element chosen for the set Qby the Caratheodory theorem, add its corresponding element from Dto A. Furthermore, deﬁne w(x,y) = |D|·u((y|y2 |1)) for every (x,y) ∈A. Corollary 17 trivially holds for (A,w). 31",
      "meta_data": {
        "arxiv_id": "2110.03195v1",
        "authors": [
          "Ibrahim Jubran",
          "Ernesto Evgeniy Sanches Shayda",
          "Ilan Newman",
          "Dan Feldman"
        ],
        "published_date": "2021-10-07T05:49:55Z",
        "pdf_url": "https://arxiv.org/pdf/2110.03195v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper presents the first algorithm to construct a (k,ε)-coreset for decision trees specifically designed for n×m signals (matrices or 2D-signals). The coreset provably approximates the regression/classification loss of any k-decision tree, achieving a size polynomial in klog(N)/ε and a construction time of O(Nk). A novel aspect is forging a link between decision trees in machine learning and partition trees in computational geometry. Experimentally, applying these coresets boosts the computation time and parameter tuning of random forests (e.g., using sklearn and LightGBM) by up to 10 times on real-world datasets, while maintaining similar accuracy. The authors also provide open-source code and demonstrate the utility for AutoML in decision tree hyperparameter tuning.",
        "methodology": "The methodology involves a three-stage coreset construction algorithm (Algorithm 3 SIGNAL-CORESET). First, a bi-criteria (α,β)k-approximation (Algorithm 4 BICRITERIA) is computed in O(knm) time to provide a rough lower bound for the optimal k-segmentation loss. Second, a (c1,c2,c3)k-balanced partition (Algorithm 2 PARTITION, which uses Algorithm 1 SLICE PARTITION) of the input signal is generated in O(|D|) time. This partition divides the signal into sub-signals such that the single-segment loss (opt1) for each sub-signal is small, and any k-segmentation intersects only a few of these sub-signals. Finally, for each sub-signal in the balanced partition, a small local coreset of size 4 is computed using Caratheodory's theorem, preserving sums of values, squared values, and weights. The union of these local coresets forms the global (k,ε)-coreset. The fitting loss to a k-segmentation `s` is approximated (Algorithm 5 FITTING-LOSS) by summing individual approximations over the sub-signals, accurately estimating non-intersected ones and error-bounding intersected ones.",
        "experimental_setup": "The coreset construction (Algorithm 3) was implemented in Python 3.7 and evaluated on a standard MSI Prestige 14 laptop. The study used existing random forest implementations as black-box solvers: RandomForestRegressor from `sklearn.ensemble` and LGBMRegressor from `lightGBM`, both with default hyperparameters. Two compression schemes were compared: their DT-coreset and a uniform RandomSample (with size matched to the coreset for fair comparison). Coreset construction fixed `k=2000` to control size. Experiments utilized two public UCI Machine Learning Repository datasets, both normalized: the Air Quality Dataset (9358 instances, 15 features) and the Gesture Phase Segmentation Dataset (9900 instances, 18 features). The task involved predicting missing entries, with a test set composed of 30% of the dataset formed by randomly chosen 5×5 patches. The Sum of Squared Errors (SSE) between predictions and ground truth served as the loss metric. Hyperparameter tuning was performed by calibrating `k` on both the full data and the coreset, with results averaged over 10 repetitions.",
        "limitations": "The existence of small coresets for decision trees is not general; they do not exist for arbitrary datasets (e.g., 1-dimensional data with binary labels). This work overcomes this by assuming the input is a discretized signal (matrix), where every coordinate has a label. The theoretical upper bounds for coreset size are noted as pessimistic, often predicting sizes larger than the original dataset, whereas empirical results show significantly smaller coresets. The paper primarily focuses on the sum of squared distances (SSE) loss function, although generalization to other loss functions is suggested. The running time of their Python implementation, while demonstrating improvements, is described as 'naive' compared to highly optimized professional libraries, indicating potential for further optimization.",
        "future_research_directions": "Future research directions include formally defining the properties of 'real-world' non-signal datasets for which the coreset empirically performs well, despite theoretical limitations. The authors also plan to generalize the results to support other loss functions beyond the sum of squares distances, anticipating that this is feasible if a k=1 coreset is known for those functions. Additionally, supporting high-dimensional data (tensors with d≥3) instead of just matrices is a straightforward generalization that can be achieved through minor modifications to the algorithms."
      }
    },
    {
      "title": "Coresets for Decision Trees of Signals",
      "abstract": "A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix\n(2D-signal) into $k\\geq 1$ block matrices (axis-parallel rectangles, leaves)\nwhere each rectangle is assigned a real label. Its regression or classification\nloss to a given matrix $D$ of $N$ entries (labels) is the sum of squared\ndifferences over every label in $D$ and its assigned label by $t$. Given an\nerror parameter $\\varepsilon\\in(0,1)$, a $(k,\\varepsilon)$-coreset $C$ of $D$\nis a small summarization that provably approximates this loss to \\emph{every}\nsuch tree, up to a multiplicative factor of $1\\pm\\varepsilon$. In particular,\nthe optimal $k$-tree of $C$ is a $(1+\\varepsilon)$-approximation to the optimal\n$k$-tree of $D$.\n  We provide the first algorithm that outputs such a $(k,\\varepsilon)$-coreset\nfor \\emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial\nin $k\\log(N)/\\varepsilon$, and its construction takes $O(Nk)$ time. This is by\nforging a link between decision trees from machine learning -- to partition\ntrees in computational geometry.\n  Experimental results on \\texttt{sklearn} and \\texttt{lightGBM} show that\napplying our coresets on real-world data-sets boosts the computation time of\nrandom forests and their parameter tuning by up to x$10$, while keeping similar\naccuracy. Full open source code is provided.",
      "full_text": "Coresets for Decision Trees of Signals Ibrahim Jubran, Ernesto Sanches, Ilan Newman, Dan Feldman University of Haifa, Israel {ibrahim.jub, ernestosanches, dannyf.post}@gmail.com {ilan}@cs.haifa.ac.il Abstract A k-decision tree t(or k-tree) is a recursive partition of a matrix (2D-signal) into k ≥1 block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classiﬁcation loss to a given matrix Dof N entries (labels) is the sum of squared differences over every label inDand its assigned label by t. Given an error parameter ε∈(0,1), a (k,ε)-coreset Cof D is a small summarization that provably approximates this loss to every such tree, up to a multiplicative factor of 1 ±ε. In particular, the optimal k-tree of C is a (1 + ε)-approximation to the optimal k-tree of D. We provide the ﬁrst algorithm that outputs such a (k,ε)-coreset for every such matrix D. The size |C|of the coreset is polynomial in klog(N)/ε, and its con- struction takes O(Nk) time. This is by forging a link between decision trees from machine learning – to partition trees in computational geometry. Experimental results on sklearn and lightGBM show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x10, while keeping similar accuracy. Full open source code is provided. 1 Introduction Decision trees are one of the most common algorithms used in machine learning today, both in the academy and industry, for classiﬁcation and regression problems [ 52]. Informally, a decision tree is a recursive binary partition of the input feature space into hyper-rectangles, where each such hyper-rectangle is assigned a label. If the labels are given from a discrete set, the trees are usually called classiﬁcation trees, and otherwise they are usually called regression trees. Variants include non-binary partitions and forests [29]. Why decision trees? Advantages of decision trees, especially compared to deep learning, include: (i) Interpretability. They are among the most popular algorithms for interpretable (transparent) machine learning [31]. (ii) Usually require small memory space, which also implies fast classiﬁcation time. (iii) Accuracy. Decision trees are considered as one of the few competitors of deep networks. In competitions, such as the ones in Kaggle [36], they are one of the favorite classiﬁers [10], especially on small or traditional tabular data. (iv) May learn from small training data. The goal is usually to compute the optimal k-tree t∗for a given dataset Dand a given number kof leaves, according to some given loss function. In practice, researchers usually use ensemble of trees called forests, e.g., a Random Forest [12], which are usually learned from different subsets of the training set. The ﬁnal classiﬁcation is then based on a combination rule, such as majority or average vote. Since both the training and classiﬁcation of each tree are computed independently and possibly in parallel, we focus on the construction of a single tree. A dataset Din this paper is a set D= {(x1,y1),··· ,(xN,yN)}⊆ A×R of pairs, where Ais the feature space. Each pair (x,y) ∈Dconsists of a database record (vector / sample) x∈Aand its real label y ∈R. As common, we assume that non-real features, such as categorical features, are arXiv:2110.03195v1  [cs.LG]  7 Oct 2021Figure 1: (Left): A one dimensional signal (orange points) and its segmentation into 25 “smooth” segments / leaves (green lines). Image taken from Section 1.10 (“Decision Trees”) of the sklearn’s User Guide [56]. The vector von top represents a subset of the signal’s values. The bottom vectors represent a 4-segmentation of v, similar to the horizontal green line segments. Each segment contains the average value of its corresponding segment from v. (Middle): A matrix that represents the 4 ×5 signal D = {((1,1),3),((1,2),4),((1,3),5),···} (in black) and a 3 ×2 matrix that represents a 3 ×2 sub-signal B = {((1,2),4),((1,3),4),((2,2),3),···} (in purple). (Right): A matrix that represents a 5-segmentation sof A = [4] ×[5]; see Deﬁnition 1. Since sis a 5-segmentation, it partitions the [4] ×[5] matrix into 5 distinct block matrices B1 (red), B2 (blue), B3 (yellow), B4 (green), and B5 (black) such that sassigns the same number for all the entries in the same block. The SSE ﬁtting loss ℓ(D,s) is the sum of squared differences over every entry in the left matrix to its corresponding entry on the right matrix. Also, there is no k-tree that can obtain the same partition. converted to real numbers; see e.g. [27]. For example, in common classiﬁcation problems A= Rd for some d≥1 and y∈{0,1}is a binary number. The resulting model may be used for prediction on another test dataset, completion of missing values, or efﬁcient storage of the original dataset by replacing the label yof each pair (x,y) ∈Dwith the label t(x) that was assigned to it by the tree t. The last technique is used e.g. in the MPEG4 encoder [ 46], where decision trees of a speciﬁc structure (quad-trees) are used to compress an image Dthat consists of pixel-grayscale pairs (x,y). Challenges. The motivation for this paper originated from the following challenges: (i) Sub-optimality. Hardness of decision tree optimization is both a theoretical and practical obsta- cle [31]. It is NP-hard to compute the optimal k-tree, or its approximation, when the number kis not ﬁxed [14, 39]. There were several attempts to improve the optimality of decision tree algorithms, from binary-split decision trees as in [6, 8], in a line of work of e.g. [9, 61]. Nevertheless, greedy implementations e.g., CART [40] and C4.5 [51] have remained the dominant methods in practice. (ii) Computation time. Due to this lack of optimality, ﬁnding a decision tree that provides a good accuracy usually requires many runs, since each of them returns only a local minimum that might be arbitrarily far from the global optimum. The ﬁnal model usually consists of a forest containing many trees. Popular forest implementations include the famous Sklearn, XGBoost, LightGBM, and CatBoost libraries [49, 15, 37, 19], which all utilize (as default) an ensemble of at least 100 trees. Moreover, there is a list of dozen parameters to calibrate including: number of trees, depth of each tree, pruning/splitting strategies on each tree and between them, and many others. To this end, the running time for obtaining reasonable results even on moderate size datasets might be impractical. (iii) Scalability. Existing techniques tend not to scale to realistically-sized problems unless simpliﬁed to trees of a speciﬁc form as stated in [31]. (iv) Streaming, parallel, and dynamic updates. The common algorithms mentioned above do not support continuous learning or updating of the modeled tree when an input sample is either added or removed from the dataset, e.g., when the dataset does not ﬁt into memory or arrives on-the-ﬂy. Similarly, we do not know techniques to train a single tree in parallel on multiple machines. 1.1 Coresets “Coresets are one of the central methods to facilitate the analysis of large data sets.” [47]. Informally, for an input dataset D, a set T of models, an approximation error ε ∈(0,1), and a loss function ℓ, a coreset C is a data structure that approximates the loss ℓ(D,t) for every model t ∈T, up to a multiplicative factor of 1 ±ε, in time that depends only on |C|. Hence, ideally, C is also much smaller than ther original input D. Why coresets? The main motivation for constructing a coreset is to compute the optimal model or its approximation, much faster, while sacriﬁcing little accuracy. Furthermore, a coreset for a family of classiﬁers is many times a “silver bullet” that provides a uniﬁed solution to all Challenges (i)-(iv) above. Combining the two main coreset properties: merge and reduce [32, 7, 26, 1], which 2are usually satisﬁed, with the fact that a coreset approximates every model, and not just the optimal model, enables it to support streaming and distributed data [11, 41], parallel computation [21], handle constrained versions of the problem [25], model compression [20], parameter tuning [43] and more. Coreset construction techniques. There are many different techniques for coreset construction, ranging from loss-less to lossy, from deterministic to randomized, and from greedy to non-greedy con- structions. Examples include accurate coresets [33] and non-accurate coresets [23] via computational geometry, random sampling-based coresets [22, 17, 44], and greedy deterministic coresets via the Frank-Wolfe algorithm [16]. Recently, many works focus on developing frameworks for general fam- ilies of loss functions, e.g., [22, 57]. We refer the interested reader to the surveys [2, 1, 50, 11, 4, 21] with references therein. Practical usage. Since a coreset is not just another solver that competes with existing solutions, but a data structure for approximating any given model in the family, we can apply existing approximation algorithms or heuristics on the coreset to obtain similar results compared to the original (full) data. Since the coreset is small, we may run these heuristics multiple times, or apply the hyperparameter tuning using the coreset [43], thus reducing the computational burden by orders of magnitude. Main challenges: (i) No coreset. Unfortunately, it is not clear at all that a small coreset exists for a given family of models. In fact, we can conclude from [54] that a coreset for decision trees does not exist in general; see details below. In this case, we can either give up on the coreset paradigm and develop a new solver, or add assumption on the input dataset, instead of targeting every possible dataset that may be very artiﬁcial and unrealistic, as the counter example in [54]. In this paper, we choose the latter option. (ii) Unlike, say, uniform sampling, every problem formulation requires a different coreset construction, which may take years of research to design. 1.2 First coreset for decision trees and their generalization In this paper, we tackle a generalized and more complex set of models than decision trees, where, rather than a recursive binary partition, we allow the input feature space Rd to be partitioned into any kdisjoint axis-parallel hyper-rectangles; see Fig. 1. This generalization is essential in order to support future non-recursive and not necessarily binary classiﬁcation models, e.g., ID3 and C4.5 [30, 51]. To our knowledge, this is the ﬁrst coreset with provable guarantees for constructing decision trees. Deﬁnition 1 (k-segmentation). For an integerd≥1 that denotes the dimension of the feature space A= Rd, and an integer k≥1 that denotes the size of the partition (number of leaves), a function s : A →R is a k-segmentation if there is a partition B= {B1,··· ,Bk}of A into k disjoint axis-parallel hyper-rectangles (blocks), such that |{s(b) |b∈B}|= 1 for every block B ∈B, i.e., sassigns a unique value for all the entries in each of its krectangles; see Fig. 1. We deﬁne the union over all possible such k-segmentations by SEG(k,d). We now deﬁne our loss function, and an optimal k-segmentation model over some set A⊆Rd. Deﬁnition 2 (Loss function). For a dataset D = {(x1,y1),··· ,(xN,yN)}⊆ A×R, an integer k≥1, and a k-segmentation s∈SEG(k,d), we deﬁne the sum of squared error (SSE) loss ℓ(D,s) := ∑ (x,y)∈D (s(x) −y)2 as the loss of ﬁtting sto D. A k-segmentation s∗is an optimal k-segmentation of Dif it minimizes ℓ(D,s) over every k-segmentation s∈SEG(k,d) i.e., s∗∈arg mins∈SEG(k,d) ℓ(D,s). The optimal SSE loss is denoted by optk(D) := ℓ(D,s∗). For example, the optimal 1-segmentation s∗of Dis the constant function s∗≡ 1 |D| ∑ (x,y)∈Dysince the mean of a set of numbers minimizes the sum of squared distances to the elements of the set. Also, opt|D|(D) = 0 for every dataset D. We are now ready to formally deﬁne a coreset for the k-segmentation problem (and k-decision trees of at most kleaves, in particular). Deﬁnition 3 (Coreset). Let D= {(x1,y1),··· ,(xn,yn)}⊆ A×R be an input dataset. Let k≥1 be an integer and ε ∈(0,1) be the desired approximation error. A (k,ε)-coreset for Dis a data structure (C,u) where C ⊆A×R is an ordered set, and u : C →[0,∞) is called a weight 3function, such that (C,u) sufﬁces to approximate the loss ℓ(D,s) of the original dataset D, up to a multiplicative factor of 1 ±ε, in time that depends only on |C|and k, for any k-segmentation s. Practical usage. As deﬁned above and discussed in Section 1.1, a coreset approximates every model in our set of models SEG(k,d). Hence, a coreset for decision trees is clearly also a coreset for forests with an appropriate tuning for k, since every tree in the forest is approximated independently by the coreset. We expect that applying existing heuristics (not necessarily with provable guarantees) such as sklearn [49] or LightGBM [37] on the coreset, would yield similar results compared to the original data. Indeed, our experimental results in Section 5 validate those claims. No coreset for general datasets. Unfortunately, even for the case of k= 4 and A⊆R, i.e., when the input is simply a one dimensional dataset D= {(x1,y1),··· ,(xn,yn)}where x1,··· ,xn are real numbers, and the labels y1,··· ,yn ∈{0,1}have only binary values, it is easy to construct datasets which have provably no k-segmentation (or even k-tree) coreset of size smaller than n; see e.g. [54]. Hence, there is no non-trivial decision tree coreset for general datasets of nvectors in any dimension. However, as we prove in the rest of the paper, a coreset does exist for datasets where the input is a matrix, i.e., a discrite signal where every coordinate in the domain is assigned a label (value), rather than a random set of nvectors. The ﬁrst coreset for n×m-signals. To overcome the above problem, while still obtaining a small coreset, we assume a discretization of the dataset so that every coordinate has a label. We also assume, mainly for simplicity and lack of space, that the input feature space is A= [n] ×[m] ⊆R2. That is, the input can be represented by an n×mmatrix. The output coreset may contain fraction of entries, as in Fig. 3, which is called an n×msignal; see Section 1.5. Our assumption on the input data seems to be the weakest assumption that can enable us to have a provably small coreset for any input. Furthermore, it seems natural for e.g. images, matrices, or any input data from sensors (such as GPS) that has a value in every cell or continuous in some other sense. Previous work. The prior works [ 54, 24, 62], which only handle the case of segmenting a 1- dimensional signal, use relaxations similar to our relaxation above to obtain a coreset of sizeO(k/ε2). However, our results apply easily for the case of vectors ( 1-dimensional signals) as in [ 54] and generalize for tensors if d≥3. We also give further applications, and provide extensive experiments with popular state of the art software. A special case for d = 2 includes image compression, where quadtrees are usually used in e.g. MPEG4 to replace the image by smooth blocks of different sizes [55], or for completion of missing values [58] Using dynamic programming, it is easy to compute the optimal tree of a 2D-signal Din O(k2n5) time [5], which is impractical even for small datasets,unless applied on a small coreset ofD. However we do not know of any such coreset construction, ford≥2, with provable guarantees on its size. To this end, the following questions are the motivation for this paper:(i): Is there a small coreset for any n×msignal (e.g. of sub-linear size)? (ii): If so, can it be computed efﬁciently? (iii): Can it be used on real-world datasets to boost the performance of existing random forest implementations? Extensions. For simplicity, we focus on the classic sum of squared distances (SSE) or the risk minimization model [ 59]. However, our suggested techniques mainly assume that a coreset for the case k = 1 is known, which is trivial for SSE, but exists for many other loss functions e.g., non-squared distances; see Section 6. 1.3 Our Contribution For any given error parameter ε∈(0,1), and an integer k≥1, this paper answers afﬁrmatively the above three questions. More formally, in this paper we provide: (i): A proof that every n ×m signal D has a (k,ε)-coreset (C,u) of size |C|polynomial in klog(nm)/ε. To our knowledge, this is the ﬁrst coreset for decision trees whose size is smaller than the input; see Theorem 8. Due to lack of space, our full proofs are given in the appendix. (ii): A novel coreset construction algorithm that outputs such a coreset (C,u) with the above guarantees, for every given input signal D. Its running time is O(nmk), i.e., linear in the input size |D|. Unlike common coreset constructions, our algorithm is deterministic; see Algorithm 3. (iii): Experimental results that apply modern solvers, such as the sklearn and LightGBM libraries, on this coreset for real-world public datasets. We measured the trade-off between the empirical accuracy of the resulting forests, and the coreset size; see Section 5. (iv): AutoML for decision trees: since the suggested coreset approximates every tree of at most k 4leaves, we may use the same coreset for hyperparameter tuning. We demonstrate this in Section 5, by calibrating the parameter kusing only the coreset, as compared to using the original (big) data. (v): Open source code for our algorithms [35]. We expect that it will be used by both the academic and industry communities, not only to improve the running time of existing projects, but also to extend the algorithms and experimental results to other libraries and cost functions; see Section 6. 1.4 Novel technique: partition trees meet decision trees In a seminal paper [28] during the 80’s of the previous century, Haussler and Welz introduced the importance of VC-dimension by Vapnik–Chervonenkis [60]. Their main application was partition trees for answering range queries. Informally, a partition tree of a given set of points on the plane is the result of computing recursively a simplicial partition which is deﬁned as follows. For a set Dof N points on the plane, a (1/ε)- simplicial partition is the partition of D into O(1/ε) subsets, such that: (i) each subset has at most 2εN points, and (ii) Every line in the plane intersects the convex hull of at most √ 1/εsets. Answering range queries of the form “how many points in Dare in a given rectangular” in sub-linear time, using partition trees, is straightforward: We can sum in O(1/ε) time the number of points in the subsets of the above simplicial partition that are not intersected by the query rectangular. We then continue recursively to count the points on each of the √ 1/εintersected sets. In other words, the main idea behind the above work is to partition the input into a (relatively small) number of subsets, each containing a fraction of the input, such that each query (in this case, a rectangular shape) might intersect only a small fraction of those subsets. Such a partition is termed a simplicial partition. The number of points contained in non-intersected subsets can be easily computed, while the sum of points in intersected subsets require a more involved solution. The novelty in that work is how to achieve such a partition of the input. Our paper closes a loop in the sense that it forges links between decision trees in machine learning – to partition trees from computational geometry. We aim to generalize the above technique from covering problems to regression and classiﬁcation problems. This is by devising an algorithm which achieves the above requirements, but where the query is a decision tree (and not a rectangular shape), and the cost function is the sum of squared distances to the query and not the number of points. More precisely, We partition the input datasetDinto a relatively small number of subsets, such that every possible decision tree (query) intersects at most few of these subsets. We then independently compress every subset via another novel algorithm such that the cost (sum of squared distances, in this case) of points contained in non-intersected subsets can be easily and accurately estimated, while the cost of points in intersected subsets can be provably approximated via a more involved calculation. This is very different from existing coreset techniques that are sampling-based [38], or utilize convex optimization greedy algorithms [16]. Our main challenge was to deﬁne and design such a “simplicial partition for sum of squared distances”, and the coreset to be computed for each subset in this partition. 1.5 Preliminaries In this section we deﬁne the notation that will be used in the next sections. Let n ≥ 1 and denote [n] = {1,··· ,n}. An n-signal is a set {(x,f(x)) | x ∈ [n]}that is deﬁned by a function f : [n] →R (known as the graph of f). For an additional integer m ≥1, an n×m signal D = {(x,g(x)) | x ∈ [n] ×[m]}is the set that corresponds to a function g : [n] ×[m] →R. That is, D represents an n×mreal matrix whose size is |D|= N = nm. For integers i1,i2,j1,j2 such that 1 ≤i1 ≤i2 ≤nand 1 ≤j1 ≤j2 ≤m, an n×msub-signal is the set B = { (x,g(x)) |x ∈{i1,··· ,i2}×{ j1,··· ,j2} } ⊆D; see Fig. 1. A sub-signal B is called a row (respectively, column) if i1 = i2 (respectively, j1 = j2). For a sub-signal B, we denote by BT = {((j,i),y) |((i,j),y) ∈B}the transposed sub-signal. A k-segmentation s is said to intersect an n×msub-signal Bif sassigns at least two distinct values to the entries of B, i.e., |{s(x) |(x,y) ∈B}|≥ 2. Furthermore, by deﬁnition, a k-segmentation sinduces a partition of an n×m sub-signal B into at most k n×m sub-signals. For two functions f,g : R →R we use the big Onotation f(x) ∈O(g(x)), thinking of O(g(x)) as the class of all functions h(x) 5such that |h(x)| ≤c|g(x)|for every x > x0, for some constants c and x0. Lastly, we denote SEGk := SEG(k,2) for brevity. Paper organization. Section 2 provides a rough approximation to the k-segmentation problem. Section 3 provides an algorithm for computing a simplicial partition for the k-segmentation problem. Each region in this partition will be then compressed individually in Section 4 to obtain our desired coreset. Experimental results and discussions are given in Section 5, and a conclusion in Section 6. 2 Bi-criteria Approximation A coreset construction usually requires some rough approximation to the optimal solution as its input. Unfortunately, we do not know how toefﬁciently compute even a constant factor approximation to the optimal k-segmentation problem in Deﬁnition 2, as explained in Section 1. Instead, we provide an (α,β)k or bi-criteria approximation [22], where the approximation is with respect to a pair of parameters: the number of segments in the partition may be up to βk instead of k, and the loss may be up to α·optk(D) instead of optk(D). Deﬁnition 4 ((α,β)k-approximation.). Let D be an n×msub-signal, k ≥1 be an integer and let α,β > 1. A function s : [ n] ×[m] → R is an (α,β)k-approximation of D, if s is a βk- segmentation whose ﬁtting loss to Dis at most αtimes the loss of the optimal k-segmentation of D, i.e., s∈SEG(βk) and ℓ(D,s) = ∑ (x,y)∈D(s(x) −y)2 ≤α·optk(D). We now describe an algorithm that computes such an approximation, in time only linear in the input’s size |D|= nm. The following lemma gives the formal statement. A suggested implementation for the algorithm is given in the appendix, as well as the full proof of the lemma; see Section B. Lemma 5. Let D= {(x1,y1),··· ,(xnm,ynm)}be an n×msub-signal and k≥1 be an integer. Then, there is an algorithm that can compute, in O(knm) time, an (α,β)k-approximation for D, where α∈O(klog(nm)) and β ∈kO(1) log2 (nm). Overview of the bicriteria algorithm from Lemma 5: The algorithm is iterative and works as follows. At the ith iteration, we ﬁnd a collection Bi of at most tdisjoint sub-signals in Di (where D0 = Dis the input), for which: (i) ∑ B∈Bi opt1(B) ≤optk(Di) ≤optk(D), and (ii) ∪B∈BiB has size |∪B∈BiB|≥| Di|/cfor some parameter cthat depends on k, i.e., those sub-signals contain at least a 1/cfraction of Di. We then deﬁne Di+1 = Di \\∪B∈BiB. After repeating this for at most ψ∈O(clog(nm)) iterations, we end up covering all entries of Dwith sub-signals where the overall loss of the sub-signals in each iteration is at most optk(D). This deﬁnes a partition of D into a collection of at most tψdisjoint sets B′, which, in turn, deﬁne a set of at most (tψ)2 distinct sub-signals. The output is now simply the function sthat assigns, for every B ∈B′and b∈B, the mean value s(b) = 1 |B| ∑ ((i,j),y)∈Byof B. See Pseudo-code in Algorithm 4 at the appendix. 3 Balanced Partition In this section we present Algorithm 2, which computes a partition similar to the simplicial partition described in Section 1.4; It computes, in O(|D|) time, a partition Bof the input D that satisﬁes the following properties: (i) |B|depends on k/ε but independent of |D|, (ii) the loss opt1(B) of every B ∈B is small, and (iii) every k-segmentation sintersects only few sub-signals B ∈B; see Deﬁnition 6, Fig. 2, and Lemma 7. A full proof is given at the appendix; see Section C. Deﬁnition 6 (Balanced Partition). Let Dbe an n×msignal, k≥1 be an integer, andc1,c2,c3 >0. A (c1,c2,c3)k-balanced partition of Dis a partition Bof Dsuch that: (i) Bcontains |B|≤ c1 n×m sub-signals, (ii) opt1(B) ≤c2 for every B ∈B, and (iii) every k-segmentation ˆsintersects at most c3 sub-signals B ∈B (i.e., assigns more than one unique number to those sub-signals). Lemma 7. Let D be an n×msignal, k ≥1 be an integer, ε ∈(0,1/4) be an error parameter, and s : [n] ×[m] →R be an (α,β)k-approximation of D, where α,β >1. Deﬁne σ := ℓ(D,s) α and γ := ε2 βk. Let Bbe an output of a call to PARTITION (D,γ,σ ); see Algorithm 2. Then Ban( O ( α γ2 ) ,γ2σ,O ( kα γ )) k -balanced partition of D. Moreover, Bcan be computed in O(nm) time. 6Figure 2: (Left): A 6 ×5 signal D consisting of 6 rows R1,··· ,R6. (Middle): A step by step illustration of the call to B := PARTITION (D,1/4,64) which partitions D into |B| = 13 sub- signals (sub-matrices) as follows. (1) B′ := SLICE PARTITION ({R1},4) (top green row). (2) B′:= SLICE PARTITION (R1 ∪R2,4) (middle green matrix), and so on as long as the output contains at most |B′|≤ 1/γ = 4 sub-signals (as in the bottom green matrix (3)). We then append B′to the output B, and repeat with the remaining {R4,R5,R6}. (4) B′:= SLICE PARTITION ({R4},4) which already returns |B′|= 5 >1/γsignals (yellow matrix). We append them to Band repeat. (Right): The ﬁnal partition B, where opt1(B) ≤γ2σ= 1/42 ·64 = 4 for every B ∈B. Overview of Algorithms 1 and 2. Algorithm 2 gets as input an n×m-signal Dand two parameters σ,γ. Algorithm 2 aims to compute a balanced partition of D; see Fig. 2. In turn, it calls Algorithm 1, which takes as input an n×msub-signal Rthat is deﬁned by several contiguous rows of the original dataset D, and a parameter σ >0, and aims to compute a partition Bof R. To do so, Algorithm 1 partitions Ralong the vertical dimension (e.g., into vertical slices), in a greedy fashion, such that for every B ∈B, opt1(B) is as large as possible, while still upper bounded by σ. This will ensure that the partition is into a relatively small number of slices. In the case where one of the sub-signals Bin this vertical partition of Rcontains only one column, and already exceeds the maximum tolerance opt1(B) >σ, we recursively apply Algorithm 1 to BT in order to partition Bhorizontally. As long as the total number of slices returned by Algorithm 1 is smaller than 1/γ, Algorithm 2 adds yet another row to the previous set of rows, and repeats the above process. At this point, the partition of the current horizontal slice (collection of rows) Ris ﬁnal, and is added to the output partition of Algorithm 2. In turn, a new horizontal slice Rof just one row, the ﬁrst row of Dthat is not included in the previous R, is initiated on which we again call Algorithm 1. Algorithm 1: SLICE PARTITION (D,σ) Input : A parameter σ >0 and an n×m signal D= {(xi,yi)}N i=1. Output :A partition Bof D. 1 B:= ∅and cbegin := 1 2 while cbegin ≤mdo 3 B := {((i,j),y) ∈D|j = cbegin} // extract first column 4 if opt1(B) >σ then 5 B′:= SLICE PARTITION (BT,σ) 6 B:= B∪ { B′T |B′∈B′ } cbegin := cbegin + 1 7 else 8 cend := cbegin 9 while opt1(B) ≤σand cend <m do 10 cend := cend + 1 and lastB := B 11 B := {((i,j),y) ∈D|i∈[cbegin,cend]} 12 B:= B∪{lastB} 13 cbegin := cend 14 return B Algorithm 2: PARTITION (D,γ,σ ) Input : An n×msignal D, a parameter γ ∈(0,1), and a lower bound σ∈[0,optk(D)]. Output : A partition Bof D; see Lemma 7 1 B:= ∅and rbegin := 1 2 while rbegin ≤ndo 3 R:= {((i,j),y) ∈D|i= rbegin} // extract first row 4 B′:= SLICE PARTITION (R,γ2σ) 5 rend := rbegin 6 lastB′:= B′ 7 while |B′|≤ 1/γand rend <n do 8 rend := rend + 1 9 lastB′:= B′ 10 S := {((i,j),y) ∈D|i∈[rbegin,rend]} // extract a slice 11 B′= SLICE PARTITION (S,γ2σ) 12 B:= B∪lastB′ 13 rbegin := rend 14 return B 74 Coreset Construction In this section, we present our main algorithm (Algorithm 3), which outputs a (k,ε)-coreset for a given n×msignal D, the number of leaves k≥1, and an approximation error ε∈(0,1). Overview of Algorithm 3: The algorithm ﬁrst utilizes the (α,β)k-approximation from Section 2 to obtain a lower bound σ≤optk(D) for the optimal k-segmentation. It then computes, as described in Section 3, a balanced partition Bof D, where opt1(B) is small and depends on σ, for every B ∈B. Finally, it computes a small representation (CB,uB) for every B ∈B, and returns the union of those representations. Each such pair (CB,uB) satisﬁes: (i) |CB|= 4, and (ii) has the same weighted sum of values, weighted sum of squared values, and sum of weights, as B, i.e.,∑ (a,b)∈CB uB((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B(y|y2 |1); see Fig. 3. Such a representation can be computed using Caratheodory’s theorem, as explained in Section E of the supplementary material. Figure 3: (Left): A matrix representing of a 5 ×5 sub-signal Bwhere yis mapped into unique colors for every (x,y) ∈B. (Middle): A representative (coreset) pair (CB,uB) for Bwhere CB ⊆Bis a (small) subset and uB : CB →[0,∞) is a weight function. That is, the pair (CB,uB) satisﬁes∑ (a,b)∈CB uB((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B(y |y2 |1). (Right): A duplication of the coreset points according to their weight. We call the resulting pair a “smoothed” version of(CB,uB); see more details and formal deﬁnition in Section D of the supplementary material. Some intuition behind Algorithm 3: Consider some k-segmentation s. By the properties of the balanced partition Bof D, only a small number of sub-signals B ∈B are intersected by s, i.e., assigned at least 2 distinct values. For every non-intersected sub-signal B ∈B, the loss ℓ(B,s) is accurately estimated by the (coreset) pair (CB,uB). On the other hand, for every sub-signal B ∈B which is intersected by s, by the guarantees of the representation (CB,uB), the loss ℓ(B,s) will be approximated, using only (CB,uB), up to some small error that depends on opt1(B). However, again by the properties of B, we have that opt1(B) is small. Hence, using the union (C,u) of the representations we can approximate ℓ(D,s) as required. Furthermore, combining that |CB|∈ O(1) for every B ∈B with the fact that |B|is small yields that |C|is indeed small; see Theorem 8. Algorithm 3: SIGNAL -CORESET (D,k,ε ); see Theorem 8 Input : An n×msignal D, an integer k≥1, and an error parameter ε∈(0,1/4). Output :A (k,ε)-coreset (C,u) for D. 1 s:= an (α,β)k approximation of Dfor α∈O(klog(nm)) and β ∈kO(1) log2 (nm) ;see Lemma 5 for suggested implementation. 2 γ := ε2/(βk), σ:= ℓ(D,s) α and C := ∅ 3 B:= PARTITION (D,γ,σ ) // see Algorithm 2. 4 for every set B ∈B do 5 (CB,uB) := a (1,0)-coreset for B, (a zero error coreset for k= 1), such that CB ⊆B, |CB|= 4, and ∑ (a,b)∈CB uB((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B(y|y2 |1) this is done using Caratheodory’s theorem; see Corollary 17 in the appendix. 6 Replace each of the coordinates aof the 4 pairs (a,b) ∈Cwith one of the 4 corner coordinates of the pairs in B; see detailed explanation if the proof of Theorem 8. 7 C := C∪CB and u((a,b)) := uB((a,b)) for every (a,b) ∈CB. 8 return (C,u) Theorem 8 (Coreset). Let D = {(x1,y1),··· ,(xN,yN)}be an n×m signal i.e., N := nm. Let k ≥1 be an integer (that corresponds to the number of leaves/rectangles), and ε ∈(0,1/4) be an error parameter. Let (C,u) be the output of a call to SIGNAL -CORESET (D,k,ε/ ∆) for a 8sufﬁciently large constant ∆ ≥1; see Algorithm 3. Then, (C,u) is a (k,ε)-coreset for Dof size |C|∈ (klog(N))O(1) ε4 ; see Deﬁnition 3. Moreover, (C,u) can be computed in O(kN) time. Coreset size. While Theorem 8 gives a worst-case theoretical upper bound, this bound is too pessimistic in practice, as common in coreset papers [42, 34]. This phenomenon is well known for coresets; see discussion e.g., in [21, 53]. The reasons might include: worst-case artiﬁcial examples vs. average behaviour on structured real-world data, noise removing/smoothing by coresets, the fact that in practice we run heuristics that output a local minima (and not optimal solutions with global minimum), non-tight analysis (especially when it comes to constants), etc. Our experiments in Section 5 show that, empirically, the constructed coresets are signiﬁcantly smaller: for N := nm∼140,000, k= 1000, and ε= 0.2, Theorem 8 predicts, in the worst case, a coreset of size larger than the full dataset size N. However, such an εerror is obtained with a coreset of size at most 1% of the input; see Fig. 4. 5 Experimental Results We implemented our coreset construction from Algorithm 3 in Python 3.7, and in this section we evaluate its empirical results, both on synthetic and real-world datasets. More results are placed in the supplementary material; see Section A. Open-source code can be found in [35]. The hardware used was a standard MSI Prestige 14 laptop with an Intel Core i7-10710U and 16GB of RAM. Since our coreset construction algorithm does not compete with existing solvers, but improves them by reducing their input as a pre-processing step, we apply existing solvers as a black box on the small coreset returned by Algorithm 3. The results show that our coreset can boost, by up to x10 times, the running time and storage cost of common random forest implementations. Implementations for forests. We used the following common implementations: (i) the func- tion RandomForestRegressor from the sklearn.ensemble package, and (ii) the function LGBMRegressor from the lightGBM package that implements a forest of gradient boosted trees. Both functions were used with their default hyperparameters, unless states otherwise. Data summarizations. We consider the following compression schemes: (i): DT-coreset(D,k,ε ) - The implementation based on Algorithm 3. In all experiments we used a constant k= 2000 for computing the coreset, regardless of the (larger) actual kvalue in each test, since k= 2000 was sufﬁcient to obtain a sufﬁciently small empirical approximation error. Hence, the parameter εcontrols the trade-off between size and accuracy. (ii): RandomSample(D,τ) - returns a uniform random sample of size τ from D. In all tests τ was set to the size of the coreset DT-coreset(D,k,ε ) for fair comparison. Datasets. We used the following pair of datasets from the public UCI Machine Learning Reposi- tory [3], each of which was normalized to have zero mean and unit variance for every feature: (i): Air Quality Dataset [18] - contains n= 9358 instances and m= 15 features. (ii) Gesture Phase Segmentation Dataset [45] - contains n= 9900 instances and m= 18 features. The experiment. The goal was to predict missing entries in every given dataset, by training random forests on the available data. The test set (missing values) consists of 30% of the dataset, and was extracted from the input dataset matrix by randomly and uniformly choosing a sufﬁcient number of 5 ×5 patches in the input dataset, and deﬁning them as missing values. The ﬁnal loss of a trained forest is the sum of squared distances between the forest predictions for the missing values, and the ground truth values. To tune the hyperparameter k, we randomly generate a set Kof possible values for kon a logarithmic scale. Then, we either: (i) apply the standard tuning (train the forest on the full data, for each value in K, and pick the one with the smallest test set error), or (ii) compress the input (only once) into a small representative set, and then apply the standard tuning on the small, rather than the full, data. The experiment was repeated 10 times. All the results are averaged over all 10 tests; see Fig. 4. Discussion. While the size and accuracy of our coreset are independent of our exact implementation of Algorithm 3, the running time is heavily based on our naive implementation, as compared to the very efﬁcient professional Python libraries. This explains why most of the running time is still devoted to the coreset construction rather than the forest training. Nevertheless, even our simple implementation yielded improvements of up to x10 in both computational time and storage, for a relatively small accuracy drop of 0.03 in the SSE. Tuning more than one hyperparameter will result in a bigger improvement. Furthermore, Fig. 4 empirically shows that tuning a hyperparameter on the 9Figure 4: Experimental results. (Top): The X-axis is the compression size. For every compression size γ, hyperparameter tuning is applied on both the coreset and the uniform sample (which are both of size γ). A random forest is then trained, on the full data, using those tuned parameters. The Y-axis presents the test set SSE loss of the trained forests. (Bottom left): Hyperparameter tuning. For every different value of k(X-axis), a forest is trained using this parameter value either on the compression (of two different sizes) or on the full data. The Y-axis presents ℓ+ k/105, where ℓis the normal SSE loss of the trained forest on the test set. (Bottom right): Time comparison. The Y-axis presents the total running time of both to compute the compression and to tune the parameter kon the compression (out of 50 different values). Note that the bottom right ﬁgures measure the total time to tune the parameter kin the bottom left ﬁgures, but using many more compression sizes. The optimal obtained parameter was then used to train the random forest in the top ﬁgures. coreset yields a loss curve very similar to the loss curve of tuning on the full data. Lastly, we observe that, in practice, our coresets have size much smaller than predicted in the pessimistic theory. 6 Conclusions and Future Work While coresets for k-trees do not exist in general, we provided an algorithm that computes such a coreset for every input n×msignal. The coreset size depends polynomialy on klog(nm)/εand can be computed in O(nmk) time. Our experimental results on real and synthetic datasets demonstrates how to apply existing forest implementations and tune their hyperparameters on our coreset to boost their running time and storage cost by up to x 10. In practice our coreset works very well also on non-signal datasets, probably since they have “real-world” properties that do not exist in the artiﬁcial worst-case example from Section 1.2. An open problem is to deﬁne these properties. Moreover, while this paper focuses on the sum of squares distances loss, we expect that the results can be generalized to support other loss functions; see Section 1.2. Lastly, supporting high-dimensional data (tensors), instead of matrices, is a straightforward generalization that can be achieved via minor modiﬁcations to our algorithms. Due to space limitation we also leave this to future work. References [1] Pankaj K Agarwal, Graham Cormode, Zengfeng Huang, Jeff M Phillips, Zhewei Wei, and Ke Yi. Mergeable summaries. ACM Transactions on Database Systems (TODS), 38(4):1–28, 2013. [2] Pankaj K Agarwal, Sariel Har-Peled, Kasturi R Varadarajan, et al. Geometric approximation via coresets. Combinatorial and computational geometry, 52:1–30, 2005. [3] Arthur Asuncion and David Newman. Uci machine learning repository, 2007. [4] Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learning. arXiv preprint arXiv:1703.06476, 2017. [5] Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966. 10[6] Kristin P Bennett. Decision tree construction via linear programming. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 1992. [7] Jon Louis Bentley and James B Saxe. Decomposable searching problems i. static-to-dynamic transformation. Journal of Algorithms, 1(4):301–358, 1980. [8] Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees.Machine Learning, 106(7):1039– 1082, 2017. [9] Rafael Blanquero, Emilio Carrizosa, Cristina Molero-Rıo, and Dolores Romero Morales. Opti- mal randomized classiﬁcation trees. In Technical Report, 2018. [10] Casper Solheim Bojer and Jens Peder Meldgaard. Kaggle forecasting competitions: An overlooked learning opportunity. International Journal of Forecasting, 37(2):587–603, 2021. [11] Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for ofﬂine and streaming coreset constructions. arXiv preprint arXiv:1612.00889, 2016. [12] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001. [13] Constantin Carathéodory. Über den variabilitätsbereich der koefﬁzienten von potenzreihen, die gegebene werte nicht annehmen. Mathematische Annalen, 64(1):95–115, 1907. [14] Venkatesan T Chakaravarthy, Vinayaka Pandit, Sambuddha Roy, Pranjal Awasthi, and Mukesh Mohania. Decision trees for entity identiﬁcation: Approximation algorithms and hardness results. In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 53–62, 2007. [15] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794, 2016. [16] Kenneth L Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):1–30, 2010. [17] Michael B Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, pages 181–190, 2015. [18] Saverio De Vito, Ettore Massera, Marco Piga, Luca Martinotto, and Girolamo Di Francia. On ﬁeld calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario. Sensors and Actuators B: Chemical, 129(2):750–757, 2008. [19] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. Catboost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363, 2018. [20] Abhimanyu Dubey, Moitreya Chatterjee, and Narendra Ahuja. Coreset-based neural network compression. In Proceedings of the European Conference on Computer Vision (ECCV), pages 454–470, 2018. [21] Dan Feldman. Core-sets: Updated survey. Sampling Techniques for Supervised or Unsupervised Tasks, pages 23–44, 2020. [22] Dan Feldman and Michael Langberg. A uniﬁed framework for approximating and clustering data. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 569–578, 2011. [23] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant- size coresets for k-means, pca, and projective clustering. SIAM Journal on Computing , 49(3):601–657, 2020. [24] Dan Feldman, Cynthia Sung, and Daniela Rus. The single pixel gps: learning big data signals from tiny coresets. In Proceedings of the 20th international conference on advances in geographic information systems, pages 23–32, 2012. [25] Dan Feldman and Tamir Tassa. More constraints, smaller coresets: Constrained matrix approxi- mation of sparse big data. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 249–258, 2015. [26] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291–300, 2004. 11[27] Melissa A Hardy. Regression with dummy variables, volume 93. Sage, 1993. [28] D Haussler and E Welzl. Epsilon-nets and simplex range queries. In Proceedings of the Second Annual Symposium on Computational Geometry, SCG ’86, page 61–71, New York, NY , USA, 1986. Association for Computing Machinery. [29] Tin Kam Ho. Random decision forests. In Proceedings of 3rd international conference on document analysis and recognition, volume 1, pages 278–282. IEEE, 1995. [30] Badr Hssina, Abdelkarim Merbouha, Hanane Ezzikouri, and Mohammed Erritali. A comparative study of decision tree id3 and c4. 5. International Journal of Advanced Computer Science and Applications, 4(2):13–19, 2014. [31] Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. Advances in Neural Information Processing Systems (NeurIPS), 2019. [32] Piotr Indyk, Sepideh Mahabadi, Mohammad Mahdian, and Vahab S Mirrokni. Composable core-sets for diversity and coverage maximization. In Proceedings of the 33rd ACM SIGMOD- SIGACT-SIGART symposium on Principles of database systems, pages 100–108, 2014. [33] Ibrahim Jubran, Alaa Maalouf, and Dan Feldman. Overview of accurate coresets. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, page e1429, 2021. [34] Ibrahim Jubran, Murad Tukan, Alaa Maalouf, and Dan Feldman. Sets clustering. InInternational Conference on Machine Learning, pages 4994–5005. PMLR, 2020. [35] Jubran, Ibrahim and Sanches, Ernesto and Newman, Ilan and Feldman, Dan. Open source code for the algorithms presented in this paper, 2021. Link for open-source code. [36] Kaggle. Kaggle website. https://www.kaggle.com/. [37] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efﬁcient gradient boosting decision tree. Advances in neural information processing systems, 30:3146–3154, 2017. [38] Michael Langberg and Leonard J Schulman. Universal ε-approximators for integrals. In Proceedings of the twenty-ﬁrst annual ACM-SIAM symposium on Discrete Algorithms, pages 598–607. SIAM, 2010. [39] Hyaﬁl Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete. Information processing letters, 5(1):15–17, 1976. [40] Wei-Yin Loh. Classiﬁcation and regression trees. Wiley interdisciplinary reviews: data mining and knowledge discovery, 1(1):14–23, 2011. [41] Hanlin Lu, Ming-Ju Li, Ting He, Shiqiang Wang, Vijaykrishnan Narayanan, and Kevin S Chan. Robust coreset construction for distributed machine learning. IEEE Journal on Selected Areas in Communications, 38(10):2400–2417, 2020. [42] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture models at scale via coresets. The Journal of Machine Learning Research, 18(1):5885–5909, 2017. [43] Alaa Maalouf, Ibrahim Jubran, and Dan Feldman. Fast and accurate least-mean-squares solvers. In Advances in Neural Information Processing Systems, pages 8305–8316, 2019. [44] Alaa Maalouf, Ibrahim Jubran, Murad Tukan, and Dan Feldman. Faster pac learning and smaller coresets via smoothed analysis. arXiv preprint arXiv:2006.05441, 2020. [45] Renata CB Madeo, Clodoaldo AM Lima, and Sarajane M Peres. Gesture unit segmentation using support vector machines: segmenting gestures from rest positions. In Proceedings of the 28th Annual ACM Symposium on Applied Computing, pages 46–52, 2013. [46] Detlev Marpe, Thomas Wiegand, and Gary J Sullivan. The h. 264/mpeg4 advanced video coding standard and its applications. IEEE communications magazine, 44(8):134–143, 2006. [47] Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff. On coresets for logistic regression. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [48] Soliman Nasser, Ibrahim Jubran, and Dan Feldman. Autonomous toy drone via coresets for pose estimation. Sensors, 20(11):3042, 2020. 12[49] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit- learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. [50] Jeff M Phillips. Coresets and sketches. arXiv preprint arXiv:1601.00617, 2016. [51] J Ross Quinlan. C4. 5: programs for machine learning. Elsevier, 2014. [52] Lior Rokach and Oded Maimon. Decision trees. In Data mining and knowledge discovery handbook, pages 165–192. Springer, 2005. [53] Frédéric Ros and Serge Guillaume. Sampling techniques for supervised or unsupervised tasks. Springer, 2020. [54] Guy Rosman, Mikhail V olkov, Danny Feldman, John W Fisher III, and Daniela Rus. Coresets for k-segmentation of streaming data. In Neural Information Processing Systems Foundation, 2014. [55] Eli Shusterman and Meir Feder. Image compression via improved quadtree decomposition algorithms. IEEE Transactions on Image Processing, 3(2):207–215, 1994. [56] sklearn. sklearn user guide - decision trees. https://scikit-learn.org/stable/ modules/tree.html. [57] Morad Tukan, Alaa Maalouf, and Dan Feldman. Coresets for near-convex functions. Advances in Neural Information Processing Systems, 33, 2020. [58] Bhekisipho Twala. An empirical comparison of techniques for handling incomplete data using decision trees. Applied Artiﬁcial Intelligence, 23(5):373–405, 2009. [59] Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural information processing systems, pages 831–838, 1992. [60] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of complexity, pages 11–30. Springer, 2015. [61] Sicco Verwer and Yingqian Zhang. Learning optimal classiﬁcation trees using a binary lin- ear program formulation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 33, pages 1625–1632, 2019. [62] Mikhail V olkov, Guy Rosman, Dan Feldman, John W Fisher, and Daniela Rus. Coresets for visual summarization with applications to loop closure. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 3638–3645. IEEE, 2015. 13Figure 5: The blobs dataset. The dataset D was generated using the function sklearn.datasets.make_blobs, and contains n= 17,000 points clustered into 3 clusters (con- taining 8500,5800, and 2700 points), each with a different label. A coreset (C,u) was constructed using Algorithm 3. From top down, the rows illustrate: (i) The input dataset D, (ii) The balanced partition of D, including the number of sets in the partition (iii) The weighted coreset points. Each point (x,y) ∈Cis plotted at location x, colored according to its label y, and its radius is proportional to its weight u(x,y). The percentage of the coreset size relative to the full data is presented. (iv) The unweighted coreset points. Each point (x,y) ∈C is plotted at location x, colored according to y, and has a ﬁxed radius. The percentage of the coreset size relative to the full data is presented. (v) The partition of the space via a decision tree computed using a call to DecisionTreeRegressor from the sklearn.tree package, where the input was the weighted coreset points only. Each region is assigned a color according to the label assigned to it by the computed tree. (vi) Similar to Row (v), but where the decision tree is trained on the full data. Algorithm 5 was used during training of the decision tree to evaluate the loss of each model. A Additional Experiments In this section, we present some additional experiments conducted using our algorithm from Sec- tions 3-4. We give visual illustration both of our coreset itself and of the result of applying a very common decision tree implementation on the coreset, as compared to running the same function on the original (full) data; see Fig. 5-6. Discussion. Visual representation. As seen in Fig. 5-6, the balanced partition in the second row partitions the input data into multiple subsets, where, as expected, ﬂat and relatively smooth regions are partitioned into a smaller number of large cells, while more complex regions are partitioned into a 14Figure 6: The moons dataset. The dataset was generated using the function sklearn.datasets.make_moons. The dataset contains n = 24 ,000 points spread across two interleaving half circles (12,000 points for each half circle), each with a different label. See caption of Fig. 5 for a detailed explanation about the rows. larger number of ﬁner cells. This is expected since the balanced partition insures a small variance inside each cell. Furthermore, as seen in the third row of the above ﬁgures, the weighted coreset contains a small number of “large” circles (points with large weight) in the ﬂat and relatively smooth regions, while it contains large number of “small” circles (points with small weight) in the more complex regions of the input. Accuracy. As seen in the last two rows of Fig. 5-6, the decision tree trained only on the coreset points resembles the decision tree trained on the full data, even for coresets of size only 6%, 8%, and 14% of the input data, as seen in Fig. 5,6, and 7 respectively. This implies a x10 faster training time of a decision tree (or, similarly, a forest) on a given coreset, compared to training it on the full data, with almost no compromises to the accuracy. The difference in the coreset size required in order to accurately represent the full data depends on the complexity of the input dataset. Indeed, the dataset in Fig. 5 is a much simpler dataset for a decision tree to classify, compared to the dataset in Fig. 7. Hence, the coreset sizes required in Fig. 5 are smaller than the ones in Fig. 7. 15Figure 7: The circles dataset. The dataset was generated using the function sklearn.datasets.make_circles. The dataset contains n= 26,000 points spread across a big circle (14,000 points) and a small circle (12,000 points), each with a different label. See caption of Fig. 5 for a detailed explanation about the rows. B Bi-criteria Approximation Notations. A sub-signal B is said to be horizontally intersected by a k-segmentation function s if there are ((i1,j1),y1),((i2,j2),y2) ∈Bwhere i1 ̸= i2 such that s(i1,j1) ̸= s(i2,j2). Similarly, a block Bof Dis said to be vertically intersected by sif there are ((i1,j1),y1),((i2,j2),y2) ∈B where j1 ̸= j2 such that s(i1,j1) ̸= s(i2,j2). B is said to be intersected by s if B is either horizontally or vertically intersected, i.e., |{s(x) |(x,y) ∈B}|>1. A set of sub-signals Bis said to be horizontally (vertically) intersected by sif it contains a sub-signal B ∈B that is horizontally (vertically) intersected by s. Also, we might abuse notation and denote by signal (sub-signal) an n×msignal (sub-signal) and by k-segmentation an n×mk-segmentation. In this section we give a constructive proof for Lemma 5. A suggested implementation for this constructive proof is given in Algorithm 4. We ﬁrst prove a small technical observation (see Observation 9), and then we prove Lemma 10, which will be used throughout the proof of Lemma 5. Observation 9. Let Aand Bbe two n×msub-signals. Then it holds that opt1(A∪B) ≥opt1(A) + opt1(B). 16Algorithm 4: BICRITERIA (D,k); Lemma 5 Input : An n×msub-signal D= {(xi,yi)}N i=1 and an integer k≥1. Output :An (α,β)k-approximation for D. 1 B:= ∅ 2 ν,γ := sufﬁciently large constants // see proof of Lemma 5 3 while |D|>k log N do 4 if Dcontains a row Rwith |R|≥ |D| νk then 5 Partition [m] into t′= γk intervals [m] = ∪t′ j=1Ij such that every j ∈[t′], the size of each corresponding sub-signal Rj = {((x1,x2),y) ∈R|x2 ∈Ij}is |Rj|∈ { |R| t′ −1,|R| t′ + 1 } . // e.g., by a greedy pass over [m]. 6 B:= the set of t′−2ksignals Rj with the smallest opt1(Rj). 7 else 8 Partition [n] into ψintervals [n] = ∪ψ j=1Ij such that for every j ∈[ψ], the size of each corresponding sub-signal Dj = {((x1,x2),y) ∈D|x1 ∈Ij}is |D| νk ≤|Dj|≤ 2|D| νk . // e.g., by a greedy pass over [n]. 9 if at least ψ/2 of the sub-signals Dj do not contain a column colof size |col|≥ |Dj| 2(νk)2 then 10 Vertically partition each of the (at leastψ/2) sub-signals Dj into ψj sub-signals, each such sub-signal Bof size |Dj| 2(νk)2 ≤|B|≤ |Dj| (νk)2 , and let B′contain the union of all those sub-signals. e.g., via a greedy algorithm. 11 B:= the set of |B′|−4ν2k3 −2kψsignals B ∈|B′|with the smallest opt1(B). 12 else 13 B:= { C |Cis a column of Dj,|C|≥ |Dj| 2(νk)2 and j ∈[ψ] } 14 D:= D\\∪B∈B′ Band B:= B∪B 15 s(b) := 1/|B|∑ (x,y)∈Byfor every b∈Band B ∈B. 16 return s Proof. Let C = A∪B. Let µ= 1 |C| ∑ (x,y)∈C ybe the weighted mean of A∪B. By Deﬁnition of opt we have that opt1(A∪B) = ∑ (x,y)∈C (y−µ)2 = ∑ (x,y)∈A (y−µ)2 + ∑ (x,y)∈B (y−µ)2 ≥opt1(A) + opt1(B), where the ﬁrst derivation holds since the mean of a points minimizes the sum of squared distances to those points, and the last derivation is by the deﬁnition of opt1. Lemma 10. Let D= {(x1,y1),··· ,(xN,yN)}be an n×msub-signal and let k≥1 be an integer. Then, in O(N) time we can ﬁnd a set Bof |B|= t∈O(k3) mutually disjoint blocks with respect to D, for which (i) ∑ B∈Bopt1(B) ≤optk(D). (ii) |∪B∈BB|∈ Ω (N k ) . Proof. Let ν >50 be an arbitrary parameter and let γ ≥8 be a parameter that will be deﬁned later. We will prove Lemma 10 for t≤2ν3k3 and for |∪B∈BB|≥ N 8νk. We start with the simple 1-dimensional case, namely – we assume that m= 1. In this case, we just partition [n] into t′= γk consecutive intervals [n] = ∪t′ 1 Ej, such that each corresponding sub-signal Dj = {((a,b),y) ∈D|a∈Ej}, j ∈[t′] of Dhas equal share of elements in D(up to ±1), i.e., 17|Dj|∈{| D|/t′−1,|D|,|D|/t′+ 1}. This can be done by moving point by point along the elements of D, which are assumed to be sorted in ascending order according toafor every ((a,b),y) ∈D, and deﬁning a new interval at the ﬁrst moment the current interval contains more than |D|/t′elements of Dor just at the prior point. We then compute for each Dj, j∈[t′] the optimal opt1(Dj),and return as output the set Bcontaining the t= t′−2ksub-signals Dj with the smallest loss opt1(Dj), among all the t′sub-signals {D1,··· ,Dt′ }. To see what guarantees we get, note that the computation takes O(nm) = O(N) time to sort the elements ((a,b),y) of Daccording to a, since a∈[nm] is a bounded integer. Afterwards, the above greedy partition also takes O(nm) = O(N) time. Furthermore, |∪B∈BB|≥ (γ−2)k·|D| γk ≥γ−2 γ |D|≥ N 8νk. For γ ≥8 this proves Property (ii) above. Finally, any n×1 k-segmentation function intersects at most 2ksub-signals from {Dj}t′ j=1 (i.e., at most 2k sub-signals are assigned more than 1 distinct value via the k-segmentation function). Hence, the optimal k-segmentation s∗of Dintersects at most 2kof those sub-signals as well. This implies that at least t′−2k≥(γ−2)kof the intervals {Dj}t′ j=1 which are assigned 1 distinct value |{s(x) |(x,y) ∈Dj}|= 1. Hence, by Observation 9, we conclude that optk(D) ≥∑ B∈Bopt1(D) which veriﬁes the 1st item above. We remark that for the 1-dimensional case we can do much better (there is an overall (1 + ε)- approximation of the k-segmentation using logarithmic number of blocks), but this will not be used here. Another remark is that we have ignored the ±1 slack in the sizes above, making the actual part of Dthat is removed at least γ−2 γ |D|−t′. This is insigniﬁcant in the 1-dimensional case above, as for t= O(1) and for |D|≥ log nthis would be an insigniﬁcant fraction, while for smaller D, we can just use single point sub-signals. The 2-dim case : Consider a row R of D, say the i′th row R = {((a,b),y) ∈D|a= i}. We call a row Rof D r-heavy if |R|≥| D|/r, namely – Rcontains at least |D|/relements from D. Analogously, we deﬁne a column to be r-heavy. Assume ﬁrst that our Dcontains a νk-heavy row R. We choose Rand use it as in the 1-dimensional case. As explained above we can ﬁnd in Ra set of disjoint sub-signals Bcontaining γkblocks and for which the ﬁrst item above holds for optk(R) and in particular for optk(D), i.e., ∑ B∈Bopt1(B) ≤ optk(R) ≤optk(D). Further, using the above guarantees for the1-dim case, |∪B∈BB|≥ γ−2 γ ·|R|≥ γ−2 γ ·|D| νk . This proves 2nd item for this case (with γ ≥3). Otherwise, let ei = |Ri|where Ri = {((a,b),y) ∈D|a= i}. By our assumption ei ≤|D|/νk for every i ∈[n]. Our algorithm is essentially identical to the 1-dimensional case, on the 1-dim array L = ( e1,...,e n1 ) where we weight the ith element by ei. Namely, we ﬁnd a partition of Linto ψ contiguous subintervals E= {E1,··· ,Eψ}such that the corresponding sub-signals Dj = {((a,b),y) ∈D|a∈Ej}of Dare as equal as possible. By our assumption this could be done so that for any j ∈ψ, the number of elements in Dj is between |D| νk and at most 2|D| νk . This is since adding a new ‘point’ from the list to an existing interval may increase the sum by at most |D|/νk. this implies that νk/2 ≤ψ≤νk. Next we perform the above algorithm again onD1,··· ,Dψ with the intention to “vertically partition” each such Dj, i.e., split each Dj into sets according to the value bfor every ((a,b),y) ∈Dj (rather than considering the value aabove). Let r= 2ν2k2, we continue with the following case analysis: (i) at least a 1 2 -fraction of {D1,··· ,Dψ}contain no a r-heavy column, and (ii) at most a ψ/2 of {D1,··· ,Dψ}contain no a r-heavy column. Case (i): At least a 1 2 -fraction of {D1,··· ,Dψ}contain no a r-heavy column. Then we partition each set Dj with no r-heavy column into ψi sub-signals { D(1) j ,··· ,D(ψi) j } of nearly equal number of points, where the partition is applied onto the values bof every ((a,b),y) ∈Dj. By a reasoning similar to that above, each r/2 ≤ψi ≤r, and each such block B ∈ { D(1) j ,··· ,D(ψi) j } contains |Di|/r≤|B|≤ 2|Di|/r. Using the bounds on |Di|and rwe get |D| 2ν3k3 ≤|B|≤ 2|D| ν3k3 . In particular 18we conclude that the total number of such sub-signals is at most t′, and t′≤2ν3k3. On the other hand, t′≥ψ 2 |D| νk ·ν3k3 2|D| ≥ν3k3/8. Let B2 be the collection of these sub-signals. We choose for our output collection, B, the set of t′−z sub-signals B ∈B2 with the smallest opt1(B), for z= 2k(r+ ψ) ≤6k3ν2. We note that Bcontains tsub-signals, where t′−z ≤t ≤t′. Further, by the lower bound on tit follows that |∪B∈BB|≥ (t′−z) · |D| 2ν3k3 ≥(ν3k3/8 −6k3ν2) |D| 2ν3k3 ≥|D| 8 (1 −48/ν). This veriﬁes the 2nd item for this case. Finally, we note that any row of D is shared by at most r sub-signals of B, and each column of D is shared by at most ψ sub-signals of B. Hence, any k-segmentation function may intersect at most z = 2k(r+ ψ) sub-signals from B. Therefore, for any k-segmentation sthere are at least t′−zsub-signals in B2 which are not-intersected by s. By our deﬁnition of Bto be the set of t′−zsub-signals in B2 with the smallest loss, we obtain that the loss ℓ(D,s) is at least ∑ B∈Bℓ(B) proving the 1st item of the lemma for this case. Case (ii): At most ψ/2 of {D1,··· ,Dψ}contain no a r-heavy column, namely – at least ψ/2 of the Di have a r-heavy column. In this case we take the heavy column from each Di as its own sub-signal. We get a collection B1 of ψ1 ≥ψ/2 blocks. We now return as output the set Bof the ψ1 −2ksub-signals B ∈B1 with the smallest opt1(B). We note that number of blocks we output in this case is at most ψ≤νk. Note also that Bcontains at least ψ/2 blocks, it follows that |∪B∈BB|) ≥ψ 2 · |D| 2ν2k2 ≥|D| 8νk which proves the 2nd item in the lemma. Finally, note that any k-segmentation scan intersect at most 2kintervals from B1 (similarly to the 1-dim case). Hence, there are at least |B1|−2k= ψ1 −2ksub-signals in B1 that are not-intersected by s, which implies that its loss ℓ(D,s) is at least the sum ∑ B∈Bopt1(B), which proves the 1st item of this lemma. Remark: we did not optimise the parameter. A slightly better partition can be obtains (less blocks), but this is good enough for our purposes. Computational time: Note that the elements of the input Dcan be sorted in lexicographic order in O(nm) = O(N) time since the coordinates aand bfor every ((a,b),y) ∈Dare bounded integers. Then, a linear-time preprocessing can be applied to the input D to store some statistics, e.g., the number of elements in each non-empty row or column, and the index of the next non-empty row or column for every element in D. Afterwards, the above greedy partition also takes O(nm) = O(N) time. We now restate and prove Lemma 5 from Section 2. Lemma 11 (Lemma 5). Let D = {(x1,y1),··· ,(xN,yN)}be an n×m signal and k ≥1 be an integer. Then, in O(kN) time we can compute an (α,β)k-approximation for D, where α ∈ O(klog N) and β ∈O(kO(1) log2 N). The proof of the above claim is a constructive proof. A suggested implementation is provided in Algorithm 4. Proof. The top level idea of the algorithm is as follows. We suggest an iterative algorithm. We start the ﬁrst iteration with D1 = Dand, using Lemma 10, ﬁnd a collection of disjoint sub-signals B1 = {B1,··· ,Bt}in D1 (which will not necessarily cover the entire signal D1), such that: (i) the sum of their 1-segmentation loss satisﬁes, ∑t i=1 opt1(Bi) ≤optk(D1) = opt k(D), and (ii) ∪B∈B1 Bhas size |∪B∈B1 B|≥| D1|/cfor some c(e.g., c∈O(k) in the lemma). Let B1 be such a collection. We then ‘delete’ fromD1 the elements of ∪B∈B1 B, and set D2 = D1 \\∪B∈B1 B. In the ith iteration, we repeat the same process with respect to the current set Di. Namely, we ﬁnd a collection Bi of at most tdisjoint sub-signals in D, for which: (i) ∑ B∈Bi opt1(B) ≤optk(Di) ≤ optk(D), and (ii) ∪B∈BiBhas size |∪B∈BiB|≥| Di|/c, i.e., those blocks cover at least a constant fraction of Di. Repeating these iterative procedure for at most ψ = O(clog(nm)) times, we end up covering all entries of Dwith sub-signals where the overall loss of the sub-signals in each iteration is at most 19optk(D). This deﬁnes a collection of at most tψsub-signals Bthat cover the entire original set D. Hence, the total overall loss over those sub-signals is ∑ B∈B′ OPT1(B) ≤ψoptk(D). By deﬁning the output function ssuch that for every B ∈B and b ∈B, sassigns to bthe mean value of B, i.e., s(b) = 1/|B|∑ ((i,j),y)∈By, we obtain that ℓ(B,s) = opt1(B) for every B ∈B, and that the ℓ(D,s) ≤∑ B∈Bℓ(B,s) ≤ψoptk(D). While for every B ∈B sassigns the same value for every element b∈B, there is not necessarily a partition of n×minto |B|∈ O(tψ) distinct axis-parallel blocks that correspond to the sub-signals of B. Therefore, sis not necessarily a |B|-segmentation function. However, looking at all possible intersections of the sub-signals in B, it is easy to realize that the tψsub-signals in Bdeﬁne a partition of Dinto at most O(t2ψ2) sub-signals B′that indeed correspond to a distinct partition of n×minto |B′|distinct axis-parallel blocks. Hence, sis guaranteed to be a |B′|-segmentation function. The parameters that are guaranteed by the lemma are c ∈O(k), t ∈O(k3). This implies that β = |B′|∈ O(t2ψ2) = O(k8 log2 nm) and α= ψ∈O(klog nm). Computational time: By Lemma 10, each of the ψiterations above takes time linear in the input size. The input size in the i’th iteration isO(N((k−1)/k)i) since at each iteration we remove at least a 1/kfraction of the input. Hence, the total running time is the sum of the geometric series N ·∑ i∈ψ((k−1)/k)i ∈O(kN). C Balanced Partition In this section we give our full proof for Lemma 7. We ﬁrst prove the following lemma regarding the output of Algorithm 1. Lemma 12. Let Dbe an n×msub-signal, and σ >0 be a parameter. Let B= { B1,··· ,B|B| } be the output of a call to SLICE PARTITION (D,σ), where the sub-signals in Bare numbered according to the order in which each of them was added to B; see Algorithm 1. Then the following properties hold: (i) Bis a partition of D. (ii) opt1(B) ≤σfor every sub-signal B ∈B. (iii) If |B|>8kthen for any k-segmentation sthat does not horizontally intersect Dwe have that ℓ(D,s) ≥ ( |B| 4 −2k ) σ. (iv) Bcan be computed in O(|D|) time. Proof. We consider the variables deﬁned in Algorithm 1. Proof of (i): By construction it immediately follows that Bis a partition of D. Proof of (ii): Consider a sub-signal B ∈B. We prove (ii) for each of the following cases: Case (a): Bwas added to Bat Line 12, and Case (b): Bwas either added to Bat Line 6 Case (a): In this case, by the condition at Line 9, Bmust satisfy that opt1(B) ≤σ. Case (b): In this case, Bwas returned via a recursive call. Hence, this case holds trivially by Case (a) above. Therefore, (ii) above holds by combining Cases (a)–(b). Proof of (iii): Let t= |B|and assume for simplicity that tis an even number. Recall that the index of each sub-signal in Bindicates its order of insertion to B, i.e., B1 is the ﬁrst sub-signal that was inserted to Band Bt was the last such sub-signal to be inserted to B. Observe that each recursive call B′:= SLICE PARTITION (BT,σ) at Line 5 returns at least |B′|≥ 2 sub-signals. This is because the recursive call happens only when opt1(BT) = opt1(B) >σ, which can only happen if |{(i,j) |((i,j),y) ∈B}|> 1, i.e., BT exceeds the maximum tolerance, and can indeed be partitioned into sub-signals. Hence, there are at least t/4 distinct pairs of consecutive sub-signals Bi and Bi+1 that were both either computed via the recursive call or both were not computed via the recursive call. We now show that each such pair satisﬁes opt1(Bi ∪Bi+1) >σ. 20Consider a pair of consecutive sub-signalsBi and Bi+1 that were both not computed via the recursive call at Line 5. Let B′⊆Bi+1 contain the elements ((i,j),y) ∈Bi+1 with the smallest value of i over all elements of Bi+1. By the greedy partition loop at Line 9 we obtain that opt1(Bi ∪B′) >σ. We now have that opt1(Bi ∪Bi+1) ≥opt1(Bi ∪B′) >σ, where the ﬁrst inequality is by Claim 9. Consider a pair of consecutive sub-signals Bi and Bi+1 that were both computed via the recursive call at Line 5. Then, similarly to the previous argument, we obtain that opt1(Bi ∪Bi+1) >σ. Now, let sbe a k-segmentation that does not horizontally intersect B, i.e., it does not horizontally intersect any B ∈B. By the deﬁnition of s, there might be at most 2ksub-signals in Bwhich are vertically intersected by s. Hence, among the t/4 distinct consecutive pairs of sub-signals discussed above there are at least t/4 −2ksuch pairs that are not intersected by s. Since Bis a partition of D, we have that ℓ(D,s) is at least the sum of opt1(Bi ∪Bi+1) ≥σ, over the above t/4 −2knon-intersected pairs of sub-signals. Hence, ℓ(D,s) ≥ (t 4 −2k ) σ= (|B| 4 −2k ) σ. Proof of (iv): The greedy Algorithm 1 can be implemented so that it computes only O(|D|) oper- ations. The most costly operation is the computation of opt1(B) for some sub-signal B. We now argue that this can be computed in O(1) time. Let Bbe a sub-signal and let µB = 1/|B|∑ (x,y)∈By be its mean value. Observe that opt1(B) = ∑ (x,y)∈B (y−µB)2 = ∑ (x,y)∈B y2 + |B|·µB −2µB ∑ (x,y)∈B y. (1) By precomputing and storing some statistics at each of the signal’s elements, then the three terms on the right hand side of (1) can all be evaluated in O(1) time for any sub-signal B. Hence, the total running time of Algorithm 1 is linear in the input size. We now restate and prove Lemma 7 from Section 3. Lemma 13. Let Dbe an n×msignal, k ≥1 be an integer, ε ∈(0,1/4) be an error parameter, and s: [n] ×[m] →R be an (α,β)k-approximation of D. Deﬁne σ := ℓ(D,s) α and γ := ε2 βk. Then algorithm PARTITION (D,γ,σ ) outputs a partition Bof D that is an ( O ( α γ2 ) ,γ2σ,O ( kα γ )) k - balanced partition in O(|D|) time. Proof. To prove thatBis a ( O ( α γ2 ) ,γ2σ,O ( kα γ )) k -balanced partition as in Deﬁnition 6, we need to prove the following properties: (i) opt1 (B) ≤γ2σfor every B ∈B. (ii) Bis a partition of Dwhose size is |B|∈ O ( α γ2 ) . (iii) For every k-segmentation ˆsthere are O ( kα γ ) sub-signals B ∈B for which ˆsassigns at least 2 distinct values, i.e., |{ˆs(x) |(x,y) ∈B}|≥ 2. Proof of (i): Observe that the output set Bcontains the the union of multiple output sets B′ := SLICE PARTITION (·,γ2σ) computed via calls to Algorithm 1. By Property (ii) of Lemma 12, every sub-signal B ∈B′in such output set B′satisﬁes that opt1 (B) ≤γ2σ. Hence, Property (i) of Lemma 7 immediately holds. 21Proof of (ii): By the greedy construction it holds that Bis a partition of D. We now prove that the number of times that Line 12 was executed is t∈O(α/γ), i.e., the number of times we append a set of signals lastB′to Bis at most O(α/γ). Let B1,··· ,Bt denote the set of sub-signals lastB′in each of the texecutions of Line 12, i.e., B1 := lastB′at the ﬁrst time Line 12 was executed. Each such set is called a horizontal set. Recall that s ∈SEGβk is a βk-segmentation. First of all, by the deﬁnition of a βk-segmentation function there are at most 2βk sets among the horizontal sets B1,··· ,Bt which can be horizontally intersected by s. Consider two consecutive horizontal sets Bi,Bi+1 that are not horizontally intersected by s, and let H = ⋃ B∈Bi∪Bi+1 B be the union of all the sub-signals in Bi ∪Bi+1. We now argue that the loss ℓ(H,s) is at least O(γσ). Since Bi and Bi+1 are two different horizontal sets, by the greedy construction we know that their union H could have been partitioned via a call to E := SLICE PARTITION (H,γ2σ) into a setE= { E1,··· ,E|E| } of at least|E|≥ 1 γ blocks. By substituting D= H,k = βk,B= Eand σ = γ2σin Property (iii) of Lemma 12, for a βk-segmentation s, we have that ℓ(H,s) ≥ (|E| 4 −2βk ) γ2σ≥ ( 1 4γ −2βk ) γ2σ ≥ ( 1 4γ −βk 9ε2 ) γ2σ= ( 1 4γ − 1 9γ ) γ2σ ≥γσ/2, where the second derivation holds for ε∈(0,1/3), and the third derivation is by the deﬁnition of γ. Assume by contradiction that there are more than 2α γ such pairs of consecutive horizontal sets Bi,Bi+1, which are not horizontally intersected by s. The loss of those slices to swould be bigger than 2α γ ·γσ 2 = ασ= ℓ(D,s), which is a contradiction. Therefore, the number of pairs of consecutive horizontal sets, which are not horizontally intersected by s, cannot exceed O ( α γ ) . Observe that the total number of horizontal sets that can be intersected by sis at most 2βk. Hence, the total number of horizontal sets is at most m∈O (α γ + 2βk ) ∈O (α γ ) . (2) We now prove that the number of output cells is at most|B|∈ O ( α γ2 ) in two steps. In step (i) we consider the horizontal sets that contain more than one row of Dand show that they contain a total of O ( α γ2 ) sub-signals. In step (ii) we consider the horizontal sets that contain exactly one row of D and prove that they also contain a total of O ( α γ2 ) sub-signals. Step (i): By (2), the total number of horizontal sets is at most m ∈O(α γ). Therefore, the total number of horizontal slices that contain more than one row of Ais also at most O ( α γ ) . By the construction in Algorithm 2, each such horizontal set Bi with more than 1 row of Ais partitioned into at most 1 γ sub-signals. Hence, the total number of blocks in horizontal sets than contain more than one row of Dis at most O ( α γ ·2 γ ) = O ( α γ2 ) . Step (ii): Consider all the horizontal sets Bi which contain one row of D, and which have been partitioned into |Bi|≤ 2βk ≤1/γblocks. The total number of blocks in such horizontal slices is thus bounded by the maximum number of horizontal slices m∈O(α/γ) times 1/γfor a total of at most O(α/γ2) blocks. For the rest of this step, we assume that all horizontal sets Bi have |Bi|≥ 2βk. Let G⊆[t] contain the indices of the horizontal sets which contain exactly one row of D, and let i∈G. Observe that Bi was computed, at some point, via a call Bi := SLICE PARTITION (∪B∈BiB,γ2σ). Also, since the points ∪B∈BiBof Bi all have the same row index, observe that Bi cannot be horizontally intersected by s. Therefore, by substituting D= ∪B∈BiB,k = βk and σ= γ2σin Property (iii) of Lemma 12 22we obtain that ℓ(Bi,s) ≥ (|Bi| 4 −2βk ) ·γ2σ. (3) Furthermore, we have that α·σ= ℓ(D,s) ≥ ∑ i∈G ℓ(Bi,s) ≥ ∑ i∈G (|Bi| 4 −2βk ) ·γ2σ, (4) where the ﬁrst derivation is by the deﬁnition of σ, the second derivation holds since {(x,y) ∈B |B ∈Bi,i ∈G}⊆ D, and the third derivation is by (3). Rearranging terms in (4) concludes Step (ii) as ∑ i∈G |Bi|≤ 4α γ2 + 8 ∑ i∈G βk ≤4α γ2 + 8tβk ≤4α γ2 + 8αβk γ ∈O (α γ2 ) . Therefore, the total number of blocks in horizontal sets that contain exactly one row of Dis at most O ( α γ2 ) . Proof of (iii): By the properties above we have that: (i) there are at mostO(α/γ) horizontal sets, and (ii) each horizontal set Bi either contains at most O(1/γ) sub-signals, or all the points ((i,j),y) ∈B of all the blocks B ∈Bi have the same row index i. Let ˆsbe a k-segmentation. ˆscan horizontally intersect all the (at most) 1/γsub-signals of at most khorizontal sets, and can vertically intersect at most 1 block from each of the O(α/γ) horizontal sets. Hence, the total number of intersected sub-signals is O(kα/γ). Computational time: We now prove that Bcan be computed in O(|D|) time. The computational time of Algorithm 2 is dominated by the computational time of Line 11 where we partition a slice S. Using Algorithm 1 we can partition each such slice S in linear O(|S|) time; see Lemma 12. Therefore, the naive implementation, i.e. by calling Algorithm 1 for every slice S, will result in O(|D|2) overall time, since many rows of Dparticipate many times in such a call to Algorithm 1. However, we can implement Line 11 inO(m) time, rather than O(|S|) time, by preprocessing the input signal D, in linear time O(|D|), and storing some statistics for every element ((i,j),y) ∈D. For example, one can store the sum of values and squared values over all elements ((i′,j′),y′) where i′<i or j′<j . Using those values we can computeopt(B) in O(1) time for every sub-signalBof D. Now, using such statistics (and possibly more statistics), Line 11 can be implemented in O(m) time via a greedy algorithm that iterates over the points of the last row R= {((i,j),y) ∈D|i= rend} added to S(i.e. with no need to iterate over other elements of S). We leave the small details to the reader. D Coreset Construction In this section, we provide the proof of correctness for our main coreset construction algorithm presented in Algorithm 3; see Theorem 15. Furthermore, we provide an algorithm than gets as input a k-segmentation s, as well as a (k,ε)-coreset for some input dataset D, which was computed using Algorithm 3. The algorithm returns a (1 + ε)-approximation to the loss ℓ(D,s), in O(k|C|) time; see Algorithm 5 and full details in Lemma 14. In what follows, for an n×msub-signal Band a weight function u: B →[0,∞), we abuse notation and denote u((a,b)) by simply u(a,b) for (a,b) ∈B. Some intuition behind Algorithm 5. Given a (k,ε)-coreset (C,u) for an input dataset D = {(x1,y1),··· ,(xN,yN)}, and a k-segmentation s, the algorithm outputs a (1 +ε)-approximation to ℓ(D,s) in time that depends only on kand |C|. During the computation of (C,u) in Algorithm 3, a partition Bof Dwas computed. Then, for every set Bin the partition B, a representative pair (CB,uB) for Bwas computed and added to C. 23To approximate the loss ℓ(D,s), we will approximate individually ℓ(B,s) for every B ∈B, and return the sum of those losses. Therefore, we now consider a single set B ∈B, and consider the following two cases. Case (i) : sassigns the same value for all the elements of B. Then, by construction, it is guaranteed that ℓ(B,s) = ∑ (x,y)∈CB uB(x,y)(s(y) −y)2. Therefore, in this case, ℓ(B,s) will be accurately estimated using (CB,uB). Case (ii) : s assigns more than one unique value to the elements of B. In this case, if we ig- nore the computational time, we would ideally want to compute a “smoothed version” (S,w) of (CB,uB), as shown in Fig. 3 (see (9)- (11) below for formal details). Then, we would return the loss ∑ (x,y)∈Sw(x,y)(s(y) −y)2. However, computing (S,w) is not necessary, since there are many subsets of B in which all the elements x∈B have simultaneously the same label in S and are assigned the same value by s. Combining this with the fact that those subsets are of rectangular (simple) shape, we obtain that the loss over those subsets can be evaluated efﬁciently, as computed in Algorithm 5. Algorithm 5: FITTING -LOSS ((C,u),s); see Lemma 14 Input : A (k,ε)-coreset (C,u) which was returned from a call to SIGNAL -CORESET (D,k,ε/ ∆) in Algorithm 3, for some n×m-signal D, k≥1, ε∈(0,1) and a sufﬁciently large ∆ ≥1. A k-segmentation (or k-tree) s. Output :A (1 + ε)-approximation to the loss ℓ(D,s). 1 loss:= 0 2 for every 4 consecutive elements ˆC = {(ai,bi)}4 i=1 in Cdo 3 Denote by Bthe sub-signal that corresponds to C′. // By construction in Algorithm 3, the coordinates a of the 4 elements (a,b) ∈ ˆC are the corners of B. 4 z:= |{s(x) |(x,y) ∈B}| 5 if z= 1 // i.e., s does not intersect B 6 then 7 lossˆC := ∑ (x,y)∈CB uB(x,y)(s(x) −y)2. // note that s(x1) = s(x2) for every x1,x2 ∈B 8 else // In this case, s intersects B 9 Denote by Sthe partition that sinduces onto [n] ×[m]. // S contains |S|≤ k subsets of [n] ×[m]. 10 i:= 1 11 for every S′∈S do 12 Denote by ℓthe label that sassigns to the elements of S′i.e., s(x,y) = ℓfor every (x,y) ∈S′ 13 z:= |B∩S′|// The number of element in the intersection of the S′ and the subset of [n] ×[m] that is represented by C′. 14 lossˆC := 0 15 while z≥1 do 16 if u(ai,bi) ≤zthen 17 lossˆC := lossˆC + u(ai,bi) ·(ℓ−bi)2 18 u(ai,bi) := 0 19 z:= z−u(ai,bi) 20 i:= i+ 1 21 else 22 lossˆC := lossˆC + z·(ℓ−bi)2 23 u(ai,bi) := u(ai,bi) −z 24 z:= 0 25 loss:= loss+ lossˆC. 26 return loss 24Lemma 14. Let D = {(x1,y1),··· ,(xN,yN)}be an n ×m signal i.e., N := nm. Let k ≥1 be an integer, ε ∈(0,1/4) be an error parameter, and (C,u) be the output of a call to SIGNAL -CORESET (D,k,ε ) (see Algorithm 3). Let sbe a k-segmentation (in particular, a k-tree). Finally, let lossbe the output of a call to FITTING -LOSS ((C,u),s); see Algorithm 5. Then there is a sufﬁciently large constant ∆ ≥1 such that |ℓ(D,s) −loss|≤ ∆ε·ℓ(D,s). Moreover,losscan be computed in O(k|C|) time. Proof. We consider the variables deﬁned in Algorithm 5. First, consider a subset ˆCof Cfrom some iteration of the For loop at Line 2 of Algorithm 5, and let Bbe the sub-signal that corresponds to ˆC, as in Line 3. We now prove that the loss lossˆC computed in the same iteration of the For loop (i.e., at Lines 3- 25) satisﬁes the following claim. Claim 14.1. Let z = |{s(x) |(x,y) ∈B}| be the number of distinct values s assigns to the coordinates of B(as computed in Line 4). Then, lossˆC satisﬁes that { ℓ(B,s) = lossˆC if z = 1⏐⏐ℓ(B,s) −lossˆC ⏐⏐≤ε·ℓ(B,s) + O ( opt1(B) ε ) otherwise Proof. We prove Claim 14.1 using the following case analysis: (i) z= 1 and (ii) z≥2. Case (i): z= 1. We prove that ℓ(B,s) = lossˆC. Since the input coreset (C,u) was computed using Algorithm 3, we know that the set ˆC was computed at Line 5 of Algorithm 3, along with a weight function ˆu. Hence, the pair ( ˆC,ˆu) satisfy, by construction, the following property: ∑ (a,b)∈ˆC ˆu((a,b)) ·(b|b2 |1) = ∑ (x,y)∈B (y|y2 |1). (5) Now, for any constant ˆs∈R, we have that ∑ (a,b)∈ˆC ˆu(a,b)(b−ˆs)2 = ∑ (a,b)∈ˆC ˆu(a,b) ·b2 + ˆs2 ∑ (a,b)∈ˆC ˆu(a,b) −2ˆs ∑ (a,b)∈ˆC ˆu(a,b)b = ∑ (x,y)∈B y2 + ˆs2 ∑ (x,y)∈B 1 −2ˆs ∑ (x,y)∈B y = ∑ (x,y)∈B (y−ˆs)2, (6) where the second equality is by (5). Since z= |{s(x) |(x,y) ∈B}|= 1, there is a constant ˆs∈R such that ℓ(B,s) = ∑ (x,y)∈B (y−ˆs)2. (7) Hence, we have that ℓ(B,s) = ∑ (x,y)∈B (y−ˆs)2 = ∑ (a,b)∈ˆC ˆu(a,b)(b−ˆs)2 = lossˆC, where the ﬁrst derivation is by (7), the second derivation is by (6), and the last derivation is by the deﬁnition of lossˆC at Line 7 of Algorithm 5. 25Case (ii): z≥2. We prove that ⏐⏐ℓ(B,s) −lossˆC ⏐⏐≤ε·ℓ(B,s) + O (opt1(B) ε ) . We ﬁrst observe that, by the triangle inequality, for anya,b,c ∈R we have that ||a−c|2 −|b−c|2|= ||a−c|−|b−c||·(|a−c|+ |b−c|) ≤|a−b|·(2|a−c|+ |a−b|) = |a−b|2 + 2|a−c|·|a−b| = |a−b|2 + 2√ε|a−c|· |a−b|√ε ≤|a−b|2 + ε·|a−c|2 + |a−b|2 ε = ε·|a−c|2 + ( 1 + 1 ε ) ·(a−b)2, (8) where the second inequality holds since 2xy≤x2 + y2 for every x,y ∈R. Smoothed coreset. In Algorithm 3 we computed some small compression CB, along with a weights function uB, for every subset Bin the partition of the input. The size |CB|of this compression is a small constant, independent of the (potentially large) size of B. The pair (CB,uB) satisfy a set of properties, which we visually demonstrate via this “smoothed coreset” notion; see Fig. 3. Informally, the “smoothed version” of(CB,uB) is another pair (C′ B,u′ B), such that C′ B contains a duplication of the elements of CB. The number of duplications of every element cfrom CB is according to its weight uB(c). We now formally deﬁne a “smoothed version” of a pair( ˆC,ˆu). A pair (S,w) is said to be a smoothed version of the pair ( ˆC,ˆu) if it satisﬁes the following properties: (i) (S,w) has the same sum of weights, sum of labels, and sum of squared labels as( ˆC,ˆu), (ii) The set of coordinates {a|(a,b) ∈S} in Scovers the entire set of coordinates{x|(x,y) ∈B}of the original set B, with possible duplicates, and (iii) The sum of weights over all elements in Swith the same coordinate is 1. Formally, ∑ (a,b)∈S w((a,b)) ·(b|b2 |1) = ∑ (a,b)∈ˆC ˆu((a,b))(b|b2 |1), (9) {x|(x,y) ∈B}= {a|(a,b) ∈S}, (10) and ∑ (a,b)∈S:a=x w((a,b)) = 1 for every (x,y) ∈B. (11) In what follows, for every pair (S,w) which is a smoothed version of ( ˆC,ˆu), we prove the following two properties: We now prove the following two properties: ﬁrst, that |ℓ(B,s) −ℓ((S,w),s)|≤ ε·ℓ(B,s) + O (opt1(B) ε ) , (12) for every every pair (S,w) which is a smoothed version of ( ˆC,ˆu). Second, we need to prove there is some pair ( ˆS, ˆw) which is a smoothed version of ( ˆC,ˆu) that satisﬁes lossˆC = ℓ(( ˆS, ˆw),s), (13) where lossˆC is the loss computed at Lines 3- 25, using only the pair ( ˆC,ˆu) (i.e., at the current iteration of the outer-most For loop of Algorithm 5), without actually computing ( ˆS, ˆw)). Case (ii) then immediately holds by combining (12) and (13) above. A proof of (12). Let (S,w) be a pair which is a smoothed version of ( ˆC,ˆu). By deﬁnition of (S,w), we have that wsums to 1 over all (a,b) ∈Swith the same a, as in (11). Therefore, for every 26(x,y) ∈Bwe can rewrite the term (y−s(x))2 as ∑ (a,b)∈S:a=xw(a,b)(y−s(x))2. Now, deﬁne yB(x) = yfor every (x,y) ∈B. We therefore have that ℓ(B,s) = ∑ (x,y)∈B (y−s(x))2 = ∑ (x,y)∈B   ∑ (a,b)∈S:a=x w(a,b)  ·(y−s(x))2 = ∑ (x,y)∈S w(x,y)(yB(x) −s(x))2, (14) where the last equality holds by (10) and by simply combining the two sums. We now have that |ℓ(B,s) −ℓ((S,u),s)| = ⏐⏐⏐⏐⏐ ∑ (x,y)∈S w(x,y)(yB(x) −s(x))2 − ∑ (x,y)∈S w(x,y)(y−s(x))2 ⏐⏐⏐⏐⏐ (15) = ⏐⏐⏐⏐⏐⏐ ∑ (x,y)∈S w(x,y) · ( (yB(x) −s(x))2 −(y−s(x))2) ⏐⏐⏐⏐⏐⏐ ≤ ∑ (x,y)∈S w(x,y) ⏐⏐(yB(x) −s(x))2 −(y−s(x))2⏐⏐ (16) ≤ ∑ (x,y)∈S w(x,y) ( ε·(yB(x) −s(x))2 + ( 1 + 1 ε ) (yB(x) −y)2 ) (17) = ε· ∑ (x,y)∈S u(x,y) ·(yB(x) −s(x))2 + ( 1 + 1 ε ) ∑ (x,y)∈S w(x,y) ·(yB(x) −y)2 = ε·ℓ(B,s) + ( 1 + 1 ε ) ∑ (x,y)∈S u(x,y) ·(yB(x) −y)2, (18) where (15) is by combining (14) and the deﬁnition of ℓ, (16) holds since the sum of absolute values is greater or equal than the absolute value of a sum, (17) holds by substituting in (8) every term in the sum, and (18) is by (14). We now bound the rightmost term of (18). Let ˆs≡1/|B|∑ (x,y)∈Bybe a 1-segmentation function that returns the label mean of B. We have that∑ (x,y)∈S w(x,y) ·(yB(x) −y)2 ≤2 · ∑ (x,y)∈S w(x,y) · ( (yB(x) −ˆs(x))2 + (y−ˆs(x))2) (19) = 2 ∑ (x,y)∈S w(x,y) ·(yB(x) −ˆs(x))2 + 2 ∑ (x,y)∈S w(x,y) ·(y−ˆs(x))2 = 2 ·(ℓ(B,ˆs) + ℓ((S,u),ˆs)) (20) = 2 ·(ℓ(B,ˆs) + ℓ(B,ˆs)) (21) = 4 ·ℓ(B,ˆs) = 4 ·opt1(B) (22) where (19) is by the weak triangle inequality, (20) is by combining the deﬁnition of ℓwith (14), (21) holds by Case (i) above, and (22) holds since the label means minimizes its sum of squared differences to the labels. 27Figure 8: (Left): The pair ( ˆC,ˆu). (Middle): A 4-segmentation s, which in- duces a partition of [5] × [5] into 4 sets B = B1 ∪ B2 ∪ B3 ∪ B4 where B1 = {(1,1),(2,1),(3,1),(4,2),··· ,}, B2 = {(1,3),(2,3),(1,4),(2,4),··· ,}, B3 = {(3,3),(4,3),(5,3)}, B4 = {(3,4),(4,4),(4,5),(3,5),···}. (Right): A smoothed version ( ˆS, ˆw) of ( ˆC,ˆu). There can be more than one unique smoothed version for the same pair ( ˆC,ˆu); see Properties in (9)-(11). The pair ( ˆS, ˆw) is constructed by iterating over every set B ∈B. Every element in Bis assigned to a label from the labels of the 4-coreset points (8,17,21,22) as follows: If |B|> ˆu(li), then ˆu(li) elements of Bare assigned to l1 and |B|− ˆu(li) are assigned to li+1. If |B|≤ ˆu(li) then all the elements of Bare assigned to ˆu(li), and |B|is subtracted from ˆu(li), and so on. If ˆuassigns fractional weights, then some elements of Bmight be assigned to more than one label, as long as the sum of weights over every element in Bis 1. By construction, ( ˆS, ˆw) satisﬁes Properties (10)-(11). Hence, ( ˆS, ˆw) is a smoothed version of ˆC,ˆu). Computing ℓ(( ˆS, ˆw,s) can be trivially computed in time only O(k|ˆC|) (rather than O(n) where nis the size of the original data), since the sets in the partition Bcontain a duplication of a constant number of labels. Equation (12) now holds by combining (18) and (22). A proof of (13). To prove (13), in Fig. 8 we construct a smoothed version ( ˆS, ˆw) of ( ˆC,ˆu) which satisﬁes (13). Furthermore, by combining the construction of ( ˆS, ˆw) with the computation of lossˆC in Lines 9-25 of Algorithm 5, we obtain, as desired, that lossˆC = ℓ(( ˆS, ˆw),s). Claim 14.1 now holds by combining cases (i) and (ii) above. We now prove Lemma 14. Consider the construction of (C,u) in Algorithm 3. By deﬁnition and by Lemma 5, the function s′computed at Line 5 of Algorithm 3 is an O(k8 log2 nm)-segmentation and satisﬁes that ℓ(D,s′) ∈O(klog n·optk(D)) . By the last derivation, let cα be the smallest constant such that ℓ(D,s′) ≤cα ·klog n·optk(D). By the last inequality and deﬁnitions of σand αwe obtain that σ:= ℓ(D,s′) α ≤cαklog nm·optk(D) α = cαklog nm·optk(D) cαklog nm = optk(D). (23) Now consider the partition Bcomputed at Line 3 of Algorithm 3 via a call to PARTITION (D,γ,σ ). By Lemma 7, Bsatisﬁes that (i) opt1 (B) ≤γ2σfor every B ∈B. (ii) Bis a partition of Dwhose size is |B|∈ O ( α γ2 ) . 28(iii) There are O ( kα γ ) sub-signals B ∈B where |{s(x) |(x,y) ∈B}|>1. (iv) Bcan be computed in O(|D|) time. Consider the pair (CB,uB) computed at Line 5 of Algorithm 3 for some B ∈B. Now, consider the For loop at Line 2 of Algorithm 5. For every pair (CB,uB), there is an iteration of this For loop for which C′= CB. In this iteration, Algorithm 5 computes a loss lossCB that corresponds to (CB,uB). We can plug B, (CB,uB), and lossCB in Claim 14.1 to obtain that:{ ℓ(B,s) = lossCB if z = 1 |ℓ(B,s) −lossCB |≤ ε·ℓ(B,s) + O ( opt1(B) ε ) otherwise (24) Let B1 ⊆ Bcontain the set of sub-signals in B that are not intersected by s, i.e., B1 = {B ∈B|| s(B)|= 1|}, and let B2 = B\\B 1 be the set of sub-signals which are partially inter- sected by s. By Property (iii) above, |B2|∈ O (kα γ ) . (25) Furthermore, by combining (24) with Property (i) above, for every B ∈B2 we have that |ℓ(B,s) −lossCB |≤ ε·ℓ(B,s) + O (opt1(B) ε ) ≤ε·ℓ(B,s) + O (γ2σ ε ) . (26) In other words, the loss ℓ(B,s) of every sub-signal B ∈B2 is approximated by lossCB up to some small error. Hence, by summing over all B ∈B2 we obtain that∑ B∈B2 |ℓ(B,s) −lossCB | ∈ ∑ B∈B2 ( ε·ℓ(B,s) + O (γ2σ ε )) (27) ≤ε·ℓ(D,s) + O ( |B2|· γ2σ ε ) (28) ≤ε·ℓ(D,s) + O (kα γ ·γ2σ ε ) ≤ε·ℓ(D,s) + O (kαγ ε ·optk(D) ) (29) ≤ε·ℓ(D,s) + O(ε·optk(D)) (30) ∈O(ε·ℓ(D,s)), (31) where (27) follows from (26), (28) is by (25), (29) is by (23), (30) holds since kαγ ≤ε2, and (31) holds since optk(D) ≤ℓ(D,s) for every k-segmentation s. Furthermore, for every B ∈B1, by (24) we have that ℓ(B,s) = lossCB . Hence, by summing over every B ∈B1 we obtain that ∑ B∈B1 ℓ(B,s) = ∑ B∈B1 lossCB . (32) In other words, the loss ℓ(B,s) of ever sub-signal B ∈B1 is accurately estimated by lossCB . Algorithm 5 then outputs the sum of losses loss:= ∑ B ∈BlossCB . (33) We hence obtain that |ℓ(D,s) −loss|= ⏐⏐⏐⏐⏐ ∑ B∈B1 (ℓ(B,s) −lossCB ) + ∑ B∈B2 (ℓ(B,s) −lossCB ) ⏐⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐⏐ ∑ B∈B1 (ℓ(B,s) −lossCB ) ⏐⏐⏐⏐⏐+ ⏐⏐⏐⏐⏐ ∑ B∈B2 (ℓ(B,s) −lossCB ) ⏐⏐⏐⏐⏐∈O(ε·ℓ(D,s)), (34) 29where the ﬁrst derivation is by 33, second derivation is by the triangle inequality, and the last is by combining (32) and (31). By (34), there is a constant ∆ ≥1 such that |ℓ(D,s) −loss|≤ ∆εℓ(D,s). This concludes the proof of the claim in Lemma 14. Computational time: Line 2 of the Algorithm 5 is a loop with |C| |ˆC|iterations. Inside this loop: if z >1 line 7 is computed in O(|ˆC|), else line 11 is another loop with O(k) iterations, inside which the line 20 is executed at most |ˆC|times and line 24 can be executed only once because it results in z= 0 and in exiting from the while loop. In total the complexity of line 15 is O(|ˆC|), of line 11 is O(k|ˆC|) and of line 2 and the whole algorithm: O(k|C|) Space complexity: Algorithm 5 uses only constant amount of additional storage space because in each line of the algorithm only numeric variables are created and variables are reused inside the loops. Theorem 15 (Coreset). Let D = {(x1,y1),··· ,(xN,yN)}be an n×m signal i.e., N := nm. Let k ≥1 be an integer (that corresponds to the number of leaves/rectangles), and ε ∈(0,1/4) be an error parameter. Let (C,u) be the output of a call to SIGNAL -CORESET (D,k,ε/ ∆) for a sufﬁciently large constant ∆ ≥1; see Algorithm 3. Then, (C,u) is a (k,ε)-coreset for Dof size |C|∈ (klog(N))O(1) ε4 ; see Deﬁnition 3. Moreover, (C,u) can be computed in O(kN) time. Proof. To prove that (C,u) is a (k,ε)-coreset for D, we need to prove that for every k-segmentation s, (C,u) sufﬁces to approximate the loss ℓ(D,s), up to a multiplicative factor of 1 + ε, in time that depends only on |C|and k. Let sbe a k-segmentation and let loss≥0 be an output of a call to FITTING -LOSS ((C,u),s); see Algorithm 5. Then, by Lemma 14, losscan be computed in O(k|C|) time (i.e., in time that depends only on |C|and k), and provides, as required, a (1 + ε)-approximation to ℓ(D,s) as |ℓ(D,s) −loss|≤ ∆ · ε ∆ ·ℓ(D,s) = ε·ℓ(D,s). Hence, (C,u) is a (k,ε)-coreset for D. Line 1 of Algorithm 3 can be computed in O(k·|D|) time by Lemma 5. Line 3 of Algorithm 3 can be computed in O(|D|) time by Lemma 7. The loop at Line 4 can be computed in ∑ B∈BO(|B|) = O(|D|) time by Section E. Hence, the call SIGNAL -CORESET (D,k,ε ) can be implemented in O(k|D|) = O(kmn) time. Proof behind Line 6. We now prove that the replacements of the coordinates applied at Line 6 does not violate the correctness of the algorithm. Observe that replacing the coordinates of entries inside each cell, while keeping the same labels, does not affect the variance of this subset. Therefore, the cost of this cell, which is computed in Algorithm 5) and depends only on the labels, remains exactly the same. Space complexity: By construction, each pair (CB,uB) computed at Line 5 can be stored using only O(1) space. Hence, the concatenation (C,u) of the |B|pairs {(CB,uB) |B ∈B} can be stored using O(|B|) ∈O(α/γ2) = O(α(βk)2/ε4) = O ( kO(1) logO(1) nm ε4 ) space. E The Caratheodory Theorem Given a point p ∈Rd inside the convex hull of a set of points P ⊆Rd, Caratheodory’s Theorem proves that there is a subset of at most d+ 1 points in P whose convex hull also contains p. Theorem 16 (Caratheodory’s Theorem [13, 48]). Let P ⊆Rd be a (multi)set of npoints. Then in O(nd3) time we can compute a subset Q⊆P and a weights functions u: Q→[0,∞) such that: (i) Q⊆P, (ii) |Q|= d+ 1, (iii) ∑ q∈Qu(q) ·q= 1 n ∑ p∈P p, and (iv) ∑ q∈Qu(q) = n. 30Corollary 17. Let Dbe an n×msub-signal. Then, in O(|D|) time we can compute a weighted n×msub-signal (A,w) such that: (i) A ⊆D, (ii) |A|= 4, (iii) ∑ (a,b)∈A w(a,b) ·(b |b2 |1) = ∑ (x,y)∈D (y|y2 |1), and ∑ (a,b)∈A w(a,b) = |D|. Proof. Deﬁne the multi-set P = { (y|y2 |1) ∈R3 |(x,y) ∈D } . Now, substituting P in The- orem 16 yields that in O(n) time we can compute a subset Q ⊆ P and a weights functions u : Q →[0,∞) such that: (i) Q ⊆P, (ii) |Q|= 4 , (iii) ∑ q∈Qu(q) ·q = 1 |D| ∑ p∈P p, and (iv) ∑ q∈Qu(q) = |D|. Now, add to Aa single element (x,y) ∈Dfor every (y |y2 |1) ∈Q. In other words, for every element chosen for the set Qby the Caratheodory theorem, add its corresponding element from Dto A. Furthermore, deﬁne w(x,y) = |D|·u((y|y2 |1)) for every (x,y) ∈A. Corollary 17 trivially holds for (A,w). 31",
      "meta_data": {
        "arxiv_id": "2110.03195v1",
        "authors": [
          "Ibrahim Jubran",
          "Ernesto Evgeniy Sanches Shayda",
          "Ilan Newman",
          "Dan Feldman"
        ],
        "published_date": "2021-10-07T05:49:55Z",
        "pdf_url": "https://arxiv.org/pdf/2110.03195v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper presents the first algorithm to construct a (k,ε)-coreset for decision trees of signals (2D matrices). The main research problem addressed is the sub-optimality, high computation time, and scalability issues of decision tree optimization, particularly the lack of a provably small coreset for general datasets. The key findings include a proof that every n × m signal has a (k,ε)-coreset of size polynomial in klog(nm)/ε, and a novel deterministic algorithm that computes this coreset in O(nmk) time. Experimental results demonstrate that applying these coresets boosts the computation time of random forests and their parameter tuning by up to x10, while maintaining similar accuracy. The paper also enables AutoML for decision trees through coreset-based hyperparameter tuning and provides open-source code.",
        "methodology": "The methodology introduces k-segmentation as a generalization of decision trees, partitioning the input feature space into k disjoint axis-parallel hyper-rectangles using Sum of Squared Error (SSE) as the loss function. The core algorithm, SIGNAL-CORESET (Algorithm 3), involves three main steps: 1) Computing an (α,β)k-approximation of the input signal D using an iterative bi-criteria approximation algorithm (Algorithm 4: BICRITERIA) to get a rough optimal solution and a lower bound σ. 2) Constructing a balanced partition B of D using Algorithm 2 (PARTITION), which leverages Algorithm 1 (SLICE PARTITION) to divide the signal into sub-signals where each sub-signal has a small opt1(B) and is intersected by few k-segmentations. 3) For each sub-signal B in the balanced partition, a small representation (CB,uB) (with |CB|=4) is computed using Carathéodory’s theorem (Corollary 17) to preserve essential statistics (sum of values, sum of squared values, sum of weights). These representations are then aggregated to form the final coreset (C,u). An additional algorithm, FITTING-LOSS (Algorithm 5), is provided to approximate the loss of any k-segmentation using the constructed coreset.",
        "experimental_setup": "The coreset construction (Algorithm 3) was implemented in Python 3.7 and evaluated on a standard MSI Prestige 14 laptop. Existing machine learning solvers were applied as black boxes on the generated coresets for performance evaluation. These solvers included RandomForestRegressor from sklearn.ensemble and LGBMRegressor from lightGBM, both using default hyperparameters. Two compression schemes were compared: DT-coreset (our Algorithm 3 with k=2000 for coreset construction) and RandomSample (uniform random sampling matched to the coreset size). Real-world public datasets from the UCI Machine Learning Repository were used: the Air Quality Dataset (n=9358, m=15) and the Gesture Phase Segmentation Dataset (n=9900, m=18), both normalized. The experimental goal was to predict missing entries, with 30% of the dataset reserved as a test set (randomly chosen 5x5 patches). The loss was measured by the sum of squared differences (SSE). Hyperparameter tuning for the k parameter of decision trees was performed by comparing standard tuning on full data versus tuning on the coreset, with experiments repeated 10 times and results averaged. Visual experiments were conducted using synthetic datasets (sklearn.datasets.make_blobs, make_moons, make_circles) to illustrate the balanced partition, weighted coreset points, and decision tree boundaries.",
        "limitations": "The existence of a small coreset for decision trees is not guaranteed for general datasets, specifically for non-matrix one-dimensional data with binary labels. The paper overcomes this by assuming the input is a matrix (a discrete signal where every coordinate has a label), which is presented as the weakest enabling assumption. The theoretical upper bound on coreset size (polynomial in klog(N)/ε^4) is acknowledged to be pessimistic in practice. The authors' implementation of the coreset construction is described as naive compared to highly optimized professional libraries, leading to coreset construction time often dominating overall runtime despite significant speedups in subsequent model training. The research primarily focuses on the Sum of Squared Differences (SSE) loss function. For simplicity and space, the algorithms are presented for 2D matrices (d=2), though generalization to higher-dimensional tensors (d>=3) is mentioned as straightforward but left for future work.",
        "future_research_directions": "Future research directions include formally defining the 'real-world' properties of datasets that enable decision tree coresets to perform well in practice, despite theoretical non-existence for arbitrary datasets. Generalizing the proposed techniques to support other loss functions beyond the Sum of Squared Distances (SSE) is another key area. Extending the algorithms to support high-dimensional data (tensors) instead of just matrices is a straightforward generalization mentioned. Furthermore, there is an opportunity to extend the algorithms and experimental results to other machine learning libraries and cost functions to broaden their impact. Improving the computational efficiency of the coreset construction itself is also an implicit direction from the authors' self-assessment of their implementation."
      }
    },
    {
      "title": "Few-bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction",
      "abstract": "Memory footprint is one of the main limiting factors for large neural network\ntraining. In backpropagation, one needs to store the input to each operation in\nthe computational graph. Every modern neural network model has quite a few\npointwise nonlinearities in its architecture, and such operation induces\nadditional memory costs which -- as we show -- can be significantly reduced by\nquantization of the gradients. We propose a systematic approach to compute\noptimal quantization of the retained gradients of the pointwise nonlinear\nfunctions with only a few bits per each element. We show that such\napproximation can be achieved by computing optimal piecewise-constant\napproximation of the derivative of the activation function, which can be done\nby dynamic programming. The drop-in replacements are implemented for all\npopular nonlinearities and can be used in any existing pipeline. We confirm the\nmemory reduction and the same convergence on several open benchmarks.",
      "full_text": "Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction Georgii Novikov1 Daniel Bershatsky 1 Julia Gusak 1 Alex Shonenkov 2 Denis Dimitrov 2 3 Ivan Oseledets 1 4 Abstract Memory footprint is one of the main limiting fac- tors for large neural network training. In back- propagation, one needs to store the input to each operation in the computational graph. Every modern neural network model has quite a few pointwise nonlinearities in its architecture, and such operation induces additional memory costs which — as we show — can be signiﬁcantly re- duced by quantization of the gradients. We pro- pose a systematic approach to compute optimal quantization of the retained gradients of the point- wise nonlinear functions with only a few bits per each element. We show that such approximation can be achieved by computing optimal piecewise- constant approximation of the derivative of the activation function, which can be done by dy- namic programming. The drop-in replacements are implemented for all popular nonlinearities and can be used in any existing pipeline. We conﬁrm the memory reduction and the same convergence on several open benchmarks. 1. Introduction Modern neural networks models are getting larger and larger. One of the main bottlenecks in the training loop is the re- quired device memory storage (Ojika et al., 2020; Gao et al., 2020). In this paper, we propose a universal approach that helps to reduce the model memory footprint during back- propagation. Note that this approach is complementary to other memory reducing techniques such as checkpointing (Chen et al., 2016) or ofﬂoading (Beaumont et al., 2021). Our method can be applied to any neural network without any additional preprocessing. The memory consumed by the model during training (ex- cept intermediate tensors) can be split into two groups: 1) 1Skoltech, Moscow, Russia 2Sber AI, Moscow, Russia 3Lomonosov MSU, Moscow, Russia 4AIRI, Moscow, Russia. Correspondence to: Georgii Novikov <georgii.novikov@skoltech.ru>. −6 −4 −2 0 2 4 6 x 0.0 0.2 0.4 0.6 0.8 1.0 GELU′(x) GELU derivative 3-bit approximation Figure 1.Optimized 3-bit piecewise-constant approximation of the derivative of the GELU activation function. the model weights (including additional memory for the optimizer state), 2) activations saved for the backward pass, over which the computation is not carried out directly at the moment, but which will be required in the future to compute the gradients. Every operation in the computational graph generates a memory footprint. It is typically overlooked, that the ap- plication of the pointwise non-linearity (such as GELU or sigmoid) actually results in storing the input for the back- ward pass. We show that instead of keeping the full input tensor, it is possible to store a low-bit representation, which allows accurate gradients approximation. . In this work, we propose to approximate the derivative of the activation function in a piecewise-constant form. Such an approximation problem has to be solved once for each activation function, and we propose a simple technique to do that. The proposed approximation divides all values into several bins and saves only its corresponding bin indices instead of storing all values. This is a lossy compresion, but the additional noise introduced by it is negligible as we will show on several benchmarks 4. Main contributions of our paper are: • We propose new approximate backward computation arXiv:2202.00441v2  [cs.LG]  2 Feb 2022Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction schemes that signiﬁcantly reduce the memory con- sumption of neural network training. • We benchmark our approach on several tasks. We show that it provides up to 40% memory reduction on various tasks while maintaining the accuracy on par with the model trained via the standard approach 2. Quantized Gradients of Activations    Tensors   saved   for backward Backward passForward pass Save Quantize  and   Save Quantized tensors  saved   for backward Figure 2.Computation graph of both forward and backward pass. Orange and purple parts of the graph correspond to standard and proposed ways of saving tensors for backward, respectively. Vector xbit stands for the tensor saved using 2-bit quantization, while x denotes its uncompressed version. Gradients of activations using automatic differentiation. Modern deep learning frameworks use the reverse mode au- tomatic differentiationto calculate the gradients of the loss over the model parameters. Forward computation can be associated with a directed acyclic graph, depicted in Fig. 2. Each operation f computes the output Xl+1 given the input Xl and has to save some information Sl that would be used on the backward pass in order to calculate the derivative ∂L/∂Xl from ∂L/∂Xl+1 and Sl. Thus, in a typical train- ing loop, the intermediates Sl of all operations in the graph are stored in the memory during the whole forward pass until they are no longer needed after the completion of the corresponding backward operation during backward pass. This generates an additional memory, which can be quite signiﬁcant and be larger than the total amount of parameters of the model. Pointwise activations. In this paper, we focus on a point- wise activation function, which is ubiquitous in modern neural network architectures. Given an input tensor Xl we apply a function f to each of the elements of this tensor: f(Xl) = [f(Xj1 ,...,jk l )]j1 ,...,jk ,f : R →R. This operation is very cheap compared to other operations in the deep neural network model and does not attract much at- tention when analysing computational complexity. However, standard implementation in such a framework as PyTorch induces not a very small memory footprint and the whole input Xl is saved for the backward pass. The backward pass for such a function consists of element- wise multiplication of the propagated gradient tensor by the derivative of the nonlinearity function at the points of the input tensor: if Xl+1 = f(Xl), then the gradient of the loss Lwith respect to Xl is computed as ∂L ∂Xl = ∂L ∂Xl+1 f′(Xl), (1) where f′(Xl) is the tensor with elements, consisting of the derivative of f evaluated in each element of Xl. From (1), it follows that for the backward pass we have to store only f′(Xl), and Xl is not needed. ReLU activation function. To illustrate our idea, con- sider one of the most popular nonlinearities, f(x) = ReLU(x) = max(0,x). Its derivative f′ takes only two values, 0 and 1 and it only require 1 bit to store. If single precision is used, then the compression is 32, which is quite noticeable. GELU activation function. In modern transformer archi- tectures (Vaswani et al., 2017) the GELU (Hendrycks & Gimpel, 2016) nonlinearity is typically used. The derivative no longer takes two values. Instead, we propose to approxi- mate f′by a piecewise-constant function. For example, if we allow 8 different values, we will need only 3 bits per each element (Fig. 1). Quantized gradients of activations. In stochastic opti- mization, if the gradient for a given batch is computed approximately, the optimization may still converge. The GELU derivative (see Fig. 1) is quite “similar” to a piecewise-constant approximation: for large values of x, it is almost exactly equal to 0 or 1, and for small values of x, a rather interesting transition from 0 to 1 occurs. In- stead of calculating the derivative exactly on the backward pass, we approximate it using a certain piecewise-constant approximation: q(x|s,y) = ∑ i yi1 [x∈[si; si+1 ]] (2) As noted above, if the approximation has kconstant inter- vals, instead of storing the full input tensor X, it will be possible to save only log kbits of information (per element of the input tensor), which, accordingly, will reduce the memory consumption by 32/log k times for single preci- sion. Fig. 1 shows an example of an optimized 3-bit piecewise- constant approximation for the GELU activation function.Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction Finding the optimal approximation parameters (boundaries of intervals and values on them) is a challenging task. We propose to ﬁnd them by minimizing the (weighted)L2 norm of the error. 3. Optimal Piecewise-constant Approximation Consider function f : R →R and its derivative f′. We will measure the quality of a piecewise constant approxima- tion (2) with a weighted L2 norm min y,s L(s,y), L(s,y) = ∫ R (f′(x) −q(x|s,y))2w(x)dx, (3) where wis some weight function reﬂecting our prior knowl- edge of the activation function argument distribution. Prac- tical choices of wmay be either 1 [x∈[A; B]] (with some reasonable Aand B, which should be large enough) which makes integral (3) tractable, or maybe, e.g., standard normal distribution. It is easy to see that the optimal value of y for L(s,y) with given s is: yi(s) = ∫si+1 si w(x)f′(x)dx∫si+1 si w(x)dx . (4) The gradient of L(s,y(s)) w.r.t. the vector s then can be derived analytically: ∂L ∂si = (2f′(si) −yi(s) −yi−1(s))(yi(s) −yi−1(s))w(si). (5) Using this formula, L(s,y) can be optimized using any gradient-based method, and optimal piecewise-constant ap- proximations can be found for the different number of bits using standard optimization techniques. Dynamic programming. The minimization problem (3) has many local minima that are far from optimal. We sug- gest using dynamic programming to get some good initial approximation that can be ﬁnetuned using gradient-based methods (but also can be used as is because it is very accu- rate on its own). We will assume that the weighting function w is chosen such that w(x) = 0for x̸∈[A; B]. Consider an auxiliary value DP(t,k) = miny1:k, s1:k+1, s1=A, sk+1=t ∫ t A (f′(x) −q(x|y,s))2w(x)dx, t∈R,k ∈N. Essentially, DP(t,k) is the optimal piecewise constant ap- proximation of size kfor the given function f′on the inter- val [A; t]. The recurrent formula for this value is: DP(t,k + 1) = min t′ DP(t′,k)+ + ∫ t t′ (f′(x) −y(t′,t))2w(x)dx, y(t′,t) = ∫ t t′ w(x)f′(x)dx, (6) since a piecewise-constant approximation of size k+ 1con- sists of corresponding approximation of size k(ﬁrst term) plus one constant interval (second term). Heret′chooses the right bound of approximation of size k, and y(t′,t) stands for the optimal value for the interval [t′; t] (4). Then the minimal value of L(s,y) of size kis equal to DP(B,k). To solve the minimization problem (6), we suggest consid- ering the discretization of t: A= t0 <t1 <··· <tn = B and reducing the calculation of DP(t,k) to its approxima- tion only in the points of discretization: DP(i,k) = min j DP(j,k) +T(j,i), T(j,i) = ∫ ti tj (f′(x) −y(j,i))2w(x)dx, y(j,i) = ∫ti tj w(x)f′(x)dx ∫ti tj w(x)dx . (7) Both y(j,i) and T(j,i) can be calculated in advance us- ing analytical formulas (if possible) or numerically for the corresponding 1-dimensional integrals. After that, the full array of DP(i,k) can be calculated in O(n2K) time and O(n2) space, where Kis the required number of constant intervals in the approximation (2). Please note that this opti- mization has to be performed only once, so ncan be chosen quite large thus the result would be very close to the global minimum. Note that the space complexity can be reduced to O(n) by rewriting (7) as F2(i) = ∫ ti A f′2(x)w(x)dx, W(i) = ∫ ti A w(x)dx, FW(i) = ∫ ti A f′(x)w(x)dx, y(j,i) = (FW(j) −FW(i))/(W(j) −W(i)), T(j,i) =F2(i) −F2(j) −y(j,i)2(W(i) −W(j)). (8) We can see that ultimately only O(n) one-dimensional in- tegrals have to be stored, and everything else can be easily evaluated in O(1) time on the spot. The one-dimensional integrals can be calculated numerically in O(n) time andFew-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction Figure 3.Examples of 3-bit approximations for derivatives of popular nonlinearities: (a) Swish, (b) SELU, and (c) Sigmoid. Please note that the derivative of the sigmoid is an even function, thus we can quantize only positive real axis [0; + inf], which essentially doubles the memory budget. space complexity as well: F2(i+ 1) =F2(i) + ∫ ti+1 ti f′2(x)w(x)dx, W(i+ 1) =W(i) + ∫ ti+1 ti w(x)dx, FW(i+ 1) =FW(i) + ∫ ti+1 ti f′(x)w(x)dx. (9) Numerical results. In Fig. 3, we provide some 3-bit ex- amples for popular activation functions obtained with de- scribed method, and in Table 3, we provide numerical values of error (3) with uniform weight on interval [−10; 10]: w(x) = { 1, if x∈[−10; 10] 0, otherwise . (10) Note that the convergence is quite fast with respect to the number of bits: the error drops by a factor of 2 −4 when an additional bit is added. It would be interesting to study the convergence rates of such approximations. 1-bit 2-bits 3-bits 4-bits ReLU 0.0 - - - GELU 0.1410 0.0406 0.0119 0.0031 Swish 0.2150 0.0479 0.0170 0.0045 Sigmoid 0.0181 0.0038 0.0009 0.0002 Tanh 0.1584 0.0319 0.0073 0.0017 SELU 0.2554 0.1010 0.0184 0.0039 Softplus 0.2902 0.0541 0.0121 0.0029 Table 1.Numerical values of error (3) with uniform weight on interval [-10; 10] (10). With precalculated piecewise-constant approximation, drop- in replacement for activation function f is very straightfor- ward. On the forward pass, instead of the full tensor X, we have to save only indices of intervals to which the elements of X belong (which would take log k bits where k is the number of intervals), and on the backward pass, we need to multiply gradient w.r.t. output not with the actual derivative of f, but with values fromy corresponding to stored indices. Pseudocode is presented in Alg. 1. 1 # Globally stored 2 # piecewise-constant 3 # approximation parameters 4 s = [...] 5 y = [...] 6 7 def forward(X): 8 X_pos = sortedsearch(s, X) 9 save_for_backward(X_pos) 10 return f(X) 11 12 def backward(dLdY): 13 X_pos = get_saved_for_backward() 14 return dLdY * y[X_pos] 15 Listing 1.Pseudo code for quantized backward layer. Arrays s and y are parameters of quantization (2), sortedsearch is a binary search method. 4. Experiments The goal of our experiments is to show that quantized gradi- ents give memory savings with no quality degradation. We evaluate our method on language models on several open benchmarks. As Few-bit backward method is a drop-in replace solution, no hyperparameter optimization was per- formed; all hyperparameters are the same across comparedFew-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction Figure 4.ResNet18 with ReLU replaced with either GELU (a, b, c) or Swish (d, e, f) nonlinearity trained on Imagenet. (a,d): Training loss. (b,e): Training loss during the last third of the training process. (c,f): Final validation top-1 accuracy. All plots are averaged across three runs with different seeds. Error bars mean minimum and maximum values. methods and are taken from the corresponding sources. In Table 4 we report results for RoBERTa-base model (Liu et al., 2019) on GLUE benchmark (Wang et al., 2019) for standard GELU and 1-, 2-, 3- and 4-bits GELU. Both task- relevant metrics and the ﬁnal value of the loss function are reported. 1- and 2-bits versions have minor performance degradation, while 3- and 4-bits GELU have no visible difference and closely match vanilla GELU performance both in terms of performance metric and loss function while saving 15% and 14% memory respectively 4. To further examine the inﬂuence of backward GELU quanti- zation on stochastic optimization, we consider the behaviour of loss function during training, depicted in Fig. 4. We can clearly see that our method repeats the dynamic of standard GELU both on train and validation sets. 1- and 2-bit ver- sions performs a little worse, while 3- and 4-bit versions are hardly distinguishable from the standard GELU. This allows concluding that such quantization is not noticeable for stochastic optimization and can be applied without loss of quality and ﬁnal properties of the model. In Fig. 5 we present training dynamic of ruDALL-E1 Male- vich (Ramesh et al., 2021) model on Russian Emoji dataset. The dataset (Shonenkov et al., 2021) contains 2749 unique emoji icons and 1611 unique texts that were collected by web scrapping (the difference in quantities is due to the fact that there are sets, within which emojis differ only in color, moreover, some elements are homonyms in Russian). ruDALL-E Malevich is a big multimodal pretrained trans- former, which learns the conditional distribution of images given some string of text (more precisely it autoregressively models the text and image tokens as a single stream of data). ruDALL-E Malevich encoder part is a 24 layer Trans- former (Vaswani et al., 2017) model with 16 attention heads, 2048 hidden dimensions and standard GELU nonlinearity, which in total has 1.3B parameters. It works with 128 text tokens, which are prepared from the text input using YTTM tokenizer2, and 1024 image tokens, which are obtained after encoding the input image using Sber-VQGAN3. Quantized 1Implementation is taken from https://github.com/sberbank- ai/ru-dalle 2Implementation is taken from https://github.com/ VKCOM/YouTokenToMe 3Implementation is taken from https://github.com/Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction Figure 5. Dynamic of loss values in ﬁnetuning of ruDALL-E Malevich with few-bit GELU activations. All experiments have following setup: train size 2474, valid size 275, loss image weight 1000, frozen MLP and attention layers, batch size 40, start lr 4e-7, max lr 1e-5, ﬁnal lr 2e-8, warmup 0.1, 8bit-Adam (Dettmers et al., 2021), weight decay 0.2, betas (0.9, 0.98), eps 1e-6, gradient checkpointing 24, trained for 6h using 1xA100. Table 2.RoBERTa-base on GLUE benchmark with different quantization budgets. Metric: mean accuracy/correlation (task speciﬁc). Averaged across ﬁve runs. 1-bit GELU 2-bits GELU 3-bits GELU 4-bits GELU Vanila GELU stsb 0.906 ( ±0.002) 0.907 ( ±0.002) 0.910 ( ±0.002) 0.909 ( ±0.002) 0.909 ( ±0.001) mnli-mm 0.870 ( ±0.001) 0.870 ( ±0.002) 0.871 ( ±0.002) 0.870 ( ±0.001) 0.871 ( ±0.002) mrpc 0.880 ( ±0.009) 0.884 ( ±0.008) 0.884 ( ±0.007) 0.885 ( ±0.008) 0.882 ( ±0.005) cola 0.595 ( ±0.016) 0.580 ( ±0.014) 0.596 ( ±0.015) 0.607 ( ±0.014) 0.604 ( ±0.013) mnli 0.873 ( ±0.001) 0.872 ( ±0.002) 0.874 ( ±0.001) 0.874 ( ±0.002) 0.874 ( ±0.001) sst2 0.939 ( ±0.003) 0.938 ( ±0.003) 0.941 ( ±0.004) 0.941 ( ±0.003) 0.943 ( ±0.002) rte 0.752 ( ±0.021) 0.756 ( ±0.023) 0.780 ( ±0.014) 0.771 ( ±0.025) 0.771 ( ±0.017) qqp 0.914 ( ±0.001) 0.915 ( ±0.000) 0.916 ( ±0.001) 0.916 ( ±0.001) 0.916 ( ±0.001) qnli 0.925 ( ±0.002) 0.925 ( ±0.002) 0.926 ( ±0.002) 0.927 ( ±0.002) 0.927 ( ±0.002) backward for ruDALL-E Malevich shows same behaviour as for RoBERTa-base architecture: 1- and 2-bit versions, although coping with training perfectly ﬁne, demonstrates minor performance degradation, while 3- and 4-bit versions are indistinguishable from the original GELU. ResNet Architecture. To explore the inﬂuence of back- ward quantization on architectures other than Transformer and nonlinearities other than GELU, we trained ResNet18 model (He et al., 2016) on ImageNet (Russakovsky et al., 2015) benchmark (Leclerc et al., 2022) dataset with ReLU replaced with Swish function (Swish(x) =xσ(x), where σ is a sigmoid function) or with GELU, see Fig. 4. The behaviour of our quantization scheme remains the same for different network architectures and nonlinearities: 1- and 2- bits have minor performance drop, while 3- and 4- bits are on par with unchanged nonlinearity. sberbank-ai/sber-vq-gan ActNN. As a baseline, we use another quantization scheme ActNN (Chen et al., 2021). It works in a much wider spectrum of situations, as it can quantize not only pointwise nonlinearity layers but also all kinds of linear layers (convolutional and dense layers), normalization lay- ers and pooling layers. Without going deep into details, ActNN divides the saved tensor H into chunks hi where each chunk is of an equal size G. Then, given the quan- tization budget of b bits, each chunk hi is normalized: ui = 2b(hi −min{hi})/(max{hi}− min{hi}), and its randomly quantized version is saved ¯ ui = ⌈ui⌉with prob. u −⌊ui⌋, ⌊ui⌋otherwise. Random rounding is performed in order to guarantee that the quantization is unbiased. For each group, two additional values min{hi}and max{hi} are saved as well, but for the group size of G = 256it is only 0.125 additional bits per element, which we ignore in our following tests. ActNN by construction does not take into account the global behaviour of the nonlinearity deriva- tive. We argue that for nonlinearity layers, it is very crucial,Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction Figure 6.RoBERTa-base on QQP task from GLUE benchmark. (a): Train loss. (b): Train loss during the last third of the training. (c): Validation loss. Averaged across 10 runs. and thus our preoptimized quantization scheme is more preferable. To prove that, we consider ActNN behaviour on the QQP task from the GLUE benchmark with respect to different quantization budgets and compare it with our method. In Fig. 8 train and validation losses are shown, and in Table 4 we report the ﬁnal accuracy for the QQP task. In general, our method with 1 bit less budget works the same or better than ActNN. ActNN Our 1-bit 0.8880 ±0.0008 0.9080 ±0.0006 2-bit 0.9072 ±0.0005 0.9097 ±0.0006 3-bit 0.9106 ±0.0003 0.9114 ±0.0007 4-bit 0.9113 ±0.0006 0.9112 ±0.0005 Memory saving. Table 4 shows memory measurements for different models with different number of bits per ele- ment for backward quantization. As was shown experimen- tally, for many tasks 3 bits is already enough for lossless training. In practice, fewer bits may be chosen to even further increase the batch size and consequently speed up training or to ﬁt a larger model on a device that did not ﬁt it before. 5. Related Work The reduction of the memory footprint is an important topic. To save memory during training, in addition to working with stored activations, the memory used to store model parameters can be compressed. Quantization (Bondarenko et al., 2021; Bengio et al., 2013; Banner et al., 2019; Ja- cob et al., 2018; Nagel et al., 2021; Krishnamoorthi, 2018) limits the admissible values of weights to some small ﬁnite set. Thus less memory is needed for storage. The low-rank representation of weights (Hrinchuk et al., 2020; Phan et al., Table 3.Peak memory usage in training time during ﬁne-tuing on GLUE. Task Batch BitWidth Mem, GiB Saving, % MRPC 128 Vanilla 11.34 0.0 3-bit 9.78 13.8 2-bit 9.71 14.4 QNLI 16 Vanilla 11.73 0.0 3-bit 10.77 8.2 2-bit 10.75 8.4 STS2 256 Vanilla 13.31 0.0 3-bit 11.25 15.5 2-bit 11.17 16.1 2020; Gusak et al., 2019; 2021; Cui et al., 2020; Novikov et al., 2018; Lebedev et al., 2015) assumes some internal structure of model weights and saves memory by explicitly using this structure with low-rank methods from linear alge- bra. Low precision learning and low precision optimizers focus on using the lower precision ﬂoats to store weights, optimization parameters, and model gradients. All of these approaches are complementary to the proposed one and can be used together. Checkpointing (Beaumont et al., 2019; 2021; Chen et al., 2016) methods save memory by the cost of more calcula- tions. It stores a fewer number of activations and repeats the calculation of the rest from the saved checkpoints. Of- ﬂoading methods (Beaumont et al., 2020) send the saved activations to the computer’s RAM and load them back to the video memory on the backwards passes, which also saves GPU memory at the cost of host-device communica- tion time. ActNN (Chen et al., 2021) is a framework for quantizing stored activations adaptively on the ﬂy. In contrast to our work, it allows quantizing not only layers of element-by-Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction Figure 7.Comparison of RoBERTa-base on QQP task from GLUE benchmark with ActNN quantization and our quantization schemes. (a): Train loss. (b): Train loss during the last third of the training. (c): Validation loss. Averaged across ten runs. Model QuantSavingMax Batch Size ResNet-101 131256x256 size1-bit 30% 170 (+29.8%)2-bit 29% 169 (+29.0%)3-bit 28% 167 (+27.5%)4-bit 27% 165 (+26.0%)DenseNet-121 126256x256 size1-bit 31% 165 (+31.0%)2-bit 30% 164 (+30.2%)3-bit 29% 162 (+28.6%)4-bit 28% 161 (+27.8%)Efﬁcient Net B7 47256x256 size1-bit 26% 59 (+25.5%)2-bit 25% 58 (+23.4%)3-bit 24% 58 (+23.4%)4-bit 23% 57 (+21.3%)RoBERTa-base 154256 seq. len 1-bit 16% 179 (+16.2%)2-bit 15% 178 (+15.6%)3-bit 15% 177 (+14.9%)4-bit 14% 176 (+14.3%)RoBERTa-large 54256 seq. len 1-bit 16% 63 (+16.7%)2-bit 16% 63 (+16.7%)3-bit 15% 62 (+14.8%)4-bit 15% 62 (+14.8%)GPT2 83256 seq. len 1-bit 42% 117 (+41.0%)2-bit 41% 116 (+39.8%)3-bit 39% 114 (+37.3%)4-bit 38% 113 (+36.1%) Table 4.Memory savings and maximum batch size for popular models for different quantization budget. You can ﬁnd more de- tailed version in Appendix B element activations but also many others, including con- volutional, normalization and linear layers. However, this method depends on the distribution of elements of quan- tizable tensors and, because of that, its performance may degrade. Our approach, on the other hand, selects data- agnostic optimal quantization, which in practice turns out to be sufﬁcient and easier to use. 6. Conclusion We have proposed a method to reduce memory consump- tion during the training of deep neural network models by storing less information for backward pass in the element- wise activation functions. For effective training, there is no need to calculate the derivative of the activation functions precisely, but only its piecewise-constant approximation is sufﬁcient. This makes it possible to save not the entire input tensor at each application of the activation function, but only the interval number in the piecewise-constant ap- proximation. Experiments show that for a wide class of models and problems, storing only 3 bits of information per tensor element does not lead to degradation of the learning quality and saves about 20 percent of memory. We have proposed an efﬁcient algorithm for constructing an optimal piecewise-constant approximation. The proposed drop-in re- placements for popular activation functions (ReLU, GELU, Swish, Sigmoid and others) do not depend on the neural net- work model, the problem to be solved, or the peculiarities of data distribution. The replacement of the original activation functions by the proposed method can be performed at any training stage (both to models trained from scratch and to pre-trained models for subsequent ﬁne-tuning) and does not require any changes in the training pipelines. An efﬁcient CUDA implementation of the proposed method, together with pre-computed piecewise-constant approximations for many popular activation functions, is available for PyTorch at GitHub repository4. 4Source code repository can be found at https://github. com/SkoltechAI/fewbit.Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction 7. Acknowledgements The work was supported by the Analytical cen- ter under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). References Banner, R., Nahshan, Y ., and Soudry, D. Post train- ing 4-bit quantization of convolutional networks for rapid-deployment. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alch ´e-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 7948– 7956, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/ c0a62e133894cdce435bcb4a5df1db2d-Abstract. html. Beaumont, O., Eyraud-Dubois, L., Herrmann, J., Joly, A., and Shilova, A. Optimal checkpointing for heterogeneous chains: how to train deep neural networks with limited memory. CoRR, abs/1911.13214, 2019. URL http: //arxiv.org/abs/1911.13214. Beaumont, O., Eyraud-Dubois, L., and Shilova, A. Op- timal GPU-CPU ofﬂoading strategies for deep neural network training. In Malawski, M. and Rzadca, K. (eds.), Euro-Par 2020: Parallel Processing - 26th In- ternational Conference on Parallel and Distributed Com- puting, Warsaw, Poland, August 24-28, 2020, Proceed- ings, volume 12247 of Lecture Notes in Computer Sci- ence, pp. 151–166. Springer, 2020. doi: 10.1007/ 978-3-030-57675-2 \\10. URL https://doi.org/ 10.1007/978-3-030-57675-2_10 . Beaumont, O., Eyraud-Dubois, L., and Shilova, A. Efﬁcient combination of rematerialization and ofﬂoading for train- ing dnns. Advances in Neural Information Processing Systems, 34, 2021. Bengio, Y ., L´eonard, N., and Courville, A. C. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432. Bondarenko, Y ., Nagel, M., and Blankevoort, T. Under- standing and overcoming the challenges of efﬁcient trans- former quantization. In Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.),Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 7947–7969. Associa- tion for Computational Linguistics, 2021. doi: 10.18653/ v1/2021.emnlp-main.627. URL https://doi.org/ 10.18653/v1/2021.emnlp-main.627. Chen, J., Zheng, L., Yao, Z., Wang, D., Stoica, I., Mahoney, M. W., and Gonzalez, J. Actnn: Reducing training mem- ory footprint via 2-bit activation compressed training. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 1803– 1813. PMLR, 2021. URL http://proceedings. mlr.press/v139/chen21z.html. Chen, T., Xu, B., Zhang, C., and Guestrin, C. Train- ing deep nets with sublinear memory cost. CoRR, abs/1604.06174, 2016. URL http://arxiv.org/ abs/1604.06174. Cui, C., Zhang, K., Daulbaev, T., Gusak, J., Oseledets, I. V ., and Zhang, Z. Active subspace of neural net- works: Structural analysis and universal attacks. SIAM J. Math. Data Sci., 2(4):1096–1122, 2020. doi: 10.1137/ 19M1296070. URL https://doi.org/10.1137/ 19M1296070. Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-bit optimizers via block-wise quantization. CoRR, abs/2110.02861, 2021. URL https://arxiv.org/ abs/2110.02861. Gao, Y ., Liu, Y ., Zhang, H., Li, Z., Zhu, Y ., Lin, H., and Yang, M. Estimating gpu memory consumption of deep learning models. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineer- ing, pp. 1342–1352, 2020. Gusak, J., Kholyavchenko, M., Ponomarev, E., Mar- keeva, L., Blagoveschensky, P., Cichocki, A., and Os- eledets, I. V . Automated multi-stage compression of neural networks. In 2019 IEEE/CVF International Con- ference on Computer Vision Workshops, ICCV Work- shops 2019, Seoul, Korea (South), October 27-28, 2019, pp. 2501–2508. IEEE, 2019. doi: 10.1109/ICCVW. 2019.00306. URL https://doi.org/10.1109/ ICCVW.2019.00306. Gusak, J., Daulbaev, T., Ponomarev, E., Cichocki, A., and Oseledets, I. Reduced-order modeling of deep neural networks. Computational Mathematics and Mathematical Physics, 61(5):774–785, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Deep resid- ual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recogni- tion, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016. doi:Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction 10.1109/CVPR.2016.90. URL https://doi.org/ 10.1109/CVPR.2016.90. Hendrycks, D. and Gimpel, K. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv. org/abs/1606.08415. Hrinchuk, O., Khrulkov, V ., Mirvakhabova, L., Orlova, E. D., and Oseledets, I. V . Tensorized embedding layers. In Cohn, T., He, Y ., and Liu, Y . (eds.),Findings of the As- sociation for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 4847–4860. Association for Computational Linguistics, 2020. doi: 10.18653/v1/ 2020.ﬁndings-emnlp.436. URL https://doi.org/ 10.18653/v1/2020.findings-emnlp.436. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A. G., Adam, H., and Kalenichenko, D. Quantization and training of neural networks for efﬁcient integer- arithmetic-only inference. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 2704– 2713. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00286. URL http://openaccess.thecvf.com/content_ cvpr_2018/html/Jacob_Quantization_ and_Training_CVPR_2018_paper.html. Krishnamoorthi, R. Quantizing deep convolutional net- works for efﬁcient inference: A whitepaper. CoRR, abs/1806.08342, 2018. URL http://arxiv.org/ abs/1806.08342. Lebedev, V ., Ganin, Y ., Rakhuba, M., Oseledets, I. V ., and Lempitsky, V . S. Speeding-up convolutional neural net- works using ﬁne-tuned cp-decomposition. In Bengio, Y . and LeCun, Y . (eds.),3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed- ings, 2015. URL http://arxiv.org/abs/1412. 6553. Leclerc, G., Ilyas, A., Engstrom, L., Park, S. M., Salman, H., and Madry, A. ffcv. https://github.com/ libffcv/ffcv/, 2022. commit xxxxxxx. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/ abs/1907.11692. Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y ., van Baalen, M., and Blankevoort, T. A white paper on neural network quantization. CoRR, abs/2106.08295, 2021. URL https://arxiv.org/abs/2106. 08295. Novikov, A., Troﬁmov, M., and Oseledets, I. Exponential machines. Bulletin of the Polish Academy of Sciences: Technical Sciences, pp. 789–797, 2018. Ojika, D., Patel, B., Reina, G. A., Boyer, T., Martin, C., and Shah, P. Addressing the memory bottleneck in AI model training. arXiv preprint arXiv:2003.08732, 2020. Phan, A. H., Sobolev, K., Sozykin, K., Ermilov, D., Gusak, J., Tichavsk´y, P., Glukhov, V ., Oseledets, I. V ., and Ci- chocki, A. Stable low-rank tensor decomposition for compression of convolutional neural network. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.), Com- puter Vision - ECCV 2020 - 16th European Confer- ence, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIX, volume 12374 of Lecture Notes in Computer Science, pp. 522–539. Springer, 2020. doi: 10.1007/ 978-3-030-58526-6 \\31. URL https://doi.org/ 10.1007/978-3-030-58526-6_31 . Ramesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Confer- ence on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Ma- chine Learning Research, pp. 8821–8831. PMLR, 2021. URL http://proceedings.mlr.press/v139/ ramesh21a.html. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M. S., Berg, A. C., and Fei-Fei, L. Ima- genet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211–252, 2015. doi: 10.1007/ s11263-015-0816-y. URL https://doi.org/10. 1007/s11263-015-0816-y . Shonenkov, A., Bakshandaeva, D., Dimitrov, D., and Nikolich, A. Emojich - zero-shot emoji generation using russian language: a technical report. CoRR, abs/2112.02448, 2021. URL https://arxiv.org/ abs/2112.02448. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998– 6008, 2017. URL https://proceedings. neurips.cc/paper/2017/hash/Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction 3f5ee243547dee91fbd053c1c4a845aa-Abstract. html. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representa- tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URLhttps://openreview. net/forum?id=rJ4km2R5t7.Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint ReductionFew-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction A. Detailed examples of few-bit approximations for popular nonlinearity layers Figure 8.1- to 4-bit approximations of popular nonlinearty layers.Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction B. Detailed memory measurements for different models We provide memory measurements for different model architectures in Table B. ”Model size” is the total memory used for storing model parameters (without model gradients and optimizator statistics). ”All activations size” is the total memory used by tensors, saved for backward pass. ”Nonlinearity activations size” is the part of all activations used by nonlinearity layers. Maximum batch size is calculated with the assumption, that three model copies are stored on the device (model parameters, model gradients and optimizer statistics like weight moments in SGD with momentum). Model Size (Mb) All Activations Size (Mb) Nonlinearity Activations Size (Mb) 1-bit Saving Max batch size 2-bit Saving Max batch size 3-bit Saving Max batch size 4-bit Saving Max batch size ResNet-18 44.6 40.0 11.5 28% 1010 (+27.5%) 27% 1001 (+26.4%) 26% 992 (+25.3%) 25% 984 (+24.2%) ResNet-50 99.2 156.8 47.9 30% 256 (+29.3%) 29% 254 (+28.3%) 28% 252 (+27.3%) 27% 249 (+25.8%) ResNet-101 171.4 234.5 73.4 30% 170 (+29.8%) 29% 169 (+29.0%) 28% 167 (+27.5%) 27% 165 (+26.0%) ResNet-152 232.3 328.2 104.9 31% 121 (+31.5%) 30% 120 (+30.4%) 29% 119 (+29.3%) 28% 117 (+27.2%) DenseNet-121 30.9 243.8 79.1 31% 165 (+31.0%) 30% 164 (+30.2%) 29% 162 (+28.6%) 28% 161 (+27.8%) DenseNet-161 112.4 457.2 145.3 31% 87 (+29.9%) 30% 87 (+29.9%) 29% 86 (+28.4%) 28% 85 (+26.9%) DenseNet-169 54.7 296.3 95.3 31% 136 (+30.8%) 30% 134 (+28.8%) 29% 133 (+27.9%) 28% 132 (+26.9%) DenseNet-201 77.4 382.2 123.9 31% 105 (+31.2%) 30% 104 (+30.0%) 29% 103 (+28.8%) 28% 102 (+27.5%) Efﬁcient Net B0 20.4 112.4 32.4 28% 360 (+27.7%) 27% 357 (+26.6%) 26% 354 (+25.5%) 25% 351 (+24.5%) Efﬁcient Net B3 47.5 218.6 59.5 26% 185 (+26.7%) 26% 183 (+25.3%) 25% 182 (+24.7%) 24% 180 (+23.3%) Efﬁcient Net B7 256.3 673.5 178.9 26% 59 (+25.5%) 25% 58 (+23.4%) 24% 58 (+23.4%) 23% 57 (+21.3%) VGG 11 507.2 100.9 37.0 36% 386 (+35.4%) 34% 382 (+34.0%) 33% 377 (+32.3%) 32% 373 (+30.9%) VGG 16 528.2 163.8 68.5 41% 237 (+40.2%) 39% 234 (+38.5%) 38% 231 (+36.7%) 37% 228 (+34.9%) VGG 19 548.4 178.8 75.0 41% 217 (+40.9%) 39% 214 (+39.0%) 38% 211 (+37.0%) 37% 208 (+35.1%) RoBERTa-base 480.7 219.6 36.0 16% 179 (+16.2%) 15% 178 (+15.6%) 15% 177 (+14.9%) 14% 176 (+14.3%) RoBERTa-large 1355.6 578.1 96.0 16% 63 (+16.7%) 16% 63 (+16.7%) 15% 62 (+14.8%) 15% 62 (+14.8%) GPT2 491.0 331.1 144.0 42% 117 (+41.0%) 41% 116 (+39.8%) 39% 114 (+37.3%) 38% 113 (+36.1%)",
      "meta_data": {
        "arxiv_id": "2202.00441v2",
        "authors": [
          "Georgii Novikov",
          "Daniel Bershatsky",
          "Julia Gusak",
          "Alex Shonenkov",
          "Denis Dimitrov",
          "Ivan Oseledets"
        ],
        "published_date": "2022-02-01T14:51:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.00441v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Few-Bit Backward, a novel method to significantly reduce memory consumption during neural network training by quantizing the gradients of pointwise activation functions. It introduces a systematic approach using optimal piecewise-constant approximation of the activation function's derivative, achieved via dynamic programming. This method provides drop-in replacements for popular nonlinearities, demonstrating up to 40% memory reduction on various tasks while maintaining accuracy comparable to standard training methods.",
        "methodology": "The core methodology involves approximating the derivative of activation functions (f') with a piecewise-constant function, q(x|s,y), where 's' defines interval boundaries and 'y' defines constant values within those intervals. Instead of storing the full input tensor for the backward pass, only low-bit indices corresponding to these intervals are saved. Optimal parameters (s, y) are found by minimizing a weighted L2 norm of the approximation error, L(s,y) = ∫R (f'(x) - q(x|s,y))^2 w(x)dx. To address local minima, dynamic programming (DP) is employed, discretizing the input domain [A; B] into 'n' points and computing DP(i, k) as the optimal k-segment approximation up to point 'ti'. This DP algorithm runs in O(n^2K) time and O(n) space. The forward pass saves the bin index for each element of the input tensor X, and the backward pass multiplies the propagated gradient by the pre-calculated constant value 'y' corresponding to the saved index.",
        "experimental_setup": "Experiments were conducted on various models and benchmarks to confirm memory savings and maintain quality. Models included RoBERTa-base (1.3B parameters, using GELU), ruDALL-E Malevich (1.3B parameters, using GELU), and ResNet18 (using ReLU, GELU, or Swish). Benchmarks and datasets included GLUE benchmark (for RoBERTa-base, specifically QQP, STS-B, MNLI, MRPC, CoLA, SST2, RTE, QNLI tasks), Russian Emoji dataset (for ruDALL-E Malevich), and ImageNet (for ResNet18). Performance was evaluated using task-relevant metrics (e.g., mean accuracy/correlation), training/validation loss dynamics, and final top-1 accuracy. The method was tested with 1, 2, 3, and 4 bits for quantization, comparing against vanilla activation functions and the ActNN quantization scheme as a baseline. No hyperparameter optimization was performed for the quantized methods; hyperparameters were kept consistent with standard training approaches.",
        "limitations": "The proposed method focuses specifically on quantizing the gradients of pointwise activation functions, meaning it does not address memory reduction in other types of layers (e.g., convolutional, linear, normalization, or pooling layers), unlike more general quantization frameworks like ActNN. While the introduced noise from lossy compression is claimed negligible, 1-bit and 2-bit quantization levels sometimes show minor performance degradation compared to the vanilla versions in certain experiments. The optimal piecewise-constant approximation needs to be pre-calculated once for each activation function, which involves solving an optimization problem.",
        "future_research_directions": "A specific future research direction mentioned is to investigate and study the convergence rates of such piecewise-constant approximations. The paper also implies that exploring the method's applicability or integration with other memory reduction techniques (e.g., checkpointing, offloading, weight quantization, low-rank representations) could be a path for further research, though not explicitly stated as a limitation of the current work."
      }
    },
    {
      "title": "Explainable k-Means and k-Medians Clustering",
      "abstract": "Clustering is a popular form of unsupervised learning for geometric data.\nUnfortunately, many clustering algorithms lead to cluster assignments that are\nhard to explain, partially because they depend on all the features of the data\nin a complicated way. To improve interpretability, we consider using a small\ndecision tree to partition a data set into clusters, so that clusters can be\ncharacterized in a straightforward manner. We study this problem from a\ntheoretical viewpoint, measuring cluster quality by the $k$-means and\n$k$-medians objectives: Must there exist a tree-induced clustering whose cost\nis comparable to that of the best unconstrained clustering, and if so, how can\nit be found? In terms of negative results, we show, first, that popular\ntop-down decision tree algorithms may lead to clusterings with arbitrarily\nlarge cost, and second, that any tree-induced clustering must in general incur\nan $\\Omega(\\log k)$ approximation factor compared to the optimal clustering. On\nthe positive side, we design an efficient algorithm that produces explainable\nclusters using a tree with $k$ leaves. For two means/medians, we show that a\nsingle threshold cut suffices to achieve a constant factor approximation, and\nwe give nearly-matching lower bounds. For general $k \\geq 2$, our algorithm is\nan $O(k)$ approximation to the optimal $k$-medians and an $O(k^2)$\napproximation to the optimal $k$-means. Prior to our work, no algorithms were\nknown with provable guarantees independent of dimension and input size.",
      "full_text": "Explainable k-Means andk-Medians Clustering Sanjoy Dasgupta University of California, San Diego dasgupta@eng.ucsd.edu Nave Frost Tel Aviv University navefrost@mail.tau.ac.il Michal Moshkovitz University of California, San Diego mmoshkovitz@eng.ucsd.edu Cyrus Rashtchian University of California, San Diego crashtchian@eng.ucsd.edu Abstract Clustering is a popular form of unsupervised learning for geometric data. Unfortunately, many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the k-means and k-medians objectives: Must there exist a tree-induced clustering whose cost is comparable to that of the best unconstrained clustering, and if so, how can it be found? In terms of negative results, we show, ﬁrst, that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and second, that any tree-induced clustering must in general incur an Ω( log k) approximation factor compared to the optimal clustering. On the positive side, we design an eﬃcient algorithm that produces explainable clusters using a tree with k leaves. For two means/medians, we show that a single threshold cut suﬃces to achieve a constant factor approximation, and we give nearly-matching lower bounds. For general k ≥ 2, our algorithm is an O(k) approximation to the optimal k-medians and an O(k2) approximation to the optimal k-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size. 1 Introduction A central direction in machine learning is understanding the reasoning behind decisions made by learned models [32, 40, 42]. Prior work on AI explainability focuses on the interpretation of a black-box model, known as post-modeling explainability [10, 47]. While methods such as LIME [ 46] or Shapley explanations [ 35] have made progress in this direction, they do not provide direct insight into the underlying data set, and the explanations depend heavily on the given model. This has raised concerns about the applicability of current solutions, leading researchers to consider more principled approaches to interpretable methods [48]. We address the challenge of developing machine learning systems that are explainable by design, starting from an unlabeled data set. Speciﬁcally, we consider pre-modeling explainability in the context of clustering. A common use of clustering is to identify patterns or discover structural properties in a data set by quantizing the unlabeled points. For instance, k-means clustering may be used to discover coherent groups among a supermarket’s customers. While there are many good clustering algorithms, the resulting cluster assignments can be hard to understand because the clusters may be determined using all the features of the data, and 1 arXiv:2002.12538v2  [cs.LG]  22 Sep 2020(a) Optimal 5-means clusters  (b) Tree based 5-means clusters x≤4.5 2 y≤−4 y≤4 1 4 x≤−3.5 0 3 (c) Threshold tree Figure 1: The optimal 5-means clustering (left) determines uses combinations of both features. The explainable clustering (middle) uses axis-aligned rectangles summarized by the threshold tree (right). Because the clusters contain nearby points, a small threshold tree makes very few mistakes and leads to a good approximation. The beneﬁt of explainability would be more apparent in higher dimensions. there may be no concise way to explain the inclusion of a particular point in a cluster. This limits the ability of users to discern the commonalities between points within a cluster or understand why points ended up in diﬀerent clusters. Our goal is to develop accurate, eﬃcient clustering algorithms with concise explanations of the cluster assignments. There should be a simple procedure using a few features to explain why any point belongs to its cluster. Small decision trees have been identiﬁed as a canonical example of an easily explainable model [ 40, 42], and previous work on explainable clustering uses an unsupervised decision tree [ 13, 21, 23, 24, 33]. Each node of the binary tree iteratively partitions the data by thresholding on a single feature. We focus on ﬁnding k clusters, and hence, we use trees with k leaves. Each leaf corresponds to a cluster, and the tree is as small as possible. We refer to such a tree as a threshold tree. There are many beneﬁts of using a small threshold tree to produce a clustering. Any cluster assignment is explained by computing the thresholds along the root-to-leaf path. By restricting to k leaves, we ensure that each such path accesses at most k−1 features, independent of the data dimension. In general, a threshold tree provides an initial quantization of the data set, which can be combined with other methods for future learning tasks. While we consider static data sets, new data points can be easily clustered by using the tree, leading to explainable assignments. To analyze clustering quality, we consider the k-means and k-medians objectives [36, 53]. The goal is to eﬃciently determine a set of k centers that minimize either the squared ℓ2 or the ℓ1 distance, respectively, of the input vectors to their closest center. Figure 1 provides an example of standard and explainable k-means clustering on the same data set. Figure 1(a) on the left shows an optimal 5-means clustering. Figure 1(b) in the middle shows an explainable, tree-based 5-means clustering, determined by the tree in Figure 1(c) on the right. The tree has ﬁve leaf nodes, and vectors are assigned to clusters based on the thresholds. Geometrically, the tree deﬁnes a set of axis-aligned cuts that determine the clusters. While the two clusterings are very similar, using the threshold tree leads to easy explanations, whereas using a standard k-means clustering algorithm leads to more complicated clusters. The diﬀerence between the two approaches becomes more evident in higher dimensions, because standard algorithms will likely determine clusters based on all of the feature values. 2To reap the beneﬁts of explainable clusters, we must ensure that the data partition is a good approximation of the optimal clustering. While many eﬃcient algorithms have been developed fork-means/medians clustering, the resulting clusters are often hard to interpret [ 6, 28, 43, 51]. For example, Lloyd’s algorithm alternates between determining the best center for the clusters and reassigning points to the closest center [ 34]. The resulting set of centers depends in a complex way to the other points in the data set. Therefore, the relationship between a point and its nearest center may be the result of an opaque combination of many feature values. This issue persists even after dimension reduction or feature selection, because a non-explainable clustering algorithm is often invoked on the modiﬁed data set. As our focus is on pre-modeling explanability, we aim for simple explanations that use the original feature vectors. Even though Figure 1 depicts a situation in which the optimal clustering is very well approximated by one that is induced by a tree, it is not clear whether this would be possible in general. Our ﬁrst technical challenge is to understand the price of explainability in the context of clustering: that is, the multiplicative blowup in k-means (or k-medians) cost that is inevitable if we force our ﬁnal clustering to have a highly constrained, interpretable, form. The second challenge is to actually ﬁnd such a tree eﬃciently. This is non-trivial because it requires a careful, rather than random, choice of a subset of features. As we will see, the kind of analysis that is ultimately needed is quite novel even given the vast existing literature on clustering. 1.1 Our contributions We provide several new theoretical results on explainable k-means and k-medians clustering. Our new algorithms and lower bounds are summarized in Table 1. Basic limitations. A partition into k clusters can be realized by a binary threshold tree with k−1 internal splits. This uses at most k−1 features, but is it possible to use even fewer, say log k features? In Section 3, we demonstrate a simple data set that requires Ω( k) features to achieve a explainable clustering with bounded approximation ratio compared to the optimal k-means/medians clustering. In particular, the depth of the tree might need to be k−1 in the worst case. One idea for building a tree is to begin with a good k-means (or k-medians) clustering, use it to label all the points, and then apply a supervised decision tree algorithm that attempts to capture this labeling. In Section 3, we show that standard decision tree algorithms, such as ID3, may produce clusterings with arbitrarily high cost. Thus, existing splitting criteria are not suitable for ﬁnding a low-cost clustering, and other algorithms are needed. New algorithms. On the positive side, we provide eﬃcient algorithms to ﬁnd a small threshold tree that comes with provable guarantees on the cost. We note that using a small number of clusters is preferable for easy interpretations, and therefore k is often relatively small. For the special case of two clusters ( k= 2), we show (Theorem 4.1) that a single threshold cut provides a constant-factor approximation to the optimal 2-medians/means clustering, with a closely-matching lower bound (Theorem 4.4), and we provide an eﬃcient algorithm for ﬁnding the best cut. For general k, we show how to approximate any clustering by using a threshold tree with k leaves (Algorithm 1). The main idea is to minimize the number of mistakes made at each node in the tree, where a mistake occurs when a threshold separates a point from its original center. Overall, the cost of the explainable clustering will be close to the original cost up to a factor that depends on the tree depth (Theorem 5.1). In the worst-case, we achieve an approximation factor of O(k2) for k-means and O(k) for k-medians compared to the cost of any clustering (e.g., the optimal cost). These results do not depend on the dimension or input size; hence, we get a constant factor approximation when k is constant. 3Approximation lower bounds. Since our upper bounds depend on k, it is natural to wonder whether it is possible to achieve a constant-factor approximation, or whether the cost of explainability grows with k. On the negative side, we identify a data set such that any threshold tree with k leaves must incur an Ω(log k)-approximation for both k-medians and k-means (Theorem 5.9). For this data set, our algorithm achieves a nearly matching bound for k-medians. k-medians k-means k= 2 k> 2 k= 2 k> 2 Upper Bound 2 O(k) 4 O(k2) Lower Bound 2 −1 d Ω(log k) 3 ( 1 −1 d )2 Ω(log k) Table 1: Summary of our upper and lower bounds on approximating the optimal k-medians/means clustering with explainable, tree-based clusters. The values express the factor increase compared to the optimal solution in the worst case. 1.2 Related work The majority of work on explainable methods considers supervised learning, and in particular, explaining predictions of neural networks and other trained models [5, 20, 22, 29, 32, 35, 40, 42, 46, 47, 48, 52]. In contrast, there is much less work on explainable unsupervised learning. Standard algorithms for k-medians/means use iterative algorithms to produce a good approximate clustering, but this leads to complicated clusters that depend on subtle properties of the data set [ 1, 6, 28, 43]. Several papers consider the use of decision trees for explainable clustering [ 13, 21, 23, 24, 33]. However, all prior work on this topic is empirical, without any theoretical analysis of quality compared to the optimal clustering. We also remark that the previous results on tree-based clustering have not considered the k-medians/means objectives for evaluating the quality of the clustering, which is the focus of our work. It is NP-hard to ﬁnd the optimal k-means clustering [4, 19] or even a very close approximation [ 8]. In other words, we expect tree-based clustering algorithms to incur an approximation factor bounded away from one compared to the optimal clustering. One way to cluster based on few features is to use dimensionality reduction. Two main types of dimensionality reduction methods have been investigated for k-medians/means. Work on feature selection shows that it is possible to cluster based on Θ( k) features and obtain a constant factor approximation for k-means/medians [15, 17]. However, after selecting the features, these methods employ existing approximation algorithms to ﬁnd a good clustering, and hence, the cluster assignments are not explainable. Work on feature extraction shows that it is possible to use the Johnson-Lindenstrauss transform to Θ( log k) dimensions, while preserving the clustering cost [ 11, 38]. Again, this relies on running a k-means/medians algorithm after projecting to the low dimensional subspace. The resulting clusters are not explainable, and moreover, the features are arbitrary linear combinations of the original features. Besides explainability, many other clustering variants have received recent attention, such as fair clus- tering [ 2, 9, 12, 16, 26, 30, 37, 49], online clustering [ 14, 18, 25, 31, 41], and the use of same-cluster queries [3, 7, 27, 39]. An interesting avenue for future work would be to further develop tree-based clustering methods by additionally incorporating some of these other constraints or objectives. 42 Preliminaries Throughout we use bold variables for vectors, and we use non-bold for scalars such as feature values. Given a set of points X= {x1,..., xn}⊆ Rd and an integer k the goal of k-medians and k-means clustering is to partition Xinto k subsets and minimize the distances of the points to the centers of the clusters. It is known that the optimal centers correspond to means or medians of the clusters, respectively. Denoting the centers as µ1,..., µk, the aim of k-means is to ﬁnd a clustering that minimizes the following objective cost2(µ1,..., µk) = ∑ x∈X ∥x −c2(x)∥2 2 , where c2(x) = arg minµ∈{µ1,...,µk}∥µ −x∥2. Similarly, the goal of k-medians is to minimize cost1(µ1,..., µk) = ∑ x∈X ∥x −c1(x)∥1 , where c1(x) = arg minµ∈{µ1,...,µk}∥µ −x∥1. As it will be clear from context whether we are talking about k-medians or k-means, we abuse notation and write cost and c(x) for brevity. We also ﬁx the data set and use opt to denote the optimal k-medians/means clustering, where the optimal centers are the medians or means of the clusters, respectively; hence, cost( opt) refers to the cost of the optimal k-medians/means clustering. 2.1 Clustering using threshold trees Perhaps the simplest way to deﬁne two clusters is to use a threshold cut, which partitions the data based on a threshold for a single feature. More formally, the two clusters can be written as ˆCθ,i = ( ˆC1, ˆC2), which is deﬁned using a coordinate i and a threshold θ ∈R in the following way. For each input point x ∈X, we place x = [x1,...,x d] in the ﬁrst cluster ˆC1 if xi ≤θ, and otherwise x ∈ˆC2. A threshold cut can be used to explain 2-means or 2-medians clustering because a single feature and threshold determines the division of the data set into exactly two clusters. For k >2 clusters, we consider iteratively using threshold cuts as the basis for the cluster explanations. More precisely, we construct a binary threshold tree. This tree is an unsupervised variant of a decision tree. Each internal node contains a single feature and threshold, which iteratively partitions the data, leading to clusters determined by the vectors that reach the leaves. We focus on trees with exactly k leaves, one for each cluster {1,2,...,k }, which also limits the depth and total number of features to at most k−1. When clustering using such a tree, it is easy to understand why x was assigned to its cluster: we may simply inspect the threshold conditions on the root-to-leaf path for x. This also ensures the number of conditions for the cluster assignment is rather small, which is crucial for interpretability. These tree-based explanations are especially useful in high-dimensional space, when the number of clusters is much smaller than the input dimension ( k≪d). More formally, a threshold tree T with k leaves induces a k-clustering of the data. Denoting these clusters as ˆCj ⊆X, we deﬁne the k-medians/means cost of the tree as cost1(T) = k∑ j=1 ∑ x∈ˆCj ∥x−median( ˆCj)∥1 cost2(T) = k∑ j=1 ∑ x∈ˆCj ∥x−mean( ˆCj)∥2 2 Our goal is to understand when it is possible to eﬃciently produce a tree T such that cost(T) is not too large compared to the optimal k-medians/means cost. Speciﬁcally, we say that an algorithm is an a-approximation, if the cost is at most a times the optimal cost, i.e., if the algorithm returns threshold tree T then we have cost(T) ≤a·cost(opt), where opt denotes the optimal k-medians/means clustering. 5xi1 ≤0.5 ei1 xi2 ≤0.5 ei2 ... ... xid ≤0.5 eid0 (a) Optimal threshold tree for the data set in Rk−1 consisting of the k−1 standard basis vectors and the all zeros vector. Any optimal tree must use all k−1 features and have depth k−1. (b) The ID3 split results in a 3-means/medians clustering with arbitrarily worse cost than the optimal because it places the top two points in separate clusters. Our algorithm (Section 5) instead starts with the optimal ﬁrst split. Figure 2: Motivating examples showing that (a) threshold trees may need depth k−1 to determine kclusters, and (b) standard decision tree algorithms such as ID3 or CART perform very badly on some data sets. 3 Motivating Examples Using k−1 features may be necessary.We start with a simple but important bound showing that trees with depth less than k (or fewer than k−1 features) can be arbitrarily worse than the optimal clustering. Consider the data set consisting of the k−1 standard basis vectors e1,..., ek−1 ∈Rk−1 along with the all zeros vector. As this data set has k points, the optimal k-median/means cost is zero, putting each point in its own cluster. Unfortunately, it is easy to see that for this data, depth k−1 is necessary for clustering with a threshold tree. Figure 2(a) depicts an optimal tree for this data set. Shorter trees do not work because projecting onto any k−2 coordinates does not separate the data, as at least two points will have all zeros in these coordinates. Therefore, any tree with depth at most k−2 will put two points in the same cluster, leading to non-zero cost, whereas the optimal cost is zero. In other words, for this data set, caterpillar trees such as Figure 2(a) are necessary and suﬃcient for an optimal clustering. This example also shows that Θ( k) features are tight for feature selection [ 17] and provides a separation with feature extraction methods that use a linear map to only a logarithmic number of dimensions [11, 38]. Standard top-down decision trees do not work.A natural approach to building a threshold tree is to (1) ﬁnd a good k-medians or k-means clustering using a standard algorithm, then (2) use it to label all the points, and ﬁnally (3) apply a supervised decision tree learning procedure, such as ID3 [ 44, 45] to ﬁnd a threshold tree that agrees with these cluster labels as much as possible. ID3, like other common decision tree algorithms, operates in a greedy manner, where at each step it ﬁnds the best split in terms of entropy or information gain. We will show that this is not a suitable strategy for clustering and that the resulting tree can have cost that is arbitrarily bad. In what follows, denote by cost(ID3ℓ) the cost of the decision tree with ℓ leaves returned by ID3 algorithm. Figure 2(b) depicts a data set X⊆ R2 partitioned into three clusters X= X0 ·∪X1 ·∪X2. We deﬁne two centers µ0 = (−2,0) and µ1 = (2,0) and for each i∈{0,1}, we deﬁne Xi as 500 i.i.d. points x ∼N(µi,ϵ) 6for some small ϵ> 0. Then, X2 = {(−2,v),(2,v)}where v→∞. With high probability, we have that the optimal 3-means clustering is ( X0,X1,X2), i.e. x ∈X gets label y ∈{0,1,2}such that x ∈Xy. The ID3 algorithm minimizes the entropy at each step. In the ﬁrst iteration, it splits between the two large clusters. As a result (−2,v) and (2,v) will also be separated from one another. Since ID33 outputs a tree with exactly three leaves, one of the leaves must contain a point from X2 together with points from either X0 or X1, this means that cost(ID33) = Ω(v) →∞. Note that cost((X1,X2,X3)) does not depend on v, and hence, it is substantially smaller than cost(ID33). Unlike ID3, the optimal threshold tree ﬁrst separates X2 from X0 ·∪X1, and in the second split it separates X0 and X1. Putting the outliers in a separate cluster is necessary for an optimal clustering. It is easy to extend this example to more clusters or to when ID3 uses more leaves. 4 Two Clusters Using a Single Threshold Cut In this section, we consider the case of k = 2 clusters, and we study how well a single threshold cut can approximate the optimal partition into two clusters. 4.1 Algorithm for k = 2 We present an algorithm to eﬃciently minimize the cost using a single threshold cut. We begin by considering a single feature i and determining the value of the best threshold θ∈R for this feature. Then, we minimize over all features i∈[d] to output the best threshold cut. We focus on the 2-means algorithm; the 2-medians case is similar. For feature i, we ﬁrst sort the input points according to this feature, i.e., assume that the vectors are indexed as x1 i ≤... ≤xn i.Notice that when restricting to this feature, there are only n−1 possible partitions of the data set into two non-empty clusters. In particular, we can calculate the cost of all threshold cuts for the ith feature by scanning the values in this feature from smallest to largest. Then, we compute for each position p∈[n−1] cost(p) = p∑ j=1 xj −µ1(p) 2 2 + n∑ j=p+1 xj −µ2(p) 2 2 , where we denote the optimal centers for these clusters as µ1(p) = 1 p ∑p j=1 xj and µ2(p) = 1 n−p ∑n j=p+1 xj because these are the means of the ﬁrst pand last n−ppoints, respectively. Because there are O(nd) possible thresholds, and naively computing the cost of each requires time O(nd), this would lead to a running time of O(n2d2). We can improve the time to O(nd2 + ndlog n) by using dynamic programming. Pseudo-code for the algorithm and description of the dynamic programming are in Appendix D. 4.2 Theoretical guarantees for k = 2 We prove that there always exists a threshold cut with low cost. Since our algorithm from the previous section ﬁnds the best cut, it achieves the guarantees of this theorem. Theorem 4.1. For any data set X⊆ Rd, there is a threshold cut ˆC such that the 2-medians cost satisﬁes cost( ˆC) ≤2 ·cost(opt), and there is a threshold cut ˆC such that the 2-means cost satisﬁes cost( ˆC) ≤4 ·cost(opt), where opt is the optimal 2-medians or means clustering. 7The key idea of the analysis is to bound the cost of the threshold clustering in terms of the number of points on which it disagrees with an optimal clustering. Intuitively, if any threshold cut must lead to a fairly diﬀerent clustering, then the cost of the optimal 2-medians/means clustering must also be large. We note that it is possible to prove a slightly weaker bound by using the midpoint (for each feature) between the centers. When there are t changes, using the midpoint shows that cost(opt) is at least t times half of the distance between the two centers. In other words, this argument only captures half of the cost. Using Halls theorem, we show that each change corresponds to a pair in the matching, and each such pair contributes to cost(opt) the distance between the centers (not half as before). This improves the bound by a factor of two. The proof for 2-means is in Appendix C. Notation. We denote the optimal clusters as C1 and C2 with optimal centers µ1 and µ2. Notice that we can assume µ1 i ≤µ2 i for each coordinate i because negating the ith coordinate for all points in the dataset does not change the 2-medians/means cost. Assume that a single threshold partitions Xinto ˆC1, ˆC2 such that t= min(|C1∆ ˆC1|,|C1∆ ˆC2|). We refer to these t points as changes and assume that t is the minimum possible over all threshold cuts. If C1,C2 is an optimal 2-medians clustering, then we prove that the cost of ˆC1, ˆC2 is at most twice the optimal 2-medians cost. Similarly, if C1,C2 is an optimal 2-means clustering, then we prove that the cost of ˆC1, ˆC2 is at most four times the optimal 2-means cost. We simply need that the threshold cut ˆC = ( ˆC1, ˆC2) minimizes the number of changes t compared to the optimal clusters. We begin with a structural claim regarding the best threshold cut. This will allow us to obtain a tighter bound on the optimal 2-medians/means cost, compared to the general k> 2 case, in terms of the necessary number of changes. We utilize Hall’s theorem on perfect matchings. Proposition 4.2 (Hall’s Theorem). Let (P,Q) be a bipartite graph. If all subsets P′⊆P have at least |P′| neighbors in Q, then there is a matching of size |P|. Lemma 4.3. Let C1 and C2 be the optimal clustering of X⊆ Rd, and assume that any threshold cut requires t changes. For each i∈[d], there are t disjoint pairs of vectors (pj,qj) in X such that pj ∈C1 and qj ∈C2 and qj i ≤pj i for every j ∈[t]. Proof. Let µ1 and µ2 be the centers for the optimal clusters C1 and C2. Focus on index i∈[d], and assume without loss of generality that µ1 i ≤µ2 i. The t pairs correspond to a matching the following bipartite graph (P,Q). Let Q = C2 and deﬁne P ⊆C1 as the t points in C1 with largest value in their ith coordinate. Connect p ∈P and q ∈Q by an edge if only if qi ≤pi. By construction, a matching with t edges implies our claim. By Hall’s theorem, we just need to prove that P′⊆P has at least |P′|neighbors. Index P = {p1,..., pt}by ascending value of ith coordinate, p1 i ≤···≤ pt i. Now, notice that vertices in P have nested neighborhoods: for all j >j′, the neighborhood of pj′ is a subset of the neighborhood of pj. It suﬃces to prove that pj has at least j neighbors, because this implies that any subset P′⊆P has at least |P′|neighbors, guaranteeing a matching of size |P|= t. Indeed, if |P′|= b then we know that pj ∈P′for some j ≥b, implying that P′has at least j ≥b= |P′|neighbors. Assume for contradiction that pj has at most j−1 neighbors. We argue that the threshold cut xi ≤pj i has fewer than t changes, which contradicts the fact that all threshold cuts must make at least t changes. By our assumption, there are at most j−1 points that are smaller than pj i and belong to the second cluster. By the deﬁnition of P, there are exactly t−j points with a larger ith coordinate than pj i in the ﬁrst cluster. Therefore, the threshold cut xi ≤pj i makes at most ( t−j) + (j−1) <t changes, a contradiction. 84.3 Upper Bound Proof for 2-medians Suppose µ1,µ2 are optimal 2-medians centers for clusters C1 and C2, and that the threshold cut ˆC makes t changes, which is the minimum possible. A simple argument allows us to upper bound the cost of the threshold cut as cost(opt) plus an error term that depends on the number of changes. More formally, Lemma 5.5 (see Section 5.2.5) in the special case of k= 2 implies that cost( ˆC) ≤cost(opt) + t µ1 −µ2 1 . In other words, the core of the argument is to prove that t µ1 −µ2 1 ≤cost(opt). Applying Lemma 4.3 for each coordinate i∈[d] guarantees tpairs of vectors (p1,q1),..., (pt,qt) with the following properties. Each pj i corresponds to the ith coordinate of some point in C1 and qj i corresponds to the ith coordinate of some point in C2. Furthermore, for each coordinate, the t pairs correspond to 2 t distinct points in X. Finally, we can assume without loss of generality that µ1 i ≤µ2 i and qj i ≤pj i, which implies that cost(opt) ≥ d∑ i=1 t∑ j=1 |µ2 i −qj i|+ |pj i −µ1 i| ≥ d∑ i=1 t∑ j=1 (µ2 i −qj i) + (pj i −µ1 i) ≥ d∑ i=1 t∑ j=1 (µ2 i −qj i) + (qj i −pj i) + (pj i −µ1 i) = t· d∑ i=1 (µ2 i −µ1 i) = t µ2 −µ1 1 . 4.4 Lower bounds for k = 2 We next show that optimal clustering is not, in general, realizable with a single threshold cut, except in a small number of dimensions (e.g., d= 1). Our lower bounds on the approximation ratio increase with the dimension, approaching two for 2-medians or three for 2-means. The two lower bounds are based on a data set X⊆ Rd consisting of 2 d points, split into two optimal clusters each with d points. The ﬁrst cluster contains the d vectors of the form 1 −ei, where ei is the ith coordinate vector and 1 is the all-ones vector. The second cluster contains their negations, −1 + ei. Due to the zero-valued coordinate in each vector, any threshold cut must separate at least one vector from its optimal center. In the case of 2-medians, each incorrect cluster assignment incurs a cost of 2 d. The optimal cost is roughly 2 d, while the threshold cost is roughly 4 d (correct assignments contribute ≈2d, plus 2d from the error), leading to an approximation ratio of nearly two. A similar result holds for 2-means. The proof of these two lower bounds is in Appendix B. Theorem 4.4. For any integer d ≥1, deﬁne data set X ⊆Rd as above. Any threshold cut ˆC must have 2-medians cost cost( ˆC) ≥ ( 2 −1 d ) ·cost(opt) and 2-means cost cost( ˆC) ≥3 ( 1 −1 d )2 ·cost(opt), where opt is the optimal 2-medians or means clustering. 95 Threshold trees with k >2 leaves We provide an eﬃcient algorithm to produce a threshold tree with k leaves that constitutes an approximate k-medians or k-means clustering of a data set X. Our algorithm, Iterative Mistake Minimization (IMM), starts with a reference set of cluster centers, for instance from a polynomial-time constant-factor approximation algorithm for k-medians or k-means [1], or from a domain-speciﬁc clustering heuristic. We then begin the process of ﬁnding an explainable approximation to this reference clustering, in the form of a threshold tree with k leaves, whose internal splits are based on single features. The way we do this is almost identical for k-medians and k-means, and the analysis is also nearly the same. Our algorithm is deterministic and its run time is only O(kdnlog n), after ﬁnding the initial centers. As discussed in Section 3, existing decision tree algorithms use greedy criteria that are not suitable for our tree-building process. However, we show that an alternative greedy criterion—minimizing the number of mistakes at each split (the number of points separated from their corresponding cluster center)—leads to a favorable approximation ratio to the optimal k-medians or k-means cost. 5.1 Our algorithm ALGORITHM 1: Iterative Mistake Minimization Input : x 1,..., xn – vectors inRd k – number of clusters Output : root of the threshold tree 1 µ1,... µk ←k-Means(x1,..., xn,k) 2 foreach j ∈[1,...,n ] do 3 yj ←arg min1≤ℓ≤k∥xj −µℓ∥ 4 end 5 return build tree({xj}n j=1,{yj}n j=1,{µj}k j=1) 1 build tree({xj}m j=1,{yj}m j=1,{µj}k j=1): 2 if {yj}m j=1 is homogeneous then 3 leaf.cluster←y1 4 return leaf 5 end 6 foreach i∈[1,...,d ] do 7 ℓi ←min1≤j≤mµyj i 8 ri ←max1≤j≤mµyj i 9 end 10 i,θ ←arg mini,ℓi≤θ<ri ∑m j=1 mistake(xj,µyj ,i,θ ) 11 M ←{j |mistake(xj,µyj ,i,θ ) = 1}m j=1 12 L ←{j |(xj i ≤θ) ∧(j ̸∈M)}m j=1 13 R ←{j |(xj i >θ) ∧(j ̸∈M)}m j=1 14 node.condition←“xi ≤θ” 15 node.lt←build tree({xj}j∈L,{yj}j∈L,{µj}k j=1) 16 node.rt←build tree({xj}j∈R,{yj}j∈R,{µj}k j=1) 17 return node 1 mistake(x,µ,i,θ ): 2 return (xi ≤θ) ̸= (µi ≤θ) ? 1 : 0 Algorithm 1 takes as input a data set X⊆ Rd. The ﬁrst step is to obtain a reference set of k centers {µ1,..., µk}, for instance from a standard clustering algorithm. We assign each data point xj the label yj of its closest center. Then, the build tree procedure looks for a tree-induced clustering that ﬁts these labels. The tree is built top-down, using binary splits. Each node ucan be associated with the portion of the input space that passes through that node, a hyper- rectangular region cell(u) ⊆Rd. If this cell contains two or more of the centersµj, then it needs to be split. We do so by picking the feature i∈[d] and threshold value θ ∈R such that the resulting split xi ≤θ sends at least one center to each side and moreover produces the fewest mistakes: that is, separates the fewest points in X∩cell(u) from their corresponding centers in {µj : 1 ≤j ≤k}∩cell(u). We do not count points whose centers lie outsidecell(u), since they are associated with mistakes in earlier splits. We ﬁnd the optimal split ( i,θ) by searching over all pairs eﬃciently using dynamic programming. We then add this node to the tree, and discard the mistakes (the points that got split from their centers) before recursing on the left and right children. We terminate at a leaf node whenever all points have the same label (i.e., a homogeneous subset). As there are k diﬀerent labels, the resulting tree has exactly k leaves. Figure 3 depicts the operation of Algorithm 1. 10We ﬁrst discuss the running time, and we analyze the approximation guarantees of IMM in Section 5.2. Time analysis of tree building.We sketch how to execute the algorithm in time O(kdnlog n) for an n-point data set. At each step of the top-down procedure, we ﬁnd a coordinate and threshold pair that minimizes the mistakes at this node (line 10 in build tree procedure). We use dynamic programming to avoid recomputing the cost from scratch for each potential threshold. For each coordinate i∈[d], we sort the data and centers. Then, we iterate over possible thresholds. We claim that we can process each node in time O(dnlog n) because each point will aﬀect the number of mistakes at most twice. Indeed, when the threshold moves, either a data point or a center moves to the other side of the threshold. Since we know the number of mistakes from the previous threshold, we count the new mistakes eﬃciently as follows. If a single point switches sides, then the number of mistakes changes by at most one. If a center switches sides, which happens at most once, then we update the mistakes for this center. Overall, each point aﬀects the mistakes at most twice (once when changing sides, and once when its center switches sides). Thus, the running time for each internal node is O(dnlog n). As the tree has k−1 internal nodes, the total time is O(kdnlog n). (a) Optimal 5-means clusters (b) 1st split: 1 mistake caused by this split (1 total mistake) (c) 2nd split: 2 mistakes caused by this split (3 total mistakes) (d) 3rd split: 12 mistakes caused by this split (15 total mistakes) (e) 4th split: 0 mistakes caused by this split (15 total mistakes) Figure 3: Figure 3(a) presents the optimal 5-means clustering. Figures 3(b)–3(e) depict the four splits of the IMM algorithm. The ﬁrst split separates between cluster 1 and the rest, with a single mistake (marked as a red cross). Next, the IMM separates cluster 3 with 2 additional mistakes. The third split separates cluster 2, and this time the minimal number of mistakes is 12 for this split. Eventually, clusters 0 and 4 are separated without any mistakes. 115.2 Approximation guarantee for the IMM algorithm Our main theoretical contribution is the following result. Theorem 5.1. Suppose that IMM takes centers µ1,..., µk and returns a tree T of depth H. Then, 1. The k-medians cost is at most cost(T) ≤(2H+ 1) ·cost(µ1,..., µk) 2. The k-means cost is at most cost(T) ≤(8Hk + 2) ·cost(µ1,..., µk) In particular, IMM achieves worst case approximation factors of O(k) and O(k2) by using any O(1) approxi- mation algorithm (compared to the optimal k-medians/means) to generate the initial centers. We state the theorem in terms of the depth of the tree to highlight that the approximation guarantee may depend on the structure of the input data. If the optimal clusters can be easily identiﬁed by a small number of salient features, then the tree may have depth O(log k). We later provide a lower bound showing that an Ω(log k) approximation factor is necessary for k-medians and k-means (Theorem 5.9). For this data set, our algorithm produces a threshold tree with depth O(log k), and therefore, the analysis is tight for k-medians. We leave it as an intriguing open question whether the bound can be improved for k-means. 5.2.1 Proof Overview for Theorem 5.1 The proof proceeds in three main steps. First, we rewrite the cost of IMM in terms of the minimum number of mistakes made between the output clustering and the clustering based on the given centers. Second, we provide a lemma that relates the cost of any clustering to the number of mistakes required by a threshold clustering. Finally, we put these two together to show that the output cost is at most an O(H) factor larger than the k-medians cost and at most an O(Hk) factor larger than the k-means cost, respectively, where H is the depth of the IMM tree, and the cost is relative to cost( µ1,..., µk). The approximation bound rests upon a characterization of the excess clustering cost induced by the tree. For any internal node u of the ﬁnal tree T, let cell(u) ⊆Rd denote the region of the input space that ends up in that node, and let B(u) be the bounding box of the centers that lie in this node, or more precisely, B(u) = {µj : 1 ≤j ≤k}∩cell(u). We will be interested in the diameter of this bounding box, measured either by ℓ1 or squared ℓ2 norm, and denoted by diam 1(B(u)) and diam2 2(B(u)), respectively. Upper bounding the cost of the tree.The ﬁrst technical claim (Lemma 5.5) will show that if IMM takes centers µ1,..., µk and returns a tree T that incurs tu mistakes at node u∈T, then • The k-medians cost of T satisﬁes cost(T) ≤cost(µ1,..., µk) + ∑ u∈T tudiam1(B(u)) • The k-means cost of T satisﬁes cost(T) ≤2 ·cost(µ1,..., µk) + 2· ∑ u∈T tudiam2 2(B(u)) Brieﬂy, any point x that ends up in a diﬀerent leaf from its correct center µj incurs some extra cost. To bound this, consider the internal node uat which x is separated from µj. Node ualso contains the center µi that ultimately ends up in the same leaf as x. For k-medians, the excess cost for x can then be bounded by ∥µi −µj∥1 ≤diam1(B(u)). The argument for k-means is similar. These ∑ utudiam(B(u)) terms can in turn be bounded in terms of the cost of the reference clustering. 12Lower bounding the reference cost.We next need to relate the cost of the centers µ1,..., µk to the number of mistakes and the diameter of the cells in the tree. Lemma 5.6 will show that if IMM makes tu mistakes at node u∈T, then • The k-medians cost satisﬁes ∑ u∈T tu ·diam1(B(u)) ≤2H·cost(µ1,..., µk). • The k-means cost satisﬁes ∑ u∈T tu ·diam2 2(B(u)) ≤4Hk ·cost(µ1,..., µk). The proof for this is signiﬁcantly more complicated than the upper bound mentioned above. Moreover, it contains the main new techniques in our analysis of tree-based clusterings. The core challenge is that we aim to lower bound the cost of the given centers using only information about the number of mistakes at each internal node. Moreover, the IMM algorithm only minimizes the number of mistakes, and not the cost of each mistake. Therefore, we must show that if every axis-aligned cut in B(u) separates at least tu points x from their centers, then there must be a considerable distance between the points in cell( u) and their centers. To prove this, we analyze the structure of points in each cell. Speciﬁcally, we consider the single-coordinate projection of points in the box B(u), and we order the centers in B(u) from smallest to largest for the analysis. If there are k′centers in node u, we consider the partition of B(u) into 2(k′−1) disjoint segments, splitting at the centers and at the midpoints between consecutive centers. Since tu is the minimum number of mistakes, we must in particular have at least tu mistakes from the threshold cut at each midpoint. We argue that each of these segments is covered at least tu times by a certain set of intervals. Speciﬁcally, we consider the intervals between mistake points and their true centers, and we say that an interval covers a segment if the segment is contained in the interval. This allows us to capture the cost of mistakes at diﬀerent distance scales. For example, if a point is very far from its true center, then it covers many disjoint segments, and we show that it also implies a large contribution to the cost. Claim 5.8 in Section 5.2.5 provides our main covering result, and we use this to argue that the cost of the given centers can be lower bounded in terms of the distance between consecutive centers in B(u). For k-medians, we can directly derive a lower bound on the cost in terms of the ℓ1 diameter diam1(B(u)). For k-means, however, we employ Cauchy-Schwarz, which incurs an extra factor of k in the bound with diam2 2(B(u)). Overall, we sum these bounds over the height H of the tree, leading to the claimed upper bounds in the above lemma. 5.2.2 Preliminaries and Notation for Theorem 5.1 Let µ1,..., µk be the reference centers, and let T be the resulting IMM tree. Each internal node ucorresponds to a value θu ∈R and a coordinate i∈[d]. The tree partitions X into k clusters ˆC1,..., ˆCk based on the points that reach the k leaves in T, where we index the clusters so that leaf j contains the centers µj and ˆµj, where ˆµj is the mean of ˆCj for k-means and the median of ˆCj for k-medians. This provides a bijection between old and new centers (and clusters). Recall that the map c: X→{ µ1,..., µk}associates each point to its nearest center (i.e., c(x) corresponds to the cluster assignment given by the centers {µ1,..., µk}). For a node u∈T, we let Xu denote the surviving data set vectors at node u∈T based on the thresholds from the root to u. We also deﬁne Ju ⊆[k] be the set of surviving centers at node ufrom the set {µ1,..., µk}, where these centers satisfy the thresholds from the root to u. Deﬁne µL,u and µR,u to be the maximal (smallest and largest) coordinate-wise values of the centers in Ju, that is, for i∈[d], we set µL,u i = min j∈Ju µj i, and µR,u i = max j∈Ju µj i. 13In other words, using the previous notation and recalling that B(u) = {µ1,..., µk}∩cell(u), we have that diam1(B(u)) = ∥µL,u −µR,u∥1 and diam 2 2(B(u)) = ∥µL,u −µR,u∥2 2. Recall that tu for node u∈T denotes the number of mistakes incurred during the threshold cut deﬁned by u, where a point x is a mistake at node u if x reaches u, it was not a mistake before, and exactly one of the following two events occurs: {c(x)i ≤θu and xi >θu} or {c(x)i >θu and xi ≤θu}. Let X= Xcor ∪Xmis be a partition of the input data set into two parts, where x is in Xcor if it reaches the same leaf node in T as its center c(x), and otherwise, x is in Xmis. In other words, Xmis contains all points x ∈X that are a mistake at any node u in T, and the rest of the points are in Xcor. We note that the notion of “mistakes” used here is diﬀerent than the deﬁnition of “changes” used for the analysis of 2-means/medians, even though we reuse some of the same notation. We need a standard consequence of the CauchySchwarz inequality to analyze the k-means cost. Claim 5.2. For any a1,...,a m ∈R, it holds that ∑k i=1 a2 i ≥1 k (∑k i=1 ai )2 . Proof. Denote by a the vector ( a1,...,a m) and by b the vector ( 1/ √ k,..., 1/ √ k). By the CauchySchwarz inequality 1 k (∑k i=1 ai )2 = ⟨a,b⟩2 ≤∑k i=1 a2 i We also need two facts, which state the optimal center for a cluster corresponds to mean or median of the points in the cluster, respectively. The proofs of these facts can be found in standard texts [50]. Fact 5.3.For any set S = {x1,..., xn}⊆ Rd, the optimal center under the ℓ2 2 cost is the mean µ = 1 n ∑ x∈Sx. Fact 5.4.For any set S = {x1,..., xn}⊆ Rd, the optimal center µ under the ℓ1 cost is the coordinate-wise median, deﬁned for i∈[d] as µi = median(x1 i,...,x n i). 5.2.3 The Two Main Lemmas and the Proof of Theorem 5.1 To prove the theorem, we state two lemmas that aid in analyzing the cost of the given clustering versus the IMM clustering. The theorem will follow from these lemmas, and we will prove the lemmas in the proceeding subsections. We start with the lemma relating the number of mistakes tu at each node u and the distance between µL,u and µR,u to the cost incurred by the given centers. Lemma 5.5. If IMM takes centers µ1,..., µk and returns a tree T of depth H that incurs tu mistakes at node u∈T, then 1. The k-medians cost of the IMM tree satisﬁes cost(T) ≤cost(µ1,..., µk) + ∑ u∈T tu∥µL,u −µR,u∥1. 2. The k-means cost of the IMM tree satisﬁes cost(T) ≤2 ·cost(µ1,..., µk) + 2· ∑ u∈T tu∥µL,u −µR,u∥2 2. 14We next bound the cost of the given centers in the terms of the number of mistakes in the tree. The key idea is that if there must be many mistakes at each node, then the cost of the given centers must actually be fairly large. Lemma 5.6. If IMM takes centers µ1,..., µk and returns a tree T of depth H that incurs tu mistakes at node u∈T, then 1. The k-medians cost satisﬁes ∑ u∈T tu∥µL,u −µR,u∥1 ≤2H·cost(µ1,..., µk). 2. The k-means cost satisﬁes ∑ u∈T tu∥µL,u −µR,u∥2 2 ≤4Hk ·cost(µ1,..., µk). Combining these two lemmas immediately implies Theorem 5.1. Proof of Theorem 5.1. For k-medians, Lemmas 5.5 and 5.6 together imply that cost(T) ≤cost(µ1,..., µk) + ∑ u∈T tu∥µL,u −µR,u∥1 ≤(2H+ 1) ·cost(µ1,..., µk). For k-means, we have that cost(T) ≤2 ·cost(µ1,..., µk) + 2· ∑ u∈T tu∥µL,u −µR,u∥2 2 ≤(8Hk + 2) ·cost(µ1,..., µk). 5.2.4 Proof of Lemma 5.5 We begin with the k-medians proof (the k-means proof will be similar). Notice that the cost can only increase when measuring the distance to the (suboptimal) center µj instead of the (optimal) center ˆµj for cluster ˆCj, and hence, cost(T) = k∑ j=1 ∑ x∈ˆCj ∥x −ˆµj∥1 ≤ k∑ j=1 ∑ x∈ˆCj ∥x −µj∥1. We can rewrite this sum using the partition Xcor and Xmis of X, using the fact that whenever x ∈Xcor, then the distance is computed with respect to the true center c(x), k∑ j=1 ∑ x∈ˆCj ∥x −µj∥1 = k∑ j=1 ∑ x∈Xcor∩ˆCj ∥x −µj∥1 + k∑ j=1 ∑ x∈Xmis∩ˆCj ∥x −µj∥1 = ∑ x∈Xcor ∥x −c(x)∥1 + k∑ j=1 ∑ x∈Xmis∩ˆCj ∥x −µj∥1 15Starting with the above cost bound, and using the triangle inequality, we see cost(T) ≤ ∑ x∈Xcor ∥x −c(x)∥1 + k∑ j=1 ∑ x∈Xmis∩ˆCj ∥x −µj∥1 ≤ ∑ x∈Xcor ∥x −c(x)∥1 + k∑ j=1 ∑ x∈Xmis∩ˆCj (∥x −c(x)∥1 + ∥c(x) −µj∥1) = cost( µ1,..., µk) + k∑ j=1 ∑ x∈Xmis∩ˆCj ∥c(x) −µj∥1 To control the second term in the ﬁnal line, we must bound the cost of the mistakes. We decompose Xmis based on the node u where x ∈Xmis is ﬁrst separated from its true center c(x) due to the threshold at node u. To this end, consider some point x ∈Xmis ∩ˆCj, where its distance is measured to the incorrect center µj ̸= c(x). Both centers c(x) and µj have survived until node u in the threshold tree T, and hence, both vectors are part of the deﬁnitions of µL,u and µR,u. In particular, we can use the upper bound ∥c(x) −µj∥1 ≤∥µL,u −µR,u∥1. There are tu points in Xmis caused by the threshold at node u, and we have that k∑ j=1 ∑ x∈Xmis∩ˆCj ∥c(x) −µj∥1 ≤ ∑ u∈T tu ·∥µL,u −µR,u∥1. Therefore, we have, as desired cost(T) ≤ cost(µ1,..., µk) + k∑ j=1 ∑ x∈Xmis∩ˆCj ∥x −µj∥1 ≤ cost(µ1,..., µk) + ∑ u∈T tu∥µL,u −µR,u∥1. Analyzing k-means is similar; we incur a factor of two by using Claim 5.2 instead of the triangle inequality: cost(T) ≤ ∑ x∈Xcor ∥x −c(x)∥2 2 + 2 k∑ j=1 ∑ x∈Xmis∩ˆCj (∥x −c(x)∥2 2 + ∥c(x) −µj∥2 2) ≤ 2 ·cost(µ1,..., µk) + 2· k∑ j=1 ∑ x∈Xmis∩ˆCj ∥c(x) −µj∥2 2 ≤ 2 ·cost(µ1,..., µk) + 2· ∑ u∈T tu∥µL,u −µR,u∥2 2 5.2.5 Proof of Lemma 5.6 To prove this lemma, we bound the cost at each node u of tree in terms of the mistakes made at this node. For this lemma, we deﬁne Xcor u to be the set of points in Xthat reach node u in T along with their center c(x). We note that Xcor u diﬀers from Xcor ∩Xu because a point x ∈Xcor u may not make it to Xcor if there is a mistake later on (i.e., Xcor is the union of Xcor u only over leaf nodes). 16Lemma 5.7. For any node u∈T, we have that ∑ x∈Xcoru ∥x −c(x)∥1 ≥tu 2 ·∥µL,u −µR,u∥1. and ∑ x∈Xcoru ∥x −c(x)∥2 2 ≥tu 4k ·∥µL,u −µR,u∥2 2. Proof. Fix a coordinate i∈[d] and a node u∈T. To simplify notation, we let z1 ≤···≤ zk′ denote the sorted values of ith coordinate of the k′≤k centers that survive until node u (so that z1 = µL,u i and zk′ = µR,u i ). Observe that for each x ∈Xcor u , the center c(x) must have survived until node u, and hence, c(x)i equals one of the values zj for j ∈[k′]. We need a deﬁnition that allows us to relate the cost in coordinate i to the distances between z1 and zk′ . For consecutive values (j,j + 1), we say that the pair ( j,j + 1) is covered by x if either • The segment [zj,zj+zj+1 2 ) is contained in the segment [ xi,c(x)i], or • The segment [ zj+zj+1 2 ,zj+1) is contained in the segment [ xi,c(x)i]. We prove the following claim, which enables us to relate the cost in the ith coordinate to the value zk′ −z1 by decomposing this value into the distance between consecutive centers. Claim 5.8. For each j = 1,2,...,k ′−1, the pair (j,j + 1) is covered by at least tu points x ∈Xcor u . Proof. Suppose for contradiction that this does not hold. We argue that we can ﬁnd a threshold value for coordinate ithat makes fewer than tu mistakes. To see this, assume that ( j,j + 1) is covered by fewer than tu points x ∈Xu. In particular, setting the threshold to be zj+zj+1 2 separates fewer than tu points x from their centers c(x). This implies that there are fewer than tu mistakes at node u, which is a contradiction because the IMM algorithm chooses the coordinate and threshold pair that minimizes the number of mistakes. Now this claim suﬃces to prove Lemma 5.7. The only challenge is that we must string together the covering points x to get a bound on zk′ −z1. We start with the k-medians proof. Using the above claim, we can lower bound the contribution of coordinate i to the cost of the given centers. Notice that the values z1 ≤···≤ zk′ partition the interval between z1 = µL,u i and zk′ = µR,u i . Thus, each time x covers a pair (j,j + 1), there must be a contribution of zj+1−zj 2 to the cost |xi−c(x)i|. Because each pair is covered at least tu times by Claim 5.8, we conclude that ∑ x∈Xcoru |xi −c(x)i|≥ tu k′−1∑ j=1 (zj+1 −zj 2 ) = tu 2 (zk′ −z1). To relate the bound to µL,u and µR,u, we note that the above argument holds for each coordinate i∈[d], and we have that ∑ x∈Xcoru ∥x −c(x)∥1 = ∑ i∈[d] ∑ x∈Xcoru |xi −c(x)i|≥ tu 2 ·∥µL,u −µR,u∥1. For the k-means proof, we apply the same argument as above, this time using Claim 5.2 to bound the sum of squared values as ∑ x∈Xcoru |xi −c(x)i|2 ≥tu k′−1∑ j=1 (zj+1 −zj 2 )2 ≥tu k   k′−1∑ j=1 (zj+1 −zj 2 )  2 = tu 4k(zk′ −z1)2, 17and therefore, summing over coordinates i∈[d], we have ∑ x∈Xcoru ∥x −c(x)∥2 2 = ∑ i∈[d] ∑ x∈Xcoru |xi −c(x)i|2 ≥tu 4k ·∥µL,u −µR,u∥2 2. Proof of Lemma 5.6. We start with the k-medians proof. The factor of H arises because the same points x ∈X can appear in at most H sets Xcor u because H is the depth of the tree. More precisely, using Lemma 5.7 for each node u, we have that H·cost(µ1,..., µk) ≥ ∑ u∈T ∑ x∈Xcoru ∥x −c(x)∥1 ≥ ∑ u∈T tu 2 ∥µL,u −µR,u∥1. Applying the same steps for the k-means cost, we have that H·cost(µ1,..., µk) ≥ ∑ u∈T ∑ x∈Xcoru ∥x −c(x)∥2 2 ≥ ∑ u∈T tu 4k∥µL,u −µR,u∥2 2. 5.3 Approximation lower bound To complement our upper bounds, we show that a threshold tree with k leaves cannot, in general, yield better than an Ω(log k) approximation to the optimal k-medians or k-means clustering. Theorem 5.9. For any k≥2, there exists a data set with k clusters such that any threshold tree T with k leaves must have k-medians and k-means cost at least cost(T) ≥Ω(log k) ·cost(opt). The data set is produced by ﬁrst picking krandom centers from the hypercube {−1,1}d, for large enough d, and then using each of these to produce a cluster consisting of the dpoints that can be obtained by replacing one coordinate of the center by zero. Thus the clusters have size d and radius O(1). To prove the lower bound, we use ideas from the study of pseudo-random binary vectors, showing that projecting the centers to any subset of m≲ log2 k coordinates take on all 2m possible values, with each occurring roughly equally often. Then, we show that (i) the threshold tree must be essentially a complete binary tree with depth Ω( log2 k) to achieve a clustering with low cost, and (ii) any such tree incurs a cost of Ω( log k) times more than the optimal for this data set (for both k-medians and k-means). The proof of Theorem 5.9 appears in Appendix A. 6 Conclusion In this paper we discuss the capabilities and limitations of explainable clusters. For the special case of two clusters (k= 2), we provide nearly matching upper and lower bounds for a single threshold cut. For general k >2, we present the IMM algorithm that achieves an O(H) approximation for k-medians and an O(Hk) approximation for k-means when the threshold tree has depth H and k leaves. We complement our upper bounds with a lower bound showing that any threshold tree with k leaves must have cost at least Ω( log k) more than the optimal for certain data sets. Our theoretical results provide the ﬁrst approximation guarantees 18on the quality of explainable unsupervised learning in the context of clustering. Our work makes progress toward the larger goal of explainable AI methods with precise objectives and provable guarantees. An immediate open direction is to improve our results for k clusters, either on the upper or lower bound side. One option is to use larger threshold trees with more than k leaves (or allowing more than k clusters). It is also an important goal to identify natural properties of the data that enable explainable, accurate clusters. For example, it would be interesting to improve our upper bounds on explainable clustering for well-separated data. Our lower bound of Ω( log k) utilizes clusters with diameter O(1) and separation Ω( d), where the hardness stems from the randomness of the centers. In this case, the approximation factor Θ( log k) is tight because our upper bound proof actually provides a bound in terms of the tree depth (which is about log k, see Appendix A.5). Therefore, an open question is whether a Θ( log k) approximation is possible for any well-separated clusters (e.g., mixture of Gaussians with separated means and small variance). Beyond k-medians/means, it would be worthwhile to develop other clustering methods using a small number of features (e.g., hierarchical clustering). Acknowledgements. Sanjoy Dasgupta has been supported by NSF CCF-1813160. Nave Frost has been funded by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (Grant agreement No. 804302). The contribution of Nave Frost is part of a Ph.D. thesis research conducted at Tel Aviv University. References [1] Ankit Aggarwal, Amit Deshpande, and Ravi Kannan. Adaptive sampling for k-means clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques , pages 15–28. Springer, 2009. [2] Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad Mahdian, Benjamin Moseley, Philip Pham, Sergei Vassilvtiskii, and Yuyan Wang. Fair hierarchical clustering. arXiv preprint arXiv:2006.10221, 2020. [3] Nir Ailon, Anup Bhattacharya, and Ragesh Jaiswal. Approximate correlation clustering using same- cluster queries. In Latin American Symposium on Theoretical Informatics , pages 14–27. Springer, 2018. [4] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. NP-hardness of Euclidean sum-of- squares clustering. Machine learning, 75(2):245–248, 2009. [5] David Alvarez-Melis, Hal Daum´ e III, Jennifer Wortman Vaughan, and Hanna Wallach. Weight of evidence as a basis for human-oriented explanations. arXiv preprint arXiv:1910.13503 , 2019. [6] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete algorithms , pages 1027–1035. Society for Industrial and Applied Mathematics, 2007. [7] Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries. In Advances in neural information processing systems , pages 3216–3224, 2016. [8] Pranjal Awasthi, Moses Charikar, Ravishankar Krishnaswamy, and Ali Kemal Sinop. The hardness of approximation of Euclidean k-means. In 31st International Symposium on Computational Geometry, 19SoCG 2015, pages 754–767. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik GmbH, Dagstuhl Publishing, 2015. [9] Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In International Conference on Machine Learning , pages 405–413, 2019. [10] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus- Robert M ˜Aˇ zller. How to explain individual classiﬁcation decisions.Journal of Machine Learning Research, 11(Jun):1803–1831, 2010. [11] Luca Becchetti, Marc Bury, Vincent Cohen-Addad, Fabrizio Grandoni, and Chris Schwiegelshohn. Oblivious dimension reduction for k-means: beyond subspaces and the Johnson-Lindenstrauss lemma. In STOC, 2019. [12] Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems , pages 4955–4966, 2019. [13] Dimitris Bertsimas, Agni Orfanoudaki, and Holly Wiberg. Interpretable clustering via optimal trees. arXiv preprint arXiv:1812.00539 , 2018. [14] Aditya Bhaskara and Aravinda Kanchana Rwanpathirana. Robust algorithms for online k-means clustering. In Proceedings of the 31st International Conference on Algorithmic Learning Theory , pages 148–173, 2020. [15] Christos Boutsidis, Petros Drineas, and Michael W Mahoney. Unsupervised feature selection for the k-means clustering problem. In NIPS, pages 153–161, 2009. [16] Ashish Chiplunkar, Sagar Kale, and Sivaramakrishnan Natarajan Ramamoorthy. How to solve fair k-center in massive data models. arXiv preprint arXiv:2002.07682 , 2020. [17] Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality reduction for k-means clustering and low rank approximation. In STOC, 2015. [18] Vincent Cohen-Addad, Benjamin Guedj, Varun Kanade, and Guy Rom. Online k-means clustering. arXiv preprint arXiv:1909.06861 , 2019. [19] Sanjoy Dasgupta. The hardness of k-means clustering. University of California, San Diego (Technical Report), 2008. [20] Daniel Deutch and Nave Frost. Constraints-based explanations of classiﬁcations. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) , pages 530–541. IEEE, 2019. [21] Ricardo Fraiman, Badih Ghattas, and Marcela Svarc. Interpretable clustering using unsupervised binary trees. Advances in Data Analysis and Classiﬁcation , 7(2):125–145, 2013. [22] Damien Garreau and Ulrike von Luxburg. Explaining the explainer: A ﬁrst theoretical analysis of lime. arXiv preprint arXiv:2001.03447 , 2020. [23] Pierre Geurts, Nizar Touleimat, Marie Dutreix, and Florence d’Alch´ e Buc. Inferring biological networks with output kernel trees. BMC Bioinformatics, 8(2):S4, 2007. 20[24] Badih Ghattas, Pierre Michel, and Laurent Boyer. Clustering nominal data using unsupervised binary decision trees: Comparisons with the state of the art methods. Pattern Recognition, 67:177–185, 2017. [25] Tom Hess and Sivan Sabato. Sequential no-substitution k-median-clustering. arXiv preprint arXiv:1905.12925, 2019. [26] Lingxiao Huang, Shaofeng Jiang, and Nisheeth Vishnoi. Coresets for clustering with fairness constraints. In Advances in Neural Information Processing Systems , pages 7587–7598, 2019. [27] Wasim Huleihel, Arya Mazumdar, Muriel M´ edard, and Soumyabrata Pal. Same-cluster querying for overlapping clusters. In Advances in Neural Information Processing Systems , pages 10485–10495, 2019. [28] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu. A local search approximation algorithm for k-means clustering. In Proceedings of the Eighteenth Annual Symposium on Computational Geometry , pages 10–18, 2002. [29] Jacob Kauﬀmann, Malte Esders, Gr´ egoire Montavon, Wojciech Samek, and Klaus-Robert M¨ uller. From clustering to cluster explanations via neural networks. arXiv preprint arXiv:1906.07633 , 2019. [30] Matth¨ aus Kleindessner, Pranjal Awasthi, and Jamie Morgenstern. Fair k-center clustering for data summarization. In International Conference on Machine Learning , pages 3448–3457, 2019. [31] Edo Liberty, Ram Sriharsha, and Maxim Sviridenko. An algorithm for online k-means clustering. In 2016 Proceedings of the eighteenth workshop on algorithm engineering and experiments (ALENEX) , pages 81–89. SIAM, 2016. [32] Zachary C Lipton. The mythos of model interpretability. Queue, 16(3):31–57, 2018. [33] Bing Liu, Yiyuan Xia, and Philip S Yu. Clustering via decision tree construction. In Foundations and Advances in Data Mining , pages 97–124. Springer, 2005. [34] Stuart Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129– 137, 1982. [35] Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pages 4765–4774, 2017. [36] J. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In Proc. 5th Berkeley Symp. Mathematical Statist. Probability , pages 281–297, 1967. [37] Sepideh Mahabadi and Ali Vakilian. (individual) fairness for k-clustering. arXiv preprint arXiv:2002.06742, 2020. [38] Konstantin Makarychev, Yury Makarychev, and Ilya Razenshteyn. Performance of Johnson-Lindenstrauss transform for k-means and k-medians clustering. In STOC, 2019. [39] Arya Mazumdar and Barna Saha. Clustering with noisy queries. In Advances in Neural Information Processing Systems, pages 5788–5799, 2017. [40] Christoph Molnar. Interpretable Machine Learning. Lulu. com, 2019. https://christophm.github. io/interpretable-ml-book/. 21[41] Michal Moshkovitz. Unexpected eﬀects of online k-means clustering. arXiv preprint arXiv:1908.06818 , 2019. [42] W James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Interpretable machine learning: deﬁnitions, methods, and applications. arXiv preprint arXiv:1901.04592 , 2019. [43] Rafail Ostrovsky, Yuval Rabani, Leonard J Schulman, and Chaitanya Swamy. The eﬀectiveness of Lloyd-type methods for the k-means problem. Journal of the ACM , 59(6):1–22, 2013. [44] J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81–106, 1986. [45] J Ross Quinlan. C4. 5: programs for machine learning . Elsevier, 2014. [46] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should I trust you?: Explaining the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1135–1144. ACM, 2016. [47] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018. [48] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence , 1(5):206–215, 2019. [49] Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In International Workshop on Approximation and Online Algorithms , pages 232–251. Springer, 2019. [50] Hinrich Sch¨ utze, Christopher D Manning, and Prabhakar Raghavan. Introduction to information retrieval. In Proceedings of the International Communication of Association for Computing Machinery Conference , volume 4, 2008. [51] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms . Cambridge university press, 2014. [52] Kacper Sokol and Peter Flach. Limetree: Interactively customisable explanations based on local surrogate multi-output regression trees. arXiv preprint arXiv:2005.01427 , 2020. [53] H. Steinhaus. Sur la division des corp materiels en parties. Bull. Acad. Polon. Sci , 1:801–804, 1956. 22Appendix Organization: • Section A contains the Ω(log k) lower bound for tree-based clustering using k leaves. • Section B provides improved lower bounds for 2-medians or 2-means for a tree with two leaves. • Section C contains the remaining upper bound proof exhibiting a 4-approximation for 2-means. • Section D contains details about improving the running time of the algorithms. A Lower Bound: Threshold Tree with exactly k leaves In this section we show that any threshold tree with k leaves must be an Ω( log k)-approximation, under the k-means and k-medians cost. We will show a data set that will cause many mistakes. This data set consists of k clusters where any two clusters are very far from each other while inside any cluster the points diﬀer by at most two features. Each cluster is created by ﬁrst taking a codeword and then changing one feature at a time to 0. The consequence of this process is that for every feature there are many points that globally are very diﬀerent yet locally all equal to 0. The proof of the lower bound has a few steps: 1. In Section A.1 we show that there is a code such that (i) every two points are far apart, and (ii) when inspecting any O(log k) features, many codewords are consistent with this local view. From thiscode we construct our data set with dk points and cost(opt) = O(dk). 2. In Section A.2 we prove that the clusters induced by the threshold tree T are similar to the original clusters, except for at most k points in each cluster. These points will cause cost( T) to be large. 3. In Section A.3 we uncover a few properties of any threshold tree created by an O(log k)-approximation algorithm: up until level O(log k) the tree has to be complete and no feature is used more than once. 4. In Section A.4 we put together all the claims and show that each level causes Ω( klog k) mistakes, each with a cost of Ω(d), thus cost(T) = Ω(kdlog k) which proves the lower bound of Ω(log k)-approximation. Data set construction. We ﬁrst take k codewords v1,..., vk ∈ {+1,−1}d that have the properties described in Claim A.2. From each codeword v we create d data points, Xv, each time by changing exactly one feature to 0 . In total we have dk points in the data set, X= ∪iXvi . The cost of the clustering that cluster together all points that belong to the same vector vi is O(dk), as the cost of each point is Θ(1) .Thus, cost(opt) ≤O(dk). A.1 The data set Proposition A.1(Hoeﬀdings inequality). Let X1,...,X n be independent random variables, where for each i, Xi ∈[0,1]. Deﬁne the random variable X = ∑n i=1 Xi. Then, for any t≥0, we have Pr(|X−E[X]|≥ t) ≤ 2e−2t2 n . Claim A.2.For any k≥3, there are k points C ⊆{±1}d that have the following properties for any ϵ≥ln(k)√ k : 1. d= k3 2. for every c ̸= c′∈C their distance is linear, i.e., |{i: ci ̸= c′ i}|≥ d/4. 233. for every ℓ≤ln(k) 50 indexes in [d], and every assignment to these indexes, the number of points in C that has these assignment is at least k(1/2ℓ −ϵ) Proof. Take k random points in {±1}d. We will show that the probability that all properties hold is bigger than 0 and this will prove our claim using the probabilistic method. To prove the second property, we use Hoeﬀding’s inequality and union bound. We can bound the probability that any two points in C agree by more than 3 d/4 coordinates by 2 k2e−d/8 <1 −e−1 for k≥3. To prove the third property we again use Hoeﬀding’s inequality and union bound. This time though we have k random variables, one for each point. There are (d ℓ ) possible ℓ coordinates, and there are 2 ℓ possible assignments to these coordinates. For speciﬁc ℓ coordinates and an assignment to these coordinates the expected number of points in C that has the speciﬁc assignment is k/2ℓ. By Hoeﬀding’s inequality, the probability that we deviate by ϵk is less than e−ϵ2k. The probability that the last property does not hold is bounded by (d ℓ ) 2ℓ+1e−2ϵ2k ≤eℓln d+2ℓ+1−2ϵ2k. Thus for ϵ≥ln(k)√ k , the last term is smaller than e−1. A.2 The cluster created by a threshold tree Claim A.3. For any threshold tree T with at most k leaves, and for any codeword v, the leaf containing v also contains at least d−k points of Xv. Proof. There are at most k≥3 leaves in T, thus in the root to leaf path of the codeword v there are at most k−1 features. Hence, all data points in Xv that agree on this features must reach the same leaf and be in the same cluster. There are at least d−k such points in Xv. Claim A.4. If there are α points from Xv1 and β points from Xv2 , v1 ̸= v2, that are in the same cluster in T, then their contribution to cost(T) is at least 1 4 min(α,β)d. The claim holds both under the ℓ1 cost and the ℓ2 squared cost. Proof. Denote the center that contains α points from Xv1 and β points from Xv2 by µ. Without loss of generality α ≤β. We can disjointly match α points from the two diﬀerent clusters ( x1,y1),..., (xα,yα), which means that their contribution to cost( T) is at least α∑ j=1 xj −µ 2 2 + µ −yj2 2 ≥ α∑ j=1 1 2 xj −yj2 2 ≥1 2 ·α· (d 4 −2 ) ·4 ≥dα 4 , where the ﬁrst inequality follows Claim 5.2, the second inequality follows from Claim A.2 and the fact that if two codewords are diﬀerent by at least d/4 features, then the points diﬀer by at least d/4 −2 features, each contributing a cost of 4, the third inequality follows from the fact that d= k3 ≥16 for k≥3. Similarly for the ℓ1 cost α∑ j=1 xj −µ  1 + α∑ j=1 µ −yj 1 ≥ xj −yj 1 ≥α· (d 4 −2 ) ·2 ≥dα 4 . 24A.3 The threshold tree The next two claims prove that if a feature is used twice or the tree is not complete until level ln(k) 50 , then the clustering tree T cannot be an O(log k)-approximation because it shows that cost(T) ⪆ d2 ≫log k·cost(opt). Claim A.5. Fix a threshold tree T with k ≥3 leaves. If there is a feature that is used twice on the same root-to-leaf path in T, then cost(T) ≥d(d−k) 4 . Proof. The proof is composed of two steps, ﬁrst we show that if a feature is used twice then there is leaf that it is unreachable by a codeword. Then we will show that this implies that two codewords share the same cluster, and thus cost(T) is high. Assume that the there are two nodes in T, both of them use the same feature i, one with threshold θ and the other with threshold θ′. If θ= θ′then there is a leaf that is not reachable. Otherwise, the two thresholds divide the line into three parts, and since the codewords have only two values, there is a leaf unreachable by any codeword. Summing up these two cases, there is a leaf that is not reached by any codeword. From the pigeonhole principle there are two codewords that share the same cluster which is a contradiction using Claims A.3 and A.4. Claim A.6. If threshold T contains a leaf at depth less than ln k 50 , then cost(T) ≥d(d−k) 4 . The claim is true both under the k-means and the k-medians cost. Proof. Assume T is not complete until level log k 50 . So there is a leaf at a level smaller than log k 50 . By the construction of the data set, there are at least ( 1 2ℓ −ϵ)k> 1 codewords that reach this leaf. The claim follows from Claims A.3 and A.4. A.4 Proof of Theorem 5.9 Assume by contradiction that T is an O(log k)-approximation. From Claim A.3 we deduce that for each codeword v, at least d−k points from Xv will be in the same cluster. From Claim A.4 and the assumption that T is O(log k)-approximation we get that each d−k such points must be in its own cluster, this cluster will be called the main cluster of Xv. The only values that features can get in the data set are +1 ,−1 or 0. Thus, we can assume, without loss of generality, that each threshold is either 0 .5 or −0.5. Focus on some node in T at level ℓ with feature i and threshold θ. If θ= 0.5, then for all codewords v with vi = 1 the point in Xv with vi will be separated for its main cluster. From the construction of the data set, there are at least ( 1 2ℓ −ϵ)k such points. Similarly for θ= −0.5, we can show that there are at least ( 1 2ℓ −ϵ)k such points. From Claim A.5, we deduce that these mistakes are disjoint. Applying Claim A.6, there are 2ℓ−1 nodes at each level up until level log k 50 .Hence, total number of mistakes, i.e., points that will not go with their codeword, can be lower bounded by the following using ϵ= ln(k)√ k and large enough k: log k 50∑ ℓ=1 2ℓ−1 (1 2ℓ −ϵ ) k≥klog k 200 . Thus, from Claim A.3 we can lower bound the cost of T: cost(T) ≥kdlog k 200 = Ω(log k)cost(opt) 25A.5 IMM Upper Bound for this dataset We sketch the proof that the IMM algorithm produces a tree of depth O(log k) for the above dataset construction with high probability. In particular, the upper bound from Theorem 5.1 is tight for k-medians up to the leading constant for this dataset. The analysis will follow the standard bound on the maximum clique size in a random graph. Consider ﬁxing any ℓ= 3 log2 k coordinates to ±1. When the set of k centers C is chosen uniformly at random from {±1}d and d = k3, we show that with high probability there are at most ℓ centers consistent with these values. When IMM builds the tree, it always chooses a threshold that reduces the number of centers in the children of the current node, and hence, it never splits on the same feature twice. Moreover, it stops the recursion when there is a single center in a leaf. Therefore, after 3 log2 k thresholds, the remaining depth of the tree is at most 3 log 2 k, and hence, the total depth of the tree is at most 6 log 2 k as well. More formally, let σ∈{±1}ℓ be any sign pattern, and let CI,σ be set of centers having pattern σ when projected onto coordinates I ⊆[d] with |I|= ℓ. Then, using the standard upper bound on the binomial coeﬃcient, we have Pr [ |CI,σ|≥ ℓ ] ≤Pr [ |CI,σ|= ℓ ] ≤E [ |{I : |CI,σ|= ℓ}| ] = (d ℓ ) 2−ℓ2 ≤ (de ℓ )ℓ 2−ℓ2 = (k3e 2ℓℓ )ℓ . Therefore, plugging in 2 ℓ = k3, we see that this probability is at most ( e/ℓ)ℓ. Taking a union bound over the 2ℓ possible settings of σ∈{±1}ℓ shows that the probability that there are ℓ centers consistent with any σ tends to zero as k increases. B Lower bounds for two clusters Without loss of generality we can assume that d≥2. We use the following dataset for both 2-medians and 2-means. It consists of 2 d points, partitioned into two clusters of size d, which are the points with Hamming distance exactly one from the vector with all 1 entries and the vector with all −1 entries: Optimal Cluster 1 Optimal Cluster 2 (0,−1,−1,−1 ..., −1) (0 ,1,1,1 ..., 1) (−1,0,−1,−1 ..., −1) (1 ,0,1,1 ..., 1) (−1,−1,0,−1 ..., −1) (1 ,1,0,1 ..., 1) ... ... (−1,−1,−1,−1 ..., 0) (1 ,1,1,1 ..., 0) Let ˆC = ( ˆC1, ˆC2) be the best threshold cut. 2-medians lower bound. The cost of the cluster with centers (1 ,..., 1) and (−1,..., −1) is 2d, as each point is responsible for a cost of 1 . Thus, cost(opt) ≤2d. There is a coordinate i and a threshold θ that deﬁnes the cut ˆC. For any coordinate i, there are only three possible values: −1,0,1. Thus θ is either in ( −1,0) or in (0 ,1). Without loss of generality, assume that θ∈(−1,0) and i= 1. Thus, the cut is composed of two clusters: one of size d−1 and the other of size d+ 1, in the following way: 26Cluster ˆC1 Cluster ˆC2 (−1,0,−1,−1 ..., −1) (1 ,0,1,1 ..., 1) (−1,−1,0,−1 ..., −1) (1 ,1,0,1 ..., 1) ... ... (−1,−1,−1,−1 ..., 0) (1 ,1,1,1 ..., 0) (0,1,1,1 ..., 1) (0,−1,−1,−1 ..., −1) Using Fact 5.4, an optimal center of the ﬁrst cluster is all −1, and the optimal center for the second cluster is all 1. The cost of the ﬁrst cluster is d−1, as each point costs 1. The cost of the second cluster is composed of two terms d for all points that include 1 in at least one coordinate and the cost of point (0,−1,..., −1) is 2(d−1) + 1. So the total cost is 4 d−2. Thus cost( ˆC) ≥(2 −1/d)cost(opt). 2-means lower bound. Focus on the clustering with centers ((d−1)/d,..., (d−1)/d) and ( −(d−1)/d,..., −(d−1)/d). The cost of each point in the data is composed of (1) one coordinate with value zero, and the cost of this coordinate is ((d−1)/d)2 (2) d−1 coordinates each with cost1/d2.Thus, each point has a cost of(d−1)2 /d2+d−1/d2. Thus, the total cost is 2(d−1)2+2(d−1) d = 2(d−1). This implies that cost( opt) ≤2(d−1). Assume without loss of generality that ˆC is deﬁned using coordinate i = 1 and threshold −0.5. The resulting clusters ˆC1 and ˆC2 are as in the case of 2-medians. The optimal centers are (see Fact 5.3): ( −1,−d−2 d−1,..., −d−2 d−1 ) and (d−1 d+ 1,d−2 d+ 1,..., d−2 d+ 1 ) . We want to lower bound cost( ˆC). We start with the cost of the ﬁrst cluster, i.e. ˆC1. To do so for each point in ˆC1, we will evaluate the contribution of each coordinate to the cost (1) the ﬁrst coordinate adds 0 to the cost (2) the coordinate with value 0, adds ( d−2 d−1 )2 to the cost (3) the rest of the d−2 coordinates adds 1/(d−1)2. Thus, each point in ˆC1 adds to the cost ( d−2 d−1 )2 + d−2 (d−1)2 = d−2 d−1 . Since ˆC1 contains d−1 points, its total cost is d−2. Moving on to evaluating the cost of ˆC2, the cost of the point (0 ,−1,..., −1) is composed of two terms (1) the ﬁrst coordinate adds ( d−1 d+1 )2 to the cost (2) each of the other d−1 coordinates adds ( 1 + d−2 d+1 )2 to the cost. Thus, this point adds (d−1 d+ 1 )2 + (d−1) ( 1 + d−2 d+ 1 )2 = (d−1)d(4d−3) (d+ 1)2 . Similarly, the point (0,1,..., 1) adds to the cost (d−1 d+ 1 )2 + (d−1) ( 1 −d−2 d+ 1 )2 = (d−1)(d+ 8) (d+ 1)2 . Finally, each of the d−1 remaining points in ˆC2 adds to the cost ( 1 −d−1 d+ 1 )2 + (d−2 d+ 1 )2 + (d−1) ( 1 −d−2 d−1 )2 = d2 + 5d−1 (d+ 1)2 27Thus, the cost of ˆC2 is (d−1)(5d2 + 3d+ 7) (d+ 1)2 Summing up the costs of ˆC1 and ˆC2, for d≥2 cost( ˆC) ≥(d−2) + (d−1)(5d2 + 3d+ 7) (d+ 1)2 ≥6(d−1) ( 1 −1 d )2 ≥3 ( 1 −1 d )2 ·cost(opt) C Upper Bound Proof for 2-Means We show that there is a threshold cut ˆC with 2-means cost satisfying cost( ˆC) ≤4 ·cost(opt). We could just use the same proof idea as in the 2-medians case that ﬁrst applies Lemma 5.5 and then uses the matching result, Lemma 4.3. This leads to a 6-approximation, instead of 4. The reason is that we apply twice Claim 5.2, which is not tight. Improving the approximation to 4 requires us to apply Claim 5.2 only once. Suppose µ1,µ2 are optimal 2-means centers for the clusters C1 and C2. Let t= min(|C1∆ ˆC1|,|C1∆ ˆC2|) be the minimum number of changes for any threshold cut ˆC1, ˆC2, and deﬁne Xmis to the set of t points in the symmetric diﬀerence, where X= Xcor ∪Xmis and Xcor ∩Xmis = ∅. Using the same argument as in the proof of Lemma 5.5, we have cost( ˆC) ≤ 2∑ j=1 ∑ x∈Xcor∩ˆCj ∥x −µj∥2 2 + 2∑ j=1 ∑ x∈Xmis∩ˆCj ∥x −µj∥2 2 = ∑ x∈Xcor ∥x −c(x)∥2 2 + ∑ x∈Xmis∩ˆC1 ∥x −µ1∥2 2 + ∑ x∈Xmis∩ˆC2 ∥x −µ2∥2 2 ≤ cost(opt) + ∑ x∈Xmis∩ˆC1 ∥x −µ1∥2 2 + ∑ x∈Xmis∩ˆC2 ∥x −µ2∥2 2 (1) The goal now is to bound the latter two terms using cost(opt). This term measures the distance of each x ∈Xmis from the “other” center, i.e., not c(x). Claim C.1. cost(opt) ≥1 3 ∑ x∈Xmis∩ˆC1 ∥x −µ1∥2 2 + 1 3 ∑ x∈Xmis∩ˆC2 ∥x −µ2∥2 2 Using Claim C.1, together with Inequality (1) we have cost( ˆC) ≤cost(opt) + 3·cost(opt) = 4 ·cost(opt), and this completes the proof. Proof of Claim C.1. Denote the tpoints in Xmis by Xmis = {r1,..., rt}.Assume that the ﬁrst ℓpoints are in the ﬁrst optimal cluster, r1,..., rℓ ∈C1, and the rest are in the second cluster, rℓ+1,..., rt ∈C2. Applying Lemma 4.3 for each coordinate i∈[d] guarantees t pairs of vectors (p1,q1),..., (pt,qt) with the following properties. Each pj i corresponds to the ith coordinate of some point in C1 and qj i corresponds to the ith coordinate of some point in C2. Furthermore, for each coordinate, the t pairs correspond to 2 t distinct points in X. Finally, we can assume without loss of generality that µ1 i ≤µ2 i and qj i ≤pj i. For each point rj in the ﬁrst ℓ points in Xmis, if rj i ≥pj i then we can replace pj with rj, thus we can assume without loss of generality that pj i ≥rj i. We next show that cost(opt) is lower bounded by a function 28of t. There will be two cases depending on whether pj i ≤µ2 i or not. The harder case is the ﬁrst where the improvement of the approximation from 6 to 4 arises. Instead of ﬁrst bounding the distance between rj and its new center using the distance to its original center and then accounting for µ1 −µ22 2 , we directly account for the distance between rj and its new center. Case 1: if pj i ≤µ2 i, then Claim 5.2 implies that (µ2 i −qj i)2 + (pj i −µ1 i)2 + (µ1 i −rj i)2 ≥ 1 3(µ2 i −qj i + pj i −µ1 i + µ1 i −rj i)2 = 1 3((µ2 i −qj i) + (pj i −rj i))2 ≥1 3(µ2 i −rj i)2. The last inequality follows from qj i ≤pj i and rj i ≤pj i, which imply that (µ2 i −qj i) + (pj i −rj i) ≥µ2 i −rj i ≥0, which means ((µ2 i −qj i) + (pj i −rj i))2 ≥(µ2 i −rj i)2. Case 2: if µ2 i ≤pj i, then again Claim 5.2 implies that (pj i −µ1 i)2 + (µ1 i −rj i)2 ≥ (µ2 i −µ1 i)2 + (µ1 i −rj i)2 ≥1 2(µ2 i −µ1 i + µ1 i −rj i)2 = 1 2(µ2 i −rj i)2, where in the ﬁrst inequality we use ( pj i −µ1 i)2 ≥(µ2 i −µ1 i)2. The two cases imply that for 1 ≤j ≤ℓ (µ2 i −qj i)2 + (pj i −µ1 i)2 + (µ1 i −rj i)2 ≥1 3(µ2 i −rj i)2. Similarly for each point rj in the last t−ℓ points in Xmis, we have (µ2 i −qj i)2 + (pj i −µ1 i)2 + (µ2 i −rj i)2 ≥1 3(µ1 i −rj i)2. Putting these together we have cost(opt) ≥ d∑ i=1 ℓ∑ j=1 (µ2 i −qj i)2 + (pj i −µ1 i)2 + (µ1 i −rj i)2 + d∑ i=1 t∑ j=ℓ+1 (µ2 i −qj i)2 + (pj i −µ1 i)2 + (µ2 i −rj i)2 ≥ 1 3 ℓ∑ j=1 d∑ i=1 (µ2 i −rj i)2 + 1 3 t∑ j=ℓ+1 d∑ i=1 (µ1 i −rj i)2 = 1 3 ℓ∑ j=1 rj −µ22 2 + 1 3 t∑ j=ℓ+1 rj −µ12 2 = 1 3 ∑ x∈Xmis∩ˆC1 ∥x −µ1∥2 2 + 1 3 ∑ x∈Xmis∩ˆC2 ∥x −µ2∥2 2 D Eﬃcient Implementation via Dynamic Programming for k = 2 D.1 The 2-means case The psudo-code for ﬁnding the best threshold for k= 2 depicted in Algorithm 2. In time O(d) we can calculate cost(p+ 1) and the new centers by using the value cost(p) and the previous centers. Throughout the computation we save in memory 29ALGORITHM 2: Optimal Threshold for2-means Input : x1,..., xn – vectors inRd Output :i – Coordinate θ – Threshold 1 best cost ←∞ 2 best coordinate ←null 3 best threshold ←null 4 u←∑n j=1 xj2 2 5 foreach i∈[1,...,d ] do 6 s ←zeros(d) 7 r ←∑n j=1 xj 8 X← sorted(x1,..., xn by coordinatei) 9 foreach xj ∈X do 10 s ←s + xj 11 r ←r −xj 12 cost ←u−1 j ∥s∥2 2 − 1 n−j ∥r∥2 2 13 if cost <best cost and xj i ̸= xj+1 i then 14 best cost ←cost 15 best coordinate ←i 16 best threshold ←xj i 17 end 18 end 19 end 20 return best coordinate,best threshold 1. Two vectors sp = ∑p j=1 xj and rp = ∑n j=p+1 xj. 2. Scalar u= ∑n j=1 xj2 2 We also make use of the identity: cost(p) = u−1 p∥sp∥2 2 − 1 n−p∥rp∥2 2 . 30This identity is correct because cost(p) = p∑ j=1 xj −µ1(p) 2 2 + n∑ j=p+1 xj −µ2(p) 2 2 = p∑ j=1 xj2 2 −2 p∑ j=1 ⟨xj,µ1(p)⟩+ p∑ j=1 µ1(p) 2 2 + n∑ j=p+1 xj2 2 −2 n∑ j=p+1 ⟨xj,µ2(p)⟩+ n∑ j=p+1 µ2(p) 2 2 = n∑ j=1 xj2 2 −2⟨ p∑ j=1 xj,µ1(p)⟩+ 1 p  p∑ j=1 xj  2 2 − 2⟨ n∑ j=p+1 xj,µ2(p)⟩+ 1 n−p  n∑ j=p+1 xj  2 2 = n∑ j=1 xj2 2 −2 p⟨sp,sp⟩+ 1 p∥sp∥2 2 − 2 n−p⟨rp,rp⟩+ 1 n−p∥rp∥2 2 = u−1 p∥sp∥2 2 − 1 n−p∥rp∥2 2 By invoking this identity, we can quickly compute the cost of placing the ﬁrst p points in cluster one and the last n−p points in cluster two. Each such partition can be achieved by using a threshold θ between xp i and xp+1 i . Our algorithm computes these costs for each feature i∈[d]. Then, we output the feature i and threshold θ that minimizes the cost. This guarantees that we ﬁnd the best possible threshold cut. Overall, Algorithm 2 iterates over the d features, and for each feature it sorts the n vectors according to their values in the current feature. Next, the algorithm iterates over the n vectors and for each potential threshold, it calculates the cost by evaluating the inner product of two d-dimensional vectors. Overall its runtime complexity is O ( nd2 + ndlog n ) . D.2 The 2-medians case The high level idea of a ﬁnding an optimal 2-medians cut is similar to the 2-means algorithm. The algorithm goes over all possible thresholds. For each threshold, it ﬁnds the optimal centers and calculates the cost accordingly. Then, it outputs the threshold cut that minimizes the 2-medians cost. Updating cost. To update the cost we need to show how to express cost(p+ 1) in terms of cost(p). We know that cost(p+ 1) is equal to cost(p+ 1) = ∑ x∈C1 x−µ1(p+ 1)  1 + ∑ x∈C2 x−µ2(p+ 1)  1 . For every feature i∈[d], there are n−1 thresholds to consider. After sorting by this feature, we can consider all splits into C1 and C2, where C1 contains the p smallest points, and C2 contains the n−p largest points. We increase p from p= 1 to p= n−1, computing the clusters and cost at each step. If p is odd then the median of C1 (i.e., the optimal center of C1) does not change compared to p−1. The only contribution to the cost is the point x that moved from C2 to C1. If p is even, then at each coordinate there are two cases, 31ALGORITHM 3: Optimal Threshold for2-medians Input : x1,..., xn – vectors inRd Output :i – Coordinate θ – Threshold 1 best cost ←∞ 2 best coordinate ←null 3 best threshold ←null 4 foreach i∈[1,...,d ] do 5 µ2(0) ←median(x1,... xn) 6 cost ←∑n j=1 xj −µ2(0)  1 7 X← sorted(x1,..., xn by coordinatei) 8 foreach j ∈[1,...,n −1] do 9 µ1(j) ←median(x1,... xj) 10 µ2(j) ←median(xj+1,... xn) 11 cost ←cost + xj −µ1(j)  1 − xj −µ2(j−1)  1 12 if cost <best cost and xj i ̸= xj+1 i then 13 best cost ←cost 14 best coordinate ←i 15 best threshold ←xj i 16 end 17 end 18 end 19 return best coordinate,best threshold depending on whether the median changes or not. If it changes, then let ∆ denote the change in cost of the points in C1 that are smaller than the median. By symmetry, the change in the cost of the points that are larger is −∆. Thus, the change of the cost is balanced by the points that are larger and smaller than the median. Similar reasoning holds for the other cluster C2. Therefore, we conclude that moving x from C2 to C1 changes the cost by exactly x −µ1(p+ 1)  1 − x −µ2(p)  1. Thus, we have the following connection between cost(p+ 1) and cost(p): cost(p+ 1) = cost(p) + x −µ1(p+ 1)  1 − x −µ2(p)  1 . Updating centers. For each p, the cost update relies on eﬃcient calculations of the centers µ1(p) and µ2(p+ 1). The centers µ1(p),µ2(p) are the medians of the clusters at the pth threshold. Note that moving from the pth thresold to the ( p+ 1)th will only change the clusters by moving one vector from one cluster to the other. We can determine the changes eﬃciently by using d arrays, one for each coordinate. Each array will contain (pointers to) the input vectors Xsorted by their ith feature value. As we move the threshold along a single coordinate, we can read oﬀ the partition into two clusters, and we can compute the median of each cluster by considering the midpoint in the sorted list. Overall, this procedure computes the cost of each threshold, while also determining the partition into two clusters and their centers (medians). The time is O(ndlog n) to sort by each feature, and O(nd2) to compute cost(p) for each p∈[n] and each feature. Therefore, the total time for the 2-medians algorithm is O(nd2 + ndlog n). 32",
      "meta_data": {
        "arxiv_id": "2002.12538v2",
        "authors": [
          "Sanjoy Dasgupta",
          "Nave Frost",
          "Michal Moshkovitz",
          "Cyrus Rashtchian"
        ],
        "published_date": "2020-02-28T04:21:53Z",
        "pdf_url": "https://arxiv.org/pdf/2002.12538v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the problem of designing explainable clustering algorithms by leveraging small decision trees (threshold trees) to partition data, aiming for both interpretability and provable quality guarantees. The core contributions include: 1) Demonstrating that popular top-down decision tree algorithms can lead to arbitrarily poor clusterings for explainable objectives. 2) Proving a fundamental lower bound of \"Ω(log k)\" for the approximation factor achievable by any tree-induced clustering with k leaves for k-means and k-medians. 3) For the special case of k=2 clusters, developing an efficient algorithm that achieves constant-factor approximations (2 for k-medians, 4 for k-means) with a single threshold cut, along with nearly matching lower bounds. 4) Introducing the \"Iterative Mistake Minimization (IMM)\" algorithm for general k, which provides approximation factors of O(H) for k-medians and O(Hk) for k-means, where H is the tree depth. In the worst case, this translates to O(k) and O(k^2) respectively. These are the first algorithms with provable guarantees independent of dimension and input size for explainable clustering.",
        "methodology": "The methodology centers on constructing binary threshold trees where each internal node splits the data based on a single feature and a threshold. This design ensures that clusters, corresponding to tree leaves, are easily explained by the conditions along the root-to-leaf path. Cluster quality is quantitatively assessed using the k-means (squared L2 distance) and k-medians (L1 distance) objectives. For k=2, an efficient algorithm is proposed that exhaustively searches for the optimal single threshold cut across all features by sorting points and using dynamic programming to minimize cost. For general k, the paper introduces the Iterative Mistake Minimization (IMM) algorithm: 1) It starts with an initial set of k cluster centers (obtained from a standard clustering algorithm). 2) Each data point is assigned a label corresponding to its closest initial center. 3) The tree is built top-down; at each node, the algorithm chooses the feature and threshold that minimizes the number of 'mistakes' (points whose assigned center falls on the opposite side of the split). 4) Mistaken points are then discarded for subsequent splits. 5) The recursion terminates when a node contains a homogeneous set of points (all assigned to the same center). The theoretical analysis employs techniques such as Hall's theorem and bounding mistake costs in terms of center distances and tree depth.",
        "experimental_setup": "The paper primarily focuses on theoretical guarantees and algorithmic design, rather than empirical evaluation. Therefore, there is no explicit experimental setup involving specific datasets, benchmarks, or validation methods. Instead, theoretical data sets are constructed to prove lower bounds and demonstrate the necessity of certain tree characteristics or the suboptimality of existing approaches. For instance, specific point configurations (e.g., k-1 standard basis vectors and the zero vector, or 2d points with Hamming distance one) are used to illustrate worst-case scenarios and establish approximation lower bounds. The Iterative Mistake Minimization (IMM) algorithm is designed to use an initial set of k centers, which can be obtained from 'a polynomial-time constant-factor approximation algorithm for k-medians or k-means' or a 'domain-speciﬁc clustering heuristic,' but the paper does not detail which specific initial clustering method was used in practice or for evaluation.",
        "limitations": "The paper identifies several limitations: 1) The approximation factors for the general k case depend on the tree depth H (O(H) for k-medians, O(Hk) for k-means), which can be O(k) in the worst case, making the guarantees less strong for very large k. 2) The maximum depth of a threshold tree might need to be k-1 in the worst case to achieve an optimal clustering (e.g., for separating k distinct points). 3) Popular top-down decision tree algorithms like ID3 or CART are shown to be unsuitable for this problem, as they can produce clusterings with arbitrarily high cost. 4) A fundamental limitation is established by the \"Ω(log k)\" approximation lower bound, indicating that a constant-factor approximation for general k is not possible with k-leaf threshold trees. 5) The proposed IMM algorithm's guarantees are relative to the cost of the initial reference clustering, meaning its overall performance is bounded by the quality of the external algorithm used to generate these initial centers.",
        "future_research_directions": "The paper suggests several promising avenues for future research: 1) Improving the approximation bounds for k clusters, potentially by tightening either the upper or lower bounds presented. 2) Investigating the use of larger threshold trees with more than k leaves to see if better approximation factors can be achieved. 3) Identifying specific properties of data sets (e.g., well-separated data) that might enable explainable and highly accurate clusters, and potentially lead to improved upper bounds for such cases. 4) Exploring whether a \"Θ(log k)\" approximation is achievable for a broader range of well-separated clusters (e.g., Gaussian mixtures) beyond the specific random center construction used for the lower bound. 5) Developing explainable clustering methods for other objectives beyond k-medians/means, such as hierarchical clustering. 6) Integrating tree-based clustering with additional constraints or objectives, like fair clustering, online clustering, or clustering with same-cluster queries."
      }
    }
  ],
  "new_method": {
    "method": "Open Problems: 1. Existing few-bit backward schemes for point-clouds ignore both fine-grained spatial variation *and* strong temporal correlation that appears in sequential scans (SLAM, autonomous driving); thus they repeatedly learn almost identical code-books every frame.\n2. Histogram-based bin search still accounts for ≥15 % of wall-time when we push to >1 M points or run on edge GPUs; this overhead grows with resolution.\n3. Re-computing edge-conv features on the backward pass doubles the number of KNN searches, wasting compute although neighbour sets change little between frames.\n4. Current methods allocate a fixed bit-width for all regions although activation entropy varies drastically; low-entropy areas could be stored cheaper without harming gradients.\n5. None of the prior works compress the large attention K/V tensors that dominate memory in modern state-space / transformer point models.\nMethods: We propose STAG-FBB (Spatio-Temporal Adaptive Gradient-aware Few-Bit Backward) consisting of four new components that plug on top of HA-FBB-PC.\nA. Streaming spatio-temporal quantile sketch\n   • Replace per-batch histograms with a GPU-friendly GK-Quantile sketch maintained per centroid over a sliding window of the last T=4 frames. 512-entry buffers approximate any quantile with <0.5 % error and update in O(log m) time.\n   • Bin boundaries for the current frame are read directly from sketch quantiles → <0.05 ms/layer regardless of point count.\nB. Tiny predictor for zero-histogram layers\n   • For small residual MLP layers we skip sketches entirely and predict the 2-bit boundaries via a 3-layer MLP that consumes (mean, var, #points) of the input patch + previous-step gradient amplitude. Parameters are shared across all layers and trained once on a held-out set (<20 k steps).\nC. Compute-reuse aware edge tensor caching\n   • We delta-encode KNN indices between successive frames with run-length & var-length codes; identical neighbourhoods are detected by a warp-level SHA-1 hash. If unchanged we reuse the saved edge-features from the previous frame instead of recomputation, cutting ∼28 % backward FLOPs in SLAM sequences.\nD. Entropy-adaptive mixed bit-width\n   • Within each centroid we measure Shannon entropy H of the 4-bit code stream online. Blocks with H<1.2 are down-shifted to 2 bits, H>2.5 kept at 4 bits, else 3 bits. A 16-byte header records the per-block bit-width. This yields extra 8-12 % memory drop.\nE. Attention-tensor support\n   • Extend the same quantile sketch + adaptive bits to PointMamba/Transformer K,V tensors (but not Q, which is re-computed). Combined with activation compression this removes the last ≥35 % memory hotspot in state-space models.\nAll kernels are <1100 LOC CUDA + Triton and exposed via PyTorch autograd.Functions.\nExperimental Setup: Architectures: PointMamba-L, PointNext-XL-Seg, DGCNN, BSC-UNet-H.\nDatasets: Sequential – SemanticKITTI (seq. 00-10), nuScenes lidar train/val; Static – ModelNet40, ScanObjectNN.\nProtocols: For sequential sets we feed 5 contiguous frames and slide window; for static we use single-frame (T=1) to confirm no regression.\nBaselines: FP16, torch.checkpoint, ActNN-Q, IFB-PC, HA-FBB-PC (our immediate predecessor).\nMetrics: peak GPU memory, wall-clock time/epoch, OA/mIoU, additional energy (nVidia SM counters), and extra FLOPs.\nAblations: (i) skip sketch (use hist) (ii) no temporal reuse (iii) fixed 3-bit (iv) no K/V compression.\nExpected Result: • STAG-FBB cuts *total* training memory by 63-68 % on sequential sets and 57-60 % on static ones vs FP16; gains +10–12 % over HA-FBB-PC.\n• Throughput improves 1.25× over HA-FBB-PC owing to sketch & reuse; overall speed within ±4 % of vanilla training (no checkpointing).\n• Accuracy drop ≤0.15 % OA / ≤0.25  mIoU compared to FP16.\n• Mixed bit-width contributes 7–9 % memory and no accuracy loss; predictor adds <0.1 % OA.\n• Attention K/V compression removes 32-38 % of remaining footprint in PointMamba, enabling 2× larger scenes on a single 24 GB RTX-3090.\nExpected Conclusion: By unifying spatial, temporal and information-theoretic adaptivity, STAG-FBB moves few-bit backward from per-batch optimisation to a streaming regime that supports long point-cloud sequences with practically zero overhead. The method not only outperforms existing activation-only compressors but also halves compute wastage by reusing edge tensors and compresses attention maps, opening the door to real-time training of autonomous-driving perception models on single prosumer GPUs. Future work will explore sketch-based quantisation for gradient *aggregation* across distributed learners, further shrinking inter-GPU communication."
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "GPU memory is still the dominant bottleneck when training modern point-cloud networks (e.g. PointMamba, DGCNN, sparse-conv UNets). Even when the models are small, the large intermediate tensors that must be kept for the backward pass – especially the activations of MLPs and point‐patch features – quickly exceed the memory of a single 12-24 GB card. Re-training or fine-tuning on real-world scans therefore requires gradient checkpointing or distributed training, both of which slow experimentation.  \n\nRecent work (Few-Bit Backward) showed that, for image models, the saved activations can be replaced by a few-bit index together with a piece-wise-constant approximation of the derivative of the non-linearity, yielding ~40 % memory reduction.  However, point-cloud networks differ substantially:\n1. their activations are much less Gaussian and often centered around zero because of Entropy-Maximising Aggregation (EMA) or batch-norm removal in binarised nets;\n2. they frequently use custom non-linear blocks (e.g. SoftPool, MaxPool, S6-SSM gates) for which closed-form derivatives are non-piece-wise;\n3. activation statistics vary strongly across local point groups.  \n\nNo method currently adapts few-bit gradient storage to these characteristics, leaving memory efficiency of point-cloud training an open problem.",
        "methods": "We propose Importance-Aware Few-Bit Backward for Point Clouds (IFB-PC).  The idea is to quantise the stored activation **indices** instead of the full tensors, but **adapt the binning to each layer and to each mini-batch** using a lightweight importance estimator.  \n\n1. Activation histogram capture.  During the first forward pass of each mini-batch we collect a 64-bin histogram of the pre-activation values for every non-linear layer (ReLU, GELU, SiLU, SoftPool output, S6 gate, etc.).  A CUDA kernel keeps this cost <1 ms.\n2. Dynamic K-Means binning.  From the histogram we run one iteration of 1-D K-Means (K = 2^b, b ∈ {1,2,3,4}) to reposition the bin boundaries so that they minimise the **weighted L2 error of the derivative** under the current distribution.  This costs O(K·bins) per layer per batch.\n3. Piece-wise-constant derivative table.  For each layer we pre-compute the derivative value (mean of true derivative inside the bin) and store it in a (K,1) table (FP16).\n4. Forward pass storage.  Instead of caching the FP16 activation, we store only the b-bit bin index per element (packed into uint8/uint32 tensors).  \n5. Backward reconstruction.  During back-prop we look up the derivative from the table and multiply it with the incoming gradient.  \n6. Importance mask.  A small subset (top α % per layer) of **high-magnitude activations** is flagged via a bitmask and cached in FP16; this prevents quantisation noise from propagating through high-energy channels (idea borrowed from sensitivity masks in compressed 3D Gaussian splatting).  \n7. Compatibility hooks.  We supply PyTorch autograd Functions that wrap ReLU / GELU / SiLU as drop-in replacements; no model-side change is required.  \n\nCompared with Few-Bit Backward, novelties are:\n• dynamic per-batch bin update (to handle heavy distribution shift across point groups);\n• support for arbitrary element-wise functions via sampled derivative lookup;\n• selective full-precision caching via importance mask.\n",
        "experimental_setup": "Models:  (a) PointMamba (official 12-layer, 384-d); (b) DGCNN; (c) sparse UNet from BSC-Net (ScanNet 2 cm).  \nDatasets: ModelNet40 (classification), ScanObjectNN (real object classification), ShapeNetPart (part segmentation), ScanNet val split (semantic seg.).  \n\nBaselines:\n1. Vanilla training (full FP16 activations).\n2. Activation checkpointing (torch.utils.checkpoint).\n3. ActNN-Q training (8-bit activation quantisation) – if memory fits.\n4. Original Few-Bit Backward (fixed bins).\n\nMetrics:\n• Peak GPU memory (PyTorch profiler).\n• Throughput (samples / s).\n• Task accuracy: OA for classification, mIoU for segmentation.\n• Additional wall-clock time per epoch.\n\nProtocol:\n• Train each model for the standard number of epochs on one RTX 3090 (24 GB).\n• For IFB-PC run with b = {2,3,4}.  \n• Compare memory/accuracy trade-off to baselines.  \n• Ablate (i) importance mask on/off, (ii) dynamic bins vs. fixed learnt offline, (iii) per-batch vs. per-epoch update.  \n• Robustness: repeat experiments with batch size doubled until OOM to measure maximum achievable batch size.  \n",
        "expected_result": "We expect IFB-PC (3-bit) to cut activation memory by ≈ 45 % on average, enabling ×1.8 larger batch size versus vanilla and ≈ 30 % vs. ActNN-Q, while keeping accuracy within −0.3 % OA / −0.5 mIoU of baseline.  2-bit variant should still yield −1 % accuracy with 60 % memory saving.  Dynamic binning is predicted to outperform fixed Few-Bit (same bits) by 0.5-1 % accuracy at equal memory.  Importance mask should provide a further 0.2-0.4 % gain for negligible memory cost (<5 %).  Training throughput penalty expected <8 % because histogram+K-Means are lightweight.",
        "expected_conclusion": "The study will demonstrate that point-cloud specific activation statistics can be exploited to push few-bit backward techniques beyond image models, delivering substantial memory savings without compromising accuracy or speed.  This allows single-GPU training/fine-tuning of recent point-cloud architectures on larger batch sizes or higher resolutions.  Future work can extend the idea to attention/key-value tensors and to on-device inference memory reduction, paving the way for full mixed-precision or binarised pipelines in 3-D perception."
      },
      "evaluate": {
        "novelty_reason": "None of the related works applies few-bit storage of backward activations to 3-D point-cloud networks. Existing approaches either (1) quantise forward activations and/or weights (ActNN, BiPointNet, BSC-Net, LiDAR-PTQ), (2) compress point data or representations for inference (MuSCLE, Compressed Gaussian Splatting, Neural-NeRF compression, OctreeOcc, OcTr), or (3) propose Few-Bit Backward only for image CNN/Transformer workloads under the assumption of near-Gaussian activations and simple ReLU-family derivatives. IFB-PC introduces three new ingredients that are absent in all prior art: (i) dynamic, per-batch 1-D k-means bin adaptation to cope with highly non-stationary, zero-centred point features generated by EMA or binarised layers; (ii) a derivative lookup table that supports arbitrary non-linear or pooling operations (SoftPool, MaxPool, S6 gates) whose gradients are not piece-wise constant; (iii) an importance mask that selectively stores a tiny fraction of high-magnitude activations in FP16 to avoid error accumulation in high-energy channels. Together they allow storing only b-bit indices instead of full activations during training of modern point-cloud backbones such as PointMamba, DGCNN, and sparse-conv UNets—something no previous memory-reduction technique targets.",
        "novelty_score": 8,
        "significance_reason": "GPU memory is currently the main obstacle to training or fine-tuning state-of-the-art point-cloud architectures on real-world scans; practitioners resort to checkpointing or distributed data-parallelism, both of which cut throughput and raise costs. By cutting activation memory ∼45 % (60 % at 2-bit) while keeping accuracy within <1 % and incurring <8 % runtime overhead, IFB-PC would let many researchers and companies train high-capacity 3-D models on a single 24 GB card and/or use larger batch sizes and higher-resolution inputs. Academically, it opens a new line of work on gradient-side quantisation tailored to irregular 3-D data and complex non-linear blocks, complementing existing weight / forward-activation quantisation studies. Societally, more efficient 3-D perception training benefits robotics, autonomous driving, AR/VR and digital twins by lowering hardware barriers and energy consumption.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "1. Existing few-bit backward methods ignore the extreme spatial non-stationarity of point–cloud activations: statistics vary not only across layers and mini-batches but also across local point groups, producing long-tailed, multi-modal distributions that global histograms cannot capture.\n2. Memory is also consumed by edge-feature tensors (e.g. DGCNN concatenation of central-and-neighbor points) and by intermediate neighbour-index buffers; these are untouched by current activation-only schemes.\n3. Current binning strategies are derivative-agnostic – they minimise ||f'−q||_2 but not the true gradient error ||δL/δx−ĝ||_2, which depends on the incoming gradient.\n4. Overheads of per-batch K-Means grow with resolution and depth; we need sub-millisecond adaptation even for 1 M-point scenes.",
        "methods": "We extend IFB-PC to Hierarchically-Adaptive Few-Bit Backward for Point Clouds (HA-FBB-PC).\nA. Two-stage spatially-conditioned codebooks\n   A1. During the forward pass we down-sample the point cloud with Farthest-Point-Sampling (≈1 K centroids) and assign every point to its nearest centroid (KNN=32).  For each centroid c we accumulate a 32-bin histogram of the pre-activation values of the current non-linearity.\n   A2. At the end of the layer we merge histograms into a small set (C=16) of codebooks with a fast streaming variant of k-Means++ (O(#bins·C)) and cache the centroid-id of every point (4 bits).  Within each codebook we apply Few-Bit indexing (b∈{2,3,4}).  Hence highly different local regions get different bin boundaries without losing global compression.\nB. Gradient-aware bin update\n   We weight each histogram entry by the mean |δL/δy| observed for that bin in the previous iteration (one-step delay), directly minimising the expected squared gradient error.  This adds <0.3 ms per layer.\nC. Edge-feature regeneration\n   For edge-conv style blocks we store only the indices of the k nearest neighbours (already needed for the forward) and recompute edge features (x_i−x_j, …) on the backward pass instead of storing the FP16 tensor (½–1 GB saving for ScanNet-scale batches).\nD. Micro-entropy coding\n   The stream of b-bit indices is run-length / Huffman encoded per layer before being flushed to global memory (CUDA kernel, ~2 ns/elem).  Compression ratios 1.2-1.6× over raw bits further cut footprint.\nE. Plug-in PyTorch ops\n   We supply autograd.Functions (AdaptiveReLU, AdaptiveSiLU, AdaptiveSoftPool) that internally handle (A–D).  No model source change is required.\nF. Optional QAT fine-tune\n   When fine-tuning large pre-trained backbones we allow the codebook means to become learnable parameters updated by SGD for the first 5 epochs, giving +0.1–0.3 % OA recovery.",
        "experimental_setup": "Architectures: PointMamba-Large (32 layers, 512-d), DGCNN, BSC-UNet-H, and PointNext-XL.\nDatasets: ModelNet40, ScanObjectNN, ShapeNetPart, ScanNet v2 (2 cm), and SemanticKITTI (voxel-ised 0.05 m).\nBaselines: FP16, torch.checkpoint, ActNN-Q (8-bit), original Few-Bit Backward, and IFB-PC.\nMetrics: peak GPU memory, wall-clock throughput, OA / mIoU / mAcc, extra energy (nVidia SM counters).\nProtocols: single RTX-3090-24 GB; batch size increased until OOM for each method; 3 runs per setting.  Ablations:\n  – spatial codebooks on/off,\n  – gradient weighting on/off,\n  – entropy coding on/off,\n  – edge-feature regeneration on/off.\nAll code in pure Python+CUDA kernels (<800 loc) released under MIT.",
        "expected_result": "• HA-FBB-PC (3-bit) reduces activation+edge memory by 55–62 % and enables ×2.3 larger batch size vs. FP16 and ×1.4 vs. IFB-PC, with ≤-0.2 % OA / ≤-0.4 mIoU drop.\n• Gradient weighting gives a consistent +0.5 % OA over value-only binning at the same bits.\n• Spatial codebooks add just 6 % run-time overhead but recover ≈0.8 % OA compared with global bins.\n• Edge-feature regeneration cuts another 8-12 % memory with <3 % extra compute.\n• Entropy coding lowers saved-index footprint by 20-25 % on average scenes.",
        "expected_conclusion": "By conditioning few-bit activation quantisation on local spatial statistics and on actual gradient magnitudes, and by regenerating structural edge tensors on demand, HA-FBB-PC pushes single-GPU training of modern point-cloud networks into regimes previously reachable only with multi-GPU setups.  The technique is orthogonal to weight/forward quantisation and could pair with binary backbones such as BiPointNet or BSC-Net for end-to-end ultra-low-memory 3-D learning.  Future work includes extending the hierarchical codebooks to attention K/V tensors and integrating learned entropy models for on-device deployment."
      },
      "evaluate": {
        "novelty_reason": "Among the listed papers, only \"Few-Bit Backward\" (for images/NLP) and its point-cloud variant IFB-PC tackle backward-pass activation quantisation.  They (i) use a single global histogram per layer, (ii) ignore spatial non-stationarity inside a point cloud, (iii) optimise ||f'−q||_2 instead of the real gradient error, and (iv) leave neighbour-edge tensors untouched.  HA-FBB-PC departs from these assumptions by (1) introducing a hierarchical, centroid-conditioned code-book system updated on-the-fly from local 32-point groups, (2) weighting histogram bins with the *incoming* |δL/δy| so the code-book directly minimises expected gradient distortion, (3) discarding and regenerating edge-conv features using saved k-NN indices, and (4) applying micro-entropy coding to the b-bit index stream.  None of the related works on LiDAR/MuSCLE, PointMamba, BiPointNet, BSC-Net, Gaussian-splat compression or LiDAR-PTQ address training-time memory or gradient-aware activation quantisation, nor do they propose spatially adaptive few-bit schemes.  Hence the method contributes several orthogonal innovations absent from prior art.",
        "novelty_score": 8,
        "significance_reason": "Reducing GPU memory during *training* of large 3-D point-cloud models directly enables bigger batch sizes or the use of single-GPU hardware in robotics, AR/VR mapping and autonomous-driving research.  HA-FBB-PC reports 55-62 % total activation+edge savings and 2.3× batch-size increase at <0.2 % accuracy loss on 5 standard datasets and four modern backbones, which is a practically meaningful leap beyond IFB-PC (additional ×1.4).  Academic impact stems from (a) providing the first gradient-aware, locality-adaptive few-bit framework, (b) extending memory-compression research from 2-D vision/NLP to irregular 3-D data, and (c) supplying open-source PyTorch ops that other researchers can immediately adopt.  Societally, cheaper hardware requirements lower the barrier for smaller labs to work on 3-D perception.  Significance is moderated by being an optimisation rather than opening an entirely new research direction, hence high but not exceptional.",
        "significance_score": 7
      }
    },
    {
      "idea": {
        "open_problems": "1. Existing few-bit backward schemes for point-clouds ignore both fine-grained spatial variation *and* strong temporal correlation that appears in sequential scans (SLAM, autonomous driving); thus they repeatedly learn almost identical code-books every frame.\n2. Histogram-based bin search still accounts for ≥15 % of wall-time when we push to >1 M points or run on edge GPUs; this overhead grows with resolution.\n3. Re-computing edge-conv features on the backward pass doubles the number of KNN searches, wasting compute although neighbour sets change little between frames.\n4. Current methods allocate a fixed bit-width for all regions although activation entropy varies drastically; low-entropy areas could be stored cheaper without harming gradients.\n5. None of the prior works compress the large attention K/V tensors that dominate memory in modern state-space / transformer point models.",
        "methods": "We propose STAG-FBB (Spatio-Temporal Adaptive Gradient-aware Few-Bit Backward) consisting of four new components that plug on top of HA-FBB-PC.\nA. Streaming spatio-temporal quantile sketch\n   • Replace per-batch histograms with a GPU-friendly GK-Quantile sketch maintained per centroid over a sliding window of the last T=4 frames. 512-entry buffers approximate any quantile with <0.5 % error and update in O(log m) time.\n   • Bin boundaries for the current frame are read directly from sketch quantiles → <0.05 ms/layer regardless of point count.\nB. Tiny predictor for zero-histogram layers\n   • For small residual MLP layers we skip sketches entirely and predict the 2-bit boundaries via a 3-layer MLP that consumes (mean, var, #points) of the input patch + previous-step gradient amplitude. Parameters are shared across all layers and trained once on a held-out set (<20 k steps).\nC. Compute-reuse aware edge tensor caching\n   • We delta-encode KNN indices between successive frames with run-length & var-length codes; identical neighbourhoods are detected by a warp-level SHA-1 hash. If unchanged we reuse the saved edge-features from the previous frame instead of recomputation, cutting ∼28 % backward FLOPs in SLAM sequences.\nD. Entropy-adaptive mixed bit-width\n   • Within each centroid we measure Shannon entropy H of the 4-bit code stream online. Blocks with H<1.2 are down-shifted to 2 bits, H>2.5 kept at 4 bits, else 3 bits. A 16-byte header records the per-block bit-width. This yields extra 8-12 % memory drop.\nE. Attention-tensor support\n   • Extend the same quantile sketch + adaptive bits to PointMamba/Transformer K,V tensors (but not Q, which is re-computed). Combined with activation compression this removes the last ≥35 % memory hotspot in state-space models.\nAll kernels are <1100 LOC CUDA + Triton and exposed via PyTorch autograd.Functions.",
        "experimental_setup": "Architectures: PointMamba-L, PointNext-XL-Seg, DGCNN, BSC-UNet-H.\nDatasets: Sequential – SemanticKITTI (seq. 00-10), nuScenes lidar train/val; Static – ModelNet40, ScanObjectNN.\nProtocols: For sequential sets we feed 5 contiguous frames and slide window; for static we use single-frame (T=1) to confirm no regression.\nBaselines: FP16, torch.checkpoint, ActNN-Q, IFB-PC, HA-FBB-PC (our immediate predecessor).\nMetrics: peak GPU memory, wall-clock time/epoch, OA/mIoU, additional energy (nVidia SM counters), and extra FLOPs.\nAblations: (i) skip sketch (use hist) (ii) no temporal reuse (iii) fixed 3-bit (iv) no K/V compression.",
        "expected_result": "• STAG-FBB cuts *total* training memory by 63-68 % on sequential sets and 57-60 % on static ones vs FP16; gains +10–12 % over HA-FBB-PC.\n• Throughput improves 1.25× over HA-FBB-PC owing to sketch & reuse; overall speed within ±4 % of vanilla training (no checkpointing).\n• Accuracy drop ≤0.15 % OA / ≤0.25  mIoU compared to FP16.\n• Mixed bit-width contributes 7–9 % memory and no accuracy loss; predictor adds <0.1 % OA.\n• Attention K/V compression removes 32-38 % of remaining footprint in PointMamba, enabling 2× larger scenes on a single 24 GB RTX-3090.",
        "expected_conclusion": "By unifying spatial, temporal and information-theoretic adaptivity, STAG-FBB moves few-bit backward from per-batch optimisation to a streaming regime that supports long point-cloud sequences with practically zero overhead. The method not only outperforms existing activation-only compressors but also halves compute wastage by reusing edge tensors and compresses attention maps, opening the door to real-time training of autonomous-driving perception models on single prosumer GPUs. Future work will explore sketch-based quantisation for gradient *aggregation* across distributed learners, further shrinking inter-GPU communication."
      },
      "evaluate": {
        "novelty_reason": "Among prior works, Few-Bit Backward and its point-cloud variant HA-FBB-PC already quantize activation gradients layer-wise using per-batch histograms, but (1) they ignore temporal redundancy between consecutive scans, (2) use uniform bit-width across the cloud, and (3) leave the large K/V tensors of modern SSM/Transformer models untouched.  Related compression works such as MuSCLE, Compressed 3D Gaussian Splatting or LiDAR-PTQ focus on inference-time bit-streams or weight/activation quantization, not on back-prop memory during training.  STAG-FBB introduces four orthogonal ideas that do not appear in any of the cited papers:  a) a GPU-resident GK quantile sketch that is updated over a sliding window of frames and eliminates histogram scans while exploiting temporal statistics; b) an online entropy estimator that drives per-block mixed 2/3/4-bit coding; c) delta-encoded KNN index hashing to reuse edge-conv features across frames and roughly halve backward FLOPs; d) extension of few-bit compression to attention key/value activations, the dominant memory term in PointMamba/Transformer point models.  No prior method jointly addresses spatial, temporal and information-theoretic adaptivity for training-time memory in 3-D point clouds, nor provides a plug-in that supports both classical edge-convs and state-space/transformer architectures.  Therefore the method offers clear conceptual and technical novelty beyond existing literature.",
        "novelty_score": 9,
        "significance_reason": "Training large sequential point-cloud networks (e.g., SLAM and autonomous-driving perception) currently requires multi-GPU setups or aggressive checkpointing because activations, gradients, and attention maps easily exceed 24–48 GB.  By cutting total memory by ≈65 % and keeping accuracy within 0.2 %, STAG-FBB allows full-length scene training on a single consumer-grade GPU and provides 1.25× throughput, directly lowering hardware cost and energy consumption for both academia and industry.  The approach is generic (works with PointNext, DGCNN, UNet, PointMamba) and its CUDA/Triton kernels are publicly usable via PyTorch autograd, so adoption barrier is low.  Academically, it extends the few-bit‐gradient research line with the first temporally adaptive sketch and with entropy-conditioned mixed-precision coding, which can inspire follow-up work on streaming quantization or distributed gradient aggregation.  Societally, enabling cheaper real-time training of autonomous-driving or robotics perception stacks accelerates deployment and democratizes research.  Thus the contribution is of substantial practical and scholarly importance, though it is an evolutionary step built on earlier few-bit ideas rather than a paradigm shift.",
        "significance_score": 8
      }
    }
  ]
}