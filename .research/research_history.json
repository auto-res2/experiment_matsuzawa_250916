{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "fast GAT training",
    "efficient graph attention",
    "GAT acceleration",
    "sparse attention GAT",
    "quantized GAT"
  ],
  "research_study_list": [
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the significant performance degradation and trainability issues observed in deeper Graph Attention Networks (GATs) when using standard initialization. The main contributions include: 1) Deriving a novel conservation law of gradient flow dynamics for GATs, which explains why a high proportion of parameters struggle to change during training, especially in deeper networks. 2) Proposing a balanced initialization scheme that, based on this conservation law, enables more effective propagation of gradients. This scheme significantly improves the trainability of deeper GATs and achieves considerable speedup in training and convergence time compared to standard initialization methods.",
        "methodology": "The core methodology involves a theoretical derivation of a conservation law for GAT gradient flow dynamics. This derivation leverages the concept of 'rescale invariance' for GATs with positive homogeneous activation functions (e.g., ReLU, LeakyReLU), establishing a structural relationship between gradients and parameters that preserves the difference between squared l2-norms of incoming and outgoing weights for a neuron. To counteract the identified imbalance, a 'balancing procedure' (Procedure 2.6) is devised: it initializes attention parameters to zero and scales feature weights across layers to ensure balanced norms, satisfying the derived conservation law. Furthermore, the paper proposes a 'balanced LL-orthogonal initialization' where feature weights are initially set to an orthogonal matrix with a looks-linear structure before applying the balancing procedure, aimed at enhancing dynamical isometry and trainability for deeper models.",
        "experimental_setup": "Experiments were conducted on nine benchmark datasets for semi-supervised node classification: Cora, Citeseer, Pubmed, Actor, Chameleon, Squirrel, Cornell, Texas, and Wisconsin. GAT models with widths of 64 (and 512 for specific tests) and depths ranging from 2 to 80 layers were trained using Stochastic Gradient Descent (SGD) and Adam optimizers. The study compared four initialization schemes: standard Xavier (Xav), Xavier with zero attention (XavZ, an ablation), balanced Xavier (BalX), and balanced Looks-Linear Orthogonal (BalO). Learning rates were adjusted per depth and dataset, and models were trained for up to 5000 epochs. Performance was evaluated by mean test accuracy (%) with 95% confidence intervals over five runs, and training speedup (epochs to best model). Additional comparisons were made against Lipschitz Normalization, GCN, and ωGAT. Hardware used included Nvidia T4 and GeForce RTX 3060 GPUs.",
        "limitations": "The derived conservation law specifically applies to the self-attention mechanisms defined in the original GAT and GATv2 models, as well as architectural variations like ωGAT. It does not directly extend to other types of self-attention (e.g., dot-product self-attention) without modifications. The theoretical framework assumes positively homogeneous activation functions, which means the performance of the balanced orthogonal initialization (BalO) may be negatively impacted by non-homogeneous functions like ELU. Additionally, while the theory assumes infinitesimal learning rates and vanilla gradient descent, its practical effectiveness with finite learning rates and Adam is empirically observed, not formally proven for all cases. The research also notes that its aim is to improve trainability and understand learning dynamics, not necessarily to achieve state-of-the-art performance on all types of graph datasets, such as heterophilic graphs, when compared to specialized models.",
        "future_research_directions": "Future research directions include extending the study of learning dynamics to other positive homogeneous models that incorporate attention mechanisms, particularly Transformers and Vision Transformers, given their inherent need for depth. Another avenue is to explore methods for achieving or approximating dynamical isometry in general Graph Neural Networks. A specific intriguing direction is to derive modifications to the conservation law for different self-attention mechanisms, such as dot-product self-attention used in models like SuperGAT and other Transformer-based architectures prevalent in Large Language Models (LLMs) and adapted for graph learning."
      }
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of accelerating Graph Neural Network (GNN) training, particularly for Graph Convolution Networks (GCNs) and attentive GNNs like Graph Attention Networks (GATs), by optimizing sampling variance. Existing sampling methods are suboptimal for GCNs and not applicable to general GNNs due to the intractability of computing optimal sampling distributions, which depend on changing node embeddings and learned weights. The main contribution is recasting the problem of optimizing samplers as an adversary bandit problem, where rewards are related to constantly varying node embeddings and learned weights. The authors propose two bandit algorithms, GNN-BS (based on Multi-Armed Bandit, MAB) and GNN-BS.M (based on MAB with multiple plays), which maintain nonparametric estimates of the sampler and update it iteratively. They theoretically demonstrate that their algorithm asymptotically approaches the optimal variance within a factor of 3 and empirically show superior performance in convergence speed, accuracy (Micro F1 scores), and reduced sample variance on multiple datasets compared to state-of-the-art approaches.",
        "methodology": "The core methodology involves formulating the optimization of sampling variance in GNNs as an adversary bandit problem. Instead of directly computing the intractable optimal sampling distribution (which requires full knowledge of neighbors' hidden embeddings and/or learned weights), the proposed methods iteratively learn and update a sampler. The 'reward' for each action (sampling a subset of neighbors) is defined as the negative derivative of the effective variance. Two algorithms are introduced: (1) GNN-BS, which samples one neighbor at a time and repeats this k times, updating the sampler using the EXP3 algorithm. (2) GNN-BS.M, which samples a k-element subset of neighbors once using an efficient k-combination sampler (DepRound) and updates the sampler using the EXP3.M algorithm. For attentive GNNs, where attention values (αij) are learned and cannot be evaluated with only sampled neighborhoods, the algorithms define adjusted feedback attention values (α'ij) using unnormalized attentions. The theoretical regret analysis shows the variance of the estimators approaches the optimal variance within a factor of 3.",
        "experimental_setup": "The proposed Bandit Samplers for GNNs were evaluated on five benchmark datasets: Cora, Pubmed, PPI, Reddit, and Flickr, along with the OGB protein dataset. Node classification tasks were performed on GCN, GAT, and GeniePath architectures, with a fixed two-layer structure. Hidden embedding dimensions were set to 16 for smaller datasets (Cora, Pubmed) and 256 for larger ones (PPI, Reddit, Flickr), with a single multi-head for attentive GNNs. For fair comparison, normalization layers were not used. Comparisons were made against various state-of-the-art sampling methods, including layer sampling approaches (GraphSAGE, FastGCN, LADIES, AS-GCN, S-GCN) and graph sampling techniques (ClusterGCN, GraphSAINT), as well as their attentive GNN counterparts (AS-GAT, GraphSAINT-GAT). Hyperparameters such as learning rate, L2 regularization, and dropout rate were optimized via grid search. Sample sizes (k) for proposed algorithms and some baselines were set to 1 (Cora, Pubmed), 5 (Flickr), or 10 (PPI, Reddit). Performance was measured using Micro F1 scores, convergence speed (epochs and training time), and average sampling variances. All experiments were run multiple times (3 for benchmarks, 10 for OGB) to report mean and standard deviation.",
        "limitations": "The current derivation of bandit samplers primarily follows node-wise sampling approaches, with its extension to layer-wise sampling left for future work. The application of the Multi-Armed Bandit (MAB) setting in GNN-BS, where a single action is repeated k times, is acknowledged as not strictly rigorous compared to the MAB with multiple plays setting (GNN-BS.M), although empirical results show similar performance. The theoretical analysis for GNN-BS.M relies on an approximation of the effective variance. The algorithms require additional storage in O(|E|) to maintain nonparametric estimates of the samplers. The paper also notes that 'graph sampling' approaches, while potentially faster under specific conditions, are less flexible and general, particularly when only partial vertices have labels, limiting direct timing comparisons in certain scenarios.",
        "future_research_directions": "The paper explicitly states two future research directions: (1) extending the derivation of the bandit samplers from node-wise sampling to layer-wise sampling approaches, and (2) exploring other bandit settings beyond the current adversary bandit formulation. Implicitly, further research could involve improving the theoretical rigor for the single-play MAB (GNN-BS) setting and exploring more precise variance approximations for the MAB with multiple plays (GNN-BS.M) to potentially enhance its theoretical guarantees or practical performance."
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9×on Wiki-40B and 12.1× on PG-19 for auto-regressive language modeling, and 4.8×on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efﬁcient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 × 1.2× 1024 9.0 × 1.3× 2048 8.9 × 1.6× 4096 13.1 × 2.7× 8192 25.6 × 4.9× Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B — All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ﬁnd existing efﬁcient attention methods suffer from at least one of the following drawbacks: • Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efﬁcient attention methods often incur signiﬁcant quality drop compared to augmented Trans- formers, and this drop outweighs their efﬁciency beneﬁts. • Overhead in Practice . As efﬁcient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. • Inefﬁcient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x ∗ gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bms→bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 return tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u ∗ attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ﬁrst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efﬁcient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciﬁcally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efﬁcient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ﬁrst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efﬁcient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efﬁ- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512–8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2×– 4.9×for language modeling on Wiki-40B and a speedup of 1.0×–4.8×for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1×and achieves signiﬁcant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X ∈RT×d be the representations over T tokens. The output for Transformer’s MLP can be formu- lated as O= φ(XWu)Wo where Wu ∈Rd×e, Wo ∈Re×d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and φis an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = φu(XWu), V = φv(XWv) ∈RT×e (1) O= (U ⊙V)Wo ∈RT×d (2) where ⊙stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniﬁed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efﬁciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U ⊙ˆV)Wo where ˆV = AV (3) where A∈RT×T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation ˆvi = ∑ j aijvj “re- trieved” from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ﬁndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = φz(XWz) ∈RT×s (4) A= relu2 ( Q(Z)K(Z)⊤+ b ) ∈RT×T (5) Modiﬁcations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 − →softmax 17.04 / 4.31 105 single-head − →multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiﬁcations on GAU. where Z is a shared representation ( s ≪d)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ﬁnd the softmax in MHSA can be simpliﬁed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciﬁed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiﬁcations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax − →relu2 17.15 / 4.77 110 multi-head − →single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiﬁcations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformer’s MHSA which comes with4d2 param- eters, GAU’s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: • First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.• In addition, the number of attention modules is naturally doubled with GAU — recall MLP+MHSA ≈2×GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneﬁts (speed and RAM efﬁciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as ˆVlin = Q ( K⊤V )    Rd×d approx −−−→ˆVquad = Softmax ( QK⊤)    RT×T V where Q,K,V ∈RT×d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deﬁne Mt = K⊤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mt−1 + KtV⊤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiﬁcantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtV⊤ t into Mt−1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefﬁciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = K⊤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value K⊤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ﬁrst com- puting {KtV⊤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ﬁnd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneﬁts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ﬁrst chunked into G non-overlapping chunks of size C, i.e. [T] →[T/C × C]. Then, Ug ∈RC×e, Vg ∈RC×e and Zg ∈RC×s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAU’s attention can be efﬁciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4×sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: ˆVquad g = relu2 ( Qquad g Kquad g ⊤ + b ) Vg. The complexity of this part is O(G×C2 ×d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: ˆVlin g = Qlin g ( G∑ h=1 Klin h ⊤ Vh ) , (7) Causal: ˆVlin g = Qlin g (g−1∑ h=1 Klin h ⊤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiﬁcant training speedup. Finally, ˆVquad g and ˆVlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug ⊙ ( ˆVquad g + ˆVlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgse→bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bse→bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgms→bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefﬁciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ﬁxed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneﬁt trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciﬁc, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset — All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efﬁcacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ﬁnd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiﬁcant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variants—Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneﬁt trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proﬁler. See Appendix B for detailed settings and model speciﬁcations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset — All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 × as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++’s ﬁnal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1×–2.5×and 1.0×–4.8×, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset — Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 × 43.14 433 1.00 × 42.80 698 1.00 × 43.27 1292 1.00 × Transformer++ 44.47 292 – 43.18 441 – 43.13 712 – 43.26 1272 1.21 × Combiner 46.04 386 – 44.68 376 – 43.99 374 – 44.12 407 – FLASH-Quad 43.40 231 2.18 × 42.01 273 3.29× 41.46 371 3.59× 41.68 560 5.23× FLASH 44.06 234 1.66× 42.17 237 3.85 × 40.72 234 6.75 × 41.07 250 12.12 × * Measured based on time taken to match Transformer+’s ﬁnal quality (at step 125K) on TPU. – Indicates that the speciﬁc model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ﬁndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciﬁcally, FLASH-Quad reduces the training time of Transformer++ by 1.2×to 2.5×and FLASH cuts the com- pute cost by 1.2×to 4.9×while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ﬁnal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23× and 12.12×of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiﬁcantly faster, demonstrating the effectiveness of our efﬁcient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ﬁne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ﬁne-tuning, we sweep over three different learn- ing rates, including 1e−4, 7e−5, and 5e−5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 — “PT“ stands for pre-training and “FT“ stands for ﬁne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, “ﬁrst-to-all” means that we also let the ﬁrst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 ×/ 1.00× Combiner 3.51 67.2 2.78×/ 2.75× FLASH-Quads=128 3.24 72.7 1.89 ×/ 1.79× FLASH-Quads=512 3.12 74.8 1.76×/ 1.67× FLASHs=512 3.23 73.3 2.61 ×/ 2.60× FLASHs=512 + ﬁrst-to-all 3.24 73.9 2.78×/ 2.69× We observe that the ﬁne-tuning results of the FLASH fam- ily can beneﬁt from several minor changes in the model conﬁguration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiﬁcant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciﬁcally, including using a small chunk size (128), disabling gradient clipping during ﬁnetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ﬁrst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8×and 2.7×as fast as Transformer+ in pretraining and ﬁne-tuning, respectively. 4.4. Ablation Studies Signiﬁcance of quadratic & linear components. To bet- ter understand the efﬁcacy of FLASH, we ﬁrst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiﬁcant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other — both are critical to the quality of the proposed mixed chunk attention. Signiﬁcance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2×speedup when the sequence length is greater than 2048), conﬁrming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneﬁcial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (→FLASH) or to TFM++ (→MC-TFM++) — Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quad→FLASH 0.0 0.05 0.06 0.07 0.07 TFM++→MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quad→FLASH -0.05 0.06 0.22 0.30 0.11 TFM++→MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efﬁcient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efﬁcient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 933–941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efﬁcient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 1243–1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651–4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efﬁcient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243–5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiﬁcations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efﬁcient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum ∈RT/C ×d and concatenate them into the local quadratic attention, i.e. ˆVg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h ⊤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C ×T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsum⊤+ bsum ) , ˆVlin g = Qlin g [T/C∑ h=1 alin gh ( Klin h ⊤ Vh )] . We indeed brieﬂy experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conﬁguration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciﬁcations Detailed speciﬁcations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conﬁgurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conﬁgurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conﬁgurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size – 512 – – 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU — Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length ×Batch size Model 512 ×4 1024 ×2 2048 ×1 4096 ×1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefﬁciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefﬁcient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,d→sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size ∗∗ 0.5)) scaledsin ∗= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len ∗= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,d→...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 ∗ cos = x2 ∗ sin, x2 ∗ cos + x1 ∗ sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 ∗ n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 ∗ n = 2]) r = (2 ∗ n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nk→mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) ∗ tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma ∗ x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x ∗ tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale ∗ x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmd→bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel ∗= causal_mask x = u ∗ tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask ∗= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel ∗ causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel, v) x = u ∗ (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces FLASH, a new Transformer model family designed to address the quadratic complexity limitation of conventional Transformers with long sequences, while maintaining high quality. The key contributions include the Gated Attention Unit (GAU) layer, which allows for the use of a simpler, weaker single-head attention without significant quality loss, and a novel \"mixed chunk attention\" approximation strategy. This strategy combines local quadratic attention within chunks and global linear attention across chunks, making it accelerator-friendly and dramatically improving auto-regressive training efficiency. FLASH demonstrates competitive perplexity with fully-augmented Transformers across context lengths from 512 to 8K, achieving significant training speedups (up to 4.9x on Wiki-40B, 12.1x on PG-19 for language modeling, and 4.8x on C4 for masked language modeling) on modern accelerators.",
        "methodology": "The methodology is built upon two core components: 1) The **Gated Attention Unit (GAU)**, which is a new layer design that integrates attention and the Gated Linear Unit (GLU) by sharing computation and introducing an attentive gating mechanism. GAU utilizes a simpler, single-head, softmax-free attention (employing squared ReLU activation) that relies less on precise attention computation. 2) The **mixed chunk attention** mechanism, which provides linear complexity. It segments the input sequence into non-overlapping chunks. Within each chunk, a precise quadratic attention is applied (local attention). For long-range interactions, a global linear attention is applied across these chunks. This chunk-level summarization is crucial for auto-regressive tasks, reducing sequential dependencies from `T` steps to `G=T/C` steps (where `C` is chunk size), thereby accelerating training while maintaining constant `O(Cd^2)` decoding memory and computation.",
        "experimental_setup": "The models were evaluated on both bidirectional (Masked Language Modeling, MLM) and auto-regressive (Language Modeling, LM) tasks. Datasets included C4 for MLM, Wiki-40B and PG-19 (noted for long documents) for LM, and TriviaQA for fine-tuning on a downstream question-answering task. Models ranged in scale from 110M to approximately 500M parameters. Experiments covered context lengths from 512 to 8192 tokens. Baselines included the vanilla Transformer, augmented Transformers (Transformer+ with RoPE, Transformer++ with RoPE+GLU), and other linear-complexity variants like Performer and Combiner. Performance was measured using perplexity (negative log perplexity) and F1 score for TriviaQA. Training speed and cost were benchmarked on 64 TPU-v4 cores, with additional auto-regressive training latency tests on a single Nvidia Tesla V100 GPU. All models were trained for 125K steps with 2^18 tokens per batch, using identical tokenizer and hyperparameters.",
        "limitations": "Existing efficient Transformer variants suffer from inferior quality compared to augmented Transformers, practical overheads due to extensive memory re-formatting operations incompatible with modern accelerators, and inefficient auto-regressive training caused by RNN-style sequential state updates. While overlapping local attention can improve quality, it introduces memory re-formatting operations that degrade actual running speed, making it less cost-effective than simply adding more layers. The optimal partial attention variant is observed to be task-specific. Additionally, an alternative approach to integrate long-range information, similar to Combiner, could increase auto-regressive decoding complexity to `O((C+T/C)d^2)`, making it length-dependent rather than constant. The paper also notes that larger chunk sizes are generally preferable for longer contexts, implying that optimizing this hyperparameter requires further search.",
        "future_research_directions": "Future research directions include investigating the scaling laws of the new FLASH model family and thoroughly evaluating its performance on a broader range of downstream tasks to understand its general applicability and efficiency benefits beyond the initial language modeling and question-answering experiments."
      }
    },
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9×on Wiki-40B and 12.1× on PG-19 for auto-regressive language modeling, and 4.8×on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efﬁcient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 × 1.2× 1024 9.0 × 1.3× 2048 8.9 × 1.6× 4096 13.1 × 2.7× 8192 25.6 × 4.9× Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B — All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ﬁnd existing efﬁcient attention methods suffer from at least one of the following drawbacks: • Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efﬁcient attention methods often incur signiﬁcant quality drop compared to augmented Trans- formers, and this drop outweighs their efﬁciency beneﬁts. • Overhead in Practice . As efﬁcient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. • Inefﬁcient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x ∗ gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bms→bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 return tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u ∗ attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ﬁrst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efﬁcient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciﬁcally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efﬁcient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ﬁrst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efﬁcient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efﬁ- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512–8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2×– 4.9×for language modeling on Wiki-40B and a speedup of 1.0×–4.8×for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1×and achieves signiﬁcant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X ∈RT×d be the representations over T tokens. The output for Transformer’s MLP can be formu- lated as O= φ(XWu)Wo where Wu ∈Rd×e, Wo ∈Re×d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and φis an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = φu(XWu), V = φv(XWv) ∈RT×e (1) O= (U ⊙V)Wo ∈RT×d (2) where ⊙stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniﬁed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efﬁciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U ⊙ˆV)Wo where ˆV = AV (3) where A∈RT×T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation ˆvi = ∑ j aijvj “re- trieved” from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ﬁndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = φz(XWz) ∈RT×s (4) A= relu2 ( Q(Z)K(Z)⊤+ b ) ∈RT×T (5) Modiﬁcations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 − →softmax 17.04 / 4.31 105 single-head − →multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiﬁcations on GAU. where Z is a shared representation ( s ≪d)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ﬁnd the softmax in MHSA can be simpliﬁed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciﬁed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiﬁcations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax − →relu2 17.15 / 4.77 110 multi-head − →single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiﬁcations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformer’s MHSA which comes with4d2 param- eters, GAU’s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: • First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.• In addition, the number of attention modules is naturally doubled with GAU — recall MLP+MHSA ≈2×GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneﬁts (speed and RAM efﬁciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as ˆVlin = Q ( K⊤V )    Rd×d approx −−−→ˆVquad = Softmax ( QK⊤)    RT×T V where Q,K,V ∈RT×d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deﬁne Mt = K⊤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mt−1 + KtV⊤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiﬁcantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtV⊤ t into Mt−1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefﬁciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = K⊤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value K⊤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ﬁrst com- puting {KtV⊤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ﬁnd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneﬁts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ﬁrst chunked into G non-overlapping chunks of size C, i.e. [T] →[T/C × C]. Then, Ug ∈RC×e, Vg ∈RC×e and Zg ∈RC×s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAU’s attention can be efﬁciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4×sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: ˆVquad g = relu2 ( Qquad g Kquad g ⊤ + b ) Vg. The complexity of this part is O(G×C2 ×d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: ˆVlin g = Qlin g ( G∑ h=1 Klin h ⊤ Vh ) , (7) Causal: ˆVlin g = Qlin g (g−1∑ h=1 Klin h ⊤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiﬁcant training speedup. Finally, ˆVquad g and ˆVlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug ⊙ ( ˆVquad g + ˆVlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgse→bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bse→bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgms→bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefﬁciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ﬁxed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneﬁt trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciﬁc, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset — All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efﬁcacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ﬁnd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiﬁcant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variants—Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneﬁt trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proﬁler. See Appendix B for detailed settings and model speciﬁcations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset — All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 × as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++’s ﬁnal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1×–2.5×and 1.0×–4.8×, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset — Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 × 43.14 433 1.00 × 42.80 698 1.00 × 43.27 1292 1.00 × Transformer++ 44.47 292 – 43.18 441 – 43.13 712 – 43.26 1272 1.21 × Combiner 46.04 386 – 44.68 376 – 43.99 374 – 44.12 407 – FLASH-Quad 43.40 231 2.18 × 42.01 273 3.29× 41.46 371 3.59× 41.68 560 5.23× FLASH 44.06 234 1.66× 42.17 237 3.85 × 40.72 234 6.75 × 41.07 250 12.12 × * Measured based on time taken to match Transformer+’s ﬁnal quality (at step 125K) on TPU. – Indicates that the speciﬁc model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ﬁndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciﬁcally, FLASH-Quad reduces the training time of Transformer++ by 1.2×to 2.5×and FLASH cuts the com- pute cost by 1.2×to 4.9×while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ﬁnal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23× and 12.12×of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiﬁcantly faster, demonstrating the effectiveness of our efﬁcient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ﬁne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ﬁne-tuning, we sweep over three different learn- ing rates, including 1e−4, 7e−5, and 5e−5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 — “PT“ stands for pre-training and “FT“ stands for ﬁne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, “ﬁrst-to-all” means that we also let the ﬁrst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 ×/ 1.00× Combiner 3.51 67.2 2.78×/ 2.75× FLASH-Quads=128 3.24 72.7 1.89 ×/ 1.79× FLASH-Quads=512 3.12 74.8 1.76×/ 1.67× FLASHs=512 3.23 73.3 2.61 ×/ 2.60× FLASHs=512 + ﬁrst-to-all 3.24 73.9 2.78×/ 2.69× We observe that the ﬁne-tuning results of the FLASH fam- ily can beneﬁt from several minor changes in the model conﬁguration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiﬁcant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciﬁcally, including using a small chunk size (128), disabling gradient clipping during ﬁnetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ﬁrst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8×and 2.7×as fast as Transformer+ in pretraining and ﬁne-tuning, respectively. 4.4. Ablation Studies Signiﬁcance of quadratic & linear components. To bet- ter understand the efﬁcacy of FLASH, we ﬁrst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiﬁcant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other — both are critical to the quality of the proposed mixed chunk attention. Signiﬁcance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2×speedup when the sequence length is greater than 2048), conﬁrming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneﬁcial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (→FLASH) or to TFM++ (→MC-TFM++) — Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quad→FLASH 0.0 0.05 0.06 0.07 0.07 TFM++→MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quad→FLASH -0.05 0.06 0.22 0.30 0.11 TFM++→MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efﬁcient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efﬁcient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 933–941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efﬁcient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 1243–1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651–4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efﬁcient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243–5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiﬁcations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efﬁcient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum ∈RT/C ×d and concatenate them into the local quadratic attention, i.e. ˆVg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h ⊤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C ×T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsum⊤+ bsum ) , ˆVlin g = Qlin g [T/C∑ h=1 alin gh ( Klin h ⊤ Vh )] . We indeed brieﬂy experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conﬁguration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciﬁcations Detailed speciﬁcations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conﬁgurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conﬁgurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conﬁgurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size – 512 – – 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU — Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length ×Batch size Model 512 ×4 1024 ×2 2048 ×1 4096 ×1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefﬁciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefﬁcient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,d→sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size ∗∗ 0.5)) scaledsin ∗= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len ∗= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,d→...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 ∗ cos = x2 ∗ sin, x2 ∗ cos + x1 ∗ sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 ∗ n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 ∗ n = 2]) r = (2 ∗ n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nk→mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) ∗ tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma ∗ x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x ∗ tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale ∗ x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmd→bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel ∗= causal_mask x = u ∗ tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask ∗= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel ∗ causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel, v) x = u ∗ (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Transformers are limited by quadratic complexity for long sequences, leading to inferior quality, practical overhead, and inefficient auto-regressive training in existing efficient variants. This paper proposes FLASH, a new Transformer model that achieves state-of-the-art quality with linear scalability over context length. Key contributions include the Gated Attention Unit (GAU), a novel layer design that enables the use of weaker single-head, softmax-free attention with minimal quality loss, and mixed chunk attention, an accelerator-friendly linear approximation method. FLASH matches the perplexity of improved Transformers across short (512) and long (8K) context lengths and achieves significant training speedups: up to 4.9x on Wiki-40B and 12.1x on PG-19 for auto-regressive language modeling, and 4.8x on C4 for masked language modeling, compared to augmented Transformers.",
        "methodology": "The methodology involves a two-step approach: 1. **Gated Attention Unit (GAU):** A new layer design incorporating a gating mechanism that alleviates the burden on self-attention, allowing for a simpler, single-head, softmax-free attention mechanism. GAU generalizes the Gated Linear Unit (GLU) by allowing an attention-based retrieval of representations (\\hat{V} = AV) to gate the input. The attention mechanism uses a squared ReLU activation and relative position bias (RoPE for longer sequences). 2. **Mixed Chunk Attention:** This method approximates the quadratic attention in GAU to achieve linear complexity. It partitions the input sequence into non-overlapping chunks of a fixed size (e.g., 256 or 512). Within each chunk, precise quadratic attention is applied (local attention). Across chunks, a global linear attention mechanism is employed to capture long-range dependencies. This mixed approach significantly reduces the sequential dependency during auto-regressive training from T steps to T/C steps (where C is chunk size), making training dramatically faster and accelerator-friendly. The model uses SiLU/Swish activation and ScaleNorm or LayerNorm.",
        "experimental_setup": "Experiments were conducted on bidirectional language modeling (Masked Language Modeling, MLM) on the C4 dataset, and auto-regressive language modeling (LM) on Wiki-40B and PG-19. Fine-tuning was performed on the TriviaQA dataset. Models were typically around 110M parameters (BERT-Base scale), with larger models (~500M) used for PG-19. All models were trained for 125K steps with 2^18 tokens per batch, exploring context lengths from 512 to 8192. Baselines included vanilla Transformer, Transformer+ (with RoPE), Transformer++ (with RoPE and GLU), Performer (linear attention), and Combiner (chunked attention). Evaluation metrics included perplexity (negative log perplexity) on validation sets and training speed (steps per second, latency per step, total TPU-core-days). Experiments were run on 64 TPU-v4 cores and Nvidia V100 GPUs.",
        "limitations": "The use of non-overlapping local attention, while efficient, may not achieve the same quality as overlapping local attention, which could be explored but introduces memory re-formatting overhead. The optimal chunk size is task-specific and requires hyperparameter search, which was not extensively explored. The paper also mentions that incorporating certain advanced chunk summary combinations (similar to Combiner) could increase auto-regressive decoding complexity to be length-dependent (O((C+T/C)d^2)), losing the constant-time benefit.",
        "future_research_directions": "Future work includes investigating the scaling laws of the new model family (FLASH) and evaluating its performance on a broader range of downstream tasks. Further exploration of optimal partial attention variants beyond non-overlapping local attention could also be a direction. Optimizing the chunk size through more extensive hyperparameter search is another potential area for improvement."
      }
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection",
      "abstract": "While the self-attention mechanism has been widely used in a wide variety of\ntasks, it has the unfortunate property of a quadratic cost with respect to the\ninput length, which makes it difficult to deal with long inputs. In this paper,\nwe present a method for accelerating and structuring self-attentions: Sparse\nAdaptive Connection (SAC). In SAC, we regard the input sequence as a graph and\nattention operations are performed between linked nodes. In contrast with\nprevious self-attention models with pre-defined structures (edges), the model\nlearns to construct attention edges to improve task-specific performances. In\nthis way, the model is able to select the most salient nodes and reduce the\nquadratic complexity regardless of the sequence length. Based on SAC, we show\nthat previous variants of self-attention models are its special cases. Through\nextensive experiments on neural machine translation, language modeling, graph\nrepresentation learning and image classification, we demonstrate SAC is\ncompetitive with state-of-the-art models while significantly reducing memory\ncost.",
      "full_text": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection Xiaoya Li*1, Yuxian Meng*1, Mingxin Zhou1, Qinghong Han1, Fei Wu2 and Jiwei Li 1 1 Shannon.AI 2 Computer Science Department, Zhejiang University {xiaoya_li,yuxian_meng,mingxin_zhou,qinghong_han,jiwei_li}@shannonai.com Abstract While the self-attention mechanism has been widely used in a wide variety of tasks, it has the unfortunate property of a quadratic cost with respect to the input length, which makes it difﬁcult to deal with long inputs. In this paper, we present a method for accelerating and structuring self-attentions: Sparse Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and attention operations are performed between linked nodes. In contrast with previous self-attention models with pre-deﬁned structures (edges), the model learns to construct attention edges to improve task-speciﬁc performances. In this way, the model is able to select the most salient nodes and reduce the quadratic complexity regardless of the sequence length. Based on SAC, we show that previous variants of self-attention models are its special cases. Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classiﬁcation, we demonstrate SAC is competitive with state-of-the-art models while signiﬁcantly reducing memory cost. 1 Introduction The self-attention mechanism has proved to beneﬁt a wide range of ﬁelds and tasks, such as natural language processing (Vaswani et al., 2017; Dai et al., 2019), computer vision (Xu et al., 2015; Jaderberg et al., 2015; Bello et al., 2019; Parmar et al., 2019), video classiﬁcation (Wang et al., 2018) and graph modeling (Veliˇckovi´c et al., 2018; Lee et al., 2019). The attention mechanism allows the model to attend to different parts of the input and capture the most salient features, and provides with the ability to handle dependencies between elements across long distances. Two conspicuous problems stand out with the self-attention mechanism: (1) the memory complexity is quadratic with respect to the input length, making it infeasible to handle long sequences; and (2) the self-attention operations are performed in a pre-deﬁned structure (usually fully-connected), which not only is costly but also lacks for the ability to learn the optimal attention structure optimized for the performance in the downstream tasks (Child et al., 2019; Sukhbaatar et al., 2019). These observations indicate that the fully-connected structure for self-attention operations can be replaced by a more sparse one where only speciﬁc edges are constructed for attention operations. In this paper, we present Sparse Adaptive Connection (SAC), which replaces the fully-connected structure in self-attention with a sparse graph-like structure adapted to different tasks. In SAC, we regard the input as a graph where a node could be a token in the sequence or an ﬂattened feature map of an image, and an edge between a pair of nodes represents they can attend to each other. To select edges, we propose an Edge Predictor which utilizes an LSTM model to dynamically predict pairs of nodes that represent two ends of an edge. In contrast with previous self-attention models with pre-deﬁned attention structures (edges), SAC learns to construct attention edges adaptively, which are optimized to improve task-speciﬁc performances using reinforcement learning models. We 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2003.09833v3  [cs.CL]  29 Sep 2020evaluate the proposed model on four tasks: neural machine translation, language modeling, graph representation learning and image classiﬁcation, and demonstrate that SAC learns to select the most salient nodes to attend to each other and is free from heavy memory cost while achieving strong performances. 2 Related Work Many recent researches have focused on modifying the structure of self-attention to relieve the computation and memory burden. Transformer-XL (Dai et al., 2019) enables learning long-term dependency by introducing a segment-level recurrence mechanism and a novel relative position encoding scheme (Shaw et al., 2018). (Guo et al., 2019) proposed Star Transformer, a variant of Transformer which replaces the fully-connected structure in self-attention with a star-shaped topology, where all tokens are connected with a relay node. (Child et al., 2019; Kitaev et al., 2020) suggest sparsifying Transformer by focusing only on a fraction of attention connections. (Child et al., 2019) introduced sparse factorizations of the attention matrix, which scale as O(np√n) with the sequence length, and a set of sparse attention kernels which efﬁciently compute subsets of the attention matrix. (Kitaev et al., 2020) proposed Reformer, a more efﬁcient yet complicated method for computing self-attention, where the dot-product attention is replaced by one that uses a locality-sensitive hashing, reducing the complexity from O(n2) to O(nlog n). (Ye et al., 2019) partitioned the sequence to different multi-scale spans and attended the context information from ﬁne-grain to coarse-grain as the relative distance increases. All above works rely on manually designed structures. Our work is inspired by (Sukhbaatar et al., 2019), which takes a step forward and uses a novel adaptive self- attention mechanism that can learn its optimal attention span for each head, and (Correia et al., 2019) which proposed adaptively sparse Transformer. Different from Yang et al. (2018), we use an LSTM to predict attention links which gives us ﬁner control of how sparse we want self-attention to be. Graph neural networks (GNNs) are known at learning local contextual information by encoding attribute features (Kipf and Welling, 2016; Hamilton et al., 2017b), but they are not able to explic- itly distinguish the most salient nodes from all its neighbors, neither can they directly attend to the nodes that are beyond one-hop away. Much work has investigated the effect of attention on GNNs (Veliˇckovi´c et al., 2018; Abu-El-Haija et al., 2018; Lee et al., 2018; Veliˇckovi´c et al., 2019). (Veliˇckovi´c et al., 2018) extended self-attention to graphs by enabling each node to attend over its neighbors, achieving state-of-the-art results for semi-supervised node classiﬁcation tasks. But they simply applied self-attention over graphs to all neighbors of a node, which might be a problem when dealing with large and noisy graphs where only few neighbors need to be aggregated. (Ye and Ji, 2019) proposed Sparse Graph Attention Network which uses a binary gate to control whether each edge should be engaged. However, these works lack ability to aggregate long-range dependencies in graphs, and they only consider neighbors that are one hop away. Various methods have been proposed to tackle this issue (Ye et al., 2020; Zhang et al., 2020; Pei et al., 2020). Similar to graphs, (Bello et al., 2019) introduced a novel two-dimensional relative self-attention mechanism for images and augmented convolutional operators with this self-attention method, showing systematic improvements on both image classiﬁcation and object detection tasks across a wide range of architectures. 3 Background: Self-Attention Given a set of nodes1 {e1,··· ,eN}as inputs, self-attention iteratively computes the representation of ei in the l-th layer by attending to all its neighbors N(ei), which is deﬁned as follows: ˜hl i = ∑ ej∈N(ei) αijvl−1 j , αij = softmax   ( ql−1 i )T kl−1 j√ d   and ql−1 i = WQhl−1 i , kl−1 j = WKhl−1 j , vl−1 j = WVhl−1 j (1) where dis the hidden dimension, WQ,WK,WV are learnable parameters and q,k,v correspond to queries, keys and values, respectively. The multi-head mechanism linearly projects the queries, keys 1We use the term “node’ in a broad sense of denoting any particular unit in text, images or graphs. 2and values multiple times with different learned linear projections, and then performs self-attention in parallel, after which the results are concatenated and again projected: hl i = Concat(˜hl,1 i ,··· ,˜hl,m i )WO (2) where the superscript 1,···,m denotes the head number, and WO is learnable parameters. After L iterations, we obtain the ﬁnal representation for each node hL i . 4 Sparse Adaptive Connection for Self-Attention The key point in SAC is to use to an LSTM edge predictor to predict edges for self-attention operations between nodes, where a node could be a token in the sequence or an ﬂattened feature map of an image. Self-attention operations are performed between linked nodes instead of in a fully-connected manner. The LSTM edge predictor is optimized to improve task-speciﬁc performances using reinforcement learning models. 4.1 LSTM Edge Predictor In SAC, an edge predictor is used to construct edges between nodes for self-attention operations. Suppose that we are given a set of nodes {e1,··· ,eN}with no edge between any pair of nodes when initialization, our aim is to generate edges using this edge predictor, with the total number αN for each layer, where αis a hyperparameter deciding how many edges should be constructed for each node on average. The Edge Predictor uses an LSTM model as a backbone and sequentially predicts edges. The prediction of an edge is decoupled into the prediction of the original node and the destination node pair. More formally, the input to Edge Predictor is a special token “[SOS]”, and the model proceeds to predict the original node and destination node of all edges (2αN nodes in total) for the ﬁrst layer, denoted by {y1 1,y1 2,··· ,y1 2αN}, where the superscript denoted the index of the layer and the subscript denoted the index of the predicted node. At each time step, the input to the LSTM model is the representation hyt for the node that has just been predicted. Then it is combined with the previously constructed representation gt to obtain gt+1 representing the current time-step using LSTMs, and gt+1 is used to predict the following node using the softmax function. The projection matrix before softmax W shares embeddings with node representations, where each column wi is the vector representation for node ei. The probability of predicting node yt+1 given gt+1 is thus given by: p(yt+1 = ei) = exp (gT t+1 ·wi)∑ jexp (gT t+1 ·wj) (3) This process is repeated 2αN times. After the end of αN edge predictions, we update the representa- tion for each node based on self-attention as will be detailed in Section 4.1 for different tasks, and proceed to the next layer. For node predictions in the following layer, the initial input now becomes hidden state for the last time-step of the previous layer. The entire process is repeated Ltimes, where Ldenotes the number of self-attention layers and the resulted nodes in layerlare {yl 1,yl 2,··· ,yl 2αN}. Compared to separately predicting edges for each node, this approach is more ﬂexible and gives us ﬁner control of the total number of edges we would like to construct. More importantly, this process is aware of previous constructed edges, both in the current layer and previous layers. The recurrent edge predictor is shown in Figure 1(b). We implement it using a single-layer LSTM model. Once having constructed all edges for each layer, we can immediately obtain the set of neighbors N(en i) for each node ei in the n-th layer. Self-attention operations with multi-head mechanism are then performed on its neighbors for each node. For text tasks, we regard each token as a node. For graph-like structures, we treat nodes in the original graph as nodes. For images, the input (H,W,F in) dimensional sensor is reshaped to a HW ×Fin matrix, where each row can be thought as a node by our deﬁnition. 4.2 Distance Encoding The input graph intrinsically displays some degree of structures. For example, in a sequence of natural language tokens, the relative distance between two tokens in the sequence or the corresponding parse 3𝑒𝑖 𝑒1 𝑒2 𝑒3 𝑒4 i really like cats Edge  Predictor 𝑒𝑖 𝑒1 𝑒2 𝑒3 𝑒4 Self- Attention Self- Attention (a) (b) e1 e3 e3 e2 e2 e4 Distance Encodings 𝐠6 ×( )+ Node Encodings 𝐰1,𝐰2,𝐰3,𝐰4 𝐯2,𝐯0,𝐯1,𝐯-1 layer 𝑛 Figure 1: An illustration of the proposed Sparse Apdative Connection. (a) shows the process of SAC to construct edges and then perform self-attention on these edges (Red is for text and green is for graphs). (b) shows the edge prediction process of (a) with distance encodings. When predicting time-step 6, the word embeddings are added with distance encodings. tree encodes structural information. As another example, in the task of node representation learning in graphs (Yang et al., 2016b; Veliˇckovi´c et al., 2017; Xu et al., 2018), the graph originally comes with the node edges. The LSTM edge predictor described above ignores this structure. To leverage original structural information, we propose distance encodings to incorporate graph structure into the edge predictor. Distance encodings only affect the destination node predictions. In contrast to only using node embedding matrix W, we add an extra distance matrix V that encodes distance information to the original projection matrix W, giving V + W. Each column in V is its corresponding distance representation to the current original node. For example in Figure 1, at time-step 6, when two edges (e1,e3),(e3,e2), and one origin node e2 have been generated, we are to use g5 ∈Rd, the output of the LSTM model at time-step 5, to predict the node at time-step 6. According to the original structure, the distance between e2 (the current origin node) and all the nodes e1,e2,e3,e4 by far are 2, 0, 1, and -1 respectively, where -1 means inability to reach. The distance vectors are thus v2,v0,v1,v−1, which are vectors of size Rd to be learned. Intuitively, this process also discourages generating duplicate edges and leverages the original structural information. In contrast to Veliˇckovi´c et al. (2017) where attention operations are only performed between nodes with literal edges in the original graph, SAC offers the ﬂexibility in leveraging the original graph structure and inﬂuence from the training signals. Additionally, SAC allows for more convenient information exchange between similar nodes that are far away in terms of distance in the original graph structure, which is because the connection construction stage has the ability to connect any pair nodes in the graph. This ability potentially leads to better performances. 4.3 Training and Test Directly training the edge predictor is impractical since we have no access to the ground-truth edges. We use REINFORCE, which is an instance of a broader class of policy gradient methods for optimization. The main idea is to use reinforcement learning to discover the best edge connections for self-attention operations. Each action ais the node predicted by edge predictor. Let Θ denote parameters of the edge predictor and Φ denote the parameters of the main network which maps an input to its ﬁnal label based on a pre-deﬁned self-attention structure. Under the framework of reinforcement learning, we ask the edge predictor to maximize its reward R(Θ), which is the log probability of predicting the correct label, e.g., for neural machine translation the reward Ris the average log probability of golden target tokens; for image classiﬁcation, the reward the log probability of the correct label. Consider the simple case where different attention layers use the same node connections, by sampling a sequence of nodes from the edge predictor, we are able to update the parameters in edge predictor using policy gradients: ∇J(Θ) = 2αN∑ i=1 ∇log p(ai|a1:i−1; Θ)(R(Θ) −b) (4) 4layer 𝑛 layer 𝑛 −1 Vanilla Self-attention Transformer-XL Seg-Length=2 BT-Transformer layer 𝑛 +1 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 Adaptive Span S=2 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 Figure 2: Connection of SAC to other methods for computing self-attention. where bdenotes the baseline which is the average of the previous rewards. Φ is updated directly based on the log-likelihood. At test time, edges are decoded using beam search. We use a beam size of 5 for all models. 4.4 Variants of Edge Predictor The vanilla version of the Edge Predictor can be further regulated, simpliﬁed or expanded with prior knowledge for preferable graph structures. All layers sharing the same structure To reduce the computational cost and RL search space, we can enforce the edge structure to be the same for all layers, where the process is only executed once instead of Ltimes. We adopt this strategy for all settings to reduce the search space. All nodes connected in each layer To enforce each node to be connected in each layer, for each node ei, it is repeatedly fed to the predictor αtimes as the original node, and we only predict the destination node. The graph can be either directed graph or undirected graph, depending on how we want self-attention to be computed. Different heads attending to different contexts (head adaptive for short) Sukhbaatar et al. (2019) shows that it is beneﬁcial if different heads attend to different spans (some focusing on the recent history, while others focusing the whole available context). We can also augment the model by assigning each head with a edge predictor, providing the ﬂexibility that different heads can attend to different chunks of context. We sequentially predict all input and output nodes for each head, and the prediction of 2αN nodes are repeated Htimes. In this way, the prediction model for the current head is aware of the information of all previous heads. A head speciﬁc embedding is appended to the node embedding in LSTMs to let the model be aware of the current head. Since this strategy signiﬁcantly increases the search space in RL, we empirically ﬁnd that it helps some settings, but not always. 4.5 Connection to Existing Methods In this subsection, we describe the connection between SAC and previous variants of self-attentions, and show that these variants computing self-attention can be obtained through SAC if we slightly modify the edge predictor. For ease of exposition, we use EP(e) ={(ei,ej)}to denote the collection of all edges for self-attention operations. Connection to vanilla self-attention (Vaswani et al., 2017) The vanilla self-attention links each pair of nodes, where EP(e) can be described as EP(e) ={(ei,ej)|i∈[1,N]}. Connection to Transformer-XL (Dai et al., 2019) Transformer-XL treats the text in a segment- by-segment style. Self-attention operations are performed between nodes within the same segment. EP(e) can be described as EP(e) ={(ei,ej)|i∈[1,N]; j ∈Segment(i)}. Connection to Adaptive Span Transformer (Sukhbaatar et al., 2019) Adaptive Span Trans- former learns an optimal attention span for each head. Suppose the span size assigned to head tis s, then EP(e,t) can be described by: EP(e)={(ei,ej)|i∈[1,N]; j = i,i −1,··· ,i −s+ 1;span = t}. Connection to BP-Transformer (Ye et al., 2019) BP-Transformer constructed a tree-like graph by adding span nodes apart from token nodes. There are 2N −1 nodes in total, where N is the sequence length. In BP-Transformer, each token (leaf) node attends to each span (non-leaf) node that includes it, which we refer to as Ancestor(ei) for node ei. It is easy to prove that a leaf node is 5Model H B edges dev test test (heads) (blocks) (BLEU) (BLEU) (cased sacreBLEU) Transformer Base (Vaswani et al., 2017) 8 6 N2 25.8 27.3 BP base (Ye et al., 2019) 28.1 27.6 Reversible base (Kitaev et al., 2020) 28.0 27.4 SAC base 8 6 2 N 17.4 18.3 17.8 SAC base 8 6 5 N 25.6 27.0 26.2 SAC base 8 6 10 N 26.0 27.7 27.0 SAC base 8 6 15 N 25.6 27.4 26.8 SAC base 16 6 10 N 26.2 28.1 27.6 SAC base 16 12 10 N 26.4 28.4 27.8 Transformer big (Vaswani et al., 2017) 16 6 N2 26.4 28.4 Reversible big (Kitaev et al., 2020) 29.1 28.4 SAC Large 16 6 10 N 26.7 28.9 28.1 SAC Large 16 18 10 N 26.9 29.4 28.6 SAC Large (dependency) 16 18 10 N 26.9 29.5 28.8 Table 1: BLEU scores on the newstest2013 for development and newstest2014 for test for WMT English-German. N denotes the length of the input sequence. associated with ⌊log2 N⌋non-leaf nodes (and thus attends to ⌊log2 N⌋nodes). Therefore, we have EP(e)={(ei,ej)|i∈[1,N]; j ∈Ancestor(ei)}. 5 Experiments 5.1 Machine Translation We use the encoder-decoder model (Bahdanau et al., 2014; Vaswani et al., 2017) as the backbone for machine translation. For the encoder, SAC constructs αN edges for each layer and self-attention operations are performed between connected nodes. For the decoder, masked attention (Vaswani et al., 2017) is applied. Speciﬁcally, given a newly generated target node, it can attend to all source nodes, dummy nodes, target nodes that come beforehand, but not target nodes that come afterwards. We again use SAC to construct edges between the newly generated node and the preceding nodes, where the input node to the edge predictor is forced to be the newly generated node, and the output node is limited to preceding nodes and the dummy nodes. Following Vaswani et al. (2017); Ott et al. (2018); Kitaev et al. (2020), we used the standard WMT 2014 English-German dataset to test the proposed model. The dataset consists of about 4.5 million sentence pairs. Sentences are encoded using BPE (Sennrich et al., 2016), which has a shared source target vocabulary of about 37000 tokens. For fair comparison, we used the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 and ϵ= 10−9 for all models. Label smoothing (Szegedy et al., 2016) with ϵ= 0.1 is applied for all models. For the base setup, following Vaswani et al. (2017), the dimensionality of inputs and outputs dmodel is set to 512, and the inner-layer has dimensionality dff is set to 2,048. For big models, dmodel is set to 1,024 and dff is set to 4,096. Models are run on 8 NVIDIA V100 GPUs. Results are shown in Table 1. As we gradually increase the number of edges for each layer (from 2 to 5 to 10 to 15 per node), we can see that the performance ﬁrst increases, reaching the highest with αset to 10, and then decreases. This means that performing attention operations between all pairs is not only unnecessary, but can hurt the performance. Memory saved from sparse connections allow for more heads to perform attentions and deeper networks with more blocks, leading to better performances over vanilla transformers. We also implement a dependency-based model, in which English sources were ﬁrst parsed using Stanford Dependency parser (Chen and Manning, 2014). Relative positions between nodes in the dependency trees are encoded in distance encodings of the edge predictor. The introduction of dependency parser for attention construction introduces +0.14 BLEU score boost. We did not observe signiﬁcant performance boost from the head-adaptive strategy, and thus omit their performances. 6Method Enwiki8 Text8 Params Trans (Al-Rfou et al., 2019) 1.11 1.18 44M Trans-XL (Dai et al., 2019) 1.06 - 41M Adaptive(Sukhbaatar et al., 2019) 1.02 1.11 39M BPT (Ye et al., 2019) 1.02 1.11 38M SAC (basic) 1.02 1.07 39M SAC (head adaptive) 1.00 1.06 39M Table 2: Performances on language modeling datasets. 5.2 Language Modeling We use character-level language modeling datasets to evaluate SAC’s ability to handle long-term dependencies. We use Enwiki8 (Mahoney, 2011) and Text8 (Mahoney, 2011) for evaluation and report the values of BPC for different models. We use the Transformer decoder architecture as the backbone. We compare SAC with other variations of transformers to ﬁt long sequences into the model, including the vanilla Transformer (Al-Rfou et al., 2019), which splits the whole sequence into smaller segments, and only trains the model within each segment and ignore the rest; Transformer-XL (Dai et al., 2019) that adopts a recurrence mechanism to cache the memory of previous segments; adaptive span model (Sukhbaatar et al., 2019) that assigns different heads with different text spans in an adaptive fashion; and the BP-Transformer (Ye et al., 2019) that splits the sequence using binary trees. For SAC, αis set to 256 for each node. The relatively small memory cost allows the model to look at a maximum context of 50k characters. Input dimensionality is set to 512, and the inner-layer dimensionality 2,048. Following (Sukhbaatar et al., 2019), we use Adagrad for optimization, with a batch size of 64 and ﬁxed learning rate of 0.07 and 32k warm-up steps. Results are shown in Table2. As can be seen, SAC-basic outperforms the other Transformers by 0.04 bcp on Text8 while signiﬁcantly reducing the memory usage for large attention spans. For Enwiki8, it ties with the best BPT model, achieving 1.02 bcp score. The improvement validates the importance modeling long-term dependencies with limited available memory. We also ﬁnd that, in the language modeling tasks, the head-adaptive strategy helps, 5.3 Representation Learning in Graphs We test the performance of the proposed model on both transductive and inductive benchmark datasets. For the transductive setup, we used the three standard citation network benchmarks, Cora, Citeseer and Pubmed (Sen et al., 2008). In the transductive setup, we used the Protein-protein interaction dataset (PPI) (Zitnik and Leskovec, 2017). The training algorithm has access to all of the nodes’ feature vectors and labels, and predictions are performed on the test nodes. The detailed descriptions for Cora, Citeseer, Pubmed and PPI are found in the Appendix due to the space limit. The difference between SAC and (Veliˇckovi´c et al., 2017) is that the latter performs self-attention operations between nodes that are connected though graph edges, while SAC perform self-attention operations between nodes linked by the edge predictor. For fast convergence, we initialize SAC using the pretrained attention model (Veliˇckovi´c et al., 2017), where attention links are just edges in the original graph. Then we start exploring edge construction across all nodes. the number of attention heads is ﬁxed to 8 and the number of blocks is set to 12. We experiment different values of α, i.e, [5, 10, 50, 100] unless the memory usage reaches limitation. We train all models with Adam (Kingma and Ba, 2014) and early stopping on the validation set. The initial learning rate is treated as a hyper-parameter trained on the validation set. Following (Veliˇckovi´c et al., 2017), we run 100 epochs in total and use an early stopping strategy on the both the cross-entropy loss and accuracy for transductive tasks and micro-F1 for inductive tasks. Each experiment is repeated three times and we report the mean value. Results are shown in Table 3. We note that SAC achieves signiﬁcant performance boosts over existing methods across all four datasets, i.e., outperforms our implemented GAT +1.8, +1.1, +0.7 and +1.1 respectively on Cora, Citeseer, Pubmed and PPI. The explanation for SAC’s advantage is as follows: graph node representation learning concerns about both label propagation and relatedness between nearby nodes in the vector space, the latter of which is what GCN handles. As veriﬁed in many 7Available data Method Cora Citeseer Pubmed PPI A DeepWalk (Perozzi et al., 2014) 67.2 43.2 65.3 – X,A DGI (Veliˇckovi´c et al., 2019) 82.3 71.8 76.8 63.8 X,A GraphSAGE (Hamilton et al., 2017a) – – – 50.2 X,A,Y SemiEmb (Weston et al., 2012) 59.0 59.6 71.7 – X,A,Y Planetoid (Yang et al., 2016a) 75.7 64.7 77.2 – X,A,Y Chebyshev (Defferrard et al., 2016) 81.2 69.8 74.4 – X,A,Y GCN (Kipf and Welling, 2016) 81.5 70.3 70.0 – X,A,Y MoNet (Monti et al., 2017) 81.7 – 78.8 – X,A,Y SGC (Wu et al., 2019) 81.0 71.9 78.9 – X,A,Y AdaLNet (Liao et al., 2019) 80.4 68.7 78.1 – X,A,Y SGAT (Ye and Ji, 2019) 84.2 68.2 77.6 96.6 X,A,Y CurvGN-n (Ye et al., 2020) 82.7 72.1 79.2 – X,A,Y GAT (Veliˇckovi´c et al., 2017) 83.0 72.5 79.0 97.3 X,A,Y SAC 84.8 73.8 79.7 98.4 X,A,Y SAC (head adaptive) 84.7 74.0 80.1 98.4 Table 3: Summary of results in terms of classiﬁcation accuracies on transductive tasks (Cora, Citeseer and Pubmed) or micro-averaged F1 score on inductive tasks (PPI). In the ﬁrst column, we report the kind of data available to each method during training (X: features, A adjacency matrix, Y: labels). CIFAR100 ImageNet GFlops top1 top5 Params GFlops top1 top5 Params WideResNet 10.4 80.3 95.0 36.3M ResNet50 8.2 76.4 93.1 25.6M Bello et al. (2019) 10.9 81.6 95.2 36.2M 8.3 77.7 93.8 25.8M SAC 11.0 82.2 95.4 36.2M 8.3 78.5 94.2 25.9M SAC (head adaptive) 11.0 82.4 95.5 36.2M 8.3 78.7 94.3 25.9M Table 4: Results of image classiﬁcation on CIFAR-100 using the Wide-ResNet 28-10 Zagoruyko and Komodakis (2016) as the backbone and on ImageNet using the ResNet-50 He et al. (2016) model. recent works Liu et al. (2018); Wang and Leskovec (2020), combining both facets leads to better performances. The attention edge prediction stage in SAC fosters information exchange between nodes that are not directly linked in graph but similar in terms of label propagation. SAC actually offers the probability in bridging the aspects, leading to better performances. 5.4 Image Classiﬁcation Augmenting convolution models with self-attention (Bello et al., 2019; Parmar et al., 2019; Hu et al., 2019; Wang et al., 2019) provides the model with the ability to capture global contexts in an image and has yielded gains in several vision tasks such as image classiﬁcation and objective detection. We follow the protocols in (Bello et al., 2019), i.e. incorporating relative position embeddings for self-attention operations and augmenting each ResNet (Zagoruyko and Komodakis, 2016; He et al., 2016) block with self-attentions. To handle the prohibitive memory cost, (Bello et al., 2019) performs self-attention operations starting from the last layer, which has the smallest spatial dimension, until memory constraints are hit. This ad-hoc strategy is replaced by SAC. Following (Bello et al., 2019), we conduct experiments on CIFAR-100 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009). For CIFAR-100, we use the Wide-ResNet-28-10, the architecture of which comprises 3 stages of 4 residual blocks each using two 3×3 convolutions. We augment each convolution of all residual blocks with the number of attention heads set to 16. For ImageNet, we use ResNet-50, the block of which consists of 1×1, 3×3, 1×1 convolutions where the last pointwise convolution expands the number of ﬁlters and the ﬁrst one contracts the number of ﬁlters. We tune αin range {5,10,20}. Results are shown in Table 4. As can be seen, the proposed SAC model signiﬁcantly outperforms the attention model in (Bello et al., 2019) with the only modiﬁcation of automatic edge construction. Speciﬁcally, the top-1 score increases from 81.6 to 82.4 for CIFAR-100 and from 77.7 to 78.7 for ImageNet. The improvement validates the importance of performing necessary attention operations under memory limit. 86 Conclusion In this work, we propose Sparse Adaptive Connection — a sparse connection method to accelerate and structure the self-attention mechanism that adapts to various downstream tasks. We use an LSTM edge predictor to construct edges for self-attention operations, which gives us control of how sparse we want self-attention to be by setting the sparse coefﬁcient α. We demonstrate that SAC is competitive with state-of-the-art models on neural machine translation, language modeling, graph classiﬁcation and image classiﬁcation, while reducing memory costs. Broader Impact Accelerating fully-connected self-attention has been a research trend in recent years. Vanilla self- attention models, such as Transformers and BERT, are not able to process extremely long text, where text must be in advance segmented into pieces and then can be individually modelled. The lack of adequate context leads to poor performances in generating long, coherent and ﬂuent text. The goal of our proposed method, SAC, is to provide a way of relieving the computation burden of vanilla self-attention by automatically searching for the best attention patterns. We believe SAC has great potentials to generate high-quality long text. While there is risk of abuse, like generating fake news, the value of SAC is generally safe and weighs more than abuse to the whole society. Acknowledgement We thank all reviewers for their insightful comments. We also want to thank Zihao Ye for his helpful suggestions on evaluations, along with suggestions on learning head-speciﬁc policies. References Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. 2018. Watch your step: Learning node embeddings via graph attention. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9180–9190. Curran Associates, Inc. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3159–3166. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V . Le. 2019. Attention augmented convolutional networks. In The IEEE International Conference on Computer Vision (ICCV). Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar. Association for Computational Linguistics. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins. 2019. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174–2184, Hong Kong, China. Association for Computational Linguistics. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2978–2988, Florence, Italy. Association for Computational Linguistics. 9Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In D. D. Lee, M. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3844–3852. Curran Associates, Inc. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee. Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-transformer. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1315–1325, Minneapolis, Minnesota. Association for Computational Linguistics. Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017a. Inductive representation learning on large graphs. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1024–1034. Curran Associates, Inc. William L. Hamilton, Rex Ying, and Jure Leskovec. 2017b. Representation learning on graphs: Methods and applications. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778. Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. 2019. Local relation networks for image recognition. In Proceedings of the IEEE International Conference on Computer Vision , pages 3464–3473. Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. 2015. Spatial trans- former networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2017–2025. Curran Associates, Inc. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Thomas N. Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efﬁcient transformer. In International Conference on Learning Representations. Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. John Boaz Lee, Ryan Rossi, and Xiangnan Kong. 2018. Graph classiﬁcation using structural attention. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, page 1666–1674, New York, NY , USA. Association for Computing Machinery. Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention graph pooling. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3734–3743, Long Beach, California, USA. PMLR. Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. 2019. Lanczosnet: Multi-scale deep graph convolutional networks. In International Conference on Learning Representations. Arthur Liberzon, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. Molecular signatures database (msigdb) 3.0. Bioinformatics, 27(12):1739– 1740. Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. 2018. Learning to propagate labels: Transductive propagation network for few-shot learning. 10Matt Mahoney. 2011. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text. html. F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. 2017. Geometric deep learning on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5425–5434, Los Alamitos, CA, USA. IEEE Computer Society. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 1–9, Brussels, Belgium. Association for Computational Linguistics. Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. 2019. Stand-alone self-attention in vision models. In Advances in Neural Information Processing Systems 32, pages 68–80. Curran Associates, Inc. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2020. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14, page 701–710, New York, NY , USA. Association for Computing Machinery. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classiﬁcation in network data. AI magazine, 29(3):93–93. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for Computational Linguistics. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335, Florence, Italy. Association for Computational Linguistics. C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. 2016. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, Los Alamitos, CA, USA. IEEE Computer Society. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In International Conference on Learning Representations. Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. 2019. Deep graph infomax. In International Conference on Learning Representations. Hongwei Wang and Jure Leskovec. 2020. Unifying graph convolutional neural networks and label propagation. Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. 2019. Eca-net: Efﬁcient channel attention for deep convolutional neural networks. arXiv preprint arXiv:1910.03151. 11Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jason Weston, Frédéric Ratle, Hossein Mobahi, and Ronan Collobert. 2012. Deep Learning via Semi-supervised Embedding, pages 639–655. Springer Berlin Heidelberg, Berlin, Heidelberg. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6861–6871, Long Beach, California, USA. PMLR. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research, pages 2048–2057, Lille, France. PMLR. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536. Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016a. Revisiting semi-supervised learning with graph embeddings. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 40–48, New York, New York, USA. PMLR. Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016b. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861. Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Ruslan Salakhutdi- nov, and Yann LeCun. 2018. Glomo: Unsupervisedly learned relational graphs as transferable representations. Yang Ye and Shihao Ji. 2019. Sparse graph attention networks. Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. 2020. Curvature graph network. In International Conference on Learning Representations. Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. Bp-transformer: Modelling long-range context via binary partitioning. Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC), pages 87.1–87.12. BMV A Press. Kai Zhang, Yaokang Zhu, Jun Wang, and Jie Zhang. 2020. Adaptive structural ﬁngerprints for graph attention networks. In International Conference on Learning Representations. Marinka Zitnik and Jure Leskovec. 2017. Predicting multicellular function through multi-layer tissue networks. Bioinformatics, 33(14):i190–i198. 12A Graph Datasets For the transductive setup, we used the three standard citation network benchmarks, Cora, Cite- seer and Pubmed (Sen et al., 2008). We followed the transductive setup adopted in (Yang et al., 2016b; Veliˇckovi´c et al., 2017; Xu et al., 2018), where nodes correspond to documents and edges to (undirected) citations. Cora contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. Citeseer contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. Pubmed contains 19717 nodes, 44338 edges, 3 classes and 500 features per node. For the inductive setup, we used the Protein-protein interaction dataset (PPI) (Zitnik and Leskovec, 2017), which aims at classifying protein roles such as cellular functions and gene ontology in various protein-protein interaction (PPI) graphs, where each graph corresponds to a different human tissue. Critically, testing graphs remain completely unobserved during training. The dataset has 56.9K nodes, 806.2 edges with 121 classes. The average number of nodes per graph is 2372. Each node has 50 features that are composed of positional gene sets, motif gene sets and immunological signatures. There are 121 labels for each node set from gene ontology, collected from the Molecular Signatures Database (Liberzon et al., 2011), and a node can have several labels simultaneously. 13barack obama is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017 . a member of the democratic party , he was the first african-american president of the united states . he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004 . barack obama is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017 . a member of the democratic party , he was the first african-american president of the united states . he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004 . 0.0 0.2 0.4 0.6 0.8 1.0 Figure 3: Predicted self-attention links for the text barack obama is an american politician and attorney who served as the 44th president of the president of the united states from 2009 to 2017. as a member of the democratic party, he was the ﬁrst african-american president of the united states. he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004. 14",
      "meta_data": {
        "arxiv_id": "2003.09833v3",
        "authors": [
          "Xiaoya Li",
          "Yuxian Meng",
          "Mingxin Zhou",
          "Qinghong Han",
          "Fei Wu",
          "Jiwei Li"
        ],
        "published_date": "2020-03-22T07:58:44Z",
        "pdf_url": "https://arxiv.org/pdf/2003.09833v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Sparse Adaptive Connection (SAC), a method to accelerate and structure the self-attention mechanism by addressing its quadratic memory complexity and the limitation of pre-defined, often fully-connected, attention structures. SAC learns to dynamically construct task-specific sparse attention edges, reducing complexity and memory cost. It demonstrates that many existing self-attention variants can be viewed as special cases of SAC. Through extensive experiments, SAC is shown to be competitive with state-of-the-art models across neural machine translation, language modeling, graph representation learning, and image classification, while significantly reducing memory requirements.",
        "methodology": "SAC treats the input sequence as a graph where self-attention operations are performed only between linked nodes. The core component is an LSTM Edge Predictor, which sequentially predicts pairs of source and destination nodes to form attention edges, aiming to construct αN edges per layer (where α is a sparsity hyperparameter). To incorporate intrinsic structural information (e.g., relative distances in text, existing graph edges), SAC employs Distance Encodings by adding a learnable distance matrix to the projection matrix during destination node prediction. The edge predictor is optimized using REINFORCE, a policy gradient method, as ground-truth edges are unavailable; it maximizes a reward signal derived from the downstream task's performance (e.g., log probability of correct labels). SAC offers variants such as sharing the same edge structure across all layers (default for reduced search space), ensuring all nodes are connected, and a head-adaptive strategy where different attention heads learn distinct edge predictors.",
        "experimental_setup": "SAC was evaluated on four diverse tasks: neural machine translation (NMT), character-level language modeling (LM), graph representation learning (GRL), and image classification (IC). For NMT, the WMT 2014 English-German dataset was used with an encoder-decoder Transformer backbone, Adam optimizer, and label smoothing. For LM, Enwiki8 and Text8 datasets were used with a Transformer decoder architecture, Adagrad optimizer, and a large context size (up to 50k characters). For GRL, transductive tasks used Cora, Citeseer, and Pubmed citation networks, while the inductive task used the Protein-protein interaction (PPI) dataset; models were initialized with a pre-trained GAT, optimized with Adam and early stopping. For IC, CIFAR-100 (using Wide-ResNet-28-10) and ImageNet (using ResNet-50) were used as backbones. Performance was measured by BLEU score (NMT), Bits Per Character (BPC) (LM), classification accuracy/micro-F1 (GRL), and top-1/top-5 accuracy (IC). Beam search (size 5) was used for decoding edges at test time. The sparsity coefficient α was varied and tuned for each task.",
        "limitations": "The primary practical limitation is the impossibility of directly training the edge predictor due to the absence of ground-truth optimal attention edges, which necessitates the use of more complex reinforcement learning methods like REINFORCE. Furthermore, while a head-adaptive strategy was explored to allow different attention heads to focus on distinct contexts, it did not consistently provide significant performance boosts across all settings, and in some cases, it considerably increased the search space without proportional benefits.",
        "future_research_directions": "The authors suggest that SAC has great potential for generating high-quality, long, and coherent text by effectively modeling long-term dependencies while significantly reducing memory costs. This implies future work could focus on applying SAC to advanced text generation tasks. Further research could also explore more sophisticated or consistently beneficial head-adaptive strategies, as well as methods for dynamically determining the optimal sparsity coefficient α rather than treating it as a fixed hyperparameter."
      }
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost",
      "abstract": "To overcome the quadratic cost of self-attention, recent works have proposed\nvarious sparse attention modules, most of which fall under one of two groups:\n1) sparse attention under a hand-crafted patterns and 2) full attention\nfollowed by a sparse variant of softmax such as $\\alpha$-entmax. Unfortunately,\nthe first group lacks adaptability to data while the second still requires\nquadratic cost in training. In this work, we propose SBM-Transformer, a model\nthat resolves both problems by endowing each attention head with a\nmixed-membership Stochastic Block Model (SBM). Then, each attention head\ndata-adaptively samples a bipartite graph, the adjacency of which is used as an\nattention mask for each input. During backpropagation, a straight-through\nestimator is used to flow gradients beyond the discrete sampling step and\nadjust the probabilities of sampled edges based on the predictive loss. The\nforward and backward cost are thus linear to the number of edges, which each\nattention head can also choose flexibly based on the input. By assessing the\ndistribution of graphs, we theoretically show that SBM-Transformer is a\nuniversal approximator for arbitrary sequence-to-sequence functions in\nexpectation. Empirical evaluations under the LRA and GLUE benchmarks\ndemonstrate that our model outperforms previous efficient variants as well as\nthe original Transformer with full attention. Our implementation can be found\nin https://github.com/sc782/SBM-Transformer .",
      "full_text": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost Sungjun Cho1 Seonwoo Min1 Jinwoo Kim2 Moontae Lee1,3 Honglak Lee1 Seunghoon Hong2,1 1LG AI Research 2KAIST 3University of Illinois Chicago Abstract To overcome the quadratic cost of self-attention, recent works have proposed various sparse attention modules, most of which fall under one of two groups: 1) sparse attention under a hand-crafted patterns and 2) full attention followed by a sparse variant of softmax such as α-entmax. Unfortunately, the ﬁrst group lacks adaptability to data while the second still requires quadratic cost in train- ing. In this work, we propose SBM-Transformer, a model that resolves both problems by endowing each attention head with a mixed-membership Stochastic Block Model (SBM). Then, each attention head data-adaptively samples a bipar- tite graph, the adjacency of which is used as an attention mask for each input. During backpropagation, a straight-through estimator is used to ﬂow gradients beyond the discrete sampling step and adjust the probabilities of sampled edges based on the predictive loss. The forward and backward cost are thus linear to the number of edges, which each attention head can also choose ﬂexibly based on the input. By assessing the distribution of graphs, we theoretically show that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation. Empirical evaluations under the LRA and GLUE bench- marks demonstrate that our model outperforms previous efﬁcient variants as well as the original Transformer with full attention. Our implementation can be found in https://github.com/sc782/SBM-Transformer. 1 Introduction The Transformer [40] architecture has been the go-to method for encoding sequential data, due to its superior performance in various tasks such as machine translation [30], image classiﬁcation [15], and protein language modeling [34]. Its key strength stems from the multi-head attention module, where a so-called attention score matrix computes how contextually important one token is to another for all possible token pairs. Each Transformer layer simultaneously pools the token representations based on the attention scores, eventually returning contextualized features without sequentially traversing through the input sequence as its recurrent neural network-based predecessors [18]. A well-known drawback of the original Transformer is its high computational cost in time and memory that increases quadratically with sequence length. This is due to the full pairwise computation of attention scores, which prohibits applying it in tasks involving long-range dependencies such as document summarization [19] or high-resolution image processing [ 53]. Many works have thus focused on developing more efﬁcient alternatives by exploiting ﬁxed or learnable attention sparsity patterns [9, 51, 22, 13], low-rank approximations [45, 48], or kernelized attention modules [21, 10]. Even though the efﬁcient alternatives hold theoretical expressibility guarantees [ 50], they are far from sufﬁcient, still failing to convince practitioners to replace the original Transformer. We believe this is mostly due to their lack of adaptability. They apply the same modiﬁcations to unanimously sparsify all the attention modules across layers, without considering the tasks at hand. Such strategy 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2210.15541v1  [cs.LG]  27 Oct 20221 LinearLinearLinear LinearLinearLinear LinearLinearLinear Scaled Dot-Product AttentionScaled Dot-Product AttentionScaled Dot-Product Attention Concatenate Linear Query 𝑸, Key 𝑲 𝑴 𝝈𝑴 𝑴⊙𝑸𝑲𝑇 𝐷 𝑽 Implicitly compute edgelistSBM 𝑸 𝑲 𝑽 Figure 1: The attention module in SBM-Transformer. In multi-head attention, each attention head samples a bipartite graph connecting queries to keys from an underlying SBM. The adjacency of the sampled graph is used as an attention mask to compute the dot products only for the sampled edges. imposes inductive bias too strongly and often leads to sub-optimal cost vs. performance trade-offs in downstream tasks [29]. In this work, we argue that to retain the utmost potential of Transformers, each attention module should have the ability to ﬂexibly choose between sparse and full attention. This is especially evident when considering many state-of-the-art systems suggest the need for a mixture of dense and sparse attention layers. For example, a qualitative analysis on pretrained BERT showed that lower layers exhibit broad dense attention while upper layers perform focused sparse attention [11]. In the case of GPT-3 [7], the Transformer blocks are manually arranged to alternate between dense and sparse attention. To contribute to the efﬁcient Transformers lineage, we propose SBM-Transformer, capable of adjusting its attention sparsity data-adaptively based without fully computing the attention score matrix (Figure 1). Leveraging a mixed-membership Stochastic Block Model (SBM) [ 2], each attention head samples a bipartite graph connecting queries to keys. Then, the adjacency of the sampled graph is used as an attention mask so that only attention scores corresponding to sampled edges are computed. The overall computational cost is linear in the number of edges, which can range from linear to quadratic in sequence length depending on the data and task under concern. Each attention head is equipped with its own underlying SBM, enabling the model to diversify the attention sparsity across heads and layers. By incorporating a straight-through estimator [4] in the discrete graph-sampling step, SBM-Transformer enjoys end-to-end differentiability and can ﬁnd the proper attention sparsity based solely upon minimizing the predictive loss. The model can also easily be further regularized by penalizing the number of sampled edges, which results in a lighter model using less computational resources during inference. To the best of our knowledge, our method is the ﬁrst Transformer architecture that can data-adaptively choose between linear to full attention with respective computational costs. To summarize, our main contributions are as follows: • We present SBM-Transformer, a novel Transformer of which each attention head can adaptively adjust its attention sparsity as well as computational cost based on the input data. • To demonstrate the beneﬁt of this ﬂexibility, we theoretically prove that SBM-Transformer retains universal approximability, and also stress-test the model under a synthetic task where full attention is required to achieve 100% accuracy. • Evaluations on LRA and GLUE benchmarks show that SBM-Transformer outperforms pre- vious efﬁcient Transformer models as well as the vanilla Transformer with dense attention. 2 Related Work In this section we discuss previous efﬁcient Transformer variants and several works similar to ours with respect to adaptively learning sparse attention patterns. We also review several works on SBMs. Efﬁcient Transformers. Many efﬁcient Transformers tackle to reduce the quadratic cost of multi- head attention with different approaches. While we discuss only a handful of representative ap- proaches, a much more comprehensive survey can be found in [39]. The Linear Transformer [21] achieves linear complexity by replacing the softmax with a low-rank kernelized function. Lin- former [45] and Nyströmformer [48] use a similar approach by low-rank approximating the attention score matrix. Performer [10] uses positive orthogonal random features to approximate the softmax kernel. Reformer [22] gathers similar tokens together through locality-sensitive hashing (LSH) and performs attention amongst tokens within the same bucket. Of all methods above, our method is 2most similar to Reformer, in the sense that we adaptively assign queries and keys into clusters and form a low-rank sparse attention pattern. However, our method performs soft-clustering with much less structural constraints, allowing each attention head to represent a wider variety of dependency structure and to adjust its sparsity towards full attention if needed. Adaptive Sparsity. With respect to ﬂexible training between sparse and dense attention, there exist some works that parameterize how sparse the attention pattern should be based on the input. The Adaptive Sparse Transformer [12] proposed replacing the usual softmax activation with α-entmax, in which the α parameter can be differentiably trained to adjust the activation between softmax and sparsemax activation [27]. SparseBERT [ 36] uses a differentiable masking technique where each attention mask is sampled from a Gumbel-sigmoid distribution using data-independent mask probability parameters. While these methods possess the ﬂexibility to adjust between sparse and full attention based on data, they still require full computation of the attention score matrix before sparsiﬁcation, and hence are unable to leverage the learned sparsity towards better model efﬁciency. To the best of our knowledge, ours is the ﬁrst work to be able to adaptively tune its attention sparsity between sparse to full attention without requiring the explicit computation of the attention score matrix, thereby avoiding quadratic cost when possible. Stochastic Block Models. The Stochastic Block Model (SBM) is a generative model that encodes the latent structure of graphs by grouping nodes into clusters. By modeling the cluster-membership of each node as well as inter-cluster relationships, SBMs can represent a wide variety of graph structures, which is a feature especially useful for generating new graphs or predicting missing edges in noisy data [1]. The standard SBM assigns each node to a single cluster, and the probability of an edge between two nodes strictly depends on the corresponding clusters. Several structural extensions include overlapping SBM [24] and mixed-membership SBM [2], which allow each node to be assigned to multiple clusters. The underlying SBM used by our framework mostly resembles these two variants, while the edge probability is modeled by a nonlinear function of two node embeddings rather than a bilinear one. There exist many other extensions including degree-corrected SBM [20] for multi-graphs and hierarchical SBM [31] for multiplex-graphs. Further details can be found in a recent survey [16]. 3 Preliminaries: Sparse Transformers We ﬁrst introduce the full attention mechanism used in the original Transformer [ 40] as well as masked attention which will serve as a backbone of our approach. 3.1 Full Attention In vanilla Transformer [40], each attention head takes a sequence of token features as inputX ∈Rn×d where nis the sequence length and dthe embedding dimension. Weight parameters WQ,WK ∈ Rd×dh and WV ∈Rd×dh with head-dimension dh ﬁrst maps the input features X into query Q, key K, and value V , respectively. Then, the attention score matrix is computed with scaled dot-product of queries and keys followed by row-wise softmax activation σ(·). Note that explicit computation of this matrix is the main bottleneck of full attention, incurring O(n2) asymptotic cost in both time and memory. The value features V are then pooled based on the attention scores, returning the output token representations. Altogether, the operation performed by each attention head can be written as Q = XW Q, K = XW K, V = XW V (1) Attn(X) =σ (QKT √dh ) V . (2) 3.2 Masked Attention One way to remove the quadratic bottleneck from the attention score matrix is to apply a binary mask M ∈{0,1}n×n and compute the scaled dot-products QiKT j /√dh only if Mij = 1. In presence of an attention mask, the operation is modiﬁed to Attnmask(X,M) =σM ( M ⊙QKT √dh ) V (3) σM(A)ij :=    exp(Aij)∑ k∈{k′|Mik′=1}exp(Aik) if Mij = 1 0 otherwise (4) 3𝑸 𝑲 𝑽 ෡𝑸 ෡𝑲 𝑪 ෡𝑺  Attention Mask 𝑴 Attnmask 𝑿𝜕ℒ/𝜕𝑿 Forward Backward Inputs Clusters Query/Key Nodes SBM + STE Output 𝑸,𝑲 Figure 2: An illustration of the attention mechanism in SBM-Transformer. Each head ﬁrst maps queries and keys to the node representation space through a shared MLP. The graph sampling module samples an attention mask from a Stochastic Block Model (SBM) parameterized by the node and cluster embeddings. The discrete sampling step is differentiable via a Straight-Through Estimator (STE). Given the mask, the output is computed via masked attention. where ⊙indicates entry-wise multiplication. Note that the masked-softmax σM(·) operator only computes unmasked terms, ensuring that each (i,j)-th attention score survives as nonzero if and only if Mij = 1. This is thus equivalent to ﬁlling in the (i,j)-th attention score with −∞if Mij = 0, then applying the standard softmax operator. Most sparsity-based efﬁcient Transformers fall under this formulation, while using different methods to either manually ﬁx or learn the mask M. For instance, local attention [9, 3, 51] with a sliding window sets Mij = 1if |i−j|<c for some context window size cwhile Reformer [22] sets Mij = 1if Qi and Kj are hashed into the same bucket. 4 Our Method: SBM-Transformer Here we discuss the details of SBM-Transformer (Figure 2). We ﬁrst illustrate the forward step of our attention module and how the underlying SBM [2] of each head, from which we sample our attention masks, is parameterized by the input tensors. We then discuss how the model enables end-to-end differentiability despite the discrete graph sampling step. 4.1 Forward step with the Stochastic Block Model In our framework, we view the attention mask M as an adjacency matrix of a bipartite graph that connects queries to keys, and let each attention head sample an adjacency matrix that best represents the contextual dependencies amongst input tokens. In order to efﬁciently sample adjacency matrices while avoiding the quadratic cost, the distribution of graphs must ﬁrst be parameterized with a sub-quadratic number of latent variables. Stochastic Block Models ﬁt perfectly for our purpose as it models graphs that are low-rank structured with klatent clusters, allowing full parameterization using O(nk) memory. More concretely, the SBM distribution is deﬁned by two nonnegative node- to-cluster memberships Y ,Z ∈Rn×k + and a so-called block matrix B ∈Rk×k + that stores the inter-cluster connection probabilities. The probability of node i being connected to node j is computed as p(i,j) =YiBZT j . Equivalently, the expectation of the adjacency matrix sampled from A ∼SBM(Y ,B,Z) can be written as E[A] =Y BZT. For proper parameterization of the SBM, we must infer the nonnegative node-memberships and block matrix from the queries and keys. To do so, we equip each attention head a 2-layer MLPdh→dh with ReLU activation, and a set of ktrainable cluster-embeddings C ∈Rk×dh . First, our model computes the block matrix ˆS ∈Rk×k + by taking dot products amongst cluster-embeddings C followed by a 2-dimensional softmax activation. The node embeddings are obtained by processing each query and key through the MLPdh→dh , mapping token representations into the node representation space. The memberships of query and key nodes, which we denote by ˆQ and ˆK, are then inferred by taking dot products of node and cluster embeddings, followed by a sigmoid function. The block matrix ˆS, query node-memberships ˆQ, and key node-memberships ˆK altogether provide a well-deﬁned parameterization for the SBM. Thus, a bipartite graph adjacency M ∈{0,1}n×m can be sampled from M ∼SBM( ˆQ, ˆS, ˆK) with expectation E[M] = ˆQ ˆS ˆKT: the probability of connecting query Qi to key Kj equals p(i,j) = ˆQi ˆS ˆKT j . Formally, the sampling procedure can be written as 4Algorithm 1: fastRG(Y ,B,Z)[35] Input : Y ∈Rn×k + , B ∈Rk×k + , Z ∈Rn×k + Output :M ∈{0,1}n×n with E[M] =Y BZT 1 Compute diagonal matrices DY = (diag(1Y ))−1 and DZ = (diag(1Z))−1 2 Column-normalize Y = Y D−1 Y and Z = ZD−1 Z 3 Compute B = DY BDZ 4 Sample number of edges m∼Poisson(1B1T) 5 Initialize M = 0 6 for i= 1 :mdo 7 Sample (U,V ) from {1,...,k }×{1,...,k }with Pr(U = u,V = v) ∝Buv 8 Sample source I from {1,...,n }with Pr(I = i) =Y iU. 9 Sample destination J from {1,...,n }with Pr(J = j) =ZjV 10 Set MIJ = 1. 11 end ˆS = softmax(CCT) (5) ˆQ = sigmoid(MLPdh→dh (Q)CT) (6) ˆK = sigmoid(MLPdh→dh (K)CT) (7) M ∼SBM( ˆQ, ˆS, ˆK) (8) For the last sampling step, we incorporate a fast random graph sampling algorithm fastRG (Alg. 1, [35]) that can sample graphs from a SBM in time and memory asymptotically linear in the number of edges. One advantage of fastRG is that each edge can be sampled in parallel, allowing high efﬁciency with the help of multiprocessing. A more signiﬁcant feature of the method is that the number of edges, which determines the overall cost, is sampled from a Poisson distribution with input-dependent mean (Line 4). Thus, the model can dynamically adjust its computational cost between linear and quadratic in sequence length based on the data. Figure 3 shows example placements of nodes and clusters on the dh-dimensional space to show how the sparse structure is determined. If all nodes and clusters are gathered closely, then all entries in ˆQ and ˆK become close to 1, resulting in p(i,j) ≈1 for all i,j and hence a dense M. If clusters are well-separated but each surrounded by some set of nodes, ˆS becomes close to diagonal while each row in ˆQ and ˆK is close to a one-hot vector indicating the cluster nearby. Such setting leads to a block diagonal mask similar to LSH bucketing of Reformer [22]. Lastly, if all clusters are far apart from the nodes, both ˆQ and ˆK approximately equal zero, zeroing out all the edge probabilities. 4.2 Backward Step with Straight-Through Estimator The graph sampling procedure is naturally a discrete operation. Thus, naive backpropagation cannot learn the proper parameterization for the SBM that minimizes the predictive loss. To cope with this non-differentiability, we incorporate a Straight-Through Estimator (STE) [ 4] to pass the gradient beyond the discrete sampling step. The STE enables providing the gradient ∂L/∂Mij to the probability for each sampled edge (i,j) (Eqn. 9). It works as if we had used a continuous mask M ⊙E[M] that stores the probability of each sampled edge instead of the binary mask M during forward propagation. This way, the probabilities of sampled edges can be learned end-to-end: the gradients provide information on whether each sampled edge was useful or not for prediction. ∂L ∂pij := ∂L ∂Mij =    ∂L ∂Aij ·QiKT j√dh if Mij = 1 0 otherwise where A := M ⊙QKT √dh (9) 5Query-nodes Key-nodes Clusters ॱሾࡹሿ (b) Block-diagonal(a) Dense (c) Sparse Figure 3: Representative examples from the SBM and resulting mask expectations (darker grid indicates edge probability closer to 1). (a) The expected mask is dense if all nodes and clusters are collapsed within a small region. (b) Clear-cut groups in the embedding space induce a block-diagonal mask. (c) Clusters located far apart from nodes lead to sparse masks. Random Edge Exploration. While this approach enables backpropagation in the same O(m) cost as in the forward step, this comes at the expense of not being able to propagate information through edges that were not sampled. This can be problematic when an edge probability accidentally collapses to zero, after which the edge becomes unlikely to ever be sampled even when it may be useful for the prediction task at hand. Therefore, we add a small perturbation δ >0 to each edge probability pij, allowing the model to explore new edges and resuscitate their sampling probabilities if necessary. We ﬁnd that a δas small as 0.01 signiﬁcantly helps in practice, and thus use this edge exploration scheme during training for our experiments. Wouldn’t the model always prefer full attention? Note that the gradient∂L/∂pij can be positive, which suppresses the probability of edge (i,j). At ﬁrst, it may seem counter-intuitive why the model would ever limit itself to using fewer edges during training without any sparsity-based regularizations. One explanation is that masked attention provides an easy way to reduce attention scores under ﬁnite head dimensions. Under full attention, it is known that the representational space of attention score matrices is limited by the head dimension and softmax activation [ 5]. This limitation inevitably introduces unwanted noise in the attention scores especially when working with long sequences. In SBM-Transformer, however, the structural sparsity in masked attention introduces another dimension that induces a larger space of row-stochastic matrices (full attention is a special case of masked attention where Mij = 1 for all i,j). Therefore, it is reasonable that the model may encourage sparsity to leverage the additional expressiveness assuming the loss landscape has local optima within the sparse attention regime. Our experiments on the LRA benchmark show that this is indeed the case, as our SBM-Transformer converges to an average attention sparsity of 20% to 30% while outperforming Transformer with full attention. We also show in the experiment that we can easily incorporate additional regularization that further encourages sparse attention masks. 4.3 SBM-Transformer is a Universal Approximator Leveraging previous work on the theoretical expressiveness of sparse attention [50, 51], we show that SBM-Transformer with a small modiﬁcation1 retains the same level of expressibility as full attention. Speciﬁcally, we show that the low-rank structure of the underlying SBMs does not degrade the expressive power of Transformer, and that SBM-Transformer can universally approximate arbitrary functions with O(n) connections. For brevity, we provide a rough overview of the proof and defer further details to Appendix A. Theorem 1. Let f ∈F be class of continuous sequence-to-sequence functions. Th,r,m SBM denote the class of SBM-Transformers with hattention heads, mhead dimension, and rdimensions in hidden layers. Then for any ϵ> 0 and 1 ≤p< ∞, there exists a function g∈T h,m,r SBM such that ∫ D ∥f(X) −E[g(X)]∥p pdX ≤ϵ (10) 1Here we consider a variant of SBM-Transformer where self-loops are added manually (i.e. Mii = 1for all i). While this is useful in theoretical analysis, we ﬁnd that not having self-loops slightly helps in empirical performance and hence omit self-loops for the main experiments. 60 250 500 750 1000 1250 1500 1750 2000 Epoch 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Training Loss Full-attn Reformer Performer Nystrom Linearized Linformer SBM(ours) 0 250 500 750 1000 1250 1500 1750 2000 Epoch 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Mask Density Figure 4: Loss (left) and mask density (right) of SBM-Transformer during training on the synthetic task. SBM-Transformer successfully converges to zero loss by tuning itself towards full attention. According to the main theorem of Yun et al. (2020) [ 49], SBM-Transformer achieves universal approximability if 1) each node attends to itself, 2) the aggregation of all attention patterns contains a Hamiltonian path, and 3) there exists a path between all node pairs. While the ﬁrst condition is trivially true due to our modiﬁcation, the other two conditions require careful choice of three SBMs. Here we ﬁrst parameterize one SBM to hard-assign tokens into kequally-sized clusters, inducing a block-diagonal attention pattern. The other two SBMs are parameterized such that the two graphs together form a star graph with kglobal relay tokens. Combining the three attention patterns lead to a parameterization of SBM-Transformer that satisﬁes all three conditions, hence proving the theorem. 5 Experiments For empirical evaluations, we ﬁrst use a synthetic task to show that our model is ﬂexible enough to learn towards full attention when needed in contrast to previous works. We then experiment on Long Range Arena (LRA) [38], a benchmark widely used to assess the capacity of efﬁcient Transformers in learning long-range contexts across different modalities. Lastly, we show results on the GLUE benchmark [43] to assess the performance of SBM-Transformer in a downstream NLP setting. All experiments were run on a remote GCP server equipped with 16 NVIDIA A100 Tensor Core GPUs. 5.1 Synthetic Task: Finding Repeated Tokens Dataset. We formulate a token-level binary classiﬁcation task as follows: each input sequence consists of N integers, each of which is uniformly sampled from {1,2,...,N }. We use N = 256in our setup. The prediction target is a sequence of equal length, where each token is labeled 1 if there exists a duplicate somewhere within the sequence, and 0 otherwise. Below is a simple example with N = 8that illustrates the task. We measure the performance of models via binary cross-entropy loss. Input: 1 4 3 7 3 2 3 1 ⇒Target: 1 0 1 0 1 0 1 1 Methods. For this task, we compare SBM-Transformer with k = 128 clusters against various efﬁcient Transformers: Linear Transformer [21], Linformer [45], Reformer [22], Performer [10], and Nyströmformer [48]. Across all methods, we use a single-layer and single-head architecture with 32 hidden dimensions. Note that due to this constrained setting, the sole head must perform full attention to compare each token to all the others in order to attain 100% accuracy. All models are trained for 2000 epochs where a new batch of sequences is sampled on-the-ﬂy at each epoch. We use a batch size of 256 and learning rate of 1e-3. Results. Figure 4 shows the training loss curves of each baseline method as well as SBM- Transformer. Full attention quickly converges to 100% accuracy, which is expected as it computes all possible pairwise interactions by default. Other models that apply low-rank or kernelized attention fail to achieve the same level of accuracy, due to limited expressibility under the constrained setting. Though SBM-Transformer converges more slowly compared to full-attention, it demonstrates the ability to drive itself towards full-attention, eventually attaining zero loss. 7Model LISTOPS(2K) T EXT(3K) R ETRIEVAL(4K) I MAGE(1K) P ATHFINDER(1K) Avg. Full-attention [40] 37.22 64.93 79.55 40.38 74.26 59.27 Linearized [21] 37.46 64.90 81.10 38.48 74.61 59.31 Reformer [22] 22.92 64.70 77.25 43.65 70.28 55.76 Performer [10] 18.25 65.00 79.01 39.80 70.79 54.57 Linformer [45] 38.44 56.28 78.09 39.53 67.62 55.99 Nyströmformer [48] 37.22 65.46 79.35 43.07 71.97 59.41 SBM-Transformer (ours)37.45 (20.09%)65.79 (26.10%)80.00 (29.46%)41.31 (20.49%)75.12 (18.56%)59.93 Table 1: LRA benchmark results. The sequence lengths are shown next to each task. For SBM- Transformer, we report the average attention sparsity across all layers and heads during test time in parentheses. Bold and underlined results indicate best and 2nd best test accuracy for each task. λ LISTOPS(2K) T EXT(3K) R ETRIEVAL(4K) I MAGE(1K) P ATHFINDER(1K) Avg. 0 37.45 (20.09%) 65.79 (26.10%) 80.00 (29.46%) 41.31 (20.49%) 75.12 (18.56%) 59.93 10−4 37.76 (10.48%) 65.48 (26.26%) 79.93 (24.62%) 41.35 (10.70%) 75.46 (5.16%) 60.00 10−3 38.23 (10.46%) 65.18 (26.03%) 80.00 (21.70%) 41.17 (24.60%) 74.49 (3.82%) 59.81 10−2 38.20 (2.95%) 65.59 (22.43%) 80.44 (6.99%) 42.20 (3.95%) 72.79 (3.76%) 59.84 10−1 37.76 (1.15%) 64.48 (10.62%) 79.46 (2.49%) 41.35 (1.33%) 73.79 (2.61%) 59.37 Table 2: LRA results of SBM-Transformer with increasing sparsity regularization weight λ. Bold results indicate best accuracy for each task and percentage in parentheses indicate average attention density. Sparsity regularization helps in reducing computational cost with small drop in performance. 5.2 Long Range Arena (LRA) To demonstrate that the ﬂexible inductive bias of SBM-Transformer is effective for modeling long- range dependencies, we test SBM-Transformer against previous work on the LRA benchmark. We also test how the performance is affected with respect to applying a sparsity-based regularizer. Dataset. LRA [38] consists of ﬁve different testbeds with varying modalities: LIST OPS [28] is a 10-way classiﬁcation task to map a sequence of single-digit numbers and 4 different set operations, to its corresponding solution. TEXT [26] is a binary classiﬁcation task where byte-level IMDB movie reviews must be classiﬁed into one of positive or negative sentiments. RETRIEVAL [32] is also a char-level binary classiﬁcation task, where two sequences from ACL Anthology papers are given as input, and the model must predict whether there exists a citation link between them. IMAGE [23] is a 10-way classiﬁcation task mapping ﬂattened pixel-sequences from CIFAR-10 to its class. PATHFINDER [25] provides ﬂattened pixel-sequences from an image and the model must decide whether two circles in the image are connected by a dashed line. For this benchmark, we use the PyTorch implementation of LRA provided by the authors of Nyströmformer [48] and adhere to the same train-test splits. Performance in all ﬁve tasks is measured using classiﬁcation accuracy. Methods. We compare SBM-Transformer against the same baselines as with the synthetic task above. For fair comparison, we set all Transformer models to use the default setting used in [ 48], which ﬁxes 2 layers, 2 attention heads, and 64 embedding dimensions. For SBM-Transformer, we use k = 128 clusters. The output token representations are mean-pooled to obtain the sequence representation for all tasks. More details on the architecture setups can be found in Appendix C. Results. Table 8 shows the test accuracies of each method. Our SBM-Transformer achieves the best overall performance, ranking ﬁrst in two tasks, and second in one other. SBM-Transformer also outperforms full attention in all ﬁve tasks while computing 30% or less attention scores on average, which supports our claim that masked attention with partial attention score computations can be preferred over full attention depending on the task. With respect to the attention mask structure, we ﬁnd that ﬂexibility of SBM is indeed beneﬁcial, as Reformer struggles in LIST OPS, most likely due to the inability of block-diagonal masks to model hierarchical contexts. Mask Density Regularization. To test if the model can effectively learn under a constraint on the computational cost, we also test the model under a sparsity-based regularizer that discourages excessive use of query-key edges. We penalize each sampled edge by adding to the predictive 8Relative FLOP Count Relative Peak Memory Usage Model L(2K) T(3K) R(4K) I(1K) P(1K) L(2K) T(3K) R(4K) I(1K) P(1K) Full-attention [40] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Linearized [21] 0.02 0.01 0.02 0.04 0.04 0.18 0.16 0.12 0.42 0.42 Reformer [22] 0.05 0.03 0.05 0.10 0.10 0.39 0.31 0.18 0.72 0.72 Performer [10] 0.18 0.12 0.18 0.36 0.36 0.76 0.70 0.60 0.96 0.96 Linformer [45] 0.33 0.22 0.33 0.66 0.66 0.26 0.22 0.14 0.34 0.34 Nyströmformer [48] 1.09 0.70 1.09 2.37 2.37 0.34 0.27 0.16 0.70 0.70 SBM-Transformer (ours)0.07 0.23 0.08 0.27 0.29 0.19 1.01 0.19 0.39 0.48 Table 3: Per-example relative FLOP count and peak memory usage during LRA inference. ListOps L1 L2 L1 L2 L1 L2 L1 L2 L1 L2 L1 L2 L1 L2 L1 L2 L1 L2 L1 L2 40 30 20 10 0 Mask density (%) Image Pathfinder32Retrieval Text λ = 0 (no reg.) λ > 0 (reg.) Figure 5: Average and standard deviation of density of masks sampled across the test set for each LRA task. The x-axis indicates the lower (L1) and upper (L2) layers and each bar represents the density averaged between the two attention heads in each layer. loss a weighted regularization term λLs, where Ls denotes the average mask density across all attention heads. Table 9 shows the performance of SBM-Transformer across varying regularization weights. Under strong regularization, the model surprisingly retains competitive performance while signiﬁcantly reducing the average mask density. This indicates that similar local optima are shared across regimes with varying attention density in the loss landscape, and the regularization term is able to drive the model towards ﬁnding optimal attention scores with smaller density. Efﬁciency. Furthermore, we compare computational costs during inference by measuring FLOP count and peak memory usage. For SBM-Transformer, we test the model trained under λ= 10−1. Due to lack of support for sparse tensor operations in existing FLOP-counters, we measure FLOP counts by manually enumerating through each tensor operation. Table 3 shows that SBM-Transformer is comparably efﬁcient across all tasks except for TEXT , where SBM-Transformer showed the largest average mask density. Note that while the cost of other baselines are ﬁxed after initialization, the cost of SBM-Transformer is data-adaptive and can vary input-by-input. Further analysis and qualitative examples demonstrating the input-dependent attention mask densities can be found in Appendix C. Layerwise Diversity in Sparsity. We also compare the densities of masks sampled at each layer of SBM-Transformer during test time to examine whether our model is capable of diversifying sparsity across layers for better performance. Recall that this allows models to gather information in different levels, as seen in pretrained BERT where lower layers focus on the overall content via dense attention while upper layers gather syntactic information with tree-like patterns [ 11]. For each of the ﬁve tasks, we pick two highest-performing models (one for unregularized and another for regularized) for measurement. Figure 5 shows the average layer-wise mask densities of unregularized and regularized SBM-Transformers across different tasks. We ﬁnd that under no regularization, the two layers can differ by more than 10% in tasks such as LIST OPS and IMAGE . This may be due to the hierarchical and compositional structure of the two tasks. We also ﬁnd that the variation is relatively low in TEXT with densities around 25%, indicating that the task requires broad attention overall. Lastly, the standard deviation is extremely large in upper layers for PATHFINDER , showing that it samples a wide variety of masks depending on the input. 95.3 General Language Understanding Evaluation (GLUE) To check whether its strong performance demonstrated in LRA extends to the downstream NLP setting as well, we evaluate SBM-Transformer against baselines on the GLUE benchmark [43]. Dataset. We consider four NLP tasks in GLUE [43]. SST-2 [37] consists of movie reviews the model must predict their positive or negative sentiments. For QQP [8], the task is to determine whether one question is a paraphrase of the other given a pair of questions. MNLI [47] consists of sentence pairs, each with a target label indicating whether the two sentences are connected through entailment, contradiction, or neither. QNLI [33] consists of sentence-question pairs and the task is to determine whether the sentence contains an answer to the question. Each task is formulated as sequence classiﬁcation, and we measure performance by F1 score on the respective validation sets. Methods. Following previous work [48], we arrange a small variant of BERT [14] with 4 layers, 8 attention heads, and 512 embedding dimensions. We replace full attention with each attention module used in previous experiments. For SBM-Transformer, we use k = 128 clusters without sparsity regularization (i.e. λ= 0). Here, we ﬁnd that adding local attention signiﬁcantly boosts performance, and thus ﬁx a sliding window of size 64 to SBM-Transformer. We ﬁrst pretrain each model under the masked language modeling objective for 50 epochs on a corpus with text from English Wikipedia, BookCorpus [55], and RealNews [52]. We then ﬁnetune each pretrained model for 5 epochs on the GLUE training sets. More details on the architecture and training setup can be found in Appendix C. Model SST-2 QQP MNLI QNLI Full-attention [40] 89.8 84.7 84.0 85.0 Reformer [22] 89.3 84.4 83.9 84.0 Performer [10] 82.0 65.6 71.4 59.3 Linformer [45] 82.0 83.2 79.3 82.5 Nyströmformer [48]89.7 83.2 84.1 84.9 SBM-Transformer (ours)89.8 85.2 83.5 83.6 Table 4: GLUE benchmark results. Bold results indicate best accuracy for each task. Results. Table 4 reports the F1 scores of each method on different NLP tasks. SBM- Transformer performs competitively against full attention overall, and outperforms all baselines in SST-2 and QQP . We also ﬁnd that the ﬁne-tuned SBM-Transformer models use 13.5% dense attention masks on average across all tasks, showing that the model can encode use- ful information from input sentences effectively under highly sparse attention. 6 Conclusion We propose SBM-Transformer, an efﬁcient Transformer that can data-adaptively choose its attention sparsity between sparse and full attention without the need to explicitly compute the full attention score matrix. Theoretically, we show that our model enjoys the same expressibility as the original Transformer due to the ﬂexibility of the latent SBM. Empirical experiments on LRA and GLUE show that our model performs competitively against previous state-of-the-art efﬁcient Transformers. Nonetheless, there are limitations due to sparse tensor operations being less optimized on GPU kernels. In the LRA experiments, we found that SBM-Transformer can result in longer runtimes compared to dense counterparts while its memory usage is much lower. While previous sparsity-based attention mechanisms with block-sparse attention are much more amenable for GPU computation [51, 9, 3], our work requires an architecture with better workload balancing and acceleration under unstructured sparsity, for which there is ongoing work [46, 54]. We still believe this work is valuable as it is the ﬁrst approach to induce per-example attention sparsity, allowing the model to adjust its computational cost based on the input. The cost being dependent on the number of edges also allows practitioners to easily impose constraints based on the available computational resources. We hope to see more GPU-friendly tensor operations optimized for ﬁne- grained sparsity in the future, at which point the value of this work will increase even further. As we propose a foundational replacement for the scaled dot-product attention module in the Transformer architecture, we do not expect any immediate negative societal impact due to this work. Acknowledgments and Disclosure of Funding We would like to thank Kun Dong for the insightful comments. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00926, 2022-0-00959, 2021-0-02068, and 2019-0-00075). 10References [1] E. Abbe. Community detection and stochastic block models: recent developments. The Journal of Machine Learning Research, 18(1):6446–6531, 2017. [2] E. M. Airoldi, D. Blei, S. Fienberg, and E. Xing. Mixed membership stochastic blockmodels. Advances in neural information processing systems, 21, 2008. [3] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. [4] Y . Bengio, N. Léonard, and A. C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [5] S. Bhojanapalli, C. Yun, A. S. Rawat, S. J. Reddi, and S. Kumar. Low-rank bottleneck in multi-head attention models. CoRR, abs/2002.07028, 2020. [6] B. Bollobás. Random graphs. In Modern graph theory, pages 215–252. Springer, 1998. [7] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. [8] Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora question pairs. 2017. [9] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. [10] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlós, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger, L. J. Colwell, and A. Weller. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. [11] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does BERT look at? an analysis of bert’s attention.CoRR, abs/1906.04341, 2019. [12] G. M. Correia, V . Niculae, and A. F. T. Martins. Adaptively sparse transformers. CoRR, abs/1909.00015, 2019. [13] G. Daras, N. Kitaev, A. Odena, and A. G. Dimakis. SMYRF: efﬁcient attention using asymmetric clustering. CoRR, abs/2010.05315, 2020. [14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [16] T. Funke and T. Becker. Stochastic block models: A comparison of variants and inference methods. PLOS ONE, 14(4):1–40, 04 2019. [17] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. CoRR, abs/1502.01852, 2015. [18] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735– 1780, 1997. [19] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang. Efﬁcient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419–1436, 2021. [20] B. Karrer and M. E. Newman. Stochastic blockmodels and community structure in networks. Physical review E, 83(1):016107, 2011. [21] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. CoRR, abs/2006.16236, 2020. 11[22] N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efﬁcient transformer. CoRR, abs/2001.04451, 2020. [23] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009. [24] P. Latouche, E. Birmelé, and C. Ambroise. Overlapping stochastic block models with application to the french political blogosphere. The Annals of Applied Statistics, 5(1):309–336, 2011. [25] D. Linsley, J. Kim, V . Veerabadran, and T. Serre. Learning long-range spatial dependencies with horizontal gated-recurrent units. CoRR, abs/1805.08315, 2018. [26] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. [27] A. F. T. Martins, M. V . Treviso, A. Farinhas, V . Niculae, M. A. T. Figueiredo, and P. M. Q. Aguiar. Sparse and continuous attention mechanisms. CoRR, abs/2006.07214, 2020. [28] N. Nangia and S. R. Bowman. Listops: A diagnostic dataset for latent tree learning. CoRR, abs/1804.06028, 2018. [29] S. Narang, H. W. Chung, Y . Tay, W. Fedus, T. Févry, M. Matena, K. Malkan, N. Fiedel, N. Shazeer, Z. Lan, Y . Zhou, W. Li, N. Ding, J. Marcus, A. Roberts, and C. Raffel. Do trans- former modiﬁcations transfer across implementations and applications? CoRR, abs/2102.11972, 2021. [30] M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation. InProceedings of the Third Conference on Machine Translation: Research Papers , pages 1–9, Brussels, Belgium, Oct. 2018. Association for Computational Linguistics. [31] T. P. Peixoto. Hierarchical block structures and high-resolution model selection in large networks. Physical Review X, 4(1):011047, 2014. [32] D. R. Radev, P. Muthukrishnan, and V . Qazvinian. The ACL Anthology network. InProceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries (NLPIR4DL), pages 54–61, Suntec City, Singapore, Aug. 2009. Association for Computational Linguistics. [33] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, Nov. 2016. Association for Computational Linguistics. [34] R. M. Rao, J. Liu, R. Verkuil, J. Meier, J. Canny, P. Abbeel, T. Sercu, and A. Rives. Msa transformer. In M. Meila and T. Zhang, editors,Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8844– 8856. PMLR, 18–24 Jul 2021. [35] K. Rohe, J. Tao, X. Han, and N. Binkiewicz. A note on quickly sampling a sparse matrix with low rank expectation. The Journal of Machine Learning Research, 19(1):3040–3052, 2018. [36] H. Shi, J. Gao, X. Ren, H. Xu, X. Liang, Z. Li, and J. T. Kwok. Sparsebert: Rethinking the importance analysis in self-attention. CoRR, abs/2102.12871, 2021. [37] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013. [38] Y . Tay, M. Dehghani, S. Abnar, Y . Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efﬁcient transformers. CoRR, abs/2011.04006, 2020. [39] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler. Efﬁcient transformers: A survey. CoRR, abs/2009.06732, 2020. [40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. [41] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018. accepted as poster. [42] A. J. Walker. An efﬁcient method for generating discrete random variables with general distributions. ACM Trans. Math. Softw., 3(3):253–256, sep 1977. 12[43] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. CoRR, abs/1804.07461, 2018. [44] M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma, L. Yu, Y . Gai, T. Xiao, T. He, G. Karypis, J. Li, and Z. Zhang. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprint arXiv:1909.01315, 2019. [45] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020. [46] Z. Wang. Sparsert: Accelerating unstructured sparsity on gpus for deep learning inference. arXiv preprint arXiv:2008.11849, 2020. [47] A. Williams, N. Nangia, and S. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. [48] Y . Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y . Li, and V . Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. CoRR, abs/2102.03902, 2021. [49] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? CoRR, abs/1912.10077, 2019. [50] C. Yun, Y . Chang, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. O(n) connections are expressive enough: Universal approximability of sparse transformers. CoRR, abs/2006.04862, 2020. [51] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontañón, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed. Big bird: Transformers for longer sequences. CoRR, abs/2007.14062, 2020. [52] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F. Roesner, and Y . Choi. Defending against neural fake news. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [53] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998–3008, 2021. [54] M. Zhu and Y . Xie. Taming unstructured sparsity on gpus via latency-aware optimization. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pages 1–6, 2020. [55] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015. 13A Proof of Theorem 1 Here we provide a detailed proof to show that SBM-Transformer is a universal approximator of arbitrary sequence-to-sequence functions. Note that a trivial solution is to use a dense mask M equal to the all-one matrix with rank 1, in which case SBM-Transformer becomes equivalent to the full attention Transformer [40] that is already known to achieve universal approximability [49]. Instead, we show that there also exists a solution with O(n) connections, leveraging previous analyses under sparse attention by Yun et al. (2020) [50] and Zaheer et al. (2020) [51]. For theoretical analysis, we consider a variant of SBM-Transformer that manually adds self-loops in the bipartite graph such that Mii = 1for all i. While adding in self-loops help towards analyzing expressibility, we ﬁnd that it does not help empirically, and hence omit the modiﬁcation in the our main method during experimentation. A comparison on performance on the LRA benchmark can be found below in Appendix C. Here we restate the necessary conditions from [50]. Let Al i ⊆[n] denote the sparsity pattern of i-th token in the lattention pattern: j ∈Al i if query iattends to key jin the l-th pattern. Then, the main theorem of Yun et al.,(2020) [50] states that as long as the set of psparsity patterns {Al i}p l=1 and the probability mapping ρ(e.g., softmax) of the sparse Transformer model satisfy the two assumptions below, then model achieves universal approximability with ﬁnite number of layers. Assumption 1. The sparsity patterns {Al i}satisfy the following: 1. For all i∈[n] and l∈[p], we have i∈Al i 2. There exists a permutation γ : [n] →[n] such that, for all i∈[n−1], γ(i) ∈∪p l=1Al γ(i+1). 3. There exists a ﬁnite s ∈N such that s = min{u |Su i = [n] for all i ∈[n]}where Su i is deﬁned recursively by S1 i := A1 k and St i := ⋃ j∈A(t−1) mod p+1 i St−1 j . Assumption 2. For any ζ >0 and η ∈(0,1], ∃t> 0 such that, for any column input v satisfying vj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑ j̸=j∗ρ[tv]j ≤η When viewing each attention pattern Al as a directed graph Gl = (V,El) with node set V := [n] and edge set El := {(j,i)|j ∈Al i ∀i,j}, each item in Assumption 1 can be equivalently written as Condition 1: For all directed graphs Gl, each node has a self-loop. Condition 2: The aggregation of all pgraphs G∗:= (V,∪p l=1El) has a Hamiltonian path that spans all nnodes. Condition 3: In a ﬁnite aggregation of sgraphs G∗s := (V,∪s l=1El), there exists a path between all possible pairs of nodes. Because we use the same softmax probability mapping, which is already proven to satisfy Assump- tion 2 in [50], we are left to show that there exists a parameterization of SBM-Transformer such that the expected attention mask patterns together satisfy the three conditions above. To do so, we ﬁrst show that a simple random ER-graph [6] can be expected to have at least one Hamiltonian cycle with expected number of edges linear in the sequence length. Lemma 1. Assume a directed Erd˝ os-Rényi random graphG(n,p) where each directed edge exists with probability p. Then, for any number of nodes n, there exists a probabilitypsuch that the expected number of edges is O(n) and the expected number of Hamiltonian cycles in G(n,p) is greater than or equal to 1. Proof. We start the proof by formulating the expected number of Hamiltonian cycles in G(n,p). Assuming directed edges, there exist (n−1)! permutations, each of which represent different possible Hamiltonian cycles. Say we have (n−1)! random variables {Xi}(n−1)! i where each Xi equals 1 when the corresponding Hamiltonian cycle exists in G, 0 otherwise. By linearity of expectation, the expected number of Hamiltonian cycles equals ∑(n−1)! i=1 E[Xi]. Then, note the probability of Xi = 1 equals pn for all isince we require ndirected edges to exist for each cycle. Therefore, the total expected number of Hamiltonian cycles equals ∑(n−1)! i=1 E[Xi] =pn(n−1)!. 14(a) A1  (b) A2  (c) A3 Figure 6: Three sparsity patterns with n = 16 and k = 4. Grey-colored blocks on the diagonal indicate manually added self-loops. Any other color indicates a cluster. Next, we show that ∑(n−1)! i=1 E[Xi] ≥ 1 if p = f(n) where f(n) = O( 1 n). Starting from∑(n−1)! i=1 E[Xi] =pn(n−1)!, using the inequality n! ≥(n/e)n leads to pn(n−1)! =pn n n! ≥pn n (n e )n Then, setting the RHS equal to 1 leads to pn n (n e )n = 1⇔nln p+ nln n e = lnn⇔ln p= lne n + lnn 1 n ⇔p= e nn 1 n For large n, 1 n dominates n 1 n and thus, the expected number of Hamiltonian cycle is larger than or equal to 1 with expected number of edges n2p= O(n) Lemma 2. There exists a parameterization of SBM-Transformer such that the sparsity patterns induced by the expected attention masks satisfy Assumption 1. Proof. Here we show that a ﬁnite number of attention patterns each representable by the SBM given some number of clusters kachieves the three conditions from Assumption 1. Here we use p = 3 attention patterns together (shown in Figure 6): A1 i = {i}∪ { j : ⌊ik n ⌋ = ⌊jk n ⌋ ∀j ∈[n] } for all i∈[n] A2 i = {i}∪{n−k+ 1,...,n −1,n}for all i∈[n] A3 i = {{i} if i≤n−k [n] if i>n −k Intuitively speaking, A1 clusters all tokens into non-overlapping kclusters, each with size n k, and connects tokens together if they are within the same cluster. The other two patterns A2 and A3 adds kglobal relay tokens for each cluster with edges going from and to all nnodes, respectively. Note that all three patterns are easily representable from separate SBMs. Then, we can show that these three patterns form directed graphs that together satisfy the three required conditions. Condition 1 is easily satisﬁed due to the manually added self-loops in all patterns. Condition 3 is also satisﬁed with s = 3 as we have k global relay tokens in both directions ( A2 and A3), connecting all pairs of tokens indirectly or directly. Lastly, Condition 2 can be satisﬁed by leveraging Lemma 1 and the global krelay tokens: Lemma 1 states that each subgraph induced by each individual cluster in A1 has at least one Hamiltonian cycle with O(n) number of edges in expectation. Then, a global Hamiltonian path can in G∗can be constructed as follows: • Traverse through the ﬁrst induced subgraph using its Hamiltonian cycle in A1, but without going back to the starting node. • Move to the n−k+ 1global relay token via the edge in A2, then move to any node in the second induced subgraph from node n−k+ 1via an edge in A3. • Traverse through the Hamiltonian cycle in the second induced subgraph, and repeat. 15This way, we can construct a global Hamiltonian path that visits all nnodes, and all three conditions are met with O(kn) number of edges in expectation. Combining Lemma 2 together with Theorem 1 of Yun et al. (2020) [50] proves our main theorem below which states that SBM-Transformer is a universal approximator in expectation. Theorem 2. Let f ∈F be class of continuous sequence-to-sequence functions. Let Th,m,r SBM denote the class of SBM-Transformers with hattention heads, mhead dimension, and r dimensions in hidden layers. Then for any ϵ> 0 and 1 ≤p< ∞, there exists a function g∈T h,m,r SBM such that ∫ D ∥f(X) −E[g(X)]∥p pdX ≤ϵ B Asymptotic Cost Analysis Table 5 shows the asymptotic computational cost and memory footprint of each step an attention head takes in SBM-Transformer given a single input. Assuming the number of clusters is signiﬁcantly smaller than the sequence length, we ﬁnd that both time and memory cost is mostly dominated by the computation of ˆQ and ˆK when the sampled graph is sparse (i.e., m= O(n)). Computation Time Memory InputsQ, K, V, andC - O(nd+kd) Node assignmentsˆQand ˆK O(nd2 +nkd) O(nd+kd+nk) Inter-cluster probabilitiesˆS O(k2d) O(k2) Sampling fromfastRG[35] O(m+n)1 O(m+nk+k2) Run GAT [41] with edge-softmax O(md) O(m+nd)2 Total O(md+nd2 +nkd+k2d) O(m+nd+nk+kd+k2) Table 5: Asymptotic costs of individual steps within the attention module of SBM-Transformer. The sequence length, number of edges, number of clusters, and head dimension are denoted as n, m, k, and d, respectively. A comparison of the overall cost of SBM-Transformer with those of other baselines is shown in Table 6. While its complexities most resemble those of Nyströmformer [48] when the sampled graphs are sparse, the cost of SBM-Transformer can exceed those of full-attention when the graph is dense, due to the additional computation in the MLPd→d used to infer node-to-cluster memberships. Model Time Memory Full-attention [40] O(n2d) O(n2 + nd) Linearized [21] O(nd2) O(nd+ d2) Reformer [22] O(nd+ nk(4n/c)2) O(nd+ nk(4n/c)2) Performer [10] O(nkd+ kd2) O(nk+ nd) Linformer [45] O(nkd+ nk) O(nk+ nd) Nyströmformer [48] O(nkd+ nk2 + k3) O(nk+ nd+ kd+ k2) SBM-Transformer (ours) O(md+ nd2 + nkd+ k2d) O(m+ nd+ nk+ kd+ k2) Table 6: Asymptotic computational costs of different attention mechanisms. The kterm denotes different parameters for each model: number of clusters for SBM-Transformer, number of hashing rounds for Reformer [22], number of random features for Performer [ 10], the projection rank for Linformer [45], and the number of landmarks for Nyströmformer [ 48]. The additional cterm in Reformer [22] indicates the number of hashing chunks, set to c= O( 1 n) as default. 2Walker’s Alias Method [42] used to sample nodes in fastRG requires O(m + n log n) operations, but the log n dependency is not visible in general. More information can be found in [35] 4We leverage highly optimized Generalized Sampled-Dense-Dense Matrix Multiplication (GSDDMM) operators provided by the Deep Graph Library [44] that avoids the O(md) memory overhead. 16C Experiments For reproducibility, we list the model and training hyperparameter settings used for each task in Table 7. Note that for SBM-Transformer, we initialize the cluster-embeddings C using the kaiming normal distribution [17], which results in an initial attention density of approximately 25%. Tables 8 and 9 provide the full LRA benchmark results with standard deviations in test-time accuracy and sparsity. As mentioned in the main paper, we ﬁnd that manually ﬁxing the self-loops in the sampled graphs slightly deteriorates performance, while it helps in proving theoretical expressibility. Parameter SYNTHETIC LISTOPS TEXT RETRIEVAL IMAGE PATHFINDER BERT GLUE # of layers 1 2 2 2 2 2 4 4 # of heads 1 2 2 2 2 2 8 8 Embedding dim. 32 64 64 64 64 64 512 512 Hidden dim. 32 128 128 128 128 128 2048 2048 Head dim. 32 32 32 32 32 32 64 64 Sequence len. 256 2048 3072 4096 1024 1024 512 512 Dropout 0.0 0.1 0.1 0.1 0.1 0.1 0.1 0.1 Attn. dropout 0.0 0.1 0.1 0.1 0.1 0.1 0.1 0.1 Pooling mode N/A MEAN MEAN MEAN MEAN MEAN N/A MEAN # of classes 2 10 2 2 10 2 50265 2 or 3 Batch size 256 128 128 32 1024 1024 256 32 Learning rate 1e-3 5e-4 5e-4 1e-4 5e-4 5e-4 1e-4 3e-5 # of training epochs 2000 5000 20000 30000 35000 62400 50 5 Table 7: Hyperparameter settings used synthetic, LRA, and GLUE experiments. For methods other than full attention [ 40], we use 128 clusters for SBM-Transformer, 2 hashing rounds for Reformer [22], 256 landmarks for Nyströmformer [48], and 256 dimensions for Linformer [45] and Performer [10]. Model LISTOPS(2K) T EXT(3K) R ETRIEVAL(4K) I MAGE(1K) P ATHFINDER(1K) Avg. Full-attention [40]37.22±0.52 64.93 ±0.46 79.55 ±1.22 40.38 ±0.76 74.26 ±0.57 59.27±0.44 Linearized [21] 37.46±0.57 64.90 ±0.49 81.10±0.16 38.48±0.57 74.61 ±1.26 59.31±0.15Reformer [22] 22.92±0.41 64.70 ±0.12 77.25 ±0.15 43.65±0.16 70.28±1.45 55.76±0.29Performer [10] 18.25±0.12 65.00 ±0.50 79.01 ±1.66 39.80 ±0.46 70.79 ±1.26 54.57±0.55Linformer [45] 38.44±0.14 56.28±1.06 78.09 ±0.12 39.53 ±0.57 67.62 ±0.65 55.99±0.14Nyströmformer [48]37.22±0.51 65.46 ±0.40 79.35±0.40 43.07 ±0.42 71.97±1.30 59.41±0.12 SBM-Transformer (+I) 37.60±0.38 64.09±1.39 79.74 ±0.27 40.64 ±0.72 74.93 ±0.32 59.40±0.20(24.64±2.49%) (25.64±0.64%) (24.26±5.21%) (24.54±3.98%) (23.84±3.59%) SBM-Transformer (+0) 37.45±0.44 65.79±0.27 80.00±0.21 41.31±0.35 75.12±0.49 59.93±0.35(20.09±15.71%)(26.10±0.01%)(29.46±3.84%)(20.49±11.43%)(18.56±0.52%) Table 8: LRA benchmark results. Bold and underlined results indicate best and 2nd best test accuracy for each task, respectively. Numbers enclosed in parentheses for SBM-Transformer indicate the density of graphs sampled during test time averaged across all attention heads. For the SBM- Transformer models, (+I) indicates that self-loops are manually ﬁxed while (+0) indicates model without the modiﬁcation. λ LISTOPS(2K) T EXT(3K) R ETRIEVAL(4K) I MAGE(1K) P ATHFINDER(1K) Avg. 0 37.45±0.44 65.79±0.27 80.00±0.21 41.31±0.35 75.12 ±0.49 59.93±0.35(20.09±15.71%) (26.10±0.01%) (29.46±3.84%) (20.49±11.43%) (18.56±0.52%) 10−4 37.76±0.60 65.48 ±0.86 79.93 ±0.16 41.35 ±0.35 75.46±0.46 60.00±0.36(10.48±7.58%) (26.26±0.53%) (24.62±3.19%) (10.70±8.49%) (5.16±1.17%) 10−3 38.23±0.63 65.18±0.46 80.00 ±0.99 41.17 ±0.53 74.49 ±0.74 59.81±0.48(10.46±7.26%) (26.03±0.06%) (21.70±2.68%) (24.60±8.61%) (3.82 ±0.52%) 10−2 38.20±0.29 65.59±0.24 80.44±1.24 42.20 ±0.64 72.79±0.80 59.84±0.42(2.95±0.88%) (22.43±1.73%) (6.99±2.28%) (3.95 ±0.68%) (3.76±0.27%) 10−1 37.76±0.83 64.48 ±0.58 79.46 ±0.47 41.35 ±0.40 73.79 ±0.07 59.37±0.37(1.15±0.15%) (10.62±2.74%) (2.49 ±0.58%) (1.33 ±0.37%) (2.61 ±0.22%) Table 9: LRA benchmark results of SBM-Transformer with increasing density regularization weight λ. Applying a density regularizer helps in encouraging sparser attention patterns which induce less computational cost, while retaining competitive performance. 17True  0.0266  0.0126  0.0219  0.0183  True  0.0255  0.0155  0.0131  0.0317  True  0.0267  0.0186  0.0206  0.0340 False  0.0306  0.0159  0.0383  0.0185  True  0.0275  0.0172  0.0334  0.0256  False  0.0265  0.0149  0.0294  0.0329 False  0.0313  0.0192  0.0217  0.0328  True  0.0213  0.0155  0.0196  0.0488  True  0.0347  0.0174  0.0316  0.0216 False  0.0254  0.0182  0.0233  0.0458  False  0.0244  0.0208  0.0158  0.0520  False  0.0234  0.0174  0.0235  0.0522 True  0.0283  0.0187  0.0129  0.0613  False  0.0306  0.0173  0.0222  0.0552  False  0.0220  0.0164  0.0337  0.0535 (a) Examples with low attention density False  0.0307  0.0162  0.2122  0.1157  False  0.0348  0.0180  0.2483  0.0763  True  0.0465  0.0212  0.2221  0.0877 True  0.0267  0.0170  0.2561  0.0779  False  0.0338  0.0192  0.2021  0.1230  True  0.0411  0.0171  0.2285  0.0958 False  0.0382  0.0191  0.2095  0.1192  True  0.0373  0.0189  0.2522  0.0929  False  0.0385  0.0209  0.2483  0.0979 False  0.0326  0.0189  0.2276  0.1328  False  0.0346  0.0186  0.2648  0.0964  False  0.0418  0.0191  0.3057  0.0510 True  0.0397  0.0185  0.2263  0.1346  True  0.0401  0.0205  0.3284  0.0320  False  0.0407  0.0199  0.3402  0.0562 (b) Examples with high attention density Figure 7: Attention density plots within individual attention heads given inputs from the LRA PATHFINDER test set. All examples shown are from a subset of the test set that the model has predicted correctly. For each set of 5 images, the leftmost image shows the original input image of which the title shows the ground-truth label. To its right are attention density plots from two heads of the ﬁrst layer followed by those from two heads of the second layer. Above each plot is the actual numeric attention density between 0 and 1. The color in each pixel indicates how many other pixels attend to that particular pixel (a color closer to bright yellow indicates more attention). Lastly, we qualitatively analyze which inputs lead to sparse or dense attention in SBM-Transformer. For easy visualization of attention densities, we choose two image-based tasks in LRA, PATHFINDER and IMAGE . We pick two model checkpoints that performed best on each of the two tasks under graph density regularization, one trained with λ= 10−4 for PATHFINDER and another trained with λ = 10−2 for IMAGE , and run predictions on the respective test sets. Figures 7 and 8 show the head-wise attention densities per input at different levels. In Figure 7, the second layer shows large variance in attention density across different PATHFINDER inputs, while the ﬁrst layer remains sparse overall. With some exceptions, we ﬁnd that the attention density of this layer is somewhat correlated with the difﬁculty of each input. Figure 7a shows visually easy inputs with near-perpendicular intersections or no intersection at all, allowing correct predictions with less than 5% average attention density. On the other hand, Figure 7b shows examples with harder difﬁculty, due to having more lines and convoluted intersections. We can see that the model uses much denser attention in such cases, and thus conjecture that the model is adaptively choosing to look at more pixel-to-pixel interactions in response to the complexity of the input. Figure 8 also shows a clear distinction between images that induce different levels of attention density. Under regularization, the ﬁrst layer of SBM-Transformer focuses attention onto dark areas in the image as shown in Figure 8b, using the contrast in the image for better prediction. When the image has high overall intensity as in Figure 8a, however, the model uses less than 3% attention on average, focusing most of the prediction onto the skip-connections, FFNs, and a small number of pixel-to-pixel interactions. Considering that this model achieves a competitive 42.20% accuracy, this shows that SBM-Transformer can well balance the tradeoff between computational cost vs. performance, further supporting the power of our adaptively sparse attention module. 18Ship  0.0384  0.0131  0.0342  0.0251  Deer  0.0322  0.0133  0.0460  0.0237  Dog  0.0353  0.0129  0.0390  0.0289 Airplane  0.0274  0.0125  0.0460  0.0306  Deer  0.0325  0.0131  0.0475  0.0255  Bird  0.0337  0.0126  0.0460  0.0263 Bird  0.0282  0.0123  0.0491  0.0292  Airplane  0.0352  0.0126  0.0454  0.0258  Deer  0.0303  0.0127  0.0479  0.0283 Ship  0.0308  0.0127  0.0476  0.0287  Ship  0.0403  0.0135  0.0376  0.0288  Airplane  0.0400  0.0129  0.0419  0.0258 Bird  0.0323  0.0131  0.0485  0.0267  Ship  0.0366  0.0134  0.0451  0.0263  Cat  0.0362  0.0138  0.0436  0.0279 (a) Examples with low attention density Ship  0.1171  0.0197  0.0545  0.0351  Dog  0.1241  0.0180  0.0528  0.0317  Frog  0.1279  0.0194  0.0485  0.0331 Frog  0.1274  0.0197  0.0486  0.0337  Cat  0.1226  0.0202  0.0576  0.0345  Automobile  0.1299  0.0187  0.0537  0.0328 Frog  0.1336  0.0197  0.0523  0.0327  Automobile  0.1333  0.0194  0.0526  0.0350  Cat  0.1443  0.0175  0.0488  0.0328 Automobile  0.1375  0.0191  0.0547  0.0335  Dog  0.1496  0.0183  0.0501  0.0323  Ship  0.1472  0.0184  0.0545  0.0331 Cat  0.1451  0.0202  0.0523  0.0355  Frog  0.1794  0.0213  0.0475  0.0321  Dog  0.1856  0.0191  0.0494  0.0337 (b) Examples with high attention density Figure 8: Similar visualization as Figure 7 for the LRA I MAGE test set. 19",
      "meta_data": {
        "arxiv_id": "2210.15541v1",
        "authors": [
          "Sungjun Cho",
          "Seonwoo Min",
          "Jinwoo Kim",
          "Moontae Lee",
          "Honglak Lee",
          "Seunghoon Hong"
        ],
        "published_date": "2022-10-27T15:30:52Z",
        "pdf_url": "https://arxiv.org/pdf/2210.15541v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces SBM-Transformer to address the quadratic cost and lack of data adaptability in existing sparse attention mechanisms. It proposes endowing each attention head with a mixed-membership Stochastic Block Model (SBM) to data-adaptively sample bipartite graphs, which serve as attention masks. Key contributions include demonstrating linear computational cost (forward and backward), proving universal approximability in expectation, and achieving superior empirical performance over previous efficient Transformer variants and the original Transformer on LRA and GLUE benchmarks. The model can flexibly adjust its attention sparsity based on the input data.",
        "methodology": "Each attention head in SBM-Transformer is equipped with a mixed-membership Stochastic Block Model (SBM). The model maps queries and keys to a node representation space via a shared 2-layer MLP (dh->dh with ReLU) and uses trainable cluster-embeddings (C) to compute a block matrix (softmax of CCT) and node-to-cluster memberships (sigmoid of MLP output dot C^T). A bipartite graph adjacency matrix, serving as the attention mask, is then sampled from this SBM using the `fastRG` algorithm, which samples edges in parallel and dynamically adjusts the number of edges. To enable end-to-end differentiability despite the discrete sampling, a Straight-Through Estimator (STE) is employed for backpropagation, allowing gradients to flow to edge probabilities. A small perturbation (delta > 0) is added to edge probabilities for random edge exploration.",
        "experimental_setup": "The model was evaluated on a synthetic task (finding repeated tokens, N=256, binary classification, 2000 epochs), the Long Range Arena (LRA) benchmark (LISTOPS, TEXT, RETRIEVAL, IMAGE, PATHFINDER tasks with sequence lengths from 1K to 4K), and the GLUE benchmark (SST-2, QQP, MNLI, QNLI NLP tasks). For LRA, models used 2 layers, 2 attention heads, 64 embedding dimensions, and SBM-Transformer used 128 clusters. For GLUE, a BERT variant with 4 layers, 8 heads, 512 embedding dimensions was used, with SBM-Transformer also using 128 clusters and incorporating a local attention window of 64. Pretraining for GLUE was done for 50 epochs on English Wikipedia, BookCorpus, and RealNews, followed by 5 epochs of finetuning. Performance was measured by binary cross-entropy for the synthetic task, classification accuracy for LRA, and F1 score for GLUE. All experiments were run on a remote GCP server with 16 NVIDIA A100 Tensor Core GPUs.",
        "limitations": "The primary limitation noted is that sparse tensor operations are less optimized on current GPU kernels, which can lead to longer runtimes for SBM-Transformer compared to dense attention counterparts, despite its lower memory usage. The work requires future advancements in GPU computation for better workload balancing and acceleration under unstructured sparsity.",
        "future_research_directions": "Future research should focus on developing more GPU-friendly tensor operations optimized for fine-grained sparsity. The ability of SBM-Transformer to adjust computational cost based on the number of sampled edges also opens possibilities for practitioners to easily impose constraints based on available computational resources."
      }
    },
    {
      "title": "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening",
      "abstract": "Subgraph GNNs enhance message-passing GNNs expressivity by representing\ngraphs as sets of subgraphs, demonstrating impressive performance across\nvarious tasks. However, their scalability is hindered by the need to process\nlarge numbers of subgraphs. While previous approaches attempted to generate\nsmaller subsets of subgraphs through random or learnable sampling, these\nmethods often yielded suboptimal selections or were limited to small subset\nsizes, ultimately compromising their effectiveness. This paper introduces a new\nSubgraph GNN framework to address these issues. Our approach diverges from most\nprevious methods by associating subgraphs with node clusters rather than with\nindividual nodes. We show that the resulting collection of subgraphs can be\nviewed as the product of coarsened and original graphs, unveiling a new\nconnectivity structure on which we perform generalized message passing.\n  Crucially, controlling the coarsening function enables meaningful selection\nof any number of subgraphs. In addition, we reveal novel permutation symmetries\nin the resulting node feature tensor, characterize associated linear\nequivariant layers, and integrate them into our Subgraph GNN. We also introduce\nnovel node marking strategies and provide a theoretical analysis of their\nexpressive power and other key aspects of our approach. Extensive experiments\non multiple graph learning benchmarks demonstrate that our method is\nsignificantly more flexible than previous approaches, as it can seamlessly\nhandle any number of subgraphs, while consistently outperforming baseline\napproaches. Our code is available at\nhttps://github.com/BarSGuy/Efficient-Subgraph-GNNs.",
      "full_text": "arXiv:2406.09291v4  [cs.LG]  29 May 2025 A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening Guy Bar-Shalom∗ Computer Science Technion - Israel Institute of Technology guy.b@campus.technion.ac.il Yam Eitan∗ Electrical & Computer Engineering Technion - Israel Institute of Technology yameitan1997@gmail.com Fabrizio Frasca Electrical & Computer Engineering Technion - Israel Institute of Technology fabrizio.frasca.effe@gmail.com Haggai Maron Electrical & Computer Engineering Technion - Israel Institute of Technology NVIDIA Research haggaimaron@gmail.com Abstract Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromis- ing their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. Our approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing. Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN. We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches. Our code is available at https://github.com/BarSGuy/Efficient-Subgraph-GNNs. 1 Introduction Subgraph GNNs [ 4, 12, 40, 8, 28, 30, 39, 3] have recently emerged as a promising direction in graph neural network research, addressing the expressiveness limitations of Message Passing Neural Networks (MPNNs) [24, 36, 25]. In essence, a Subgraph GNN operates on a graph by transforming it into a collection of subgraphs, generated based on a specific selection policy. Examples of such policies include removing a single node from the original graph or simply marking a node without changing the graph’s original connectivity [27]. The model then processes these subgraphs using an equivariant architecture, aggregates the derived representations, and makes graph- or node- level predictions. The growing popularity of Subgraph GNNs stems not only from their enhanced ∗Equal contribution. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).expressive capabilities over MPNNs but also from their impressive empirical results, as notably demonstrated on well-known molecular benchmarks [39, 12, 3]. Unfortunately, Subgraph GNNs are hindered by substantial computational costs as they necessitate message-passing operations across all subgraphs within the bag. Typically, the number of subgraphs is the number of nodes in the graph, n— for bounded degree graphs, this results in a time complexity scaling quadratically ( O(n2)), in contrast to the linear complexity of a standard MPNN. This significant computational burden makes Subgraph GNNs impractical for large graphs, hindering their applicability to important tasks and widely used datasets. To overcome this challenge, various studies have explored methodologies that process only a subset of subgraphs from the bag. These methods range from simple random sampling techniques [8, 4, 41, 3] to more advanced strategies that learn to select the most relevant subset of the bag to process [5, 20, 30]. However, while random sampling of subgraphs yields subpar performance, more sophisticated learnable selection strategies also have significant limitations. Primarily, they rely on training-time discrete sampling which complicates the optimization process, as evidenced by the high number of epochs required to train them [ 20, 5, 30]. As a result, these methods often allow only a very small bag size, yielding only modest performance improvements compared to random sampling and standard MPNNs. Our approach. The goal of this paper is to devise a Subgraph GNN architecture that can flexibly generate and process variable-sized bags, and deliver strong experimental results while sidestepping intricate and lengthy training protocols. Specifically, our approach aims to overcome the common limitation of restricting usage to a very small set of subgraphs. {a, b,c, d}a{e}a{f}a {a, b,c, d}b{e}b{f}b {a, b,c, d}c{e}c{f}c {a, b,c, d}d{e}d{f}d {a, b,c, d}e{e}e{f}e {a, b,c, d}f{e}f{f}fbcdea f{e}{f} {a, b,c, d} ={a, b,c, d}#bcdea f !\"(!){e}{f} {a, b,c, d}bcdea f % %! {e}{f} !(#)# #!(#) !(#)# Figure 1: Product graph construction. Left: Transforming of the graph into a coarse graph; Right: Cartesian product of the coars- ened graph with the original graph. The vertical axis corresponds to the subgraph dimension (super-nodes), while the horizontal axis corresponds to the node dimension (nodes). Our proposed method builds upon and extends an observa- tion made by Bar-Shalom et al. [3], who draw an analogy be- tween using Subgraph GNNs and performing message-passing operations over a larger “prod- uct graph”. Specifically, it was shown that when considering the maximally expressive (node- based) Subgraph GNN suggested by [39]2, the bag of subgraphs and its update rules can be ob- tained by transforming a graph through the graph cartesian product of the original graph with itself, i.e., G□G, and then processing the resulting graph using a standard MPNN. In our approach, we propose to modify the first term of the product and replace it with a coarsened version of the original graph, denoted T (G), obtained by mapping nodes to super-nodes (e.g., by applying graph clustering, see Figure 1(left)), making the resulting product graph T (G)□G significantly smaller. This construction is illustrated in Fig- ure 1(right). This process effectively associates each subgraph – a row in Figure 1(right) – with a set of nodes produced by the coarsening function T . Different choices of T allow for both flexible bag sizes and a simple, meaningful selection of the subgraphs. While performing message passing on T (G)□G serves as the core update rule in our architecture, we augment our message passing operations with another set of operations derived from the symmetry structure of the resulting node feature tensor, which we call symmetry-based updates. Specifically, our node feature tensor is indexed by pairs (S, v) where S is a super-node and v is an original node. Accordingly, X is a T × n × d tensor, where d is the feature dimension, and T is the number of super-nodes (a constant hyper-parameter). As super-nodes are sets of nodes, X can also be viewed as a (very) sparse 2n × n × d tensor where 2n is the number of all subsets of the vertex set. Since the symmetric group Sn acts naturally on this representation, we use it to develop symmetry based updates. Interestingly, we find that this node feature tensor, X, adheres to a specific set of symmetries, which, to the best of our knowledge, is yet unstudied in the context of machine learning: applying 2The architecture suggested in [39] was shown to be at least as expressive as all previously studied node-based Subgraph GNNs 2a permutation σ ∈ Sn to the nodes in S and to v results in an equivalent representation of our node feature tensor. We formally define the symmetries of this object and characterize all the affine equivariant operations in this space. We incorporate these operations into our message-passing by encoding the parameter-sharing schemes [31] as additional edge features. These additional update rules significantly improve experimental results. We note that our symmetry analysis may be useful for processing bags derived from other high-order generation policies [30, 20] by treating tuples of nodes as sets. Inspired by these symmetries and traditional binary-based [ 4] and shortest path-based [ 39] node- marking strategies, we propose four natural marking strategies for our framework. Interestingly, unlike the full-bag scenario, they vary in expressiveness, with the shortest path-based technique being the most expressive. Bag size Zinc12k (MAE) ↑ ↑ Full bag settingSmall bagsetting Figure 2: The performance landscape of Subgraph GNNs with varying number of subgraphs: Our method leads in the lower bag-size set, outperforming other approaches in nearly all cases. Additionally, our method matches the performance of state-of-the- art Subgraph GNNs in the full-bag setting. The full mean absolute error (MAE) scores along with standard deviations are available in Table 9 in the appendix. The flexibility and effectiveness of our full framework are illus- trated in Figure 2, depicting de- tailed experimental results on the popular ZINC -12 K dataset [32]. Our method demonstrates a sig- nificant performance boost over baseline models in the small bag setting (for which they are de- signed), while achieving results that compare favourably to state- of-the-art Subgraph GNNs in the full bag setting. Additionally, we can obtain results in-between these two regimes. Contributions. The main con- tributions of this paper are: (1) the development of a novel, flexi- ble Subgraph GNN framework that enables meaningful con- struction and processing of bags of subgraphs of any size; (2) a characterization of all affine invariant/equivariant layers defined on our node feature tensors; (3) a theoretical analysis of our framework, including the expressivity benefits of our node-marking strategy; and (4) a comprehensive experimental evaluation demonstrating the advantages of the new approach across both small and large bag sizes, achieving state-of-the-art results, often by a significant margin. 2 Related work Original GraphSubgraphs s vv Original Graph Subgraphs s v v Subgraph GNNs. Subgraph GNNs [40, 8, 28, 4, 41, 27, 12, 30, 17, 39, 3] rep- resent a graph as a collection of subgraphs, obtained by a predefined generation policy. For example, each subgraph can be generated by marking exactly one node in the original graph (see inset 3) – an approach commonly referred to as node marking [27]; this marked node is considered the root node in its subgraph. Several recent papers focused on scaling these methods to larger graphs, starting with basic random selection of subgraphs from the bag, and extending beyond with more sophisticated techniques that aim to learn how to select subgraphs. To elaborate, [5] introduced Policy-Learn (PL), an approach based on two models, where the first model predicts a distribution over the nodes of the original graph, and the second model processes bags of subgraphs sampled from this distribution. MAG-GNN [20] employs a similar approach utilizing Reinforcement Learning. Similarly to our approach, this method permits high-order policies by associating subgraphs with tuples rather than individual nodes, allowing for the marking of several nodes within a subgraph. 3The Figure was taken with permission from [3] 3However, as mentioned before, these approaches involve discrete sampling while training, making them very hard to train (1000-4000 epochs vs. ∼400 epochs of state-of-the-art methods [3, 39] on the ZINC -12 K dataset), and limiting their usage to very small bags. Finally, we mention another high-order method, OSAN, introduced by [30], which learns a distribution over tuples that represent subgraphs with multiple node markings. In contrast to these previous approaches, we suggest a simpler and more effective way to select subgraphs and also show how to leverage the resulting symmetry structure to augment our message-passing operations. Symmetries in graph learning. Many previous works have analyzed and utilized the symmetry structure that arises from graph learning setups [22, 23, 18, 2]. Specifically relevant to our paper is the work of [22] that characterized basic equivariant linear layers for graphs, the work of [ 1] that characterizes equivariant maps for many other types of incidence tensors that arise in graph learning, and the works [4, 12] that leveraged group symmetries for designing Subgraph GNNs in a principled way. 3 Preliminaries Notation. Let G be a family of undirected graphs, and consider a graph G = (V, E) within this family. The adjacency matrix A ∈ Rn×n defines the connectivity of the graph4, while the feature matrix X ∈ Rn×d represents the node features. Here, V and E represent the sets of nodes and edges, respectively, with |V | = n indicating the number of nodes. We use the notation v1 ∼A v2 to denote that v1 and v2 are neighboring nodes according to the adjacency A. Additionally, we define [n] := {1, 2, . . . n}, and P([n]) as the power set of [n]. Subgraph GNNs as graph products. In a recent work, [3] demonstrated that various types of update rules used by current Subgraph GNNs can be simulated by employing the Cartesian graph product between the original graph and another graph, and running standard message passing over that newly constructed product graph. Formally, the cartesian product of two graphs G1 (n1 nodes) and G2 (n2 nodes), denoted by G1□G2, forms a graph with vertex set V (G1) × V (G2). Two vertices (u1, u2) and (v1, v2) are adjacent if either u1 = v1 and u2 is adjacent to v2 in G2, or u2 = v2 and u1 is adjacent to v1 in G1. We denote by A ∈Rn1·n2×n1·n2 and X ∈Rn1·n2×d the adjacency and node feature matrices of the product graph; in general, we use calligraphic letters to denote the adjacency and feature matrices of product graphs, while capital English letters are used for those of the original graphs. In particular, for the graph cartesian product, G1□G2, the following holds: AG1□G2 = A1 ⊗ I + I ⊗ A2. (1) For a detailed definition of the cartesian product of graphs, please refer to Definition A.1. As a concrete example for the analogy between Subgraph GNNs and the Cartesian product of graphs, we refer to a result by [3], which states that the maximally expressive node-based Subgraph GNN architecture GNN-SSWL+ [39], can be simulated by an MPNN on the Cartesian product of the original graph with itself, denoted as G□G. As we shall see, our framework utilizes a cartesian product of the original graph and a coarsened version of it, as illustrated in Figure 1 (right). Equivariance. A function L : U → W is called equivariant if it commutes with the group action. More formally, given a group element, g ∈ G, the function L should satisfy L(g · v) = g · L(v) for all v ∈ U and g ∈ G. L is said to be invariant if L(g · v) = L(v). 4 Coarsening-based Subgraph GNN Overview. This section introduces the Coarsening-based Subgraph GNN (CS-GNN) framework. The main idea is to select and process subgraphs in a principled and flexible manner through the following approach: (1) coarsen the original graph via a coarsening function, T – see Figure 1(left); (2) Obtain the product graph – Figure 1(right) defined by the combination of two adjacencies, AT (G) (red edges), AG (grey edges), which arise from the graph Cartesian product operation (details follow); (3) leveraging the symmetry of this product graph to develop symmetry-based updates, described by AEquiv (this part is not visualized in Figure 1). The general update of our suggested layer takes the following form 4, 4Edge features are also allowed but are omitted here for simplicity 4Xt+1(S, v) = ft \u0010 X(S, v)t, (2) { {X(S′, v′)t} }(S′,v′)∼AG(S,v) | {z } Original connectivity (horizontal) , { {X(S′, v′)t} }(S′,v′)∼AT (G) (S,v) | {z } Induced connectivity (vertical) , { {X(S′, v′)t} }(S′,v′)∼AEquiv (S,v) | {z } Symmetry-based updates \u0011 , where the superscript t indicates the layer index. In what follows, we further elaborate on these three steps (in Sections 4.1 to 4.2). We note that each connectivity in Equation (2) is processed using a distinct MPNN, and after stacking of those layers, we apply a pooling layer5 to obtain a graph representation; that is, ρ(XT) = MLPT \u0010P S \u0010Pn v=1 XT(S, v) \u0011\u0011 ; T denotes the final layer. For more specific implementation details, we refer to Appendix F. 4.1 Construction of the coarse product graph As mentioned before, a maximally expressive node-based Subgraph GNN can be realized via the Cartesian product of the original graph with itself G□G. In this work, we extend this concept by allowing the left operand in the product to be the coarsened version ofG, denoted as T (G), as defined next. This idea is illustrated in Figure 1. Graph coarsening. Consider a graph G = (V, E) with n nodes and an adjacency matrix A. Graph coarsening is defined by the function T : G → G, which maps G to a new graph T (G) = (V T , ET ) with an adjacency matrix AT ∈ R2n×2n and a feature matrix XT ∈ R2n×d. Here, V T , the vertex set of the new graph represents super-nodes – defined as subsets of [n] . Additionally, we require that nodes in V T induce a partition over the nodes of the original graph6. The connectivity ET is extremely sparse and induced from the original graph’s connectivity via the following rule: AT (S1, S2) = \u001a1 if ∃v ∈ S1, ∃u ∈ S2 s.t. A(v, u) = 1, 0 otherwise, (3) To clarify, in our running example (Figure 1), it holds that AT ({a, b, c, d}, {e}) = 1 , while AT ({a, b, c, d}, {f}) = 0. For a more formal definition, refer to Definition A.3. More specifically, our implementation of the graph coarsening function T employs spectral cluster- ing7 [34] to partition the graph into T clusters, which in our framework controls the size of the bag. This results in a coarsened graph with fewer nodes and edges than G. We highlight and stress that the space complexity of this sparse graph, T (G), is upper bounded by that of the original graph G (we do not store 2n nodes). Defining the (coarse) product graph T (G)□G. We define the connectivity of the product graph, see Figure 1(right), by applying the cartesian product between the coarsened graph, T (G), and the original graph, G. The product graph is denoted by T (G)□G, and is represented by the matrices AT (G)□G ∈ R(2n×n)×(2n×n) and X ∈R2n×n×d8, where by recalling Equation (1), we obtain, AT (G)□G = ≜AT (G) z }| { AT ⊗ I + ≜AG z }| { I ⊗ A . (4) The connectivity in this product graph induces the horizontal (AG) and vertical updates (AT (G)) in Equation (2), visualized in Figure 1(right) via grey and red edges, respectively. 5For some of the theoretical analysis, this pooling operation is expressed as: ρ(XT) = MLPT \u0000P S \u0000 MLPT\u0000Pn v=1 XT(S, v) \u0001\u0001\u0001 6Our method also supports the case of which it is not a partition. 7Other graph coarsening or clustering algorithms can be readily used as well. 8We note that while the node matrix of the product graph,X, can be initialized in various ways, e.g., deep sets- based architecture [38], in our implementation we simply use the original node features, i.e., X(S, v) =X(v), given that X is the node feature matrix of the original graph. 54.2 Symmetry-based updates In the previous subsection, we used a combination of a coarsening function and the graph Cartesian product to derive the two induced connectivities AG, AT (G) of our product graph. We use these connectivities to to perform message-passing on our product graph (see Equation (2)). Inspired by recent literature on Subgraph GNNs [ 12, 3, 39], which incorporates and analyzes additional non-local updates arising from various symmetries (e.g., updating a node’s representation via all nodes in its subgraphs), this section aims to identify potential new updates that can be utilized over our product graph. To that end, we study the symmetry structure of the node feature tensor in our product graph, X(S, v).The new updates described below will result in the third term in Equation (2), dubbed Symmetry-based updates (AEquiv). For better clarity in this derivation, we change the notation from nodes (v) to indices (i). 4.2.1 Symmetries of our product graph Since the order of nodes in the original graphG is arbitrary, each layer in our architecture must exhibit equivariance to any induced changes in the product graph. This requires maintaining equivariance to permutations of nodes in both the original graph and its transformation T (G). As a result, recalling that A ∈R(2n×n)×(2n×n) and X ∈R2n×n×d represent the adjacency and feature matrices of the product graph, the symmetries of the product graph are defined by an action of the symmetric group Sn. Formally, a permutation σ ∈ Sn acts on the adjacency and feature matrices by: (σ · A) \u0000 S1, i1, S2, i2 \u0001 = A \u0000 σ−1(S1), σ−1(i1), σ−1(S2), σ−1(i2) \u0001 , (5) (σ · X)(S, i) = X \u0000 σ−1(S), σ−1(i) \u0001 , (6) where we define the action of σ ∈ Sn on a set S = {i1, i2, . . . , ik} of size k as: σ · S := {σ−1(i1), σ−1(i2), . . . , σ−1(ik)} := σ−1(S). 4.2.2 Derivation of linear equivariant layers for the node feature tensor We now characterize the linear equivariant layers with respect to the symmetry defined above, focusing on Equation (6). We adopt a similar notation to [ 22], and assume for simplicity that the number of feature channels is d = 1 (extension to multiple features is straightforward [ 22]). In addition, our analysis considers the case where V T encompasses all potential super-nodes formed by subsets of [n] (i.e we use the sparse coarsened adjacency9). Our main tool is the characterization of linear equivariant layers for permutation symmetries as parameter-sharing schemes [35, 31, 22]. In a nutshell, this characterization states that the parameter vectors of the biases, invariant layers, and equivariant layers can be expressed as a learned weighted sum of basis tensors, where the basis tensors are indicators of the orbits induced by the group action on the respective index spaces. We focus here on presenting the final results and summarize them in Proposition 4.1 at the end of this subsection. Detailed discussion and derivations are available in Appendix E. Equivariant bias and invariant layers. The bias vectors of the linear layers in our space are in R2n×n. As shown in Figure 3(right), the set of orbits induced by the action of Sn satisfies: (P([n]) × [n])/Sn := {γk∗ : k = 1, . . . , n; ∗ ∈ {+, −}}. (7) Here, γk+ corresponds to all pairs (S, i) ∈ P([n]) × [n] with |S| = k and i /∈ S, and γk− to all pairs with |S| = k and i ∈ S. As stated in [35, 31, 22], the tensor set {Bγ S,i}γ∈(P([n])×[n])/Sn where: Bγ S,i = \u001a1, if (S, i) ∈ γ; 0, otherwise. (8) are a basis of the space of bias vectors of the invariant linear layers induced by the action of Sn. 9This is because the action of Sn is well defined over the index set P(V [[n]]) × [n] but not over V × V T 6…………… …… … … … !=#$∈!!=#$∉!!='$∈!!='$∉!… !=#$∈!!=#$∉!… !=#$∈!!=#$∉!!='$∈!!='$∉!… !='$∈!!='$∉! Figure 3: Visualization via heatmaps (different colors correspond to different parameters) of the parameter-sharing scheme determined by symmetries for a graph with n = 6 nodes, zooming-in on the block which corresponds to sets of size two. Left: Visualization of the weight matrix for the equivariant basis BΓ S1,i1;S2,i2 (a total of 35 parameters in the block). Right: Visualization of the bias vector for the invariant basis Bγ S,i (a total of 2 parameters in the block). Symmetry-based updates reduce parameters more effectively than previously proposed linear equivariant layers by treating indices as unordered tuples (see Appendix E.3 for a discussion). Weight matrices. Following similar reasoning, consider elements (S1, i1, S2, i2) ∈ (P([n]) × [n] × P([n]) × [n]). In Appendix E we characterize the orbits of Sn in this space as a partition in which each partition set is defined according to six conditions. Some of these conditions include the sizes of S1, S2 and S1 ∩ S2, which remain invariant under permutations. Given an orbit, Γ ∈ (P([n]) × [n] × P([n]) × [n])/Sn, we define a basis tensor, BΓ ∈ R2n×n×2n×n by setting: BΓ S1,i1;S2,i2 = \u001a1, if (S1, i1, S2, i2) ∈ Γ; 0, otherwise. (9) A visualization of the two basis vectors in Equations (8) and (9), is available in Figure 3. The following (informal) proposition summarizes the results in this section (the proof is given in Appendix G), Proposition 4.1 (Basis of Invariant (Equivariant) Layers). The tensors Bγ (BΓ) in Equation (8) (Equation (9)) form an orthogonal basis (in the standard inner product) of the invariant layers and biases (Equivariant layers – weight matrix) . 4.2.3 Incorporating symmetry-based updates in our framework In the previous subsection, we derived all possible linear invariant and equivariant operations that respect the symmetries of our product graph. We now use this derivation to define the symmetry-based updates in Equation (2), which correspond to the construction of AEquiv and the application of an MPNN. To begin, we note that any linear equivariant layer can be realized through an MPNN [ 13] applied to a fully connected graph with appropriate edge features. This is formally stated in Lemma F.1, the main idea is to encode the param- eters on the edges of this graph (see visualization inset). Thus, the natural construction of AEquiv corresponds to a fully connected graph, with appropriate edge features derived from the parameter-sharing scheme we have developed. {a, b,c, d}a{e}a{f}a {a, b,c, d}b{e}b{f}b {a, b,c, d}c{e}c{f}c {a, b,c, d}d{e}d{f}d {a, b,c, d}e{e}e{f}e {a, b,c, d}f{e}f{f}f However, one of our main goals and guidelines in develop- ing our flexible framework is to maintain efficiency, and to align with the (node-based) maximally expressive GNN, namely GNN-SSWL+ [39, 3], for the case of a trivial coarsening function, T (G) = G (which correspond to the full-bag setting). To achieve this, we opt for a sparser choice by using only a subset of the basis vectors (defined in Equation (9)) to construct AEquiv. Specifically, the ma- trix AEquiv corresponding to the chosen subset of basis vectors is visualized inset – the parameter-sharing scheme is represented by edges with matching 7colors. To clarify, the nodes (S, v) that satisfy v ∈ S “send messages” (i.e., broadcast their rep- resentation) to all the nodes (S′, v′) such that v = v′. A more formal discussion regarding our implementation of those symmetry based updates is given in Appendix F.4. Maintaining sparsity. While the updates above are defined over the sparse representation of the coarse product graph, in practice we use its dense representation, treating it as a graph over the set of nodes V × V T , which requires space complexity O(T · |V |). The update rules above are adapted to this representation simply by masking all nodes (S, v) in the sparse representation such that S /∈ V T . We note the models using the resulting update rule remain invariant to the action of Sn. See discussion in [1]. 4.3 Marking Strategies and Theoretical Analysis One of the key components of subgraph architectures is their marking strategy. Two widely used approaches in node-based subgraph architectures are binary-based node marking [4] and distance- based marking [39], which were proven to be equally expressive in the full-bag setup [39]. Empirically, distance-based marking has been demonstrated to outperform other strategies across several standard benchmarks. In this section, our aim is to develop and theoretically justify an appropriate marking strategy, specifically tailored to the structure of our product graph. We present and discuss here our main results, and refer to Appendix C for a more formal discussion. Building on existing marking strategies and considering the unique structure of our product graph, we propose two natural extensions to both the binary node marking [4] and distance-based marking strategies [39]. Extending binary node marking, we first suggest Simple Marking(πS), where an element (S, v) is assigned a binary feature that indicates whether node v belongs to subgraph S (v ∈ S). The second extension, Node + Size Marking(πSS ), builds on the simple marking by assigning an additional feature that encodes the size of the super-node S. For distance-based strategies, we propose Minimum Distance(πMD ), where each element (S, v) is assigned the smallest (minimal) shortest path distance (SPD) from node v to any node u ∈ S. Finally, Learned Distance Function(πLD) extends this further by assigning to each element (S, v) the output of a permutation-invariant learned function, which takes the set of SPDs between node v and the nodes in S as input. Surprisingly, unlike the node-based full-bag case, we find that these marking strategies are not all equally expressive. We conveniently gather the first three strategies as Π = {πS, πSS , πMD } and summarize the relation between all variants as follows: Proposition 4.2 (Informal – Expressivity of marking strategies.). (i) Strategies in Π are all equally expressive, independently of the transformation function T . (ii) The strategy πLD is at least as expressive as strategies in Π. Additionally, there exists transformation functions s.t. it is strictly more expressive than all of them. The above is formally stated in Propositions C.1 and C.2, and more thoroughly discussed in Ap- pendix C. In light of the above proposition, we instatiate the learned distance function πLD strategy when implementing our model, as follows, XS,v ← X u∈S zdG(v,u) (10) where dG(v, u) denotes the shortest path distance between nodes v and u in the original G10. Coarsening Function and Expressivity. We investigate whether our CS-GNN framework offers more expressiveness compared to directly integrating information between the coarsened graph and the original graph. The two propositions below illustrate that a simple, straight forward integration of the coarsen graph with the original graph (this integration is referred to as the sum graph – formally defined in Definition D.2), and further processing it via standard message-passing, results in a less expressive architecture. Furthermore, when certain coarsening functions are employed within the CS-GNN framework, our resulting architecture becomes strictly more expressive than conventional node-based 10To facilitate this, we maintain a lookup table where each index corresponds to a shortest path distance, assigning a learnable embedding, zdG(v,u) ∈ Rd, to each node (S, v). 8subgraph GNNs. These results suggest that the interplay between the coarsening function and the subgraph layers we have developed enhances the model’s overall performance. We summarize this informally below and provide a more formal discussion in Appendix D. Proposition 4.3 (Informal – CS-GNN goes beyond coarsening). For any transformation function T , CS-GNN can implement message-passing on the sum graph, hence being at least as expressive. Also, there exist transformations T ’s s.t. CS-GNN is strictly more expressive than that. Proposition 4.4 (Informal – CS-GNN vs node based subgraphs). There exist transformations T ’s s.t. our CS-GNN model using T as its coarsening function is strictly more expressive than GNN-SSWL+. 5 Experiments Table 1: Results on ZINC -12 K dataset. Top two results are reported as First and Second. Method Bag size ZINC (MAE ↓) GIN [36] T = 1 0.163 ± 0.004 OSAN [30] T = 2 0.177 ± 0.016 Random [20] T = 2 0.131 ± 0.005 PL [5] T = 2 0.120 ± 0.003 Mag-GNN [20] T = 2 0.106 ± 0.014 Ours T = 2 0.109 ± 0.005 Random [20] T = 3 0.124 ± N/A Mag-GNN [20] T = 3 0.104 ± N/A Ours T = 3 0.096 ± 0.005 Random [20] T = 4 0.125 ± N/A Mag-GNN [20] T = 4 0.101 ± N/A Ours T = 4 0.090 ± 0.003 Random [5] T = 5 0.113 ± 0.006 PL [5] T = 5 0.109 ± 0.005 Ours T = 5 0.095 ± 0.003 GNN-SSWL+ [39] Full 0.070 ± 0.005 Subgraphormer [3] Full 0.067 ± 0.007 Subgraphormer+PE [3] Full 0.063 ± 0.001 Ours Full 0.062 ± 0.0007 We experimented extensively over seven different datasets to answer the following questions: (Q1) Can CS-GNN outperform efficient Subgraph GNNs operating on small bags? (Q2) Does the additional symmetry-based updates boost performance? (Q3) Does CS-GNN offer a good so- lution in settings where full-bag Subgraph GNNs cannot be applied? (Q4) Does CS-GNN in the full-bag setting validate its theory and match state-of-the-art full-bag Sub- graph GNNs? In the following sections, we present our main results and refer to Appendix F for additional experiments and details. Baselines. For each task, we include several baselines. The RANDOM baseline corresponds to random subgraph selection. We report the best performing random baseline from all prior work [ 5, 20, 30, 3]. The other two (non- random) baselines are: (1) LEARNED [5, 20, 30], which represents methods that learn the specific subgraphs to be used; and (2) FULL [39, 3], which corresponds to full-bag Subgraph GNNs. ZINC . We experimented with both the ZINC-12 K and ZINC-F ULL datasets [32, 14, 10], adhering to a 500k parameter budget as prescribed. As shown in Table 1, CS-GNN outperforms all efficient baselines by a significant margin, with at least a +0.008 MAE improvement for bag sizes T ∈ {3, 4, 5}. Additionally, in the full-bag setting, our method recovers state-of-the-art results. The results for ZIN C-FULL are available in Table 8 in the Appendix. OGB. We tested our framework on several datasets from the OGB benchmark collection [16]. Table 4 shows the performance of our method compared to both efficient and full-bag Subgraph GNNs. Our CS-GNN outperforms all baselines across all datasets for bag sizes T ∈ {2, 5}, except for the MOLHIV dataset with T = 2, where PL achieves the best results and our method ranks second. In the full-bag setting, CS-GNN is slightly outperformed by the top-performing Subgraph GNNs but still offers comparable results. Table 2: Results on PEPTIDES dataset. Model ↓ / Dataset → PEPTIDES -FUNC PEPTIDES -STRUCT (AP ↑) (MAE ↓) GCN [19] 0.5930±0.0023 0.3496±0.0013 GIN [36] 0.5498±0.0079 0.3547±0.0045 GatedGCN [7] 0.5864±0.0077 0.3420±0.0013 GatedGCN+RWSE [9] 0.6069±0.0035 0.3357±0.0006 Random [3] 0.5924±0.005 0.2594±0.0021 Ours 0.6156±0.0080 0.2539±0.0015 Peptides. We experimented on the PEPTIDES - FUNC and PEPTIDES -STRUCT datasets [ 9] – which full-bag Subgraph GNNs already strug- gle to process – evaluating CS-GNN’s ability to scale to larger graphs. The results are sum- marized in Table 2. CS-GNN outperforms all MPNN variants, even when incorporating struc- tural encodings such as GATEDGCN+RWSE. Additionally, our method surpasses the ran- dom11 baseline on both datasets. 11For the PEPTIDES datasets, we benchmarked our model against the random variant of Subgraphormer + PE, which similarly incorporates information from the Laplacian eigenvectors. To ensure a fair comparison, 9Table 3: Ablation study. Bag size w/ w/o T=2 0.109±0.005 0.143±0.003 T=3 0.096±0.005 0.101±0.006 T=4 0.090±0.003 0.106±0.001 T=5 0.095±0.003 0.104±0.005 Ablation study – symmetry-based updates. We assessed the impact of the symmetry-based update on the performance of CS-GNN. Specifically, we ask, do the symmetry-based up- dates significantly contribute to the performance of CS-GNN? To evaluate this, we conducted several experiments using the ZINC -12 K dataset across various bag sizes, T ∈ {2, 3, 4, 5}, comparing CS-GNN with and without the symmetry-based up- date. The results are summarized in Table 3. It is clear that the symmetry-based updates play a key role in the performance of CS-GNN. For a bag size of T = 2, the inclusion of the symmetry-based update improves the MAE by a significant 0.034. For other bag sizes, the improvements range from 0.005 to 0.016, clearly demonstrating the benefits of including the symmetry-based updates. Table 4: Results on OGB datasets. The top two results are reported as First and Second. Model↓/ Dataset→ Bag size MOLHIV MOLBACE MOLESOL (ROC-AUC↑) (ROC-AUC↑) (RMSE↓) GIN [36] T = 1 75.58±1.40 72.97±4.00 1.173±0.057 Random [5] T = 2 77.55±1.24 75.36±4.28 0.951±0.039 PL [5] T = 2 79.13±0.60 78.40±2.85 0.877±0.029 Mag-GNN [20] T = 2 77.12±1.13 - - Ours T = 2 77.72±0.76 80.58±1.04 0.850±0.024 OSAN [30] T = 3 - - 0.959±0.184 OSAN [30] T = 5 - 76.30±3.00 - PL [5] T = 5 78.49±1.01 78.39±2.28 0.883±0.032 Random [5] T = 5 77.30±2.56 78.14±2.36 0.900±0.032 Ours T = 5 79.09±0.90 79.64±1.43 0.863±0.029 GNN-SSWL+ [39] Full 79.58±0.35 82.70±1.80 0.837±0.019 Subgraphormer Full 80.38±1.92 81.62±3.55 0.832±0.043 Subgraphormer + PE Full 79.48±1.28 84.35±0.65 0.826±0.010 Ours Full 79.44±0.87 80.71±1.76 0.814±0.021 Discussion. In what follows, we address re- search questions Q1 to Q4. (A1) Tables 1, 2 and 4 clearly demonstrate that we outperform efficient Subgraph GNNs (which operate on a small bag) in 10 out of 12 dataset and bag size combinations. (A2) Our ablation study on the ZINC -12 K dataset, as shown in Table 3, clearly demonstrates the benefits of the symme- try-based updates across all the considered bag sizes. (A3) Table 2 demonstrates that CS-GNN provides an effective solution when the full-bag setting cannot be applied, outperforming all baselines. (A4) On the ZINC -12 K dataset (see Table 1), CS-GNN achieves state-of-the-art re- sults compared to Subgraph GNNs. On the OGBG datasets (see Table 4), our performance is comparable to these top-performing Subgraph GNNs. 6 Conclusions In this work, we employed graph coarsenings and leveraged the insightful connection between Subgraph GNNs and the graph Cartesian product to devise CS-GNN, a novel and flexible Subgraph GNN that can effectively generate and process any desired bag size. Several directions for future research remain open. Firstly, we experimented with spectral clustering based coarsening, but other strategies are possible and are interesting to explore. Secondly, in our symmetry-based updates, we have only considered a portion of the whole equivariant basis we derived: evaluating the impact of other basis elements deserve further attention, both theoretically and in practice. Finally, whether Higher-Order Subgraph GNNs can benefit from our developed parameter-sharing scheme remains an intriguing open question. Limitations. Our method operates over a product graph. Although we provide control over the size of this product graph, achieving better performance requires a larger bag size. This can become a complexity bottleneck, particularly when the original graph is large. Acknowledgements The authors are grateful to Beatrice Bevilacqua, for helpful discussions, and constructive conversations about the experiments. HM is the Robert J. Shillman Fellow and is supported by the Israel Science Foundation through a personal grant (ISF 264/23) and an equipment grant (ISF 532/23). FF is funded by the Andrew and Erna Finci Viterbi Post-Doctoral Fellowship; FF partially performed this work while visiting the Machine Learning Research Unit at TU Wien led by Prof. Thomas Gärtner. we used a single vote and the same exact bag size of 35 subgraphs. Additionally, since Subgraphormer + PE employs GAT [33] as the underlying MPNN, we also utilized GAT for this specific experiment to maintain consistency and fairness. 10References [1] Marjan Albooyeh, Daniele Bertolini, and Siamak Ravanbakhsh. Incidence networks for geo- metric deep learning. arXiv preprint arXiv:1905.11460, 2019. [2] Waiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks. arXiv preprint arXiv:2006.15646, 2020. [3] Guy Bar-Shalom, Beatrice Bevilacqua, and Haggai Maron. Subgraphormer: Unifying subgraph GNNs and graph transformers via graph products. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=6djDWVTUEq. [4] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. International Conference on Learning Representations, 2022. [5] Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, and Haggai Maron. Efficient subgraph gnns by learning effective selection policies. International Conference on Learning Representations, 2024. [6] Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www. wandb.com/. Software available from wandb.com. [7] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017. [8] Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph representations. In Advances in Neural Information Processing Systems, volume 34, 2021. [9] Vijay Prakash Dwivedi, Ladislav Rampášek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems, 35:22326–22340, 2022. [10] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):1–48, 2023. [11] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019. [12] Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. Advances in Neural Information Processing Systems, 35:31376–31390, 2022. [13] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning , pages 1263–1272. PMLR, 2017. [14] Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018. [15] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019. [16] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020. [17] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with i2-gnns. In The Eleventh International Conference on Learning Representations, 2022. 11[18] Nicolas Keriven and Gabriel Peyré. Universal invariant and equivariant graph neural networks. Advances in Neural Information Processing Systems, 32, 2019. [19] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations, 2016. [20] Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, and Muhan Zhang. Mag- gnn: Reinforcement learning boosted graph neural network. Advances in Neural Information Processing Systems, 36, 2024. [21] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness- aware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning, 2021. [22] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. arXiv preprint arXiv:1812.09902, 2018. [23] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. Advances in neural information processing systems, 32, 2019. [24] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 4602–4609, 2019. [25] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. arXiv preprint arXiv:2112.09992, 2021. [26] Pál András Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. CoRR, abs/2201.12884, 2022. [27] Pál András Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. In International Conference on Machine Learning, pages 17323–17345. PMLR, 2022. [28] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. Advances in Neural Information Processing Systems, 34:21997–22009, 2021. [29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [30] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. Advances in Neural Information Processing Systems , 35: 21030–21045, 2022. [31] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter- sharing. In International conference on machine learning, pages 2892–2901. PMLR, 2017. [32] Teague Sterling and John J Irwin. Zinc 15–ligand discovery for everyone. Journal of chemical information and modeling, 55(11):2324–2337, 2015. [33] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. International Conference on Learning Representations, 2017. [34] Ulrike V on Luxburg. A tutorial on spectral clustering.Statistics and computing, 17:395–416, 2007. [35] Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. Discrete applied mathematics, 69(1-2):33–60, 1996. 12[36] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? International Conference on Learning Representations, 2018. [37] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. Advances in Neural Information Processing Systems, 32, 2019. [38] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017. [39] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests. International Conference on Machine Learning, 2023. [40] Muhan Zhang and Pan Li. Nested graph neural networks. In Advances in Neural Information Processing Systems, volume 34, 2021. [41] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In International Conference on Learning Representations, 2022. 13Appendix The appendix is organized as follows: • In Appendix A, we provide some basic definitions that will be used in later sections of the paper. • In Appendix B we discuss some theoretical aspects of our model implementation, and its relation to Equation (2). • In Appendix C we define four natural general node marking policies and analyze their theoretical effects on our model, as well as their relation to some node-based node marking policies. Finally, we provide a principled derivation of one of these policies using the natural symmetry of our base object. • In Appendix D.1 we compare our model to node-based subgraph GNNs, which are the most widely used variant of subgraph GNNs. Additionally, we demonstrate that different choices of coarsening functions can recover various existing subgraph GNN designs. • In Appendix D.2 we demonstrate how our model can leverage the information provided by the coarsening function in an effective way, comparing its expressivity to a natural baseline which also leverages the coarsening function. We show that for all coarsening functions, we are at least as expressive as the baseline and that for some coarsening functions, our model is strictly more expressive. • In Appendix E we delve deeper into the characterization of all linear mapsL : RP([n])×[n] → RP([n])×[n] that are equivariant to the action of the symmetric group. • In Appendix F we provide experimental details to reproduce the results in Section 5, as well as a comprehensive set of ablation studies. • In Appendix G we provide detailed proofs to all propositions in this paper. A Basic Definitions We devote this section to formally defining the key concepts of this paper, as well as introducing new useful notation. We start by defining the two principle components of our pipeline, the cartesian product graph and the coarsening function: Definition A.1 (Cartesian Product Graph). Given two graphs G1 and G2, their Cartesian product G1□G2 is defined as: • The vertex set V (G1□G2) = V (G1) × V (G2). • Vertices (u1, u2) and (v1, v2) in G1□G2 are adjacent if: – u1 = v1 and u2 is adjacent to v2 in G2, or – u2 = v2 and u1 is adjacent to v1 in G1. Definition A.2 (Coarsening Function). A Coarsening function T (·) is defined as a function that, given a graph G = (V, E) with vertex set V = [n] and adjacency matrix A ∈ Rn×n, takes A as input and returns a set of \"super-nodes\" T (A) ⊆ P([n]). The function T (·) is considered equivariant if, for any permutation σ ∈ Sn, the following condition holds: T (σ · A) = σ · T(A). (11) Here, σ ·A, and σ ·T (A) represent the group action of the symmetric groupSn on Rn×n, and P([n]) respectively. A coarsening function allows us to naturally define a graph structure on the \"super-nodes\" obtained from a given graph in the following way: Definition A.3 (Coarsened Graph). Given a coarsening function T (·) and a graph G = (V, E) with vertex set V = [n] , adjacency matrix A ∈ Rn×n, we abuse notation and define the coarsened graph T (G) = (V T , ET ) as follows: • V T = T (A) 14• ET = {{S, S′} |S, S′ ∈ T(A), ∃i ∈ S, i′ ∈ S′ s.t. Ai,i′ = 1}. The adjacency matrix of the coarsened graph can be expressed in two ways. The dense representation AT dense ∈ R|V T |×|V T | is defined by: AT dense(S, S′) = \u001a1 {S, S′} ∈ET 0 otherwise. (12) The sparse representation AT sparse ∈ RP([n])×P([n]) is defined by: AT sparse(S, S′) = \u001a1 S, S′ ∈ V T , {S, S′} ∈ET 0 otherwise. (13) We note that if the coarsened graph T (G) has a corresponding node feature map X : V T → Rd, it also has sparse and dense vector representations defined similarly. Though the dense representation seems more natural, the sparse representation is also useful, as the symmetric group Sn acts on it by: σ · AT sparse(S, S′) = AT sparse(σ−1(S), σ−1(S′)). (14) When the type of representation is clear from context, we abuse notation and write AT . Note also that in the above discussion, we have used the term \"node feature map\". Throughout this paper, in order to denote the node features of a graph G = (V, E) with |V | = n, we use both the vector representation X ∈ Rn×d and the map representation X : V → Rd interchangeably. Now, recalling that our pipeline is defined to create and update a node feature map X(S, v) supported on the nodes of the product graph G□T (G), we define a general node marking policy, the following way: Definition A.4 (General Node Marking Policy). A general node marking policy π(·, ·), is a function which takes as input a graphG = (V, E), and a coarsening function T (·), and returns a node feature map X : V T × V → Rd. In Appendix C We provide four different node marking policies, and analyze the effect on our pipeline. We now move on to define the general way in which we update a given node feature map on the product graph. Definition A.5 (General CS-GNNLayer Update). Given a graph G = (V, E) and a coarsening function T (·), let Xt(S, v) : V × V T → Rd denote the node feature map at layer t. The general CS-GNNlayer update is defined by: Xt+1(S, v) = ft \u0012 Xt(S, v), aggt 1{ {(Xt(S, v′), ev,v′) | v′ ∼G v} }, aggt 2{ {(Xt(S′, v), ˜eS,S′) | S′ ∼GT S} }, aggt 3{ {(Xt(S′, v), z(S, v, S′, v)) | S′ ∈ V T s.t. v ∈ S′} }, aggt 4{ {(Xt(S, v′), z(S, v, S, v′)) | v′ ∈ V s.t. v′ ∈ S} } \u0013 . (15) Here, ft is an arbitrary (parameterized) continuous function, aggt i, i = 1 , . . .4 are learnable permutation invariant aggregation functions,ev,v′, ˜eS,S′ are the (optional) edge features of G and T (G) respectively and the function z : P([n]) ×[n] ×P ([n]) ×[n] → Rd maps each tuple of indices v = (S, v, S′, v′) to a vector uniquely encoding the orbit of v under the action of Sn as described in 73. We note that for brevity, the notation used in the main body of the paper omits the aggregation functions aggt 1, . . . ,aggt 4 and the edge features from the formulation of some of the layer updates. However, we explicitly state each component of the update, as we heavily utilize them in later proofs. We also note that this update is different than the general layer update presented in Equation (2), as it doesn’t use all global updates characterized in 9. The reason for this is that some of the global updates have an asymptotic runtime of ˜O(n2)where n is the number of nodes in the input graph. As our goal was to create models that improve on the scalability of standard subgraph GNNs which have 15an asymptotic runtime of ˜O(n2), We decided to discard some of the general global updates and keep only the ones that are induced by the last two entries in equation 15 which all have a linear runtime. After a stacking of the layers in Equation (15), we employ the following pooling procedure on the final node feature map XT : ρ(XT ) = MLP2  X S ∈V T   MLP1 \u0000 X v∈V XT (S, v) \u0001 !! . (16) Finally, we define the set of all functions that can be expressed by our model: Definition A.6 (Expressivity of Family of Graph Functions). Let F be a family of graph functions, we say that F can express a graph function g(·) if for every finite family of graphs G there exists a function f ∈ Fsuch that: f(G) = g(G) ∀G ∈ G. (17) Here, G is a finite family of graphs if all possible values of node/edge features of the graphs inG form a finite set, and the maximal size of the graphs within G is bounded. Definition A.7 (Family of Functions Expressed By CS-GNN) . Let π be a general node marking policy and T be a coarsening function. Define S(T , π) to be the family of graph functions, which when given input graph G = (V, E), first compute X0(S, v) using π(G, T ), then update this node feature map by stacking T layers of the form 15, and finally pooling X0(S, v) using equation 16. We define CS-GNN(T , π) to be the set of all functions that can be expressed by S(T , π). B Theoretical Validation of Implementation Details In this section, we provide implementation details of our model and prove that they enable us to recover the conceptual framework of the model discussed thus far. First, we note that in Section 4.2, we characterized all equivariant linear maps L : RP([n])×[n] → RP([n])×[n] in order to incorporate them into our layer update. Given the high dimensionality of the space of all such linear maps, and in order to save parameters, we demonstrate that it is possible to integrate these layers into our layer update by adding edge features to a standard MPNN model. This is formalized in the following proposition: Lemma B.1 (Parameter Sharing as MPNN). Let B1, . . . Bk : Rn×n be orthogonal matrices with entries restricted to 0 or 1, and letW1, . . . Wk ∈ Rd×d′ denote a sequence of weight matrices. Define B+ = Pk i=1 Bi and choose z1, . . . zk ∈ Rd∗ to be a set of unique vectors representing an encoding of the index set. The function that represents an update via parameter sharing: f(X) = kX i=1 BiXWi, (18) can be implemented on any finite family of graphs G, by a stack of MPNN layers of the following form [13], ml v = X u∈NB+ (v) Ml(Xl u, eu,v), (19) Xl+1 v = Ul(Xl v, ml v), (20) where Ul, Ml are multilayer perceptrons (MLPs). The inputs to this MPNN are the adjacency matrix B+, node feature vector X, and edge features – the feature of edge (u, v) is given by: eu,v = kX i=1 zi · Bi(u, v). (21) Here, Bi(u, v) denotes the (u, v) entry to matrix Bi. The proof is given in Appendix G. The analysis in Section 4.2 demonstrates that the basis of the space of all equivariant linear maps L : RP([n])×[n] → RP([n])×[n] satisfies the conditions of Lemma F.1. Additionally, we notice that some of the equivariant linear functions have an asymptotic runtime of 16˜O(n2) where n is the number of nodes in the input graph. As our main goal is to construct a more scalable alternative to node-based subgraph GNNs, which also have a runtime of ˜O(n2), we limit ourselves to a subset of the basis for which all maps run in linear time. This is implemented by adding edge features to the adjacency matrices AP1 and AP2 , defined later in this section. We now move on to discussing our specific implementation of the general layer update from Defini- tion A.5. Given a graph G = (V, E) and a coarsening function T , we aim to implement this general layer update by combining several standard message passing updates on the product graph G□T (G). In the next two definitions, we define the adjacency matrices supported on the node set V × V T , which serve as the foundation for these message passing procedures, and formalize the procedures themselves. Definition B.1 (Adjacency Matrices on Product Graph). Let G = (V, E) be a graph with adjacency matrix A and node feature vector X, and let T (·) be a coarsening function. We define the following four adjacency matrices on the vertex set V T × V : AG(S, v, S′, v′) = \u001a1 v ∼G v′, S= S′ 0 otherwise. (22) AT (G)(S, v, S′, v′) = \u001a1 S ∼T (G) S′, v= v′ 0 otherwise. (23) AP1 (S, v, S′, v′) = \u001a1 v ∈ S′, v= v′ 0 otherwise. (24) AP2 (S, v, S′, v′) = \u001a1 v′ ∈ S, S′ = S 0 otherwise. (25) Given edge features {ev,v′ | v ∼G v′} and {˜eS,S′ | s ∼T (G) s′} corresponding to the graphs G and T (G), respectively, we can trivially define the edge features corresponding toAG and AGT as follows: eG(S, v, S′, v′) = ev,v′, (26) eT (G)(S, v, S′, v′) = ˜eS,S′. (27) In addition, for i = 1, 2, we define the edge features corresponding to adjacency matrices APi as follows: ePi(S, v, S′, v′) = z(S, v, S′, v′). (28) Here, the function z : P([n]) × [n] × P([n]) × [n] → Rd maps each tuple v = (S, v, S′, v′) to a vector uniquely encoding the orbit of v under the action of Sn as described in Equation 73. Definition B.2 (CS-GNN Update Implementation). Given a graph G = (V, E), and a coarsening function T (·), let A1 . . . A4 enumerate the set of adjacency matrices {AG, AT (G), AP1 , AP2 }. We define a CS-GNN layer update in the following way: Xt i (S, v) = Ut i  (1 + ϵt i) · Xt(S, v) + X (S′,v′)∼Ai(S,v) Mt(Xt(S′, v′) + ei(S, v, S′, v′))  . (29) Xt+1(S, v) = Ut fin  4X i=1 Xt i (S, v) ! . (30) Here Xt(S, v) and Xt+1(S, v) denote the node feature maps of the product graph at layers t and t + 1, respectively. e1(S, v, S′, v′), . . . , e4(S, v, S′, v′) denote the edge features associated with adjacency matrices A1, . . . , A4. ϵt 1, . . . , ϵt 4 represent learnable parameters in R, and Ut 1, . . . , Ut 4, Ut fin, Mt all refer to multilayer perceptrons. 17The next proposition states that using the layer update defined in equations 29 and 30 is enough to efficiently recover the general layer update defined in equation 15. Proposition B.1 (Equivalence of General Layer and Implemented Layer). Let T (·) be a coarsening function, π be a generalized node marking policy, andG be a finite family of graphs. Applying a stack of t general layer updates as defined in Equation 15 to the node feature map X(S, v) induced by π(G, T ), can be effectively implemented by applying a stack of t layer updates specified in Equations 29 and 30 to X(S, v). Additionally, the depths of all MLPs that appear in 29 and 30 can be bounded by 4. C Node Marking Policies – Theoretical Analysis In this section, we define and analyze various general node marking policies, starting with four natural choices. Definition C.1 (Four General Node Marking policies). Let G = (V, E) be a graph with adjacency matrix A ∈ Rn×n and node feature vector X ∈ Rn×d, and let T (·) be a coarsening function. All of the following node marking policies take the form: π(G, T ) = X(S, v) = [Xu, bπ(S, v)], (31) where [·, ·] denotes the concatenation operator. We focus on four choices forbπ(S, v): 1. Simple Node Marking: bπ(S, v) = \u001a1 if v ∈ S, 0 if v /∈ S. (32) We denote this node marking policy by πS. 2. Node + Size Marking: bπ(S, v) = \u001a(1, |S|) if v ∈ S, (0, |S|) if v /∈ S. (33) We denote this node marking policy by πSS. 3. Minimum Distance: bπ(S, v) = min v′∈S dG(v, v′) (34) where dG(v, v′) is the shortest path distance between nodes v and v′ in the original graph. We denote this node marking policy by πMD. 4. Learned Distance Function: bπ(S, v) = ϕ({dG(v, v′) | v′ ∈ S}) (35) where ϕ(·) is a learned permutation-invariant function. We denote this node marking policy by πLD. We note that when using the identity coarsening function T (G) = G, our general node marking policies output node feature maps supported on the product V × V . Thus, they can be compared to node marking policies used in node-based subgraph GNNs. In fact, in this case, both πS and πSS reduce to classical node-based node marking, while πMD and πLD reduce to distance encoding. The definitions of these can be found in [39]. Interestingly, even though in the case of node-based subgraph GNNSs, both distance encoding and node marking were proven to be maximally expressive [39], in our case for some choices of T , πLD is strictly more expressive than the other three choices. The exact effect of each generalized node marking policy on the expressivity of our model is explored in the following two propositions. Proposition C.1 (Equal Expressivity of Node Marking Policies). For any coarsening function T (·) the following holds: CS-GNN(T , πS) = CS-GNN(T , πSS) = CS-GNN(T , πMD). (36) 18Proposition C.2 (Expressivity of Learned Distance Policy). For any coarsening functionT (·) the following holds: CS-GNN(T , πS) ⊆ CS-GNN(T , πLD). (37) In addition, for some choices of T (·) the containment is strict. The proofs of both propositions can be found in Appendix G. Finally, we provide a principled approach to deriving a generalized node marking policy based on symmetry invariance, and prove its equivalence to πSS. Given a graph G = (V, E) with V = [n], adjacency matrix A, and node feature vector X ∈ Rn×d, along with a coarsening function T (·), We define an action of the symmetric group Sn on the space RP([n])×[n] as follows: σ · X(S, v) = X(σ−1(S), σ−1(v)) for σ ∈ Sn, X ∈RP([n])×[n]. (38) Now, for each orbit γ ∈ (P([n]) × [n])/Sn, we define 1γ ∈ RP([n])×[n] as follows: 1γ(S, v) = \u001a1 ( S, v) ∈ γ, 0 otherwise. (39) Choosing some enumeration of the orbit set (P([n]) × [n])/Sn = {γ1, . . . , γk}, We now define the invariant generalized node marking policy πinv by first setting: bsparse πinv (S, v) : P([n]) × [n] → Rk and bπinv : V T × V → Rk as follows: bsparse πinv (S, v) = [1γ1 (S, v), . . . ,1γk (S, v)] S ∈ P(V ), v∈ V, (40) bπinv (S, v) = bsparse πinv (S, v) S ∈ V T , v∈ V. (41) Then, we define the node feature map induced by πinv as: Xπinv (S, v) = [Xv, bπinv (S, v)]. (42) Interestingly, πinv, derived solely from the group action of Sn on P([n]) × [n], is equivalent to the generalized node marking policy πSS. This is stated more rigorously in the following proposition: Proposition C.3 (Node + Size Marking as Invariant Marking). Given a graph G = (V, E) with node feature vector X ∈ Rn×d, and a coarsening function T (·), let XπSS , Xπinv be the node feature maps induced by πSS and πinv respectively. Recall that: XπSS (S, v) = [Xv, bπSS (S, v)], (43) Xπinv (S, v) = [Xv, bπinv (S, v)]. (44) The following now holds: bπinv (S, v) = OHE(bπSS (S, v)) ∀S ∈ V T , ∀v ∈ V. (45) Here, OHE denotes a one-hot encoder, independent of the choice of bothG and T . The proof of proposition C.3 can be found in Appendix G. D Expressive Power of CS-GNN D.1 Recovering Subgraph GNNs In this section, we demonstrate that by choosing suitable coarsening functions, our architecture can replicate various previous subgraph GNN designs. We begin by focusing on node-based models, which are the most widely used type. We define a variant of these models which was proven in [39] to be maximally expressive, and show that our approach can recover it. Definition D.1 (Maximally Expressive Subgraph GNN). We define MSGNN(πNM) as the set of all functions expressible by the following procedure: 191. Node Marking:The representation of tuple (u, v) ∈ V × V is initially given by: X0(u, v) = \u001a1 if u = v, 0 if u ̸= v. (46) 2. Update: The representation of tuple (u, v) is updated according to: Xt+1(u, v) = ft \u0012 Xt(u, v), Xt(u, u), Xt(v, v), aggt 1{ {(Xt(u, v′), ev,v′) | v′ ∼ v} }, aggt 2{ {(Xt(v, u′), eu,u′) | u′ ∼ u} } \u0013 . (47) 3. Pooling: The final node feature vector XT (u, v) is pooled according to: MLP2  X u∈V MLP1  X v∈V XT (u, v) !! . (48) Here, for anyt ∈ [T], ft is any continuous (parameterized) functions,aggt 1, ,aggt 2 are any continuous (parameterized) permutation-invariant functions and MLP1, MLP2 are multilayer preceptrons. Proposition D.1 (CS-GNN Can Implement MSGNN). Let T (·) be the identity coarsening function defined by: T (G) = {{v} |v ∈ V } ∀G = (V, E). (49) The following holds: CS-GNN(T , πS) = MSGNN(πNM). (50) The proof of proposition D.1 can be found in Appendix G. We observe that, similarly, by selecting the coarsening function: T (G) = E ∀G = (V, E), (51) one can recover edge-based subgraph GNNs. An example of such a model is presented in [4] (DS- GNN), where it was proven capable of distinguishing between two 3-WL indistinguishable graphs, despite having an asymptotic runtime of ˜O(m2), where m is the number of edges in the input graph. This demonstrates our model’s ability to achieve expressivity improvements while maintaining a (relatively) low asymptotic runtime by exploiting the graph’s sparsity through the coarsening function. Finally, we note that by selecting the coarsening function: T (G) = {S ∈ P(V ) | |S| = k} G = (V, E), (52) We can recover an unordered variant of thek-OSAN model presented in [30]. D.2 Comparison to Natural Baselines In this section, we demonstrate how our model can leverage the information provided by the coars- ening function T (·) in an effective way. First, we define a baseline model that incorporatesT in a straightforward manner. We then prove that, for anyT (·), our model is at least as expressive as this baseline. Additionally, we show that for certain choices of T (·), our model exhibits strictly greater expressivity. To construct the baseline model, we first provide the following definition: Definition D.2 (Coarsened Sum Graph). Given a graph G = (V, E) and a coarsening function T (·), we define the coarsened sum graph GT + = (V T + , ET + ) by: • V T + = V ∪ V T . • ET + = E ∪ ET ∪ {{S, v} |S ∈ V T , v∈ V v∈ S}. If graph G had a node feature vector X ∈ Rn×d, we define the node feature vector of GT + as: Xv = \u001a[Xv, 1] v ∈ V 0d+1 v ∈ V T . (53) Here we concatenated a 1 to the end of node features of V to distinguish them from the nodes of V T . 20{a, b,c, d}#bcde a f ! \"(!) {e}{f} {a, b,c, d}bcde a f % %! {e}{f} The connectivity of the sum graph (for our running example Figure 1) is visualized inset. We now define our baseline model: Definition D.3 (Coarse MPNN). Let T (·) be a coarsening function. De- fine MPNN+(T ) as the set of all functions which can be expressed by the following procedure: 1. Preprocessing: We first construct the sum graph GT + of the input graph G, along with a node feature map X0 : V T + → Rd defined according to equation 53. 2. Update: The representation of node v ∈ V T + is updated according to: For v ∈ V : Xt+1(v) = ft V \u0000 Xt(v), aggt 1{ {(Xt(u), eu,v) | u ∼G v} }, aggt 2{ {Xt(S) | S ∈ V T , v∈ S} } \u0001 , For S ∈ V T : Xt+1(S) = ft V T \u0000 Xt(S), aggt 1{ {(Xt(S′), eS,S′) | S′ ∼T (G) S} }, aggt 2{ {Xt(v) | v ∈ V, v∈ S \t } }). (54) 3. Pooling: The final node feature vector XT (·) is pooled according to: MLP   X v∈V T + XT (v)  . (55) Here, for t ∈ [T], ft V ,, ft V T are continuous (parameterized) functions and , aggt 1, aggt 2T are continuous (parameterized) permutation invariant functions. Finally, we notice that for the trivial coarsening function defined by T∅(G) = ∅, (56) the update in Equation (54) devolves into a standard MPNN update, as defined in [13] and so we define: MPNN = MPNN+(T∅). (57) In essence, given an input graph G = (V, E), the MPNN+(T ) pipeline first constructs the coarsened graph T (G). It then adds edges between each super-node S ∈ V T and the nodes it is comprised of (i.e., any v ∈ S). This is followed by a standard message passing procedure on the graph. The following two propositions suggest that this simple approach to incorporating T into a GNN pipeline is less powerful than our model. Proposition D.2 (CS-GNN Is at Least as Expressive as Coarse MPNN ). For any coarsening function T (·) the following holds: MPNN ⊆ MPNN+(T ) ⊆ CS-GNN(T , πS) (58) Proposition D.3 (CS-GNN Can Be More Expressive Than MPNN +). Let T (·) be the identity coarsening function defined by: T (G) = {{v} |v ∈ V } G = (V, E). (59) The following holds: MPNN = MPNN+(T ). (60) Thus: MPNN+(T ) ⊂ CS-GNN(T , πS), (61) where this containment is strict. 21The proofs to the last two propositions can be found in Appendix G. Proposition D.3 demonstrates that CS-GNNis strictly more expressive than MPNN+ when using the identity coarsening function. However, this result extends to more complex coarsening functions as well. We briefly discuss one such example. Let T (·) be the coarsening function defined by: T△(G) = {v1, v2, v3 | G[v1, v2, v3] ∼= △}, (62) i.e. for an input graph G, the set of super-nodes is composed of all triplets of nodes whose induced subgraph is isomorphic to a triangle. To see that CS-GNN is strictly more expressive then MPNN+ when using T△(·), we look at the two graphs G and H depicted in Figure 4. In the figure, we see the two original graphs, G and H, their corresponding sum graphsGT△ + and HT△ + , and a subgraph of their corresponging product graphs G□T△(G) and H□T△(H) induced by the sets {(S0, v) | v ∈ VG} and {(S0, v) | v ∈ VH} respectively (this can be thought of as looking at a single subgraph from the bag of subgraphs induced by CS-GNN). One can clearly see that both the original graphs and their respective sum graphs are 1-WL indistinguishable. On the other hand, the subgraphs induced by our method are 1-WL distinguishable. Since for both G and H the \"bag of subgraphs\" induced by CS-GNN is composed of 6 isomorphic copies of the same graph, this would imply that our method can distinguish between G and H, making it strictly mor expressive then MPNN+. Figure 4: Rows 1 and 3 depict two 1-WL indistinguishable graphs> Rows 2 and 4 depict the sum graph of each of these graphs, as well as one subgraph of their product graphs induced by all node, super-node tuples whose super-node is fixed. 22We conclude this section with the following proposition, showing there exists coarsening functions which, when combined with CS-GNN, results in an architecture that is strictly more expressive then node-based subgraph GNNs. Proposition D.4 (CS-GNN can be strictly more expressive then node-based subgraph GNNs). Let T be the coarsening function defined by: T (G) = {{v} |v ∈ V } ∪E G = (V, E). (63) The following holds: 1. Let G1, G2 be a pair of graphs such that there exists a node-based subgraph GNN model M where M(G1) ̸= M(G2). There exists a CS-GNNmodel M′ which uses T such that M′(G1) ̸= M′(G2). 2. There exists a pair of graphs G1, G2 such that for any subgraph GNN model M it holds that M(G1) = M(G2), but there exists a CS-GNNmodel M′ which uses T such that M′(G1) ̸= M′(G2). This proposition is proved in Appendix G. E Linear Invariant (Equivariant) Layer – Extended Section We introduce some key notation. In the matrix X, the i-th row corresponds to the i-th subset S arranged in the lexicographic order of all subsets of [n], namely, [{0}, {0, 1}, {0, 2}, . . . ,{0, 1, 2, . . . , n}]. Each i-th position in this sequence aligns with the i-th row index in X. It follows, that the standard basis for such matrices in R2n×n is expressed as e(S) · e(i)T , where e(S) is a 1-hot vector, with the value 1 positioned according to S in the lexicographic order. For a matrix X ∈ Ra×b, the operation of vectorization, denoted by vec(X), transforms X into a single column vector in Rab×1 by sequentially stacking its columns; in the context of X, the basis vectors of those vectors are e(i) ⊗ e(S). The inverse process, reshaping a vectorized matrix back to its original format, is denoted as [vec(X)] = X. We also denote an arbitrary permutation by σ ∈ Sn. The actions of permutations on vectors, whether indexed by sets or individual indices, are represented by PS ∈ GL(2n) and PI ∈ GL(n), respectively. This framework acknowledges Sn as a subgroup of the larger permutation group S2n, which permutes all 2n positions in a given vector vS ∈ R2n . Let L ∈ R1×2n·n be the matrix representation of a general linear operator L : R2n×n → R in the standard basis. The operator L is order-invariant iff L vec(PT SXPI) = L vec(X). (64) Similarly, let L ∈ R2n·n×2n·n denote the matrix for L : R2n×n → R2n×n. The operator L is order-equivariant if and only if [L vec(PT SXPI)] = PT S[L vec(X)]PI. (65) Using properties of the Kronecker product (see Appendices E.1 and E.2 for details), we derive the following conditions for invariant and equivariant linear layers: Invariant L : PI ⊗ PS vec(L) = vec(L), (66) Equivariant L : PI ⊗ PS ⊗ PI ⊗ PS vec(L) = vec(L). (67) Solving Equations (66) and (67). Let σ ∈ Sn denote a permutation corresponding to the permutation matrix P. Let P ⋆ L denote the tensor that results from expressing L after renumbering the nodes in V T , Vaccording to the permutation σ. Explicitly, for L ∈ R2n×n, the (σ(S), σ(i))-entry of P ⋆ L equals to the (S, i)-entry of L. The matrix that corresponds to the operator P⋆ in the standard basis, e(i) ⊗ e(S) is the kronecker product PI ⊗ PS. Since vec(L) is exactly the coordinate vector of the tensor L in the standard basis we have, vec(P ⋆ L) = PI ⊗ PS vec(L), (68) following the same logic, the following holds for the equivariant case, where L ∈ R2n·n×2n·n, vec(P ⋆ L) = PI ⊗ PS ⊗ PI ⊗ PS vec(L). (69) 23Given Equations (66) and (68) and Equations (67) and (69), it holds that we should focus on solving, P ⋆ L = L, ∀P permutation matrices, (70) for both cases where L ∈ R2n×n and L ∈ R2n×n×2n×n, corresponding to the bias term, and linear term. Bias. To this end, let us define an equivalence relation in the index space of a tensor inR2n×n. Given a pair (S, i) ∈ P([n]) × [n], we define γk+ to correspond to all pairs (S, i) such that |S| = k and i /∈ S. Similarly, γk− corresponds to all pairs (S, i) such that |S| = k and i ∈ S. We denote this equivalence relation as follows: (P([n]) × [n])/∼ ≜ {γk∗ : k = 1, . . . , n; ∗ ∈ {+, −}}. (71) For each set-equivalence class γ ∈ (P([n]) × [n])∼, we define a basis tensor, Bγ ∈ R2n×n by setting: Bγ S,i = \u001a1, if (S, i) ∈ γ; 0, otherwise. (72) Following similar reasoning, consider elements (S1, i1, S2, i2) ∈ (P([n]) × [n] × P([n]) × [n]). We define a partition according to six conditions: the relationship between i1 and i2, denoted as i1 ↔ i2, which is determines by the condition: i1 = i2 or i1 ̸= i2; the cardinalities of S1 and S2, denoted as k1 and k2, respectively; the size of the intersection S1 ∩ S2, denoted as k∩; the membership of il in Sl for l ∈ {1, 2}, denoted as δsame ∈ {1, 2, 3, 4}; and the membership of il1 in Sl2 for distinct l1, l2 ∈ {1, 2}, denoted as δdiff ∈ {1, 2, 3, 4}. The equivalence relation thus defined can be represented as: (P([n]) × [n] × P([n]) × [n])/∼ ≜ {Γ↔;k1;k2;k∩;δsame;δdiff }. (73) For each set-equivalence class Γ ∈ (P([n]) × [n] × P([n]) × [n])/∼, we define a basis tensor, BΓ ∈ R2n×n×2n×n by setting: BΓ S1,i1;S2,i2 = \u001a1, if (S1, i1, S2, i2) ∈ Γ; 0, otherwise. (74) The following two proposition summarizes the results in this section, Lemma E.1 (γ (Γ) are orbits). The sets {γk∗ : k = 1, . . . , n; ∗ ∈ {+, −}} and {Γ↔;k1;k2;k∩;δsame;δdiff } are the orbits ofSn on the index space (P([n])×[n]) and (P([n])×[n]×(P([n])×[n]), respectively. Proposition E.1 (Basis of Invariant (Equivariant) Layer). The tensors Bγ (BΓ) in Equation (72) (Equation (74)) form an orthogonal basis (in the standard inner product) to the solution of Equa- tion (66) (Equation (67)). The proofs are given in Appendix G. E.1 Full Derivation of Equation (66). Our goal is to transition from the equation, L vec(PT SXPI) = L vec(X) (64) to the form, PI ⊗ PS vec(L) = vec(L) (66) We introduce the following property of the Kronecker product, vec(ABC) = (CT ⊗ A)vec(B). (75) Using Equation (75) on the left side of Equation (64), we obtain LPT I ⊗ PT S vec(X) = L vec(X), (76) since this should be true for any X ∈R2n×n, we derive LPT I ⊗ PT S = L. (77) Applying the transpose operation on both sides, and noting that (PT I ⊗PT S)T = PI ⊗PS, we obtain PI ⊗ PSLT = LT . (78) Recalling that L ∈ R1×2n·n, and thus LT ∈ R2n·n×1, we find that LT = vec(L). Substituting this back into the previous equation we achieve Equation (66). 24E.2 Full Derivation of Equation (67). Our goal is to transition from the equation, [L vec(PT SXPI)] = PT S[L vec(X)]PI (65) to the form, PI ⊗ PS ⊗ PI ⊗ PS vec(L) = vec(L). (67) Applying the property in Equation (75), after the reverse operation of the vectorization, namely, [vec(ABC)] = [(CT ⊗ A)vec(B)] (79) on the right hand side of Equation (65), for A ≜ PT S; (80) B ≜ [L vec(X)]; (81) C ≜ PI, (82) we obtain, [L vec(PT SXPI)] = [PT I ⊗ PT SL vec(X)]. (83) Thus, by omitting the revere-vectorization operation, L vec(PT SXPI) = PT I ⊗ PT SL vec(X). (84) Noting that (PT I ⊗ PT S)−1 = PI ⊗ PS, and multiplying by this inverse both sides (from the left), we obtain, PI ⊗ PSL vec(PT SXPI) = L vec(X). (85) Applying, again, the property in Equation (75), we obtain, PI ⊗ PSLPT I ⊗ PT S vec(X) = L vec(X). (86) Since this should be true for any X ∈R2n×n, we derive, PI ⊗ PSLPT I ⊗ PT S = vec(L). (87) Again, applying Equation (75) on the left side, where, A ≜ PI ⊗ PS; (88) B ≜ L; (89) C ≜ PT I ⊗ PT S, (90) we get the following equality, PI ⊗ PSLPT I ⊗ PT S = PI ⊗ PS ⊗ PI ⊗ PS vec(L). (91) By substituting this to the left side of Equation (87) we obtain Equation (67). E.3 Comparative Parameter Reduction in Linear Equivariant Layers To demonstrate the effectiveness of our parameter-sharing scheme, which results from considering unordered tuples rather than ordered tuples, we present the following comparison. 3-IGNs [22] are structurally similar to our approach, with the main difference being that they consider indices as ordered tuples, while we consider them as sets. Both approaches use a total of six indices, as shown in the visualized block in Figure 3, making 3-IGNs a natural comparator. By leveraging our scheme, we reduce the number of parameters from 203 (the number of parameters in 3-IGNs) to just 35! 25Table 5: Overview of the graph learning datasets. Dataset # GraphsAvg. # nodesAvg. # edgesDirected Prediction task Metric ZINC-12K[32] 12,000 23.2 24.9 No Regression Mean Abs. Error ZINC-FULL[32] 249,456 23.2 49.8 No Regression Mean Abs. Error OGBG-MOLHIV[16] 41,127 25.5 27.5 No Binary Classification AUROC OGBG-MOLBACE[16] 1513 34.1 36.9 No Binary Classification AUROC OGBG-MOLESOL[16] 1,128 13.3 13.7 No Regression Root Mean Squ. Error PEPTIDES-FUNC[9] 15,535 150.9 307.3 No 10-task ClassificationAvg. Precision PEPTIDES-STRUCT[9] 15,535 150.9 307.3 No 11-task RegressionMean Abs. Error F Extended Experimental Section F.1 Dataset Description In this section we overview the eight different datasets considered; this is summarized in Table 5. ZINC-12 K and ZINC-F ULL Datasets [32, 14, 10]. The ZINC-12 K dataset includes 12,000 molecular graphs sourced from the ZINC database, a compilation of commercially available chemical compounds. These molecular graphs vary in size, ranging from 9 to 37 nodes, where each node represents a heavy atom, covering 28 different atom types. Edges represent chemical bonds and there are three types of bonds. The main goal when using this dataset is to perform regression analysis on the constrained solubility (logP) of the molecules. The dataset is divided into training, validation, and test sets with 10,000, 1,000, and 1,000 molecular graphs respectively. The full version, ZINC-F ULL , comprises approximately 250,000 molecular graphs, ranging from 9 to 37 nodes and 16 to 84 edges per graph. These graphs also represent heavy atoms, with 28 distinct atom types, and the edges indicate bonds between these atoms, with four types of bonds present. OGBG -MOLHIV , OGBG -MOLBACE , OGBG -MOLESOL Datasets [16]. These datasets are used for molecular property prediction and have been adopted by the Open Graph Benchmark (OGB, MIT License) from MoleculeNet. They use a standardized featurization for nodes (atoms) and edges (bonds), capturing various chemophysical properties. PEPTIDES -FUNC and PEPTIDES -STRUCT Datasets [9]. The PEPTIDES -FUNC and PEPTIDES - STRUCT datasets consist of atomic graphs representing peptides released with the Long Range Graph Benchmark (LRGB, MIT License). In PEPTIDES -FUNC , the task is to perform multi-label graph classification into ten non-exclusive peptide functional classes. Conversely, PEPTIDES -STRUCT is focused on graph regression to predict eleven three-dimensional structural properties of the peptides. We note that for all datasets, we used the random splits provided by the public benchmarks. F.2 Experimental Details Implementation Details. Our implementation of Equation (2) is given by: X(l+1) = MLP  3X i=1 MPNN(l+1,i) (X, Ai) ! , (92) where A1 = AG, A2 = AT (G), and A3 = AEquiv. For all considered datasets, namely, ZINC-12 K, ZINC-F ULL , OGBG -MOLHIV , OGBG -MOLBACE , and OGBG -MOLESOL , except for the PEPTIDES -FUNC and PEPTIDES -STRUC datasets, we use a GINE [15] base encoder. Given an adjacency matrix A, and defining e(S′,v′),(S,v) to denote the edge features from node (S′, v′) to node (S, v), it takes the following form: X(S, v) = MLP \u0012 (1 + ϵ) · X(S, v) + X (S′,v′)∼A(S,v) ReLU \u0000 X(S′, v′) + e(S′,v′),(S,v) \u0001\u0013 . (93) We note that for the symmetry-based updates, we switch the ReLU to an MLP12 to align with the theoretical analyses13 (Appendix B), stating that we can implement the equivariant update developed 12With the exception of the OGB datasets, to avoid overfitting. 13The theoretical analysis assumes the usage of an MLP for all three considered updates. 26in Section 4.2. A more thorough discussion regarding the implementation of the symmetry-based updates is given in Appendix F.4. When experimenting with the PEPTIDES -FUNC and PEPTIDES -STRUC datasets, we employ GAT [33] as our underlying MPNN to ensure a fair comparison with the random baseline—the random variant of Subgraphormer + PE [3]. To clarify, we consider the random variant ofSubgraphormer + PE as a natural random baseline since it incorporates the information in the eigenvectors of the Laplacian (which we also do via the coarsening function). To maintain a fair comparison, we use a single vote for this random baseline, and maintained the same hyperparameters. Our experiments were conducted using the PyTorch [29] and PyTorch Geometric [11] frameworks (resp. BSD and MIT Licenses), using a single NVIDIA L40 GPU, and for every considered experi- ment, we show the mean ± std. of 3 runs with different random seeds. Hyperparameter tuning was performed utilizing the Weight and Biases framework [6] – see Appendix F.3. All our MLPs feature a single hidden layer equipped with a ReLU non-linearity function. For the encoding of atom numbers and bonds, we utilized learnable embeddings indexed by their respective numbers. In the case of the OGBG -MOLHIV , OGBG -MOLESOL , OGBG -MOLBACE datasets, we follow Frasca et al. [12], therefore adding a residual connection between different layers. Additionally, for those datasets (except OGBG -MOLHIV ), we used linear layers instead of MLPs inside the GIN layers. Moreover, for these four datasets, and for the PEPTIDES datasets, the following pooling mechanism was employed ρ(X) = MLP  X S   1 n nX v=1 X(s, v) !! . (94) For the PEPTIDES datasets, we also used a residual connection between layers. F.3 HyperParameters In this section, we detail the hyperparameter search conducted for our experiments. Besides standard hyperparameters such as learning rate and dropout, our specific hyperparameters are: 1. Laplacian Dimension: This refers to the number of columns used in the matrix U, where L = UT λU, for the spectral clustering in the coarsening function. 2. SPD Dimension: This represents the number of indices used in the node marking equation. To clarify, since |S| might be large, we opt for using the first k indices that satisfy i ∈ S, sorted according to the SPD distance. SPD Dimension. For the Laplacian dimension, we chose a fixed value of 10 for all bag sizes for both ZINC -12 K and ZINC -FULL datasets. For OGBG -MOLHIV , we used a fixed value of 1, since the value 10 did not perform well. For the PEPTIDES datasets, we also used the value 1. For the OGBG -MOLESOL and OGBG -MOLBACE datasets, we searched over the two values {1, 2}. Laplacian Dimension. For the Laplacian dimension, we searched over the values {1, 2} for all datasets. Standard Hyperparameters. For ZINC -12 K, we used a weight decay of 0.0003 for all bag sizes, except for the full bag size, for which we used 0.0001. All of the hyperparameter search configurations are presented in Table 6, and the selected hyperpa- rameters are presented in Table 7. Table 6: Hyperparameters search for CS-GNN. Dataset Bag size Num. layersLearning rateEmbedding sizeEpochsBatch sizeDropoutLaplacian dimensionSPD dimension ZINC-12K T= 2 6 0.0005 96 400 128 0 {1,2} 10ZINC-12K T∈ {3,4,5,8,18} 6 0.0007 96 400 128 0 {1,2} 10ZINC-12K T=“full” 6 0.0007 96 500 128 0 {1,2} 10 ZINC-FULL T= 4 6 0.0007 96 400 128 0 {1,2} 10ZINC-FULL T=“full” 6 {0.001,0.0005} 96 500 128 0 {1,2} 10 OGBG-MOLHIV T∈ {2,5,“full”} 2 0.01 60 100 32 0.5 {1,2} 1OGBG-MOLESOLT∈ {2,5,“full”} 3 0.001 60 100 32 0.3 {1,2} { 1, 2 }OGBG-MOLBACET∈ {2,5,“full”} {2,3} 0.01 60 100 32 0.3 {1,2} { 1, 2 } PEPTIDES-FUNC T= 30 5 {0.01,0.005} 96 200 128 0 {1,2} 1PEPTIDES-STRUC T= 30 4 {0.01,0.005} 96 200 128 0 {1,2} 1 27Table 7: Chosen Hyperparameters for CS-GNN. Dataset Bag size Num. layersLearning rateEmbedding sizeEpochsBatch sizeDropoutLaplacian dimensionSPD dimension ZINC-12K T= 2 6 0.0005 96 400 128 0 1 10ZINC-12K T= 3 6 0.0007 96 400 128 0 2 10ZINC-12K T= 4 6 0.0007 96 400 128 0 1 10ZINC-12K T= 5 6 0.0007 96 400 128 0 1 10ZINC-12K T= 8 6 0.0007 96 400 128 0 1 10ZINC-12K T= 18 6 0.0007 96 400 128 0 1 10ZINC-12K T=“full” 6 0.0007 96 500 128 0 N/A 10 ZINC-FULL T= 4 6 0.0007 96 400 128 0 1 10ZINC-FULL T=“full” 6 0.0005 96 500 128 0 N/A N/A OGBG-MOLHIV T= 2} 2 0.01 60 100 32 0.5 1 1 OGBG-MOLHIV T= 5 2 0.01 60 100 32 0.5 1 1 OGBG-MOLHIV T=“full” 2 0.01 60 100 32 0.5 N/A N/A OGBG-MOLESOL T= 2 3 0.001 60 100 32 0.3 1 2 OGBG-MOLESOL T= 5 3 0.001 60 100 32 0.3 1 2 OGBG-MOLESOLT=“full” 3 0.001 60 100 32 0.3 N/A N/A OGBG-MOLBACE T= 2 3 0.01 60 100 32 0.3 1 1 OGBG-MOLBACE T= 5 3 0.01 60 100 32 0.3 1 2 OGBG-MOLBACET=“full” 3 0.01 60 100 32 0.3 N/A N/A PEPTIDES-FUNC T= 30 5 0.005 96 200 128 0 1 1PEPTIDES-STRUC T= 30 4 0.01 96 200 128 0 1 1 Optimizers and Schedulers. For the ZINC-12 K and ZINC -FULL datasets, we employ the Adam optimizer paired with a ReduceLROnPlateau scheduler,factor set to 0.5, patience at 40 14, and a minimum learning rate of 0. For the OGBG -MOLHIV dataset, we utilized the ASAM optimizer [21] without a scheduler. For both OGBG -MOLESOL and OGBG -MOLBACE , we employed a constant learning rate without any scheduler. Lastly, for the PEPTIDES -FUNC and PEPTIDES -STRUCT datasets, the AdamW optimizer was chosen in conjunction with a cosine annealing scheduler, incorporating 10 warmup epochs. F.4 Implementation of Linear Equivariant and Invariant layers – Extended Section In this section, in a more formal discussion, we specify how to integrate those invariant and equivariant layers to our proposed architecture. We start by drawing an analogy between parameter sharing in linear layers and the operation of an MPNN on a fully connected graph with edge features in the following lemma, Lemma F.1 (Parameter Sharing as MPNN). Let B1, . . . Bk : Rn×n be orthogonal matrices with entries restricted to 0 or 1, and letW1, . . . Wk ∈ Rd×d′ denote a sequence of weight matrices. Define B+ = Pk i=1 Bi and choose z1, . . . zk ∈ Rd∗ to be a set of unique vectors representing an encoding of the index set. The function, which represents an update via parameter sharing: f(X) = kX i=1 BiXWi, (95) can be implemented by a stack of MPNN layers of the following form [13], ml u = X v∈NB+ (u) Ml(Xl v, eu,v), , (96) Xl+1 u = Ul(Xl v, ml v), (97) where Ul, Ml are multilayer preceptrons (MLPs). The inputs to this MPNN are the adjacency matrix B+, node feature vector X, and edge features – the feature of edge (u, v) is given by: eu,v = kX i=1 zi · Bi(u, v). (98) Here, Bi(u, v) denotes the (u, v) entry to matrix Bi. The proof is given in Appendix G. 14For ZINC -12 K, T ∈ {2, “full”}, we used a patience of 50. 28Table 8: Comparison over the ZINC -FULL molecular dataset under 500k parameter budget. The best performing method is highlighted in blue, while the second best is highlighted in red. Model ↓ / Dataset → ZINC-F ULL (MAE ↓) MAG-GNN [20] (T = 4) 0.030 ±0.002 Ours (T = 4) 0.027 ±0.002 GNN-SSWL [39] (T = “full” ) 0.026 ±0.001 GNN-SSWL+ [39] (T = “full” ) 0.022 ±0.001 Subgraphormer [3] (T = “full” ) 0.020 ±0.002 Subgraphormer + PE [3] (T = “full” ) 0.023 ±0.001 Ours (T = “full” ) 0.021 ±0.001 Thus, our implementation for the global update is as follows, X(S, i) = MLP  (1 + ϵ) · X(S, i) + X (S′,i′)∼AEquiv (S,i) MLP \u0012 X(S′, i′) + e(S′,i′),(S,i) \u0013 , (99) where e(S′,i′),(S,i) = P Γ zΓ · BΓ S,i;S′,i′ and zΓ are orthogonal 1-hot vectors for different Γ’s. The connectivity AEquiv is such that AEquiv (S, v, S′, v′) contains the value one iff v ∈ S, v= v′. This corresponds to choosing only several Γ’s in the partition, and since each Γ is invariant to the permutation, this choice still maintains equivariance. F.5 Additional Results ZINC -FULL . Below, we present our results on theZINC -FULL dataset for a bag size of T = 4 and the full-bag. For the bag size T = 4, we benchmark against MAG-GNN [20], which in their experiments used the best out of the bag sizes T ∈ {2, 3, 4}; however, they did not specify which one performed the best. The results are summarized in Table 8. ZINC -12 K – additional results. We present all the results from Figure 2, along with some additional ones, in Table 9. Runtime comparison. We compare the training time and prediction performance on the ZINC -12 K dataset. For all methods, we report the training and inference times on the entire training and test sets, respectively, using a batch size of 128. Our experiments were conducted using an NVIDIA L40 GPU, while for the baselines, we used the timing reported in [5], which utilized an RTX A6000 GPU. The runtime comparison is presented in Table 10. F.6 Z INC 12K Product Graph Visualization In this subsection, we visualize the product graph derived from the first graph in the ZINC 12K dataset. Specifically, we present the right part of Figure 1, for the case of the real-world graphs in the ZINC 12K dataset. We perform this visualization for different cluster sizes, T ∈ {2, 3, 4, 5, 8, 12}, which also define the bag size, hence the notation T. The nodes in the product graph, T (G)□G, are (S, v), where S is the coarsened graph node (again a tuple), and v is the node index (of a node from the original graph). For better clarity, we color the nodes (S, v) with v ∈ S using different colors, while reserving the gray color exclusively for nodes (S, v) where v /∈ S. The product graphs are visualized in Figures 5 to 10 below. 29Table 9: Test results on the ZINC -12 K molecular dataset under 500k parameter budget. The top two results are reported as First and Second. Method Bag size ZINC (MAE ↓) GCN [19] T = 1 0.321 ± 0.009 GIN [36] T = 1 0.163 ± 0.004 OSAN [30] T = 2 0.177 ± 0.016 Random [20] T = 2 0.131 ± 0.005 PL [5] T = 2 0.120 ± 0.003 Mag-GNN [20] T = 2 0.106 ± 0.014 Ours T = 2 0.109 ± 0.005 Random [20] T = 3 0.124 ± N/A Mag-GNN [20] T = 3 0.104 ± N/A Ours T = 3 0.096 ± 0.005 Random [20] T = 4 0.125 ± N/A Mag-GNN [20] T = 4 0.101 ± N/A Ours T = 4 0.090 ± 0.003 Random [5] T = 5 0.113 ± 0.006 PL [5] T = 5 0.109 ± 0.005 Ours T = 5 0.095 ± 0.003 Random [5] T = 8 0.102 ± 0.003 PL [5] T = 8 0.097 ± 0.005 Ours T = 8 0.094 ± 0.006 Ours T = 18 0.082 ± 0.003 NGNN [40] Full 0.111±0.003 DS-GNN [4] Full 0.116±0.009 DSS-GNN [4] Full 0.102±0.003 GNN-AK [41] Full 0.105±0.010 GNN-AK+ [41] Full 0.091±0.002 SUN [12] Full 0.083±0.003 OSAN [30] Full 0.154±0.008 GNN-SSWL+ [39] Full 0.070 ± 0.005 Subgraphormer [3] Full 0.067 ± 0.007 Subgraphormer+PE [3] Full 0.063 ± 0.001 Ours Full 0.062 ± 0.0007 Table 10: Run time comparison over the ZINC -12 K dataset. Time taken at train for one epoch and at inference on the test set. All values are in milliseconds. Method Train time (for a single epoch; ms) Test time (ms) MAE ↓ GIN [36] 1370.10 ± 10.97 84.81 ± 0.26 0.163 ± 0.004 OSAN [30](T = 2) 2964.46 ± 30.36 227.93 ± 0.21 0.177 ± 0.016 PL [5] (T = 2) 2489.25 ± 9.42 150.38 ± 0.33 0.120 ± 0.003 Ours (T = 2) 2764.60 ± 234 383.14 ± 15.74 0.109 ± 0.005 30/uni25A1 = G/u1D4AF(G) /u1D4AF(G) /uni25A1G Figure 5: T = 2. /uni25A1 = G/u1D4AF(G) /u1D4AF(G) /uni25A1G Figure 6: T = 3. /uni25A1 = G/u1D4AF(G) /u1D4AF(G) /uni25A1G Figure 7: T = 4. 31/uni25A1 = G/u1D4AF(G) /u1D4AF(G) /uni25A1G Figure 8: T = 5. /uni25A1 = G/u1D4AF(G) /u1D4AF(G) /uni25A1G Figure 9: T = 8. /uni25A1 = G/u1D4AF(G) /u1D4AF(G) /uni25A1G Figure 10: T = 12. 32G Proofs G.1 Proofs of Appendix B We first state the memorization theorem, proven in [37] , which will be heavily used in a lot of the proofs in this section. Theorem G.1 (Memorization Theorem). Consider a dataset {xj, yj}N j=1 ∈ Rd × Rdy , with each xj being distinct and every yj ∈ {0, 1}dy . There exists a 4-layer fully connected ReLU neural network fθ : Rd → Rdy that perfectly maps each xj to its corresponding yj, i.e., fθ(xj) = yj for all j. We now restate and prove the propositions and lemmas of Appendix B. Lemma B.1 (Parameter Sharing as MPNN). Let B1, . . . Bk : Rn×n be orthogonal matrices with entries restricted to 0 or 1, and letW1, . . . Wk ∈ Rd×d′ denote a sequence of weight matrices. Define B+ = Pk i=1 Bi and choose z1, . . . zk ∈ Rd∗ to be a set of unique vectors representing an encoding of the index set. The function that represents an update via parameter sharing: f(X) = kX i=1 BiXWi, (18) can be implemented on any finite family of graphs G, by a stack of MPNN layers of the following form [13], ml v = X u∈NB+ (v) Ml(Xl u, eu,v), (19) Xl+1 v = Ul(Xl v, ml v), (20) where Ul, Ml are multilayer perceptrons (MLPs). The inputs to this MPNN are the adjacency matrix B+, node feature vector X, and edge features – the feature of edge (u, v) is given by: eu,v = kX i=1 zi · Bi(u, v). (21) Here, Bi(u, v) denotes the (u, v) entry to matrix Bi. Proof. Since we are concerned only with input graphsG from a finite family of graphs (where \"finite\" means that the maximal graph size is bounded and all possible node and edge feature values come from a finite set), we assume that for any v ∈ [n], i ∈ [k], both the input feature vectors Xv ∈ Rd and the encoding vectors zi ∈ Rd∗ are one-hot encoded. We aim to show that under these assumptions, any function f(·) of the form 95 can be realized through a single-layer update detailed in Equations 97 , 96, where M is a 4 layer MLP , and U is a single linear layer. The proof involves the following steps: 1. Compute [B1X, . . . , BkX] using the message function M. 2. Compute f(X) using the update function U. Step 1: We notice that for every i ∈ [k], v ∈ [n] we have: (BiX)v = X Bi(v,u)=1 Xu = X u∈NB+ (v) Xu · 1zi(eu,v). (100) Here 1zi is the indicator function of the set {zi}. We notice that since Xu and zi are one-hot encoded, there is a finite set of possible values for the pair (Xu, eu,v). In addition, the function: enc(Xu, eu,v) = [Xu · 1z1 (eu,v), . . . , Xu · 1zk (eu,v)] (101) outputs vectors in the set {0, 1}d×k. Thus, employing the memorization theorem G.1, we define a dataset {xj, yj}N j=1 by taking the xjs to be all possible (distinct) values of (Xu, eu,v) with each 33corresponding yi being the output enc(xi). We note that there are finitely many such values as both Xu and eu,v are one-hot encoded. The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network M such that: M(Xu, eu,v) = enc(Xu, eu,v). (102) and so, equation 100 implies: mv = X u∈NB+ (v) M(Xu, eu,v) = [(B1X)v, . . . ,(BkX)v]. (103) Step 2: Define Pi : Rk×d → Rd as the projection operator, extracting coordinates d · i + 1 through d · (i + 1) from its input vector: Pi(V ) = V |d·i+1:d·(i+1). (104) We define the update function to be the following linear map: U(Xv, mv) = kX i=1 Pi(mv)Wi. (105) Combining equations 103 and 105 we get: ˜Xv = U(Xv, mv) = kX i=1 (BiX)v · Wi = f(X)v. (106) Proposition B.1 (Equivalence of General Layer and Implemented Layer). Let T (·) be a coarsening function, π be a generalized node marking policy, andG be a finite family of graphs. Applying a stack of t general layer updates as defined in Equation 15 to the node feature map X(S, v) induced by π(G, T ), can be effectively implemented by applying a stack of t layer updates specified in Equations 29 and 30 to X(S, v). Additionally, the depths of all MLPs that appear in 29 and 30 can be bounded by 4. Proof. For convenience, let us first restate the general layer update: Xt+1(S, v) = ft \u0012 Xt(S, v), aggt 1{ {(Xt(S, v′), ev,v′) | v′ ∼G v} }, aggt 2{ {(Xt(S′, v), ˜eS,S′) | S′ ∼GT S} }, aggt 3{ {(Xt(S′, v), z(S, v, S′, v)) | S′ ∈ V T s.t. v ∈ S′} }, aggt 4{ {(Xt(S, v′), z(S, v, S, v′)) | v′ ∈ V s.t. v′ ∈ S} } \u0013 , (15) as well as the two step implemented layer update: Xt i (S, v) = Ut i  (1 + ϵt i) · Xt(S, v) + X (S′,v′)∼Ai(S,v) Mt(Xt(S′, v′) + ei(S, v, S′, v′))  . (29) Xt+1(S, v) = Ut fin  4X i=1 Xt i (S, v) ! . (30) We aim to demonstrate that any general layer, which updates the node feature map Xt(S, v) at layer t to node feature map Xt+1(S, v) at layer t + 1 as described in equation 15, can be effectively implemented using the layer update processes outlined in equations 29 and 30. 34As we are concerned only with input graphs belonging to the finite graph family G (where \"finite\" indicates that the maximal graph size is bounded and all node and edge features have a finite set of possible values), we assume that the values of the node feature map Xt(S, v) and the edge feature vectors ei(S, v, S′, v′) are represented as one-hot vectors in Rk. We also assume that the parameterized functions ft and aggt 1, . . .aggt 4, which are applied in Equation 15 outputs one-hot vectors. Finally, we assume that there exists integers d, d∗, such that the node feature map values are supported on coordinates 1, . . . d, the edge feature vectors are supported on coordinates d + 1, . . . d+ d∗, and coordinates d + d∗ + 1, . . . kare used as extra memory space, with: k > d× d∗ + d + d∗. (107) We note that the last assumption can be easily achieved using padding. The proof involves the following steps: 1. For i = 1, . . . ,4, Use the term: mt i = X (S′,v′)∼Ai(S,v) Mt(Xt(S′, v′) + ei(S, v, S′, v′)) (108) to uniquely encode: { {(Xt(S′, v′), ei(S, v, S′, v′)) | (S′, v′) ∼Ai (S, v)} }. (109) 2. Use the term: Xt ∗ = 4X i=1 Xt i (S, v) (110) to uniquely encode the input of ft as a whole. 3. Implement the parameterized function ft. Step 1: Since we assume that node feature map values and edge feature vectors are supported on orthogonal sub-spaces of Rk, the term: Xt(S, v) + ei(S, v, S′, v′) (111) uniquely encodes the value of the tuple: (Xt(S, v), ei(S, v, S′, v′)). (112) Since Xt(S, v) is a one-hot encoded vector with d possible values, while ei(S, v, S′, v′) is a one-hot encoded vector with d∗ possible values, their sum has d · d∗ possible values. Thus there exists a function: enc : Rk → Rk which encodes each such possible value as a one-hot vector in Rk supported on the last k − d − d∗ coordinates (this is possible because of equation 107). Now, employing theorem G.1, we define the xjs as all possible (distinct) values of 111, with each corresponding yj being the output enc(xj). The theorem now tells us that there exists a 4-layer fully connected ReLU neural network capable of implementing the function enc(·). We choose Mt to be this network. Now since mt i, defined in equation 108 is a sum of one-hot encoded vectors, it effectively counts the number of each possible value in the set 109. This proves step 1. Step 2: First, we note that: { {(Xt(S, v′), ev,v′) | v′ ∼G v} } = { {(Xt(S′, v′), eG(S, v, S′, v′)) | (S, v) ∼AG (S′, v′)} } (113) { {(Xt(S′, v), es′,s) | S′ ∼T (G) S} } = { {(Xt(S′, v′), eT (G)(S, v, S′, v′)) | (S, v) ∼AT (G) (S′, v′)} } (114) { {(Xt(S′, v), z(S, v, S′, v′)) | v ∈ S′} } = { {(Xt(S′, v′), eP1 (S, v, S′, v′)) | (S, v) ∼AP1 (S′, v′)} } (115) 35{ {(Xt(S, v′), z(S, v, S′, v′)) | v′ ∈ S} } = { {(Xt(S′, v′), eP2 (S, v, S′, v′)) | (S, v) ∼AP2 (S′, v′)} } (116) Now, since mt i and Xt(S, v) are supported on orthogonal sub-spaces of Rk, the sum Xt(S, v) + mt i uniquely encodes the value of: \u0000 Xt(S, v), { {(Xt(s, v), ei(S, v, S′, v′)) | (S, v) ∼Ai (S,′ v′)} } \u0001 . (117) Thus, we choose ϵt 1, . . . , epsilont 4 to be all zeroes. To compute the aggregation functions aggt 1, . . . ,aggt 4 using these unique encodings, and to avoid repetition of the value Xt(S, v), we define auxiliary functions ˜aggt i : Rk → Rki for i = 1, . . . ,4 as follows: ˜aggt 1(Xt(S, v) + mt 1) = \u0000 Xt(S, v), aggt 1{ {(Xt(S, v′), e1(S, v, S′, v′)) | (S, v) ∼A1 (S,′ v′)} } \u0001 (118) and for i >1: ˜aggt i(Xt(S, v) + mt i) = aggl i{ {(Xt(s, v), ei(S, v, S′, v′)) | (S, v) ∼Ai (S,′ v′)} }. (119) Here, since we avoided repeating the value of Xt(S, v) by only adding it to the output of ˜aggt 1(·), the expression: \u0000 ˜aggt 1(Xt(S, v) + mt 1), . . . ,˜aggt 4(Xt(S, v) + mt 4) \u0001 (120) is exactly equal to the input of ft. In addition, since the function aggt i outputs one-hot encoded vectors, and the vector Xt(S, v) is one-hot encoded, the output of ˜aggt i is always within the set {0, 1}ki. Now for any input vector X ∈ Rk define: V t 1 (X) = ( ˜aggt 1(X), 0k2 , 0k3 , 0k4 ). (121) V t 2 (X) = (0k1 , ˜aggt 2(X), 0k3 , 0k4 ). (122) V t 3 (X) = (0k1 , 0k2 , ˜aggt 3(X), 0k4 ). (123) V t 4 (X) = (0k1 , 0k2 , 0k3 , ˜aggt 4(X)). (124) We note that since the output of aggt i is always within the set {0, 1}ki, the outputs of V t i is always within {0, 1}k1+···+k4 . Now for i = 1, . . .4, employing theorem G.1 we define a dataset {xj, yj}N j=1 by taking the xjs as all possible (distinct) values of Xt(S, v) + mt i, with each corresponding yj being the output V t i (xj). We note that there are finitely many such values as both Xt(S, v) and mt i are one-hot encoded vectors. The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network capable of implementing the function V t i (·). We choose Ut i to be this network. Equations 121 - 124 now give us: 4X i=1 Xt i (S, v) = \u0000 ˜aggt 1(Xt(S, v) + mt 1), . . . ,˜aggt 4(Xt(S, v) + mt 4) \u0001 . (125) which as stated before, is exactly the input to ft. This proves step 2. Step 3: We employ theorem G.1 for one final time, defining a dataset {xj, yj}N j=1 by taking the xjs as all possible(distinct) values of: 4X i=1 Xt i (S, v) (which we showed is a unique encoding to the input of ft(·)), with each corresponding yj being the output ft(xj). We note that Given the finite nature of our graph set, there are finitely many such values. Recalling that ft(·) outputs one-hot encoded vectors, The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network capable of implementing the function ft(·). We choose Ut fin to be this network. This completes the proof. 36G.2 Proofs of Appendix C Proposition C.1 (Equal Expressivity of Node Marking Policies). For any coarsening function T (·) the following holds: CS-GNN(T , πS) = CS-GNN(T , πSS) = CS-GNN(T , πMD). (36) Proof. Let Π = {πS, πSS, πMD} be the set of all relevant node initialization policies, and assume for simplicity that our input graphs have no node features (the proof can be easily adjusted to account for the general case). For each π ∈ Π, let Xπ(S, v) denote the node feature map induced by general node marking policy π, as per Definition C.1. We notice it is enough to prove for each π1, π2 ∈ Π that Xπ1 (S, v) can be implemented by updating Xπ2 (S, v) using a stack of T layers of type 54. Thus, we prove the following four cases: • Node + Size Marking ⇒ Simple Node Marking. • Minimum Distance ⇒ Simple Node Marking. • Simple Node Marking ⇒ Node + Size Marking. • Simple Node Marking ⇒ Minimum Distance. Node + Size Marking ⇒ Simple Node Marking: In this case, we aim to update the node feature map: X0(S, v) = XπSS (S, v) = \u001a(1, |S|) v ∈ S (0, |S|) v /∈ S. (126) We notice that: X0(S, v) = ⟨(1, 0), XπS (S, v)⟩, (127) where ⟨·, ·⟩ denotes the standard inner product in R2. Using a CS-GNN update as per equation 15, with the update function: f1(X0(S, v), ·, ·, ·, ·) = ⟨(1, 0), X0(S, v)⟩, (128) where f(a, ·, ·, ·, ·) indicates that the function f depends solely on the parameter a, we obtain: X1(S, v) = f1(X0(S, v), ·, ·, ·, ·) = XπS (S, v). (129) This implies that for any coarsening function T (·), the following holds: CS-GNN(T , πS) ⊆ CS-GNN(T , πSS). (130) Minimum Distance ⇒ Simple Node Marking: In this case, we aim to update the node feature map: X0(S, v) = XπMD (S, v) = min v∈s dG(u, v) (131) We notice that: XS(S, v) = g(X0(S, v)) (132) where g : R → R is any continuous function such that: 1. g(x) = 1 ∀x >1 2 , 2. g(x) = 0 ∀x <1 4 . Using a CS-GNN update as per equation 15, with the update function: f1(X0(S, v), ·, ·, ·, ·) = g(X0(S, v)), (133) we obtain: X1(S, v) = f1(X0(S, v), ·, ·, ·, ·) = XπS (S, v). (134) 37This implies that for any coarsening function T (·) the following holds: CS-GNN(T , πS) ⊆ CS-GNN(T , πMD). (135) Simple Node Marking ⇒ Node + Size Marking: In this case, we aim to update the node feature map: X0(S, v) = XπS (S, v) = \u001a1 v ∈ S 0 v /∈ S. (136) We notice that: X v′∈S X0(S, v′) = |S|. (137) Using a CS-GNN update as per Equation (15), with aggregation function: aggl 4{ {(X0(S, v′), z(S, v, S, v′)) | v′ ∈ S} }= X v′∈S X0(S, v′), (138) and update function: f1   X0(S, v), ·, ·, ·, X v′∈S X0(S, v′) ! =   X0(S, v), X v′∈S X0(S, v′) ! , (139) we obtain: X1(S, v) = f1   X0(S, v), ·, ·, ·, X v′∈S X0(S, v′) ! = XπSS (S, v). (140) This implies that for any coarsening function T (·) the following holds: CS-GNN(T , πSS) ⊆ CS-GNN(T , πS). (141) Simple Node Marking ⇒ Minimum Distance: In this case, we aim to update the node feature map: X0(S, v) = XπS (S, v) = \u001a1 v ∈ S 0 v /∈ S. (142) We shall prove thatXπMD can be expressed by updating X0(S, v) with a stack of CS-GNN layers. We do this by inductively showing that this procedure can express the following auxiliary node feature maps: Xt ∗(S, v) = \u001aminv′∈S dG(v, v′) + 1 min v′∈S dG(v, v′) ≤ t 0 otherwise. (143) We notice first that: X0(S, v) = X0 ∗ (S, v). (144) Now for the induction step, assume that there exists a stack of t CS-GNN layers such that: Xt(S, v) = Xt ∗(S, v). (145) We observe that equation: min v′∈S dG(v, v′) = t + 1 (146) holds if and only if the following two conditions are met: min v′∈S dG(v, v′) > t (147) ∃u ∈ NG(v) s.t. min u′∈S dG(u, u′) = t. (148) Equations 143 imply: min v′∈S dG(v, v′) > t⇔ Xt(S, v) = 0. (149) 38In addition, since the node feature map Xt = Xt ∗ is bounded by t + 1, Equation (143) implies: ∃u ∈ NG(v) s.t. min u′∈S dG(u, u′) = t ⇔ max{Xt(s, u) | v ∼G u} = t + 1. (150) Now, let gt : R2 → R be any continuous function such that for every pair of natural numbers a, b∈ N: 1. gt(a, b) = t + 2 if a = 0, b= t + 1, 2. gt(a, b) = a otherwise. Equations 146 - 150 imply: Xt+1 ∗ (S, v) = gt(Xt(S, v), max{Xt(s, u) | v ∼G u}). (151) Using a CS-GNN update as per Equation (15), with aggregation function: aggt 1{ {(Xt(S, v′), ev,v′) | v′ ∼G v} }= max v′∼Gv Xt(S, v′). (152) and update function: ft(Xt(S, v), max v′∼Gv Xt(S, v′), ·, ·, ·) = gt(Xt(S, v), max v′∼Gv Xt(S, v′)) (153) we obtain: Xt+1(S, v) = ft(Xt(S, v), max v′∼Gv Xt(S, v′), ·, ·, ·) = Xt+1 ∗ (S, v). (154) This completes the induction step. Now, let G be a finite family of graphs, whose maximal vertex size is n. We notice that: XπMD (S, v) = Xn ∗ (S, v) − 1, (155) Which implies that there exists a stack of n CS-GNN layers such that: X0(S, v) = XπS (S, v) and Xn(S, v) = XπMD (S, v). (156) This implies: CS-GNN(T , πMD) ⊆ CS-GNN(T , πS). (157) This concludes the proof. Figure 11: Graphs G and H defined in the proof of Proposition C.2. In each graph, the circle marks the single super-node induced by T , while the number next to each node u is the maximal SPD between u and the nodes that compose the super-node. 39Proposition C.2 (Expressivity of Learned Distance Policy). For any coarsening functionT (·) the following holds: CS-GNN(T , πS) ⊆ CS-GNN(T , πLD). (37) In addition, for some choices of T (·) the containment is strict. Proof. First, since we are concerned with input graphs belonging to a finite graph family G, the learned function ϕ(·) implemented by an MLP can express any continuous function on G. This follows from Theorem G.1 (see the proof of Proposition B.1 for details). By choosing ϕ = min(·) in equation 35, it is clear that for any coarsening function T (·) we have: CS-GNN(T , πS) = CS-GNN(T , πMD) ⊆ CS-GNN(T , πLD). (158) We now construct a coarsening function T (·) along with two graphs, G and H, and demonstrate that there exists a function in CS-GNN(T , πLD) that can separate G and H. However, every function in CS-GNN(T , πS) cannot separate the two. For an input graph G = (V, E) define: T (G) = {{u ∈ V | degG(u) = 3}}. (159) i.e., T (·) returns a single super-node composed of all nodes with degree 3. Now, defineG = (VG, EG) as the graph obtained by connecting two cycles of size four by adding an edge between a single node from each cycle. Additionally, define H = (VH, EH) as the graph formed by joining two cycles of size five along one of their edges. See Figure 11 for an illustration of the two graphs. By choosing ϕ = max(·) in equation 35 a quick calculation shows that: X S∈VT (G) X v∈VG XπLD (S, v) = 16, (160) while: X S∈VT (H) X v∈VH XπLD (S, v) = 14. (161) Refer to Figure 11 for more details. Observe that: f(G) = X s∈VT (H) X u∈VH XπLD (S, v) ∈ CS-GNN(T , πLD) (162) Thus it is enough to show that: f(G) = f(H), ∀f ∈ CS-GNN(T , πS). (163) To achieve this, we use the layer update as per Definition B.2, which was demonstrated in Proposi- tion B.1 to be equivalent to the general equivariant message passing update in Definition A.5. First, we observe that the graphs G and H are WL-indistinguishable. We then observe that since |V T | = 1, the graphs induced by the adjacency matrices AG and AH in Definition B.1 are isomorphic to the original graphs G and H, respectively, and therefore they are also WL-indistinguishable. Additionally, we notice that the graphs induced by the adjacency matrices AT (G) and AT (H) in Definition B.1 are both isomorphic to the fully disconnected graph with 8 nodes, making them WL-indistinguishable as well. We also observe that there exists a bijection σ : VG → VH that maps all nodes of degree 3 in G to all nodes of degree 3 in H. The definition of T (·) implies that σ is an isomorphism between the adjacency matrices APi corresponding to G and H, where i = 1, 2. Finally, we notice that for both G, and H, the node feature map induced by πS satisfies: XπS (S, v) = deg(v) − 2. (164) This node feature map can be easily implemented by the layer update in definition B.2 and so it can be ignored. Since all four graphs corresponding to G that are induced by the adjacency matrices in Definition B.1, are WL-indistinguishable from their counterpart corresponding to H, and equation 29 in definition B.2 is an MPNN update, which is incapable of distinguishing graphs that are WL-indistinguishable, we see that equation 163 holds, concluding the proof. 40Proposition C.3 (Node + Size Marking as Invariant Marking). Given a graph G = (V, E) with node feature vector X ∈ Rn×d, and a coarsening function T (·), let XπSS , Xπinv be the node feature maps induced by πSS and πinv respectively. Recall that: XπSS (S, v) = [Xv, bπSS (S, v)], (43) Xπinv (S, v) = [Xv, bπinv (S, v)]. (44) The following now holds: bπinv (S, v) = OHE(bπSS (S, v)) ∀S ∈ V T , ∀v ∈ V. (45) Here, OHE denotes a one-hot encoder, independent of the choice of bothG and T . Proof. Let G = (V, E) be a graph with V = [n], and let T (·) be a coarsening function. Recall that the maps bπSS (·, ·) and bπinv (·, ·) are both independent of the connectivity of G and are defined as follows: bπSS (S, v) = \u001a(1, |S|) v ∈ S, (0, |S|) v /∈ S. (165) bπinv (S, v) = [1γ1 (S, v), . . . ,1γk (S, v)]. (166) Here, v ∈ [n], S ∈ T([n]) ⊆ P([n]), γ1, . . . , γk is any enumeration of the set of all orbits (P([n]) × [n])/Sn, and 1γi denotes the indicator function of orbit γi. Since any tuple (S, v) ∈ P([n]) × [n] belongs to exactly one orbit γi, we note that the right hand side of Equation (166) is a one-hot encoded vector. Thus, it suffices to show that for every v, v′ ∈ [n] and S, S′ ∈ P([n]), we have: bπSS (S, v) = bπSS (S,′ v′) ⇔ bπinv (S, v) = bπinv (S,′ v′). (167) This is equivalent to: (P([n]) × [n])/Sn = {{(S, v) | |S| = i, 1S(v) = j} |i ∈ [n], j∈ {0, 1}}. (168) Essentially, this means that each orbit corresponds to a choice of the size of s and whether v ∈ S or not. To conclude the proof, it remains to show that for any two pairs (S, v), (S,′ v′) ∈ P([n]) × [n], there exists a permutation σ ∈ Sn such that: σ · (S, v) = (S,′ v′) (169) if and only if |S| = |S′| and 1S(v) = 1S′(v′). (170) Assume first that σ · (S, v) = (S′, v′), then σ−1(S) = S′ and since σ is a bijection, |S| = |S′|. In addition σ−1(v) = v′ thus: v ∈ S ⇔ v′ = σ−1(v) ∈ σ−1(S) = S′. (171) Assume now that: |S| = |S′| (172) 1S(v) = 1S′(v′) (173) It follows that for some r, m∈ [n]: |S \\ {v}| = |S′ \\ {v′}| = r and |[n] \\ (S ∪ {v})| = |[n] \\ (S′ ∪ {v′})| = m (174) Write: S \\ {v} = {i1, . . . , ir}, S ′ \\ {v′} = {i′ 1, . . . , i′ r}, [n] \\ (S ∪ {v}) = {j1, . . . jm}, [n] \\ (S′ ∪ {v′}) = {j′ 1, . . . j′ m} and define: σ(x) =    v′ x = v i′ l x = il, l∈ [r] j′ l x = jl, l∈ [m] (175) We now have: σ · (S, v) = (S′, v′). (176) This concludes the proof. 41G.3 Proofs of Appendix D.1 Proposition D.1 (CS-GNN Can Implement MSGNN). Let T (·) be the identity coarsening function defined by: T (G) = {{v} |v ∈ V } ∀G = (V, E). (49) The following holds: CS-GNN(T , πS) = MSGNN(πNM). (50) Proof. Abusing notation, for a given graph G = (V, E) we write T (G) = G, V T = V . First, we observe that: v ∈ {u} ⇔u = v, (177) This implies that the initial node feature map X0(u, v) induced by πS is equivalent to the standard node marking described in equation 46. Additionally, we note that the pooling procedures for both models, as described in equations 16 and 55, are identical. Therefore, it is sufficient to show that the CS-GNN and MSGNN layer updates described in equations 15 and 47 respectively are also identical. For this purpose, let Xt(v, u) be a node feature map supported on the set V × V . The inputs to the MSGNN layer are the following: 1. Xt(u, v). 2. Xt(u, u). 3. Xt(v, v). 4. agg t 1{ {(Xt(u, v′), ev,v′) | v′ ∼ v} }. 5. agg t 2{ {(Xt(u′, v), eu,u′) | u′ ∼ u} }. The inputs to the CS-GNN layer are the following: 1. Xt(S, v) ⇒ Xt(u, v). 2. agg t 1{ {(Xt(S, v′), ev,v′) | v′ ∼G v} } ⇒aggt 1{ {(Xt(u, v′), ev,v′) | v′ ∼ v} }. 3. agg t 2{ {(Xt(S′, v), ˜eS,S′) | S′ ∼GT S} } ⇒aggt 2{ {(Xt(u, u′), eu,v′) | v′ ∼ v} }. 4. agg t 3{ {(Xt(S′, v), z(S, v, S′, v)) | ∀s′ ∈ V T s.t. v ∈ S′} } ⇒ { {(Xt(v, v), z(u, v, v, v))} }. 5. agg t 4{ {(Xt(S, v′), z(S, v, S, v′)) | ∀u′ ∈ V s.t. v′ ∈ S} } ⇒ { {(Xt(u, u), z(u, v, u, u))} }. The terms z(u, v, v, v) and z(u, v, u, u) appearing in the last two input terms of the CS-GNN layer uniquely encode the orbit tuples (u, v, v, v) and (u, v, u, u) belong to respectively. Since these orbits depend solely on whether u = v, these values are equivalent to the node marking feature map X0(u, v). Therefore, these terms can be ignored. Observing the two lists above, we see that the inputs to both update layers are identical (ignoring the z(·) terms), Thus, as both updates act on these inputs in the same way, the updates themselves are identical. and so MSGNN(πNM) = CS-GNN(T , πS). (178) G.4 Proofs of Appendix D.2 Proposition D.2 (CS-GNN Is at Least as Expressive as Coarse MPNN ). For any coarsening function T (·) the following holds: MPNN ⊆ MPNN+(T ) ⊆ CS-GNN(T , πS) (58) 42Proof. For convenience, let us first restate the CS-GNN layer update: Xt+1(S, v) = ft \u0012 Xt(S, v), aggt 1{ {(Xt(S, v′), ev,v′) | v′ ∼G v} }, aggt 2{ {(Xt(S′, v), ˜eS,S′) | S′ ∼GT S} }, aggt 3{ {(Xt(S′, v), z(S, v, S′, v)) | s′ ∈ V T s.t. v ∈ S′} }, aggt 4{ {(Xt(S, v′), z(S, v, S, v′)) | u′ ∈ V s.t. v′ ∈ S} } \u0013 , (15) as well as the MPNN+ layer update: For v ∈ V : Xt+1(v) = ft V \u0000 Xt(v), aggt 1{ {(Xt(v′), ev,v′) | v ∼G v′} }, aggt 2{ {Xt(S) | S ∈ V T , v∈ S} } \u0001 , For S ∈ V T : Xt+1(S) = ft V T \u0000 Xt(S), aggt 1{ {(Xt(S′), eS,S′) | S ∼GT S′} }, aggt 2{ {Xt(v) | v ∈ V, v∈ S \t } }). (54) We note that by setting ft V T to be a constant zero and choosing ft V to be any continuous function that depends only on its first two arguments, the update in equation 54 becomes a standard MPNN layer. This proves: MPNN ⊆ MPNN+(T). (179) Next, we prove the following 2 Lemmas: Lemma G.1. Given a graph G = (V, E) such that V = [n] with node feature vector X ∈ Rn×d, and a coarsening function T (·), there exists a CS-GNN(T , πS) layer such that: X1(S, v) = [0d+1, Xv, 1] = [ ˜X0(S), ˜X0(v)]. (180) Here [·, ·] denotes concatenation and ˜X0(·) denotes the initial node feature map of the coarsened sum graph GT +. Lemma G.2. Let ˜Xt(·) denote the node feature maps of GT + at layers t of a stack of MPNN+(T ) layers. There exists a stack of t + 1 CS-GNN(T , πS) layers such that: Xt+1(S, v) = [ ˜Xt(S), ˜Xt(v)]. (181) proof of Lemma G.1. Recall that the initial node feature map of CS-GNN(T , πS) is given by: X0(S, v) = \u001a[Xv, 1] v ∈ S [Xv, 0] v /∈ S. (182) In addition, the initial node feature map of MPNN+(T ) is given by: ˜X0(v) = \u001a[Xv, 1] v ∈ V 0d+1 v ∈ V T . (183) Thus, we choose a layer update as described in equation 15 with: X1(S, v) = f0(X0(S, v), ·, ·, ·, ·) = [0d+1, X0(S, v)1:d, 1] (184) Here, f(a, ·, ·, ·) denotes that the function depends only on the parameter a, and Xa:b indicates that only the coordinates a through b of the vector X are taken. This gives us: X1(S, v) = [ ˜X0(S), ˜X0(v)]. (185) 43proof of Lemma G.2. We prove this Lemma by induction on t. We note that Lemma G.1 provides the base case t = 0. Assume now that for a given stack of t + 1 MPNN+(T ) layer updates, with corresponding node feature maps: ˜Xi : V T + → Rdi i = 1 . . . , t+ 1, (186) there exists a stack of t + 1 CS-GNN(T , πS) layers with node feature maps: Xi : V T × V → R2di i = 1, . . . , t+ 1, (187) such that: Xt+1(S, v) = [ ˜Xt(S), ˜Xt(v)]. (188) We shall show that there exists a single additional CS-GNN(T , πS) layer update such that: Xt+2(S, v) = [ ˜Xt+1(S), ˜Xt+1(v)]. (189) For that purpose we define the following CS-GNN(T , πS) update (abusing notation, the left hand side refers to components of the CS-GNN(T , πS) update at layer t + 1, while the right hand side refers to components of the MPNN+(T ) update at layer t): aggt+1 1 = aggt 11:dt, aggt+1 2 = aggt 1dt+1:2dt, aggt+1 3 = aggt 21:dt, aggt+1 4 = aggt 2dt+1:2dt, (190) ft+1(a, b, c, d, e) = [ft V (a1:dt, b, d), ft V T (adt+1:2dt, c, e)]. (191) Here the operation agga:b initially projects all vectors in the input multi-set onto coordinatesa through b, and subsequently passes them to the function agg. equations 190 , 191 guarantee that: Xt+2(S, v)1:dt+1 = ft V \u0000 Xt(S, v)1:dt, aggt 1{ {(S, v′)1:dt | v ∼G v′} }, aggt 2{ {(S′, v)1:dt | v ∈ S′} } \u0001 = ˜Xt+1(v), Xt+2(S, v)dt+1+1:2dt+1 = ft V T \u0000 Xt(S, v)dt+1:2dt, aggt 1{ {(S′, v)dt+1:2dt | S′ ∼T (G) S} }, aggt 2{ {(S, v′)dt+1:2dt | v′ ∈ S} } \u0001 = ˜Xt+1(S). (192) This proves the Lemma. Now, for a given finite family of graphsG and a function f ∈ MPNN+(T ), there exists a stack of T MPNN+(T ) layers such that: f(G) = U   X v∈V T + ˜XT (v)   ∀G ∈ G. (193) Here, ˜XT : V T + → RdT denotes the final node feature map, and U is an MLP. Lemma G.2 now tells us that there exists a stack of T + 1 CS-GNN(T , πS) layers such that: XT+1(S, v) = [ ˜XT (S), ˜XT (v)]. (194) Similarly to Lemma G.1, we use one additional layer to pad XT+1(S, v) as follows: XT+2(S, v) = [ ˜XT (S), ˜XT (v), 1]. (195) 44We notice that: X s∈V T XT+2(S, v) = \" X S∈V T ˜XT (S), X S∈V T ˜XT (v), X S∈V T 1 # = \" X S∈V T ˜XT (S), |V T | ·˜XT (v), |V T | # . (196) Thus, in order to get rid of the |V T | term, We define: MLP1(a, b, c) = [a, 1 c · b, 1], a, b ∈ RdL, c >0. (197) We note that since we are restricted to a finite family of input graphs, the use of an MLP in equation 200 can be justified using Theorem G.1 (see the proof of Proposition B.1 for a detailed explanation). Equations 196 and 200 imply: MLP1  X s∈V T XT+2(S, v) ! = \" X S∈V T ˜XT (S), ˜XT (v), 1 # (198) Thus, similarly to equation 196: X v∈V MLP1  X S∈V T XT+2(S, v) ! = \" |V | · X S∈V T ˜XT (S), X v∈V ˜XT (v), |V | # (199) And so, in order to get rid of the |V | term, We define: MLP2(a, b, c) = U(a · 1 c + b, 1), a, b ∈ RdT , c >0. (200) Thus for all G ∈ G: MLP2  X v∈V MLP1  X S∈V T XT+2(S, v) !! = MLP2  \" |V | · X S∈V T ˜XT (S), X v∈V ˜XT (v), |V | #! = U   X v∈V T + ˜XT (v)   = f(G). (201) and so f ∈ CS-GNN(T , πS). This proves: MPNN+(T) ⊆ CS-GNN(T , πS). (202) Proposition D.3 (CS-GNN Can Be More Expressive Than MPNN +). Let T (·) be the identity coarsening function defined by: T (G) = {{v} |v ∈ V } G = (V, E). (59) The following holds: MPNN = MPNN+(T ). (60) Thus: MPNN+(T ) ⊂ CS-GNN(T , πS), (61) where this containment is strict. 45Proof. First, using the notation ˜v to mark the single element set {v} ∈V T , We notice that the MPNN+(T ) layer update described in equation 54, becomes: For v ∈ V : Xt+1(v) = ft V \u0012 Xt(v), Xt(˜v), aggt{ {(Xt(v′), ev,v′) | v′ ∼G v} }, \u0013 , For ˜v ∈ V T : Xt+1(˜v) = ft V T \u0012 Xt(˜v), Xt(v), aggt{ {(Xt( ˜v′), e˜v, ˜v′) | v ∼G v′} } \u0013 . (203) Now, for a given finite family of graphsG and a function f ∈ MPNN+(T ), there exists a stack of T MPNN+(T ) layers such that: f(G) = U   X v∈V T + XT (v)   ∀G ∈ G. (204) Here, XT : V T + → Rd denotes the final node feature map, and U is an MPL. We now prove by induction on t that there exists a stack of t standard MPNN layers, with corresponding node feature map Xt : V → R2dt such that : Xt(v) = [Xt(v), Xt(˜v)]. (205) Here, [·, ·] stands for concatenation. We assume for simplicity that the input graph G does not have node features, though the proof can be easily adapted for the more general case. We notice that for the base case t = 0, equation 53 in definition D.2 implies: X0(v) = \u001a1 v ∈ V, 0 v ∈ V T . (206) Thus, we define: X0(v) = (1, 0). (207) This satisfies Equation (205), establishing the base case of the induction. Assume now that Equa- tion (205) holds for some t ∈ [T]. Let aggt, ft V , ft V T be the components of layer t, as in equation 203. We define: ˜aggt = [aggt|1:dt, aggt|dt+1:2dt]. (208) Here the operation agga:b initially projects all vectors in the input multi-set onto coordinatesa through b, and subsequently passes them to the function agg. Additionally, let d∗ denote the dimension of the output of the function aggt. We define: ˜ft(a, b) = \u0002 ft V (a|1:dt, a|dt+1:2dt, b|1:d∗) , ft V (a|dt+1:2dt, a|1:dt, b|d∗+1:2d∗) \u0003 . (209) Finally, we update our node feature map Xt using a standard MPNN update according to: Xt+1(v) = ˜fl \u0000 Xt(v), { {(Xt(v′), ev,v′) | v′ ∼G v} } \u0001 . (210) equations 203, 205 and 210 now guarantee that: Xt+1(v) = [Xt(v), Xt+1(˜v)]. (211) This concludes the inductive proof. We now define: MLP(x) = U(x|1:dT ) + U(x|dT +1:2dT ). (212) This gives us: U \u0012 X v∈V T + XT (v) \u0013 = MLP \u0012 X v∈V XT (v) \u0013 = f(G). (213) We have thus proven thatf ∈ MPNN and so: MPNN+(N) ⊆ MPNN. (214) Combining this result with Proposition D.2, we obtain: MPNN = MPNN+(T ). (215) 46Finally, since Proposition D.1 tells us that CS-GNN(T , πS) has the same implementation power as the maximally expressive node policy subgraph architecture MSGNN, which is proven to be strictly more expressive than the standard MPNN, we have: MPNN+(T ) ⊂ CS-GNN(T , πS). (216) Proposition D.4 (CS-GNN can be strictly more expressive then node-based subgraph GNNs). Let T be the coarsening function defined by: T (G) = {{v} |v ∈ V } ∪E G = (V, E). (63) The following holds: 1. Let G1, G2 be a pair of graphs such that there exists a node-based subgraph GNN model M where M(G1) ̸= M(G2). There exists a CS-GNNmodel M′ which uses T such that M′(G1) ̸= M′(G2). 2. There exists a pair of graphs G1, G2 such that for any subgraph GNN model M it holds that M(G1) = M(G2), but there exists a CS-GNNmodel M′ which uses T such that M′(G1) ̸= M′(G2). Proof. First, notice that the super-nodes produced by T are either of size 1, in which case they correspond to nodes, or they are of size two, in which case they correspond to edges. Since an CS-GNNmodel processes feature maps Xt(S, v) where in the initial layer the sset size of S is encoded in Xt(S, v), we can easily use the CS-GNNupdate in Definition A.5 to ignore all values of Xt(S, v) were |S| = 2 (This can be done by using ft, aggt 1, . . .aggt 1 in Definition A.5 to zero out these values at each update). This means CS-GNNusing T is able to simulate an CS-GNNupdate with the identity coarsening function, which was shown in Proposition D.1 to be as expressive as GNN-SSWL+ (Definition D.1) which is a maximally expressive node-based subgraph GNN, thus proving part (1) of the proposition. To prove part (2), notice that using the same reasoning as before, an CS-GNNmodel using T as a coarsening function cal implement an CS-GNNmodel using the edge coarsening function: T ′(G) = E G = (V, E). (217) An CS-GNNmodel with the identity coarsening function can be interpreted as a GNN-SSWL+ model. Similarly, an CS-GNNmodel using the edge coarsening function T ′ generalizes the GNN-SSWL+ framework by extending it from node-based subgraph GNNs to edge-based subgraph GNNs. In fact, the same proof in [39, 12] showing that GNN-SSWL+ is at least as expressive as a DSS subgraph GNN using the node deletion policy (see [ 4] for a definition of the DSS subgraph GNN), can be used to show that CS-GNNusing the edge coarsening function T ′ is at least as expressive as a DSS subgraph GNN with an edge deletion policy. The latter model was shown in [4] to be able to separate a pair of 3-WL indistinguishable graphs. In contrast, node-based subgraph GNNs were shown in [12] to not be able to separate any pair of 3-WL indistinguishable graphs. Thus, there exists a pair of graphs which CS-GNNusing T can separate while node-based subgraph GNNs cant, proving part (2) of the proposition. G.5 Proofs of Appendix E Lemma E.1 (γ (Γ) are orbits). The sets {γk∗ : k = 1, . . . , n; ∗ ∈ {+, −}} and {Γ↔;k1;k2;k∩;δsame;δdiff } are the orbits ofSn on the index space (P([n])×[n]) and (P([n])×[n]×(P([n])×[n]), respectively. Proof. We will prove this lemma for γ. The proof for Γ follows similar reasoning; we also refer the reader to [22] for a general proof. We will prove this lemma through the following three steps. (1). Given indices (S, i) ∈ P([n]) × [n], there exists γ ∈ (P([n]) × [n])∼ such that (S, i) ∈ γ. (2). Given indices (S, i) ∈ γ, for any σ ∈ Sn, it holds that (σ−1(S), σ−1(i)) ∈ γ. 47(3). Given (S, i) ∈ γ and (S′, i′) ∈ γ (the same γ), it holds that there exists a σ ∈ Sn such that σ · (S, i) = (S′, i′). We prove in what follows. (1). Given indices (S, i) ∈ P([n]) × [n], w.l.o.g. we assume that |S| = k, thus if i ∈ S (i /∈ S) it holds that (S, i) ∈ γk− \u0000 (S, i) ∈ γk+ \u0001 , recall Equation (71). (2). Given indices (S, i) ∈ γ, note that any permutation σ ∈ Sn does not change the cardinality of S nor the inclusion (or exclusion) of i in S. Recalling Equation (71), we complete this step. (3). Given that (S, i) ∈ γ and (S′, i′) ∈ γ, and recalling Equation (71), we note that |S| = |S′| and that either both i ∈ S and i′ ∈ S′, or both i /∈ S and i′ /∈ S′. (3.1). In (3.1) we focus on the case where i /∈ S and i′ /∈ S′. Let S = {i1, . . . , ik} and S′ = {i′ 1, . . . , i′ k}. Then, we have ({i1, . . . , ik}, j) and ({i′ 1, . . . , i′ k}, j′). Define σ ∈ Sn such that σ(il) = i′ l for l ∈ [k], and σ(j) = j′. Since ({i1, . . . , ik}, j) consists of k + 1 distinct indices and ({i′ 1, . . . , i′ k}, j′) also consists of k + 1 distinct indices, this is a valid σ ∈ Sn. (3.2). Here, we focus on the case where i ∈ S and i′ ∈ S′. This proof is similar to (3.1), but without considering the indices j and j′, as they are included in S and S′, respectively. Proposition E.1 (Basis of Invariant (Equivariant) Layer). The tensors Bγ (BΓ) in Equation (72) (Equation (74)) form an orthogonal basis (in the standard inner product) to the solution of Equa- tion (66) (Equation (67)). Proof. We prove this proposition for the invariant case. The equivariant case is proved similarly – we also refer the reader for [22] for a general proof. We will prove this in three steps, (1). For any γ ∈ (P([n]) × [n])∼ it holds that Bγ S,i solves Equation (66). (2). Given a solution L to Equation (66), it is a linear combination of the basis elements. (3). We show that the basis vectors are orthogonal and thus linearly independent. We prove in what follows. (1). Given γ ∈ (P([n]) × [n])∼, we need to show that Bγ S,i = Bγ σ−1(S),σ−1(i). Since any γ ∈ (P([n]) × [n])∼ is an orbit in the index space (recall Lemma E.1), and Bγ S,i are indicator vectors of the orbits this always holds. (2). Given a solution L to Equation (66), it must hold that LS,i = Lσ−1(S),σ−1(i). Since the set {γk∗ : k = 1, . . . , n; ∗ ∈ {+, −}} corresponds to the orbits in the index space with respect to Sn, L should have the same values over the index space of these orbits. Let’s define these values asαγ for each γ ∈ {γk∗ : k = 1, . . . , n; ∗ ∈ {+, −}}. Thus, we obtain that L′ = P γ∈(P([n])×[n])∼ αγ · Bγ, since Bγ are simply indicator vectors of the orbits. This completes this step. (3). Once again, since the basis elements are indicator vectors of disjoint orbits we obtain their orthogonality, and thus linearly independent. 48NeurIPS Paper Checklist The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: • You should answer [Yes] , [No] , or [NA] . • [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. • Please provide a short (1–2 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: • Delete this instruction block, but keep the section heading “NeurIPS paper checklist\", • Keep the checklist subsection headings, questions/answers and guidelines below. • Do not modify the questions and only use the provided macros for your answers. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The abstract spells out all the main contributions in the present paper, both theoretical and empirical ones. These are extensively discussed and recapitulated in the Introduction Section 1 (see paragraphs “Our approach” and “Contributions”). The scope of the paper is well defined in the first periods of the abstract and comprehensively articulated in the first two paragraphs of the Introduction Section 1. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations 49Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to paragraph “Limitations” in Section 6. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Please refer to Appendices B to E and F.4, which include precise and contextu- alized statements of all theoretical results and derivations, and to Appendix G for proofs thereof. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] 50Justification: Please refer to Appendix F.1 for a description of the employed datasets and splitting procedure, Appendices F.2 and F.3 for a list of experimental details and hyperparameter settings, and Appendix F.5 for a series of complementary results. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code to reproduce our results can be found in the following GitHub repository: https://github.com/BarSGuy/Efficient-Subgraph-GNNs. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. 51• The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Appendix F.1 for a description of the employed datasets and splitting procedure, Appendices F.2 and F.3 for a list of experimental details and hyperparameter settings, including the utilized training procedures. The results for baselines approaches are reported according to what stated in Section 5 and Appendix F.2. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our results in Section 5 are reported in terms of mean and standard deviation calculated over different model initializations (i.e., by setting different random seeds prior to code execution). Table 9 reports error bars for the results illustrated in Figure 2. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources 52Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The hardware employed to obtain all experimental results, as well as a runtime comparison, are described in Appendix F.2 (see “Implementation Details”). Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have made sure to comply to the Code of Ethics and to preserve our anonymity. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The models we developed are not generative, hence not posing risks of malicious use as for what concerns fabricating misleading or otherwise fake information, online profiles and media. Additionally, although our approach improves the efficiency of certain Graph Neural Networks, the models we developed are not scalable enough to apply and impact (online) social networks: represented as graphs, their scale is way beyond that considered in our experiments. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. 53• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our model(s) do not have a high risk of misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: As explicitly mentioned in the main corpus of the paper, the inset figure in Section 2 is taken with permission by the original authors. Creators of datasets employed in this study, as well as the benchmark frameworks used are properly referenced and cited. For these last we report license information in Appendix F.1, also reported for code assets Appendix F.2. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets 54Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 55",
      "meta_data": {
        "arxiv_id": "2406.09291v4",
        "authors": [
          "Guy Bar-Shalom",
          "Yam Eitan",
          "Fabrizio Frasca",
          "Haggai Maron"
        ],
        "published_date": "2024-06-13T16:29:06Z",
        "pdf_url": "https://arxiv.org/pdf/2406.09291v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a novel Subgraph GNN framework to address the scalability issues and suboptimal subgraph selection methods of existing approaches. Its main contributions are: (1) developing a flexible framework for constructing and processing bags of subgraphs of any size, achieved by associating subgraphs with node clusters via graph coarsening; (2) characterizing affine invariant/equivariant layers defined on the resulting node feature tensors; (3) providing a theoretical analysis of the framework, including the expressivity benefits of its node-marking strategy; and (4) demonstrating superior experimental performance across multiple graph learning benchmarks, outperforming baselines and matching state-of-the-art full-bag Subgraph GNNs for both small and large bag sizes.",
        "methodology": "The framework, called Coarsening-based Subgraph GNN (CS-GNN), transforms graphs by first coarsening the original graph G into T(G) (a graph of super-nodes, e.g., via spectral clustering) to control bag size. It then constructs a product graph T(G)□G using the Cartesian product, forming a new connectivity structure for message passing. Generalized message passing is performed on this product graph, integrating original graph connectivity, induced connectivity from the coarsened graph, and novel symmetry-based updates. These symmetry-based updates leverage identified permutation symmetries in the node feature tensor by characterizing and incorporating affine equivariant operations as additional edge features. Four node marking strategies are proposed (Simple, Node+Size, Minimum Distance, Learned Distance Function), with Learned Distance Function (πLD) chosen for implementation, using shortest path distances. The model uses GINE or GAT as base encoders and employs specific pooling layers for graph representation.",
        "experimental_setup": "Extensive experiments were conducted on seven datasets: ZINC-12K, ZINC-FULL (molecular property regression, MAE); OGBG-MOLHIV, OGBG-MOLBACE, OGBG-MOLESOL (molecular property prediction, AUROC/RMSE); and PEPTIDES-FUNC, PEPTIDES-STRUCT (peptide property prediction, Average Precision/MAE). Baselines included efficient Subgraph GNNs (OSAN, Random, PL, MAG-GNN) for small bags, full-bag Subgraph GNNs (GNN-SSWL+, Subgraphormer, etc.), and various MPNNs (GIN, GCN, GatedGCN). Validation involved reporting mean ± standard deviation over 3 runs with different random seeds. Hyperparameters (learning rate, dropout, embedding size, epochs, batch size, number of layers, Laplacian dimension, SPD dimension) were tuned using Weights and Biases. Optimizers like Adam with ReduceLROnPlateau, ASAM, AdamW with cosine annealing, and constant learning rates were used. Experiments were run on a single NVIDIA L40 GPU.",
        "limitations": "The method operates over a product graph. While control over the size of this product graph is provided, achieving better performance generally necessitates a larger bag size. This can lead to a complexity bottleneck, especially when dealing with large original graphs.",
        "future_research_directions": "Future work includes exploring alternative graph coarsening strategies beyond spectral clustering. Additionally, further evaluation of the impact of other basis elements from the derived equivariant basis is suggested, as only a portion was considered in the current work. Finally, investigating whether Higher-Order Subgraph GNNs can benefit from the developed parameter-sharing scheme remains an intriguing open question."
      }
    },
    {
      "title": "Efficient Attention via Control Variates",
      "abstract": "Random-feature-based attention (RFA) is an efficient approximation of softmax\nattention with linear runtime and space complexity. However, the approximation\ngap between RFA and conventional softmax attention is not well studied. Built\nupon previous progress of RFA, we characterize this gap through the lens of\ncontrol variates and show that RFA can be decomposed into a sum of multiple\ncontrol variate estimators for each element in the sequence. This new framework\nreveals that exact softmax attention can be recovered from RFA by manipulating\neach control variate. Besides, it allows us to develop a more flexible form of\ncontrol variates, resulting in a novel attention mechanism that significantly\nreduces the approximation gap while maintaining linear complexity. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art efficient\nattention mechanisms on both vision and language tasks.",
      "full_text": "Published as a conference paper at ICLR 2023 EFFICIENT ATTENTION VIA CONTROL VARIATES Lin Zheng1∗ Jianbo Yuan2 Chong Wang3∗ Lingpeng Kong1 1The University of Hong Kong 2ByteDance Inc. 3Apple Inc. {lzheng2,lpk}@cs.hku.hk jianbo.yuan@bytedance.com mr.chongwang@apple.com ABSTRACT Random-feature-based attention (RFA) is an efﬁcient approximation of softmax at- tention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control vari- ates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more ﬂexible form of control variates, resulting in a novel attention mechanism that signiﬁcantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efﬁcient attention mechanisms on both vision and language tasks.1 1 I NTRODUCTION Random-feature-based attention (RFA, also known as Performer; Choromanski et al., 2021; Peng et al., 2021b) is an established fast approximation to the conventional softmax attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017), which successfully scales Transformer models to processing much longer sequences (Choromanski et al., 2021). At its core is the usage of random features (RF; Rahimi & Recht, 2008) to linearize the exponential kernel in softmax attention, which reduces the computational cost from quadratic to linear runtime and space complexity. Despite its efﬁciency, recent studies have pointed out that such approximation suffers from substantial performance degeneration (Xiong et al., 2021a; Zheng et al., 2022b). In this work, we generalize the formulation of RFA via control variates (Owen, 2013), which characterizes the approximation gap between RFA and softmax attention in theory. We ﬁrst show that RFA can be decomposed from a global approximation over the whole sequence into a sum of local control variate estimators, each of which is applied to an individual element in the sequence. Under this formulation, RFA is equivalent to employing the same coefﬁcient for all control variate estimators to scale their variance isotropically (§3.1). Besides, we prove that if we optimize the coefﬁcient of each control variate to minimize the estimation variance individually, RFA estimation becomes exact, that is, softmax attention is recovered with zero bias and zero variance (§3.2). Our key observation is that such formulation reveals a localized perspective of the RFA approximation. Instead of directly seeking a better estimate over the entire sequence, we can break down the problem into smaller problems that aim at improving the approximation for each subsequence (§4). The control variate estimator for each subsequence can be tuned separately and combined to yield better estimation, which provably reduces approximation error in the global sense (§4.1). Nevertheless, one caveat is that as the number of sub-problems increases, the approximation gap will be reduced but at the expense of higher computational complexity. For instance, if we optimize the control variate for every single element, softmax attention would be recovered as desired but with quadratic complexity. To attain a good trade-off between approximation quality and efﬁciency, we develop a new Efﬁcient attention via control V Ariates (EV A) that implements this divide-and-conquer strategy efﬁciently. In EVA, the sequence is partitioned into a ﬁxed number of disjoint subsets. For the subset ∗The majority of this work was done while these authors were at Bytedance. 1Our code and models are available at this link. 1 arXiv:2302.04542v1  [cs.LG]  9 Feb 2023Published as a conference paper at ICLR 2023 that might bear the highest correlations to the query, we explicitly optimize the control variate for each element, which recovers exact softmax attention probabilities; while for the others, the control variate coefﬁcient is shared locally among all elements within the same subset. The resulting attention mechanism is not only highly effective but also runs with the same computational complexity as RFA (§4.2). Extensive experiments on both language and vision tasks demonstrate that EVA outperforms the state-of-the-art efﬁcient attention methods (§5). 2 B ACKGROUND 2.1 S OFTMAX ATTENTION MECHANISM Assume there exist a set of N queries {qn}N n=1 and M key-value pairs K = [k1,..., kM] and V = [v1,..., vM], where queries, keys and values are all d-dimensional vectors. The softmax attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) is deﬁned as an average over the value vectors weighted by the dot-product similarities of the queries and keys. For the n-th query, the attention mechanism outputs SoftmaxAttn(qn,K,V) := M∑ m=1 exp ( q⊤ nkm ) ∑M m′=1 exp (q⊤nkm′) vm. (1) In the case of self-attention (Lin et al., 2017; Vaswani et al., 2017), we haveM = N, which results in quadratic computational complexity since we have to compute the similarity for each query-key pair explicitly. 2.2 R ANDOM -FEATURE -BASED ATTENTION WITH SELF -NORMALIZED IMPORTANCE SAMPLING Recently, Zheng et al. (2022b) identiﬁes that softmax attention (Equation 1) can be written as an expectation over an attention-like aggregating function, SoftmaxAttn(qn,K,V) = M∑ m=1 exp ( q⊤ nkm ) ∑M m′=1 exp (q⊤nkm′) vm = Eω∼pn(ω) [fn(ω)] , (2) where fn(ω) := ∑M m=1 ξ(qn,ω)ξ(km,ω)vm ∑M m′=1 ξ(qn,ω)ξ(km′,ω) , p n(ω) := N(ω; 0,I) ∑M m=1 ξ(qn,ω)⊤ξ(km,ω) Z . (3) Here ξ(·,·) is the randomized mapping deﬁned in such a way that exp ( q⊤ nkm ) = Eω∼N(0,I) [ ξ(qn,ω)⊤ξ(km,ω) ] , and Z = ∑M m=1 exp ( q⊤ nkm ) denotes the normalizing con- stant of distribution pn. Throughout this paper, we consider the positive randomized mapping ξ(x,ω) = exp ( ω⊤x −1 2 ∥x∥2 ) (Choromanski et al., 2021) unless otherwise speciﬁed. Random-Feature-based Attention (RFA) methods (Choromanski et al., 2021; Peng et al., 2021b) can be interpreted as performing self-normalized importance sampling (SNIS; Hesterberg, 1995) to approximate Equation 2 (Zheng et al., 2022b). In SNIS, one draws Monte Carlo samples from some proposal distribution q(ω) instead of the true distribution pn(ω) and estimates the target expectation as Eω∼pn(ω) [fn(ω)] = Eω∼q(ω) [ pn(ω) q(ω) fn(ω) ] ≈ ∑S s=1 pn(ω) q(ω) fn(ωs) ∑S s=1 pn(ωs) q(ωs) , where ω1,...,ω S ∼q(ω). Vanilla RFA amounts to constructing the SNIS estimation with q(ω) = N(ω; 0,I). The SNIS representation also turns out equivalent to the more established form of RFA, RFA(qn,K,V) := ∑S s=1 pn(ωs) q(ωs) f(ωs) ∑S s=1 pn(ωs) q(ωs) = ∑M m=1 φ(qn,ω)⊤φ(km,ω)vm ∑M m′=1 φ(qn,ω)⊤φ(km′,ω) , (4) where the random feature, denoted by φ(x,ω) := 1/ √ S[ξ(x,ω1),...,ξ (x,ωS)]⊤, is proposed to approximate exponential kernels in its original motivation (see Appendix A for a detailed review). 2Published as a conference paper at ICLR 2023 2.3 C ONTROL VARIATES Control variates aim to reduce the estimation variance of an expectation E [g(ω)]. Assuming our original RFA estimation is g(ω) ∈Rd and there is some control variate h(ω) ∈R with a known expectation E [h(ω)], we can employ the control variate h(ω) with the coefﬁcient β∈Rd as follows, ˜g(ω) =g(ω) −βh(ω) +βE [h(ω)] (5) Note that the resulting estimator remains unbiased since E [˜g(ω)] = E [g(ω)] −βE [h(ω)] + βE [h(ω)] = E [g(ω)]. However, the estimation variance can be largely reduced if g(·) and the scaled control variate βh(ω) are positively correlated (Owen, 2013). 3 D ISSECTING RFA WITH CONTROL VARIATES In this section, we ﬁrst go through the connections among RFA, importance sampling, and control variates, revealing a decomposed formulation of RFA (§3.1), and then quantify the approximation gap between RFA and softmax attention (§3.2) from these connections. 3.1 RFA AS A SUM OF LOCAL CONTROL VARIATE ESTIMATORS As shown in Equation 4, RFA estimation considers all key-value pairs and produces a global approximation over the entire sequence. In contrast, our work develops a decomposed representation of RFA based on the recent advances in SNIS (Vlassis et al., 2021), which indicates that an SNIS estimate is asymptotically equivalent to a control variate estimate (the detailed derivations is deferred to Appendix B.2). In particular, we have ∑S s=1 pn(ωs) q(ωs) f(ωs) ∑S s=1 pn(ωs) q(ωs) = 1 S S∑ s=1 pn(ωs) q(ωs) f(ωs) − ∑S s=1 pn(ωs) q(ωs) f(ωs) ∑S s=1 pn(ωs) q(ωs) ( 1 S S∑ s=1 pn(ωs) q(ωs) −1 ) := g(ω) −ˆβ(ω) (h(ω) −E [h(ω)]) := ˜g(ω), (6) where g(ω) := 1 S ∑S s=1 pn(ωs) q(ωs) f(ωs) is our base estimate, h(ω) := 1 S ∑S s=1 pn(ωs) q(ωs) is the control variate with control coefﬁcient ˆβ(ω) := (∑S s=1 pn(ωs) q(ωs) f(ωs) )/(∑S s=1 pn(ωs) q(ωs) ) = g(ω) h(ω) . We now examine the formulation of g(·) and h(·) in the context of RFA. According to Equation 3, g(ω) = 1 S S∑ s=1 pn(ωs) q(ωs) f(ωs) = S∑ s=1 α(ωs) M∑ m=1 ξ(qn,ωs)ξ(km,ωs)vm, h(ω) = 1 S S∑ s=1 pn(ωs) q(ωs) = S∑ s=1 α(ωs) M∑ m=1 ξ(qn,ωs)ξ(km,ωs), where α(ωs) := 1 S N(ωs;0,I) Zq(ωs) collects terms that is constant w.r.t. queries, keys, and values. Our key observation is that by changing the order of summations, both g(·) and h(·) can be decomposed as g(ω) =∑M m=1 gm(ω) and h(ω) =∑M m=1 hm(ω) respectively, where gm(ω) = S∑ s=1 α(ωs)ξ(qn,ωs)ξ(km,ωs)vm, h m(ω) = S∑ s=1 α(ωs)ξ(qn,ωs)ξ(km,ωs). As a result, we can decompose the entire RFA estimate in Equation 6 into a summation of M control variate estimates following ˜g(ω) =g(ω) −ˆβ(ω) (h(ω) −E [h(ω)]) = ( M∑ m=1 gm(ω) ) −ˆβ(ω) (( M∑ m=1 hm(ω) ) −E [ M∑ m=1 hm(ω) ]) = M∑ m=1 gm(ω) −ˆβ(ω) (hm(ω) −E [hm(ω)]) := M∑ m=1 ˜gm(ω). (7) 3Published as a conference paper at ICLR 2023 Here ˜gm(ω) = gm(ω) −ˆβ(ω) (hm(ω) −E [hm(ω)]) denotes the corresponding control variate estimator of the m-th key-value pair,2 and ˆβ(ω) is the coefﬁcient shared across the entire sequence. 3.2 O PTIMIZING COEFFICIENTS IN RFA L OCALLY RECOVERS SOFTMAX ATTENTION Based on the decomposition of RFA in Equation 7, we have one local control variate attached to each key-value pair. To see the beneﬁt of such decomposition, we demonstrate that softmax attention is equivalent to associating each control variate with a locally optimized coefﬁcient ˆβm in RFA. Proposition 1. Let ˜gm(ω) = gm(ω) −ˆβm(hm(ω) −E [hm(ω)]). We denote the variance of some estimator g(ω) as Var [g(ω)] := Cov [g(ω),g(ω)]. Then the optimal ˆβm that minimizes Tr (Var [˜gm(ω)]) (i.e., the sum variance over all dimensions) is of the form β∗ m := arg min β Tr (Var [˜gm(ω)]) =vm = gm(ω) hm(ω). (8) Furthermore, by letting ˆβm = β∗ m for all m= 1,2,...,M , we have Tr (Var [˜gm(ω)]) = 0. As a result, Tr (Var [˜g(ω)]) = 0and thus RFA(qn,K,V) =˜g(ω) =SoftmaxAttn(qn,K,V). The proof is deferred to Appendix B.4. This proposition implies optimizing ˆβm for each key- value pair in the decomposed formulation of RFA recovers the exact softmax attention. It not only characterizes the theoretical gap introduced by RFA but also sheds light on how to improve RFA towards softmax attention from a localized perspective. Furthermore, it delineates the trade-off between estimation quality and computational costs. On the one hand, if we use a distinct ˆβm for each estimator, we could achieve a perfect estimation, albeit at the expense of computing exp q⊤ nkm for every query-key pair explicitly with quadratic time and space complexity. On the other hand, if a single shared coefﬁcient is employed, it degrades to conventional RFA, where all the control variate estimators can be merged and computed together in linear complexity (Choromanski et al., 2021; Peng et al., 2021b; Zheng et al., 2022b). 4 EVA: E FFICIENT ATTENTION VIA CONTROL VARIATES In this section, we demonstrate that the control variate formulation offers a natural way to improve RFA with a ﬁner-grained treatment over control variates. We describe the improved efﬁcient attention mechanism EVA in §4.1 and its practical implementation in §4.2. 4.1 C ONTROL VARIATES WITH LOCALLY SHARED COEFFICIENTS We denote[M] := {1,2,...,M }as the set of all key-value indices. Instead of employing the same co- efﬁcient for all control variates as in RFA, we propose to partition[M] into Csubsets P1,P2,..., PC and allocate a locally shared βc for each subset Pc. For all βc and their optimum β∗ m for each token, deﬁne the weighted mean squared error (weighted MSE) as ∑C c=1 ∑ m∈Pc αm∥βc −β∗ m∥2, where αm > 0 and ∑C c=1 ∑ m∈Pc αm = 1. To see the beneﬁt of partitioning, we demonstrate that there always exists some {βc}C c=1 that achieves lower weighted MSE than any globally shared coefﬁcient (see Appendix B.5 for a formal argument). The next question is how to determine{βc}C c=1. According to Proposition 1, a natural choice is to adapt the optimal coefﬁcients (Equation 8) to the case of partitioned subsets. We justify this choice by proving that it is also optimal in minimizing the MSE above weighted by the true attention probabilities. Proposition 2. Suppose U is a set of key-value indices,β∗ m is the optimal coefﬁcient for eachm∈U as deﬁned in Proposition 1, and P1,P2,..., PC are an arbitrary partition of U, where each subset Pc is associated with a distinct βc. We consider the following weighted mean squared error, J(β1,..., βC) := C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ∥βc −β∗ m∥2 . (9) 2Note that the expectation of individual control variates hm(·) is still in closed form as E [hm(ω)] = exp(q⊤ n km)/Z. The derivation can be found in Appendix B.3. 4Published as a conference paper at ICLR 2023 Then for each c= 1,...,C we have β∗ c := arg min βc J(β1,..., βC) = E [∑ m∈Pc gm(ω) ] E [∑ m∈Pc hm(ω) ]. (10) As a consequence, with βc = β∗ c, the partition scheme must achieve lower weighted mean squared error than any globally shared β, that is, J(β1 = β∗ 1,..., βC = β∗ C) ≤J(β1 = β,..., βC = β). The proof can be found in Appendix B.6. Apart from measuring the squared errors for all coefﬁcients, Equation 9 also governs the signiﬁcance of each error by its corresponding softmax weights, which attains closer alignment with true softmax attention. Therefore, this proposition implies that it is much easier for the partitioned control variate estimators to obtain coefﬁcients closer to their optimum while faithfully respecting softmax attention. The optimal coefﬁcients β∗ c could be estimated via Monte Carlo samples as β∗ c ≈ˆβc(ω) = (∑ m∈Pc gm(ω) ) / (∑ m∈Pc hm(ω) ) , which is a widely adopted strategy in the control variate literature (Wang et al., 2013; Owen, 2013). The resulting estimator for each subset Pc takes the form ∑ m∈Pc ( gm(ω) −ˆβc(ω)hm(ω) +ˆβc(ω)exp(q⊤ nkm) Z ) = ∑ m∈Pc exp(q⊤ nkm) Z ˆβc(ω). (11) Partially Optimized Coefﬁcients. Given the optimality of using a separate coefﬁcient for each key-value pair, we could further improve the estimation by selecting some subset E ⊆[M] and employ ˆβm = ˆβ ∗ m = vm for each m∈E. Without loss of generality, we assume E∩Pc = ∅ for all c= 1,...,C and [M] = (⋃C c=1 Pc ) ∪E. According to Proposition 1, for each m∈Ewe have ˜gm(ω) =gm(ω) −ˆβmhm(ω) +ˆβm exp(q⊤ nkm) Z = exp(q⊤ nkm)vm Z . (12) We choose E by running an additional sparse attention mechanism (e.g., local window attention (Child et al., 2019) or Reformer (Kitaev et al., 2020)), which tend to select tokens that are more relevant to the query in sub-quadratic complexity. Since estimates on these critical tokens are exact, this strategy not only reduces the overall squared error (Equation 9), but also produces a more informative context for queries, which often translates into better empirical performance. Combining Equations 12 and 11 together, we obtain an improved Efﬁcient attention via control V Ariates (EV A), EVA(qn,K,V) := ˜g(ω) = ∑ m∈E ˜gm(ω) + ∑ m/∈E ˜gm(ω) = ∑ m∈E exp(q⊤ nkm) Z vm + C∑ c=1 ∑ m∈Pc exp(q⊤ nkm) Z ˆβc(ω). (13) Comparison with Vanilla RFA. EVA and vanilla RFA can be re-written in a similar way (see Appendix B.7 for a detailed derivation), RFA(qn,K,V) = ∑M m=1 gm(ω) ∑M m=1 hm(ω) , (14) EVA(qn,K,V) = ∑ m∈E exp(q⊤ nkm) Z gm(ω) hm(ω) + C∑ c=1 ∑ m∈Pc exp(q⊤ nkm) Z ∑ m∈Pc gm(ω)∑ m∈Pc hm(ω). (15) Intuitively, we can think of EVA as a calibrated version of RFA. Instead of directly computing and aggregating the random feature approximation for all tokens as in RFA (Equation 14), EVA (Equation 15) ﬁrst constructs local estimation for either a single token ( m ∈E) or a subset (e.g., Pc), and then corrects these approximations by their corresponding true attention scores (e.g.,∑ m∈Pc exp(q⊤ nkm) for Pc). These adjusted local estimates are ﬁnally aggregated and globally normalized. Thanks to the decomposed representation of RFA, we can realize this divide-and-conquer strategy in a principled manner, which imposes ﬁner-grained control on the whole estimation accuracy and enjoys increased approximation ﬁdelity. 5Published as a conference paper at ICLR 2023 Table 1: Classiﬁcation accuracy on ImageNet1k in comparison to different RF-based approxima- tions. †vanilla PVT-v2-b3 (Wang et al., 2021b) uses a convolutional kernel to downsample key and value vectors, resulting in fewer FLOPs but with signiﬁcant performance degradation. Model DeiT-Tiny DeiT-Small PVT-v2-b3# Param. FLOPs Top-1 Acc.# Param. FLOPs Top-1 Acc.# Param. FLOPs Top-1 Acc. Local 5.7M 1.1G 67.10 22.0M 4.3G 74.06 36.0M 7.2G 83.34 Performer 5.7M 1.2G 65.92 22.0M 4.4G 74.29 36.0M 8.2G 82.40LARA 5.8M 1.2G 71.48 22.2M 4.5G 79.48 39.9M 7.7G 83.47 EVA(Ours) 5.8M 1.2G 73.00 22.2M 4.4G 80.65 36.1M 7.4G 83.71 Softmax 5.7M 1.3G 72.98 22.0M 4.6G 80.36 45.2M 6.9G† 83.14† 4.2 P RACTICAL IMPLEMENTATION According to the formulation (Equation 13) of EVA, the terms within Ecould be computed efﬁciently due to its limited size; however, the partitioning requires computing ∑ m∈Pc exp(q⊤ nkm) explicitly for each subset, which again builds up to quadratic computational complexity. As discussed above,∑ m∈Pc exp(q⊤ nkm) serves as a weight to correct the contribution from each subset Pc. In this regard, we propose to approximate such control by ∑ m∈Pc exp(q⊤ nkm) ≈exp(q⊤ n˜kc), where ˜kc is an adaptive vector summarizing the information of all keys belonging toPc (see Appendix C for more details). Such heuristic not only avoids computing the exponential dot product of each query-key pair explicitly, but also induces a fast approximation of the normalizing constant, Z = ∑ m∈E exp(q⊤ nkm) + C∑ c=1 ∑ m∈Pc exp(q⊤ nkm) ≈ ∑ m∈E exp(q⊤ nkm) + C∑ c=1 exp(q⊤ n˜kc). Equipped with these results, our EVA estimator (Equation 13) can be reduced as follows, EVA(qn,K,V) ≈ ∑ m∈Eexp(q⊤ nkm)vm + ∑C c=1 exp(q⊤ n˜kc)ˆβc(ω) ∑ m∈Eexp(q⊤nkm) +∑C c=1 exp(q⊤n˜kc) . (16) Parameterization Details. We deﬁne E in the same way as a simple block-wise local attention (Xiong et al., 2021a). The input sequence is ﬁrst chunked into multiple blocks (or 2D windows for images), and each query qn is associated with a speciﬁc En that only contains tokens within the same block as the query. For the remaining indices [M] \\En, we evenly split it into Ccontiguous chunks {Pn 1 ,..., Pn C}. Note that we add the superscript nhere to denote the dependence on the query position; however, for notational brevity, we omit the notation when there is no ambiguity. The pseudo-code of EVA is provided in Algorithm 1 of Appendix. More implementation details, including the deﬁnition of ˜kc and ˆβc(ω) in Equation 16, are deferred to Appendix C. Extension to Autoregressive Modeling. The decoder (or causal) self-attention, where each query can only attend to previous tokens, is the key ingredient in Transformer-based generative modeling (Vaswani et al., 2017; Brown et al., 2020). We demonstrate that it is straightforward to extend EVA to support such auto-regressive modeling with few modiﬁcations. Thanks to the decomposed formulation of EVA, we only need to incorporate two triangular mask matrices into the computation, which eliminate the information from future singletons m∈Eand entire future subsets Pc respectively. Unlike previous RFA methods, which are slow during training due to their recurrent computation (Choromanski et al., 2021; Peng et al., 2021b), the resulting causal variant remains highly efﬁcient. More details can be found in Appendix D, including a pseudo-code Algorithm 2. 5 E XPERIMENTAL RESULTS In this section, we evaluate our proposed method on various tasks, including image classiﬁcation (§5.1), language tasks (§5.2), and Long Range Arena benchmark (Appendix F). Details of experimen- tal protocols and baselines can be found in Appendix E. 6Published as a conference paper at ICLR 2023 Table 2: Image classiﬁcation accuracy on ImageNet1k dataset with DeiT-Tiny-784. Model # Param. FLOPs Top-1 Acc. Performer (Choromanski et al., 2021) 5.7M 4.9G 67.19Local attention (Child et al., 2019) 5.7M 4.4G 70.62Scatterbrain (Chen et al., 2021a) 5.7M 5.2G 73.50Nyströmformer (Xiong et al., 2021b) 5.7M 4.8G 74.20LARA (Zheng et al., 2022b) 5.8M 4.6G 75.02Combiner (Ren et al., 2021) 5.7M 4.7G 75.56Long-Short (Zhu et al., 2021) 6.1M 5.0G 76.41 EVA(Ours) 5.8M 4.6G 76.67 Softmax 5.7M 7.0G 77.16 Table 3: Masked Language Modeling Perplexity on the Books3 validation dataset. Model # Param. FLOPs Perplexity Performer (Choromanski et al., 2021) 126M 213G 8.61Linformer (Wang et al., 2020) 129M 193G 5.16LARA (Zheng et al., 2022b) 126M 194G 4.39Reformer (Kitaev et al., 2020) 126M 205G 4.28Local attention (Child et al., 2019) 136M 183G 4.27Combiner (Ren et al., 2021) 136M 187G 4.12Long-Short (Zhu et al., 2021) 142M 218G 4.01 EVA(Ours) 136M 184G 3.94EVA-4096 (Ours) 136M 387G 3.73 Softmax 126M 252G 3.74 Table 4: BLEU scores on the test set of WMT14 En-De. † numbers are taken from Zheng et al. (2022b). Model # Param.BLEU Performer-128† 60.92M 23.5 LARA-16† 60.96M 26.4 LARA-32† 60.96M 26.8 LARA-64† 60.96M 27.0 EVA-16 60.96M 27.2 EVA-32 60.96M 27.3 EVA-64 60.96M 27.5 Softmax 60.92M 27.5 Table 5: Validation (Val.) and Test perplexity (PPL) on Wikitext-103. 256/480 indicate evaluation context window sizes. † numbers are due to Kasai et al. (2021). Model # Params. 256 480Val. Test Val. Test Softmax† 449M 17.9 18.5 – – ELU† 449M 22.0 22.8 – – RFA† 449M 20.4 21.3 – – T2R† 450M 20.1 20.8 – – EVA(Ours) 450M 17.9 18.6 17.7 18.3 Softmax 247M 18.8 19.5 18.4 19.1 EVA(Ours) 247M 18.8 19.4 18.5 19.1 5.1 I MAGE CLASSIFICATION We explore the ability to learn visual representations for different attention mechanisms in vision transformers (ViTs; Dosovitskiy et al., 2021). In particular, we replace softmax attention used in ViTs with its efﬁcient variants and evaluate their performance on the ImageNet1k dataset (Deng et al., 2009), which contains over 1,280K and 50K images of 1,000 classes for training and validation splits, respectively. For the transformer model, we consider both a plain ViT (DeiT; Dosovitskiy et al., 2020; Touvron et al., 2021) and a pyramidal ViT (PVT; Wang et al., 2021b) to test the performance. The former maintains the same sequence length (which is set to 196 by default) across all transformer layers, while the latter processes much longer sequences (up to 3136 tokens) at early layers and progressively reduces the sequence length to form a hierarchical structure. Detailed experimental settings could be found in Appendix E.2. Results. We ﬁrst compare the performance of EVA against our main baselines on the standard ViT architectures. As shown in Table 1, EVA signiﬁcantly improves the performance of previous RFA approaches (including Performer (Choromanski et al., 2021) and LARA (Zheng et al., 2022b)) and local attention by a large margin, and even outperforms the conventional softmax attention. We then consider a more challenging setting, where the plain architecture DeiT-Tiny is used but the sequence length is scaled up to 784 (denoted as DeiT-Tiny-784). We compare EVA against other attention variants in this setting and report the classiﬁcation results in Table 2. EVA outperforms most previous baselines and remains highly competitive with softmax attention, illustrating its effectiveness. 5.2 M ACHINE TRANSLATION AND LANGUAGE MODELING We further evaluate EVA on the natural language domain. Speciﬁcally, we consider three tasks: • Masked language modeling (MLM) on a pretraining-scale book corpus Books3 in the Pile dataset suite (Presser, 2020; Gao et al., 2020), consisting of over 196,640 published books. • Machine translation (MT) on WMT14 En-Debenchmark (Bojar et al., 2014). • Autoregressive language modeling (Autoregressive LM)on a large-scale token-level LM bench- mark Wikitext-103 (Merity et al., 2016). Results. We report MLM validation perplexity in Table 3, where the sequence length is 2048 by default. EVA substantially improves previous methods based on random features (including Performer and LARA) and outperforms the other efﬁcient attention mechanisms. Thanks to the linear 7Published as a conference paper at ICLR 2023 1000 2000 3000 4000 5000 6000 7000 8000 Sequence Length 0 5 10 15 20 25Memory Consumption (GB) Linformer Local Attention Performer EVA Nyströmformer LARA Long-short Softmax (a) Memory consumption. 1000 2000 3000 4000 5000 6000 7000 8000 Sequence Length 20 40 60 80 100 120 140 160Running Time (in milliseconds) Linformer Local Attention Performer EVA Nyströmformer LARA Long-short Softmax (b) Running time. 0 10 20 30 40 50 Running Time (in hours) 2 3 4 5 6 7 8 9Validation Loss Softmax EVA (c) Training speed-up. Figure 1: Left and middle: empirical memory consumption and running time comparison respectively of different attention mechanisms under various sequence lengths. Right: a snapshot of MLM validation loss curve versus actual elapsed time during training. complexity of EVA, it can be scaled further to much longer sequences. With input sequences of length increased to 4096, EVA (denoted as “EVA-4096”) attains lower validation perplexity than exact softmax attention, which demonstrates its capability of scaling to much longer sequences. Besides, machine translation results are compared in Table 4, where in this taskC = 8by default and EVA-m denotes EVA with |E|=m. EVA outperforms previous random feature methods by a large margin and achieves translation quality on par with full softmax attention even under the setting of small |E|and C. For Autoregressive LM (Table 5), EVA achieves the same perplexity as softmax attention with much lower computational complexity. Comparing against various random feature methods reported by previous work Kasai et al. (2021), we observe a signiﬁcant performance gain brought from EVA even under a Transformer with half parameters. When further increasing the transformer model size as the setting in Kasai et al. (2021), EV A still scales as effectively as softmax attention with a comparable perplexity while outperforming previous random feature methods by a larger margin. These results indicate the substantially enlarged capacity of EVA to approximate softmax attention. 5.3 A NALYSIS Table 6: Classiﬁcation accuracy on ImageNet1k dataset. Model Mem.(G) Time(ms/iter)|E| C Top-1 Acc. Performer8.1 87 0 1 67.19Local 7.8 65 49 0 70.62 EVA 8.4 77 0 49 74.339.1 87 49 1 74.109.4 89 49 16 75.839.9 94 49 49 76.6712.5 119 49 196 77.1011.9 108 196 49 77.36 Softmax 17.7 99 n.a. n.a. 77.16 Table 7: MLM validation perplexity on Books3. “–” indicates fail to converge. Model Mem.(G) Time(ms/iter)|E| C Perplexity Performer-40964.8 39 0 1 –Local-4096 4.4 29 256 0 4.34 EVA-4096 5.8 40 256 128 3.826.4 41 256 256 3.736.9 47 512 128 3.71 Softmax-409621.2 102 n.a. n.a. 3.65 Running Time & Memory Comparison. We con- duct a simulation experiment to evaluate the empirical efﬁciency of various attention methods, which is mea- sured by the running time per iteration and memory footprint under different sequence lengths. The setup can be found in Appendix E.4. As illustrated in Fig- ures 1a and 1b, EVA only incurs a little computational overhead compared to Performer and local attention and achieves much better running time speed-up than Long-Short (Zhu et al., 2021), a strong baseline across various tasks albeit with much longer running time and larger memory consumption. In Figure 1c, we further visualize the speed-up of EVA relative to conventional softmax attention by plotting the validation loss curve versus actual elapsed time during training transform- ers (equivalent to 32 GPU days). It can be seen that EVA can achieve a much lower loss after running for the same elapsed time; in contrast, conventional soft- max attention needs to run almost 3×longer to match the loss quantity. Overall, our method attains a good trade-off between quality and empirical efﬁciency. Ablation Study. In this section, we conduct an ablation study on image classiﬁcation and MLM tasks to investigate the effects of main hyper-parameters in EVA (see Table 8 for more comprehensive analysis). In particular, we vary |E|and the partition size Cand evaluate their performance on both image classiﬁcation and masked language modeling. As presented in Table 6 and Table 7, increasing |E|amounts to obtaining exact estimates for more key-value pairs, which greatly improves empirical performance; besides, increasing Cwould process control variates at a ﬁner scale, also translating into better modeling quality, consistent with our theoretical analysis (§4.1). 8Published as a conference paper at ICLR 2023 6 R ELATED WORK Control Variates. Control variates are a widely used variance reduction technique in reinforcement learning (Greensmith et al., 2004; Grathwohl et al., 2018; Vlassis et al., 2021), stochastic optimization (Wang et al., 2013), variational inference (Paisley et al., 2012; Ranganath et al., 2014; Geffner & Domke, 2018; Tucker et al., 2017; Grathwohl et al., 2018), Markov chain Monte Carlo (Baker et al., 2019) and many other topics. Our construction with control variates provides a new perspective on designing faster yet more accurate attention approximations. Efﬁcient Attention Mechanisms. A lot of research work has put the focus on reducing the quadratic complexity of conventional softmax attention. A widely used approach is to deﬁne a sparse attention pattern so that each query is limited to only attending to a subset of tokens. The sparse pattern could be either learnable (Kitaev et al., 2020; Vyas et al., 2020; Tay et al., 2020; Roy et al., 2021; Madaan et al., 2022) or simply ﬁxed (Liu et al., 2018; Parmar et al., 2018; Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Liu et al., 2021; Xiong et al., 2021a; Wang et al., 2022; Chen et al., 2022; Hutchins et al., 2022). Another paradigm is to adopt low-rank approximations, including via the Nyström method (Xiong et al., 2021b), down-sampling with learnable projections (Wang et al., 2020; Peng et al., 2021a), or explicitly compressing sequences (Rae et al., 2020; Dai et al., 2020; Ma et al., 2021; Jaegle et al., 2021). There are also studies improving both sparse and low-rank methods for better attention matrix approximation (Nguyen et al., 2021; Zhu et al., 2021; Chen et al., 2021a; Ren et al., 2021; Zhu & Soricut, 2021; Hua et al., 2022; Zeng et al., 2022). Instead of adopting approximate methods, a recent line of work (Rabe & Staats, 2021; Dao et al., 2022) proposes to compute the exact softmax attention in an online manner (Milakov & Gimelshein, 2018) without materializing the full attention matrix. In this way, softmax attention can be computed in linear memory complexity, and the runtime can also be greatly improved by further minimizing memory accesses (Dao et al., 2022). Random-Feature-based Attention. Random-feature-based methods are a popular alternative that uses random features (Rahimi & Recht, 2008) to linearize exponential kernels in softmax attention (Katharopoulos et al., 2020; Choromanski et al., 2021; Peng et al., 2021b). Recent work attempts to improve RFA approximation from several aspects, such as designing more accurate random feature maps (Choromanski et al., 2022; Likhosherstov et al., 2022; Chowdhury et al., 2022), incorporating relative positional or other task-speciﬁc biases (Liutkus et al., 2021; Luo et al., 2021; Chen, 2021; Zheng et al., 2022a; Qin et al., 2022b; Wu et al., 2022; Qin et al., 2022a), or leveraging connections to fast weight programmers (Peng et al., 2021b; Schlag et al., 2021; Irie et al., 2021). Prior work closely related to ours includes Zheng et al. (2022b), which reinterprets RFA using self-normalized importance sampling (Hesterberg, 1995) and theoretically extends the random feature approximation from individual exponential kernels to the whole softmax attention. Our work further generalizes this result via control variates and characterizes the approximation gap caused by RFA. Scatterbrain (Chen et al., 2021a) is also similar to our work in that it also reﬁnes RF approximation on critical local regions. However, it is developed based on a different motivation that attempts to approximate the attention matrix with a combination of sparse and low-rank matrices. Interestingly, we ﬁnd that Scatterbrain can be cast as a special case under our framework; see Appendix G for a detailed discussion about connections between EVA and previous attention mechanisms. 7 C ONCLUSION AND LIMITATIONS In this work, we develop an efﬁcient attention mechanism EVA via control variates. Our framework reveals a localized perspective of RFA approximation, which not only bridges the gap between RFA and exact softmax attention but also attains a good trade-off between modeling quality and efﬁciency. We evaluate our method on both vision and language tasks and demonstrate substantial improvements over previous baselines. There are some limitations of our framework. For instance, the approximation in computing control variate estimation for each partitioned subset is crude and might limit the potential modeling capacity; in addition, we only explore the most straightforward partitioning strategy that evenly splits the sequence into multiple contiguous chunks; while in general, the partition could contain arbitrary subsequences or be adaptive to inputs via clustering methods, which can be guided by task-speciﬁc inductive biases. It is interesting to investigate these limitations to unleash the expressiveness of EVA further, which we leave for future work. 9Published as a conference paper at ICLR 2023 ACKNOWLEDGMENTS We would like to thank the HKU NLP group, the Shark-NLP group, and the anonymous reviewers for their valuable suggestions that greatly helped improve this work. This work is partially supported by the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21. REFERENCES Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268–284, 2020. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=ByxZX20qFQ. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Jack Baker, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. Control variates for stochastic gradient mcmc. Statistics and Computing, 29(3):599–615, 2019. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pp. 12–58, 2014. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben- jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021a. Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterﬂy: Simple and efﬁcient sparse training for neural network models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=Nfl-iXa-y7R. Chun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689, 2021b. Peng Chen. PermuteFormer: Efﬁcient relative position encoding for long sequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10606–10618, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.828. URL https://aclanthology.org/ 2021.emnlp-main.828. 10Published as a conference paper at ICLR 2023 Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations , 2021. URL https: //openreview.net/forum?id=Ua6zuk0WRH. Krzysztof Marcin Choromanski, Han Lin, Haoxian Chen, Arijit Sehanobish, Yuanzhe Ma, Deepali Jain, Jake Varley, Andy Zeng, Michael S Ryoo, Valerii Likhosherstov, Dmitry Kalashnikov, Vikas Sindhwani, and Adrian Weller. Hybrid random features. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=EMigfE6ZeS. Sankalan Pal Chowdhury, Adamos Solomou, Kumar Avinava Dubey, and Mrinmaya Sachan. Learning the transformer kernel. Transactions on Machine Learning Research , 2022. URL https: //openreview.net/forum?id=tLIBAEYjcv. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efﬁcient language processing. Advances in neural information processing systems, 33:4271–4282, 2020. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efﬁcient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=YicbFdNTTy. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Tomas Geffner and Justin Domke. Using large ensembles of control variates for variational inference. Advances in Neural Information Processing Systems, 31, 2018. Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=SyzKd1bCW. Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(9), 2004. 11Published as a conference paper at ICLR 2023 Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Techno- metrics, 37(2):185–194, 1995. ISSN 00401706. URL http://www.jstor.org/stable/ 1269620. Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129–8138, 2020. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block- Recurrent Transformers. arXiv preprint arXiv:2203.07852, 2022. Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Going beyond linear trans- formers with recurrent fast weight programmers. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https://openreview.net/forum?id=ot2ORiBqTa1. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Car- reira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research , pp. 4651–4664. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/jaegle21a.html. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into RNNs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10630–10643, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.830. URL https://aclanthology.org/ 2021.emnlp-main.830. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=rkgNKkHtvB. Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022. Valerii Likhosherstov, Krzysztof Choromanski, Avinava Dubey, Frederick Liu, Tamas Sarlos, and Adrian Weller. Chefs’ random tables: Non-trigonometric random features. arXiv preprint arXiv:2205.15317, 2022. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Confer- ence on Learning Representations, 2018. URL https://openreview.net/forum?id= Hyg0vbWC-. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10012–10022, October 2021. 12Published as a conference paper at ICLR 2023 Antoine Liutkus, Ondˇrej Cífka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan Yang, and Gael Richard. Relative positional encoding for transformers with linear complexity. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7067–7079. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/liutkus21a.html. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2019. Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear complexity. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.),Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=X7XNPor93uG. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettle- moyer. Luna: Linear uniﬁed nested attention. arXiv preprint arXiv:2106.01540, 2021. Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, and Prateek Jain. Treeformer: Dense gradient trees for efﬁcient attention computation. arXiv preprint arXiv:2208.09015, 2022. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and Bao Wang. Fmmformer: Efﬁcient and ﬂexible transformer via decomposed near-ﬁeld and far-ﬁeld attention. Advances in Neural Information Processing Systems, 34, 2021. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1–9, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301. URL https://aclanthology.org/W18-6301. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://aclanthology.org/N19-4009. Zijing Ou, Tingyang Xu, Qinliang Su, Yingzhen Li, Peilin Zhao, and Yatao Bian. Learning set functions under the optimal subset oracle via equivariant variational inference. arXiv preprint arXiv:2203.01693, 2022. Art B. Owen. Monte Carlo theory, methods and examples. 2013. John Paisley, David M. Blei, and Michael I. Jordan. Variational bayesian inference with stochastic search. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML’12, pp. 1363–1370, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. InInternational Conference on Machine Learning, pp. 4055–4064. PMLR, 2018. 13Published as a conference paper at ICLR 2023 Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, and Noah A Smith. Abc: Attention with bounded-memory control. arXiv preprint arXiv:2110.02488, 2021a. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations, 2021b. Shawn Presser. Books3. 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833. Zhen Qin, XiaoDong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. arXiv preprint arXiv:2210.10340, 2022a. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Confer- ence on Learning Representations, 2022b. URL https://openreview.net/forum?id= Bl8CQrx2Up4. Markus N Rabe and Charles Staats. Self-attention does not need O(n2) memory. arXiv preprint arXiv:2112.05682, 2021. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10428–10436, 2020. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Com- pressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020a. URL http://jmlr.org/papers/v21/20-074.html. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020b. URL http://jmlr.org/papers/v21/20-074.html. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y . Singer, and S. Roweis (eds.),Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/ paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf. Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artiﬁcial intelligence and statistics, pp. 814–822. PMLR, 2014. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. Advances in Neural Information Processing Systems, 34, 2021. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. doi: 10.1162/tacl_a_00353. URL https://aclanthology.org/2021. tacl-1.4. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9355–9366. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/ schlag21a.html. 14Published as a conference paper at ICLR 2023 Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse Sinkhorn attention. In Hal Daumé III and Aarti Singh (eds.),Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9438–9447. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/tay20a.html. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efﬁcient transformers. In International Conference on Learning Representations , 2021. URL https: //openreview.net/forum?id=qVyeW-grC2k. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10347–10357. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/touvron21a.html. Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. arXiv preprint arXiv:2204.01697, 2022. George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Asso- ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Nikos Vlassis, Ashok Chandrashekar, Fernando Amat, and Nathan Kallus. Control variates for slate off-policy evaluation. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https://openreview. net/forum?id=e9_UPqMNfi. Apoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention. Advances in Neural Information Processing Systems, 33, 2020. Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran As- sociates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/ 9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021a. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021b. Yuxin Wang, Chu-Tak Lee, Qipeng Guo, Zhangyue Yin, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. What dense graph do you need for self-attention? In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 22752–22768. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr. press/v162/wang22l.html. 15Published as a conference paper at ICLR 2023 Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021. Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing transformers with conservation ﬂows. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 24226– 24242. PMLR, 17–23 Jul 2022. URLhttps://proceedings.mlr.press/v162/wu22m. html. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and Ross Girshick. Early convolutions help transformers see better. arXiv preprint arXiv:2106.14881, 2021. Wenhan Xiong, Barlas O˘guz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Wen-tau Yih, and Yashar Mehdad. Simple local attentions remain competitive for long-context tasks. arXiv preprint arXiv:2112.07210, 2021a. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pp. 14138–14148, 2021b. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. Advances in Neural Information Processing Systems, 34, 2021. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Trans- formers for longer sequences. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17283–17297. Cur- ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf. Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh. Multi resolution analysis (MRA) for approximate self-attention. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 25955–25972. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ zeng22a.html. Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358, 2021. Lin Zheng, Huijie Pan, and Lingpeng Kong. Ripple attention for visual perception with sub-quadratic complexity. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 26993–27010. PMLR, 17–23 Jul 2022a. URL https://proceedings.mlr.press/v162/zheng22a.html. Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mecha- nism. arXiv preprint arXiv:2204.04667, 2022b. Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efﬁcient transformers for language and vision. Advances in Neural Information Processing Systems, 34, 2021. Zhenhai Zhu and Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 3801–3815, Online, August 2021. Association for Computational Lin- guistics. doi: 10.18653/v1/2021.acl-long.294. URL https://aclanthology.org/2021. acl-long.294. 16Published as a conference paper at ICLR 2023 Appendix A A Brief Review of Vanilla Random Feature Attention 18 B Proofs & Derivations 18 B.1 An Extended Review of Control Variates . . . . . . . . . . . . . . . . . . . . . . . 18 B.2 Derivation of SNIS as Control Variate Estimation . . . . . . . . . . . . . . . . . . 19 B.3 Derivation of the Expectation of Per-term Control Variates . . . . . . . . . . . . . 19 B.4 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.5 A Formal Analysis of the Advantage of Partitioning . . . . . . . . . . . . . . . . . 20 B.6 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.7 Derivation of Equations 14 and 15 . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C More Implementation Details for EV A 23 D A Causal Variant of EV A 25 E Experimental Details 26 E.1 Efﬁcient Attention Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 E.2 Image Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 E.3 Machine Translation and Language Modeling . . . . . . . . . . . . . . . . . . . . 29 E.4 Experimental Settings of Efﬁciency Comparison . . . . . . . . . . . . . . . . . . . 31 F Experiments on Long Range Arena 32 G Connections to Other Attention Mechanisms 33 G.1 RFA, Softmax Attention, and EV A . . . . . . . . . . . . . . . . . . . . . . . . . . 33 G.2 Connections to LARA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 G.3 Connections to Clustered Attention . . . . . . . . . . . . . . . . . . . . . . . . . . 33 G.4 Connections to Combiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 G.5 Connections to Scatterbrain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 17Published as a conference paper at ICLR 2023 A A B RIEF REVIEW OF VANILLA RANDOM FEATURE ATTENTION Vanilla random feature attention methods, such as Performer (Choromanski et al., 2021; Peng et al., 2021b), seek to approximate the softmax attention mechanism through random features (Rahimi & Recht, 2008) φ(x,ω) := 1/ √ S[ξ(x,ω1),...,ξ (x,ωS)]⊤. Here, ω1,...,ω S ∼N(0,I), and ξ(x,ω) is the randomized mapping such that exp ( q⊤ nkm ) = Eωs∼N(0,I) [ ξ(qn,ωs)⊤ξ(km,ωs) ] . (17) Therefore, we can draw multiple Monte Carlo samples to estimate the exponential kernel, exp ( q⊤ nkm ) ≈1 S S∑ s=1 ξ(qn,ωs)⊤ξ(km,ωs) := φ(qn,ω)⊤φ(km,ω), and then approximate the attention mechanism as M∑ m=1 exp ( q⊤ nkm ) ∑M m′=1 exp (q⊤nkm′) vm ≈ ∑M m=1 φ(qn,ω)⊤φ(km,ω)vm ∑M m′=1 φ(qn,ω)⊤φ(km′,ω) . (18) It is recently generalized as a self-normalized importance sampling estimator to approximate softmax attention (Zheng et al., 2022b), as described in §2.2. We refer the generalized random feature based approximations as RFA. B P ROOFS & DERIVATIONS B.1 A N EXTENDED REVIEW OF CONTROL VARIATES The control variate method takes the following form, ˜g(ω) =g(ω) −βh(ω) +βE [h(ω)] , (19) Given the particular forms of g(·) and h(·), βcan be optimized to minimize the estimation variance. For notational convenience, we denote the covariance between a scalar and a random vector as Cov [h(ω),g(ω)] := E [(h(ω) −E [h(ω)]) (g(ω) −E [g(ω)])], and the variance of a random vector as Var [g(ω)] := Cov [g(ω),g(ω)]. In particular, we have Var [˜g(ω)] = Var [g(ω) −βh(ω)] = Var [g(ω)] −2 Cov [βh(ω),g(ω)] + Var [βh(ω)] = Var [g(ω)] −2 Cov [h(ω),g(ω)] β⊤+ Var [h(ω)] ββ⊤. We hope an optimal βwould minimize Tr (Var [˜g(ω)]), that is, the sum of estimating variance for each dimension. By differentiating, we obtain β∗= arg min β Tr (Var [˜g(ω)]) =Cov [h(ω),g(ω)] Var [h(ω)] . (20) Since both the covariance and the variance may be intractable to compute, the optimalβ∗is generally not available in closed form. Nevertheless, with the optimal coefﬁcient, the variance of such control variate estimate would never be larger than the plain estimator g(·). 18Published as a conference paper at ICLR 2023 B.2 D ERIVATION OF SNIS AS CONTROL VARIATE ESTIMATION For notational convenience, we denote the importance weight as W(ωs) := pn(ωs)/q(ωs). Then we have ˜g(ω) = ∑S s=1 pn(ωs) q(ωs) f(ωs) ∑S s=1 pn(ωs) q(ωs) = ∑S s=1 W(ωs)f(ωs) ∑S s=1 W(ωs) = ∑S s=1 W(ωs)f(ωs) ∑S s=1 W(ωs) −1 S S∑ s=1 W(ωs)f(ωs) +1 S S∑ s=1 W(ωs)f(ωs) = ∑S s=1 W(ωs)f(ωs) ∑S s=1 W(ωs) − ∑S s=1 W(ωs) ∑S s=1 W(ωs) 1 S S∑ s=1 W(ωs)f(ωs) +1 S S∑ s=1 W(ωs)f(ωs) = 1 −1 S ∑S s=1 W(ωs) ∑S s=1 W(ωs) S∑ s=1 W(ωs)f(ωs) +1 S S∑ s=1 W(ωs)f(ωs) = ∑S s=1 W(ωs)f(ωs) ∑S s=1 W(ωs) ( 1 −1 S S∑ s=1 W(ωs) ) + 1 S S∑ s=1 W(ωs)f(ωs) = 1 S S∑ s=1 W(ωs)f(ωs) − ∑S s=1 W(ωs)f(ωs) ∑S s=1 W(ωs) ( 1 S S∑ s=1 W(ωs) −1 ) = g(ω) −ˆβ(ω) (h(ω) −E [h(ω)]) , Note that the expectation of importance weights equals 1, that is, E [h(ω)]= E [ 1 S S∑ s=1 W(ωs) ] =Eω1,...,ωS∼q(ω) [ S∑ s=1 1 S p(ωs) q(ωs) ] = 1 S S∑ s=1 Eωs∼q(ω) [p(ωs) q(ωs) ] = 1. Same as SNIS, this estimator is still biased due to the dependence of ˆβ(ω) on ω. However, it would asymptotically become unbiased since ˆβ(ω) is consistent and converges to a constant βw.r.t. ω given a large number of samples, ˆβ(ω) = g(ω) h(ω) p − →E [g(ω)] E [h(ω)] = Epn(ω) [f(ω)]   constant := β. (21) B.3 D ERIVATION OF THE EXPECTATION OF PER-TERM CONTROL VARIATES According to the deﬁnition of randomized mappings, we have E [hm(ω)] =Eω1,...,ωS∼q(ω) [ 1 S S∑ s=1 N(ωs; 0,I) Zq(ωs) ξ(qn,ωs)ξ(km,ωs) ] = 1 S S∑ s=1 1 Z ∫ ξ(qn,ωs)ξ(km,ωs)N(ωs; 0,I)dωs = exp(q⊤ nkm) Z . (22) B.4 P ROOF OF PROPOSITION 1 Proof. We start with the formulation of g(·) and h(·), gm(ω) hm(ω) = ∑S s=1 N(ωs;0,I) Zq(ωs) ξ(qn,ωs)ξ(km,ωs)vm ∑S s=1 N(ωs;0,I) Zq(ωs) ξ(qn,ωs)ξ(km,ωs) = vm. 19Published as a conference paper at ICLR 2023 As a result, we have gm(ω) =hm(ω)vm and E [gm(ω)] =E [hm(ω)] vm. We now investigate the optimal βm according to Equation 20, β∗ m = arg min β Tr (Var [˜gm(ω)]) = Cov [hm(ω),gm(ω)] Var [hm(ω)] = E [(h(ω) −E [h(ω)]) (h(ω) −E [h(ω)])] vm E [(h(ω) −E [h(ω)]) (h(ω) −E [h(ω)])] = vm = gm(ω) hm(ω). In terms of the variance, we again use gm(ω) =hm(ω)vm to obtain ˜gm(ω) =gm(ω) −ˆβm(hm(ω) −E [hm(ω)]) = gm(ω) −vmhm(ω) +vmE [hm(ω)] = vmE [hm(ω)] = exp(q⊤ nkm) Z vm. (23) Since this holds true for every term m= 1,...,M , our estimate becomes exactly softmax attention, ˜g(ω) = M∑ m=1 ˜gm(ω) = M∑ m=1 exp(q⊤ nkm) Z vm = M∑ m=1 exp(q⊤ nkm)∑M m′=1 exp(q⊤nkm) vm. Since all randomness is eliminated, the estimate is exact with zero bias and variance. That is, RFA(qn,K,V) =˜g(ω) =SoftmaxAttn(qn,K,V). B.5 A F ORMAL ANALYSIS OF THE ADVANTAGE OF PARTITIONING In this section, we demonstrate the advantage of partitioning by showing that there always exists some set {βc}C c=1 that achieves lower weighted MSE than any globally shared coefﬁcient, as discussed in §4.1. Lemma 3. Suppose β∗ m is the optimal coefﬁcient for each m∈[M] as deﬁned in Proposition 1, and P1,P2,..., PC are an arbitrary partition of [M], where each subset Pc is associated with a distinct βc. We consider the following weighted mean squared error, J(β1,..., βC) := C∑ c=1 ∑ m∈Pc αm∥βc −β∗ m∥2 , (24) where αm > 0 for each m ∈[M] and ∑C c=1 ∑ m∈Pc αm = 1. Then for any choice of {αm}M m=1 and any globally shared coefﬁcient β, there exists some {β∗ c}C c=1 so that J(β1 = β,..., βC = β) ≥J(β1 = β∗ 1,..., βC = β∗ C). Proof. Let β∗ c = ∑ m∈Pc αmβ∗ m∑ m∈Pc αm for each c= 1,...,C . Then we have ∑ m∈Pc αm(β∗ c −β∗ m) =β∗ c (∑ m∈Pc αm ) − ∑ m∈Pc αmβ∗ m = ∑ m∈Pc αmβ∗ m∑ m∈Pc αm (∑ m∈Pc αm ) − ∑ m∈Pc αmβ∗ m = ∑ m∈Pc αmβ∗ m − ∑ m∈Pc αmβ∗ m = 0. (25) 20Published as a conference paper at ICLR 2023 According to Equations 25 and 24, for any βwe have the following inequality, J(β1 = β,..., βC = β) = C∑ c=1 ∑ m∈Pc αm∥β−β∗ m∥2 = C∑ c=1 ∑ m∈Pc αm∥β−β∗ c + β∗ c −β∗ m∥2 = C∑ c=1 ∑ m∈Pc αm ( ∥β−β∗ c∥2 + 2 (β−β∗ c)⊤(β∗ c −β∗ m) +∥β∗ c −β∗ m∥2 ) = C∑ c=1 ∑ m∈Pc αm∥β−β∗ c∥2 + 2 C∑ c=1 ∑ m∈Pc αm(β−β∗ c)⊤(β∗ c −β∗ m)    =0 + C∑ c=1 ∑ m∈Pc αm∥β∗ c −β∗ m∥2 = C∑ c=1 ∑ m∈Pc αm∥β−β∗ c∥2 + C∑ c=1 ∑ m∈Pc αm∥β∗ c −β∗ m∥2 ≥ C∑ c=1 ∑ m∈Pc αm∥β∗ c −β∗ m∥2 = J(β1 = β∗ 1,..., βC = β∗ C). As a result, for any choice of {αm}M m=1 and any globally shared coefﬁcient β, there always exists some {βc}C c=1 that achieves lower (or equal) weighted MSE, and a solution can be simply βc =∑ m∈Pc αmβ∗ m∑ m∈Pc αm . B.6 P ROOF OF PROPOSITION 2 Proof. We ﬁrst consider the case of partitioned indices, where each subsetPc is associated with some speciﬁc βc. To see the global minimum of J, we differentiate on both sides and obtain ∂J(β1,..., βC) ∂βc = ∂ ∂βc C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ∥βc −β∗ m∥2 = ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′)2 (βc −β∗ m) . By setting the partial derivative to zero, we obtain β∗ c = ∑ m∈Pc exp ( q⊤ nkm ) β∗ m∑ m∈Pc exp (q⊤nkm) = ∑ m∈Pc exp ( q⊤ nkm ) vm ∑ m∈Pc exp (q⊤nkm) = ∑ m∈Pc E [gm(ω)]∑ m∈Pc E [hm(ω)] = E [∑ m∈Pc gm(ω) ] E [∑ m∈Pc hm(ω) ]. As a consequence, with βc = β∗ c, the partition scheme must achieve lower weighted mean squared error than any globally shared ˆβ, that is, J(β1 = β∗ 1,..., βC = β∗ C) ≤J(β1 = ˆβ,..., βC = ˆβ). 21Published as a conference paper at ICLR 2023 In fact, with βc = β∗ c, the partition scheme usually enjoys much lower error than adopting a globally shared coefﬁcient. To see the error reduction of using the partitioned strategy, we ﬁrst have J(β1 = ˆβ,..., βC = ˆβ) = C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ˆβ−β∗ m  2 = C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ˆβ−βc + βc −β∗ m  2 = C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ˆβ−βc + βc −β∗ m  2 = C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) (ˆβ−βc  2 + ( ˆβ−βc )⊤ (βc −β∗ m) +∥βc −β∗ m∥2 ) . Since C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ( ˆβ−βc )⊤ (βc −β∗ m) = C∑ c=1 ( ˆβ−βc )⊤ ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) (βc −β∗ m) = C∑ c=1 ( ˆβ−βc )⊤ ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) (∑ m∈Pc exp ( q⊤ nkm ) vm ∑ m∈Pc exp (q⊤nkm) −β∗ m ) = C∑ c=1 ( ˆβ−βc )⊤ ∑ m∈Pc exp ( q⊤ nkm ) (vm −β∗ m)∑ m′∈U exp (q⊤nkm′) = 0, plugging this result back we obtain J(β1 = ˆβ,..., βC = ˆβ) = C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) (ˆβ−βc  2 + ( ˆβ−βc )⊤ (βc −β∗ m) +∥βc −β∗ m∥2 ) = C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) (ˆβ−βc  2 + ∥βc −β∗ m∥2 ) = C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ˆβ−βc  2    ≥0 + C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ∥βc −β∗ m∥2 ≥ C∑ c=1 ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈U exp (q⊤nkm′) ∥βc −β∗ m∥2 . The last inequality holds since the ﬁrst term is always non-negative. Note that the ﬁrst term computes the squared error between ˆβand each βc, weighted by the sum of attention scores over the corre- sponding subset. As a result, it is usually positive and the error reduction is signiﬁcant if each βc deviates from ˆβa lot. However, although the optimal coefﬁcient in the partitioning always leads to lower error to the optimal individual coefﬁcient, note that it does not necessarily yield lower estimation variance. 22Published as a conference paper at ICLR 2023 Table 8: Classiﬁcation results on ImageNet1k dataset under different hyper-parameter conﬁgura- tions of EVA. By default, we set |E|= 49and C = 49across all variants below. Component Speciﬁcation Top-1 Acc. Partition Scheme of{P1,..., PC} partition over[M] \\E 76.53 partition over[M] 76.39 Parameterization ofσ(·) σ(·) = LN(Linear(·)) 76.67 σ(·) = Identity(·) 75.95 Number of Groups (C= 1) Number of Samples = 1 74.10 Number of Samples = 49 76.39 Number of Groups (C= 49) Number of Samples = 1 76.67 Number of Samples = 49 76.75 Proposal Parameterizationqc(ω) := N(ω;µc,I) µc = ˜qc+ ˜kc 76.67 µc = ˜qc 76.77 µc = 0 76.24 µc = Trainable parameters 76.39 Softmax 77.16 B.7 D ERIVATION OF EQUATIONS 14 AND 15 According to the deﬁnition of gm(·) and hm(·) in §3.1, for the vanilla RFA (Equation 4) we have RFA(qn,K,V) = ∑S s=1 pn(ωs) q(ωs) f(ωs) ∑S s=1 pn(ωs) q(ωs) = g(ω) h(ω) = ∑M m=1 gm(ω) ∑M m=1 hm(ω) . Besides, since vm = gm(ω)/hm(ω) and ˆβc(ω) = (∑ m∈Pc gm(ω) ) / (∑ m∈Pc hm(ω) ) , we can re-write EVA as EVA(qn,K,V) := ˜g(ω) = ∑ m∈E ˜gm(ω) + ∑ m/∈E ˜gm(ω) = ∑ m∈E exp(q⊤ nkm) Z vm + C∑ c=1 ∑ m∈Pc exp(q⊤ nkm) Z ˆβc(ω) = ∑ m∈E exp(q⊤ nkm) Z gm(ω) hm(ω) + C∑ c=1 ∑ m∈Pc exp(q⊤ nkm) Z ∑ m∈Pc gm(ω)∑ m∈Pc hm(ω). C M ORE IMPLEMENTATION DETAILS FOR EVA In this section, we provide more details of EVA. We also conduct a comprehensive ablation study to test the effect of different components in our implementation and report the results in Table 8. The pseudo-code for EVA is listed in Algorithm 1. Approximating ∑ m∈Pc exp(q⊤ nkm) and Parameterizing ˜kc. In our implementation, we approx- imate the sum of exponentials as ∑ m∈Pc exp(q⊤ nkm) ≈exp(q⊤ n˜kc). Here we provide an informal justiﬁcation for this approximation. Our main motivation for such approximation is based on the simple intuition that the sum of exponentials grows as fast as the maximum exponential value, as reﬂected by the following inequality, max m∈Pc exp(q⊤ nkm) ≤ ∑ m∈Pc exp(q⊤ nkm) ≤|Pc|max m∈Pc exp(q⊤ nkm). This means we can approximate the sum of exponentials by ﬁrst computing the group representative ˜kc := arg maxkm∈{km|m∈Pc}exp(q⊤ nkm), evaluating the corresponding exponential exp(q⊤ n˜kc) 23Published as a conference paper at ICLR 2023 and then multiplying it by some scalar. Since computing the argmax operation still needs to compare each exponential dot-product, it will still incur quadratic computational costs. To circumvent this, we adopt a heuristic strategy that computes a learnable group representation, which attempts to compensate for the approximation error while only evaluating one exponential dot product. Through preliminary experiments, we try various choices to compute the representative vector of each subset, such as max and average pooling; however, we found these strategies produce almost equally good performance. As a result, we adopt the average pooling by default due to its simplicity. To be speciﬁc, we implement it as ˜kc = σ ( 1 |Pc| ∑ m∈Pc km ) , (26) where σ(·) is a trainable linear projection with the same hidden dimension size as inputs, followed by a layer normalization operation (Ba et al., 2016) to stabilize training. We leave further improving the approximation, such as deriving tighter error bounds or using more expressive pooling methods (Zaheer et al., 2017; Ou et al., 2022) as future work. Parameterizing ˆβc(ω). As discussed in §4.1, we have ˆβc(ω) = ∑ m∈Pc gm(ω)∑ m∈Pc hm(ω) = ∑S s=1 N(ωs;0,I) Zq(ωs) ∑ m∈Pc ξ(qn,ωs)ξ(km,ωs)vm ∑S s=1 N(ωs;0,I) Zq(ωs) ∑ m∈Pc ξ(qn,ωs)ξ(km,ωs) . Compared to the SNIS formulation of vanilla RFA Equation 4, we can express it as RFA(qn,K,V) = ∑S s=1 pn(ωs) q(ωs) f(ωs) ∑S s=1 pn(ωs) q(ωs) = ∑M m=1 gm(ω) ∑M m=1 hm(ω) . We can think of each coefﬁcient ˆβc(ω) as computing the output of a localized RFA for each group Pc. From this perspective, we can recast each coefﬁcient ˆβc(ω) as an SNIS estimator as well, which tries to estimate Eω∼pc(ω) [fc(ω)] = ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈Pc exp (q⊤nkm′)vm (27) where fc(ω) := ∑ m∈Pc ξ(qn,ω)ξ(km,ω)vm ∑ m′∈Pc ξ(qn,ω)ξ(km′,ω) , pc(ω) := N(ω; 0,I) ∑ m∈Pc ξ(qn,ω)⊤ξ(km,ω)∑ m′∈Pc exp (q⊤nkm′) = ∑ m∈Pc exp ( q⊤ nkm ) ∑ m′∈Pc exp (q⊤nkm′)N(ω; qn + km,I). This interpretation indicates that a good proposal distribution qc(ω) should be speciﬁc to each subset Pc. To get close to the true distribution pc(ω) while keeping efﬁcient computation, Zheng et al. (2022b) suggests parameterizing the proposal distribution as qc(ω) := N(ω; µc,I) =N(ω; ˜qc + ˜kc,I), (28) where ˜qc is calculated similarly to Equation 26. We refer readers to Zheng et al. (2022b) for more discussions about the parameterization choice of proposal distributions. We conduct further ablation studies to test the effect of proposal parameterizations in our proposed model, as shown in Table 8. In particular, we found our model is robust to different parameterization approaches. 24Published as a conference paper at ICLR 2023 The essence in making the algorithm memory-efﬁcient is to use onlyone sample in calculating ˆβc(ω). In this case, we have ˆβc(ω) = ∑ m∈Pc gm(ω)∑ m∈Pc hm(ω) = N(ωc;0,I) Zqc(ωc) ∑ m∈Pc ξ(qn,ωc)ξ(km,ωc)vm N(ωc;0,I) Zqc(ωc) ∑ m∈Pc ξ(qn,ωc)ξ(km,ωc) = N(ωc;0,I) Zqc(ωc) ξ(qn,ωc) ∑ m∈Pc ξ(km,ωc)vm N(ωc;0,I) Zqc(ωc) ξ(qn,ωc) ∑ m∈Pc ξ(km,ωc) = ∑ m∈Pc ξ(km,ωc)vm ∑ m∈Pc ξ(km,ωc) , w c ∼qc(ω). Since this degenerated formulation eliminates the dependence on individual queriesqn, we could pre- compute ˆβc(ω) for each Pc, and then re-uses them for each query, which takes up O(Cd) memory. If multiple samples are used instead, the inﬂuence of queries needs to be explicitly taken into account and thus we need to compute a distinct ˆβc(ω) for each query, leading to O(NCd) memory usage, which incurs a signiﬁcant compute overhead. On the other hand, if we set C = 1, that is, using a shared ˆβc(ω) over all m /∈E, our approach does not suffer from this issue, since the memory usage is at most O(Nd). To investigate the effect of using larger Cor increasing the number of samples, we conduct an ablative analysis as in Table 8, and ﬁnd that 1) when C = 1, the performance degrades a lot when using one sample, which can be largely improved by adopting more samples; while when C >1, our partitioning strategy dominates and increasing the number of samples only improves performance marginally. This also validates the effectiveness of adopting a ﬁner-grained treatment over control variates. Partitioning Strategy. EVA signiﬁcantly improves random feature approximation by trying to locally estimate each subset of tokens, which is a much easier task than approximating the whole sequence as in previous RFA methods. To achieve this, EVA partitions the whole token sequence into multiple subsets according to the current query position n, which is denoted by {En,Pn 1 ,Pn 2 ,..., Pn C}N n=1.3 For elements in subset En, we optimize the control variate coefﬁcient to give an exact estimate for each single token m∈En. In addition, we impose T5-style relative positional encoding (Raffel et al., 2020a) over elements in En. While for some other subset Pc, we employ the shared coefﬁcient to approximate all tokens belonging to Pc. We assume all E1,...,E N are of the same cardinality K, and |Pn c |is the same for any c= 1,...,C and n= 1,...,N . The partition strategy {En,Pn 1 ,Pn 2 ,..., Pn C}N n=1 is decided based on a simple criterion: • for En, it contains Klocal neighbors with respect to each query n. To further simplify implemen- tation and reduce memory usage, we chunk the whole sequence into contiguous blocks of size K, and all adjacent queries belonging to the same block will share this block as the subset En; • as for Pn 1 ,Pn 2 ,..., Pn C, we follow a similar treatment by splitting the complement [M] \\En into Ccontiguous chunks of the same size. For ease of implementation, we simply partition the whole index set [M] into multiple groups instead of [M] \\En, which circumvents the overload for explicitly performing set difference operations in practical implementation. Although this leads to extra approximation error, this amounts to putting more attention weights on tokens belonging to the subset Eand we found this approximation does not lead to performance degradation (Table 8). D A C AUSAL VARIANT OF EVA In this section, we describe the causal variant of EVA, where each query can only attend to historical tokens. Thanks to the partitioning scheme, all future information with respect to the current query token can be masked conveniently. Following the formulation of EVA, we partition the whole sequence into C+ 1subsets {En,Pn 1 ,Pn 2 ,..., Pn C}with respect to each query qn. To fulﬁll the 3Here we add the superscript n to reﬂect the dependence on query position n. 25Published as a conference paper at ICLR 2023 Algorithm 1 Pseudo-code for EVA Input: the randomized mapping ξ(·,·), queries Q := {qn}N n=1, keys K := {km}M m=1, values V := {vm}M m=1 and partitions of the sequence {En,Pn 1 ,Pn 2 ,..., Pn C}N n=1; Output: attention output Y := {yn}N n=1; for c= 1,2,...,C do Compute ˜kc according to Equation 26; Compute qc(ω) according to Equation 28; Sample ωc ∼qc(ω); ⊿During inference, simply set ωc = Eqc(ω) [ω] Compute ˆβc(ω) =∑ m∈Pnc ξ(km,ωc)∑ m∈Pnc ξ(km,ωc) vm; end for for n= 1,2,...,N do Compute S= ∑ m∈En exp ( q⊤ nkm ) vm; ⊿Compute attention scores in the selected subset E Compute R= ∑C c=1 exp ( q⊤ n˜kc ) ˆβc(ω); ⊿Compute approx. expected control variates Compute Z = ∑ m∈En exp ( q⊤ nkm ) + ∑C c=1 exp ( q⊤ n˜kc ) ; Compute yn = (S+ R) /Z; end for Return Y := [y1,..., yN]. causal requirement, we design two different types of masking matrices to deal with both En and {Pn c }C c=1 respectively. • For En, we adopt a single lower-triangular matrix with shape K×K(recall that each set En is of size K) to mask future tokens locally, similar to the case of standard decoder softmax attention. Future tokens that do not belong toEnare handled by masking functions for{Pn c }C c=1, as described below. • For {Pn c }C c=1, we make use of the fact n∈En. Since any Pn c and En are disjoint, we only need to mask all subsets Pn c that appear after En. This amounts to ﬁrst allocating a lower-triangular matrix with shape C×C, and then conducting future masking at a subset level. The pseudo-code for the causal variant of EVA is listed in Algorithm 2. E E XPERIMENTAL DETAILS All of our experiments are conducted with at most 16 NVIDIA V100 GPUs. E.1 E FFICIENT ATTENTION BASELINES We compare our proposed attention mechanism EVA against various baselines: • Performer (Choromanski et al., 2021), which uses the plain random features to approximate softmax attention; • LARA (Zheng et al., 2022b), an advanced RF approximation that makes use of multiple adaptive proposals to construct the SNIS estimator; • Linformer (Wang et al., 2020), a low-rank approximation that uses a learnable matrix to project the key-value sequence into a shorter one; • Nyströmformer (Xiong et al., 2021b), a low-rank approximation that adopts the Nyström method to approximate softmax attention map with a sub-sampled matrix; • Local attention (Child et al., 2019), a simple sparse approximation that splits the whole sequence into multiple blocks and only allows the query to attend to tokens within the same block; • Reformer (Kitaev et al., 2020), a sparse approximation where hash functions are used to adaptively distribute sequence tokens into multiple buckets, and each token can only attend to tokens within the same bucket; 26Published as a conference paper at ICLR 2023 Algorithm 2 Pseudo-code for Causal EVA Input: the randomized mapping ξ(·,·), queries Q := {qn}N n=1, keys K := {km}M m=1, values V := {vm}M m=1, and partitions of the sequence {En,Pn 1 ,Pn 2 ,..., Pn C}N n=1; Output: attention output Y := {yn}N n=1; for c= 1,2,...,C do Compute ˜kc according to Equation 26; Compute qc(ω) according to Equation 28; Sample ωc ∼qc(ω); ⊿During inference, simply set ωc = Eqc(ω) [ω] Compute ˆβc(ω) =∑ m∈Pnc ξ(km,ωc)∑ m∈Pnc ξ(km,ωc) vm; end for Let K ←|EN|; ⊿we assume all En are the same in size Initialize ME ∈{0,1}K×K such that ME i,j = 1 i≤j; ⊿Intra-Emasking matrix Initialize MP∈{0,1}C×C such that MP c,t = 1 c≤t; ⊿Inter-Pmasking matrix for n= 1,2,...,N do Find index tsuch that Pn t is the most recent chunk on the left of E; Let bn ←mini{i: i∈En}; ⊿The least position within En; used for shifting token indices. ⊿The same masking matrix ME can be reused across nvia shifting token positions by bn. Compute S= ∑ m∈En ME m−bn,n−bn exp ( q⊤ nkm ) vm; Compute R= ∑C c=1 MP c,texp ( q⊤ n˜kc ) ˆβc(ω); Compute Z = ∑ m∈En ME m−bn,n−bn exp ( q⊤ nkm ) + ∑C c=1 MP c,texp ( q⊤ n˜kc ) ; Compute yn = (S+ R) /Z; end for Return Y := [y1,..., yN]. • Scatterbrain (Chen et al., 2021a), an approach that combines Performer and sparse attention. The details can be found in Appendix G. Here we implement the sparse module as a simple local attention to ensure a fair comparison; • Combiner (Ren et al., 2021), a probabilistic approach that constructs a structured factorization over the softmax probability distribution via a sparse mechanism. Combiner allows both direct and indirect calculations of conditional probabilities, where the direct probability is implemented as the sparse mechanism while the indirect probability is implemented through a local abstraction over a group of tokens. Similarly, we implement the sparse mechanism as a simple local attention, which corresponds to the Combiner-Fixed variant (Ren et al., 2021); • Transformer-LS, or Long-Short (Zhu et al., 2021), which is proposed to model long-term and short-term dependencies via low-rank structures and local attention respectively. The low-rank structure is deﬁned as an input-dependent weight matrix that compresses the sequence into a shorter one; while the local attention is deﬁned similarly as above. Note that for all mechanisms that involve a local attention, we split the sequence intonon-overlapping blocks (or 2D windows in terms of images) and each query can only attend to tokens within the same block. We also use the relative positional embedding (Raffel et al., 2020b; Liu et al., 2021) within the local attention computation. Unlike Transformer-LS (Zhu et al., 2021) that allows each query to attend to multiple blocks, we do not use this extension as we ﬁnd greatly increases memory consumption, although it does improve the model performance. E.2 I MAGE CLASSIFICATION Through the experiments on image classiﬁcation, we consider four different vision transformer (ViT) architectures: 27Published as a conference paper at ICLR 2023 Table 9: Our hyper-parameter conﬁguration for different attention mechanisms on DeiT-Tiny-784. Attention Hyper-parameter conﬁguration on image classiﬁcation Local attention Window size 49 Scatterbrain (Kitaev et al., 2020)umber of random feature samples 96 Local attention window size 49 Nyströmformer (Xiong et al., 2021b)Number of landmarks 49 Performer (Choromanski et al., 2021)Number of random feature samples 128 Type of random feature Positive Combiner (Ren et al., 2021) Mode Fixed Span size 49 Conditional distribution parameterizationDeepSets-Max Transformer-LS (Zhu et al., 2021)Dynamic projection dimension 16 Local window size 49 EVA(Ours) Number of partitioned groups (C) 49 Size ofE 49 • DeiT-Tiny (Touvron et al., 2021), which maintains the sequence length as 196 across all transformer layers. For the particular tiny variant, the number of transformer layers is set to 12, the embedding dimension is set to 196 and the number of heads is 3; • DeiT-Small (Touvron et al., 2021), which scales the embedding dimension and number of attention heads in DeiT-Tiny up to 384 and 6, respectively; • DeiT-Tiny-784, where the architecture is the same as DeiT-Tiny but the patch size in the tokenization step is decreased from 16 to 8. This effectively increases the sequence length from 196 to 784, which we found consistently improves predictive accuracy at the cost of signiﬁcantly increased time and memory consumption. Under this setting, we also see clearer differences among these attention variants and it helps better evaluate the ability of different attention models to learn visual representations; • PVT-v2-B3 (Wang et al., 2021b), a pyramidal transformer architecture that processes much longer token sequences at early layers and progressively reduces the sequence length to form a hierarchical structure. It patchiﬁes input images into 3136 (56 ×56) tokens, and then processes the sequence through 4 stages. Each stage contains several transformer layers and a down-sampling operation, which reduces the sequence length by a factor of 4 and increases the embedding dimension by 2×. Due to the prohibitively long sequences initially, PVT applies an additional down-sampling module on input sequences to obtain key and value vectors, which are then passed through a normal softmax attention mechanism. To evaluate different RF approximations, we remove the down-sampling operation and directly operate on the original sequence length, which results in much fewer model parameters than vanilla PVT-v2-B3. We refer readers to Wang et al. (2021b) for detailed architecture conﬁgurations. For training, we do not use the [CLS] token for classiﬁcation (Touvron et al., 2021); instead, we pool over the output of the last transformer layer to extract features and feed them into the classiﬁer head. We followed the same protocol to train all model variants. Closely following DeiT Touvron et al. (2021), we employ the AdamW (Loshchilov & Hutter, 2019) optimizer to train models for 300 epochs, where the number of warm-up epochs is 10, the learning rate is 0.001 with cosine learning rate decay (Loshchilov & Hutter, 2016), and batch size is set to 1024. The adopted augmentation and regularization are the same as DeiT, except that we remove repeated augmentation (Hoffer et al., 2020) in DeiT models as it often slows down convergence, as also observed in previous studies (Xiao et al., 2021).4 The speciﬁc conﬁgurations of each attention mechanism on DeiT-Tiny-784 are listed in Table 9. The hyper-parameter setup for each attention variant follows previous practices (Wang et al., 2021a;b; Zheng et al., 2022b) closely to ensure a similar computational cost. Comparison to State-of-the-Art Model Architectures. We also compare our model against re- cent state-of-the-art (SOTA) model architectures with similar parameter sizes on ImageNet1k benchmark. As reported in Table 10, we observe that PVT-v2 (Wang et al., 2021b) withEVA greatly 4we retain the repeated augmentation technique in training PVT to be consistent with the original training protocol in Wang et al. (2021b). 28Published as a conference paper at ICLR 2023 Table 10: Results on ImageNet1k dataset compared with SOTA model architectures. Model # Param. FLOPs Top-1 Acc. PVT-v1-M (Wang et al., 2021a) 44M 6.7G 81.2 RegNetY-8G (Radosavovic et al., 2020)39M 8.0G 81.7 CvT-21 (Wu et al., 2021) 32M 7.1G 82.5 SOFT-M (Lu et al., 2021) 45M 7.2G 82.9 RegNetY-16G (Radosavovic et al., 2020)84M 16.0G 82.9 UniFormer-S (Li et al., 2022) 22M 3.6G 82.9 Swin-S (Liu et al., 2021) 50M 8.7G 83.0 Swin-B (Liu et al., 2021) 88M 15.4G 83.3 RegionViT-M (Chen et al., 2021b) 42M 7.9G 83.4 ViL-M (Zhang et al., 2021) 40M 9.1G 83.5 Focal-S (Yang et al., 2021) 51M 9.1G 83.5 PVT-v2-b3 + LARA (Zheng et al., 2022b)40M 7.7G 83.6 MaxViT-T (Tu et al., 2022) 31M 5.6G 83.6 UniFormer-B (Li et al., 2022) 50M 8.3G 83.9 PVT-v2-b3 (Wang et al., 2021b) 45M 6.9G 83.1 PVT-v2-b3 +EVA 36M 7.4G 83.7 improves the predictive accuracy and performs competitively with recent SOTA architectures while using fewer parameters and FLOPs. E.3 M ACHINE TRANSLATION AND LANGUAGE MODELING Our implementation for all language tasks is based on FairSeq toolkit (Ott et al., 2019). To compare different methods, we report BLEU scores on the test set as the main metric for MT and perplexity for both Autoregressive LM and MLM tasks. For the hyper-parameters |E|and C in EVA, we set |E|= 2Cby default, as we ﬁnd that this choice attains a good trade-off between performance and computational costs across various tasks; while for C, it is determined based on previous practice for each task. Here we provide the detailed experimental protocol for each task. Masked Language Modeling. Following the standard pretraining practice as in RoBERTa (Liu et al., 2019), in MLM, we aim to reconstruct a subset of tokens in the input sequence that are randomly masked out, which is the core element of BERT-style natural language pretraining (Devlin et al., 2019). This setting allows us to investigate the generalization ability of our model on larger model sizes and much more data. The task performance is measured with validation perplexity, which reﬂects how well the model ﬁts the pretraining corpus and also exhibits good correlations with downstream task metrics. For the used corpus Books3, we randomly select 100 books without replacement for the validation split, similar to the setup in C4 dataset (Raffel et al., 2020b). For the model, we use the RoBERTa-base architecture (Liu et al., 2019), where all the layer normalization operations (Ba et al., 2016) are placed before attention and FFN blocks (i.e., we adopt the pre-norm architecture), which leads to much more stable training for efﬁcient attention mechanisms. We replace all softmax attention with EVA to test its effectiveness. The training setting and attention-speciﬁc parameters, which follow previous studies (Xiong et al., 2021a) to ensure a similar computational cost, can be found in Table 11 and Table 12 respectively. Machine Translation. We follow Ott et al. (2018) to process WMT14 En-Dedataset, resulting in around 4.5M/3K/3K English-German sentence pairs for training/validation/testing splits, respectively, and a shared vocabulary is obtained between the source and target language of around 32K BPE types. The architecture and training speciﬁcs closely follow Vaswani et al. (2017), as listed in Table 13. We follow the previous protocol Zheng et al. (2022b) by replacing all encoder self-attention blocks in the encoder-decoder Transformer with EVA. For EVA, we ﬁnd it beneﬁcial to introduce an overlapping variant of E, where we allow E to be overlapped with each other. Following previous practice in the context of local attention (Xiong et al., 2021a), Enot only contains all elements within the designated chunk but also additionally includes half the tokens in its neighboring chunks. As a result, EVA-32 corresponds to |E|= 32with a contiguous chunk size of 16. During inference, we follow the same setup as Zheng et al. (2022b) and average the last 10 model checkpoints to obtain the ﬁnal model parameters. We apply beam search with size 4, length penalty 0.6, and compound split 29Published as a conference paper at ICLR 2023 Table 11: Our hyper-parameter conﬁguration for Masked Language Modeling (MLM). Hyper-parameter MLM Number of transformer encoder layers 12 Hidden size 768 hidden size in FFN 3072 Number of attention heads 12 Batch size 256 Sequence length {2048,4096} Number of training steps 200K Number of warm-up steps 5K Weight decay rate 0.01 Peak Learning Rate 1e-4 Learning rate decay Linear Optimizer Adam Adamϵ 1e-6 Adam(β1,β2) (0.9, 0.98) Gradient Clipping 0.0 Dropout 0.1 Attention dropout (if applicable) 0.0 Table 12: Our hyper-parameter conﬁguration for different attention mechanisms on MLM task. * We used the exact positive random feature map (Choromanski et al., 2021) in our preliminary experiments. However, it failed to converge and exhibited substantial training instability. Therefore, we replace the positive random feature with a simple ReLU kernel function for MLM experiments, which yields better training performance. Attention Hyper-parameter conﬁguration on MLM Local attention Window size 256 Linformer (Wang et al., 2020) Projected dimension 256 Reformer (Kitaev et al., 2020) Number of hashes 4 Chunk size 64 Performer (Choromanski et al., 2021)Number of random feature samples 256 Type of random feature ReLU* LARA (Zheng et al., 2022b) Number of landmarks 256 Combiner (Ren et al., 2021) Mode Fixed Span size 256 Conditional distribution parameterizationDeepSets-Max Transformer-LS (Zhu et al., 2021)Dynamic projection dimension 128 Local window size 256 EVA(Ours) Number of partitioned groups (C) 128 Size ofE 256 post-processing. Since the input sequences in WMT14 En-Debenchmark are much shorter than the other tasks considered in this paper (with an average sequence length of around 25 tokens), we start with C = 8, |E|= 16and gradually increase |E|to test the translation performance, similar to the setup in Ma et al. (2021); Zheng et al. (2022b). Note that increasing Calso leads to better translation quality, although we found the performance gain is slightly less effective than that of increasing|E| (c.f. Tables 6 and 7). Autoregressive Language Modeling. We consider Wikitext-103 benchmark in this task, which consists of around 103M/218K/246K tokens for training/validation/testing splits, respec- tively. We adopt the vanilla transformer decoder architecture (Vaswani et al., 2017), replace all decoder self-attention modules in the Transformer with the causal EVA mechanism, and evaluate EVA under two different setups: 1) a standard 16-layer Transformer LM (with model sizes of around 247M) as in Baevski & Auli (2019), and 2) a larger 32-layer Transformer LM (with model sizes of around 450M) as in Kasai et al. (2021). We follow their hyper-parameter settings to train all models, 30Published as a conference paper at ICLR 2023 Table 13: Our hyper-parameter conﬁguration for machine translation. Hyper-parameter Machine Translation Number of transformer encoder layers 6 Number of transformer decoder layers 6 Hidden size 512 hidden size in FFN 2048 Number of attention heads 8 Maximum number of tokens in a batch 32768 Number of training steps 300K Number of warm-up steps 6K Weight decay rate 0.0 Peak Learning Rate 0.0007 Label Smoothing 0.1 Learning rate decay Inverse square root Optimizer Adam Adamϵ 1e-6 Adam(β1,β2) (0.9, 0.98) Gradient Clipping 5.0 Dropout 0.1 Attention dropout (if applicable) 0.1 Table 14: Our hyper-parameter conﬁguration for autoregressive language modeling. Hyper-parameter LM in Baevski & Auli (2019)LM in Kasai et al. (2021) Number of transformer decoder layers 16 32 Hidden size 1024 1024 hidden size in FFN 4096 4096 Number of attention heads 8 8 Number of tokens in a batch 65536 65536 Number of training steps 286K 286K Number of warm-up steps 16K 16K Weight decay rate 0.0 0.0 Peak Learning Rate 1.0 1.0 Learning rate decay cosine cosine Optimizer nag nag Gradient Clipping 0.1 0.1 Dropout 0.3 0.3 LayerDrop – 0.2 Attention dropout 0.1 0.1 where the corresponding conﬁgurations are listed in Table 14. 5 The vocabulary size is 267,744 with adaptive input embeddings (Baevski & Auli, 2019). During training, we set the sequence length to 512 and evaluate the validation/test PPL with various context window sizes in{256,480}, aligning with previous work (Baevski & Auli, 2019; Kasai et al., 2021). For other random feature baselines, unfortunately, we failed to fully replicate their results as reported in Kasai et al. (2021), where RFA in our implementation achieved a test perplexity of 29.0 even under a 449M Transformer model. For EVA, we set |E|= 128and C = 64by default for both 16-layer and 32-layer settings, ensuring similar computational cost to previous work that also evaluates random feature methods (typically with 128 or 256 random-feature dimension size) on Wikitext-103 language modeling task (Schlag et al., 2021; Kasai et al., 2021). E.4 E XPERIMENTAL SETTINGS OF EFFICIENCY COMPARISON For the simulation experiment conducted in §5.3, we adopt the same transformer architecture across all attention variants. In particular, it uses 8 transformer layers, 192 embedding dimensions, and 2 attention heads so that longer sequences can ﬁt into our devices. The batch size is set to 64 across 5The setup in Baevski & Auli (2019) can be found in the corresponding Fairseq train- ing script: https://github.com/pytorch/fairseq/blob/master/examples/language_ model/README.adaptive_inputs.md. 31Published as a conference paper at ICLR 2023 0 1000 2000 3000 4000 5000 6000 7000 8000 Sequence Length 0 2 4 6 8 10 12 14Memory Consumption (GB) Linformer Local Attention EVA Long-short FlashAttention Softmax (a) Memory consumption. 0 1000 2000 3000 4000 5000 6000 7000 8000 Sequence Length 0 50 100 150 200Running Time (in ms) Linformer Local Attention EVA Long-short FlashAttention Softmax 256 512 5 10  (b) Running time. Figure 2: Left and right: Additional empirical memory consumption and running time comparison for different attention mechanisms under various sequence lengths. 8 V100 GPUs, and the statistics are computed by averaging the results of 30 runs. Besides, in our ablation study, the efﬁciency metrics reported in Table 6 and Table 7 are evaluated under the same setup used during training. Remark on Modeling Short Sequences. Unfortunately, similar to most previous efﬁcient attention baselines, EVA also runs slower than softmax attention under shorter sequences (e.g., length of 128 or 256), but it soon catches up in running speed, and the reduction of memory consumption is still signiﬁcant. Besides, in short-sequence settings (such as the case of DeiT-Tiny/Small with sequences of 196 tokens), EVA often performs on par with or better than conventional softmax attention (see Table 1), whereas most previous attention variants usually perform much worse. This implies EVA can achieve a better trade-off between efﬁciency and quality: for short sequences, EVA is possible to achieve stronger performance competitive with softmax attention (despite in longer running time); while for long sequences, EVA can be run much faster with less memory. Comparison to Memory-efﬁcient Attention Mechanisms. In this section, we conduct an empiri- cal efﬁciency comparison between efﬁcient approximate attention methods and FlashAttention, one of the memory-efﬁcient attention mechanisms (Rabe & Staats, 2021; Dao et al., 2022) with optimized memory accesses. FlashAttention computes the exact softmax attention in an online manner without materializing the full attention matrix, achieving linear memory complexity with respect to sequence lengths; besides, both runtime and memory usage are further improved by minimizing IO accesses. We benchmark different attention modules on one NVIDIA GeForce RTX 3090 GPU, where we measure the memory usage and runtime of running a single attention block, consisting of 8 attention heads with 512 embedding dimension size, for both a forward and backward pass. As shown in Figure 2, we observe that FlashAttention achieves signiﬁcant memory usage reduction for softmax attention approximation and even consumes much less memory than all considered approximate baselines under all sequence lengths. In terms of runtime, we notice that FlashAttention runs faster than most attention baselines under sequence lengths less than 2048 despite scaling quadratically, but EVA, along with other more efﬁcient approximate variants, begin to catch up at longer sequence lengths. This implies that the quadratic computational costs of softmax attention still bottleneck its runtime performance, aligning with one of the main ﬁndings in Dao et al. (2022). According to this empirical study, we observe that FlashAttention offers a general and effective technique to speed up softmax attention; since many approximate variants (including EVA) exhibit a similar formulation to softmax attention (e.g., Equation 16), we expect they can also beneﬁt from the optimized online softmax calculation technique and memory accesses of FlashAttention (Dao et al., 2022). F E XPERIMENTS ON LONG RANGE ARENA Long Range Arena (LRA; Tay et al., 2021) is a lightweight benchmark that assesses the ability of efﬁcient attention methods to model long sequences in diverse domains. We follow the same hyper-parameter setup as Xiong et al. (2021b) to re-evaluate all attention baselines and report the 32Published as a conference paper at ICLR 2023 Table 15: Classiﬁcation accuracy (%) on LRA benchmark with different efﬁcient attention mecha- nisms. Model ListOps Text Retrieval Image Pathﬁnder Avg. Softmax 38.66 64.91 80.70 40.61 68.29 58.63 Linformer 38.21 53.91 77.66 39.40 66.44 55.12 Performer 29.84 65.30 77.70 38.29 66.39 55.50 Reformer 27.12 63.90 78.08 42.40 51.90 52.69 Scatterbrain 38.21 64.04 77.83 42.51 60.62 56.64 Combiner 38.26 63.98 81.47 42.80 55.94 56.49 LARA 37.10 64.62 80.82 38.99 68.96 58.10 Nyströmformer 38.46 65.28 80.44 39.71 68.98 58.57 Local 38.46 63.70 80.71 42.25 68.46 58.72 Long-short 38.56 63.46 81.73 40.54 71.28 59.11 EVA 38.61 64.31 80.21 43.24 70.90 59.45 comparison in Table 15. We observe that EVA largely improves previous RFA methods such as Performer (Choromanski et al., 2021) and LARA (Zheng et al., 2022b), and performs competitively with full softmax attention. Notably, EVA even achieves better average results over all tasks, with higher accuracy on Image and Pathﬁnder benchmarks, suggesting its capability of capturing long-term dependencies. For LRA benchmark, we set all attention-speciﬁc hyper-parameters to 128 (e.g., the number of landmarks in Nyströmformer (Xiong et al., 2021b) and LARA (Zheng et al., 2022b), the window size in local attention and Combiner (Ren et al., 2021), etc.). We set |E|= 128and C = 64 by default for EVA without any further tuning and ﬁnd this setup works well. G C ONNECTIONS TO OTHER ATTENTION MECHANISMS G.1 RFA, S OFTMAX ATTENTION , AND EVA As mentioned in our main text, one of the main contributions of this work is to develop a more general framework that bridges RFA and conventional softmax attention. To see how EVA (Equation 13) achieves this goal formally, note that if either |E|= M or C = M, EVA would be equivalent to standard softmax attention; while if we set |E|= 0and C = 1, EVA would recover vanilla RFA. G.2 C ONNECTIONS TO LARA Notably, EVA and LARA (Zheng et al., 2022b) are two efﬁcient attention mechanisms that are both built upon the self-normalized importance sampling (SNIS) formulation of RFAs. LARA (Zheng et al., 2022b) puts the main focus on the proposal distribution used in SNIS and tries to design importance sampling proposals that are closer to the true underlying distribution. The proposed usage of multiple proposals further improves the estimation quality of SNIS and achieves strong empirical performance while still keeping linear complexity. In contrast to LARA, in this work we do not focus on the design choice of proposals used in importance sampling but aim to generalize the SNIS formulation further via control variates. As demonstrated in §3.2, our theory clearly delineates how the gap between such SNIS estimation and softmax attention can be closed by manipulating control variates. Since LARA and RFA are both SNIS estimators (their main difference lies in the choice of proposal distributions), our generalization also applies to LARA. To summarize, compared with LARA, EVA is a more general framework and improves conventional RFA from an orthogonal perspective. G.3 C ONNECTIONS TO CLUSTERED ATTENTION Clustered attention (Vyas et al., 2020) is an efﬁcient attention mechanism that ﬁrst clusters the set of queries into multiple groups, computes the mean centroid of each group, and then performs attention between query centroids and original key-value pairs. This framework is fast and effective and enjoys well-bounded approximation error. 33Published as a conference paper at ICLR 2023 Clustered attention and EVA share some similarities in two aspects. First, both of them adopt the partitioning technique to reduce the computational complexity while remaining effective; and secondly, both observe that the efﬁcient attention mechanism can be improved by reﬁning the approximation over speciﬁc elements. For instance, clustered attention can be improved (Vyas et al., 2020) by selecting top-kkey-value pairs that are most relevant to each centroid and then reﬁning the approximation by recomputing attention weights over these keys using original queries; while EVA notices that we can directly employ the optimal control variate coefﬁcient for a subset of key-value pairs (m∈E) while still remaining efﬁcient, which yields a more accurate approximation. Nevertheless, our main technical contribution is to develop a control variate formulation in the context of RFA and demonstrate that how RFA can be further improved locally. On the other hand, while clustered attention (Vyas et al., 2020) clusters queries, EVA partitions key-value pairs. This property makes EVA more amenable to the case of autoregressive language modeling since we do not impose clustering structures over the query set, and thus the causal relation among queries can be well maintained. G.4 C ONNECTIONS TO COMBINER Combiner (Ren et al., 2021) is a recently proposed attention mechanism that also partitions the sequence into chunks combined with local attention. The key difference between EVA and Combiner is the motivation, where Combiner introduces a structured factorization over the attention probability distribution, while our approach is built from the control variate perspective. G.5 C ONNECTIONS TO SCATTERBRAIN In this section, we show that Scatterbrain (Chen et al., 2021a) can be cast as a special case of our framework EVA, although they are proposed based on quite different motivations. A Brief Review of Scatterbrain. Scatterbrain (Chen et al., 2021a) notes that sparse attention and RFA can approximate sharp and ﬂat regions of the softmax attention matrix well, respectively. Based on this insight, Scatterbrain is proposed to ﬁrst compute a Performer approximation to softmax attention and then cancel out the approximation error on critical regions via a sparse mechanism. Speciﬁcally, Scatterbrain (Chen et al., 2021a) deﬁnes a sparse matrix S ∈ RN×M) so that for each (n,m) ∈ S that indexes a non-zero entry. For notational simplicity, we also denote Supp(S) ={(i,j)|Sij ̸= 0}and Suppn(S) ={m|Snm ̸= 0}. With random features φ(·,·) deﬁned in Appendix A, we let Snm = exp ( q⊤ nkm ) −φ(qn,ω)⊤φ(km,ω). We then add it back to the approximate output: y′ n = M∑ m=1 φ(qn,ω)⊤φ(km,ω)vm + SV = M∑ m=1 φ(qn,ω)⊤φ(km,ω)vm + ∑ m′∈Suppn(S) Snm′vm′ = ∑ m/∈Suppn(S) φ(qn,ω)⊤φ(km,ω)vm + ∑ m′∈Suppn(S) exp ( q⊤ nkm′ ) vm′. (29) The sparse mechanism can be thought of as modeling the error due to RFA and eliminating it on the support of S. After the correction step, Scatterbrain further adds a post-hoc normalization step to obtain a normalized attention output: yn = ∑ m/∈Suppn(S) φ(qn,ω)⊤φ(km,ω)vm + ∑ m′∈Suppn(S) exp ( q⊤ nkm′ ) vm′ ∑ m/∈Suppn(S) φ(qn,ω)⊤φ(km,ω) +∑ m′∈Suppn(S) exp (q⊤nkm′) . (30) Intuitively, Scatterbrain (Chen et al., 2021a) produces accurate approximation in the support of the sparse matrix and remains the random feature approximation outside the support. 34Published as a conference paper at ICLR 2023 Scatterbrain is a Special Case of EV A. For notational convenience, we denote E := Suppn(S). According to Proposition 1, suppose we employ optimal coefﬁcients ˆβm for all entries in Suppn(S), and use the same coefﬁcient ˆβfor all the remaining entries (in other words, we let C = 1and the whole index set is only partitioned into two subsets {E,[M] \\E}). Then we have ˜gm(ω) = { gm(ω) −ˆβmhm(ω) +ˆβm exp(q⊤ n km) Z = exp(q⊤ n km)vm Z , if m∈E, gm(ω) −ˆβhm(ω) +ˆβexp(q⊤ n km) Z , if m /∈E. And the resulting estimator overall becomes ˜g(ω) = M∑ m=1 ˜gm(ω) = ∑ m∈E ˜gm(ω) + ∑ m/∈E ˜gm(ω) = ∑ m∈E exp(q⊤ nkm)vm Z + ∑ m/∈E ( gm(ω) −ˆβhm(ω) +ˆβexp(q⊤ nkm) Z ) = ∑ m∈E exp(q⊤ nkm)vm Z + ∑ m/∈E ( gm(ω) −ˆβhm(ω) ) + ˆβ ∑ m/∈E exp(q⊤ nkm) Z = ∑ m∈E exp(q⊤ nkm)vm Z + ∑ m/∈E ( gm(ω) −ˆβhm(ω) ) + ˆβ ( 1 − ∑ m∈E exp(q⊤ nkm) Z ) . Scatterbrain (Chen et al., 2021a) can be a special case of this estimation algorithm if we set the proposal distribution to q(ω) =N(ω; 0,I), and estimate the normalizing constant as follows. Z = Eω∼q(ω) [ N(ω; 0,I) (∑ m∈Eξ(qn,ω)⊤ξ(km,ω) +∑ m/∈Eξ(qn,ω)⊤ξ(km,ω) ) q(ω) ] = ∑ m∈E exp(q⊤ nkm) +Eω∼q(ω) [ N(ω; 0,I) ∑ m/∈Eξ(qn,ω)⊤ξ(km,ω) q(ω) ] ≈ ∑ m∈E exp(q⊤ nkm) +1 S S∑ s=1 N(ω; 0,I) ∑ m/∈Eξ(qn,ω)⊤ξ(km,ω) q(ωs) = ∑ m∈E exp(q⊤ nkm) +1 S S∑ s=1 ∑ m/∈E ξ(qn,ω)⊤ξ(km,ω) = ∑ m∈E exp(q⊤ nkm) + ∑ m/∈E φ(qn,ω)⊤φ(km,ω) := ∑ m∈E exp(q⊤ nkm) + ∑ m/∈E ˜hm(ω), where we deﬁne ˜hm(ω) =Zhm(ω), as in this case g(ω) = 1 S S∑ s=1 pn(ωs) q(ωs) f(ωs) = 1 S S∑ s=1 1 Z M∑ m=1 ξ(qn,ωs)ξ(km,ωs)vm, h(ω) = 1 S S∑ s=1 pn(ωs) q(ωs) = 1 S S∑ s=1 1 Z M∑ m=1 ξ(qn,ωs)ξ(km,ωs). 35Published as a conference paper at ICLR 2023 With these speciﬁcations, we obtain ˜g(ω) = ∑ m∈E exp(q⊤ nkm)vm Z + ∑ m/∈E ( gm(ω) −ˆβhm(ω) ) + ˆβ ( 1 − ∑ m∈E exp(q⊤ nkm) Z ) = ∑ m∈E exp(q⊤ nkm)vm Z + ∑ m/∈E ( gm(ω) −ˆβhm(ω) ) + ˆβZ−∑ m∈Eexp(q⊤ nkm) Z ≈ ∑ m∈E exp(q⊤ nkm)vm Z + ∑ m/∈E ( gm(ω) −ˆβhm(ω) ) + ˆβ ∑ m/∈E ˜hm(ω) Z = ∑ m∈E exp(q⊤ nkm)vm Z + ∑ m/∈E ( gm(ω) −ˆβhm(ω) ) + ˆβ ∑ m/∈E hm(ω) = ∑ m∈Eexp(q⊤ nkm)vm Z + ∑ m/∈E gm(ω) = ∑ m∈Eexp(q⊤ nkm)vm Z + ∑ m/∈E 1 S ∑S s=1 ξ(qn,ωs)ξ(km,ωs)vm Z = ∑ m∈Eexp(q⊤ nkm)vm Z + ∑ m/∈E φ(qn,ω)⊤φ(km,ω)vm Z ≈ ∑ m/∈Eφ(qn,ω)⊤φ(km,ω)vm + ∑ m′∈Eexp ( q⊤ nkm′ ) vm′ ∑ m/∈Eφ(qn,ω)⊤φ(km,ω) +∑ m′∈Eexp (q⊤nkm′) (31) which is equivalent to Scatterbrain (Equation 30). Note that this equivalence would hold irrespective of the choice of shared coefﬁcients ˆβ, which possibly indicates that the formulation of Scatterbrain limits the potential beneﬁt of optimizing control variates under our framework. 36",
      "meta_data": {
        "arxiv_id": "2302.04542v1",
        "authors": [
          "Lin Zheng",
          "Jianbo Yuan",
          "Chong Wang",
          "Lingpeng Kong"
        ],
        "published_date": "2023-02-09T10:16:20Z",
        "pdf_url": "https://arxiv.org/pdf/2302.04542v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper characterizes the approximation gap between Random-Feature-based Attention (RFA) and conventional softmax attention through the lens of control variates. It reveals that RFA can be decomposed into a sum of multiple control variate estimators and that exact softmax attention can be recovered by optimizing each control variate's coefficient locally. Based on this, the authors propose an Efficient attention via Control Variates (EVA) mechanism that significantly reduces this approximation gap while maintaining linear runtime and space complexity. EVA achieves this by partitioning the sequence into subsets, optimizing control variates explicitly for high-correlation subsets, and sharing coefficients locally for others. Experiments demonstrate that EVA outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.",
        "methodology": "The core methodology involves generalizing RFA using control variates. RFA is decomposed into a sum of local control variate estimators, one for each element in the sequence. It's theoretically proven that if the coefficient of each control variate is individually optimized to minimize estimation variance, RFA becomes exact, recovering softmax attention. EVA implements a divide-and-conquer strategy: the sequence is partitioned into a fixed number of disjoint subsets. For a selected subset E (expected to have high correlations to the query), control variates are explicitly optimized for each element to recover exact softmax probabilities. For other subsets Pc, a control variate coefficient is shared locally among elements. The sum of exponentials in `sum(exp(q^T k))` for partitioned subsets is approximated by `exp(q^T k_tilde)`, where `k_tilde` is an adaptive vector obtained through average pooling followed by a trainable linear projection and layer normalization. `beta_c(omega)` is parameterized as a localized RFA output for each group `Pc`. EVA is extended to autoregressive modeling using triangular mask matrices.",
        "experimental_setup": "EVA's performance was evaluated on a diverse set of tasks: Image Classification (ImageNet1k using DeiT-Tiny, DeiT-Small, DeiT-Tiny-784, and PVT-v2-B3 architectures), Masked Language Modeling (Books3 corpus with RoBERTa-base), Machine Translation (WMT14 En-De benchmark with a Transformer encoder-decoder), and Autoregressive Language Modeling (Wikitext-103 with 16-layer and 32-layer Transformer decoders). Baselines included Performer, LARA, Linformer, Nyströmformer, Local attention, Reformer, Scatterbrain, Combiner, and Long-Short Transformer, with FlashAttention included for an efficiency comparison. Training protocols varied by task, generally using AdamW or Adam optimizers, cosine or linear learning rate decay, and specific warm-up steps. Metrics were Top-1 Accuracy for image classification, perplexity for language modeling, and BLEU scores for machine translation. Efficiency was measured by running time per iteration and memory consumption under various sequence lengths on NVIDIA V100 and RTX 3090 GPUs.",
        "limitations": "The approximation method used in computing control variate estimation for each partitioned subset is considered crude, potentially limiting the model's full modeling capacity. The study explored only the most straightforward partitioning strategy, which involves evenly splitting the sequence into contiguous chunks. More flexible or adaptive partitioning strategies were not investigated.",
        "future_research_directions": "Future work could investigate and address the current limitations of the framework. This includes exploring more sophisticated or tighter error bounds for the approximation of `sum(exp(q^T k))` in partitioned subsets. Utilizing more expressive pooling methods (beyond simple average pooling) for generating group representations (`k_tilde`) is another potential area. Furthermore, future research could focus on developing and evaluating more advanced partitioning strategies, such as using arbitrary subsequences or adaptive partitioning guided by clustering methods and task-specific inductive biases. The paper also suggests that EVA could benefit from integrating optimized online softmax calculation techniques and memory access strategies like those used in FlashAttention."
      }
    },
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9×on Wiki-40B and 12.1× on PG-19 for auto-regressive language modeling, and 4.8×on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efﬁcient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 × 1.2× 1024 9.0 × 1.3× 2048 8.9 × 1.6× 4096 13.1 × 2.7× 8192 25.6 × 4.9× Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B — All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ﬁnd existing efﬁcient attention methods suffer from at least one of the following drawbacks: • Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efﬁcient attention methods often incur signiﬁcant quality drop compared to augmented Trans- formers, and this drop outweighs their efﬁciency beneﬁts. • Overhead in Practice . As efﬁcient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. • Inefﬁcient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x ∗ gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bms→bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 return tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u ∗ attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ﬁrst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efﬁcient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciﬁcally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efﬁcient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ﬁrst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efﬁcient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efﬁ- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512–8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2×– 4.9×for language modeling on Wiki-40B and a speedup of 1.0×–4.8×for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1×and achieves signiﬁcant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X ∈RT×d be the representations over T tokens. The output for Transformer’s MLP can be formu- lated as O= φ(XWu)Wo where Wu ∈Rd×e, Wo ∈Re×d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and φis an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = φu(XWu), V = φv(XWv) ∈RT×e (1) O= (U ⊙V)Wo ∈RT×d (2) where ⊙stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniﬁed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efﬁciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U ⊙ˆV)Wo where ˆV = AV (3) where A∈RT×T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation ˆvi = ∑ j aijvj “re- trieved” from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ﬁndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = φz(XWz) ∈RT×s (4) A= relu2 ( Q(Z)K(Z)⊤+ b ) ∈RT×T (5) Modiﬁcations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 − →softmax 17.04 / 4.31 105 single-head − →multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiﬁcations on GAU. where Z is a shared representation ( s ≪d)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ﬁnd the softmax in MHSA can be simpliﬁed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciﬁed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiﬁcations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax − →relu2 17.15 / 4.77 110 multi-head − →single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiﬁcations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformer’s MHSA which comes with4d2 param- eters, GAU’s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: • First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.• In addition, the number of attention modules is naturally doubled with GAU — recall MLP+MHSA ≈2×GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneﬁts (speed and RAM efﬁciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as ˆVlin = Q ( K⊤V )    Rd×d approx −−−→ˆVquad = Softmax ( QK⊤)    RT×T V where Q,K,V ∈RT×d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deﬁne Mt = K⊤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mt−1 + KtV⊤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiﬁcantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtV⊤ t into Mt−1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefﬁciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = K⊤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value K⊤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ﬁrst com- puting {KtV⊤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ﬁnd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneﬁts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ﬁrst chunked into G non-overlapping chunks of size C, i.e. [T] →[T/C × C]. Then, Ug ∈RC×e, Vg ∈RC×e and Zg ∈RC×s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAU’s attention can be efﬁciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4×sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: ˆVquad g = relu2 ( Qquad g Kquad g ⊤ + b ) Vg. The complexity of this part is O(G×C2 ×d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: ˆVlin g = Qlin g ( G∑ h=1 Klin h ⊤ Vh ) , (7) Causal: ˆVlin g = Qlin g (g−1∑ h=1 Klin h ⊤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiﬁcant training speedup. Finally, ˆVquad g and ˆVlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug ⊙ ( ˆVquad g + ˆVlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgse→bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bse→bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgms→bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefﬁciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ﬁxed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneﬁt trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciﬁc, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset — All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efﬁcacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ﬁnd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiﬁcant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variants—Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneﬁt trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proﬁler. See Appendix B for detailed settings and model speciﬁcations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset — All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 × as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++’s ﬁnal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1×–2.5×and 1.0×–4.8×, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset — Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 × 43.14 433 1.00 × 42.80 698 1.00 × 43.27 1292 1.00 × Transformer++ 44.47 292 – 43.18 441 – 43.13 712 – 43.26 1272 1.21 × Combiner 46.04 386 – 44.68 376 – 43.99 374 – 44.12 407 – FLASH-Quad 43.40 231 2.18 × 42.01 273 3.29× 41.46 371 3.59× 41.68 560 5.23× FLASH 44.06 234 1.66× 42.17 237 3.85 × 40.72 234 6.75 × 41.07 250 12.12 × * Measured based on time taken to match Transformer+’s ﬁnal quality (at step 125K) on TPU. – Indicates that the speciﬁc model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ﬁndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciﬁcally, FLASH-Quad reduces the training time of Transformer++ by 1.2×to 2.5×and FLASH cuts the com- pute cost by 1.2×to 4.9×while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ﬁnal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23× and 12.12×of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiﬁcantly faster, demonstrating the effectiveness of our efﬁcient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ﬁne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ﬁne-tuning, we sweep over three different learn- ing rates, including 1e−4, 7e−5, and 5e−5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 — “PT“ stands for pre-training and “FT“ stands for ﬁne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, “ﬁrst-to-all” means that we also let the ﬁrst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 ×/ 1.00× Combiner 3.51 67.2 2.78×/ 2.75× FLASH-Quads=128 3.24 72.7 1.89 ×/ 1.79× FLASH-Quads=512 3.12 74.8 1.76×/ 1.67× FLASHs=512 3.23 73.3 2.61 ×/ 2.60× FLASHs=512 + ﬁrst-to-all 3.24 73.9 2.78×/ 2.69× We observe that the ﬁne-tuning results of the FLASH fam- ily can beneﬁt from several minor changes in the model conﬁguration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiﬁcant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciﬁcally, including using a small chunk size (128), disabling gradient clipping during ﬁnetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ﬁrst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8×and 2.7×as fast as Transformer+ in pretraining and ﬁne-tuning, respectively. 4.4. Ablation Studies Signiﬁcance of quadratic & linear components. To bet- ter understand the efﬁcacy of FLASH, we ﬁrst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiﬁcant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other — both are critical to the quality of the proposed mixed chunk attention. Signiﬁcance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2×speedup when the sequence length is greater than 2048), conﬁrming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneﬁcial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (→FLASH) or to TFM++ (→MC-TFM++) — Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quad→FLASH 0.0 0.05 0.06 0.07 0.07 TFM++→MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quad→FLASH -0.05 0.06 0.22 0.30 0.11 TFM++→MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efﬁcient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efﬁcient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 933–941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efﬁcient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 1243–1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651–4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efﬁcient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243–5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiﬁcations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efﬁcient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum ∈RT/C ×d and concatenate them into the local quadratic attention, i.e. ˆVg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h ⊤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C ×T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsum⊤+ bsum ) , ˆVlin g = Qlin g [T/C∑ h=1 alin gh ( Klin h ⊤ Vh )] . We indeed brieﬂy experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conﬁguration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciﬁcations Detailed speciﬁcations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conﬁgurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conﬁgurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conﬁgurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size – 512 – – 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU — Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length ×Batch size Model 512 ×4 1024 ×2 2048 ×1 4096 ×1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefﬁciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefﬁcient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,d→sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size ∗∗ 0.5)) scaledsin ∗= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len ∗= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,d→...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 ∗ cos = x2 ∗ sin, x2 ∗ cos + x1 ∗ sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 ∗ n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 ∗ n = 2]) r = (2 ∗ n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nk→mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) ∗ tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma ∗ x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x ∗ tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale ∗ x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmd→bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel ∗= causal_mask x = u ∗ tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask ∗= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel ∗ causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel, v) x = u ∗ (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the quadratic complexity of Transformer models with long sequences, proposing FLASH (Fast Linear Attention with a Single Head). FLASH introduces a new model family that achieves parity with fully augmented Transformers (Transformer++) in quality while offering linear scalability over context size on modern accelerators. Key contributions include the Gated Attention Unit (GAU) for a more performant and approximation-friendly layer, and a novel mixed chunk attention mechanism that combines local quadratic attention within chunks and global linear attention across chunks, significantly improving auto-regressive training efficiency and overall speed. FLASH demonstrates training speedups of up to 4.9x on Wiki-40B and 12.1x on PG-19 for auto-regressive language modeling, and 4.8x on C4 for masked language modeling, while maintaining competitive perplexity across context lengths from 512 to 8K.",
        "methodology": "The methodology is developed in two steps: First, the Gated Attention Unit (GAU) is proposed, which is a simpler and more performant layer than traditional Transformer blocks. GAU unifies attention and Gated Linear Unit (GLU) by introducing a gating mechanism that allows for a weaker (single-head, softmax-free, using squared ReLU activation) attention without quality loss, making it more amenable to approximation. Second, an efficient method called mixed chunk attention is introduced to approximate the quadratic attention in GAU, achieving linear complexity. This involves chunking the input sequence into non-overlapping segments. Within each chunk, precise local quadratic attention is applied, while across chunks, a global linear attention mechanism captures long-range interactions. For auto-regressive tasks, this global attention performs a cumulative sum at the chunk level, reducing sequential dependency from T steps to G = T/C steps, thereby significantly accelerating training on modern accelerators by minimizing memory re-formatting operations.",
        "experimental_setup": "The models were evaluated on various large-scale datasets across different tasks: masked language modeling (MLM) on the C4 dataset, auto-regressive language modeling (LM) on Wiki-40B and PG-19, and fine-tuning for question answering on TriviaQA. Model scales ranged from 110M parameters (BERT-Base scale) for most experiments to approximately 500M parameters for PG-19. Experiments covered a wide range of context lengths from 512 to 8192 tokens. Training was consistently performed for 125K steps with 2^18 tokens per batch. Performance was measured using negative log perplexity for MLM/LM and F1 score for TriviaQA. The hardware used for main evaluations was 64 TPU-v4 cores, with additional auto-regressive training latency comparisons on a single Nvidia V100 GPU. Baselines included the vanilla Transformer, Transformer+ (with RoPE), Transformer++ (with RoPE and GLU), Performer (ReLU-kernel variant with RoPE), and Combiner (Rowmajor-Axial variant with RoPE). FLASH models utilized SiLU/Swish activation and either ScaleNorm or LayerNorm.",
        "limitations": "The current design employs non-overlapping local attention for practical speed and memory efficiency on accelerators; however, overlapping local attention could potentially improve quality at the cost of increased memory re-formatting operations and slower speeds. The optimal chunk size for the mixed chunk attention can vary with context length and may require hyperparameter search for further performance optimization. Additionally, achieving peak fine-tuning performance for the FLASH family on downstream tasks sometimes benefits from minor configuration tweaks, such as increasing the single-head attention's head size from 128 to 512, disabling gradient clipping, or using softmax for the [CLS] token, suggesting that default settings might not be universally optimal.",
        "future_research_directions": "Future work includes investigating the scaling laws of the new FLASH model family and further exploring and improving its performance on various downstream tasks."
      }
    },
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9×on Wiki-40B and 12.1× on PG-19 for auto-regressive language modeling, and 4.8×on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efﬁcient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 × 1.2× 1024 9.0 × 1.3× 2048 8.9 × 1.6× 4096 13.1 × 2.7× 8192 25.6 × 4.9× Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B — All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ﬁnd existing efﬁcient attention methods suffer from at least one of the following drawbacks: • Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efﬁcient attention methods often incur signiﬁcant quality drop compared to augmented Trans- formers, and this drop outweighs their efﬁciency beneﬁts. • Overhead in Practice . As efﬁcient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. • Inefﬁcient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x ∗ gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bms→bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 return tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u ∗ attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ﬁrst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efﬁcient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciﬁcally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efﬁcient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ﬁrst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efﬁcient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efﬁ- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512–8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2×– 4.9×for language modeling on Wiki-40B and a speedup of 1.0×–4.8×for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1×and achieves signiﬁcant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X ∈RT×d be the representations over T tokens. The output for Transformer’s MLP can be formu- lated as O= φ(XWu)Wo where Wu ∈Rd×e, Wo ∈Re×d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and φis an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = φu(XWu), V = φv(XWv) ∈RT×e (1) O= (U ⊙V)Wo ∈RT×d (2) where ⊙stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniﬁed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efﬁciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U ⊙ˆV)Wo where ˆV = AV (3) where A∈RT×T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation ˆvi = ∑ j aijvj “re- trieved” from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ﬁndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = φz(XWz) ∈RT×s (4) A= relu2 ( Q(Z)K(Z)⊤+ b ) ∈RT×T (5) Modiﬁcations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 − →softmax 17.04 / 4.31 105 single-head − →multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiﬁcations on GAU. where Z is a shared representation ( s ≪d)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ﬁnd the softmax in MHSA can be simpliﬁed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciﬁed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiﬁcations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax − →relu2 17.15 / 4.77 110 multi-head − →single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiﬁcations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformer’s MHSA which comes with4d2 param- eters, GAU’s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: • First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.• In addition, the number of attention modules is naturally doubled with GAU — recall MLP+MHSA ≈2×GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneﬁts (speed and RAM efﬁciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as ˆVlin = Q ( K⊤V )    Rd×d approx −−−→ˆVquad = Softmax ( QK⊤)    RT×T V where Q,K,V ∈RT×d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deﬁne Mt = K⊤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mt−1 + KtV⊤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiﬁcantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtV⊤ t into Mt−1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefﬁciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = K⊤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value K⊤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ﬁrst com- puting {KtV⊤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ﬁnd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneﬁts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ﬁrst chunked into G non-overlapping chunks of size C, i.e. [T] →[T/C × C]. Then, Ug ∈RC×e, Vg ∈RC×e and Zg ∈RC×s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAU’s attention can be efﬁciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4×sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: ˆVquad g = relu2 ( Qquad g Kquad g ⊤ + b ) Vg. The complexity of this part is O(G×C2 ×d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: ˆVlin g = Qlin g ( G∑ h=1 Klin h ⊤ Vh ) , (7) Causal: ˆVlin g = Qlin g (g−1∑ h=1 Klin h ⊤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiﬁcant training speedup. Finally, ˆVquad g and ˆVlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug ⊙ ( ˆVquad g + ˆVlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgse→bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bse→bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgms→bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefﬁciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ﬁxed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneﬁt trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciﬁc, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset — All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efﬁcacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ﬁnd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiﬁcant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variants—Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneﬁt trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proﬁler. See Appendix B for detailed settings and model speciﬁcations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset — All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 × as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++’s ﬁnal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1×–2.5×and 1.0×–4.8×, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset — Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 × 43.14 433 1.00 × 42.80 698 1.00 × 43.27 1292 1.00 × Transformer++ 44.47 292 – 43.18 441 – 43.13 712 – 43.26 1272 1.21 × Combiner 46.04 386 – 44.68 376 – 43.99 374 – 44.12 407 – FLASH-Quad 43.40 231 2.18 × 42.01 273 3.29× 41.46 371 3.59× 41.68 560 5.23× FLASH 44.06 234 1.66× 42.17 237 3.85 × 40.72 234 6.75 × 41.07 250 12.12 × * Measured based on time taken to match Transformer+’s ﬁnal quality (at step 125K) on TPU. – Indicates that the speciﬁc model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ﬁndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciﬁcally, FLASH-Quad reduces the training time of Transformer++ by 1.2×to 2.5×and FLASH cuts the com- pute cost by 1.2×to 4.9×while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ﬁnal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23× and 12.12×of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiﬁcantly faster, demonstrating the effectiveness of our efﬁcient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ﬁne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ﬁne-tuning, we sweep over three different learn- ing rates, including 1e−4, 7e−5, and 5e−5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 — “PT“ stands for pre-training and “FT“ stands for ﬁne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, “ﬁrst-to-all” means that we also let the ﬁrst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 ×/ 1.00× Combiner 3.51 67.2 2.78×/ 2.75× FLASH-Quads=128 3.24 72.7 1.89 ×/ 1.79× FLASH-Quads=512 3.12 74.8 1.76×/ 1.67× FLASHs=512 3.23 73.3 2.61 ×/ 2.60× FLASHs=512 + ﬁrst-to-all 3.24 73.9 2.78×/ 2.69× We observe that the ﬁne-tuning results of the FLASH fam- ily can beneﬁt from several minor changes in the model conﬁguration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiﬁcant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciﬁcally, including using a small chunk size (128), disabling gradient clipping during ﬁnetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ﬁrst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8×and 2.7×as fast as Transformer+ in pretraining and ﬁne-tuning, respectively. 4.4. Ablation Studies Signiﬁcance of quadratic & linear components. To bet- ter understand the efﬁcacy of FLASH, we ﬁrst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiﬁcant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other — both are critical to the quality of the proposed mixed chunk attention. Signiﬁcance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2×speedup when the sequence length is greater than 2048), conﬁrming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneﬁcial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (→FLASH) or to TFM++ (→MC-TFM++) — Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quad→FLASH 0.0 0.05 0.06 0.07 0.07 TFM++→MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quad→FLASH -0.05 0.06 0.22 0.30 0.11 TFM++→MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efﬁcient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efﬁcient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 933–941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efﬁcient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 1243–1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651–4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efﬁcient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243–5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiﬁcations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efﬁcient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum ∈RT/C ×d and concatenate them into the local quadratic attention, i.e. ˆVg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h ⊤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C ×T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsum⊤+ bsum ) , ˆVlin g = Qlin g [T/C∑ h=1 alin gh ( Klin h ⊤ Vh )] . We indeed brieﬂy experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conﬁguration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciﬁcations Detailed speciﬁcations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conﬁgurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conﬁgurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conﬁgurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size – 512 – – 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU — Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length ×Batch size Model 512 ×4 1024 ×2 2048 ×1 4096 ×1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefﬁciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefﬁcient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,d→sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size ∗∗ 0.5)) scaledsin ∗= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len ∗= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,d→...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 ∗ cos = x2 ∗ sin, x2 ∗ cos + x1 ∗ sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 ∗ n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 ∗ n = 2]) r = (2 ∗ n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nk→mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) ∗ tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma ∗ x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x ∗ tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale ∗ x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmd→bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel ∗= causal_mask x = u ∗ tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask ∗= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel ∗ causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel, v) x = u ∗ (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the quadratic complexity of Transformers with respect to sequence length, which limits their application to long contexts, and the shortcomings of existing efficient Transformer variants (inferior quality, practical overhead, inefficient auto-regressive training). It proposes FLASH, a novel model family that achieves quality parity with fully-augmented Transformers while offering linear scalability. Key contributions include the Gated Attention Unit (GAU) for higher-quality attention approximation and an accelerator-friendly mixed chunk attention strategy. FLASH demonstrates significant training speedups (up to 4.9x on Wiki-40B, 12.1x on PG-19) across various tasks and context lengths (up to 8K) without compromising perplexity.",
        "methodology": "The FLASH model is developed in two steps. First, it introduces the Gated Attention Unit (GAU), a simpler yet performant layer that generalizes the Gated Linear Unit (GLU) by incorporating an attentive gating mechanism. GAU uses a weaker, single-head, softmax-free attention (employing squared ReLU activation and per-dimension scaling/offsets for Query and Key from a shared representation Z) which makes it more amenable to approximation without significant quality loss. Second, FLASH employs an efficient mixed chunk attention method to approximate GAU's quadratic attention linearly. This involves chunking the input sequence into non-overlapping segments of size C. Within each chunk, precise local quadratic attention is applied. Across chunks, a global linear attention mechanism is used, with summations performed at the chunk level. For auto-regressive training, this chunk-level summation reduces the sequential dependency from T steps to G = T/C steps, dramatically improving training speed compared to conventional linear attention. The mixed chunk attention ensures linear complexity with respect to sequence length in training and constant O(Cd^2) decoding memory/computation per step at inference.",
        "experimental_setup": "The efficacy of FLASH was evaluated on: Tasks: Bidirectional masked language modeling (MLM) and auto-regressive language modeling (LM). Fine-tuning was also performed on a downstream QA task. Datasets: C4 dataset for MLM; Wiki-40B and PG-19 (for longer sequences and larger models) for LM; TriviaQA for fine-tuning. Model Scales: Models ranged from 110M to 500M parameters. Context Lengths: Experiments covered a wide range of context lengths from 512 to 8192 tokens. Baselines: Vanilla Transformer, Transformer+ (with RoPE), Transformer++ (with RoPE + GLU), Performer (representative linear attention), and Combiner (chunked attention design). All models were implemented in the same codebase for fair comparison. Hardware: Training speed was measured using 64 TPU-v4 cores and a single Nvidia Tesla V100 GPU (for auto-regressive training comparison). Metrics: Perplexity (negative log perplexity) on validation sets, training latency per step, total training cost in TPU-v4-core-days, and F1 score for TriviaQA. Ablation studies were conducted to assess the contributions of local/global attention components, GAU, and chunk size.",
        "limitations": "The paper notes that allowing overlapping local attention (similar to Longformer or BigBird) can improve quality but introduces many memory re-formatting operations that harm actual running speed, suggesting a trade-off where adding more layers might be more cost-effective. It also mentions that the optimal partial attention variant is likely task-specific. For the global linear attention component, an explored variant that combined chunk summaries with an extra attention matrix improved quality but increased auto-regressive decoding complexity to O((C+ T/C)d^2), making it length-dependent, hence not included in the default configuration to maintain constant decoding complexity. The choice of chunk size can affect both quality and training cost, requiring hyperparameter search for optimal performance.",
        "future_research_directions": "Future work involves investigating the scaling laws of the new FLASH model family and its performance on various downstream tasks."
      }
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies and experimentally demonstrates a structural limitation of Graph Attention Networks (GATs): their inability to switch off task-irrelevant neighborhood aggregation, which leads to over-smoothing and hinders performance on heterophilic datasets. To address this, the authors propose GATE, an extension of GAT that can flexibly switch neighborhood aggregation on and off as necessary. GATE alleviates over-smoothing, benefits from higher depth for non-linear feature transformations, and often outperforms GATs on real-world heterophilic datasets. It also offers interpretable learned self-attention coefficients. A synthetic test bed is also constructed to analyze a model’s ability to utilize the appropriate amount of neighborhood aggregation.",
        "methodology": "The core methodology involves modifying the GAT architecture based on theoretical insights from a conservation law of GAT gradient flow dynamics. GATE extends GAT by introducing separate attention parameters for a node's own features (`al_t`) and its neighbors' features (`al_s`), which allows for flexible weighting of their importance. Specifically, the attention mechanism `euv` in GAT (Eq. 3) is replaced with Eq. (4) in GATE, enabling the model to explicitly control the contribution of self-attention versus neighborhood aggregation. The modified conservation law for GATE shows that it can switch off neighborhood aggregation in a well-trainable parameter regime, unlike GAT. Initializations are adapted: `a` parameters are initialized to zero, and `W`, `U`, `V` matrices with an orthogonal looks-linear structure.",
        "experimental_setup": "The experimental setup includes both synthetic and real-world graphs. The synthetic test bed consists of two node classification problems: self-sufficient learning (label-relevant information only in node's own features) and neighbor-dependent learning (label-relevant information in k-hop neighbors' features), using Erdős–Rényi graphs (N=1000, p=0.01) and the Cora dataset structure with original/randomized labels. Real-world evaluations are performed on five heterophilic benchmark datasets (roman-empire, amazon-ratings, questions, minesweeper, tolokers) and three OGB datasets (OGB-arxiv, OGB-products, OGB-mag). Additional small-scale datasets (Cora, Citeseer, Actor, Texas, Wisconsin) with varying homophily levels are also used. Performance is measured by test accuracy or AUC-ROC. Models are trained with Adam optimizer for a maximum of 10000 (synthetic), 2000 (OGB), or 5000 (other real-world) epochs, without weight decay or dropout. Model depth and network width are varied or fixed (e.g., 64 for synthetic and most real-world). Over-smoothing is quantitatively assessed using a modified Dirichlet energy (EGAT).",
        "limitations": "The paper primarily highlights limitations of GATs, which GATE aims to resolve. For GATE, a noted limitation is that in the neighbor-dependent learning task, it cannot achieve perfect 100% test accuracy, attributed to data points close to a non-crisply defined decision boundary. For smaller real-world datasets, models, including GATE, show a drop in performance with increased depth, suggesting potential overfitting without regularization or skip connections. The paper also mentions that determining the optimal degree of smoothing and a threshold for 'over'-smoothing is complex and task-dependent.",
        "future_research_directions": "The paper suggests that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. It also mentions that methods such as graph rewiring to overcome problems like over-squashing are complementary and may be combined with GATE. Another direction could be to derive conservation laws inherent to other GNN architectures like FAGCN and GraphSAGE and study how they govern parameter behavior."
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the learning dynamics of Graph Attention Networks (GATs), revealing that a significant portion of parameters in GATs with standard initialization struggle to change during training, a problem amplified in deeper networks. It derives a novel conservation law of GAT gradient flow dynamics to explain this issue. Based on this insight, the authors propose a balanced initialization scheme that enables more effective gradient propagation, facilitates the training of deeper GATs, and achieves considerable speedup in training and convergence time.",
        "methodology": "The core methodology involves a theoretical derivation of a conservation law for GAT gradient flow dynamics, leveraging the concept of rescale invariance and positive homogeneity of activation functions (e.g., ReLU). This law establishes a relationship between the squared L2-norms of incoming and outgoing feature and attention weights at each neuron. The theoretical insight explains why imbalanced initial weight norms, particularly with standard Xavier initialization, hinder gradient flow in deeper GATs. To mitigate this, a practical balancing procedure is introduced: 1) setting attention parameters to zero, 2) scaling incoming weights of the first layer, and 3) scaling outgoing weights of subsequent layers to match the norms of incoming weights. An advanced 'Balanced Orthogonal Initialization' (BalO) is also proposed, which incorporates a looks-linear (LL) mirrored block structure to further enhance trainability, inspired by dynamical isometry in traditional deep neural networks.",
        "experimental_setup": "Experiments were conducted on nine benchmark datasets for semi-supervised node classification: Cora, Citeseer, Pubmed, Actor, Chameleon, Cornell, Squirrel, Texas, and Wisconsin, using standard train/validation/test splits. The models, specifically GATs (referred to as GATv2), were trained using the Pytorch Geometric framework on Nvidia T4 or RTX 3060 GPUs. Two optimizers, SGD and Adam, were employed for up to 5000 epochs (or until convergence with loss <= 10^-4). Model selection was based on the highest validation accuracy, with results reported as the mean \\u00b195% confidence interval over five runs. The study compared several initialization schemes: standard Xavier (Xav), Xavier with zero attention (XavZ), balanced Xavier (BalX), and balanced LL-orthogonal (BalO). Architectural variations such as multiple attention heads, ELU activation, dropout, weight decay, and unshared weights were also explored. The effectiveness was evaluated based on test accuracy, epochs to convergence, relative parameter change, and relative gradient norms. Comparisons were also made against Lipschitz Normalization and demonstrated for GCNs and \\u00ce\\u00a9GAT.",
        "limitations": "The derived conservation law is specifically applicable to the self-attention mechanisms found in the original GAT and GATv2 models, as well as architectural variations like \\u00ce\\u00a9GAT. It does not directly apply to other forms of self-attention, such as dot-product self-attention (e.g., in SuperGAT), which would require modifications to the conservation law. Additionally, the theoretical assumptions, particularly the positive homogeneity of activation functions, are not met by certain functions like ELU, which can negatively impact the performance of the LL-orthogonal initialization (though Adam optimizer may partially compensate). The research also notes that achieving perfect dynamical isometry in general GNNs (beyond perceptrons) remains an open challenge.",
        "future_research_directions": "Future research could extend the study of learning dynamics to other positive homogeneous models incorporating attention mechanisms, particularly Transformers and Vision Transformers, which often require significant depth. Further investigation into how dynamical isometry can be achieved or approximated in general Graph Neural Networks (GNNs) beyond current approaches for perceptrons is also suggested. Additionally, adapting and modifying the derived conservation law to accommodate different types of self-attention mechanisms, such as dot-product self-attention used in models like SuperGAT and other Transformer-based architectures for graph learning and Large Language Models (LLMs), is identified as an intriguing direction."
      }
    },
    {
      "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
      "abstract": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
      "full_text": "Accelerating Transformer Pre-training with 2:4 Sparsity Yuezhou Hu1 Kang Zhao Weiyu Huang 1 Jianfei Chen 1 Jun Zhu 1 Abstract Training large transformers is slow, but recent innovations on GPU architecture give us an ad- vantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a “flip rate” to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through es- timator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model’s quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two tech- niques to practically accelerate training: to calcu- late transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar con- vergence to dense training algorithms on several transformer pre-training tasks, while actual ac- celeration can be observed on different shapes of transformer block apparently. Our toolkit is avail- able at https://github.com/huyz2023/ 2by4-pretrain. 1. Introduction Pre-training large-scale transformers is hard, for its intensive computation and time-consuming process (Anthony et al., 2020). To accelerate training, sparsity-based methods have recently emerged as a promising solution, and one of the hardware-friendly sparse patterns is 2:4 sparsity. In a 2:4 sparse matrix, every four consecutive elements contain two 1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University. Correspondence to: Jianfei Chen <jianfeic@tsinghua.edu.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). zeros. Within a tensor core, a 2:4 sparse matrix multiplica- tion (2:4-spMM) could be 2x faster than its dense equivalent on NVIDIA Ampere architecture GPUs. Some works use 2:4 sparsity for accelerating training (Hubara et al., 2021; Lu et al., 2023; McDanel et al., 2022; Chmiel et al., 2023). However, they mainly target on con- volutional neural networks (CNNs) (Hubara et al., 2021; McDanel et al., 2022), whose architecture, optimizer and training procedure are different from transformers. Whether these 2:4 sparse training methods are capable for transform- ers remains under-explored. In practice, we find two bar- riers: 1) Low accuracy. The hyperparameters in some accuracy preserving techniques for transformers vary sig- nificantly from that for CNNs, which is ineffective if trans- planted directly. Remarkably, simply halving the inner di- mensionality of a feed-forward network can also reduce the same amount of computational cost, but provides bet- ter performance than most of proposed 2:4 sparse training methods. 2) Inefficiency. All previous works on 2:4 training stay on simulation, and do not provide actual acceleration results. Besides, they don’t focus on other key operations be- yond matrix multiplication that affect the practical time cost, such as overheads of pruning and activation functions. They usually lead to substantial mismatches between simulation and actual acceleration performance. In this work, we aim to propose an end-to-end acceleration method for pre-training transformers based on 2:4 sparsity. Here are our major contributions: • We propose three accuracy-preserving techniques (two for masked decay and one for dense fine-tune) for 2:4 training. First, we propose to apply the masked decay on gradients rather than on weight. Second, we show that the feasible masked decay factor on transformers may be very small (100x smaller than it has been reported on CNNs) and devise a method to quickly determine an available decay factor. Besides, our analysis demonstrates that employing a dense fine-tuning stage at the end of pre- training, rather than at the beginning, can enhance the quality of transformers. • We analyze practical factors affecting the 2:4 training speed of transformers, which is rarely considered by pre- vious works. We identify two speed bottlenecks: prun- ing overhead and gated activation functions’ overhead. 1 arXiv:2404.01847v3  [cs.LG]  27 Oct 2024Accelerating Transformer Pre-training with 2:4 Sparsity We proposed kernel-level accelerated methods to address each of these bottlenecks. • To the best of our knowledge, this is the first report on end-to-end acceleration on pre-training transformers (Fig- ure 7, Table 11). Experiments show that transformers pre-trained using our proposed sparse training scheme are comparable or even superior in accuracy to those trained with dense training methods (Table 5, 6). 2. Related Work Existing sparsity-based methods can be classified into two categories: accelerating inference and accelerating training. For training acceleration, they can be further grouped by whether 2:4 sparsity is involved. Sparsity for Inference Acceleration Early methods in- clude one-shot pruning (Han et al., 2015; 2016; Lee et al., 2018; Mishra et al., 2021). Later methods (Evci et al., 2021; Zhou et al., 2021; Lasby et al., 2023) suggest using dynamic sparse training (DST). Particularly, Zhou et al. (2021) pro- poses sparse-refined straight-through estimator (SR-STE) for 2:4 inference. Iterative magnitude-based pruning (IMP) methods (Chen et al., 2020; 2021; You et al., 2022), orig- inated from the winning lottery ticket theory (Frankle & Carbin, 2019; Frankle et al., 2020), can also be viewed as a DST approach. All these methods only speedup the forward pass. They are insufficient to accelerate training. 2:4 Semi-Structured Sparsity for Training Acceleration Accelerating training by 2:4 sparsity is hard, because both the forward and backward passes need to be accelerated. On some GPUs involving sparse tensor cores, 2:4-spMMs perform 2x faster than dense GEMMs (Mishra et al., 2021; BUSATO & POOL). In light of this, (Hubara et al., 2021) firstly proposes a transposable N:M mask to accelerate both output activations and input gradients computation in back- ward pass. Zhang et al. (2023) improve transposable mask to bi-directional mask (Bi-Mask) to further boost mask di- versity. To accelerate calculating weight gradient via 2:4- spMM, an unbiased minimum-variance estimator (MVUE) is introduced (Chmiel et al., 2023). In addition, Xu et al. (2022) also achieve fully sparse training of CNNs using spatial similarity. However, all these works do not report end-to-end training speedups on 2:4 sparse tensor cores, and they are built for CNNs. Practical 2:4 training acceleration on transformers has not been reported so far. Other Structured Sparsity for Training Acceleration Structured sparsity means channel-wise pruning to dense networks. For instance, training a large model and then compressing it to be thinner or shallower seems effective (Li et al., 2020; Zhou et al., 2020), given a fixed accuracy requirement. However, it’s not memory-efficient due to the larger model’s redundancy. In addition, low-rank adaption proves to be an effective method to reduce fine-tuning costs (Hu et al., 2023), but it can’t accelerate the pre-training. 3. Preliminary In this section, we first present the mathematical formula- tions of dense training and fully sparse training. Afterward, we revisit the related methods which are helpful to achieve fully sparse training with 2:4 sparsity, including SR-STE (Zhou et al., 2021), transposable N: M mask (Hubara et al., 2021), and MVUE (Chmiel et al., 2023). 3.1. Dense Training Problem Formulation Dense training solves an opti- mization problem minw L(w), where L is a loss function, w ∈ RD is the collection of dense weights of all layers, flat- tened to a vector. The loss is optimized by gradient descent optimization algorithms such as SGD, Adam (Kingma & Ba, 2017) and AdamW (Loshchilov & Hutter, 2019). GEMMs of a Linear Layer in Dense Training In each training step, a single linear layer performs three general matrix multiplications (GEMMs): Z = XW⊤, ∇X = ∇ZW, ∇W = ∇⊤ ZX, (1) where X, W and Z are input activations, weights, and out- put activations, with shape X, ∇X ∈ Rp×q, W, ∇W ∈ Rr×q, and Z, ∇Z ∈ Rp×r. Here, the three GEMMs com- putes output activations, input activation gradients, and weight gradients, respectively. Without loss of generality, we assume the input X to be a 2D matrix rather than a 3D tensor. In the feed-forward networks of a transformer, this can be done by simply flattening the input tensors’ first two axes, i.e., axes of batch size and sequence length. 3.2. Fully Sparse Training with 2:4 Sparsity GEMMs can be accelerated with structured sparsity. Partic- ularly, 2:4 sparsity (Mishra et al., 2021) is a semi-structured sparsity pattern supported on NVIDIA Ampere architec- tures. A 2:4 sparse matrix partitions its elements into groups of four numbers, where each group has exactly two zeros. Depending on the direction of partition, there are row-wise 2:4 sparse matrix and column-wise 2:4 sparse matrix; see Appendix A.1. With such sparsity, a GEMM C = AB can be accelerated by 2x with the 2:4-spMM kernel if either A is row-wise 2:4 sparse, or B is column-wise 2:4 sparse. To accelerate training, each GEMM in Equation (1) should have one 2:4 sparse operand. In general, weights and out- put activation gradients are selected to be pruned due to relatively lower pruning-induced loss (Chmiel et al., 2023). 2Accelerating Transformer Pre-training with 2:4 Sparsity That is, Z = XSwt(W⊤), (2) ∇X = ∇ZSw(W), (3) ∇W = Sz(∇⊤ Z)X. (4) In Equations (2) to (4), Swt, Sw, and Sz represent the prun- ing functions of W⊤, W, and ∇⊤ Z. They take dense matri- ces as input, and outputs 2:4 sparse matrices. By intuition, a pruning function picks out the 2 elements with the max magnitudes in the adjoining 4 elements and zero out the rest. With hardware support, computing Equations (2) to (4) can be theoretically 2x faster than Equation (1). This method use 2:4-spMMs for all matrix multiplications in forward and backward propagation, so we call it fully sparse training (FST). Note that Equation (4) contains a straight-through estimator (STE), which we will explain later. Transposable Masks Hubara et al. (2021) suggest that a weight matrix and its transpose can be simply pruned by multiplying binary masks, i.e., Swt(W⊤) =W⊤ ⊙ Mwt, S w(W) =W ⊙ Mw, where Mwt, Mw ∈ {0, 1}p×q are 2:4 sparse, and ⊙ is element-wise product. To utilize 2:4-spMM, the two binary masks should be mutually transposable: Mwt = M⊤ w, (5) which they call as transposable masks (same as our defina- tion in Section 5.1). In this manner, the backward pass share the same sparse weight matrix with the forward pass. The authors also propose a 2-approximation method for generat- ing such masks with claimed low computational complexity. Minimum-Variance Unbiased Estimator Chmiel et al. (2023) propose to calculate the 2:4 sparse masks of neural gradients by MVUE, i.e., Sz(∇⊤ Z) = MVUE(∇⊤ Z). (6) Compared to the commonly used minimum square error esti- mation, MVUE guarantees unbiasedness and minimizes the variance of the sparsified gradients, which is more favorable for promoting the convergence of training. 3.3. Optimization Strategies for Sparse Training The optimization of a sparse network is difficult as it has non- differentiable pruning functions. The optimization objective can be formulated as minw L(˜ w). The network makes prediction with a sparse weight vector ˜ w= m(w) ⊙ w, where the mask m(w) ∈ {0, 1}D is the concatenation of masks for each layer. If a layer is not sparsified, then the corresponding mask is an all-one matrix. Computing the gradient is tricky since the mask m is dynamically com- puted based on the dense weight w: by chain rule we have ∇wL(˜ w) =∂ ˜w ∂w ∇˜ wL(˜ w), where ∂ ˜w ∂w is a Jacobian matrix. However, ˜w is not differentiable with w since it includes a non-differentiable mask-computing-function m(·) in it. Thus, it takes some skills to estimate the gradients and up- date the parameters. STE As ˜w is an approximation of w, a straight-through estimator (STE, Bengio et al. (2013)) directly passes the gradient of ˜w to w: ∇wL(˜ w) ← ∇˜ wL(˜ w). (7) SR-STE There is a problem with STE: only a portion of the weights in a layer participate in the forward calculation, but all the weights receive gradients. This indicates that the gradients associated with masked weights1 might be inac- curate. To suppress those inaccurate gradients, Zhou et al. (2021) proposes sparse-refined straight-through estimator (SR-STE) which adds a decay term when updating: wt ← wt−1 − γ(∇wLt(˜ wt−1) +λW (m(wt−1)) ⊙ wt−1), (8) where γ stands for the learning rate, λW is the decay fac- tor, and m(wt−1) denotes the logical not operation of m(wt−1). This decay term alleviates the change of weight mask. With SR-STE, the optimization target becomes min w L(˜ w) +λW 2 ∥w ⊙ m(w)∥2 2. (9) 4. Accuracy Preserving Techniques While the methods reviewed in Section 3 can successfully perform FST on small-scale models such as ResNet and DenseNet, it is not clear whether they can be directly ap- plied to pre-train large transformers. It is challenging for FST to preserve the accuracy of dense training, since the weights and masks need to be learned jointly, which is a non- differentiable, combinatorial optimization problem. More- over, unlike inference acceleration methods, FST has no pre-trained dense model to start with. In this section, we pro- pose three practical techniques to improve the convergence of FST for transformers: transformer-specific masked decay, Fast decay factor determination and dense fine-tuning. 4.1. Flip Rate: Stability of Training Inspired by previous work (Zhou et al., 2021; You et al., 2022), we define a “flip rate” to measure how frequently the mask vector changes after one optimizer step. This metric could be used to monitor whether the network connection is stable during training. 3Accelerating Transformer Pre-training with 2:4 Sparsity Figure 1.Flip rates change throughout the training of differentλW on Transformer-base. Note that these models utilize an identical learning rate schedule. Table 1.Training results of different λW on Transformer-base. As λW increases from 0 to 2e-4, accuracy first rises and then drops, which means that λW should be neither too big nor too small to reach the optimal results. λW AVG EPOCH LOSS VAL LOSS TEST BLEU DENSE 4.558 3.978 26.15 0 (STE) 4.76 4.164 24.98 6E-7 4.684 4.079 25.68 6E-6 4.626 4.033 25.81 2E-6 4.64 4.041 25.94 2E-5 4.642 4.049 25.74 2E-4 4.662 4.06 25.62 Definition 4.1. Suppose wt is a D-dimensional weight vector at time t, and the flip rate rt is defined as the change in proportion of the mask vector after an optimizer step: rt = ∥m(wt) − m(wt−1)∥1/D ∈ [0, 1]. The larger rt is, the more unstable the network connections become. You et al. (2022) suggest that a sparse neural network acts differently in different training phases. In the early phase of training, it eagerly explores different connection modes, which means the masks vector change rapidly over time. Later, the masks gradually become stable, and the network turns itself to fine-tune weight values. In terms of flip rate, we hypothesize that A healthy training process comes with the flip rate rt rising at the beginning of training and then gradually fading to 0. We measure flip rate change for dense training, STE and SR-STE with different λW in Figure 1. For dense training, we compute the flip rate by pruning the dense weight in each iteration, despite the pruned weight is never used for training. In terms of flip rate, dense training is healthy: itsrt exactly increases first before declines. If a training process 1Unlike some relevant literature, we use “masked weights” and “pruned weights” to denote the weights that are set to 0. consistently has higher flip rate than dense training, which we call as “flip rate explosion”, it may suffer from a loss in final accuracy due to unstable training; see Table 1. In practice, STE suffers from a flip rate explosion, while SR- STE takes effect by “freezing” masks of weights: by adding a decay term, it decrease the number of flips. This inhibition effect is related to the decay factor of SR-STE: the larger λW is, the stronger the inhibition of flips is, and the smaller flip rate goes. In this section, all methods we propose involve our ultimate principle: the peak of the curve should be sufficiently high to fully explore different connection modes, and the tail should be sufficiently low for the optimization process to converge. 4.2. Transformer-Specific Masked Decay Based on our insights on flip rate, we propose a method to suppress the frequent change of masks during FST for transformers, which we call masked decay. Unlike Equation (8) which imposes regularization directly on weights, we propose to add masked decay on gradients, i.e., gt ← ∇wLt(˜ wt−1) +λW (m(wt−1) ⊙ wt−1). (10) On SGD, applying decay on weights and on gradients are equivalent, but on popular optimizers like Adam and AdamW they aren’t. Specifically, Adam updates weights by wt ← wt−1 − γ(β1ut−1 + (1− β1)gt) (1 − βt 1)(√ˆvt + ϵ) (11) where u and v are the first and second order momentum of w. Compared to Equation (8), the masked decay regu- larization term in Equation (10) would be later normalized by √ˆvt + ϵ in Equation (11), before it is subtracted from weights. In this way, each dimension receives a different intensity of decay (“masked decay”). More specifically, weights with larger gradients get smaller decay intensity, and vice versa. In FST, we periodically prune weights by their magnitudes. STE may cause the network to fall into such “dilemma points”, where a portion of pruned weights and unpruned weights have nearly the same L1 norm. Thus, the network consistently oscillate between two possible masks m1 and m2, and is unlikely to jump out the dilemma itself. To illustrate this, we split each weight matrix by small 4 × 4 blocks. We count each block’s cumulative flip number and measure the ”L1 norm gap” by gi = ∥m1 ⊙ wi∥1 − ∥m2 ⊙ wi∥1, where wi is the i-th 4 × 4 weights, m1 ⊙ wi and m2 ⊙ wi have the first and second largest L1-norm among different pruning binary masks. The selected mask is most likely to oscillate between m1 and m2, especially when gi is small. In STE, there exists more 4 × 4 blocks 4Accelerating Transformer Pre-training with 2:4 Sparsity Figure 2.Scatter plots of cumulative flip number and L1 norm gap gi on every 4 × 4 block. All results are selected on Transformer- base, with epoch=20. (a) shows the result of dense model. (b)-(d) shows that of masked decaying on gradients, no decaying, and masked decaying on weights. Also, we do it on purpose to choose an extremely large λW for SR-STE. Figure 3.Applying masked decay on weights takes no effect to inhibit flip rate on BERT-base (compared to applying directly on gradient). Table 2.Optimal λW for multiple models. MODEL OPTIMAL λW RESNET18 (Z HOU ET AL ., 2021) 2 E-4 BERT-BASE 6E-6 TRANSFORMER -BASE 1E-6 DEIT-TINY 2E-3 GPT-2 124M 6 E-5 350M 2 E-4 774M 2 E-4 1558M 6 E-5 with high flip num and low ”L1 norm gap”; see Figure 2. This results in overall flip rate explosion of STE. On these occasions, we argue that an evenly masked de- cay applied on weights is insufficient to save the training from such “traps”. The weights don’t differentiate them- selves after an update, so masks may oscillate back. By normalizing the weight gradients with √ˆvt + ϵ, our masked decay amplifies the regularization strength for the dimen- sion with smaller gradient, pushing it towards zero. Then, the regularized dimension can no longer compete with other dimensions. So we effectively break the tie and push the training process out of the trap, towards a “healthier” state. The comparison results between our masked decay defined in Equation (10) and the conventional counterpart in Equa- tion (8) are shown in Figure 3. Results show that applying masked decay on weights takes no effect to inhibit flip rate explosion of STE, while applying on gradients works fine. 4.3. Fast Decay Factor Determination The determination of the decay factor λW in Equation (10) is non-trivial: if λW is excessively large, then the “peak” of the flip rate curve is not high enough; ifλW is too small, the “tail” of the curve is not low enough. Both do not provide a healthy training process. Besides, we find that λW values for CNNs and other small-scale networks differ significantly from those for transformers, while on transformers, optimal λW can span up to three orders of magnitude (Table 2). As pre-training large transformers is costly, grid searching for λW with the final accuracy is impractical, so it is vital to determine a feasible λW as quickly as possible. To quickly determine λW , here we propose a test-based method: 1) Grid search on the warm-up stage of training. For each λW value in a candidate set, sample a corresponding flip rate of the sparse network from a small number of training steps. Note that sampling in early training stage is enough to obtain a representative flip rate specific to a sparse network. 2) Comparison with the dense counterparts. Suppose rt0 to be the standard flip rate on the dense network at time t0 and r ′ t0 to be the sparse network’s flip rate. Their ratio is µ = r ′ t0/rt0 . We suggest that a feasibleλW should have µ ∈ [0.60, 0.95] and the sparse network may suffer from an accuracy drop if µ ≥ 1. 4.4. Dense Fine-Tuning To better improve accuracy, we suggest using a “dense fine- tuning” procedure at the end of training. Formally, we select a switch point ts. FST is performed while t ≤ ts, and dense training is switched to if t > ts. Why Choose Dense Fine-Tuning Instead of Dense Pre- training? While previous work (Han et al., 2017) suggest to switch between sparse and dense training stages, some recent works like STEP (Lu et al., 2023) utilize dense pre- training rather than dense fine-tuning, which means a dense network is initially trained for a period of time before being switched to a sparse one. However, we argue that dense pre- training is meaningless in our FST process. As described in 5Accelerating Transformer Pre-training with 2:4 Sparsity Figure 4.Dense fine-tuning versus dense pre-training on BERT- base Section 4.1, the peak of the flip rate curve should be suffi- ciently high to explore connection modes, so what matters most to the flip rate is the magnitudes of weights, which are the key to determine if connections are built or demol- ished. In this regard, both FST and dense pre-training are capable of delivering proper gradient magnitudes, so dense pre-training is a waste. The precise gradients are generally more necessary in the later stages of training, where the flip rate of the dense network comes to its tail. Figure 4 visual- izes the loss curve of pre-training BERT-base, where dense pre-train obtains nearly the same result as the naive SR-STE method. From this, we propose the following insight: If dense pre-training of tα steps provides slight improve- ment of accuracy, then moving the tα dense steps to the end gives far more improvement than dense pre-training. As for the specific position of the switch point in training, STEP (Lu et al., 2023) suggests that the dense pre-training occupy 10% to 50% of the total steps. Likewise, we deter- mine that our dense fine-tuning takes up the last 1/6 of total steps for balance training efficiency and accuracy. 5. Training Acceleration Techniques For transformers, the forward pass of FST involves prun- ing weights in FFNs with transposable 2:4 masks and then performing normal forward propagation. During backward propagation in FST, the gradients of input activations and weight gradients in FFNs are derived by Equation (3) and (4), respectively. Note that we also utilize MVUE to prune gradients of output activations, i.e., Equation (6). Compared to dense training, our FST replaces all the GEMMs in FFNs with 2:4-spMMs that theoretically perform 2x faster than their dense counterparts on GPUs within sparse tensor cores. In addition to speeding up the most time-consuming GEMMs in FFNs, there are three major operations that also have non-negligible impacts on training speed: 1) Pruning. In FST, pruning includes two steps: finding a mask that satisfies the 2:4 sparse patterns and then enforc- ing the mask to the corresponding dense matrices. In our case, we find that the time cost of finding transposable masks is time-consuming. 2) Activation functions. In transformers, SwiGLU and GEGLU (Shazeer, 2020) are popular. These two acti- vation functions involve a gate mechanism to regulate activations. This mechanism easily induces the GPU L2 cache misses, thus decreasing the computing speed. 3) Updating optimizer states. The excessive update fre- quency can introduce additional time overheads. Below, we show our methods to accelerate these operations, the main workflow of which is shown in Appendix B. 5.1. Fast Computation of Transposable Masks Problem Formulation We aim to find such a mask matrix M ∈ {0, 1}r×q for every W ∈ Rr×q in the FFN layer that 1) each adjoining 4 × 4 block contains 8 non-zero positions; each row and column in the block occupies 2 non-zero elements exactly; 2) maxM ∥M ⊙ W∥1. Then M would be our targeting transposable mask. As described in Equation (5), both a transposable mask itself and its transposition conform to the format of 2:4 sparsity. Previous 2-approximation algorithm (Hubara et al., 2021) consists of two steps: sort elements, and pick elements out of the array. They claim that the procedure has less computational complexity. However, in practice, the sorting and picking process contains too many jumps in its control flow, and may be fatal to modern GPU architecture. To make full use of the GPUs’ parallel computation capability (SIMD and SIMT), we convert the transposable mask-search process into a convolution operation which traverse all the masks to obtain the optimal one in three steps: 1) Create a convolutional kernel in the shape of 4 × 4 × nt, where nt denotes the number of transposable masks. In the case of 2:4 sparsity, mask diversity nt = 90. These mask blocks for 2:4 sparsity can be selected by exhaus- tively inspecting all potential masks offline. 2) Calculate the index matrix via Algorithm 1. The index matrix denotes which 4 × 4 mask in the convolutional kernel is the optimal mask that retains most of the weight norms after being applied to weights. Algorithm 1 transposable mask search Input: mask pattern m′, weight matrix W 1. W = abs(W) 2. out = conv2d(W, m′, stride= 4, padding= 0) 3. index = argmax(out, dim= 2) return index 3) Replace all the elements in the index matrix by the cor- responding 4 × 4 block, which is the desired mask. 6Accelerating Transformer Pre-training with 2:4 Sparsity Figure 5.Transposable mask search Figure 6.left: adapted method; right: intuitive method Table 3.Throughput of two transposable search kernels on RTX3090 (TB/s). INPUT METHOD 2-APPROX OURS FP16 FP32 FP16 FP32 3072 × 768 18.5 36.4 69.2 104.7 4096 × 1024 22.5 38.4 91.9 131.5 5120 × 1280 22.6 44.4 91 128.2 1024 × 1600 22.8 44.8 95 134.5 8192 × 2048 23 45.1 99.4 142.9 16384 × 4096 23.2 45.4 100.1 144.8 30768 × 8192 23.2 45.5 100.9 145.1 Table 4.Throughput of two GEGLU implementations on RTX3090 with fp16 column-major input tensors (TB/s). INPUT METHOD INTUITIVE OURS 32 × 512 × 768 18.4 55.5 32 × 512 × 1024 19.9 55.7 32 × 512 × 1280 18.2 55.9 32 × 512 × 1600 18.4 55.9 32 × 512 × 2048 19.5 56 32 × 512 × 4096 11.8 56.1 32 × 512 × 8192 12.1 56.2 Notably, step (1) is executed offline. Step (2) and (3) are fre- quently performed during FST. The workflow of our method is shown in Figure 5. Compared to the 2-approximation al- gorithm, our method is up to about 5 times faster (Table 3). 5.2. Acceleration of Gated Activation Functions Activation functions with gated mechanisms are widely used in transformers such as GLM (Du et al., 2022) and LLaMA (Touvron et al., 2023). Typical gated activation functions involve SwiGLU and GEGLU. The bottleneck of such activation functions is that the gate operations easily incur GPU L2 cache miss. Take GEGLU as an example: GEGLU(X, U, V, b, c) = GELU(XU⊤ +b)⊙(XV⊤ + c), where X ∈ Rp×q, U, V ∈ Rr×q, b, c ∈ Rr. In prac- tice, this function is composed of three steps: 1) Concatenate U and V into a new weight matrix W ∈ R2r×q, and b, c into a new bias vector d ∈ R2r. 2) Directly calculate Z = XW⊤ + d ∈ Rp×2r as a com- pressed matrix. 3) Split the Z in the second dimension intoZ1, Z2 ∈ Rp×r. Calculate GELU(Z1) ⊙ Z2. Different from dense model, where output activations are row-major matrices, in FST, the output activations are column-major; see Appendix A.2. This property results in the third step being extremely time-consuming if conven- tionally Z is accessed along the row dimension. To illustrate, Figure 6 shows that in a column-major matrix Z, accessing along the column accords with array layout. Thus, adjacent elements loaded into the GPU cache can be probably hit. By contrast, accessing along the row does not fully utilize the efficiency of GPU cache. In light of this, we carefully imple- ment a GEGLU kernel where elements are accessed along the column dimension. In this way, GEGLU is performed 5 times faster than the naive counterpart; see Table 4. 5.3. Other Implementation Details Reducing Updating Frequency We find that a 2:4 mask doesn’t change a lot after one optimization step, and it is not necessary to update a mask frequently. For the sake of efficiency, we update the transposable masks of weights every l optimizer steps. We usually take l = 40in practice. Utilities For 2:4-spMMs, we use CUTLASS (Thakkar et al., 2023). Other GPU kernels are implemented in Triton, including transposable mask search kernel, pruning kernel, MVUE kernel, GEGLU kernel, and masked decay kernel. 6. Experiments In this section, we validate the proposed training speedup methods on several transformers, including BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), Transformer- 2Results reported in the original paper; see https: //github.com/facebookresearch/deit/blob/ main/README_deit.md. 3DeiT-base dense model using the original recipe. 7Accelerating Transformer Pre-training with 2:4 Sparsity Table 5.GLUE scores of different 2:4 training methods with BERT. METHODLOSS AVG SCORECOLA MNLI MNLIEXTRAMRPC QNLI QQP RTE SST-2 STS-B DENSE 2.066979.8±0.4 45.3±1.1 82.6±0.2 83.4±0.1 78.8±1.7/86.1±1 89 .3±0.2 90.3±0.1/87.1±0 55.8±0.9 91±0.5 83 .7±1/83.7±1HALF 2.128077.9±0.4 37.2±1.3 82.4±0.1 83±0.3 75 .1±1.4/84.2±0.7 88 .8±0.3 89.9±0.1/86.6±0.1 51.2±2.4 92.1±0.5 82.1±0.5/82.3±0.4STEP 2.1179 77.7±0.1 40.4±1.4 82.2±0.1 82.8±0.1 74.5±0.7/83.5±0.4 88 .3±0.4 90.2±0.1/87±0.1 50.8±2.1 92.3±0.3 79.7±1.2/80.7±0.6BI-MASK2.117677.7±0.3 38.3±0.7 82.3±0.1 83±0.1 74 .3±0.7/83±0.6 88 .3±0.3 90.2±0.1/86.9±0.1 53.1±1.4 90.9±0.3 80.9±0.7/81.7±0.4OURS 2.0968 79.6±0.6 44.4±1.9 82.6±0.2 83±0.1 80.9±0.7/87.4±0.4 88.4±0.3 90.3±0.1/87±0.1 54.3±1 91.2±0.4 82.9±2.1/83±1.7 Table 6.GLUE scores with different model sizes on GPT-2 models. PARAMSMETHODVAL LOSSAVGSCORECOLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI 124M DENSE 2.907 73.9±1.1 44.6±0.9 82±0.1 78.3±1.3/84.8±1 88 .4±0.2 90±0 86 .5±0/61.3±1.5 91 .9±0.2 77.3±3.2/77.9±2.9 24.3±7.1OURS 2.952 74.3±0.5 44.8±1.3 81.5±0.2 77.5±1.8/84.2±1.3 87.8±0.1 89.5±0.1 85.9±0.1/66±1 90.6±0.4 80±0.8/80.3±0.5 23.9±6.4 350M DENSE 2.618 76.3±0.1 54.3±0.4 85.1±0.1 80.7±1/86.6±0.7 90 .7±0.1 91±0.1 87.8±0.1/64.9±1.7 93.5±0.4 81.7±1.2/82.2±0.8 17.6±3.2OURS 2.688 77.1±0.2 51.8±1.8 84.3±0.1 80.6±1.3/86.5±0.8 90.4±0.2 90.7±0.1 87.5±0.1/66.7±1.3 93.3±0.4 83.4±1.1/83.5±1.1 26.4±4 774M DENSE 2.493 76.2±0.4 57.5±2 86.1±0.1 80.3±1.3/86.4±0.9 91.4±0.2 91.1±0.1 88±0.1/67.7±2.6 94 .6±0.4 77.3±3.3/78.4±2.9 15.1±2.3OURS 2.564 77.1±0.4 55.9±0.9 85.6±0.2 81.2±0.6/87±0.4 91.4±0.1 91±0.1 87.8±0.1/71.5±0.7 94.2±0.4 81.8±1.3/82.3±1.2 15.8±1.2 1558MDENSE 2.399 76.5±0.5 55.3±2 87 ±0.1 79 ±1/85.3±0.8 91 .8±0.3 91.3±0.1 88.3±0.1/73.3±2 95 .9±0.3 78.5±2.4/79.2±2.5 13±1.3OURS 2.489 77.1±0.5 56.4±3 86.6±0.1 80±0.4/86.1±0.3 91.9±0.1 91.4±0.1 88.4±0.1/75±1.8 95.2±0.4 80.6±1.1/81.1±1.3 12.7±1.1 Table 7.SQuAD scores on GPT-2 models. PARAMS METHOD EM F1 124M DENSE 67.6 78.8 OURS 67.5 78 .5 350M DENSE 73.2 83.6 OURS 71.9 82 .4 774M DENSE 74.3 84.9 OURS 74.3 84 .6 Table 8.Experimental results for DeiT. SIZE METHOD ACC@1 A CC@5 DEIT-TINY ORIGINAL 2 72.2 91.1 DENSE 3 72.9 91.6 OURS 70.4 90 .1 DEIT-SMALL ORIGINAL 79.9 90.5 DENSE 79.9 94.5 BI-MASK 77.6 - OURS 79.2 94.8 DEIT-BASE ORIGINAL 81.8 95.6 DENSE 81.0 95.0 OURS 81.3 95 .4 Table 9.Experimental results for Transformer-base. METHOD AVG EPOCH LOSS TEST BLEU VAL BLEU VAL LOSS DENSE 4.558 26.15 26.56 3.982 HALF 4.659 26.12 26.36 4.041 STEP 4.692 25.27 25.85 4.082 OURS 4.649 26 .48 26 .78 3 .977 base for machine translation (Vaswani et al., 2023), and DeiT (Touvron et al., 2021b). For BERT, we use Cramming (Geiping & Goldstein, 2022) to pre-train a 16-layer BERT model with the sequence length of 512 on the C4 dataset (Raffel et al., 2019). For GPT-2, we use nanoGPT (Karpathy, 2023) to pre-train GPT-2 124M, 355M, 774M, and 1.5B on OpenWebText (Gokaslan & Cohen, 2019). Both BERT and GPT-2 models are estimated on GLUE (Wang et al., 2018). For DeiT (Touvron et al., 2021a), we pre-train DeiT-tiny on ImageNet-1K dataset (Deng et al., 2009). Besides, we use fairseq (Ott et al., 2019) to train Transformer-base on the WMT 14 En-De dataset (Bojar et al., 2014) and measure the BLEU (Papineni et al., 2002) score of the trained model. Of note, we use n to denote the length of sequences, d to denote the input and output dimensions of each trans- former block, dff to denote the inner dimensions of the FFNs in each transformer block, h to denote the number of heads, and N to denote the micro-batch size on each device. The pre-training and evaluation scripts are pub- licly available at https://github.com/thu-ml/ 2by4-pretrain-acc-examples . 6.1. Accuracy Results To investigate the effect of different 2:4 sparse training meth- ods, we pre-train a sparse BERT-base model on the C4 dataset using two sparse training methods: STEP (Lu et al., 2023) and Bi-Mask (Zhang et al., 2023). Besides, we also pre-train a dense BERT-base and a ‘Half’ BERT-base for comparison. Of note, ‘Half’ denotes a smaller yet still dense BERT-base model. To create Half model, we simply reduce the dff of each FFN layer in the original BERT-base by half while maintaining the original value of d. Theoretically, this adjustment halves the floating operations (FLOPs) of the original FFN layer as well. Except for the FFN layers, the shapes of the rest layers remain unaltered. All the pre-trained models are measured on GLUE bench- mark (WNLI excluded). Surprisingly, Table 5 shows that despite having identical FLOPs, the 2:4-sparse BERT-base trained with STEP and Bi-Mask shows inferior average scores compared to the Half model. The Half model attains 8Accelerating Transformer Pre-training with 2:4 Sparsity Table 10.Experimental results of masked decay, MVUE, and dense fine-tuning (FT) with BERT-Base. For decay term, we use both techniques in Sections 4.2 and 4.3. MASKED DECAY MVUE D ENSE FT L OSS AVG SCORE % % % 2.1553 77.6 ± 0.2 ! % % 2.1096 79.2 ± 0.2 ! ! % 2.1172 78.4 ± 0.3 ! % ! 2.0896 79.4 ± 0.2 ! ! ! 2.0968 79 .6 ±0.6 Table 11.Actual pre-train speed up on the whole network. PARAMETERS BATCH SIZE SPEEDUP 124M 16 1.18 350M 8 1.2 774M 4 1.21 Figure 7.Result of acceleration ratio S of different batch sizes and embedding Sizes. (a) shows the acceleration of a FFN layer. (b)-(d) shows the acceleration of a transformer block when n = 2048, 1024, 512. an average score of 77.9 on GLUE tests, while STEP and Bi-Mask only reach 77.7 due to the weaknesses in MRPC, QNLI, and STSB. By comparison, BERT-base trained in our proposed training method achieves 79.6 on GLUE, which significantly outperforms other sparse training methods and is comparable with the dense baseline, i.e., 79.8. Besides, we pre-train GPT-2 models with proposed meth- ods. Table 6 and 7 shows that our method for model sizes of 124M, 350M, 775M and 1558M achieves lossless scores compared with dense baselines. Similarly, DeiT and Transformer-base trained with our method also reach com- parable results to dense training; see Table 8 and 9. For GPT-2 and BERT, the training loss curves are sketched in Appendix C. Ablation Study We aim to investigate the effect of masked decay, MVUE and dense fine-tuning introduced in Section 4.2, 3.2, and 4.4. The 16-layer BERT-base is used for ablation study. Results in Table 10 show that: 1) The dense fine-tuning procedure helps to improve accuracy on GLUE by 2 points at most ; 2) MVUE leads to insignifi- cant, controllable accuracy loss; 3) By combining all these techniques together, 2:4 sparse training for transformers achieves comparable accuracy results as dense training. 6.2. Speedup Results The training acceleration techniques proposed in Section 5 are evaluated using GPT-2 models and RTX3090 GPUs. FP16 mixed precision training is used on all models. The practical speedups of a single FFN layer, a single trans- former block, and the entire network, compared to their re- spective dense counterparts, are reported. All the measured datum contain both forward and backward propagation. Feed-forward Network Layers For a single FFN layer, we fix n = 2048and change d. Results in Figure 7 show that a FFN layer can be accelerated up to 1.7x faster than its corresponding dense layer. Transformer Block We measure the acceleration ratio of a transformer block when n = 512, 1024, 2048. Results in Figure 7 show that in most cases, a transformer block can be accelerated to 1.3x faster via 2:4 sparsity. To illustrate this, a detailed profile result is given in Appendix D. End-to-end Acceleration Finally, we test the practical speedups of training GPT-2 models. Results in Table 11 show that our training method conducts up to 1.2x faster than the dense training on a single RTX3090. 7. Conclusions In this study, we are the first to propose accelerating the pre-training of transformers by 2:4 sparsity. We analyze the limitations of previous 2:4 training methods, including the impropriety in choosing positions and determining values of the masked decay factor, speed bottleneck incurred by computing transposable masks and gated activation func- tions. We propose a series of techniques to tackle them. Our training method is validated on DeiT, BERT, Transformer- base and GPT-2 models. In particular, we have attained 1.2x end-to-end training acceleration for the GPT-2 774M model without losing its accuracy. 9Accelerating Transformer Pre-training with 2:4 Sparsity Acknowledgements We would like to thank Ziteng Wang, Bingrui Li and Haocheng Xi for valuable discussions and help on the training large transformers. This work was supported by the National Key Research and Development Pro- gram of China (No. 2021ZD0110502), NSFC Projects (Nos. 62376131, 62061136001, 62106123, 62076147, U19A2081, 61972224), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z is also supported by the XPlorer Prize. Impact Statement Our proposed efficient algorithm can be used to accelerate pre-training large-scale transformers like GLM (Du et al., 2022), LLaMA (Touvron et al., 2023), etc. Recently, large transformers have exhibited remarkable efficacy in various fields such as natural language processing, computer vision, and speech recognition. However, the pre-training stage of large transformers is computationally intensive and time- consuming. For instance, pre-training a GPT-4 can span several months, even using a supercomputer equipped with thousands of GPUs. Thus, acceleration approaches are nec- essary. Our fully sparse training approach of transformers can potentially accelerate the FFN layers of a model by the- oretical 2x faster, without loss of accuracy. Thus, it can be potentially used to save energy and reduce carbon footprint. But this work can also be used to accelerate baleful software, like software that generates malicious contents, which may have a negative impact on human society. References Anthony, L. F. W., Kanding, B., and Selvan, R. Carbon- tracker: Tracking and predicting the carbon footprint of training deep learning models, 2020. Bengio, Y ., L´eonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint- Amand, H., Soricut, R., Specia, L., and Tamchyna, A. Findings of the 2014 workshop on statistical machine translation. In WMT@ACL, 2014. URL https://api. semanticscholar.org/CorpusID:15535376. BUSATO, F. and POOL, J. Exploiting nvidia ampere struc- tured sparsity with cusparselt [online]. 2020 [visited on 2021-10-10]. Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y ., Wang, Z., and Carbin, M. The lottery ticket hypothesis for pre- trained bert networks, 2020. Chen, X., Cheng, Y ., Wang, S., Gan, Z., Wang, Z., and Liu, J. Earlybert: Efficient bert training via early-bird lottery tickets, 2021. Chmiel, B., Hubara, I., Banner, R., and Soudry, D. Min- imum variance unbiased n:m sparsity for the neural gradients. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=vuD2xEtxZcj. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding, 2019. Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. Glm: General language model pretraining with autoregressive blank infilling, 2022. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners, 2021. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis, 2020. Geiping, J. and Goldstein, T. Cramming: Training a lan- guage model on a single gpu in one day, 2022. Gokaslan, A. and Cohen, V . Openwebtext cor- pus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural networks, 2015. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016. Han, S., Pool, J., Narang, S., Mao, H., Gong, E., Tang, S., Elsen, E., Vajda, P., Paluri, M., Tran, J., Catanzaro, B., and Dally, W. J. Dsd: Dense-sparse-dense training for deep neural networks, 2017. Hu, Z., Lan, Y ., Wang, L., Xu, W., Lim, E.-P., Lee, R. K.-W., Bing, L., and Poria, S. Llm-adapters: An adapter fam- ily for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. 10Accelerating Transformer Pre-training with 2:4 Sparsity Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., and Soudry, D. Accelerated sparse neural training: A provable and efficient method to find n:m transposable masks, 2021. Karpathy, A. nanogpt. https://github.com/ karpathy/nanoGPT/, 2023. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2017. Lasby, M., Golubeva, A., Evci, U., Nica, M., and Ioannou, Y . Dynamic sparse training with structured sparsity, 2023. Lee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train big, then compress: Rethinking model size for efficient training and inference of trans- formers. In International Conference on machine learn- ing, pp. 5958–5968. PMLR, 2020. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization, 2019. Lu, Y ., Agrawal, S., Subramanian, S., Rybakov, O., Sa, C. D., and Yazdanbakhsh, A. Step: Learning n:m struc- tured sparsity masks from scratch with precondition, 2023. McDanel, B., Dinh, H., and Magallanes, J. Accelerating dnn training with structured data gradient pruning, 2022. Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural networks, 2021. Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL- HLT 2019: Demonstrations, 2019. Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. Bleu: a method for automatic evaluation of machine translation. 10 2002. doi: 10.3115/1073083.1073135. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsu- pervised multitask learners. 2019. URL https: //api.semanticscholar.org/CorpusID: 160025533. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. Shazeer, N. Glu variants improve transformer, 2020. Thakkar, V ., Ramani, P., Cecka, C., Shivam, A., Lu, H., Yan, E., Kosaian, J., Hoemmen, M., Wu, H., Kerr, A., Nicely, M., Merrill, D., Blasig, D., Qiao, F., Majcher, P., Springer, P., Hohnerbach, M., Wang, J., and Gupta, M. CUTLASS, January 2023. URL https://github. com/NVIDIA/cutlass. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image trans- formers & amp; distillation through attention. In Interna- tional Conference on Machine Learning, volume 139, pp. 10347–10357, July 2021a. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J ´egou, H. Training data-efficient image trans- formers & distillation through attention, 2021b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and anal- ysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. URL https://api. semanticscholar.org/CorpusID:5034059. Xu, W., He, X., Cheng, K., Wang, P., and Cheng, J. Towards fully sparse training: Information restoration with spatial similarity. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 2929–2937, 2022. You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y . Drawing early-bird tickets: Towards more efficient training of deep networks, 2022. Zhang, Y ., Luo, Y ., Lin, M., Zhong, Y ., Xie, J., Chao, F., and Ji, R. Bi-directional masks for efficient n:m sparse training, 2023. Zhou, A., Ma, Y ., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H. Learning n:m fine-grained structured sparse neural networks from scratch, 2021. Zhou, D., Ye, M., Chen, C., Meng, T., Tan, M., Song, X., Le, Q., Liu, Q., and Schuurmans, D. Go wide, then narrow: Efficient training of deep thin networks. In In- ternational Conference on Machine Learning, pp. 11546– 11555. PMLR, 2020. 11Accelerating Transformer Pre-training with 2:4 Sparsity A. 2:4-spMM A.1. 2:4 Sparsity Examples of row-wise, column-wise and transposable 2:4 sparse matrix are shown in Figure 8. Note that transposable 2:4 sparsity aligns with both row-wise and column-wise 2:4 sparsity. Figure 8.Row-wise 2:4, column-wise and transposable 2:4 sparse matrix. A.2. Array Layout The array layout of different types of matrix multiplications are listed in Table 12, which explains why output activations and activation gradients are column-major matrices in FST. Table 12.Array layout of MN. Here S denotes that the matrix is in row-wise 2:4 sparsity, R denotes row-major dense matrix, and C denotes column-major dense matrix. M N S S ⊤ R C S % % R R S⊤ % % % % R % C R R C % C R R B. Workflow The main workflow of a single linear layer in FST process is depicted in Figure 9. Figure 9.2:4 sparse training iteration for a layer on a single batch. 12Accelerating Transformer Pre-training with 2:4 Sparsity C. Training Loss Curve For BERT-base and GPT-2, we depict training loss curve in Figure 10. Figure 10.Left: train loss of GPT-2; right: train loss of BERT. D. Profiling result To explain how we reach 1.3x block speedup, we profile our code and break down the time costs as shown in the table below; see Table 13. Table 13.Time costs of each part of our network and the dense model in one iteration per layer. m denotes the accumulation steps over micro batches. Our method is evaluated on GPT-2, with batch size 16, sequence length 1024, embedding dimension 1024 and heads number 16. DENSE (MS/EXEC ) SPARSE (MS/EXEC ) ACCELERATION RATIO S FREQUENCY (EXEC /ITER ) FFN LINEAR FWD GEMM 12173.8 7305.78 1.666324472 - BWD GEMM 23295 14080.82 1.654378083 - MVUE+ PRUNE 0 171.4 - - TOTAL 23295 14252.22 1.634482207 - TOTAL 35468.8 21558 1.645273216 - OTHERS 4 FWD 167 118.17 - - BWD 65.5 20.03 - - TOTAL 232.5 138.2 - - TOTAL FWD 12340.8 7423.95 1.662295678 - BWD 23360.5 14272.25 1.636777663 - TOTAL 35701.3 21696.2 1.645509352 - OTHERS FWD 6874.3 7090.55 - - BWD 13920.7 14117.45 - - TOTAL 20795 21208 - - TOTAL FWD 19215.1 14514.5 1.323855455 - BWD 37281.2 28389.7 1.313194574 - TOTAL 56496.3 42904.2 1.316801152 - MASKED DECAY 0 45.2 - 1 m PRUNE WEIGHTS 0 320.3 - 1 m TRANSPOSABLE MASK SEARCH 0 634.8 - 1 40m 4All functions in FFN except linear layers, i.e., activation function and dropout. 13",
      "meta_data": {
        "arxiv_id": "2404.01847v3",
        "authors": [
          "Yuezhou Hu",
          "Kang Zhao",
          "Weiyu Huang",
          "Jianfei Chen",
          "Jun Zhu"
        ],
        "published_date": "2024-04-02T11:12:42Z",
        "venue": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
        "pdf_url": "https://arxiv.org/pdf/2404.01847v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates accelerating feed-forward networks (FFNs) in transformer pre-training using 2:4 sparsity on NVIDIA Ampere GPUs. It identifies limitations of prior 2:4 sparse training methods for transformers (low accuracy and inefficiency). The main contributions include proposing three accuracy-preserving techniques: applying masked decay on gradients, a fast method to determine the decay factor in the warm-up stage using a 'flip rate' metric, and employing a dense fine-tuning procedure at the end of pre-training. Additionally, two training acceleration techniques are introduced: computing transposable 2:4 masks via convolution and accelerating gated activation functions (e.g., GEGLU) by reducing GPU L2 cache misses. The work is presented as the first to report end-to-end acceleration for pre-training transformers, achieving comparable or superior accuracy to dense training and up to 1.2x end-to-end speedup for models like GPT-2 774M.",
        "methodology": "The core methodology involves Fully Sparse Training (FST) where all General Matrix Multiplications (GEMMs) in FFNs are replaced with 2:4 sparse matrix multiplications (2:4-spMMs). To preserve accuracy, the authors introduce a 'flip rate' metric to monitor mask stability, propose a transformer-specific masked decay applied to gradients (instead of weights, which is more effective with Adam/AdamW optimizers), and a fast decay factor determination method based on comparing the sparse network's flip rate to its dense counterpart during the warm-up stage. A dense fine-tuning stage is adopted for the final 1/6 of training steps. For acceleration, transposable 2:4 mask computation is reformulated as a convolution operation to leverage GPU parallelism, significantly speeding up the mask search. Gated activation functions like GEGLU are optimized by implementing kernel-level changes to ensure column-major memory access, reducing L2 cache misses. Transposable masks are updated every 40 optimizer steps to reduce overhead. CUTLASS is used for 2:4-spMMs, and other kernels are implemented in Triton.",
        "experimental_setup": "The proposed methods are validated on several transformer models including BERT (16-layer), GPT-2 (124M, 355M, 774M, 1.5B), Transformer-base for machine translation, and DeiT (tiny, small, base). Datasets used include C4 for BERT pre-training, OpenWebText for GPT-2 pre-training, ImageNet-1K for DeiT, and WMT 14 En-De for Transformer-base. Evaluation is performed on GLUE benchmark (for BERT and GPT-2), SQuAD (for GPT-2), ImageNet-1K (ACC@1, ACC@5 for DeiT), and BLEU score (for Transformer-base). Comparison is made against dense training, a 'Half' dense model (reduced FFN dimensionality), STEP, and Bi-Mask methods. An ablation study on BERT-base investigates the individual effects of masked decay, MVUE, and dense fine-tuning. Experiments are conducted using FP16 mixed precision training on RTX3090 GPUs. Speedups are reported for single FFN layers, transformer blocks, and end-to-end network training.",
        "limitations": "The paper acknowledges a potential negative societal impact, as accelerating AI training could also be used to speed up 'baleful software' that generates malicious content. While a 1.2x end-to-end acceleration is achieved, it does not fully realize the theoretical 2x speedup of 2:4-spMM on FFNs, indicating other bottlenecks beyond those addressed. The determination of the optimal masked decay factor (λW) still relies on a heuristic range for the flip rate ratio (µ ∈ [0.60, 0.95]), implying some empirical tuning might still be necessary. The research primarily focuses on NVIDIA Ampere GPUs, so the direct applicability and performance on other hardware architectures are not detailed.",
        "future_research_directions": "The authors suggest that the proposed efficient algorithm can be directly used to accelerate pre-training large-scale transformers such as GLM and LLaMA, implying further application and evaluation on contemporary large language models. Potential future work could involve exploring additional optimizations to bridge the gap between the observed 1.2x end-to-end speedup and the theoretical 2x speedup of 2:4-spMMs. Investigating the generalizability of these techniques to other structured sparsity patterns or different hardware architectures could also be a direction. Furthermore, research into more robust or automated methods for hyperparameter tuning, like the decay factor λW, could be beneficial. Addressing the ethical implications and potential for misuse of accelerated AI systems is also an important area for continued consideration."
      }
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection",
      "abstract": "While the self-attention mechanism has been widely used in a wide variety of\ntasks, it has the unfortunate property of a quadratic cost with respect to the\ninput length, which makes it difficult to deal with long inputs. In this paper,\nwe present a method for accelerating and structuring self-attentions: Sparse\nAdaptive Connection (SAC). In SAC, we regard the input sequence as a graph and\nattention operations are performed between linked nodes. In contrast with\nprevious self-attention models with pre-defined structures (edges), the model\nlearns to construct attention edges to improve task-specific performances. In\nthis way, the model is able to select the most salient nodes and reduce the\nquadratic complexity regardless of the sequence length. Based on SAC, we show\nthat previous variants of self-attention models are its special cases. Through\nextensive experiments on neural machine translation, language modeling, graph\nrepresentation learning and image classification, we demonstrate SAC is\ncompetitive with state-of-the-art models while significantly reducing memory\ncost.",
      "full_text": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection Xiaoya Li*1, Yuxian Meng*1, Mingxin Zhou1, Qinghong Han1, Fei Wu2 and Jiwei Li 1 1 Shannon.AI 2 Computer Science Department, Zhejiang University {xiaoya_li,yuxian_meng,mingxin_zhou,qinghong_han,jiwei_li}@shannonai.com Abstract While the self-attention mechanism has been widely used in a wide variety of tasks, it has the unfortunate property of a quadratic cost with respect to the input length, which makes it difﬁcult to deal with long inputs. In this paper, we present a method for accelerating and structuring self-attentions: Sparse Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and attention operations are performed between linked nodes. In contrast with previous self-attention models with pre-deﬁned structures (edges), the model learns to construct attention edges to improve task-speciﬁc performances. In this way, the model is able to select the most salient nodes and reduce the quadratic complexity regardless of the sequence length. Based on SAC, we show that previous variants of self-attention models are its special cases. Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classiﬁcation, we demonstrate SAC is competitive with state-of-the-art models while signiﬁcantly reducing memory cost. 1 Introduction The self-attention mechanism has proved to beneﬁt a wide range of ﬁelds and tasks, such as natural language processing (Vaswani et al., 2017; Dai et al., 2019), computer vision (Xu et al., 2015; Jaderberg et al., 2015; Bello et al., 2019; Parmar et al., 2019), video classiﬁcation (Wang et al., 2018) and graph modeling (Veliˇckovi´c et al., 2018; Lee et al., 2019). The attention mechanism allows the model to attend to different parts of the input and capture the most salient features, and provides with the ability to handle dependencies between elements across long distances. Two conspicuous problems stand out with the self-attention mechanism: (1) the memory complexity is quadratic with respect to the input length, making it infeasible to handle long sequences; and (2) the self-attention operations are performed in a pre-deﬁned structure (usually fully-connected), which not only is costly but also lacks for the ability to learn the optimal attention structure optimized for the performance in the downstream tasks (Child et al., 2019; Sukhbaatar et al., 2019). These observations indicate that the fully-connected structure for self-attention operations can be replaced by a more sparse one where only speciﬁc edges are constructed for attention operations. In this paper, we present Sparse Adaptive Connection (SAC), which replaces the fully-connected structure in self-attention with a sparse graph-like structure adapted to different tasks. In SAC, we regard the input as a graph where a node could be a token in the sequence or an ﬂattened feature map of an image, and an edge between a pair of nodes represents they can attend to each other. To select edges, we propose an Edge Predictor which utilizes an LSTM model to dynamically predict pairs of nodes that represent two ends of an edge. In contrast with previous self-attention models with pre-deﬁned attention structures (edges), SAC learns to construct attention edges adaptively, which are optimized to improve task-speciﬁc performances using reinforcement learning models. We 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2003.09833v3  [cs.CL]  29 Sep 2020evaluate the proposed model on four tasks: neural machine translation, language modeling, graph representation learning and image classiﬁcation, and demonstrate that SAC learns to select the most salient nodes to attend to each other and is free from heavy memory cost while achieving strong performances. 2 Related Work Many recent researches have focused on modifying the structure of self-attention to relieve the computation and memory burden. Transformer-XL (Dai et al., 2019) enables learning long-term dependency by introducing a segment-level recurrence mechanism and a novel relative position encoding scheme (Shaw et al., 2018). (Guo et al., 2019) proposed Star Transformer, a variant of Transformer which replaces the fully-connected structure in self-attention with a star-shaped topology, where all tokens are connected with a relay node. (Child et al., 2019; Kitaev et al., 2020) suggest sparsifying Transformer by focusing only on a fraction of attention connections. (Child et al., 2019) introduced sparse factorizations of the attention matrix, which scale as O(np√n) with the sequence length, and a set of sparse attention kernels which efﬁciently compute subsets of the attention matrix. (Kitaev et al., 2020) proposed Reformer, a more efﬁcient yet complicated method for computing self-attention, where the dot-product attention is replaced by one that uses a locality-sensitive hashing, reducing the complexity from O(n2) to O(nlog n). (Ye et al., 2019) partitioned the sequence to different multi-scale spans and attended the context information from ﬁne-grain to coarse-grain as the relative distance increases. All above works rely on manually designed structures. Our work is inspired by (Sukhbaatar et al., 2019), which takes a step forward and uses a novel adaptive self- attention mechanism that can learn its optimal attention span for each head, and (Correia et al., 2019) which proposed adaptively sparse Transformer. Different from Yang et al. (2018), we use an LSTM to predict attention links which gives us ﬁner control of how sparse we want self-attention to be. Graph neural networks (GNNs) are known at learning local contextual information by encoding attribute features (Kipf and Welling, 2016; Hamilton et al., 2017b), but they are not able to explic- itly distinguish the most salient nodes from all its neighbors, neither can they directly attend to the nodes that are beyond one-hop away. Much work has investigated the effect of attention on GNNs (Veliˇckovi´c et al., 2018; Abu-El-Haija et al., 2018; Lee et al., 2018; Veliˇckovi´c et al., 2019). (Veliˇckovi´c et al., 2018) extended self-attention to graphs by enabling each node to attend over its neighbors, achieving state-of-the-art results for semi-supervised node classiﬁcation tasks. But they simply applied self-attention over graphs to all neighbors of a node, which might be a problem when dealing with large and noisy graphs where only few neighbors need to be aggregated. (Ye and Ji, 2019) proposed Sparse Graph Attention Network which uses a binary gate to control whether each edge should be engaged. However, these works lack ability to aggregate long-range dependencies in graphs, and they only consider neighbors that are one hop away. Various methods have been proposed to tackle this issue (Ye et al., 2020; Zhang et al., 2020; Pei et al., 2020). Similar to graphs, (Bello et al., 2019) introduced a novel two-dimensional relative self-attention mechanism for images and augmented convolutional operators with this self-attention method, showing systematic improvements on both image classiﬁcation and object detection tasks across a wide range of architectures. 3 Background: Self-Attention Given a set of nodes1 {e1,··· ,eN}as inputs, self-attention iteratively computes the representation of ei in the l-th layer by attending to all its neighbors N(ei), which is deﬁned as follows: ˜hl i = ∑ ej∈N(ei) αijvl−1 j , αij = softmax   ( ql−1 i )T kl−1 j√ d   and ql−1 i = WQhl−1 i , kl−1 j = WKhl−1 j , vl−1 j = WVhl−1 j (1) where dis the hidden dimension, WQ,WK,WV are learnable parameters and q,k,v correspond to queries, keys and values, respectively. The multi-head mechanism linearly projects the queries, keys 1We use the term “node’ in a broad sense of denoting any particular unit in text, images or graphs. 2and values multiple times with different learned linear projections, and then performs self-attention in parallel, after which the results are concatenated and again projected: hl i = Concat(˜hl,1 i ,··· ,˜hl,m i )WO (2) where the superscript 1,···,m denotes the head number, and WO is learnable parameters. After L iterations, we obtain the ﬁnal representation for each node hL i . 4 Sparse Adaptive Connection for Self-Attention The key point in SAC is to use to an LSTM edge predictor to predict edges for self-attention operations between nodes, where a node could be a token in the sequence or an ﬂattened feature map of an image. Self-attention operations are performed between linked nodes instead of in a fully-connected manner. The LSTM edge predictor is optimized to improve task-speciﬁc performances using reinforcement learning models. 4.1 LSTM Edge Predictor In SAC, an edge predictor is used to construct edges between nodes for self-attention operations. Suppose that we are given a set of nodes {e1,··· ,eN}with no edge between any pair of nodes when initialization, our aim is to generate edges using this edge predictor, with the total number αN for each layer, where αis a hyperparameter deciding how many edges should be constructed for each node on average. The Edge Predictor uses an LSTM model as a backbone and sequentially predicts edges. The prediction of an edge is decoupled into the prediction of the original node and the destination node pair. More formally, the input to Edge Predictor is a special token “[SOS]”, and the model proceeds to predict the original node and destination node of all edges (2αN nodes in total) for the ﬁrst layer, denoted by {y1 1,y1 2,··· ,y1 2αN}, where the superscript denoted the index of the layer and the subscript denoted the index of the predicted node. At each time step, the input to the LSTM model is the representation hyt for the node that has just been predicted. Then it is combined with the previously constructed representation gt to obtain gt+1 representing the current time-step using LSTMs, and gt+1 is used to predict the following node using the softmax function. The projection matrix before softmax W shares embeddings with node representations, where each column wi is the vector representation for node ei. The probability of predicting node yt+1 given gt+1 is thus given by: p(yt+1 = ei) = exp (gT t+1 ·wi)∑ jexp (gT t+1 ·wj) (3) This process is repeated 2αN times. After the end of αN edge predictions, we update the representa- tion for each node based on self-attention as will be detailed in Section 4.1 for different tasks, and proceed to the next layer. For node predictions in the following layer, the initial input now becomes hidden state for the last time-step of the previous layer. The entire process is repeated Ltimes, where Ldenotes the number of self-attention layers and the resulted nodes in layerlare {yl 1,yl 2,··· ,yl 2αN}. Compared to separately predicting edges for each node, this approach is more ﬂexible and gives us ﬁner control of the total number of edges we would like to construct. More importantly, this process is aware of previous constructed edges, both in the current layer and previous layers. The recurrent edge predictor is shown in Figure 1(b). We implement it using a single-layer LSTM model. Once having constructed all edges for each layer, we can immediately obtain the set of neighbors N(en i) for each node ei in the n-th layer. Self-attention operations with multi-head mechanism are then performed on its neighbors for each node. For text tasks, we regard each token as a node. For graph-like structures, we treat nodes in the original graph as nodes. For images, the input (H,W,F in) dimensional sensor is reshaped to a HW ×Fin matrix, where each row can be thought as a node by our deﬁnition. 4.2 Distance Encoding The input graph intrinsically displays some degree of structures. For example, in a sequence of natural language tokens, the relative distance between two tokens in the sequence or the corresponding parse 3𝑒𝑖 𝑒1 𝑒2 𝑒3 𝑒4 i really like cats Edge  Predictor 𝑒𝑖 𝑒1 𝑒2 𝑒3 𝑒4 Self- Attention Self- Attention (a) (b) e1 e3 e3 e2 e2 e4 Distance Encodings 𝐠6 ×( )+ Node Encodings 𝐰1,𝐰2,𝐰3,𝐰4 𝐯2,𝐯0,𝐯1,𝐯-1 layer 𝑛 Figure 1: An illustration of the proposed Sparse Apdative Connection. (a) shows the process of SAC to construct edges and then perform self-attention on these edges (Red is for text and green is for graphs). (b) shows the edge prediction process of (a) with distance encodings. When predicting time-step 6, the word embeddings are added with distance encodings. tree encodes structural information. As another example, in the task of node representation learning in graphs (Yang et al., 2016b; Veliˇckovi´c et al., 2017; Xu et al., 2018), the graph originally comes with the node edges. The LSTM edge predictor described above ignores this structure. To leverage original structural information, we propose distance encodings to incorporate graph structure into the edge predictor. Distance encodings only affect the destination node predictions. In contrast to only using node embedding matrix W, we add an extra distance matrix V that encodes distance information to the original projection matrix W, giving V + W. Each column in V is its corresponding distance representation to the current original node. For example in Figure 1, at time-step 6, when two edges (e1,e3),(e3,e2), and one origin node e2 have been generated, we are to use g5 ∈Rd, the output of the LSTM model at time-step 5, to predict the node at time-step 6. According to the original structure, the distance between e2 (the current origin node) and all the nodes e1,e2,e3,e4 by far are 2, 0, 1, and -1 respectively, where -1 means inability to reach. The distance vectors are thus v2,v0,v1,v−1, which are vectors of size Rd to be learned. Intuitively, this process also discourages generating duplicate edges and leverages the original structural information. In contrast to Veliˇckovi´c et al. (2017) where attention operations are only performed between nodes with literal edges in the original graph, SAC offers the ﬂexibility in leveraging the original graph structure and inﬂuence from the training signals. Additionally, SAC allows for more convenient information exchange between similar nodes that are far away in terms of distance in the original graph structure, which is because the connection construction stage has the ability to connect any pair nodes in the graph. This ability potentially leads to better performances. 4.3 Training and Test Directly training the edge predictor is impractical since we have no access to the ground-truth edges. We use REINFORCE, which is an instance of a broader class of policy gradient methods for optimization. The main idea is to use reinforcement learning to discover the best edge connections for self-attention operations. Each action ais the node predicted by edge predictor. Let Θ denote parameters of the edge predictor and Φ denote the parameters of the main network which maps an input to its ﬁnal label based on a pre-deﬁned self-attention structure. Under the framework of reinforcement learning, we ask the edge predictor to maximize its reward R(Θ), which is the log probability of predicting the correct label, e.g., for neural machine translation the reward Ris the average log probability of golden target tokens; for image classiﬁcation, the reward the log probability of the correct label. Consider the simple case where different attention layers use the same node connections, by sampling a sequence of nodes from the edge predictor, we are able to update the parameters in edge predictor using policy gradients: ∇J(Θ) = 2αN∑ i=1 ∇log p(ai|a1:i−1; Θ)(R(Θ) −b) (4) 4layer 𝑛 layer 𝑛 −1 Vanilla Self-attention Transformer-XL Seg-Length=2 BT-Transformer layer 𝑛 +1 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 Adaptive Span S=2 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 Figure 2: Connection of SAC to other methods for computing self-attention. where bdenotes the baseline which is the average of the previous rewards. Φ is updated directly based on the log-likelihood. At test time, edges are decoded using beam search. We use a beam size of 5 for all models. 4.4 Variants of Edge Predictor The vanilla version of the Edge Predictor can be further regulated, simpliﬁed or expanded with prior knowledge for preferable graph structures. All layers sharing the same structure To reduce the computational cost and RL search space, we can enforce the edge structure to be the same for all layers, where the process is only executed once instead of Ltimes. We adopt this strategy for all settings to reduce the search space. All nodes connected in each layer To enforce each node to be connected in each layer, for each node ei, it is repeatedly fed to the predictor αtimes as the original node, and we only predict the destination node. The graph can be either directed graph or undirected graph, depending on how we want self-attention to be computed. Different heads attending to different contexts (head adaptive for short) Sukhbaatar et al. (2019) shows that it is beneﬁcial if different heads attend to different spans (some focusing on the recent history, while others focusing the whole available context). We can also augment the model by assigning each head with a edge predictor, providing the ﬂexibility that different heads can attend to different chunks of context. We sequentially predict all input and output nodes for each head, and the prediction of 2αN nodes are repeated Htimes. In this way, the prediction model for the current head is aware of the information of all previous heads. A head speciﬁc embedding is appended to the node embedding in LSTMs to let the model be aware of the current head. Since this strategy signiﬁcantly increases the search space in RL, we empirically ﬁnd that it helps some settings, but not always. 4.5 Connection to Existing Methods In this subsection, we describe the connection between SAC and previous variants of self-attentions, and show that these variants computing self-attention can be obtained through SAC if we slightly modify the edge predictor. For ease of exposition, we use EP(e) ={(ei,ej)}to denote the collection of all edges for self-attention operations. Connection to vanilla self-attention (Vaswani et al., 2017) The vanilla self-attention links each pair of nodes, where EP(e) can be described as EP(e) ={(ei,ej)|i∈[1,N]}. Connection to Transformer-XL (Dai et al., 2019) Transformer-XL treats the text in a segment- by-segment style. Self-attention operations are performed between nodes within the same segment. EP(e) can be described as EP(e) ={(ei,ej)|i∈[1,N]; j ∈Segment(i)}. Connection to Adaptive Span Transformer (Sukhbaatar et al., 2019) Adaptive Span Trans- former learns an optimal attention span for each head. Suppose the span size assigned to head tis s, then EP(e,t) can be described by: EP(e)={(ei,ej)|i∈[1,N]; j = i,i −1,··· ,i −s+ 1;span = t}. Connection to BP-Transformer (Ye et al., 2019) BP-Transformer constructed a tree-like graph by adding span nodes apart from token nodes. There are 2N −1 nodes in total, where N is the sequence length. In BP-Transformer, each token (leaf) node attends to each span (non-leaf) node that includes it, which we refer to as Ancestor(ei) for node ei. It is easy to prove that a leaf node is 5Model H B edges dev test test (heads) (blocks) (BLEU) (BLEU) (cased sacreBLEU) Transformer Base (Vaswani et al., 2017) 8 6 N2 25.8 27.3 BP base (Ye et al., 2019) 28.1 27.6 Reversible base (Kitaev et al., 2020) 28.0 27.4 SAC base 8 6 2 N 17.4 18.3 17.8 SAC base 8 6 5 N 25.6 27.0 26.2 SAC base 8 6 10 N 26.0 27.7 27.0 SAC base 8 6 15 N 25.6 27.4 26.8 SAC base 16 6 10 N 26.2 28.1 27.6 SAC base 16 12 10 N 26.4 28.4 27.8 Transformer big (Vaswani et al., 2017) 16 6 N2 26.4 28.4 Reversible big (Kitaev et al., 2020) 29.1 28.4 SAC Large 16 6 10 N 26.7 28.9 28.1 SAC Large 16 18 10 N 26.9 29.4 28.6 SAC Large (dependency) 16 18 10 N 26.9 29.5 28.8 Table 1: BLEU scores on the newstest2013 for development and newstest2014 for test for WMT English-German. N denotes the length of the input sequence. associated with ⌊log2 N⌋non-leaf nodes (and thus attends to ⌊log2 N⌋nodes). Therefore, we have EP(e)={(ei,ej)|i∈[1,N]; j ∈Ancestor(ei)}. 5 Experiments 5.1 Machine Translation We use the encoder-decoder model (Bahdanau et al., 2014; Vaswani et al., 2017) as the backbone for machine translation. For the encoder, SAC constructs αN edges for each layer and self-attention operations are performed between connected nodes. For the decoder, masked attention (Vaswani et al., 2017) is applied. Speciﬁcally, given a newly generated target node, it can attend to all source nodes, dummy nodes, target nodes that come beforehand, but not target nodes that come afterwards. We again use SAC to construct edges between the newly generated node and the preceding nodes, where the input node to the edge predictor is forced to be the newly generated node, and the output node is limited to preceding nodes and the dummy nodes. Following Vaswani et al. (2017); Ott et al. (2018); Kitaev et al. (2020), we used the standard WMT 2014 English-German dataset to test the proposed model. The dataset consists of about 4.5 million sentence pairs. Sentences are encoded using BPE (Sennrich et al., 2016), which has a shared source target vocabulary of about 37000 tokens. For fair comparison, we used the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 and ϵ= 10−9 for all models. Label smoothing (Szegedy et al., 2016) with ϵ= 0.1 is applied for all models. For the base setup, following Vaswani et al. (2017), the dimensionality of inputs and outputs dmodel is set to 512, and the inner-layer has dimensionality dff is set to 2,048. For big models, dmodel is set to 1,024 and dff is set to 4,096. Models are run on 8 NVIDIA V100 GPUs. Results are shown in Table 1. As we gradually increase the number of edges for each layer (from 2 to 5 to 10 to 15 per node), we can see that the performance ﬁrst increases, reaching the highest with αset to 10, and then decreases. This means that performing attention operations between all pairs is not only unnecessary, but can hurt the performance. Memory saved from sparse connections allow for more heads to perform attentions and deeper networks with more blocks, leading to better performances over vanilla transformers. We also implement a dependency-based model, in which English sources were ﬁrst parsed using Stanford Dependency parser (Chen and Manning, 2014). Relative positions between nodes in the dependency trees are encoded in distance encodings of the edge predictor. The introduction of dependency parser for attention construction introduces +0.14 BLEU score boost. We did not observe signiﬁcant performance boost from the head-adaptive strategy, and thus omit their performances. 6Method Enwiki8 Text8 Params Trans (Al-Rfou et al., 2019) 1.11 1.18 44M Trans-XL (Dai et al., 2019) 1.06 - 41M Adaptive(Sukhbaatar et al., 2019) 1.02 1.11 39M BPT (Ye et al., 2019) 1.02 1.11 38M SAC (basic) 1.02 1.07 39M SAC (head adaptive) 1.00 1.06 39M Table 2: Performances on language modeling datasets. 5.2 Language Modeling We use character-level language modeling datasets to evaluate SAC’s ability to handle long-term dependencies. We use Enwiki8 (Mahoney, 2011) and Text8 (Mahoney, 2011) for evaluation and report the values of BPC for different models. We use the Transformer decoder architecture as the backbone. We compare SAC with other variations of transformers to ﬁt long sequences into the model, including the vanilla Transformer (Al-Rfou et al., 2019), which splits the whole sequence into smaller segments, and only trains the model within each segment and ignore the rest; Transformer-XL (Dai et al., 2019) that adopts a recurrence mechanism to cache the memory of previous segments; adaptive span model (Sukhbaatar et al., 2019) that assigns different heads with different text spans in an adaptive fashion; and the BP-Transformer (Ye et al., 2019) that splits the sequence using binary trees. For SAC, αis set to 256 for each node. The relatively small memory cost allows the model to look at a maximum context of 50k characters. Input dimensionality is set to 512, and the inner-layer dimensionality 2,048. Following (Sukhbaatar et al., 2019), we use Adagrad for optimization, with a batch size of 64 and ﬁxed learning rate of 0.07 and 32k warm-up steps. Results are shown in Table2. As can be seen, SAC-basic outperforms the other Transformers by 0.04 bcp on Text8 while signiﬁcantly reducing the memory usage for large attention spans. For Enwiki8, it ties with the best BPT model, achieving 1.02 bcp score. The improvement validates the importance modeling long-term dependencies with limited available memory. We also ﬁnd that, in the language modeling tasks, the head-adaptive strategy helps, 5.3 Representation Learning in Graphs We test the performance of the proposed model on both transductive and inductive benchmark datasets. For the transductive setup, we used the three standard citation network benchmarks, Cora, Citeseer and Pubmed (Sen et al., 2008). In the transductive setup, we used the Protein-protein interaction dataset (PPI) (Zitnik and Leskovec, 2017). The training algorithm has access to all of the nodes’ feature vectors and labels, and predictions are performed on the test nodes. The detailed descriptions for Cora, Citeseer, Pubmed and PPI are found in the Appendix due to the space limit. The difference between SAC and (Veliˇckovi´c et al., 2017) is that the latter performs self-attention operations between nodes that are connected though graph edges, while SAC perform self-attention operations between nodes linked by the edge predictor. For fast convergence, we initialize SAC using the pretrained attention model (Veliˇckovi´c et al., 2017), where attention links are just edges in the original graph. Then we start exploring edge construction across all nodes. the number of attention heads is ﬁxed to 8 and the number of blocks is set to 12. We experiment different values of α, i.e, [5, 10, 50, 100] unless the memory usage reaches limitation. We train all models with Adam (Kingma and Ba, 2014) and early stopping on the validation set. The initial learning rate is treated as a hyper-parameter trained on the validation set. Following (Veliˇckovi´c et al., 2017), we run 100 epochs in total and use an early stopping strategy on the both the cross-entropy loss and accuracy for transductive tasks and micro-F1 for inductive tasks. Each experiment is repeated three times and we report the mean value. Results are shown in Table 3. We note that SAC achieves signiﬁcant performance boosts over existing methods across all four datasets, i.e., outperforms our implemented GAT +1.8, +1.1, +0.7 and +1.1 respectively on Cora, Citeseer, Pubmed and PPI. The explanation for SAC’s advantage is as follows: graph node representation learning concerns about both label propagation and relatedness between nearby nodes in the vector space, the latter of which is what GCN handles. As veriﬁed in many 7Available data Method Cora Citeseer Pubmed PPI A DeepWalk (Perozzi et al., 2014) 67.2 43.2 65.3 – X,A DGI (Veliˇckovi´c et al., 2019) 82.3 71.8 76.8 63.8 X,A GraphSAGE (Hamilton et al., 2017a) – – – 50.2 X,A,Y SemiEmb (Weston et al., 2012) 59.0 59.6 71.7 – X,A,Y Planetoid (Yang et al., 2016a) 75.7 64.7 77.2 – X,A,Y Chebyshev (Defferrard et al., 2016) 81.2 69.8 74.4 – X,A,Y GCN (Kipf and Welling, 2016) 81.5 70.3 70.0 – X,A,Y MoNet (Monti et al., 2017) 81.7 – 78.8 – X,A,Y SGC (Wu et al., 2019) 81.0 71.9 78.9 – X,A,Y AdaLNet (Liao et al., 2019) 80.4 68.7 78.1 – X,A,Y SGAT (Ye and Ji, 2019) 84.2 68.2 77.6 96.6 X,A,Y CurvGN-n (Ye et al., 2020) 82.7 72.1 79.2 – X,A,Y GAT (Veliˇckovi´c et al., 2017) 83.0 72.5 79.0 97.3 X,A,Y SAC 84.8 73.8 79.7 98.4 X,A,Y SAC (head adaptive) 84.7 74.0 80.1 98.4 Table 3: Summary of results in terms of classiﬁcation accuracies on transductive tasks (Cora, Citeseer and Pubmed) or micro-averaged F1 score on inductive tasks (PPI). In the ﬁrst column, we report the kind of data available to each method during training (X: features, A adjacency matrix, Y: labels). CIFAR100 ImageNet GFlops top1 top5 Params GFlops top1 top5 Params WideResNet 10.4 80.3 95.0 36.3M ResNet50 8.2 76.4 93.1 25.6M Bello et al. (2019) 10.9 81.6 95.2 36.2M 8.3 77.7 93.8 25.8M SAC 11.0 82.2 95.4 36.2M 8.3 78.5 94.2 25.9M SAC (head adaptive) 11.0 82.4 95.5 36.2M 8.3 78.7 94.3 25.9M Table 4: Results of image classiﬁcation on CIFAR-100 using the Wide-ResNet 28-10 Zagoruyko and Komodakis (2016) as the backbone and on ImageNet using the ResNet-50 He et al. (2016) model. recent works Liu et al. (2018); Wang and Leskovec (2020), combining both facets leads to better performances. The attention edge prediction stage in SAC fosters information exchange between nodes that are not directly linked in graph but similar in terms of label propagation. SAC actually offers the probability in bridging the aspects, leading to better performances. 5.4 Image Classiﬁcation Augmenting convolution models with self-attention (Bello et al., 2019; Parmar et al., 2019; Hu et al., 2019; Wang et al., 2019) provides the model with the ability to capture global contexts in an image and has yielded gains in several vision tasks such as image classiﬁcation and objective detection. We follow the protocols in (Bello et al., 2019), i.e. incorporating relative position embeddings for self-attention operations and augmenting each ResNet (Zagoruyko and Komodakis, 2016; He et al., 2016) block with self-attentions. To handle the prohibitive memory cost, (Bello et al., 2019) performs self-attention operations starting from the last layer, which has the smallest spatial dimension, until memory constraints are hit. This ad-hoc strategy is replaced by SAC. Following (Bello et al., 2019), we conduct experiments on CIFAR-100 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009). For CIFAR-100, we use the Wide-ResNet-28-10, the architecture of which comprises 3 stages of 4 residual blocks each using two 3×3 convolutions. We augment each convolution of all residual blocks with the number of attention heads set to 16. For ImageNet, we use ResNet-50, the block of which consists of 1×1, 3×3, 1×1 convolutions where the last pointwise convolution expands the number of ﬁlters and the ﬁrst one contracts the number of ﬁlters. We tune αin range {5,10,20}. Results are shown in Table 4. As can be seen, the proposed SAC model signiﬁcantly outperforms the attention model in (Bello et al., 2019) with the only modiﬁcation of automatic edge construction. Speciﬁcally, the top-1 score increases from 81.6 to 82.4 for CIFAR-100 and from 77.7 to 78.7 for ImageNet. The improvement validates the importance of performing necessary attention operations under memory limit. 86 Conclusion In this work, we propose Sparse Adaptive Connection — a sparse connection method to accelerate and structure the self-attention mechanism that adapts to various downstream tasks. We use an LSTM edge predictor to construct edges for self-attention operations, which gives us control of how sparse we want self-attention to be by setting the sparse coefﬁcient α. We demonstrate that SAC is competitive with state-of-the-art models on neural machine translation, language modeling, graph classiﬁcation and image classiﬁcation, while reducing memory costs. Broader Impact Accelerating fully-connected self-attention has been a research trend in recent years. Vanilla self- attention models, such as Transformers and BERT, are not able to process extremely long text, where text must be in advance segmented into pieces and then can be individually modelled. The lack of adequate context leads to poor performances in generating long, coherent and ﬂuent text. The goal of our proposed method, SAC, is to provide a way of relieving the computation burden of vanilla self-attention by automatically searching for the best attention patterns. We believe SAC has great potentials to generate high-quality long text. While there is risk of abuse, like generating fake news, the value of SAC is generally safe and weighs more than abuse to the whole society. Acknowledgement We thank all reviewers for their insightful comments. We also want to thank Zihao Ye for his helpful suggestions on evaluations, along with suggestions on learning head-speciﬁc policies. References Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. 2018. Watch your step: Learning node embeddings via graph attention. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9180–9190. Curran Associates, Inc. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3159–3166. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V . Le. 2019. Attention augmented convolutional networks. In The IEEE International Conference on Computer Vision (ICCV). Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar. Association for Computational Linguistics. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins. 2019. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174–2184, Hong Kong, China. Association for Computational Linguistics. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2978–2988, Florence, Italy. Association for Computational Linguistics. 9Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In D. D. Lee, M. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3844–3852. Curran Associates, Inc. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee. Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-transformer. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1315–1325, Minneapolis, Minnesota. Association for Computational Linguistics. Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017a. Inductive representation learning on large graphs. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1024–1034. Curran Associates, Inc. William L. Hamilton, Rex Ying, and Jure Leskovec. 2017b. Representation learning on graphs: Methods and applications. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778. Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. 2019. Local relation networks for image recognition. In Proceedings of the IEEE International Conference on Computer Vision , pages 3464–3473. Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. 2015. Spatial trans- former networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2017–2025. Curran Associates, Inc. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Thomas N. Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efﬁcient transformer. In International Conference on Learning Representations. Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. John Boaz Lee, Ryan Rossi, and Xiangnan Kong. 2018. Graph classiﬁcation using structural attention. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, page 1666–1674, New York, NY , USA. Association for Computing Machinery. Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention graph pooling. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3734–3743, Long Beach, California, USA. PMLR. Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. 2019. Lanczosnet: Multi-scale deep graph convolutional networks. In International Conference on Learning Representations. Arthur Liberzon, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. Molecular signatures database (msigdb) 3.0. Bioinformatics, 27(12):1739– 1740. Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. 2018. Learning to propagate labels: Transductive propagation network for few-shot learning. 10Matt Mahoney. 2011. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text. html. F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. 2017. Geometric deep learning on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5425–5434, Los Alamitos, CA, USA. IEEE Computer Society. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 1–9, Brussels, Belgium. Association for Computational Linguistics. Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. 2019. Stand-alone self-attention in vision models. In Advances in Neural Information Processing Systems 32, pages 68–80. Curran Associates, Inc. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2020. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14, page 701–710, New York, NY , USA. Association for Computing Machinery. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classiﬁcation in network data. AI magazine, 29(3):93–93. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for Computational Linguistics. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335, Florence, Italy. Association for Computational Linguistics. C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. 2016. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, Los Alamitos, CA, USA. IEEE Computer Society. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In International Conference on Learning Representations. Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. 2019. Deep graph infomax. In International Conference on Learning Representations. Hongwei Wang and Jure Leskovec. 2020. Unifying graph convolutional neural networks and label propagation. Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. 2019. Eca-net: Efﬁcient channel attention for deep convolutional neural networks. arXiv preprint arXiv:1910.03151. 11Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jason Weston, Frédéric Ratle, Hossein Mobahi, and Ronan Collobert. 2012. Deep Learning via Semi-supervised Embedding, pages 639–655. Springer Berlin Heidelberg, Berlin, Heidelberg. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6861–6871, Long Beach, California, USA. PMLR. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research, pages 2048–2057, Lille, France. PMLR. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536. Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016a. Revisiting semi-supervised learning with graph embeddings. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 40–48, New York, New York, USA. PMLR. Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016b. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861. Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Ruslan Salakhutdi- nov, and Yann LeCun. 2018. Glomo: Unsupervisedly learned relational graphs as transferable representations. Yang Ye and Shihao Ji. 2019. Sparse graph attention networks. Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. 2020. Curvature graph network. In International Conference on Learning Representations. Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. Bp-transformer: Modelling long-range context via binary partitioning. Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC), pages 87.1–87.12. BMV A Press. Kai Zhang, Yaokang Zhu, Jun Wang, and Jie Zhang. 2020. Adaptive structural ﬁngerprints for graph attention networks. In International Conference on Learning Representations. Marinka Zitnik and Jure Leskovec. 2017. Predicting multicellular function through multi-layer tissue networks. Bioinformatics, 33(14):i190–i198. 12A Graph Datasets For the transductive setup, we used the three standard citation network benchmarks, Cora, Cite- seer and Pubmed (Sen et al., 2008). We followed the transductive setup adopted in (Yang et al., 2016b; Veliˇckovi´c et al., 2017; Xu et al., 2018), where nodes correspond to documents and edges to (undirected) citations. Cora contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. Citeseer contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. Pubmed contains 19717 nodes, 44338 edges, 3 classes and 500 features per node. For the inductive setup, we used the Protein-protein interaction dataset (PPI) (Zitnik and Leskovec, 2017), which aims at classifying protein roles such as cellular functions and gene ontology in various protein-protein interaction (PPI) graphs, where each graph corresponds to a different human tissue. Critically, testing graphs remain completely unobserved during training. The dataset has 56.9K nodes, 806.2 edges with 121 classes. The average number of nodes per graph is 2372. Each node has 50 features that are composed of positional gene sets, motif gene sets and immunological signatures. There are 121 labels for each node set from gene ontology, collected from the Molecular Signatures Database (Liberzon et al., 2011), and a node can have several labels simultaneously. 13barack obama is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017 . a member of the democratic party , he was the first african-american president of the united states . he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004 . barack obama is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017 . a member of the democratic party , he was the first african-american president of the united states . he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004 . 0.0 0.2 0.4 0.6 0.8 1.0 Figure 3: Predicted self-attention links for the text barack obama is an american politician and attorney who served as the 44th president of the president of the united states from 2009 to 2017. as a member of the democratic party, he was the ﬁrst african-american president of the united states. he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004. 14",
      "meta_data": {
        "arxiv_id": "2003.09833v3",
        "authors": [
          "Xiaoya Li",
          "Yuxian Meng",
          "Mingxin Zhou",
          "Qinghong Han",
          "Fei Wu",
          "Jiwei Li"
        ],
        "published_date": "2020-03-22T07:58:44Z",
        "pdf_url": "https://arxiv.org/pdf/2003.09833v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Sparse Adaptive Connection (SAC), a method to accelerate and structure the self-attention mechanism, addressing its quadratic memory and computational cost with respect to input length and its reliance on pre-defined, often fully-connected structures. SAC learns to construct sparse, task-specific attention graphs, significantly reducing memory cost while achieving competitive or state-of-the-art performance across neural machine translation, language modeling, graph representation learning, and image classification. A key contribution is demonstrating that many existing self-attention variants (e.g., Transformer-XL, Adaptive Span Transformer) are special cases of SAC, providing a unifying framework.",
        "methodology": "SAC models the input sequence or data as a graph where attention operations are performed only between learned linked nodes. It employs an LSTM-based Edge Predictor that dynamically constructs edges by sequentially predicting node pairs, aiming for an average of alpha (α) edges per node per layer. To incorporate original structural information (e.g., relative distances in text, graph connectivity), Distance Encodings are integrated into the edge predictor's destination node predictions. The Edge Predictor's parameters are optimized using the REINFORCE reinforcement learning algorithm, where the reward is based on the log probability of the correct label for the downstream task. At test time, edge decoding is performed using beam search. Variants like sharing edge structures across all layers or head-adaptive predictors are also explored.",
        "experimental_setup": "The model was evaluated across four diverse tasks. For **neural machine translation**, the WMT 2014 English-German dataset (4.5 million sentence pairs) was used with a Transformer encoder-decoder backbone, optimized with Adam, and measured by BLEU scores on newstest2013/2014. For **character-level language modeling**, Enwiki8 and Text8 datasets were used with a Transformer decoder backbone, optimized with Adagrad, and evaluated by Bits Per Character (BPC). For **graph representation learning**, transductive tasks used Cora, Citeseer, and Pubmed citation networks (evaluated by accuracy), while the inductive task used the Protein-protein interaction (PPI) dataset (evaluated by micro-averaged F1 score). Models were initialized with a pre-trained Graph Attention Network (GAT) and optimized with Adam. For **image classification**, CIFAR-100 used a Wide-ResNet-28-10 backbone, and ImageNet used a ResNet-50 backbone, both augmented with self-attention and relative position embeddings. Evaluation metrics included Top-1/Top-5 accuracy, GFlops, and parameters. Various α values (sparsity coefficients) were tuned for each task.",
        "limitations": "The primary limitation stems from the complexity of optimizing the edge prediction mechanism; training is impractical without ground-truth edges, necessitating reinforcement learning (REINFORCE), which can introduce challenges such as a larger search space and potential for local optima. While 'head-adaptive' variants offer flexibility, they significantly increase the reinforcement learning search space and do not consistently provide performance benefits across all settings. The effectiveness of distance encodings relies on the availability and quality of explicit structural information within the original data. The broader impact section acknowledges a general risk of abuse, such as generating fake news, which is a common concern for powerful generative models.",
        "future_research_directions": "The paper explicitly highlights the potential of SAC to generate high-quality long text by enabling self-attention to process significantly longer contexts due to reduced computational burden. Further research could explore more sophisticated or efficient reinforcement learning strategies for optimizing the edge predictor, potentially reducing the training complexity. Investigating alternative architectures or richer structural encodings for the edge prediction mechanism could also be a fruitful direction. Additionally, applying SAC to other domains or tasks that suffer from the quadratic complexity of self-attention and could benefit from adaptive sparse connections is a natural extension."
      }
    },
    {
      "title": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers",
      "abstract": "Recently, Transformer networks have redefined the state of the art in many\nNLP tasks. However, these models suffer from quadratic computational cost in\nthe input sequence length $n$ to compute pairwise attention in each layer. This\nhas prompted recent research into sparse Transformers that sparsify the\nconnections in the attention layers. While empirically promising for long\nsequences, fundamental questions remain unanswered: Can sparse Transformers\napproximate any arbitrary sequence-to-sequence function, similar to their dense\ncounterparts? How does the sparsity pattern and the sparsity level affect their\nperformance? In this paper, we address these questions and provide a unifying\nframework that captures existing sparse attention models. We propose sufficient\nconditions under which we prove that a sparse attention model can universally\napproximate any sequence-to-sequence function. Surprisingly, our results show\nthat sparse Transformers with only $O(n)$ connections per attention layer can\napproximate the same function class as the dense model with $n^2$ connections.\nLastly, we present experiments comparing different patterns/levels of sparsity\non standard NLP tasks.",
      "full_text": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers Chulhee Yun MIT chulheey@mit.edu Yin-Wen Chang Google Research NY yinwen@google.com Srinadh Bhojanapalli Google Research NY bsrinadh@google.com Ankit Singh Rawat Google Research NY ankitsrawat@google.com Sashank J. Reddi Google Research NY sashank@google.com Sanjiv Kumar Google Research NY sanjivk@google.com Abstract Recently, Transformer networks have redeﬁned the state of the art in many NLP tasks. However, these models suffer from quadratic computational cost in the input sequence length n to compute pairwise attention in each layer. This has prompted recent research into sparse Transformers that sparsify the connections in the attention layers. While empirically promising for long sequences, funda- mental questions remain unanswered: Can sparse Transformers approximate any arbitrary sequence-to-sequence function, similar to their dense counterparts? How does the sparsity pattern and the sparsity level affect their performance? In this paper, we address these questions and provide a unifying framework that captures existing sparse attention models. We propose sufﬁcient conditions under which we prove that a sparse attention model can universally approximate any sequence-to- sequence function. Surprisingly, our results show that sparse Transformers with only O(n) connections per attention layer can approximate the same function class as the dense model with n2 connections. Lastly, we present experiments comparing different patterns/levels of sparsity on standard NLP tasks. 1 Introduction Transformer networks [28] and their variants [31] have played a key role in the recent advancement of the state of the art in many natural language processing tasks, such as machine translation [28], language modeling [ 23, 24], and question answering [ 10, 17, 31]. The key component of these networks is the self-attention layer [ 1, 18], which updates the embeddings of the input tokens based on their context. Naturally, the self-attention layer also plays the key role in the analysis of Transformers [3, 4, 12, 20, 33]; for example, Yun et al. [33] show that Transformers can approximate any continuous sequence-to-sequence functions (i.e., universal approximation), by proving that self-attention layers can compute contextual mappings of the input embeddings. On the other hand, the self-attention layer is also the main bottleneck in scaling these models. It involves computation of pairwise inner products between input tokens, which results in quadratic computational complexity O(n2) in the length of the input sequence n. To mitigate this issue, researchers have developed methods to sparsify the pairwise interactions/connections in self-attention layers to reduce the computational complexity and/or improve model interpretability, and have shown successful empirical results on tasks with long sequence lengths [ 2, 6, 8, 9, 11, 16, 22, 25, 26, 32, 34, 35]. For example, Child et al. [6] propose sparse Transformers for sequence generation. One of the sparsity patterns considered in [6] is the STRIDED pattern, where the sparse attention layers alternate between two patterns: each token attends to only i) wlocal neighbors, and then ii) one after 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2006.04862v2  [cs.LG]  19 Dec 2020every wtokens in a strided manner. By choosing w= O(√n), they propose sparse attention layers with O(n3/2) connections and show improvements on both speed and performance over the dense Transformer. In the existing results, the rule of thumb for designing sparsity patterns (e.g.,STRIDED ) is connectivity; the intuition is that if each token can attend to the other tokens in multiple “hops,” then the resulting sparse Transformers do not lose much expressive power. However, there has been no formal justiﬁcation for this intuition. How does sparsifying the interaction in the self-attention layers affect the model’s expressive power and ability to learn? What are the sparsity levels at which the model still retains its rich expressive power, and how is it affected by the sparsity pattern? Such fundamental questions about sparse attention models still remain unanswered. 1.1 Summary of contributions In this paper, we take the ﬁrst step towards a theoretical understanding of sparse Transformers. • We propose a uniﬁed framework to analyze sparse Transformers, which generalizes the existing approaches that sparsify attention layers (§ 3.1). • We propose a set of intuitive conditions on the sparsity pattern (Assumption 1) and the probability map (Assumption 2). Then, in Theorem 1, we show that Sparse Transformers, of ﬁxed width and arbitrary depth, satisfying these conditions are universal approximators of any continuous sequence-to-sequence functions for any given ﬁxed sequence length (§ 3.2 and § 3.3). • We next show some examples of existing sparse Transformers [2, 6, 8, 9, 11, 34, 35] that satisfy these conditions, and hence have universal approximability (§ 3.4). Surprisingly, we show that there are sparse Transformers with only O(n) connections per self-attention layer (instead of n2) that have enough expressive power to approximate arbitrary continuous functions (Corollary 2). • We report experimental results on standard NLP tasks using sparse Transformers, comparing different sparsity patterns/levels (§ 5). 2 Preliminaries and related works In this section, we summarize the notation we will use throughout the paper, give a brief overview of Transformers, and then discuss existing efforts to sparsify the self-attention mechanism. 2.1 Notation For a positive integer a, we denote [a] = {1,2,...,a }. For any vector v ∈Rd, let vj denote its j-th coordinate. For any matrix A ∈Rd×n, let Aj denote its j-th column, and ASdenote the submatrix consisting of columns of A in an index set S⊆ [n]. We use ∥A∥p to denote the entry-wise ℓp norm of A. Let σS[·] be the softmax operator, which takes a matrix as input and applies softmax operation to each column of the matrix, which results in a column stochastic matrix. 2.2 Transformers and their universal approximation power A Transformer network, consisting of multiple layers of Transformer blocks, implements a sequence- to-sequence function that maps Rd×n to Rd×n. A Transformer Block (TB) consists of two layers: a self-attention layer and a token-wise feed-forward layer, and both layers have an identity skip connection. More concretely, for an input X ∈Rd×n consisting of d-dimensional embeddings of n tokens, a Transformer block consists of the following two layers: Attn(X) = X + WO   Head1(X) ... Headh(X)  ; Head i(X) = Wi VX ·σS[(Wi KX)TWi QX] (1a) TB(X) = Attn(X) + W2 ·ReLU(W1 ·Attn(X)), (1b) where WO ∈Rd×mh, Wi V,Wi K,Wi Q ∈Rm×d, W2 ∈Rd×r,and W1 ∈Rr×d. Although our analysis and experiments rely on bias vectors, we omit those in (1) for simplicity. To endow the network with information about the position of input tokens, it is common to add a positional embedding E ∈Rd×n to the input X before feeding it to the network. The positional 2embedding E can be ﬁxed [28] or trainable [10]; we consider the latter. Using a trainable E, Th,m,r is deﬁned to be a class of functions of the form X ↦→t(X + E), where tis a composition of any number of Transformer blocks with hattention heads of head size m, and hidden layers of width r. Thus, Th,m,r is a class of Transformers with a ﬁxed width while the depth can be arbitrary. Further, let Fbe the class of continuous functions f : D →Rd×n deﬁned on any compact domain D ⊂Rd×n, where continuity is deﬁned with respect to the entry-wise ℓp norm (1 ≤p <∞). Yun et al. [33, Theorem 3] show that T2,1,4 can universally approximate F. More precisely, for any f ∈F , ϵ >0 and 1 ≤p <∞, there exists a function g ∈T 2,1,4 such that dp(f,g) := ( ∫ D ∥f(X) −g(X)∥p pdX)1/p ≤ϵ. Our goal in this paper is to study, in a similar manner, the expressive power of sparse Transformers. 2.3 Sparse Transformers As seen in Eq. (1a), the self-attention layer involves computing the inner product between each pair of tokens, which we will refer to as theattention score matrix Ai := (Wi KX)TWi QX ∈Rn×n. This leads to quadratic computational complexity in n, which makes it expensive to apply Transformers to tasks with long sequence lengths. One popular approach to mitigate this problem is to sparsify the self-attention layers. We sub-classify sparse Transformers into three categories and summarize them below. For a more extensive summary, please see a recent survey [27]. The ﬁrst category reduces computation by making Ai sparse in a pre-determined manner. Each token in the sequence only attends to a ﬁxed smaller set of other tokens instead of the whole sequence [2, 6, 22]. In some papers, auxiliary tokens are added to improve connectivity between existing tokens while maintaining sparsity [11, 32]. One drawback of these approaches is that the sparsity pattern is independent of input, so it cannot adapt to the data. To remedy this issue, [26] proposes to learn local attention span from data. In a concurrent paper, Zaheer et al. [34] propose the BIGBIRD sparsity pattern which falls into this category. For BIGBIRD , the authors show its theoretical properties such as universal approximation and Turing completeness, as well as its superior empirical performance. We note that our paper focuses on universal approximation for abroader class of sparse Transformers, by proposing a unifying framework to analyze them. The second category studies making Ai sparse after the full Ai has been computed [8, 9, 35]. Here, the focus is not on the computational gain via sparsity, because the full score matrix Ai has to be computed ﬁrst; rather, the goal here is to make attention layers more interpretable, as well as to improve performance. This line of works modiﬁes σS in (1a) to other probability maps, by using top-kelements or adopting sparser variants such as sparselin-gen or α-entmax [15, 21]. Compared to the ﬁrst category, this approach has an advantage that sparsity patterns are adaptive to data. The last category attempts to get the best of both worlds. This line of works tries to learn sparsity patterns from data using extra components predicting the connection between tokens, e.g., k-means clustering [25], LSTM [16], or locality-sensitive hashing [14]. This way, one can adaptively determine the sparsity patterns before computing the score matrix. However, the drawback of this approach is that one needs extra computation to train/run these additional components, which may be expensive. 3 Universal approximation theorem for sparse Transformers In this section, we derive a unifying framework to study sparse Transformers. We then propose a set of conditions on the sparse self-attention layers, and prove that the sparse Transformers satisfying theses conditions are universal approximators of any continuous sequence-to-sequence functions. Finally, we show some examples of existing sparse Transformers that satisfy these conditions. 3.1 A unifying framework for sparse Transformers We modify the Transformer block in (1) to the following sparse Transformer block (STB): SAttnl(X) = X + WO   SHead1,l(X) ... SHeadh,l(X)  , SHeadi,l(X)k = Wi VXAl k ·ρ[(Wi KXAl k )TWi QXk] STBl(X) = SAttnl(X) + W2 ·ReLU(W1 ·SAttnl(X)), (2) 3where the sets Al k ⊆[n], for k ∈[n] and l ∈[p], deﬁne the psparsity patterns (formally deﬁned below), which are indexed by l∈[p]. Moreover, the parameter dimensions stay the same as in (1). Note that there are three main modiﬁcations from the dense Transformer. • (Cycling blocks) There are superscripts l ∈[p] added to the symbols such as SAttn. Unlike dense Transformers, some sparse Transformers cycle through pdifferent patterns. For example, the STRIDED pattern [6] described in § 1 alternates between two different patterns, which corresponds to p= 2. We add the superscript lto include such cases in our formulation. We assume that the layers in a sparse Transformer cycle through STB1,..., STBp. • (Sparsity patterns) Note that SHeadi,l(X)k denotes the k-th column of the i-th sparse attention head. Unlike dense Transformers, the inner product of the k-th query vector Wi QXk is taken only with Wi KXAl k , the key vectors of tokens in the set Al k ⊆[n]. Hence, instead of all n tokens, the k-th token computes attention scores with only tokens in Al k. For l∈[p], we refer to the collection of the index sets {Al k}k∈[n], or simply {Al k}, as a sparsity pattern. As a result, SHeadi,l(X)k is a linear combination of columns in Wi VXAl k , rather than the whole sequence. • (Probability map) After computing the attention score matrix, the dense Transformer (1) uses the softmax operator σS to get a column stochastic matrix. In the sparse Transformers, we generalize σS to ρ. The probability map ρis any map that takes a matrix as input and outputs a column stochastic matrix. As a sanity check, by choosing p = 1 , A1 k = [ n] for all k ∈[n], and ρ = σS, we recover the dense Transformer (1). Note also that the sparse Transformer formulation covers the ﬁrst and second categories of existing results discussed in § 2.3. The ﬁrst category corresponds to choosing a predetermined sparsity pattern(s) {Al k}, while setting ρ= σS. The second category corresponds to opting for a probability map ρother than softmax σS, while maintaining A1 k = [n] for all k∈[n]. In this paper, we assume for simplicity that all sparse attention heads SHead1,l,..., SHeadh,l in a single layer have identical sparsity patterns {Al k}. However, since our result only requires two sparse attention heads per layer (as we will see in Theorem 1), our result can be easily extended to the case that allows multiple sparsity patterns in a single layer. Similar to Th,m,r in § 2.2, we deﬁne the class of functions represented by sparse Transformers. We hide the dependence of this class on the sparsity patterns and probability map to simplify the notation. STh,m,r := {X ↦→t(X + E) |tis a composition of cycling sparse Transformer blocks STBl, each with hheads of head size mand hidden layer size r, and positional embedding E ∈Rd×n is trainable}. (3) 3.2 Conditions on sparsity patterns and probability map In this section, we deﬁne a set of conditions on the sparsity patterns {Al k}and the probability map ρ that ensures that the sparse Transformer universally approximate the function class F(cf. § 2.2). For k∈[n] and the index sets {Al k}l∈[p], we deﬁne a sequence of sets {St k}t≥1 in a recursive way: S1 k := A1 k, St k := ⋃ j∈A(t−1) mod p+1 k St−1 j . The set St k is the set of all tokens that the k-th token can directly/indirectly attend to, after tsparse attention layers with sparsity patterns cycling through {A1 k},{A2 k},..., {Ap k}. We now state our conditions on sparsity patterns. Assumption 1. The sparsity patterns {Al k}satisfy the following: 1. For all k∈[n] and l∈[p], we have k∈Al k. 2. There exists a permutation γ : [n] →[n] such that, for all i∈[n−1], γ(i) ∈⋃p l=1 Al γ(i+1). 3. There exists a ﬁnite s∈N such that s= min{u|Su k = [n] for all k∈[n]}. 4Assumption 1.1 is equivalent to saying that every token always attends to itself. Assumption 1.2 re- quires that there is a chain ofdirect connections that covers allntokens; note that the set⋃p l=1 Al γ(i+1) is the set of all tokens that the γ(i+ 1)-th token directly attends to. To elaborate more about the chain, consider a directed graph with nvertices corresponding to the ntokens. For any j ∈⋃p l=1 Al k, we add a directed edge j →k. Given a graph constructed this way, Assumption 1.2 requires that the graph has a Hamiltonian path γ(1) →γ(2) →···→ γ(n). Assumption 1.3 requires that after s sparse attention layers, every token can attend to all the other tokens, either directly or indirectly. As we discuss in § 3.4, the statements in Assumption 1 are natural enough to be satisﬁed by many existing sparsity patterns studied in the literature. In fact, Assumption 1.3 is necessary for universal approximation. If p= 1, n= 2, A1 1 = {1}and A1 2 = {1,2}, then the ﬁrst token never attends to the second, so this sparse Transformer cannot approximate a function whose ﬁrst output token is dependent on both input tokens. The other two assumptions are required in parts of our proof, which involve “propagating information” over all the tokens in a sequential manner. We now state the assumption on the probability map ρ[·]. For this, we deﬁne σH[·] to be the hardmax operator, which outputs the one-hot representation of the arg max entry for each column of the input matrix. Since ρis a column-wise operator that outputs a column-stochastic matrix, we state the assumption for the operation of ρon a single column. Assumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying vj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑ j̸=j∗ρ[tv]j ≤η. Assumption 2 requires that, for inputs that have some margin between the unique maximum entry and the other entries, ρ[·] can closely approximate the behavior of the hardmax operator by scaling its input by a positive factor t. This assumption is satisﬁed by softmax σS and other sparse variants such as sparselin-gen and α-entmax, as we show in § B of the supplementary material. It is straightforward to check that the dense Transformer, which corresponds to p= 1, A1 k = [n], and ρ[·] = σS[·] in our framework, satisﬁes both Assumptions 1 and 2. 3.3 Sparse Transformers are universal approximators The key justifying intuition for adopting sparse attention layers is that, if each token can attend to the other tokens in multiple hops1, then these models do not lose too much expressive power. However, turning this intuition into a rigorous analysis is not straightforward. Moreover, recent results show that limited width can render universal approximation impossible even with arbitrary depth [13, 19], highlighting the challenges in analyzing sparse (limited “width”) Transformers. We now state our main theorem, which shows that if the sparsity patterns{Al k}and the probability map ρsatisfy Assumptions 1 and 2, sparse Transformers with h= 2 attention heads of size m= 1, and hidden layer width r = 4 are universal approximators of continuous sequence-to-sequence functions on any compact domain (recall that Fdenotes the class of such continuous functions). Theorem 1. Consider any f ∈F , and the class of sparse Transformers ST2,1,4 (cf. (3)) with the underlying sparse attention layers satisfying Assumptions 1 and 2. Then, for any ϵ >0 and 1 ≤p< ∞, there exists a function g∈ST 2,1,4 such that dp(f,g) := (∫ D ∥f(X) −g(X)∥p pdX )1/p ≤ϵ. As discussed earlier, dense Transformers satisfy Assumptions 1 and 2, which means that Theorem 1 subsumes the existing result [33] for dense Transformers. We note that the required h, m, and rin Theorem 1 are independent of d, n, or the sparsity patterns. We provide a high-level proof sketch of Theorem 1 in § 4.1. There, we also discuss how many layers are sufﬁcient for ϵ-approximation of f, and show that Theorem 1 requires only ptimes more self-attention layers than Yun et al. [33]. We would like to emphasize that Theorem 1 provides the ﬁrst formal evidence that well-designed sparse attention layers do not limit Transformer’s universal approximation power. In § 3.4, we show a surprising fact that some existing sparse self-attention layers with only O(n) connections (as opposed to n2 in regular self-attention layers) retain enough expressive power to approximate F. Combined with the number of layers analyzed in § 4.1, this means that our analysis reduces the 1Note that this corresponds to our Assumption 1.3. 5connections per layer from n2 to O(n), with only ptimes more attention layers. This advantage of sparse Transformers over their dense counterpart becomes even stronger with increasing sequence length n, providing a theoretical support for the adoption of sparsity for tasks with long sequence lengths. On a ﬁnal note, Theorem 1 views the sequence length nas a ﬁxed constant. Hence, our result does not contradict a recent paper by Hahn [12] which studies the limitation of Transformers for varying n. Also, our analysis applies to the encoder part of the Transformer network [28]. 3.4 Analysis of existing sparse Transformers By Theorem 1, any sparse Transformer that satisﬁes our Assumptions 1 and 2 has universal approxi- mation ability. In this section, we give some examples of such sparse Transformers. Child et al. [6] propose two kinds of 2-step sparsity patterns (i.e., p= 2) for sequence generation tasks, namely STRIDED and FIXED patterns. We consider the extension of their auto-regressive patterns (i.e., attending only to past tokens) to the whole sequence. In the STRIDED pattern, a token ﬁrst attends to its wneighbors and then attends to one token after every wtokens in a strided manner. The sparsity pattern for the k-th token reads A1 k = [n] ∩{k−⌈w/2⌉,...,k −1,k,k + 1,...,k + ⌊w/2⌋}, A2 k = [n] ∩{...,k −2w,k −w,k,k + w,k + 2w,... }. (4) In the FIXED pattern, we divide the token into segments of length w. A token in a segment has access to other tokens in the same segment, and then the last tokens of the other segments: A1 k = [n] ∩{⌈k/w⌉·w−w+ 1,..., ⌈k/w⌉·w}, A2 k = [n] ∩({k}∪{w,2w,3w,... }) . (5) The STRIDED and FIXED patterns satisfy both Assumption 1 and 2 for all values of w. Speciﬁcally, Assumption 1.3 holds with s= 2, because any token can directly/indirectly access all the tokens in two hops. As for Assumption 1.2, the identity permutation γ(i) = isufﬁces to satisfy the assumption for both patterns. By choosing w = O(√n), sparse Transformers with the STRIDED and FIXED patterns achieve universal approximation power with O(n3/2) connections per attention layer. Guo et al. [11] consider the STAR sparsity pattern where they add an auxiliary relay token that attends to all the tokens, and the other tokens attend only to 2w neighboring tokens and the relay token. There is only one sparsity pattern, so p= 1. The S TAR sparsity pattern can be written as A1 k={n}∪ { (i−1) mod (n−1) + 1 |i∈{k−w,...,k + w} } for k∈[n−1], A1 n=[n], (6) where w≥1. For any ﬁxed w, this sparse Transformer has O(n) connections per attention layer, and it satisﬁes both assumptions. Speciﬁcally, Assumption 1.2 is satisﬁed with the identity permutation, i.e., γ(i) = (i) for i∈[n]. Since any token can access other tokens within two hops, Assumption 1.3 is satisﬁed with s = 2 . This demonstrates that O(n) connections per layer sufﬁce for sparse attention layers to have universal approximation power. One can similarly check that the sliding window sparsity patterns with/without global attention, proposed in Longformer [2], also satisfy the assumptions with O(n) connections. For the BIGBIRD sparsity pattern [34], it is also straightforward to check that a combination of its window attention and global attention satisﬁes Assumption 1 with O(n) connections. We state this interesting observation as a corollary below. Corollary 2. There exist sparse Transformers withO(n) connections per self-attention layer that are universal approximators in the sense of Theorem 1. Recall that another line of results that replaces softmax σS with sparse variants ρ[8, 9, 35] also ﬁts into our formulation, with p = 1 and A1 k = [n]. As we show in § B, these alternative ρ’s satisfy Assumption 2. Thus, by Theorem 1, these models also have the universal approximation property. 4 Proof sketch and discussion 4.1 Sketch of proof of Theorem 1 Now, we sketch the proof of Theorem 1, which consists of three steps. Throughout the proof, we assume without loss of generality that D ⊂[0,1)d×n. Step 1. In the ﬁrst step, we approximate f ∈F with a piecewise constant function. Towards this, consider a class of piecewise constant functionsF(δ) that map D to Rd×n, where δ >0 and δ−1 is an 6integer. Any function in F(δ) maps cubes of the form G+ [0,δ)d×n to matrices AG ∈Rd×n, where G ∈{0,δ,..., 1−δ}d×n. We approximate f with a function f ∈F(δ) such that dp(f,f) ≤ϵ/2, by choosing small enough δ. We defer the statement and the proof to § C of the supplementary material. Step 2. We then approximate f ∈F(δ) with a sparse Transformer network with a slightly modiﬁed architecture. In this architecture, we replace ReLU in the feed-forward layer with any piecewise linear activation φ ∈Φ, where Φ denotes the class of (possibly discontinuous) piecewise linear functions with three pieces. We also replace ρin the sparse attention layer with the hardmax σH operator. We refer to the function class represented by the modiﬁed sparse Transformer as ST h,m,r . By a careful construction, Lemma 3 shows that any f ∈F(δ) can be exactly represented by the modiﬁed Transformer. To this end, we ﬁrst carefully choose the positional embedding E. We then quantize the inputs using feed-forward layers (Lemma 6), construct a contextual mapping using self-attention layers to map the quantized inputs to unique “ids” (Lemma 7), and then construct a value mapping with feed-forward layers to map the ids to desired output values (Lemma 8). See § D and § E in the supplementary material for details. Lemma 3. For any f ∈F(δ), there exists g∈ST 2,1,1 such that f(X) = g(X) for all X ∈D. Step 3. The ﬁnal step is to approximate the function g ∈ ST 2,1,1 with a sparse Transformer g ∈ ST2,1,4. This is done by approximating φ and σH with ReLU and ρ, respectively, while carefully bounding the accumulation of errors introduced by the approximation. See § F in the supplementary material for the details. Lemma 4. For g∈ST 2,1,1 in Lemma 3, there exists g∈ST 2,1,4 such that dp(g,g) ≤ϵ/2. Combining these three steps, we establish that dp(f,g) ≤dp(f,f) + dp(f,g) + dp(g,g) ≤ϵ. How many layers are sufﬁcient? In § D, Lemmas 6–8 show that we need dn δ sparse Transformer blocks (2) for quantization, p(n−1) δd + sfor the contextual mapping, and n δdn for the value mapping. Recall that pis from (2), sis from Assumption 1, and δis from Step 1 above. In comparison, § C of [33] shows that the dense counterpart requires dn δ , n δd + 1, and n δdn Transformer blocks (1) for the three corresponding lemmas. Note two observations: 1) The value mapping dominates the depth, and its depth requirements are identical for the two cases; and 2) For contextual mappings (where the attention layers are used), we need roughly ptimes more layers for sparse models. Recall from § 3.4 that pis usually a small constant. These observations mean that sparse Transformers can achieve universal approximation using depth of the same order in d, nand δas the dense Transformers. 4.2 Key challenges in the proof While the high level outline of the proof is similar to the one for dense Transformers [33], the proof in [33] crucially relies on having all connections for computing attention in each layer, which we do not have in sparse Transformers. The sparsity in attention mechanism and the choice of general probability map ρpose nontrivial challenges in the proof. We highlight the key differences below. Establishing the Step 2 of the dense result [33] relies on constructing a contextual mapping using attention layers. A contextual mapping is a function that maps tokens in different sequences to unique values, thereby allowing Transformers to distinguish the same token appearing in different contexts. A crucial ingredient in the construction of such a mapping is a shift operation implemented with two attention heads in an attention layer. This shift operation involves each token taking the maximum and minimum over the entire sequence, which obviously cannot be done with sparse Transformers as it would require each token to attend to all the other tokens in the sequence. We circumvent this issue by carefully choosing the positional embedding E dependent on γ(cf. Assumption 1.2), and ensuring that a similar shift operation is applied in a desired order even under sparsity. As the ﬁnal phase of the contextual mapping in [33], a single attention layer shifts the entire sequence by the maximum over the sequence. Again, this cannot be directly implemented due to sparsity. Using Assumption 1.3, we instead prove that by stacking ssparse layers, one can successfully implement a similar operation that shifts the entire sequence by the maximum over the whole sequence, up to some controlled errors. This way, we overcome the difﬁculties posed by the sparsity and construct a new version of contextual mappings. The details can be found in § E.2 of the supplementary material. Moreover, the proof of Step 3 in [33] uses the simple fact that softmax can approximate hardmax arbitrarily closely. Since we do not restrict ourselves to softmax and generalize the probability map, 7Table 1: Accuracy on the synthetic copying task. Percentages in parentheses mark the sparsity levels. STRIDED FIXED STAR RANDOM Depth UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) (87%) (90%) 1-layer 0.82% 0.82% 0.80% 7.04% 0.76% 0.80% 1.53% 33.14% 2-layer 100.00% 100.00% 81.24% 69.26% 56.45% 96.01% 29.70% 63.41% 3-layer 100.00% 100.00% 100.00% 99.98% 99.08% 98.58% 42.18% 70.29% 4-layer 100.00% 100.00% 100.00% 100.00% 99.64% 100.00% 83.57% 95.49% a more careful argument is required. Since there are many layers in the network g, it turns out that approximating it with an original sparse Transformer in ST2,1,4 requires carefully controlling the approximation errors accumulated over layers. The proof of Lemma 4 in § F of the supplementary material shows that this is indeed possible by utilizing Assumption 2. 5 Experiments We now present our experimental study comparing different design and implementation choices, including sparsity patterns and levels, on four tasks: i) a synthetic copying task, ii) language modeling, iii) translation, and iv) GLUE tasks. Our goal is to understand the effect of such choices while employing sparse Transformers to the tasks with small sequence lengths, complementing the existing results for sparse Transformers on long sequence tasks. 5.1 Experiment Settings We consider four sparsity patterns: STRIDED (4), FIXED (5), STAR (6) and RANDOM . The ﬁrst three patterns are proposed in [6] and [11]; we test them for different values of w. In case of the RANDOM pattern, given a sparsity level, we make connections uniformly at random. Following [6], STRIDED and FIXED patterns are tested for three different head conﬁgurations: i) SEQUENTIAL , where the sparse attention layers alternate between {A1 k}and {A2 k}, as described in the previous sections; ii) UNION , where all sparse attention layers use the sparsity pattern{A1 k∪A2 k}; and iii) MULTIHEAD , where half of the attention heads in every attention layer use {A1 k}and the other half use {A2 k}. Note that, given the same sequence length, UNION is less sparse than the other two conﬁgurations. Thus, to ensure fair comparisons, we compare different conﬁgurations based on their sparsity levels. We use maximum sequence length 256 in all our experiments, except 128 for GLUE tasks. For the copying task, we experiment with only one sparse Transformer block (cf. Eq (2)), with varying numbers of attention layers with 4 attention heads. For language modeling and translation, we use the Tensor2Tensor [29] framework and employ 12-block and 6-block (respectively) Transformers with 8 attention heads per block. For GLUE tasks, we experiment with the BERTBASE model. For more details of the setup, see § G of the supplementary material. 5.2 Results Copying task. We consider a synthetic copying task proposed in [ 14], where the input sequence has the format 0s0s, where s is a 127 length sequence of symbols in [0,127]. The models have to predict (copy) the second part, given the ﬁrst half of the input. This task tests the ability of sparse Transformers to communicate the information. Table 1 presents the results for this task. Except for the STAR and RANDOM patterns, we can see that the networks learn to copy the sequences with four sparse attention layers. One possible explanation for the bad performance of STAR is that, except for the relay token, it only attends to local neighbors while the task requires to copy distant tokens. Language modeling. We conduct the language modeling experiments on the One Billion Word Benchmark [5] which has almost one billion tokens and a vocabulary of more than 800K unique tokens. In Figure 1a, we plot the perplexity against the sparsity level. We observe that the STRIDED pattern and the STAR achieve the best performance across all sparsity levels. For both the STRIDED and FIXED patterns, the UNION conﬁguration shows the best performance. Translation. For the translation task, we train the model on WMT18 English-Czech (en-cs) dataset and test it on the Newstest 2015 dataset. We plot the BLEU score against the sparsity level in Figure 1b. We apply the same sparsity pattern to both the encoder and the decoder. The STRIDED 8(a) One Billion Benchmark  (b) WMT en-cs Figure 1. Comparison of sparsity patterns and different head conﬁgurations on the One Billion Benchmark (a language modeling task) and WMT en-cs (a translation task). Note that the number of connections in the attention layers goes down as we increase the sparsity level. (a) MNLI  (b) XNLI Figure 2. Comparison of sparsity patterns and different head conﬁgurations on the MNLI and XNLI (sentence-pair classiﬁcation tasks), using the BERTBASE model. and FIXED patterns with UNION conﬁguration show the best scores, which are similar to the dense attention. The U NION conﬁguration is also the least sensitive to the sparsity levels. GLUE Tasks. We experiment with the BERTBASE model and report results on two sentence-pair classiﬁcation tasks: MNLI [30] (Figure 2a) and XNLI [7] (Figure 2b). We plot the average accuracy of three runs on the dev set against the sparsity level. Additional results of the CoLA and MRPC tasks are reported in § H of the supplementary material. Discussion. In all tasks, the RANDOM pattern performs worse than the deterministic patterns, demonstrating the need for a careful design of sparsity patterns. Overall, our experiments suggest that the design of the optimal sparsity patterns is heavily dependent on speciﬁc tasks. For example, the STAR pattern shows the best performance on the language modeling task, while having trouble with copying, translation, and BERT experiments. Among the three head conﬁgurations tested for STRIDED and FIXED , the UNION performs the best in language modeling and translation but suffers in BERT tasks. In translation experiments, we see an interesting trend that the performance of MULTIHEAD conﬁguration improves as sparsity increases. We conjecture that this is due to the fact that in STRIDED and FIXED , we have |A1 k|= O(w) and |A2 k|= O(n/w) (cf. Eqs (4) and (5)), so the sparsest choice of w= O(√n) is the one with the best “balance” between|A1 k|and |A2 k|. 6 Conclusion Recently, sparse Transformers have received a lot of attention as they enable more efﬁcient/faster attention mechanisms for the tasks with very long sequence lengths. We take an initial step to provide a theoretical understanding of these models. We provide a unifying framework that captures existing sparse attention models, and prove a universal approximation theorem for sparse Transformers which holds under intuitive conditions on sparsity patterns and probability maps. We also carry out experiments comparing different sparsity patterns and levels on standard NLP tasks. We hope that this work will shed light on the understanding of sparsity in attention layers, and provide guidance for the design of sparse attention models. 9Broader Impact This work studies theoretical aspects of a class of widely used neural network models in NLP and related areas. Since we do not propose a new method nor a new dataset, we expect that the impact of this work on ethical aspects and future societal consequences will be small, if any. Other than that, this work brings new insights into the sparsity in attention models, hence may make an impact on the study of faster and more efﬁcient NLP models. Acknowledgments and Disclosure of Funding CY acknowledges partial support as a graduate Research Assistant from the NSF Grant (CAREER 1846088). CY also acknowledges Korea Foundation for Advanced Studies for their support. References [1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015. [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document Transformer. arXiv preprint arXiv:2004.05150, 2020. [3] Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. arXiv preprint arXiv:2002.07028, 2020. [4] Gino Brunner, Yang Liu, Damián Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. On identiﬁability in Transformers. arXiv preprint arXiv:1908.04211, 2019. [5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005. [6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse Transformers. arXiv preprint arXiv:1904.10509, 2019. [7] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018. [8] Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse Transformers. arXiv preprint arXiv:1909.00015, 2019. [9] Baiyun Cui, Yingming Li, Ming Chen, and Zhongfei Zhang. Fine-tune BERT with sparse self-attention mechanism. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3539–3544, 2019. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [11] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Star-Transformer. arXiv preprint arXiv:1902.09113, 2019. [12] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156–171, 2020. [13] Jesse Johnson. Deep, skinny neural networks are not universal approximators. In International Conference on Learning Representations, 2019. [14] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient Transformer. arXiv preprint arXiv:2001.04451, 2020. 10[15] Anirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra, Karthik Sankaranarayanan, and Harish G Ramaswamy. On controllable sparse alternatives to softmax. In Advances in Neural Information Processing Systems, pages 6422–6432, 2018. [16] Xiaoya Li, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. Sac: Accelerating and structuring self-attention via sparse adaptive connection. arXiv preprint arXiv:2003.09833, 2020. [17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [18] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention- based neural machine translation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1412–1421, Lisbon, Portugal, September 2015. Association for Computational Linguistics. [19] Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approxi- mation. arXiv preprint arXiv:2006.08859, 2020. [20] Jorge Pérez, Javier Marinkovi´c, and Pablo Barceló. On the Turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. [21] Ben Peters, Vlad Niculae, and André FT Martins. Sparse sequence-to-sequence models. arXiv preprint arXiv:1905.05702, 2019. [22] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019. [23] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. Technical Report, OpenAI, 2018. [24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Technical Report, OpenAI, 2019. [25] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse attention with routing Transformers. arXiv preprint arXiv:2003.05997, 2020. [26] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in Transformers. arXiv preprint arXiv:1905.07799, 2019. [27] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv preprint arXiv:2009.06732, 2020. [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, pages 5998–6008, 2017. [29] Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, et al. Tensor2tensor for neural machine translation. arXiv preprint arXiv:1803.07416, 2018. [30] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 1112–1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101. [31] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. [32] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-Transformer: Modelling long-range context via binary partitioning. arXiv preprint arXiv:1911.04070, 2019. 11[33] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are Transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2020. [34] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020. [35] Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun. Ex- plicit sparse Transformer: Concentrated attention through explicit selection. arXiv preprint arXiv:1912.11637, 2019. [36] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015. 12A Outline and notation The supplementary material is organized as follows. First, § B proves that the softmax operator as well as its sparse versions indeed satisfy Assumption 2. Next, § C provides formal statements of Step 1 in the proof sketch (§ 4.1). The outline of proof of Lemma 3 (Step 2 in the proof sketch) is presented in § D, followed by a separate section (§ E) proving the three key sublemmas in the proof. The proof of Step 3, Lemma 4, is given in § F. Lastly, § G and § H present the detailed setup of our experiments and additional experiment results, respectively. We next review some of the notation and also introduce additional notation used throughout the supplementary material. For a positive integer a, let [a] := {1,...,a }. For a,b,c ∈R where b−a> 0 is an integer multiple of c> 0, we write [a: c: b] := {a,a + c,a + 2c,...,b −c,b}. For any matrix A ∈Rd×n, let Aj denote its j-th column, and ASdenote the submatrix consisting of columns of A in the index set S⊆ [n]. We also use Ai,j to denote its (i,j)-th entry. Let 1 {·}be the 0-1 indicator for an event. Let 1n ∈Rn be a vector whose components are all 1. B Sparse probability maps satisfy Assumption 2 In this section, we show that the softmax operatorσS as well as the probability maps ρused to replace softmax in the existing approaches, namely softmax with only top-kinputs [35], sparselin-gen [9], and α-entmax [8], all satisfy Assumption 2. We restate the assumption for reader’s convenience: Assumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying vj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑ j̸=j∗ρ[tv]j ≤η. As in the assumption, we only consider the operation of these probability maps on a single vector, as they are applied column-wise. For each of the probability maps, we will show that for any ζ >0 and η∈(0,1], we can choose t> 0 that satisﬁes the conditions of Assumption 2. B.1 Softmax & softmax with top- kinputs Given an input vector v ∈Rn, the j-th coordinate of the output of softmax σS[v] is deﬁned as σS[v]j := exp(vj)∑n i=1 exp(vi). We assume without loss of generality that the entry ofv is in decreasing order, where the ﬁrst two entries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence of t> 0 such that σS[tv]1 = exp(tv1)∑n i=1 exp(tvi) ≥1 −η. Then, ∑n j=2 σS[tv]j ≤ηfollows. Now, since vi ≤v1 −ζfor i∈[2 : n], note that σS[tv]1 = exp(tv1)∑n i=1 exp(tvi) ≥ exp(tv1) exp(tv1) + (n−1) exp(tv1 −tζ) = 1 1 + (n−1) exp(−tζ). Since 1 1+(n−1) exp(−tζ) is an increasing function in t >0, one can increase tsufﬁciently large to make it greater than 1 −η. The same argument holds for the softmax with top-kinputs, used in [35]. By the assumption on v, entries v1,...,v k are the top kcomponents. Thus, ρ[tv]1 ≥ 1 1 + (k−1) exp(−tζ) ≥1 −η can be satisﬁed by choosing large enough t> 0. B.2 Sparselin-gen We now consider the case where ρis sparselin-gen [15], which was used to sparsify the attention score matrices in [9]. Given a regularization parameter λ∈[0,1), the sparselin-gen used in [9] is deﬁned as ρ[v] := arg min p∈∆n−1 ∥p −v∥2 −λ∥p∥2 , 13where ∆n−1 := {p ∈Rn |p ≥0,∑n i=1 pi = 1}is the probability simplex. Then, the solution for optimization problem above can be written as ρ[v]j = max { 0,vj −τ(v) 1 −λ } , for j ∈[n], where τ : Rn →R is a threshold function that chooses the threshold τ(v) such that ∑n j=1 ρ[v]j = 1. Now, assume without loss of generality that the entry of v is in decreasing order, where the ﬁrst two entries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence of t> 0 such that ρ[tv]1 ≥1 −η. This is done by choosing t= 1−η ζ . To see this, notice that if vj’s are in decreasing order, then ρ[v]j are also in decreasing order. Now consider ρ[tv]1 = max { 0,tv1 −τ(tv) 1 −λ } , ρ[tv]2 = max { 0,tv2 −τ(tv) 1 −λ } . If ρ[tv]2 = 0, then ρ[tv]j = 0 for all j = 3,...,n , and ρ[tv]1 = 1 ≥1 −η. If ρ[tv]2 >0, then ρ[tv]1 −ρ[tv]2 = tv1 −τ(tv) 1 −λ −tv2 −τ(tv) 1 −λ = t(v1 −v2) 1 −λ ≥t(v1 −v2) ≥tζ = 1 −η. B.3 α-entmax Next, we consider the case where ρis α-entmax [21], which was used to sparsify the attention score matrices in [8]. Given a parameter α≥1, the α-entmax is deﬁned as ρ[v] := arg max p∈∆n−1 pTv + Hα(v), where ∆n−1 is the probability simplex and Hα is the Tsallis continuous family of entropies Hα(v) := { 1 α(α−1) ∑ jvj −vα j α> 1, −∑ jvjlog vj α= 1. As shown in [8], the solution of α-entmax is equal to softmax if α= 1, and otherwise (α> 1) it is given in the form ρ[v]j = [ max{0,(α−1)vj −τ(v)} ] 1 α−1 , for j ∈[n], where τ : Rn →R is a threshold function that chooses the threshold τ(v) such that ∑n j=1 ρ[v]j = 1. Since softmax (α= 1) is already covered above, we focus on α> 1. Again, assume without loss of generality that the entry of v is in decreasing order, where the ﬁrst two entries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence of t> 0 such that ρ[tv]1 ≥1 −η. This is done by choosing t= 1/ζ(α−1). Note that (α−1)t(v1 −v2) ≥1 due to our choice of t. Then, we will show that with such a t, ρ[tv]1 = 1 must hold. For the sake of contradiction, suppose not: ρ[tv]1 <1. Then, by monotonicity of ρ[tv]j, we have ρ[tv]2 >0. This means ρ[tv]2 = [ (α−1)tv2 −τ(tv) ] 1 α−1 >0, in particular, we have (α−1)tv2 −τ(tv) >0. However, recall that (α−1)t(v1 −v2) ≥1, which implies (α−1)tv1 −τ(tv) >1. This results in ρ[tv]1 = [ (α−1)tv1 −τ(tv) ] 1 α−1 >1, thus contradicting ρ[tv]1 <1. Therefore, ρ[tv]1 = 1 must hold. C Details of the Step 1 in the proof sketch (§ 4.1) We start by formally deﬁning the function class F(δ). F(δ) := { Z ↦→ ∑ G∈Gδ AG1 { Z ∈G + [0,δ)d×n} |Z ∈D,AG ∈Rd×n } , where Gδ := {0,δ,..., 1 −δ}d×n. We now state and prove the lemma. 14Lemma 5. For any f ∈F and ϵ >0, there exists a small enough δ >0 such that there exists f ∈F(δ) such that dp(f,f) ≤ϵ/2. Proof Since f : D → Rd×n is a continuous function on a compact domain, it is uniformly continuous. Also, continuity is deﬁned with respect to entry-wise ℓp norm which is equivalent to entry-wise ℓ∞norm, uniform continuity leads to ∀ϵ> 0,∃δ >0 such that ∀X,Y ,∥X −Y ∥∞<δ =⇒ ∥f(X) −f(Y )∥p <ϵ/2. Then, suppose we create a set of cube grid pointsGδ := {0,δ,..., 1−δ}d×n, and deﬁne a piece-wise constant approximation f(X) = ∑ G∈Gδ f(G)1 { X ∈G + [0,δ)d×n} . Note that for any X ∈G + [0,δ)d×n we have ∥X −G∥∞<δ, so we have f(X) −f(X)  p = ∥f(X) −f(G)∥p <ϵ/2. This implies that dp(f,f) = (∫ D f(X) −f(X) p p )1/p ≤ϵ/2, ﬁnishing the proof of the lemma. D Proof of Lemma 3 (Step 2 in § 4.1) In this section, we describe in further details how modiﬁed sparse Transformers (the class ST 2,1,1 ) are able to exactly express arbitrary piecewise constant functions in F(δ). We show that we can compute a contextual mapping of the entire input sequences without relying on dense self-attention layers. The token-wise feed-forward layers then transform these contextual mappings to the desired output sequence. To give a high level summary of the proof, we want to show that given a piece-wise constant function f ∈F(δ), there exists a modiﬁed Transformer network g∈ST 2,1,1 that exactly represents f. Recall ﬁrst that the function class ST 2,1,1 has an additive positional embedding matrix E ∈Rd×n that is added to input before the input is fed to the network. We start by choosing the positional embedding E and construct a Transformer network that implements quantization of the input, contextual mapping of the quantized input, and value mapping of the context ids. 1. Choose the positional embedding E according to γin Assumption 1.2. After addition, each column of the input Xk + Ek are in disjoint intervals. 2. Given the input X + E, a series of modiﬁed feed-forward layers quantizes it so that each entry of the quantized input has a value in {0,δ,...,n −δ}(Lemma 6). 3. Next, a series of modiﬁed sparse self-attention layers takes the quantized input H and implement a contextual mapping qsuch that, for different quantized input sequences H and H′, all the elements in q(H) and q(H′) are distinct (Lemma 7). 4. Finally, a series of modiﬁed feed-forward layers maps each element in the context id q(H) to the desired output value of f ∈Fat the input X (Lemma 8). We defer the proofs of Lemmas 6, 7, and 8 to a separate section: see § E. Before discussing the details of each step, we note that although a Transformer network stacks self-attention and feed-forward layers in an alternate manner, we can use a series of arbitrary number of the same layers, thanks to skip connections. The outline of the proof is similar to [ 33], but key component in their proof called selective shift operation relies on the fact that each token can attend to the entire sequence; this is not true in sparse Transformers, which poses a nontrivial challenge. We overcome this issue by a more careful construction of the positional embedding E and sparse self-attention layers. 15D.1 Choosing the positional embedding Recall from Assumption 1.2 that there exists a permutation γ : [n] →[n] such that for all i∈[n−1], γ(i) is one of the tokens that the γ(i+ 1)-th token directly attends to. Using this permutation γ, we choose the columns of positional embedding E in the following way: Eγ(1) = (n−1)1n, and Eγ(i) = (i−2)1n, for i∈[2 : n] As a result, theγ(1)-th column ofX+E will be in the range[n−1,n)d, and similarlyXγ(i)+Eγ(i) ∈ [i−2,i −1)d for i∈[2 : n]. This means that the entries corresponding to different tokens lie be in disjoint intervals of the form [j,j + 1), where j ∈[0 : n−1]. D.2 Quantization by feed-forward layers Note from the previous step that each entry of X + E must be in [0,n). Next, we quantize this interval [0,n) of input using to a set of δ-grid points {0,δ,...,n −δ}. This allows us to deal with ﬁnite set of values, which proves useful in the later stages of the proof. The next lemma shows that the quantization can be carried out using a seried of the modiﬁed feed-forward layers. Lemma 6. Consider a entry-wise quantization map gent q : R →R: gent q (t) = {kδ if kδ ≤t< (k+ 1)δ, k∈[0 : n/δ−1], t otherwise. There exists a function gq : Rd×n ↦→Rd×n composed of dn δ token-wise feed-forward layers with r= 1 and an activation φ∈Φ, which implements the entry-wise quantization gent q to each entry of its input. D.3 Contextual mapping by sparse self-attention layers After the input X + E is quantized, the output of gq must be in the following set Hδ ⊂Rd×n: Hδ := {G + E ∈Rd×n |G ∈Gδ}, where Gδ := {0,δ,..., 1 −δ}d×n was deﬁned to be the δ-cubic grid points of [0,1)d×n. Using this ﬁnite set of sequences, we construct a contextual mapping that maps each sequence in Hδ to unique numbers. Recall that the sparse attention layer has psparsity patterns that rotate in cycles, and Assumption 1.3 assumes that one token directly/indirectly access all the other tokens after ssuch sparse attention layers. We now state the lemma. Lemma 7. Assume that n≥2, and δ−1 is an integer satisfying δ−1 ≥2. Suppose that the sparse self-attention layers (h = 2,m = 1) satisfy Assumption 1 and employ the hardmax σH operator, and that the positional embedding E was chosen as described in § D.1. Then, there exist a function gc : Rd×n →Rd×n composed of p(n−1) δd + ssparse self-attention layers, and a vector u ∈Rd, such that q(H) := uTgc(H) satisﬁes the following properties: 1. For any H ∈Hδ, the entries of q(H) are all distinct. 2. For any H,H′∈Hδ such that H ̸= H′, all entries of q(H), q(H′) are distinct. This contextual mapping maps each unique sequence/context into different context ids, enabling the network to distinguish the same token appearing in different sequences. D.4 Value mapping by feed-forward layers After the contextual mapping, we use the token-wise feed-forward layers to map each different context ids to the desired output value of the target function f. More speciﬁcally, recall the function gc from Lemma 7. For any H ∈Hδ, we need to map the output gc(H) of Lemma 7 to the desired function value f(H −E) (recall that H is the quantized input after adding E to X, so we need to subtract E). This is done by implementing a token-wise value mapping using the feed-forward layers. 16Lemma 8. There exists a function gv : Rd×n → Rd×n composed of n(1 δ)dn token-wise feed- forward layers (r = 1) with an activation φ′∈Φ such that gv is deﬁned by a token-wise function gtkn v : Rd →Rd on each column, gv(Z) = [ gtkn v (Z1) ··· gtkn v (Zn) ] , where for all H ∈Hδ and k∈{1,...,n }, gtkn v (gc(H)k) = f(H −E)k. D.5 Finishing the proof Given Lemmas 6, 7, and 8, one can easily check that for any G ∈Gδ := {0,δ,..., 1 −δ}d×n and any input value X ∈G + [0,δ)d×n, we have gv ◦gc ◦gq(X + E) = gv ◦gc(G + E) = [ gtkn v (gc(G + E)1) gtkn v (gc(G + E)2) ··· gtkn v (gc(G + E)n) ] = [ f(G)1 f(G)2 ··· f(G)n ] = f(G) = f(X). Therefore, we have constructed a modiﬁed sparse Transformer networkg(X) := gv ◦gc ◦gq(X +E) that satisﬁes g(X) = f(X) for all X ∈D, hence proving Lemma 3. E Proof of Lemmas 6, 7, and 8 E.1 Proof of Lemma 6 The proof goes as follows. Using n δ token-wise feed-forward layers, we implement the quantization function gent q that quantizes the ﬁrst row of the input. Then we stack another n δ layers to quantize the second row, and so on. For the ﬁrst row, we add n/δlayers of the following form, for k∈[0 : n/δ−1]. Z ↦→Z + e(1)φ((e(1))TZ −kδ1T n), φ(t) = {0 t< 0 or t≥δ, −t 0 ≤t<δ, where e(1) ∈Rd is the ﬁrst canonical basis vector e(1) = (1,0,..., 0). Each layer quantizes Z1,: in [kδ,kδ + δ) to kδ, without modifying other intervals or other rows of Z. Note that the activation φis a piecewise linear function with three pieces; hence, φ∈Φ. Therefore, the layers satisfy the deﬁnition of modiﬁed feed-forward layers. We can now repeat the same construction for the d−1 remaining rows. E.2 Proof of Lemma 7 In order to construct a network gc that implements the contextual mapping, we ﬁrst introduce two operations referred to as the sparse selective shift operation and all-max-shift operation, implemented by at most two (modiﬁed) sparse attention heads of head size 1. Then, we proceed to stack layers implementing the selective shift operations and all-max-shift operations, and prove that these layers map input H ∈Hδ to unique context ids. E.2.1 Preliminaries Sparse selective shift operation. Given any vector u ∈Rd, ﬁrst consider the following function implementable with a sparse attention head with head size 1 and sparsity pattern {Al k}k∈[n]. For k∈[n], the function ψl : Rd×n →R1×n computes each of its output column in the following way: ψl(Z; bQ)k := uTZAl k σH[(uTZAl k )T(uTZk −bQ)] = { maxj∈Al k uTZj if uTZk >bQ, minj∈Al k uTZj if uTZk <bQ. One can consider a sparse self-attention layer that consists of two such heads, with bQ <b′ Q: Ψl(Z; c,bQ,b′ Q) := Z + [ ce(1) −ce(1)][ψl(Z; bQ) ψl(Z; b′ Q) ] . 17The (1,k)-th entry of Ψl(Z; c,bQ,b′ Q) reads Ψl(Z; c,bQ,b′ Q)1,k = Z1,k + c(ψl(Z; bQ)k −ψl(Z; b′ Q)k) = { Z1,k + c(maxj∈Al k uTZj −minj∈Al k uTZj) if bQ <uTZk <b′ Q, Z1,k if uTZk /∈[bQ,b′ Q]. This means that for input columns Zk satisfying uTZk ∈(bQ,b′ Q) only, Ψl shifts up the ﬁrst entry of Zk by the difference of maximum and minimum values of uTZj over the sparsity pattern j ∈Al k, while leaving other columns intact. By choosing bQ and b′ Q properly, we can selectively modify certain columns without touching other columns; we refer to this operation Ψl as the sparse selective shift operation, and we will see later that this is indeed the key ingredient of our proof. In fact, this operation is a sparse version of the selective shift operation used in [ 33]. Since Al k is usually only a small subset of [n], one cannot calculate the maximum and minimum of uTZj over the whole sequence, as done in [33]. Instead, we use Assumption 1.2 and a more careful choice of E to get around the restriction posed by sparsity. All-max-shift operation. Suppose the input Z ∈Rd×n satisﬁes uTZ >0 entry-wise, for a vector u ∈Rd. Then, the all-max-shift operation Ωl : Rd×n →Rd×n is a sparse self-attention layer that consists of one attention head: Ωl(Z; c) = Z + ce(1)ψl(Z; 0). The (1,k)-th entry of Ωl(Z; c) reads Ωl(Z; c)1,k = Z1,k + cψl(Z; 0)k = Z1,k + cmax j∈Al k uTZj. So, for each column k, the all-max-shift operation shifts up the ﬁrst entry of Zk by the maximum value of uTZj over the sparsity pattern j ∈Al k. Unlike the selective shift operation, the all-max-shift operation is applied to all the columns. Column ids. Recall that the any input to this step is in Hδ := {G + E ∈Rd×n |G ∈Gδ := [0 : δ: 1 −δ]d×n}. Because of the way E is chosen according to the permutation γin Assumption 1.2, for any H ∈Hδ we have Hγ(1) ∈[n−1 : δ: n−δ]d, Hγ(i) ∈[i−2 : δ: i−1 −δ]d for all i∈[2 : n]. Now consider u := (1,δ−1,δ−2,...,δ −d+1). It is easy to check that for any H ∈Hδ, the map Hk ↦→uTHk is one-to-one, and uTHγ(1) ∈ [ (n−1) d−1∑ i=0 δ−i : δ: (n−1) d−1∑ i=0 δ−i + δ−d+1 −δ ] , uTHγ(i) ∈ [ (i−2) d−1∑ i=0 δ−i : δ: (i−2) d−1∑ i=0 δ−i + δ−d+1 −δ ] , for i∈[2 : n]. (7) Hence, for each column Hk, the inner product uTHk is in an interval disjoint from the other columns. Thus, uTHk can be thought as a “column id” that identiﬁes the column’s original input valueGk as well as its position k. Note furthermore that for any H ∈Hδ, uTHγ(2) <uTHγ(3) <··· <uTHγ(n) <uTHγ(1). (8) E.2.2 Construction of layers Given these preliminaries, we now describe our construction of gc. Recall from Assumption 1.2 that the permutation γsatisﬁes γ(i−1) ∈⋃p l=1 Al γ(i) for i∈[2 : n]. From this, for i∈[2 : n] we let 18li ∈[p] be any index such that γ(i−1) ∈Ali γ(i). For simplicity of notation, let zk := uTHk for k∈[n] and ∆ = ∑d−1 i=0 δ−i. Next, starting from i= 2, we want to sequentially stack δ−d sparse selective shift operations Ψli(·; δ−d,b −δ/2,b + δ/2), in increasing order of b∈ [ (i−2)∆ : δ: (i−2)∆ + δ−d+1 −δ ] . That is, we want to add sparse attention layers with sparsity patterns Ali γ(i) that apply the selective shift operation to each possible value of zγ(i). Recall that the sparsity patterns have to cycle from A1 k to Ap k, so we have to place other remaining p−1 sparsity patterns (whose indices are not li) in between the Ψli layers. This can be done by setting all the other sparse attention layers to be the identity. This way, we stack a total of pδ−d sparse attention layers for i= 2, another pδ−d for i= 3, and so on, up to i= n. After these layers, we further stack sall-max-shift operations. For i= 1,...,s , we add all-max-shift operations of the form Ω(i−1) mod p+1(·; 2snδ−nd−1). Here, the superscript (i−1) mod p+ 1 is there to make sure that we cycle through the sparsity patterns from 1 to p, until we stack slayers in total. This ﬁnishes the construction of our function gc composed of p(n−1) δd + ssparse self-attention layers. E.2.3 Selective shift operations We now explain how these stacked self-attention layers implement a contextual mapping. This subsection will consider the selective shift operations part; all-max-shift operations are described in the next subsection. Suppose that after the input H ∈Hδ is processed through the ﬁrst p(n−1) δd layers, we get ˜H ∈Rd×n at the output. We will show at the end of this subsection that the map H ↦→uT˜Hγ(n) is a one-to-one map for column γ(n), so the selective shift operations compute a “unique id” for each possible input sequenceH ∈Hδ. First selective shift. First consider the ﬁrst pδ−d layers. Omitting layers that are identity, they are essentially selective shift operations Ψl2 (·; δ−d,b −δ/2,b + δ/2) for b ∈[0 : δ : δ−d+1 −δ]. Since [0 : δ : δ−d+1 −δ] is the set of possible values of zγ(2), these layers perform selective shift operation on the γ(2)-th column without changing the other columns. Each possible value of Hγ(2) undergoes one and only shift operation (by the corresponding layer with b= uTHγ(2)), by which the (1,γ(2))-th entry of the input is updated. Recall by Assumption 1.2 thatγ(1) ∈Al2 γ(2), and that zγ(1) and zγ(2) are the maximum and minimum over the whole sequence z1,...,z n (see (8)). By Assumption 1.1 we also have γ(2) ∈Al2 γ(2). Since both γ(1) and γ(2) are in Al2 γ(2), the maximum and minimum value ofzj := uTHj’s overj ∈Al2 γ(2) are zγ(1) and zγ(2), respectively. Therefore, the (1,γ(2))-th entry of the input matrix is shifted up as follows: ˜H1,γ(2) := H1,γ(2) + δ−d(zγ(1) −zγ(2)). Let ˜Hγ(2) be the γ(2)-th column after the shift operation has shifted H1,γ(2) to ˜H1,γ(2). Then, deﬁne ˜zγ(2) := uT˜Hγ(2) = zγ(2) + δ−d(zγ(1) −zγ(2)). Note that ˜zγ(2) >zγ(1) because zγ(2) + δ−d(zγ(1) −zγ(2)) >zγ(1) ⇔(δ−d −1)(zγ(1) −zγ(2)) >0, which is true. Therefore, ˜zγ(2) becomes the new maximum among the current values zγ(1),˜zγ(2),zγ(3),...,z γ(n), and the new minimum element is zγ(3). Second selective shift. We now consider the nextpδ−d layers, which are essentially Ψl3 (·; δ−d,b− δ/2,b + δ/2) for b∈[∆ : δ: ∆ +δ−d+1 −δ]. They apply the shift operation to the γ(3)-th column. Since we have γ(2),γ(3) ∈Al3 γ(3), the shift operation similarly yields ˜zγ(3) := zγ(3) + δ−d(˜zγ(2) −zγ(3)) = zγ(3) + δ−d(zγ(2) −zγ(3)) + δ−2d(zγ(1) −zγ(2)). 19We can also show ˜zγ(3) >˜zγ(2), because zγ(3) + δ−d(˜zγ(2) −zγ(3)) >˜zγ(2) ⇔(δ−d −1)(˜zγ(2) −zγ(3)) >0. So after this operation ˜zγ(3) and zγ(4) are the new maximum and minimum over the updated sequence zγ(1),˜zγ(2),˜zγ(3),zγ(4),...,z γ(n). Repeating the process. The same process continues. The next pδ−d layers shifts the γ(4)-th columns and results in ˜zγ(4) which is greater than ˜zγ(3). After the ﬁrst p(n−1)δ−d layers, all columns except γ(1)-th column have been shifted, resulting in zγ(1),˜zγ(2),..., ˜zγ(n) satisfying (n−1)∆ ≤zγ(1) <˜zγ(2) <··· <˜zγ(n). (9) Let us denote the output of the p(n−1)δ−d-th layer as ˜H. Selective shifts implement a one-to-one map. Next, we prove that the map from H ∈Hδ to ˜zγ(n) := uT˜Hγ(n) = zγ(n) + n−1∑ i=1 δ−id(zγ(n−i) −zγ(n+1−i)) is one-to-one. Recall that for each column Hk, the map Hk ↦→ uTHk =: zk is one-to-one. Also, permutation of columns is one-to-one, which implies that it sufﬁces to show that the map[zγ(1) ... z γ(n) ] ↦→˜zγ(n) is one-to-one. Suppose we have two sequences [zγ(1) ... z γ(n) ] and [z′ γ(1) ... z ′ γ(n) ] that map to the same value of ˜zγ(n) = ˜z′ γ(n). Then, 0 = ˜zγ(n) −˜z′ γ(n) = zγ(n) −z′ γ(n) + n−1∑ i=1 δ−id(zγ(n−i) −zγ(n+1−i) −z′ γ(n−i) + z′ γ(n+1−i)). Suppose zγ(n) ̸= z′ γ(n). Since they both lie inside [(n−2)∆ : δ: (n−2)∆ + δ−d+1 −δ], we have −δ−d+1 + δ≤zγ(n) −z′ γ(n) ≤δ−d+1 −δ. Note that all the terms other than zγ(n) −z′ γ(n) are of “coarser resolution.” For example, the ﬁrst term δ−d(zγ(n−1) −zγ(n) −z′ γ(n−1) + z′ γ(n)) in the summation can only take values 0,δ−d+1,−δ−d+1,2δ−d+1,−2δ−d+1,... , so it can never cancel the difference zγ(n) −z′ γ(n) and make the sum ˜zγ(n) −˜z′ γ(n) zero. This implies that zγ(n) = z′ γ(n) must hold. Next, suppose zγ(n−1) ̸= z′ γ(n−1). Since we have zγ(n) = z′ γ(n), −δ−2d+1 <δ−d(zγ(n−1) −zγ(n) −z′ γ(n−1) + z′ γ(n)) = δ−d(zγ(n−1) −z′ γ(n−1)) <δ−2d+1. But similarly, any other terms in the summation have coarser resolution than δ−2d+1, so they cannot cancel the difference δ−d(zγ(n−1) −z′ γ(n−1)). Thus zγ(n−1) = z′ γ(n−1) must hold. Repeating the same argument up to γ(1) proves that the two sequences must be equal: [zγ(1) ... z γ(n) ] =[z′ γ(1) ... z ′ γ(n) ] . This proves that the map H ↦→˜zγ(n) is one-to-one and ˜zγ(n) can be seen as the unique id for the input sequence H ∈Hδ. E.2.4 All-max-shift operations Next, we explain the operation of the sall-max-shift layers. Recall from Assumption 1.3 that any token can attend to all the other tokens after ssteps, either directly or indirectly. Also recall from the last subsection that the input to the ﬁrst all-max-shift layer is ˜H, and the maximum entry of uT˜H is ˜zγ(n), the unique id for input H. From the statement of Lemma 7, the output after the s all-max-shift operations for input H is denoted as gc(H). In this subsection, we show that through s all-max-shift operations, the maximum ˜zγ(n) will propagate to all tokens and be a “dominant” term, which determines the interval that uTgc(H) lies in. As a result, we can show Properties 7.1 and 7.2 of gc at the end. 20Some preliminaries. Note that the unique id ˜zγ(n) has the following upper bound: ˜zγ(n) := zγ(n) + n−2∑ i=1 δ−id(zγ(n−i) −zγ(n+1−i)) + δ−(n−1)d(zγ(1) −zγ(2)) ≤zγ(n) + δ−d n−2∑ i=1 (zγ(n−i) −zγ(n+1−i)) + δ−(n−1)d(zγ(1) −zγ(2)) = zγ(n) + δ−d(zγ(2) −zγ(n)) + δ−(n−1)d(zγ(1) −zγ(2)) = δ−(n−1)dzγ(1) −(δ−(n−1)d −δ−d)zγ(2) −(δ−d −1)zγ(n) ≤δ−(n−1)dzγ(1) ≤δ−(n−1)d((n−1)∆ + δ−d+1 −δ) ≤δ−(n−1)d(n−1 + δ)(δ−d −1) ≤δ−nd −δ (10) where we used ∆ := ∑d−1 i=0 δ−i = δ−d−1 δ−1−1 ≤δ−d −1. A similar bound ˜zγ(i) ≤nδ−id −δ (11) also holds from a similar derivation. Next, recall from Assumption 1.3 the deﬁnitions S1 k := A1 k, St k := ⋃ j∈A(t−1) mod p+1 k St−1 j , and that there exists s≥1 such that, for all k∈[n], Ss k = [n]. Finally, the following inequality will be useful throughout: for any integer s≥1, (2s+ 1 2s ) ≤ (2s+ 1 2s )2 ≤···≤ (2s+ 1 2s )s ≤2. (12) Let us now describe the operation that the all-max-shift layers Ω(i−1) mod p+1(·; 2snδ−nd−1), i = 1,...,s , carry out. First all-max-shift. The input to the ﬁrst all-max-shift layer is ˜H. Let the output of the layer be M1. Recall that uT˜H consists of values zγ(1),˜zγ(2),..., ˜zγ(n), which are all strictly greater than 0 and strictly less than nδ−nd (by (10)). So, for each column k∈[n], the layer update reads M1 1,k := ˜H1,k + 2snδ−nd−1 max j∈A1 k uT˜Hj = ˜H1,k + 2snδ−nd−1uT˜Hj1 k , where j1 k := arg maxj∈A1 k uT˜Hj. After the update, uTM1 k is “dominated” by2snδ−nd−1uT˜Hj1 k , meaning that for any k,k′∈[n], uT˜Hj1 k <uT˜Hj1 k′ =⇒ uTMk <uTMk′. This is because the minimum gap between different values of uT˜Hj1 k is at least δ, and we have uT˜Hk <nδ−nd <2snδ−nd−1 ·δ, so if uT˜Hj1 k <uT˜Hj1 k′, that solely determines the order uTMk <uTMk′ because uT˜Hk cannot reverse it. Also, by the deﬁnition of j1 k, for any index set B∈ [n] we have max i∈B uT˜Hj1 i = max j∈⋃ i∈BA1 i uT˜Hj. (13) If s≥2, we move on to the second layer. Second all-max-shift. At the second all-max-shift, we have sparsity patterns A1 mod p+1 k . Let us the output of this layer as M2. For each column k∈[n], the layer update reads M2 1,k := M1 1,k + 2snδ−nd−1 max j∈A1 mod p+1 k uTM1 j = M1 1,k + 2snδ−nd−1uTM1 j2 k , 21where j2 k := arg maxj∈A1 mod p+1 k uTM1 j. If we look at the update more closely, we can apply (13) and get uTM2 k = uT˜Hk + 2snδ−nd−1uT˜Hj1 k + 2snδ−nd−1(uT˜Hj2 k + 2snδ−nd−1 max i∈A1 mod p+1 k uT˜Hj1 i ) = uT˜Hk + 2snδ−nd−1(uT˜Hj1 k + uT˜Hj2 k ) + (2snδ−nd−1)2 max j∈S2 k uT˜Hj. Again, the last term dominates the rest of the terms in uTM2 k, because the minimum gap between different values of maxj∈S2 k uT˜Hj is at least δ, and uTM2 k −(2snδ−nd−1)2 max j∈S2 k uT˜Hj = uT˜Hk + 2snδ−nd−1(uT˜Hj1 k + uT˜Hj2 k ) <(1 + 4snδ−nd−1)nδ−nd ≤(1 + 4s)n2δ−2nd−1 ≤(2snδ−nd−1)2 ·δ= 4s2n2δ−2nd−1. The last inequality holds due to inequality (12), because (2s+ 1 2s )2 ≤2 ⇔1 + 4s≤4s2 is true for s≥2. Remaining all-max-shifts. If s≥3, we move on to the third layer, which outputs M3. Similarly, we can show that uTM3 k is dominated by (2snδ−nd−1)3 maxj∈S3 k uT˜Hj because the rest of the terms in uTM3 k is strictly upper-bounded uTM3 k −(2snδ−nd−1)3 max j∈S3 k uT˜Hj <(1 + 3·2snδ−nd−1 + 3 ·(2snδ−nd−1)2)nδ−nd−1, which can then be shown to be smaller than (2snδ−nd−1)3 ·δ: (1 + 3·2snδ−nd−1 + 3·(2snδ−nd−1)2)nδ−nd ≤(1 + 6s+ 12s2)n3δ−3nd−2 ≤8s3n3δ−3nd−3 ·δ. The last inequality is due to the fact that 1 + 6s+ 12s2 ≤8s3 for s≥3, which can derived from (12). Repeating this process, after all slayers we get Ms, and uTMs k is dominated by (2snδ−nd−1)smax j∈Ss k uT˜Hj = (2snδ−nd−1)smax j∈[n] uT˜Hj = (2snδ−nd−1)s˜zγ(n). This is because the remaining terms in uTMs k can be strictly upper-bounded uTMs k −(2snδ−nd−1)s˜zγ(n) < (s−1∑ i=0 (s i ) (2snδ−nd−1)i ) nδ−nd, which is then dominated by the smallest difference possible in (2snδ−nd−1)s˜zγ(n): (s−1∑ i=0 (s i ) (2snδ−nd−1)i ) nδ−nd ≤ (s−1∑ i=0 (s i ) (2s)i ) (nδ−nd−1)s−1nδ−nd = ((1 + 2s)s −(2s)s)(nδ−nd−1)s ·δ≤(2snδ−nd−1)s ·δ. The last inequality used (1 + 2s)s −(2s)s ≤(2s)s, derived from (12). E.2.5 Verifying Properties 7.1 and 7.2 After these all-max-shift operations, we deﬁne the output Ms of the last all-max-shift layers to be the output of the function gc for input H, i.e., gc(H) := Ms. Property 7.1 requires that for any H ∈Hδ, all the components uTgc(H) need to be distinct. This is true, because for each column of uTgc(H), we have uTgc(H)k mod 2snδ−nd = uT˜Hk. 22This is because anything added by the all-max-shift operations is an integer multiple of 2snδ−nd, and uT˜Hk <nδ −nd <2nδ−nd for all k. Recall that ˜H is the input matrix for the ﬁrst max-shift operation, and that the components of uT˜H are zγ(1),˜zγ(2),..., ˜zγ(n), which were shown to be distinct by (9). Since uTgc(H)k produce distinct outputs for a mod operation, they themselves have to distinct. This proves Property 7.1. Also, by the “domination” argument in the previous subsection, the outputgc(H) has the property that for any column, uTgc(H)k lies inside an interval determined by ˜zγ(n), the unique id for the input H: uTgc(H)k ∈ [ (2snδ−nd−1)s˜zγ(n),(2snδ−nd−1)s(˜zγ(n) + δ) ) , and these intervals do not overlap because any different values of ˜zγ(n) must differ by at least δ. This means that for any input H,H′∈Hδ, the components in uTgc(H) and uTgc(H′) lie in disjoint intervals. Together with Property 7.1, this proves Property 7.2. E.3 Proof of Lemma 8 To prove this lemma, we implement a token-wise function that maps gtkn v (gc(H)k) = f(H −E)k, for all H ∈Hδ and k ∈[n]. From the construction of Lemma 7, there are n|Hδ|= n δdn distinct values of uTgc(H)k, and different values of uTgc(H)k differ by at least δ. The implementation of gtkn v can be done by stacking feed-forward layers so that each layer maps one unique number to the corresponding output column. More precisely, choose any H ∈Hδ. For each of the nvalues of uTgc(H)k, we add one feed- forward layer of the form Z ↦→Z+(f(H−E)k−gc(H)k)φ′(uTZ−uTgc(H)k1T n), φ′(t) = {0 t< −δ/2 or t≥δ/2, 1 −δ/2 ≤t<δ/ 2. This layer updates any column j of its input Z that satisﬁes uTgc(H)k −δ/2 ≤ uTZj < uTgc(H)k + δ/2, without modifying any other columns that are out of this range. We stack these layers for all possible values ofH ∈Hδ. After n δdn such layers, we get the desired function gv that satisﬁes gv(Z) = [ gtkn v (Z1) ··· gtkn v (Zn) ] , where for all H ∈Hδ and k∈[n], gtkn v (gc(H)k) = f(H −E)k. F Proof of Lemma 4 (Step 3 in § 4.1) In this section, we describe how the modiﬁed sparse Transformer networkg∈ST 2,1,1 constructed in Lemma 3 can be approximated with an original sparse Transformer network g∈ST 2,1,4. Recall that gis a “modiﬁed” sparse Transformer network, which employ the hardmax σH operators in place of ρ operators in sparse self-attention layers and piecewise linear activations φ∈Φ instead of ReLUs in feed-forward layers. The goal of this lemma is to approximate the functiong= gv ◦gc ◦gq ∈ST 2,1,1 with a standard sparse Transformer g = ˜gv ◦˜gc ◦˜gq ∈ST 2,1,4 with accuracy dp(g,g) ≤ϵ/2. As the construction of gconsists of three steps, we will approximate each of them step by step. The whole intuition behind the proof is that as long as we are considering Lp approximation, we can approximate σH and φ∈Φ as closely as we want with ρand ReLUs, respectively. However, as the proof will show, controlling the aggregated error over layers is not a trivial job. F.1 Approximating the quantization function gq (Lemma 6) We ﬁrst consider approximating gq from Lemma 6 with a standard feed-forward layer counterpart, ˜gq. Recall from § E.1 that the modiﬁed feed-forward layers used in gq are of the form Z ↦→Z + e(i)φ((e(i))TZ −kδ1T n), φ(t) = {0 t< 0 or t≥δ, −t 0 ≤t<δ, (14) 23for i∈[d] and k ∈[0 : n/δ−1]. Note that the activation φ∈Φ can be closely approximated by three ReLUs: ˜φα(t) := −ReLU(t) + 1 αReLU(t−(1 −α)δ) −1 −α α ReLU(t−δ) =    0 t≤0 or t≥δ, −t 0 ≤t≤(1 −α)δ, 1−α α (t−δ) (1 −α)δ≤t≤δ, where 0 < α <1. Note that ˜φα(t) = φ(t) except for an interval ((1 −α)δ,δ), and by shrinking α >0 this interval can be made arbitrarily small. Consider approximating the layers (14) with standard feed-forward layers, by replacing φwith its approximation ˜φα. Let the resulting function be ˜gq ∈ST 2,1,3. Then, it is easy to check that gq(X + E) = ˜gq(X + E) holds if all coordinates of X ∈[0,1)d×n are in the intervals of the form [kδ,(k+ 1 −α)δ] for some k ∈[0 : n/δ−1]; i.e., the intervals in which ˜φα perfectly approximates φ. The Lebesgue measure of the set of such inputs X is ((1 −α)δ)nd × 1 δnd = (1 −α)nd, and this can be made arbitrarily close to 1 by makingαsmall. As a result, “most” of the inputX ∈D satisﬁes gq(X + E) = ˜gq(X + E) ∈Hδ, while a small fraction (of measure at most 1 −(1 −α)nd) can map to some other values. For most of the remaining of the proof, we will consider the fraction of inputs mapped correctly to Hδ and bound their approximation error. We will come back to the 1 −(1 −α)nd fraction at the end of the proof. F.2 Approximating the contextual mapping gc (Lemma 7) Let us now consider approximating the contextual mapping gc in Lemma 7, constructed using the hardmax σH operators, with the standard sparse self-attention layers employing ρoperator. We will call the approximation ˜gc. Recall that ρsatisﬁes Assumption 2: Assumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying vj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑ j̸=j∗ρ[tv]j ≤η. This means that ρcan closely approximate σH in the sense that whenever the input vector v to the ρoperator has a maximum element vj∗ by some margin ζ, then the j∗-th component of the output ρ[tv] is close to 1, while the other components of ρ[tv] are close to 0. Recall that gc consists of two parts. The ﬁrst part is a composition of sparse selective shift operations, and the second is a composition of all-max-shift operations. We will ﬁrst examine how “errors” are introduced when σH is replaced with ρin both operations, discuss how the errors accumulate, and show how to choose the right ζand ηto control the errors in the approximation ˜gc. Errors introduced by ρ: Sparse selective shift operation. Recall that the key component in both the selective shift operation and all-max-shift operation is the sparse attention head ψl(·), which computes its k-th column as the following: ψl(Z; bQ)k := uTZAl k σH[(uTZAl k )T(uTZk −bQ)] = { maxj∈Al k uTZj if uTZk >bQ, minj∈Al k uTZj if uTZk <bQ. Now suppose we replaced σH with ρsatisfying Assumption 2. Suppose each entry in uTZ differs at least by δ, which is true in the construction of gc. We choose ζ = δ/2 and some 0 <η <1, and corresponding t> 0. Then, replace σH[·] with ρ[t·] and deﬁne ˜ψl(Z; bQ)k := uTZAl k ρ[t(uTZAl k )T(uTZk −bQ)]. If uTZk >bQ, it is easy to check that ˜ψl(Z; bQ)k satisﬁes (1 −η) max j∈Al k uTZj + η min j∈Al k uTZj ≤˜ψl(Z; bQ)k ≤max j∈Al k uTZj. (15) 24Similarly, if uTZk <bQ, we have min j∈Al k uTZj ≤˜ψl(Z; bQ)k ≤(1 −η) min j∈Al k uTZj + ηmax j∈Al k uTZj. Now consider the approximate sparse selective shift operator ˜Ψl, implemented with ˜ψl. For bQ <b′ Q, we deﬁne ˜Ψl(Z; c,bQ,b′ Q) := Z + [ ce(1) −ce(1)] [ ˜ψl(Z; bQ) ˜ψl(Z; b′ Q) ] . For any column Zk satisfying bQ <uTZk <b′ Q, we have (1 −2η) ( max j∈Al k uTZj −min j∈Al k uTZj ) ≤˜ψl(Z; bQ)k −˜ψl(Z; b′ Q)k ≤max j∈Al k uTZj −min j∈Al k uTZj, and for any column Zk satisfying uTZk /∈[bQ,b′ Q], we get |˜ψl(Z; bQ)k −˜ψl(Z; b′ Q)k|≤ η ( max j∈Al k uTZj −min j∈Al k uTZj ) . Recall that for the hardmax σH version, we had ψl(Z; bQ)k −ψl(Z; b′ Q)k = { maxj∈Al k uTZj −minj∈Al k uTZj if bQ <uTZk <b′ Q, 0 if uTZk /∈[bQ,b′ Q]. From this observation, the approximation error ˜Ψl−Ψl of the selective shift operator on the (j,k)-th entry of the output can be bounded as follows: ˜Ψl(Z; c,bQ,b′ Q)j,k −Ψl(Z; c,bQ,b′ Q)j,k ∈    [−2cηDl k,0] if j = 1,uTZk ∈(bQ,b′ Q), [−cηDl k,cηDl k] if j = 1,uTZk /∈[bQ,b′ Q], {0} if j ̸= 1, where we used Dl k := maxj∈Al k uTZj −minj∈Al k uTZj for simplicity. Errors introduced by ρ: All-max-shift operation. Next, we examine the approximation error of the all-max-shift operation introduced by replacement of σH with ρ. Let us deﬁne the approximate all-max-shift operation ˜Ωl: ˜Ωl(Z; c) = Z + ce(1) ˜ψl(Z; 0). From (15), we can check that the approximation error ˜Ωl −Ωl of the all-max-shift operation is bounded as ˜Ωl(Z; c)j,k −Ωl(Z; c)j,k ∈ {[−cηDl k,0] if j = 1, {0} if j ̸= 1. Errors in selective shift operations. Given these approximation error bounds of single operations, we now analyze the accumulation of errors through multiple layers. We ﬁrst consider the ﬁrst pδ−d self-attention layers in gc. Recall that they consist of selective shift layersΨl2 (·; δ−d,b−δ/2,b+δ/2) for b∈[0 : δ: δ−d+1 −δ] and (p−1)δ−d identity layers. A natural way to approximate these layers with standard self-attention layers is to use approximate layers ˜Ψl2 (·; δ−d,b −δ/2,b + δ/2), with sufﬁciently large t> 0. As we have seen above, there is no error introduced by ρexcept for the ﬁrst row. Thus, we will analyze the approximation error of ˜Ψl2 (·; δ−d,b −δ/2,b + δ/2) for the ﬁrst row only. Let us remind the readers how the ﬁrst selective shift operation (done by the ﬁrst pδ−d layers) originally worked in gc. The input to gc is H, and we deﬁne zk := uTHk and ∆ = ∑d−1 i=0 δ−i. Recall from Eqs. (7) and (8) in § E.2 that 0 ≤zγ(2) <zγ(3) <··· <zγ(n) <zγ(1) ≤(n−1)∆ + δ−d+1 −δ <nδ−d 25and zγ(2) ∈[0 : δ: δ−d+1 −δ], so zγ(2) will undergo the selective shift by one of the self-attention layers, which updates the (1,γ(2))-th entry of the input. Let ˜Hγ(2) be the updated value of the column and ˜zγ(2) := uT˜Hγ(2). The new sequence satisﬁes ∆ ≤zγ(3) <··· <zγ(n) <zγ(1) <˜zγ(2) <nδ−2d, where the strict upper bound on ˜zγ(2) is from Eq. (11). In case of the approximation ˜Ψl2 , we have seen that the error depends on the gap between maximum and minimum of uTZj’s, and this gap may grow larger as error accumulates; in the worst case, it may grow exponentially. To see this, suppose a0 and b0 are the maximum and minimum value of uTZj’s, and they go through a selective shift operation, but they do not belong to the range of the operation (bQ,b′ Q). Then, a0 and b0 will be updated to a1 and b1, which are bounded by a1 ≤a0 + δ−dη(a0 −b0), b1 ≥b0 −δ−dη(a0 −b0). After the next layer, we get a2 ≤a1 + δ−dη(a1 −b1) ≤a0 + δ−dη(a0 −b0) + δ−dη(1 + 2δ−dη)(a0 −b0), b2 ≥b1 −δ−dη(a1 −b1) ≥b0 −δ−dη(a0 −b0) −δ−dη(1 + 2δ−dη)(a0 −b0). Similarly, after ksuch layers, we get ak ≤a0 + (a0 −b0)δ−dη k−1∑ i=0 (1 + 2δ−dη)i, bk ≥b0 −(a0 −b0)δ−dη k−1∑ i=0 (1 + 2δ−dη)i, showing that the gap ak −bk may grow exponentially in the worst case: ak −bk ≤(1 + 2δ−dη)k(a0 −b0). In the error-less case (σH), for any input sequence H, the maximum possible difference between maximum and minimum of uTH is bounded above by nδ−d, and after one selective shift operation was done on the γ(2)-th column, the difference is then bounded by nδ−2d. Therefore, the worst-case possible error introduced by ρ is bounded above by the sum of the worst-case errors calculated assuming that we started off with max-min difference nδ−2d. Using this observation, the error on each ﬁrst-row entry of the sequence after the ﬁrst pδ−d layers is bounded above by 2nδ−2d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i, (16) where a factor of 2 is introduced because when the selective shift operation is applied to the γ(2)-th column, it may introduce an error which is twice the magnitude of the error introduced to the other columns. We want to make (16) smaller than δ 8n. By Assumption 2, we can always choose t> 0 that satisﬁes the assumption for ζ = δ 2, and η= 1 2 δ2dlog ( 1 + δ2d˜δ 8n2 ) >0, where ˜δ:= min { δ,21−1/pϵ n1/p } . Using such t, we can control the total accumulated error by the ﬁrst pδ−d selective shift operations below ˜δ 8n: 2nδ−2d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i ≤2nδ−3dη(1 + 2δ−dη)δ−d −1 (1 + 2δ−dη) −1 = nδ−2d    1 + log ( 1 + δ2d˜δ 8n2 ) δ−d   δ−d −1  ≤nδ−2d ( exp log ( 1 + δ2d˜δ 8n2 ) −1 ) = nδ−2dδ2d˜δ 8n2 = ˜δ 8n. 26Therefore, after the ﬁrst pδ−d selective shift layers, the accumulated error for each entry of the ﬁrst row is at most ˜δ/8n. We can also apply similar arguments to the remaining selective shift layers. For example, for the j-th set of pδ−d selective shift layers where the operation is done on γ(j+ 1)-th column of the input, the gap between the maximum and the minimum, including the accumulated error from previous layers, is bounded above by nδ−(j+1)d. Therefore, for this set of layers, the maximum accumulated error is bounded by 2nδ−(j+1)d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i. So, choosing t> 0 that satisﬁes Assumption 2 for η = δ 2 and η = 1 2 δ2dlog(1 + δ(j+1)d˜δ 8n2 ), we can control the accumulated error introduced by the pδ−d layers below δ 8n: 2nδ−(j+1)d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i ≤2nδ−(j+2)dη(1 + 2δ−dη)δ−d −1 (1 + 2δ−dη) −1 ≤nδ−(j+1)d    1 + log ( 1 + δ(j+1)d˜δ 8n2 ) δ−d   δ−d −1  ≤ ˜δ 8n. In total, the accumulated error by the ﬁrst p(n−1)/δd layers, which correspond to the selective shift operation part of the construction, is at most (n−1)˜δ 8n ≤ ˜δ 8 . Errors in all-max-shift operations. For all-max-shift operations, we approximate the hardmax σH all-max-shift operations Ωl(Z; nδ−nd) with its ρ-counterparts, ˜Ωl(Z; nδ−nd). We can similarly bound the accumulated error in the all-max-shift operations. Recall from § E.2 that during the whole series of all-max-shift operations, the maximum entry in the sequence is upper-bounded by (2snδ−nd−1)snδ−nd and minimum entry is lower-bounded by(n−1)∆. Therefore, the gap between the max and min elements, taking into consideration the errors from selective shift operations, is bounded from above by (2snδ−nd−1)snδ−nd. Then, using a similar argument as the select shift operation layers, the maximum error is bounded above by (2snδ−nd−1)snδ−nd ·nδ−ndη s−1∑ i=0 (1 + nδ−ndη)i, and we want to make it smaller than ˜δ 8 . By Assumption 2, we can always choose t> 0 that satisﬁes the assumption for ζ = δ 2, and η= δnd sn log ( 1 + δs(nd+1)+nd˜δ 2s+3ssns+1 ) >0. Using such t, we can control the total accumulated error by the ﬁrst pδ−d selective shift operations below ˜δ 8 : (2snδ−nd−1)snδ−nd ·nδ−ndη s−1∑ i=0 (1 + nδ−ndη)i ≤(2snδ−nd−1)snδ−nd ·nδ−ndη(1 + nδ−ndη)s −1 (1 + nδ−ndη) −1 = (2snδ−nd−1)snδ−nd    1 + log ( 1 + δs(nd+1)+nd˜δ 2s+3ssns+1 ) s   s −1   ≤(2snδ−nd−1)snδ−ndδs(nd+1)+nd˜δ 2s+3ssns+1 = ˜δ 8. 27So far, we have analyzed the total accumulated error of approximating the contextual mapping function gc (constructed with hardmax σH) with an approximation ˜gc (constructed with ρ). We have seen that for any input H ∈Hδ, the approximation error can be controlled so that the error by the selective shift operation part is at most ˜δ/8 and the all-max-shift operation part is at most ˜δ/8. Therefore, the total error of the (j,k)-th entry can be bounded as ˜gc(H)j,k −gc(H)j,k ∈ { [− ˜δ 4 , ˜δ 4 ] j = 1, {0} j ̸= 1, for any H ∈Hδ. F.3 Approximating the value mapping gv (Lemma 8) We now consider the approximation of the value mappinggv with standard feed-forward layers. In gv, we implemented the function with layers of the form Z ↦→Z+(f(H−E)k−gc(H)k)φ′(uTZ−uTgc(H)k1T n), φ′(t) = {0 t< −δ/2 or t≥δ/2, 1 −δ/2 ≤t<δ/ 2. Since the output of contextual mapping gc(H) and its approximation ˜gc(H) differ in only the ﬁrst row and by ˜δ/4 ≤δ/4, one can approximate each layer in gv by replacing φ′with an approximation ˜φ′, implementable with four ReLU’s: ˜φ′(t) =    0 t< −δ/2 or t≥δ/2, 4 δt+ 2 −δ/2 ≤t< −δ/4, 1 −δ/4 ≤t<δ/ 4, −4 δt+ 2 δ/4 ≤t<δ/ 2. Let ˜gv be the approximation of gv constructed this way. Because the error on ˜gc is bounded by ˜δ/4, the error on the ﬁnal output ˜gv is also bounded by ˜δ/4. That is, for any H ∈Hδ, ˜gv(˜gc(H))j,k −gv(gc(H))j,k ∈ { [− ˜δ 4 , ˜δ 4 ] j = 1, {0} j ̸= 1. Hence, using ˜δ:= min { δ,21−1/pϵ n1/p } , we have ∥˜gv(˜gc(H)) −gv(gc(H))∥p p ≤n (˜δ 4 )p ≤1 2 (ϵ 2 )p , for all H ∈Hδ. F.4 Finishing the proof Recall from § F.1 that the approximated quantization function ˜gq maps most of the input X ∈D to H ∈Hδ, and a small fraction of them (of measure at most 1 −(1 −α)nd) to something else. Note now that the original function g = gv ◦gc ◦gq and the approximation g = ˜gv ◦˜gc ◦˜gq are both bounded, so there is a global constant Bsuch chat ∥g(X + E) −g(X + E)∥p ≤Bfor all X ∈D. We can divide the integral overD to two disjoint sets. The ﬁrst one D1 := {X ∈D |˜gq(X + E) ∈ Hδ}is the set of input X mapped to Hδ by ˜gq, and the other is its complement D2 = D \\D1. dp(g,g)p := ∫ D ∥g(X + E) −g(X + E)∥p pdX = ∫ D1 ∥g(X + E) −g(X + E)∥p pdX + ∫ D2 ∥g(X + E) −g(X + E)∥p pdX ≤1 2 (ϵ 2 )p + (1 −(1 −α)nd)Bp. One can make α close enough to 1 so that the second term is less than 1 2 (ϵ 2 )p . This makes dp(g,g) ≤ϵ/2, hence ﬁnishing the proof. 28G Experimental setup G.1 Copying task We generated the synthetic dataset for the copying task. The input sequence to the copying task has the format 0s0s, where s is a 127 length sequence of symbols randomly sampled from the range of [0,127]. The training set contains 100K sequences, while the testing set contains 10K sequences. We implement the copying task as a masked-LM [10] style prediction task by masking all the tokens in the second half of the sequence. For the test examples, each masked token is predicted independently. For the results reported in § 5, we experiment with bidirectional models, where each token can attend to both previous and future tokens. The maximum sequence length is n= 256, and we use embedding dimension d= 256. The model has 1 to 4 attention layers with h= 4 attention heads of size m= 64, followed by a feed-forward hidden layer of size r= 512. We train the model with the AdamW optimizer with weight decay and no dropout. We train the model using 3,000 warmup steps and a total of 500K training steps. The learning rate is 1e−4. We use the batch size 1,024 on 8 TPUv3 chips. For all sparsity patterns other than the RANDOM pattern, we choose the segment length wto be 16 for all patterns. This segment length results in the sparsest level for the STRIDED and FIXED patterns. In Table 1, we include the sparsity level as a reference. For this task, we report the prediction accuracy for all the tokens. G.2 Language modeling For the language modeling task, we train on the One Billion Word Benchmark [5] which contains almost one billion tokens and a vocabulary of more than 800K tokens. We use the Transformer model in the Tensor2Tensor framework [29]. We use a 12-block (cf. (2)) Transformer, with embedding dimension d = 256, maximum sequence length n = 256, number of heads h= 8, head size m= 64, and feed-forward hidden layer size r = 1024. Since language modeling task is auto-regressive (attending to only past tokens) in nature, we evaluate the (sparse) attention score matrices and mask them to be an upper-triangular matrix. We train the model with the Adafactor with weight decay. We train the model using 10K warmup steps and a total of 240K steps. We use the batch size 4,096 on 8 TPUv2 chips. For this task, we report the perplexity. G.3 Translation For the translation task, we train on the WMT18 en-cs datasets (Europarl v7, Common Crawl corpus, News Commentary v13, and CzEng), with a total of 15M pairs of sentences, and test on the newstest2015 en-cs dataset, with 2,656 pairs. We use the encoder-decoder architecture and apply the sparse attention on both encoder and decoder. We use the Transformer model in the Tensor2Tensor framework [ 29] and the same setup as the language modeling task, except for having 6 blocks in the Transformer networks, with head size m= 32 and having autoregressive patterns only in decoders. For this task, we report the cased BLEU score. G.4 GLUE tasks For the GLUE tasks, we use the pre-training and ﬁne-tuning framework [10]. Following Devlin et al. [10] we ﬁrst pre-train a BERTBASE model for 450K steps on the BooksCorpus [36] (800M words) and the English Wikipedia datasets (2,500M words). We later ﬁnetune the model on data from each task separately. For each setting, we use the same sparsity pattern and head conﬁguration in both the pre-training and the ﬁne-tuning stages. The sequence length is n= 128 in both stages. We report the average accuracy of three runs on the dev set for all tasks. For each setting, we pre-train a model and run ﬁne-tuning three times. 29Table 2. Accuracy on the synthetic copying task when using an auto-regressive model. Percentages in parentheses mark the sparsity levels. STRIDED FIXED STAR RANDOM Depth UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) (87%) (90%) 1-layer 0.79% 0.78% 0.78% 7.02% 7.04% 0.81% 0.77% 33.13% 2-layer 12.40% 8.26% 1.57% 73.43% 13.24% 92.10% 12.32% 67.30% 3-layer 94.50% 65.58% 60.88% 99.87% 70.82% 99.84% 14.03% 89.50% 4-layer 100% 100% 98.40% 99.97% 99.16% 99.97% 31.19% 95.88% (a) WMT en-de  (b) WMT de-en Figure 3. Comparison of sparsity patterns and different head conﬁgurations on the WMT de-en and en-de translation tasks. (a) CoLA  (b) MRPC Figure 4. Comparison of sparsity patterns and different head conﬁgurations on the CoLA and MRPC tasks for the BERTBASE model. H Additional experimental results We report additional experimental results in this section. H.1 Copying task We include the results for the copying task using auto-regressive (unidirectional) models as in LM, where each token can only attend to previous tokens, in Table 2. In this case, the STAR pattern cannot attend to the last replay token. Indeed, the STAR pattern shows better performance when the model is bidirectional (cf. Table 1). H.2 Translation We present experimental results of the translation tasks on the WMT English-German and German- English datasets in Figure 3. We train on WMT18 (Europarl v7, Common Crawl corpus and News Commentary v13) and test on newstest 2015 datasets. The ﬁgures show similar trends to the results on the WMT en-cs dataset in Figure 1b. 30H.3 GLUE tasks Figure 4 presents the results comparing the sparsity patterns and the head conﬁgurations on the CoLA and MRPC tasks using the BERTBASE model. CoLA is a single-sentence classiﬁcation task, asking if a sentence is a grammatical English sentence. MRPC is a sentence-pair classiﬁcation task, where each example is a pair of sentences and the label indicates whether the sentences are semantically equivalent. 31",
      "meta_data": {
        "arxiv_id": "2006.04862v2",
        "authors": [
          "Chulhee Yun",
          "Yin-Wen Chang",
          "Srinadh Bhojanapalli",
          "Ankit Singh Rawat",
          "Sashank J. Reddi",
          "Sanjiv Kumar"
        ],
        "published_date": "2020-06-08T18:30:12Z",
        "pdf_url": "https://arxiv.org/pdf/2006.04862v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the quadratic computational cost of Transformer networks by investigating sparse Transformers. It proposes a unifying framework for existing sparse attention models and establishes sufficient conditions under which sparse Transformers with O(n) connections per attention layer can universally approximate any continuous sequence-to-sequence function, similar to dense models with n^2 connections. This provides the first formal evidence that well-designed sparse attention layers do not limit Transformer's universal approximation power. Additionally, the paper presents experimental comparisons of different sparsity patterns and levels on standard NLP tasks.",
        "methodology": "The authors propose a unifying framework that modifies the standard Transformer block to a sparse Transformer block (STB). This framework incorporates cycling sparsity patterns (p different patterns applied in sequence), explicit sparsity patterns (tokens attend only to a subset Al_k), and generalized probability maps (ρ, extending softmax to sparse variants like sparselin-gen or α-entmax). The core theoretical contribution is a universal approximation theorem (Theorem 1), which relies on two key assumptions: Assumption 1 defines conditions on sparsity patterns (self-attention, Hamiltonian path connectivity, and reachability of all tokens in 's' hops), and Assumption 2 defines conditions on the probability map (approximating hardmax by input scaling). The proof involves three steps: approximating continuous functions with piecewise constant functions, representing these with a modified sparse Transformer (using hardmax and piecewise linear activations) through carefully constructed positional embeddings, quantization, contextual mapping (utilizing 'sparse selective shift' and 'all-max-shift' operations), and value mapping, and finally approximating the modified Transformer with a standard sparse Transformer while controlling error accumulation.",
        "experimental_setup": "Experiments were conducted on four tasks: a synthetic copying task, language modeling (One Billion Word Benchmark), machine translation (WMT18 English-Czech, English-German, German-English datasets), and GLUE tasks (MNLI, XNLI, CoLA, MRPC) using a BERTBASE model. Four sparsity patterns were compared: STRIDED, FIXED, STAR, and RANDOM. For STRIDED and FIXED patterns, three head configurations were tested: SEQUENTIAL (alternating patterns), UNION (combined pattern), and MULTIHEAD (different heads use different patterns). Maximum sequence length was 256 (128 for GLUE tasks). Model architectures varied based on tasks, using 1-4 attention layers for copying, 12-block Transformer for LM, 6-block Transformer for translation, and BERTBASE for GLUE. Validation metrics included prediction accuracy for copying and GLUE, perplexity for language modeling, and BLEU score for translation.",
        "limitations": "The universal approximation theorem views the sequence length (n) as a fixed constant, meaning the results do not apply to scenarios with varying sequence lengths. The analysis of universal approximation is primarily applicable to the encoder part of the Transformer network. Experimentally, the paper observed that the design of optimal sparsity patterns is heavily dependent on specific tasks, as different patterns (e.g., STAR, UNION configurations) performed best on different tasks but poorly on others. Random sparsity patterns consistently yielded worse performance, highlighting the need for careful design.",
        "future_research_directions": "The authors hope their work will shed light on the understanding of sparsity in attention layers and provide guidance for the design of more efficient and faster sparse attention models. Implicitly, based on the identified limitations and experimental observations, future research could explore methods for designing more generalizable or adaptive sparsity patterns that perform robustly across various tasks. Further theoretical analysis could also investigate universal approximation for varying sequence lengths and extend the analysis to the decoder components of Transformers."
      }
    },
    {
      "title": "Robust Graph Representation Learning via Neural Sparsification"
    },
    {
      "title": "Even Sparser Graph Transformers",
      "abstract": "Graph Transformers excel in long-range dependency modeling, but generally\nrequire quadratic memory complexity in the number of nodes in an input graph,\nand hence have trouble scaling to large graphs. Sparse attention variants such\nas Exphormer can help, but may require high-degree augmentations to the input\ngraph for good performance, and do not attempt to sparsify an already-dense\ninput graph. As the learned attention mechanisms tend to use few of these\nedges, such high-degree connections may be unnecessary. We show (empirically\nand with theoretical backing) that attention scores on graphs are usually quite\nconsistent across network widths, and use this observation to propose a\ntwo-stage procedure, which we call Spexphormer: first, train a narrow network\non the full augmented graph. Next, use only the active connections to train a\nwider network on a much sparser graph. We establish theoretical conditions when\na narrow network's attention scores can match those of a wide network, and show\nthat Spexphormer achieves good performance with drastically reduced memory\nrequirements on various graph datasets.",
      "full_text": "Even Sparser Graph Transformers Hamed Shirzad University of British Columbia shirzad@cs.ubc.ca Honghao Lin Carnegie Mellon University honghaol@andrew.cmu.edu Balaji Venkatachalam Meta∗ bave@meta.com Ameya Velingker Independent Researcher∗ ameyav@gmail.com David P. Woodruff CMU & Google Research dwoodruf@cs.cmu.edu Danica J. Sutherland UBC & Amii dsuth@cs.ubc.ca Abstract Graph Transformers excel in long-range dependency modeling, but generally require quadratic memory complexity in the number of nodes in an input graph, and hence have trouble scaling to large graphs. Sparse attention variants such as Exphormer can help, but may require high-degree augmentations to the input graph for good performance, and do not attempt to sparsify an already-dense input graph. As the learned attention mechanisms tend to use few of these edges, such high- degree connections may be unnecessary. We show (empirically and with theoretical backing) that attention scores on graphs are usually quite consistent across network widths, and use this observation to propose a two-stage procedure, which we call Spexphormer: first, train a narrow network on the full augmented graph. Next, use only the active connections to train a wider network on a much sparser graph. We establish theoretical conditions when a narrow network’s attention scores can match those of a wide network, and show that Spexphormer achieves good performance with drastically reduced memory requirements on various graph datasets. Code can be found at https://github.com/hamed1375/Sp_Exphormer. 1 Introduction The predominant story of the last half-decade of machine learning has been the runaway success of Transformer models (Vaswani et al., 2017), across domains from natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Zaheer et al., 2020) to computer vision (Dosovitskiy et al., 2020) and, more recently, geometric deep learning (Dwivedi and Bresson, 2020; Kreuzer et al., 2021; Ying et al., 2021; Rampášek et al., 2022; Shirzad et al., 2023; Müller et al., 2023). Conventional (“full”) Transformers, however, have a time and memory complexity of O(nd2 + n2d), where n is the number of entities (nodes, in the case of graphs), and d is the width of the network. Many attempts have been made to make Transformers more efficient (see Tay et al. (2020) for a survey on efficient variants for sequence modeling). One major line of work involves sparsifying the attention mechanism, constraining attention from all O(n2) pairs to some smaller set of connections. For instance, for sequential data, BigBird (Zaheer et al., 2020) constructs a sparse attention mechanism by combining sliding windows, Erd˝os-Rényi auxiliary graphs, and universal connectors. Similarly, for graph data, Exphormer (Shirzad et al., 2023) constructs a sparse interaction graph consisting of edges from the input graph, an overlay expander graph, and universal connections. We refer to such a network as a sparse attention network. Exphormer reduces each layer’s complexity from O(nd2 + n2d) to O((m + n)d2), where n is the number of nodes, m is the number of interaction edges in the sparse attention mechanism, and d is ∗Work done in part while at Google. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2411.16278v1  [cs.LG]  25 Nov 2024the hidden dimension or width. Even so, training is still very memory-intensive for medium to large scale graphs. Also, for densely-connected input graphs with Θ(n2) edges, there is no asymptotic improvement in complexity, as Exphormer uses all of theΘ(n2) edges of the original input graph. Our goal is to scale efficient graph Transformers, such as Exphormer, to even larger graphs. One general approach for scaling models to larger graphs is based on batching techniques. Prominent approaches include egocentric subgraphs and random node subsets (Wu et al., 2022, 2023, 2024). Egocentric subgraphs choose a node and include all of its k-hop neighbors; the expander graphs used in Exphormer, however, are exactly defined so that the size of these subgraphs grows exponentially in the number of layers – prohibitively expensive for larger graphs. A similar issue arises with universally-connected nodes, whose representation depends on all other nodes. For uniformly- random subset batching, as the number b of batches into which the graph is divided grows, each edge has chance 1 b to appear in a given step. Thus, b cannot be very large without dropping important edges. A similar problem can happen in random neighbor sampling methods such as GraphSAGE (Hamilton et al., 2017). Although this model works well on message-passing neural networks (MPNNs) which only use the graph edges, using it for expander-augmented graphs will select only a small ratio of the expander edges, thereby breaking the universality properties provided by the expander graph. Expander graphs enable global information propagation, and when created with Hamiltonian cycles and self-loops, produce a model that can provably approximate a full Transformer (Shirzad et al., 2023, Theorem E.3). Yet not all of these edges turn out to be important in practice: we expect some neighboring nodes in the updated graph to have more of an effect on a given node than others. Thus, removing low-impact neighbors can improve the scalability of the model. The challenge is to identify low-impact edges without needing to train the (too-expensive) full model. Figure 1 illustrates other advantages of this batching approach; this is also discussed further in Appendix G. One approach is to train a smaller network to identify which edges are significant. It is not obvious a priori that attention scores learned from the smaller network will estimate those in the larger network, but we present an experimental study verifying that attention scores are surprisingly consistent as the network size reduces. We also give theoretical indications that narrow networks are capable of expressing the same attention scores as wider networks of the same architecture. Our approach. We first train a small-width network in order to estimate pairwise attention score patterns, which we then use to sparsify the graph and train a larger network. We first train the graphs without edge attributes. This reduces the complexity of Exphormer to O(md + nd2) and then by training a much smaller width ds ≪ d network, reduces the time and memory complexity by at least a factor of d/ds. We also introduce two additions to the model to improve this consistency. Training this initial network can still be memory-intensive, but as the small width implies the matrix multiplications are small, it is practical to train this initial model on a CPU node with sufficient RAM (typically orders of magnitude larger than available GPU memory), without needing to use distributed computation. Once this initial model is trained, the attention scores can be used in creating a sparse graph, over which we train the second network. These initial attention scores can be used as edge features for the second network. As mentioned previously, we use the attention scores obtained from the trained low-width network to sparsify the graph. By selecting a fixed number c of edges per attention layer for each node, we reduce the complexity of each layer to O(nd2 + ndc). This sparsification alleviates the effect of a large number of edges, and allows for initial training with a larger degree expander graph, since most of the expander edges will be filtered for the final network. This sparsification differs from conventional graph sparsification algorithms (for MPNNs) in three ways. First, we use expander edges, self-loops, and graph edges and sparsify the combination of these patterns together. Second, this sparsification is layer-wise, which means that in a multi-layer network the attention pattern will vary from layer to layer. Finally, our sampling uses a smaller network trained on the same task, identifying important neighbors based on the task, instead of approaches independent of the task such as sampling based on PageRank or a neighbor’s node degree. Another advantage of this approach is that the fixed number of neighbors for each node enables regular matrix calculations instead of the edge-wise calculations used by Kreuzer et al. (2021); Shirzad et al. (2023), greatly improving the speed of the model. After this reduction, batching can be done based on the edges over different layers, enabling Transformers to be effectively batched while still effectively approximating the main Transformer model, enabling modeling long-range dependencies. In batching large graphs, naive implementations of sampling without replacement from 2(a) (b) (d) (e) (f) (c) Figure 1: Figure (a) shows a very simple synthetic graph where each node has a binary classification task of determining whether there exists a node of the opposite color in the same connected component. This task requires learning long-range dependencies. Figure (b) shows a natural clustering of the graph. This clustering would mean no node can do its task if models are trained only on one cluster at a time. Figure (c) shows a neighbor sampling starting from the green node, where random sampling fails to select the single important edge that bridges to the different-colored nodes. Figure (d) shows a random subset sampling strategy, where the task is solvable if and if only the two sides of the bridge between the two colors get selected. If we increase the size of each cluster, while keeping just one edge between two colors, the probability of selecting the bridge in any batch goes to zero, and thus the training will fail in this scenario. (e) shows attention scores between the nodes if trained with an attention-based network. Dashed lines have near zero attention scores, and thicker lines indicate a larger attention score. Knowing these attention scores will mean each node with just one directional edge can do the task perfectly. The attention edges are shown in (f). In case two nodes are equally informative; selecting either of them leads to the correct result. attention edges with varying weights can be very slow. This is especially true if the attention scores are highly concentrated on a small number of neighbors for most of the nodes. We use reservoir sampling (Efraimidis and Spirakis, 2006), enabling parallel sampling with an easy, efficient GPU implementation, improving the sampling process significantly. We only use the Transformer part of the Exphormer model, not the dual MPNN+Transformer architecture used by Shirzad et al. (2023); Rampášek et al. (2022). Unlike the Exphormer approach, we do not assume that the expander graph is of degree O(1); we can see this as interpolating between MPNNs and full Transformers, where smaller degree expander graphs mostly rely on the graph edges and are more similar to MPNNs, while higher degree expander graphs can resemble full attention, in the most extreme case of degree n − 1 exactly recovering a full Transformer. To summarize, the contributions of this paper are as follows: 1) We experimentally and theoretically analyze the similarity of attention scores for networks of different widths, and propose two small architectural changes to improve this similarity. 2) We propose layer-wise sparsification, by sampling according to the learned attention scores, and do theoretical analysis on the sparsification guarantees of the attention pattern. 3) Our two-phase training process allows us to scale Transformers to larger datasets, as it has significantly smaller memory consumption, while maintaining competitive accuracy. 2 Related Work Graph Transformer Architectures. Attention mechanisms were proposed in early (message- passing) Graph Neural Network (GNN) architectures such as Graph Attention Networks (GAT) (Veliˇckovi´c et al., 2018), where they guide node aggregation among neighbors, without using positional encodings. GraphBert (Zhang et al., 2020) finds node encodings based on the underlying graph structure. Subsequent work has proposed full-fledged graph Transformer models that generalize sequence Transformers (Dwivedi and Bresson, 2020) and are not limited to message passing between nodes of the input graph; these include Spectral Attention Networks (SAN) (Kreuzer et al., 2021), Graphormer (Ying et al., 2021), GraphiT (Mialon et al., 2021), etc. GraphGPS (Ram- pášek et al., 2022) combines attention mechanisms with message passing, allowing the best of both worlds. 3Efficient Graph Transformers. Several recent works have proposed various scalable graph trans- former architectures. NAGphormer (Chen et al., 2022a) and Gophormer (Zhao et al., 2021) use a sampling-based approach. On the other hand, Difformer (Wu et al., 2023) proposes a continuous time diffusion-based transformer model. Exphormer (Shirzad et al., 2023) proposes a sparse graph that combines the input graph with edges of an expander graph as well as virtual nodes. They show that their model works better than applying other sparse Transformer methods developed for sequences. Another work, NodeFormer (Wu et al., 2022), which is inspired by Performer (Choromanski et al., 2021), uses the Gumbel-Softmax operator as a kernel to efficiently propagate information among all pairs of nodes. SGFormer (Wu et al., 2024) shows that just using a one layer transformer network can sometimes improve the results of GCN-based networks and the low memory footprint can help scale to large networks. Perhaps most conceptually similar to our work is Skeinformer (Chen et al., 2022b), which uses sketching techniques to accelerate self-attention. Sampling and batching techniques. Some sampling-based methods have been used to alleviate the problem of “neighborhood explosion.” For instance, sampling was used in GraphSAGE (Hamilton et al., 2017), which used a fixed-size sample from a neighborhood in the node aggregation step. GraphSAINT (Zeng et al., 2020) scales GCNs to large graphs by sampling the training graph to create minibatches. Other. Expander graphs were used in convolutional networks by Prabhu et al. (2018). 3 Preliminaries and Notation Exphormer. EXPHORMER is an expander-based sparse attention mechanism for graph transformers that uses O(|V | + |E|) computation, where G = (V, E) is the underlying input graph. Exphormer creates an interaction graph H that consists of three main components: edges from the input graph, an overlaid expander graph, and virtual nodes (which are connected to all the original nodes). For the expander graph component, Exphormer uses a constant-degree random expander graph, with O(n) edges. Expander graphs have several useful theoretical properties related to spectral approximation and random walk mixing, which allow the propagation of information between pairs of nodes that are distant in the input graph G without explicitly connecting all pairs of nodes. The expander edges introduce many alternative short paths between the nodes and avoid the information bottleneck that can be caused by the virtual nodes. Our model. We use H to denote the attention pattern, and NH(i) the neighbors of node i under that pattern. Let X = (x1, x2, . . . ,xn) ∈ Rd×n be the matrix of d-dimensional embeddings for all of the n nodes. Our primary “driver” is then h-head attention: using ⊙ for element-wise multiplication, ATTN H(X):,i = xi + hX j=1 Vj i · σ \u0010\u0000 Ej ⊙ Kj\u0001T Qj i + Bj \u0011 , where Vj i = Wj V XNH(i), K = Wj KXNH(i), and Qj i = Wj Qxi, are linear mappings of the node features for the neighbors XNH(i), and Ej = Wj EENH(i) and Bj = Wj BENH(i) are linear maps of the edge features E, which is a dE × |NH(i)| matrix of features for the edges coming in to node i. Exphormer uses learnable edge features for each type of added edge, and original edge features for the graph’s edges. If the graph does not have any original edge features, it uses a learnable edge feature across all graph edges. Edge features help the model distinguish the type of attention edges. Here, σ is an activation function. In both Exphormer and our work the activation function is ReLU. In the absence of edge features, which is the case for most of the transductive datasets, including the datasets that have been used in this paper, Ee for any attention edge e can have one of three possible representations, and so Ej can be computed more simply by first mapping these three types of edge features with Wj E for head j, and then replacing the mapped values for each edge type. This simple change reduces the complexity of the Exphormer from O(md2 + nd2) to O(md + nd2). Compared to prior work, we introduce Bj as a simpler route for the model to adjust the importance of different edge types. Considering Exphormer as an interpolation between MPNNs and full Transformers, the Bj model has an easier path to allow for attention scores to be close to zero for all non-graph attention edges, without restricting the performance of the attention mechanism on 4graph edges. Consequently, it can function roughly as an MPNN (similar to GAT) by zeroing out the non-local attention paths. We use dE = d, and have each layer output features of the same width as its input, so that each of the Wj · parameter matrices except for Wj B are d × d, and Wj B is d × 1. As a simple illustration that Ej is insufficient to allow near-zero attention scores, thus highlighting the importance of Bj, note that if the columns of K and Q are distributed independently and uniformly on a unit ball (e.g., under a random initialization), there is no vector Ej which is identical for all edges of an expander graph that can make the attention scores for all the expander edges near-zero. Our network compared to Exphormer. We use Exphormer as the base model because it provides us the flexibility to adjust the sparsity of the attention graph and to interpolate between MPNNs and full Transformers. Exphormer can model many long-range dependencies that are not modeled by MPNNs and are very expensive to model in a full Transformer. For example, one cannot train a full Transformer model in the memory of a conventional GPU device for a dataset such as Physics, which has a graph on just 34K nodes. In our instantiation of Exphormer, we add self-loops for every node and use d/2 random Hamiltonian cycles to construct our expander graph as described in (Shirzad et al., 2023, Appendix C.2). We do not add virtual nodes in our networks. (Even so, the resulting network is still a universal approximator; Shirzad et al., 2023, Theorem E.3). Although the best known results for Exphormer combine sparse attention with MPNNs, in this work, we avoid the MPNN component for scalability reasons. We also make two additional changes; see Section 4. 4 Method Our method consists of a two-phase training process. The first phase trains a model we call the Attention Score Estimator Network, whose goal is to estimate the attention scores for a larger network. This model is not particularly accurate; its only goal is for each node to learn which neighbors are most important. The learned attention scores for each layer of the first network are then used to construct sparse interaction graphs for each layer in a second model, which is trained (with hyperparameter tuning for the best results) and serves as the final predictor. Attention Score Estimator Network. For this network, we use a width of 4 or 8, with just one attention head, in our training. We tune the other hyperparameters in order to have a converged training process with reasonably high accuracy, but we do not spend much time optimizing this network as it is sufficient to learn the important neighbors for each node, i.e., edges with high attention scores. This network will be trained with as many layers as the final network we want to train. Because it is so narrow, it has many fewer parameters and hence much less memory and time complexity, making it cheaper to train. Moreover, we only need to do this training once per number of layers we consider, conditioned on the fact that the training converges, even if the final model has a large number of hyperparameters. Compared to Exphormer, we use a much higher-degree expander graph: 30 to 200 instead of the 6 used for most transductive graphs by Shirzad et al. (2023). As most of the considered datasets do not have edge features, we use a learnable embedding for each type of edge (graph edge, expander edge, or self-loop). We also make two small changes to the architecture and the training process of this model, discussed below. Section 5.1 shows experimentally that the low-width network is a good estimator of the attention scores for a large-width network. Normalizing V . Having a smaller attention score, αij < αij′ , does not necessarily mean that j’s contribution to i’s new features is smaller than that of j′: if ∥Vj∥ ≫ ∥Vj′ ∥, the net contribution of j could be larger. Although Transformers typically use layer normalization, they do not typically do so after mapping X to V. We normalize the rows of V to have the same vector sizes for all nodes. In our experiments, normalizing to size one reduced performance significantly; however, adding a learnable global scale s, so that Vi becomes sVi ||Vi||2 , maintained performance while making attention scores more meaningful. Variable Temperature One of the side goals is to have sharper attention scores, guiding the nodes to get their information from as few nodes as possible. Using temperature in the attention mechanism can do this, where logits will be divided by a temperature factor τ before being fed into a softmax. Normal attention corresponds to τ = 1; smaller τ means sharper attention scores. However, setting the temperature to a small value from the beginning will make the random initialization more significant, and increase the randomness in the training process. Instead, we start with τ = 1.0 and gradually anneal it to 0.05 by the end of the training. We set an initial phase for λ epochs 5v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 1  v 2  v 3  v 4  v 5  v 6  v 7  v 8  KV Q Add & SoftMax Sparse MatMul Feed Forward Network Normalize E B Dot Product Low-width Network Layer (b)  v 1 v 2  v 3  v 4  v 5 v 6  v 7  v 8  v 1 v 2  v 3  v 4  v 5 v 6  v 7  v 8  + v 1 v 2  v 3  v 4  v 5 v 6  v 7  v 8  (a) (c)  v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 1  v 2  v 3  v 4  v 5  v 6  v 7  v 8  Layer-wise Sampling KV Q Add & SoftMax Sparse MatMul Feed Forward Network E B Dot Product High-width Network Layer (f) (d) (e)  Figure 2: Steps of our method. (a) The attention mechanism for the attention score estimator network combines graph edges with an expander graph and self-loops. The expander graphs are constructed by combining a small number of Hamiltonian cycles – here two, in red and in purple – then confirming the spectral gap is large enough. (b) Self-attention layers in the estimator network use this sparse attention mechanism; its self-attention layers normalize V. (c, d) Attention scores are extracted from this network for each layer, and used to sample, in (e), a sparse directed graph, which becomes the attention graph for the final network (f). This network, with a much larger feature dimension, does not normalize V. where we use τ = 1; this lets the model learn which neighbors are more important for each node slowly. We multiply τ with a factor γ after each epoch, obtaining a temperature in epoch t > λof max(γt−λ, 0.05). We use λ = 5 and γ = 0.99 or 0.95 depending on how fast the learning converges. Sparser Attention Pattern. The memory and time complexity of Exphormer is linearly dependent on the number of edges. Also, with a small number of layers, the expander degree should be high enough to ensure a large enough receptive field for each node in order to learn the long-range dependencies. Not all these edges are equally important, and many of them will have a near-zero effect on the final embedding of each node. Reducing the number of edges can alleviate memory consumption. Additionally, a sparser pattern lets us use batching techniques for the larger graphs. In this work, we analyze how effectively the sparser model can work and up to what factor we can sparsify. For each layer, e.g., ℓ, we select a degℓ as a fixed degree for each node and sample without replacement according to the attention score estimator network’s attention scores in each epoch of training or evaluation. Having the same degree for each node’s attention pattern also means that attention can be calculated using (much-more-optimized) standard matrix multiplications, rather than the propagation techniques used in Exphormer and SAN (Kreuzer et al., 2021). To sparsify the graph, in each epoch, we sample a new set of edges according to the learned attention scores from the smaller network. The reason why we do this rather than a simpler strategy such as selecting top-scored edges is that in many cases, several nodes can have very similar node features. If we assume nodes u1, u2, . . . , up from the neighbors of node v have almost the same features, and if the attention scores for these nodes are α1, α2, . . . , αp, any linear combination of Pp i=1 αi = α will lead to the same representation for node v. If features are exactly the same, α will be divided between these nodes, and even if α is large, each node’s attention score from v can be small. By sampling, we have a total α chance of selecting any of the nodes u1:p. In each epoch, we re-sample a new set of edges for each node from its original neighborhood. Faster Sampling Using Reservoir Sampling. Sampling without replacement using default li- brary calls is very slow, especially if few neighbors dominate the attention scores. We instead use reservoir sampling (Efraimidis and Spirakis, 2006), which is GPU-friendly and parallelizable. For reservoir sampling of k neighbors from the neighborhood of node i, with attention scores a = ( a1, a2, ··· , a|NH(i)|), we first take a uniform random sample u = ( u1, u2, ··· , u|NH(i)|), where the ui are i.i.d. samples from Uniform(0, 1). Then we calculate 1 a ⊙log(u) with element-wise 6multiplication, and select the indices with the top k values from this list. Selecting k-th rank from n values and pivoting has a worst-case O(n) time algorithm, which is much faster than the O(nk) worst case time for trial-and-error. Pseudocode is given in Algorithm 1. The GPU-friendly version of this can be implemented by sampling for nodes in parallel, but requires forming a regular matrix for the attention scores. This can be done by extending each attention score vector to the maximum degree, or selecting a value k′ ≫ k and first sampling k′ and selecting the top k′ attention scores from each node, making sure that the sum of the rest of the neighbor’s attention scores are very near to zero. Then by forming a rectangular attention matrix, uniform sampling and element-wise multiplications are much faster on GPU, and sampling from the entire batch is much more efficient. Algorithm 1 Reservoir Sampling from a Node’s Neighborhood Input: Attention scores a = a(ℓ) i,NH(i), number of neighbors to sample: degℓ Output: List of degℓ neighbors of node i 1: function RESERVOIR SAMPLE (a, degℓ) 2: u ∼ Uniform(0, 1)|NH(i)| 3: return argtopdegℓ(1 a ⊙ log(u)) 4: end function Batching. Each batch starts with a random subset of “target” nodes B. These are the nodes whose last-layer representations we will update in this optimization step. To calculate these representations, we need keys and values based on the previous layer’s representations for the relevant neighbors of each target node (again, sampling neighbors from the graph augmented by an expander graph). To approximate this, we sample degL neighbors for each target node. Then we have a set of at most |B|(degL +1) nodes whose representations we need to calculate in layerL−1; we repeat this process, so that in layer ℓ we need to compute representations for up toQ(ℓ) ≤ min(|B|QL i=ℓ+1(degi +1), n) query nodes, with |Q(ℓ)|degℓ attention edges. Pseudocode is given in Algorithm 2. When the number of layers L and degree degℓ are not too large, this batching can be substantially more efficient than processing the entire graph. Moreover, compared to other batching techniques, our approach selects neighbors according to their task importance. Except for optimization dynamics in the training process corresponding to minibatch versus full-batch training, training with batches is identical to training with the entire sparsified graph; if we choose a large degℓ equal to the maximum degree of the augmented graph, this is exactly equivalent to SGD on the full graph, without introducing any biases in the training procedure. This is in stark contrast to previous approaches, as illustrated in Figure 1. Unlike these prior approaches, which typically use the full graph at inference time, we can run inference with batch size as small as one (trading off memory for computation). Algorithm 2 Neighborhood Sampling for a Batch of Nodes Input: Attention scores in each layer: a = n a(ℓ) i,j | ∀i ∈ V, j∈ NH(i), ,1 ≤ ℓ ≤ L o , number of neighbors to sample in each layer: deg = {deg1, ··· , degL}, and a batch of nodes B ⊆ V Output: Q(ℓ), K(ℓ), V(ℓ), query, key, and value nodes in each layer 1: function SAMPLE NEIGHBORHOOD (B, a, deg) 2: V(L+1) ← B 3: for ℓ ← L to 1 do 4: Q(ℓ) ← V(ℓ+1) 5: for i ← i ∈ Q(ℓ) do 6: K(ℓ) i ← RESERVOIR SAMPLE (ai,NH(i), degℓ) 7: end for 8: K(ℓ) ← S i∈Qℓ K(ℓ) i 9: V(ℓ) ← Q(ℓ) SK(ℓ) 10: end for 11: return \b\u0000 V(ℓ), Q(ℓ), K(ℓ)\u0001 | 1 ≤ ℓ ≤ L \t 12: end function 7Fixed Node Degree Layers. Sparse matrix operations are not yet nearly as efficient as dense operations on GPU devices. Exphormer and SAN use a gather operation, which is memory-efficient but not time-efficient on a GPU (Zaheer et al., 2020). By normalizing the degree, instead of having |Q(ℓ)|degℓ separate dot products between the query and key vectors, we can reshape the key vectors to be of size |Q(ℓ)| ×degℓ ×d and the query is of shape |Q(ℓ)| ×d. Now the dot product of query and key mappings can be done using |Q(ℓ)|, degℓ ×d by d × 1 matrix multiplications. This same size matrix multiplication can be done using highly optimized batch matrix multiplication operations in e.g. PyTorch and Tensorflow (Paszke et al., 2019; Abadi et al., 2015). 4.1 Theoretical Underpinnings We first study the approximability of a network with a smaller hidden dimension or width. Formally, suppose that the width of a wide network is D. Then there exists a network with narrow dimensions for WQ and WK, of dimension O(log n ε2 ) × D instead of D × D, whose attention scores agree with those of the wide network up to O(ε) error (Theorem E.4). This reduction helps with the most intensive part of the calculation; others are linear with respect to the number of nodes n. While this is not the model we use in practice, Shirzad et al. (2024, Section 4) explore some scenarios common in graph Transformers that allow for the existence of “fully” narrow networks with accurate attention scores. They support these claims with experiments that show compressibility for some datasets we use. This is an existence claim; we will justify experimentally that in practice, training a narrow network does approximate attention scores well. We then study the sampling procedure of our sparsification method. Under certain assumptions, we show that sampling roughly O(n log n/ε2) entries of the attention matrix A (corresponding to sampling this many edges in the graph) suffices to form a matrix B with ∥A − B∥2 ≤ ε∥A∥2, if we can access the entries of A (Theorem E.5). We cannot actually access the matrix A, but we do have attention scores A′ from a narrow network. We show that if the entries ofA are not seriously under-estimated by A′, the same bound on the number of samples still holds (Proposition E.7). Table 1: Comparison of our model with other GNNs on six homophilic datasets. The reported metric is accuracy for all datasets. Model Computer Photo CS Physics WikiCS ogbn-arxiv GCN 89.65 ±0.52 92.70 ±0.20 92.92 ±0.12 96.18 ±0.07 77.47 ±0.85 71.74 ±0.29 GRAPHSAGE 91.20 ±0.29 94.59 ±0.14 93.91 ±0.13 96.49 ±0.06 74.77 ±0.95 71.49 ±0.27 GAT 90.78 ±0.13 93.87 ±0.11 93.61 ±0.14 96.17 ±0.08 76.91 ±0.82 72.01 ±0.20 GRAPHSAINT 90.22 ±0.15 91.72 ±0.13 94.41 ±0.09 96.43 ±0.05 - 68.50 ±0.23 NODEFORMER 86.98±0.62 93.46 ±0.35 95.64 ±0.22 96.45 ±0.28 74.73 ±0.94 59.90 ±0.42 GRAPHGPS 91.19 ±0.54 95.06 ±0.13 93.93 ±0.12 97.12 ±0.19 78.66 ±0.49 70.92 ±0.04 GOAT 90.96 ±0.90 92.96 ±1.48 94.21 ±0.38 96.24 ±0.24 77.00 ±0.77 72.41 ±0.40 EXPHORMER+GCN 91.59 ±0.31 95.27 ±0.42 95.77 ±0.15 97.16 ±0.13 78.54 ±0.49 72.44 ±0.28 EXPHORMER* 91.16 ±0.26 95.36 ±0.17 95.19 ±0.26 96.40 ±0.20 78.19 ±0.29 71.27 ±0.27 SPEXPHORMER 91.09±0.08 95.33 ±0.49 95.00 ±0.15 96.70 ±0.05 78.2 ±0.14 70.82 ±0.24 Avg. Edge Percent 7.6% 8.2% 12.8% 11.3% 8.6% 13.7% 5 Experimental Results 5.1 Attention Score Estimation To show how well the smaller network estimates the attention scores for a larger network, we conduct experiments on two smaller datasets, where we can reasonably train the full network at higher width for many runs in order to estimate the distribution of the attention scores. To this end, we use the Actor (Lim et al., 2021) and Photo (Shchur et al., 2018) datasets. We train the network for hidden dimensions h varying from 4 to 64 for both datasets. For each h we train the network 100 times. We consider the distribution of attention scores for each node, and estimate the energy distance (Székely and Rizzo, 2013; an instance of the maximum mean discrepancy, Sejdinovic et al., 2013) for that node’s attention scores across each pair of h sizes. 8uniformrandom 4 8 16 32 64 0.00 0.02 0.04 0.06 0.08 0.10 Actor Dataset without Expanders (a) uniformrandom 4 8 16 32 64 0.00 0.05 0.10 0.15 0.20 Actor Dataset with Expanders (b) uniformrandom 4 8 16 32 64 0.00 0.05 0.10 0.15 0.20 Amazon-Photo Dataset without Expanders (c) uniformrandom 4 8 16 32 64 0.0 0.1 0.2 0.3 Amazon-Photo Dataset with Expanders (d) Figure 3: Energy distance between the attention scores of various networks to a network of width 64. “Uniform” refers to the baseline placing equal scores to each neighbor, while “random” refers to the baseline with uniformly distributed logits. The remaining bars refer to networks trained on the appropriately labeled width. We ran this experiment in two scenarios: first, with just graph edges, and then, by adding expander and self-loop edges. It might be that the model, just by examining the category of the edges, may give a lower score to one type, making distributions seem more similar despite not identifying a small number of important neighbors as we want. However, in the presence of only one type of edge, the model can still consistently estimate which nodes should have a higher attention score. We compare attention scores from our model with the uniform distribution on the neighbors (each neighbor of node i has score 1 di ), and to a distribution with logits uniform over [−8, 8]. The choice of 8 here is because in the network we clip the logits with an absolute value higher than8. Figure 3 shows that even width-4 networks provide far superior estimates of attention scores than these baselines. In Appendix F, we extend our analysis with several experiments: examining pairwise energy distances between all pairs of hidden dimensions as well as uniform and random distributions, providing layer- wise results (Appendix F.2), analyzing the sharpness or smoothness of attention scores across layers (Appendix F.3), assessing their similarity between layers (Appendix F.4), and measuring precision, recall, density, and coverage in estimating the attention scores of the larger network using a smaller one (Appendix F.5). Additionally, we investigate the sum of top-k attention scores (Appendix F.6) and evaluate the role of different edge types in learning representations (Appendix F.7). Our key insights are as follows: Insight 1.Attention scores from a network with a smaller hidden dimension serve as a good estimator for the attention scores in a network with a higher hidden dimension. Insight 2.Attention scores are smoother in the first layer, and become sharper in subsequent layers. Insight 3.The attention scores in the layers after the first are consistently very similar to one another, but distinct from the attention scores in the first layer. Insight 4.The sum of the top-k attention scores is substantially lower than one for many nodes, even for relatively large k values such as 10. Table 2: Comparison of our model with other GNNs on five heterophilic datasets. The reported metric is ROC-AUC (×100) for the Minesweeper, Tolokers, and Questions datasets, and accuracy for all others. Model Actor Minesweeper Tolokers Roman-Empire Amazon-Ratings Questions GLOGNN 36.4 ±1.6 51.08 ±1.23 73.39 ±1.17 59.63 ±0.69 36.89 ±0.14 65.74 ±1.19 GCN 33.23 ±1.16 89.75 ±0.52 83.64 ±0.67 73.69 ±0.74 48.70 ±0.63 76.09 ±1.27 GRAPHGPS 37.1 ±1.5 90.63 ±0.67 83.71 ±0.48 82.00 ±0.61 53.10 ±0.42 71.73 ±1.47 NAGPHORMER - 84.19 ±0.66 78.32 ±0.95 74.34 ±0.77 51.26 ±0.72 68.17 ±1.53 NODEFORMER 36.9±1.0 86.71 ±0.88 78.10 ±1.03 64.49 ±0.73 43.86 ±0.35 74.27 ±1.46 GOAT - 81.09 ±1.02 83.11 ±1.04 71.59 ±1.25 44.61 ±0.50 75.76 ±1.66 EXPHORMER+GAT 38.68 ±0.38 90.74 ±0.53 83.77 ±0.78 89.03 ±0.37 53.51 ±0.46 73.94 ±1.06 EXPHORMER* 39.01 ±0.69 92.26 ±0.56 83.53 ±0.28 84.91 ±0.25 46.80 ±0.53 73.35 ±1.78 SPEXPHORMER 38.59±0.81 90.71 ±0.17 83.34 ±0.31 87.54 ±0.14 50.48 ±0.34 73.25 ±0.41 Avg. Edge Percent 5.8% 17.8% 8.9% 31.1% 15.3% 13.8% 9Actor Photo Minesweeper Tolokers CS ComputerPhysicsogbn-arxiv Dataset 0 5 10 15 20 25 30 35 40GigaBytes Attention Score Estimator Spexphormer Exphormer w Degree 6 Exphormer w Degree 30 Figure 4: Memory usage comparison: Attention Score Estimator network and Spexphormer vs. Exphormer with expander degrees 6 and 30. Exphormer with de- gree 30 for the ogbn-arxiv dataset could not fit into the memory of a 40GB GPU device, and thus the number here is a lower bound. Model ogbn-proteins Amazon2M Pokec * MLP 72.04 ±0.48 63.46±0.10 60.15±0.03GCN 72.51 ±0.35 83.90±0.10 62.31±1.13SGC 70.31 ±0.23 81.21±0.12 52.03±0.84GCN-NSAMPLER 73.51±1.31 83.84±0.42 63.75±0.77GAT-NSAMPLER 74.63±1.24 85.17±0.32 62.32±0.65SIGN 71.24 ±0.46 80.98±0.31 68.01±0.25NODEFORMER 77.45±1.15 87.85±0.24 70.32±0.45SGFORMER 79.53±0.38 89.09±0.10 73.76±0.24SPEXPHORMER 80.65±0.07 90.40±0.03 74.73±0.04 Memory Information for SPEXPHORMER Memory (MB) 2232 3262 2128Batch Size 256 1000 500Hidden Dimension 64 128 64Number of layers 2 2 2Number of Parameters 79,224 300,209 83,781 Table 3: Comparative results on large graph datasets, with ROC-AUC(×100) reported for the ogbn-proteins dataset and accuracy for all others. GPU memory usage, batch sizes, hidden dimensions used to obtain these numbers, and the total number of parameters have been added at the bottom of the table. 5.2 Model Quality We conduct experiments on twelve medium-sized graphs, including six homophilic datasets: CS, Physics, Photo, Computer (Shchur et al., 2018), WikiCS (Mernyei and Cangea, 2020), and ogbn-arxiv (Hu et al., 2021); and six heterophilic datasets: Minesweeper, Tolokers, Roman-empire, Amazon- ratings, Questions (Platonov et al., 2023), and Actor (Lim et al., 2021). For the CS, Physics, Photo, and Computer datasets, we use a random train/validation/test split of 60%/20%/20%. For WikiCS and ogbn-arxiv we follow the standard data split provided by the original source. For the Actor dataset, we use a 50%/25%/25% split following Wu et al. (2022). For the Minesweeper, Tolokers, Roman-empire, Amazon-ratings, and Questions datasets, we use the standard split from Platonov et al. (2023). Results for these experiments are provided in Tables 1 and 2. The EXPHORMER model presented in the tables refers to the attention mechanism of EXPHORMER without incorporating any MPNN components. Interestingly, the results on the Roman-Empire and Amazon-Ratings datasets revealed that removing certain edges led to better performance compared to simply adding an expander layout. In these medium-sized datasets, we are able to train the full Exphormer model. Our goal is to determine the extent of performance reduction when using two memory-efficient networks to estimate the original network. Results show that the two memory-efficient networks can efficiently estimate the original network, enabling us to scale the Exphormer to larger graph datasets. We compare the maximum required memory of the attention score estimator and final networks with that of the corresponding Exphormer model in Figure 4. We then experiment on large graph datasets: ogbn-proteins, Amazon2M (Hu et al., 2021), and Pokec (Takac and Zabovsky, 2012). The results provided in Table 3 demonstrate superior performance of our model despite limited memory constraints. We follow the standard data split for the ogbn-proteins dataset and follow Wu et al. (2024) for the dataset split on the Amazon2M and Pokec datasets, with 10%/10%/80% and 50%/25%/25% train/validation/test ratios. We emphasize that this split differs from the original dataset split used by many other works, making those numbers incomparable. In all our experiments, we train the smaller network once, and then for the second network, we always use the same initial network’s learned attention scores. Attention scores are collected from the network training step with the highest validation accuracy. We use a subset of the following models in each of our tables as baselines, depending on the type of the dataset and scalability level of the models, GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017), GAT (Veliˇckovi´c et al., 2018), GraphSAINT (Zeng et al., 2020), Nodeformer (Wu et al., 2022), Difformer (Wu et al., 2023), SGFormer (Wu et al., 2024), GraphGPS (Rampášek et al., 2022), GOAT (Kong et al., 2023), GloGNN (Li et al., 2022), SGC (Wu et al., 2019), NAGphormer (Chen et al., 2022a), Exphormer (Shirzad et al., 2023), and SIGN (Frasca et al., 2020). We borrow most of the baseline numbers in the tables from Wu et al. (2024); Deng et al. (2024). 10Table 4: Ablation studies on two homophilic and two heterophilic datasets. Metrics: accuracy for Photo and Computer, ROC-AUC (×100) for Tolokers and Minesweeper. For the initial network, we report the result for the network used for training the Spexphormer and thus, there is no confidence interval for them. Model/Dataset Computer Photo Minesweeper Tolokers Initial Network 85.23 91.70 85.67 80.16 Spexphormer-uniform 86.65 ±0.46 94.21 ±0.22 84.15 ±0.22 82.56 ±0.17 Spexphormer-max 89.31 ±0.31 95.07 ±0.20 87.92 ±0.26 80.85 ±0.23 Spexphormer w.o. temp 89.05 ±0.35 95.30 ±0.16 90.02 ±0.02 83.34 ±0.13 Spexphormer w.o. layer norm 89.70±0.25 94.91 ±0.18 89.65 ±0.10 84.06±0.10 Spexphormer 91.09±0.08 95.33 ±0.49 90.71 ±0.17 83.34±0.13 5.3 Ablation Studies We benchmark the effect of different parts of the model in Table 4. Spexphormer-uniform, rather than sampling based on the estimated attention scores, samples uniformly from the augmented graph; this is always worse than attention-based sampling, but the gap is larger for some datasets than others. Spexphormer-max takes the edges with the highest attention scores, rather than sampling; this again performs somewhat worse across datasets. Spexphormer w.o. temp uses a constant temperature of 1 in the initial attention score estimator network; Spexphormer w.o. layer norm removes our added layer normalization. These changes are smaller, and in one case layer normalization makes the results worse. Across the four datasets, however, it seems that both temperature and layer norm help yield more informative and sparser attention scores. 6 Conclusion & Limitations We analyzed the alignment of the attention scores among models trained with different widths. We found that the smaller network’s attention score distributions usually align well with the larger network’s. We also theoretically analyzed the compressibility of the larger Graph Transformer models. Based on these observations, we used a sampling algorithm to sparsify the graph on each layer. As a result of these two steps, the model’s memory consumption reduces significantly, while achieving a competitive accuracy. This strategy also lets us use novel batching techniques that were not feasible with expander graphs of a large degree. Having a regular degree enables using dense matrix multiplication, which is far more efficient with current GPU and TPU devices. While our method successfully scales to datasets with over two million nodes, it relies on large CPU memory for the attention score estimation for these datasets. For extremely large datasets, this is still infeasible without highly distributed computation. Estimated attention scores can be shared and used for training various networks based on attention scores, however, so this only needs to only be computed once per dataset and depth. An area for potential future work is to combine sampling with simultaneous attention score estimation in a dynamic way, scaling this estimation to larger graphs. Acknowledgments and Disclosure of Funding This work was supported in part by the Natural Sciences and Engineering Resource Council of Canada, the Fonds de Recherche du Québec - Nature et technologies (under grant ALLRP-57708- 2022), the Canada CIFAR AI Chairs program, the BC DRI Group, Calcul Québec, Compute Ontario, and the Digital Resource Alliance of Canada. Honghao Lin was supported in part by a Simons Investigator Award, NSF CCF-2335412, and a CMU Paul and James Wang Sercomm Presidential Graduate Fellowship. References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Vasudevan, V ., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y ., and Zheng, X. (2015). 11TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org. Achlioptas, D., Karnin, Z. S., and Liberty, E. (2013). Near-optimal entrywise sampling for data matrices. Advances in Neural Information Processing Systems, 26. Chen, J., Gao, K., Li, G., and He, K. (2022a). Nagphormer: Neighborhood aggregation graph transformer for node classification in large graphs. CoRR, abs/2206.04910. Chen, Y ., Zeng, Q., Hakkani-Tur, D., Jin, D., Ji, H., and Yang, Y . (2022b). Sketching as a tool for understanding and accelerating self-attention for long sequences. In Carpuat, M., de Marneffe, M., and Ruíz, I. V . M., editors,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 5187–5199. Association for Computational Linguistics. Choromanski, K. M., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlós, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. (2021). Rethinking attention with performers. In ICLR. Cramér, H. (1928). On the composition of elementary errors: First paper: Mathematical deductions. Scandinavian Actuarial Journal, 1928(1):13–74. Deng, C., Yue, Z., and Zhang, Z. (2024). Polynormer: Polynomial-expressive graph transformer in linear time. arXiv preprint arXiv:2403.01232. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Di Giovanni, F., Giusti, L., Barbero, F., Luise, G., Lio, P., and Bronstein, M. M. (2023a). On over-squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865–7885. PMLR. Di Giovanni, F., Rusch, T. K., Bronstein, M. M., Deac, A., Lackenby, M., Mishra, S., and Veliˇckovi´c, P. (2023b). How does over-squashing affect the power of gnns? arXiv preprint arXiv:2306.03589. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Dwivedi, V . P. and Bresson, X. (2020). A generalization of transformer networks to graphs.CoRR, abs/2012.09699. Efraimidis, P. S. and Spirakis, P. G. (2006). Weighted random sampling with a reservoir. Information processing letters, 97(5):181–185. Finkelshtein, B., Ceylan, ˙I. ˙I., Bronstein, M., and Levie, R. (2024). Learning on large graphs using intersecting communities. arXiv preprint arXiv:2405.20724. Franks, B. J., Morris, C., Velingker, A., and Geerts, F. (2024). Weisfeiler-leman at the margin: When more expressivity matters. arXiv preprint arXiv:2402.07568. Frasca, F., Rossi, E., Eynard, D., Chamberlain, B., Bronstein, M., and Monti, F. (2020). Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198. Hamilton, W., Ying, Z., and Leskovec, J. (2017). Inductive representation learning on large graphs. Advances in neural information processing systems, 30. Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y ., and Leskovec, J. (2021). OGB-LSC: A large-scale challenge for machine learning on graphs. CoRR, abs/2103.09430. Johnson, W. B. (1984). Extensions of lipshitz mapping into hilbert space. In Conference modern analysis and probability, 1984, pages 189–206. 12Kakade, S. and Shakhnarovich, G. (2009). Lecture notes in large scale learning. https://home. ttic.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf. Kipf, T. N. and Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. Kong, K., Chen, J., Kirchenbauer, J., Ni, R., Bruss, C. B., and Goldstein, T. (2023). Goat: A global transformer on large-scale graphs. In International Conference on Machine Learning , pages 17375–17390. PMLR. Kreuzer, D., Beaini, D., Hamilton, W. L., Létourneau, V ., and Tossou, P. (2021). Rethinking graph transformers with spectral attention. arXiv preprint arXiv:2106.03893. Li, X., Zhu, R., Cheng, Y ., Shan, C., Luo, S., Li, D., and Qian, W. (2022). Finding global homophily in graph neural networks when meeting heterophily. In International Conference on Machine Learning, pages 13242–13256. PMLR. Lim, D., Hohne, F., Li, X., Huang, S. L., Gupta, V ., Bhalerao, O., and Lim, S. N. (2021). Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems, 34:20887–20902. Liu, X., Yan, M., Deng, L., Li, G., Ye, X., and Fan, D. (2021). Sampling methods for efficient training of graph convolutional networks: A survey. IEEE/CAA Journal of Automatica Sinica, 9(2):205–234. Mernyei, P. and Cangea, C. (2020). Wiki-cs: A wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901. Mialon, G., Chen, D., Selosse, M., and Mairal, J. (2021). Graphit: Encoding graph structure in transformers. CoRR, abs/2106.05667. Müller, L., Galkin, M., Morris, C., and Rampášek, L. (2023). Attending to graph transformers. arXiv preprint arXiv:2302.04181. Naeem, M. F., Oh, S. J., Uh, Y ., Choi, Y ., and Yoo, J. (2020). Reliable fidelity and diversity metrics for generative models. In International Conference on Machine Learning, pages 7176–7185. PMLR. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. (2020). Geom-gcn: Geometric graph convolutional networks. arXiv preprint arXiv:2002.05287. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. (2023). A critical look at the evaluation of GNNs under heterophily: Are we really making progress? arXiv preprint arXiv:2302.11640. Prabhu, A., Varma, G., and Namboodiri, A. M. (2018). Deep expander networks: Efficient deep networks from graph theory. In Ferrari, V ., Hebert, M., Sminchisescu, C., and Weiss, Y ., editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, volume 11217 of Lecture Notes in Computer Science, pages 20–36. Springer. Rampášek, L., Galkin, M., Dwivedi, V . P., Luu, A. T., Wolf, G., and Beaini, D. (2022). Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:14501–14515. Rizzo, M. L. and Székely, G. J. (2016). Energy distance. wiley interdisciplinary reviews: Computa- tional statistics, 8(1):27–38. Rusch, T. K., Bronstein, M. M., and Mishra, S. (2023). A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993. 13Sajjadi, M. S., Bachem, O., Lucic, M., Bousquet, O., and Gelly, S. (2018). Assessing generative models via precision and recall. Advances in neural information processing systems, 31. Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fukumizu, K. (2013). Equivalence of distance- based and RKHS-based statistics in hypothesis testing. The Annals of Statistics , 41(5):2263 – 2291. Shchur, O., Mumme, M., Bojchevski, A., and Günnemann, S. (2018). Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868. Shirzad, H., Lin, H., Venkatachalam, B., Velingker, A., Woodruff, D. P., and Sutherland, D. J. (2024). A theory for compressibility of graph transformers for transductive learning. In Machine Learning and Compression Workshop at NeurIPS. arXiv preprint arXiv:2411.13028. Shirzad, H., Velingker, A., Venkatachalam, B., Sutherland, D. J., and Sinop, A. K. (2023). Exphormer: Sparse transformers for graphs. In ICML. Székely, G. J. and Rizzo, M. L. (2013). Energy statistics: A class of statistics based on distances. Journal of Statistical Planning and Inference, 143(8):1249–1272. Takac, L. and Zabovsky, M. (2012). Data analysis in public social networks. InInternational scientific conference and international workshop present day trends of innovations, volume 1. Tay, Y ., Dehghani, M., Bahri, D., and Metzler, D. (2020). Efficient transformers: A survey.arXiv preprint arXiv:2009.06732. Topping, J., Di Giovanni, F., Chamberlain, B. P., Dong, X., and Bronstein, M. M. (2021). Understand- ing over-squashing and bottlenecks on graphs via curvature. arXiv preprint arXiv:2111.14522. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In NeurIPS, pages 5998–6008. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . (2018). Graph attention networks. In ICLR. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K. (2019). Simplifying graph convolutional networks. In International conference on machine learning , pages 6861–6871. PMLR. Wu, Q., Yang, C., Zhao, W., He, Y ., Wipf, D., and Yan, J. (2023). Difformer: Scalable (graph) transformers induced by energy constrained diffusion. arXiv preprint arXiv:2301.09474. Wu, Q., Zhao, W., Li, Z., Wipf, D. P., and Yan, J. (2022). Nodeformer: A scalable graph structure learning transformer for node classification. NeurIPS, 35:27387–27401. Wu, Q., Zhao, W., Yang, C., Zhang, H., Nie, F., Jiang, H., Bian, Y ., and Yan, J. (2024). Simplifying and empowering transformers for large-graph representations. Advances in Neural Information Processing Systems, 36. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y ., and Liu, T.-Y . (2021). Do transformers really perform bad for graph representation? ArXiv, abs/2106.05234. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. (2020). Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V . K. (2020). Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on Learning Represen- tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Zhang, J., Zhang, H., Xia, C., and Sun, L. (2020). Graph-Bert: Only attention is needed for learning graph representations. arXiv preprint arXiv:2001.05140. Zhao, J., Li, C., Wen, Q., Wang, Y ., Liu, Y ., Sun, H., Xie, X., and Ye, Y . (2021). Gophormer: Ego-graph transformer for node classification. CoRR, abs/2110.13094. 14A Notation Table Table 5: A summary of the notation used in this paper. The hat notation always refers to a compressed network equivalent of a vector or matrix from the reference network. Notation Definition n The number of nodes in the graph m The number of attention edges in total, including graph and expander edges d Hidden dimension of a narrow network D Hidden dimension of the original large graph L The total number of layers in the network ℓ Arbitrary layer index V Value mapping of the vectors in the attention mechanism Q Query mapping of the vectors in the attention mechanism K Key mapping of the vectors in the attention mechanism W(ℓ) · Weight matrix of mapping such as key, query, value, edge features, or bias in layerℓ cW(ℓ) · Low dimensional network’s weight matrix for a mapping in layerℓ M· A linear mapping matrix (usually from the higher dimension to the smaller) ReLU Rectified Linear Unit H(ℓ) Output of layerℓ −1 from the reference network ¯H(ℓ) A low-rank estimation ofH(ℓ) bH(ℓ) Output of layerℓ −1 from a compressed network h(ℓ) i columni of matrixH(ℓ) a(ℓ) ij The Attention score between nodesi andj in layerℓ ˆa(ℓ) ij The attention score between nodesi andj in layerℓ from a smaller network B Dataset Descriptions Below, we provide descriptions of the datasets on which we conduct experiments. A summarized statistics of these datasets have been provided in Table 6. Amazon datasets Amazon Computers and Amazon Photo are Amazon co-purchase graphs. Nodes represent products purchased. An edge connects a pairs of products purchased together. Node features are bag-of-words encoded reviews of the products. Class labels are the product category. Amazon-Ratings The Amazon-ratings is an Amazon co-purchasing dataset. Each node represents a product and the edges are between the nodes purchased together frequently. Node features are the average of word embeddings from the product description. The task is to predict the average rating of the product. Amazon2M Amazon2M dataset is a graph from the co-purchasing network. Each node represents an item. Edges between items represents products purchased together. The node features are generated from the product description. The node labels are from the top-level categories the product belongs to. WikiCS WikiCS contains pages from Wikipedia. Each node represents an article from Wikipedia related to the Computer Science field. Edges represent the hyperlinks between the articles. The node features are the average of the word embeddings from the articles. The task is to classify the nodes into ten different branches of the field. Actor dataset The actor dataset is created by the actor-only subgraph of a larger graph of actor, director, writer, and film co-occuring on a Wikipedia page, limited to English-language films. Each node corresponds to an actor. Edges denote co-occurence on a Wikipedia page. Node features are based on the terms in the actor’s page. The prediction task is categorizing into one of five categories (Pei et al., 2020). 15Roman-Empire This dataset is a graph constructed from the “Roman Empire” article from Wikipedia. Each node is a word from this text. Two words are connected to each other if they follow each other in the text, or they are connected in the dependency tree of the sentence. The task is to predict the syntactic role of the word in the sentence. Graph is highly sparse and heterophilic. Coauthor datasets The datasets, CS and Physics are co-authorship graphs from Microsoft Aca- demic Graph. The nodes represent the authors and an edge connects two authors who share a paper. The node features are the keywords in the papers. The class represents the active area of study for the author. ogbn-arxiv (Hu et al., 2021) The ogbn-arxiv dataset is from OGBN datasets. The nodes represents the papers and edges represent the citations between the papers. Nodes are 128-dimensional feature vector that is an average of the embeddings of words in the title and abstract. The prediction task is to identify the category of the 40 subject areas. ogbn-proteins dataset The ogbn-proteins dataset is an undirected graph with edge weights and types based on species. The nodes represent proteins from eight different species. Edges indicate various biologically meaningful associations between the proteins (e.g., co-expression, homology etc.). The edges are eight-dimensional, with each dimension having a value from [0,1] indicates the confidence score. The prediction task is a multi-label binary classification among 112 labels — to predict the presence of protein functions. The performance measurement is the average of ROC-AUC scores across the 112 tasks. Minesweeper The dataset is a graph representation of the 100x100 grid from the Minesweeper game. A node represents a cell and the edges connect a node to its eight neighboring cells. 20% of the nodes are marked as mines. The features of the nodes are the one-hot encoding of the mines among the neighbors. For 50% of the nodes the features are unknown and indicated by a separate binary feature. Tolokers Tolokers is a graph representation of the workers in a crowd-sourcing platform, called Toloka. Each node represents a worker. Two nodes are connected if the workers have worked on the same task. Node features are based on the worker’s task performance statistics and other profile information. The task is to predict which nodes have been banned for a project. Questions This dataset is derived from the Yandex Q question-answering platform, focusing on interactions among users interested in the topic of medicine from September 2021 to August 2022. Nodes represent users, and edges denote answers given to another user’s questions. Node features include fastText-based embeddings of user descriptions, supplemented by a binary indicator for missing descriptions. The task is to predict user activity status at the end of the period. Pokec Pokec is a large-scale social network dataset. Nodes represents users of the network. Nodes features include profile data like geographical region, age etc. The task is to predict the gender of users based on the graph. C More Experiments C.1 Time-Memory Trade-off One advantage of our method is its ability to trade time for memory without sacrificing accuracy. Figure 5 illustrates this trade-off on two datasets: ogbn-proteins and arxiv. In these experiments, all hyperparameters are kept constant, with the only variation being the batch size. The results demonstrate that memory usage and runtime can be adjusted without introducing bias into the training process. It is important to note that in random subset batching, the average degree of nodes and the number of edges included in the training process are closely related to the batch size. A very small batch size relative to the graph size can randomly exclude a significant portion of the graph’s edges during training, potentially ignoring critical edges without considering their importance. 16Table 6: Dataset statistics. The reported number of edges is the number of directed edges, which will be twice the number of actual edges for the undirected graphs. Dataset Nodes Edges Average Degree Node Features Classes Metric Amazon Photo 7,487 238,162 31.13 745 8 Accuracy Coauthor Physics 34,493 495,924 14.38 8,415 5 Accuracy Amazon Computer 13,381 491,722 35.76 767 10 Accuracy Coauthor CS 18,333 163,788 8.93 6,805 15 Accuracy WikiCS 11,701 431,726 36.90 300 10 Accuracy ogbn-arxiv 169,343 2,332,486 13.77 128 40 Accuracy Actor 7,600 33,391 4.39 932 5 Accuracy Minesweeper 10,000 78,804 7.88 7 2 AUC Tolokers 11,758 1,038,000 88.28 10 10 AUC Roman-Empire 22,662 65,854 2.91 300 18 Accuracy Amazon-Ratings 24,492 186,100 7.60 300 5 Accuracy Questions 48,921 307,080 6.28 301 2 AUC Pokec 1,632,803 30,622,564 18.75 65 2 AUC ogbn-proteins 132,534 79,122,504 597.00 8 112 AUC Amazon2M 2,449,029 123,718,280 50.52 100 47 AUC 256 512 1000 2048 4096 Batch Size 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Memory (GB) 60 70 80 90 100 110 Time per Epoch (s) ogbn-proteins Dataset (a) 128 256 512 1000 2048 4096 8192 Batch Size 1.5 2.0 2.5 3.0 3.5 4.0Memory (GB) 15 20 25 30 35 Time per Epoch (s) ogbn-arxiv Dataset (b) Figure 5: The memory and runtime trade-off for the ogbn-proteins and ogbn-arxiv datasets. The plot demon- strates that memory and time can be effectively exchanged in our approach. The reported runtime includes the whole process of preprocessing the batches, train, and validation on validation and test sets. All experiments were conducted on a V100 GPU with 32GB of memory. C.2 Neighborhood Expansion The level of neighborhood expansion significantly impacts the efficiency of our model. As described in Algorithm 2, neighborhood expansion begins from the final nodes for which we require representations in the final layer and proceeds backward through the layers, sampling neighbors based on attention scores at each layer. We analyze the number of nodes across four datasets: Amazon-Photo, Coauthor-CS, Minesweeper, and Tolokers, to observe how the number of nodes increases as we trace back through the layers. This experiment is conducted with varying sampling degrees per layer, and the results are summarized in Figure 6. In all experiments, we assume that representations are needed for 10 final nodes. We sample 100 times of these 10 random seed nodes and plot average and standard deviations of the neighborhood node counts. The process has an upper bound, which is the total number of nodes in the graph. As the number of sampled nodes approaches this limit, the likelihood of encountering new nodes decreases. We compare these results with full-neighborhood sampling methods, as employed in k-hop neighborhood-induced subgraph techniques, and demonstrate that in the presence of expander graphs, this neighborhood can rapidly encompass the entire graph. The impact of limited neighborhood sampling becomes even more pronounced on extremely large graphs. 174 3 2 1 0 Layer Number 0 2000 4000 6000 8000Number of Nodes Dataset: Amazon-Photo 4 3 2 1 0 Layer Number 0 5000 10000 15000 20000 25000 30000 35000Number of Nodes Dataset: Coauthor-Physics 4 3 2 1 0 Layer Number 0 2000 4000 6000 8000 10000Number of Nodes Dataset: Minesweeper 4 3 2 1 0 Layer Number 0 2000 4000 6000 8000 10000 12000Number of Nodes Dataset: Tolokers degree = 1 degree = 3 degree = 5 degree = 10 Full Neighborhood Figure 6: The neighborhood expansion of the graph is analyzed to determine the number of nodes required in each layer to obtain representations for 10 nodes in the final layer. This is compared between Spexphormer’s degree-based sampling and full-neighborhood selection. The shadowed regions in the plot represent the 95% confidence intervals, calculated from 100 iterations of sampling the ten final nodes. C.3 Memory and Runtime with Graph Size Inspired by Finkelshtein et al. (2024), we compare the memory and runtime of our method to a Graph Convolutional Network (GCN) during a forward pass. In many real-world scenarios, the average degree of nodes is not constant and tends to scale with the graph size. One advantage of our method is its ability to subsample neighborhoods for each node, identifying a small yet representative set of neighbors. While GCN is a more computationally efficient network, we demonstrate that, even with a small but superlinear growth in neighborhood size, the memory and runtime requirements of GCN can surpass those of our method, which employs a sparse but regular self-attention layer with a fixed neighborhood size. In these experiments, we evaluate different growth factors for the GCN and varying neighborhood sampling sizes for our sparse self-attention method. For these comparisons, no batching is used; the entire process operates on the whole graph. Both models consist of a single layer of the corresponding network followed by a linear layer that maps the values to dimension 1. We use a hidden dimension of 128. In this setup, the GCN has approximately 16K parameters, while the self-attention layer in our method has about 65K parameters. We vary the number of nodes from10K to 50K, using an Erd˝os-Rényi distribution with specified probabilities p, denoted as ER(n, p). Here, n represents the number of nodes, and p is the probability that any pair of nodes is independently connected. In the GCN model input, p varies with n and can also be viewed as a function of n. The average degree of a node in this model is pn. For the Spexphormer model, we sample d-regular graphs as input. Node features are drawn from N(0, I128), where I128 is the 128-dimensional identity matrix. 18The results, shown in Figure 7, indicate that except for a constant average degree in the GCN model, the memory and runtime growth rates are higher for the GCN under all other configurations. For sufficiently large and dense graphs, our method proves to be significantly more efficient in both memory and runtime. 10 15 20 25 30 35 40 45 50 Number of Nodes (×1000) 0 1 2 3 4 5Memory Usage (GB) Spexphormer, d=5 Spexphormer, d=10 Spexphormer, d=20 GCN, p = 20 n GCN, p = 4log n n GCN, p = 1 4 n GCN, p = 0.002 (a) 10 15 20 25 30 35 40 45 50 Number of Nodes (×1000) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Runtime (ms) Spexphormer, d=5 Spexphormer, d=10 Spexphormer, d=20 GCN, p = 20 n GCN, p = 4log n n GCN, p = 1 4 n GCN, p = 0.002 (b) Figure 7: The memory and runtime comparison between our model and the GCN demonstrates that our model, with sparsification, significantly outperforms even a very simple GCN model on a forward pass. C.4 Accuracy, Memory, and Runtime with Sampling Degree For four datasets—Tolokers, Minesweeper, Amazon-Photo, and Coauthor-CS—we analyze how accuracy, memory usage, and runtime change as the sampling degree is varied. In this experiment, all hyperparameters are fixed except for the sampling degree degℓ, which is kept consistent across all layers to simplify the analysis. The results are shown in Figure 8, where we plot both the Accuracy/AUC results and the memory/runtime metrics. For more heterophilic datasets, larger neighborhood sampling generally improves performance; however, the improvement becomes marginal beyond a certain point, while memory usage and runtime continue to increase linearly. For homophilic datasets, a very small neighborhood size is sufficient, and increasing the neighborhood size further does not provide noticeable benefits. D Experiment Details D.1 Hyperparameters In our networks, we use a higher expander degree than what was used in the EXPHORMER paper. Since many of these edges will get a small attention score, a higher attention score increases the receptive field of the nodes, letting the final network be able to sample from wider options and have better access to long-range dependencies. We also noticed, the attention scores in the first layer are usually more flat than the other layers and so we usually sample more edges from the first attention layer. For the comparisons both on the results and the memory we have given the same expander degree to the Exphormer and the ogbn-arxiv dataset could barely fit into a 40GB GPU memory device with higher expander degree. For the attention score estimator network, we do not use dropout, and we only use one attention head in these networks. The number of layers is always equal between both networks. We use AdamW optimization algorithm in all our networks and use a cosine learning rate scheduler with it. We use weight decay of 1e −3 in all networks. We use layer norm in attention score estimator networks to keep attention scores more meaningful, but use a batch norm for better results in the final SPEXPHORMER model. Other key hyperparameters can be found in Tables 7 to 9. 191 3 5 7 10 15 Sampled Graph Node Degree 76 77 78 79 80 81 82 83 84AUC (a) Tolokers AUC 1 3 5 7 10 15 Sampled Graph Node Degree 0 100 200 300 400 500 600 700Memory Usage (MB) (b) Tolokers memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.00 0.05 0.10 0.15 0.20Runtime(s) (c) Tolokers runtime 1 3 5 7 10 15 Sampled Graph Node Degree 80 82 84 86 88 90 92AUC (d) Minesweeper AUC 1 3 5 7 10 15 Sampled Graph Node Degree 0 100 200 300 400 500 600Memory Usage (MB) (e) Minesweeper memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200Runtime(s) (f) Minesweeper runtime 1 3 5 7 10 15 Sampled Graph Node Degree 93.5 94.0 94.5 95.0 95.5 96.0 96.5Accuracy (g) Photo accuracy 1 3 5 7 10 15 Sampled Graph Node Degree 0 100 200 300 400 500 600 700 800Memory Usage (MB) (h) Photo memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30Runtime(s) (i) Photo runtime 1 3 5 7 10 15 Sampled Graph Node Degree 94.2 94.4 94.6 94.8 95.0 95.2Accuracy (j) CS accuracy 1 3 5 7 10 15 Sampled Graph Node Degree 0 500 1000 1500 2000Memory Usage (MB) (k) CS memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.0 0.2 0.4 0.6 0.8Runtime(s) (l) CS runtime Figure 8: AUC and accuracy results, along with memory and runtime analysis, are presented for four datasets: two homophilic datasets (Amazon-Photo and Coauthor-CS) and two heterophilic datasets (Tolokers and Minesweeper). Larger sampling degrees generally lead to better results; however, for the homophilic datasets, even a very small neighborhood size can yield substantial performance. Increasing the sampling degree increases memory and runtime requirements accordingly. D.2 Hardware For all trainings of the medium-sized graph datasets and the final network training of the large-sized graphs, we used GPUs of type A100 with 40GB memory, and V100, both 32GB and 16GB versions. While these are powerful GPUs, we have always monitored the GPU memory usage for computational efficiency, ensuring that no more than 8GB is used for whole graph training and no more than 4GB of GPU memory is used with batching. Training with even less memory is feasible with smaller batch sizes. 20Table 7: Hyperparameters used for training the networks for homophilous datasets. Hyperparameter OGBN-Arxiv Computer Photo CS Physics WikiCS Attention Score Estimator L 3 4 4 4 4 4 ds 8 4 4 4 4 4 Num Epochs 200 200 200 200 200 100 Learning Rate 0.01 0.1 0.001 0.002 0.001 0.1 Final Spexphormer Network dl 96 80 56 64 64 64 degℓ [6, 6, 6] [5, 5, 5, 5] [5, 5, 5, 5] [5, 5, 5, 5] [5, 5, 5, 5] [8, 5, 5, 5] Number of Heads 2 2 2 2 2 2 Learning Rate 0.01 0.001 0.01 0.002 0.001 0.001 Num Epochs 600 150 100 120 80 100 Dropout 0.3 0.5 0.5 0.4 0.4 0.5 Table 8: Hyperparameters used for training the networks for heterophilic datasets. HyperparameterActor Minesweeper Tolokers Roman-Empire Amazon-ratings Questions Attention Score Estimator L 3 4 4 4 4 4 ds 4 4 4 4 4 4 Num Epochs 100 100 200 200 100 100 Learning rate 0.01 0.01 0.01 0.01 0.01 0.01 Final Spexphormer Network dl 32 32 32 40 64 32 degℓ [2, 2, 2] [12,5,5,5] [12, 10, 10, 10] [12, 10, 10, 10] [8, 5, 5, 5] [5, 5, 5, 5] Number of Heads 4 4 4 2 2 2 Learning Rate 0.01 0.01 0.01 0.03 0.01 0.01 Num Epochs 100 80 200 200 200 80 Dropout 0.5 0.2 0.25 0.1 0.1 0.5 For calculating the attention scores on the large graph datasets, we have used CPU devices Intel Xeon E5-2680 v4, with 500GB of memory. Except for the Amazon2M dataset, for the other datasets 200GB of memory would be sufficient. E Theory In this section, we theoretically analyze the compressibility of the Graph Transformer architecture and also sparsification guarantees using the attention score estimator network. For simplification, we use the following formulation of a single head Transformer network: h(ℓ+1/2) i = degiX j=1 a(l) ij V(ℓ) j , h(ℓ+1) i = W(ℓ) 2 \u0010 σ \u0010 W(ℓ) 1 \u0010 h(ℓ+1/2) i \u0011\u0011\u0011 , a(l) ij = exp \u0010 K(ℓ) j · Q(ℓ) i \u0011 P u∈NH(i) exp \u0010 K(ℓ) u · Q(ℓ) i \u0011, where, V(ℓ) = W(ℓ) V h(ℓ), Q(ℓ) = W(ℓ) Q h(ℓ), K(ℓ) = W(ℓ) K h(ℓ), and σ can be any 1-Lipchitz activation function, such as ReLU, which has been used in practice in our networks. We re- move the normalization parts from the architecture but assume that in all steps for all vectors, ∥Xi∥2, ∥h(ℓ+1/2) i ∥2, ∥h(ℓ) i ∥2 ≤ √α, and all linear mapping W· matrices’ operator norm is bounded by a constant β. The first assumption is realistic because of the layer-norm applied between the layers in real-world architectures. The second assumption is also justified as the operator norms are near 2 21Table 9: Hyperparameters used for training the networks for the large graphs datasets. Hyperparameter ogbn-proteins Amazon2M Pokec Attention Score Estimator L 2 2 2 ds 8 8 8 expander degree 200 30 30 Num Epochs 150 150 150 Learning rate 0.01 0.01 0.01 Final Spexphormer Network dl 64 128 64 degℓ [50, 30] [10,10] [20, 20] Number of Heads 1 1 1 Learning Rate 0.005 0.001 0.01 Num Epochs 200 200 300 Dropout 0.1 0.2 0.2 Batch size 256 1000 500 GPU Memory 2232MB 3262MB 2128MB in the initialization of the network by the default PyTorch initialization and during the optimization we expect the operator norm to not increase drastically from the initialization. Also, we assume h(0) = X, which is the input features. For a simpler notation, we will use D for a hypothetical large network hidden dimension in this analysis, and d is the hidden dimension of the narrow network. For simplicity, in our analysis, we assume X ∈ Rn×D. In case each node has less than D features, we can concatenate them with zeros. E.1 On the Compressibility of the Graph Transformer Our approach uses a narrow network to estimate the attention scores. We want to show if we have a large network with good accuracy for a task on a graph, we can have a less complex network that can work on the same input graph and the error of this network is bounded by O(ε) from the large network. The most memory/time-intensive part of a Transformer architecture is its attention score calculation part. The rest of the sections are node/token-wise and linear with respect to the number of nodes. The attention score estimation part of a full-Transformer layer requires O(n2d) operations and O(md) operators are required for a sparse Transformer with m attention edges. In the main Exphormer network, this would also be more intensive as the edge features mappings requireO(md2) operations, but since we replace edge feature mappings with edge embeddings by their type, this part in case we do not have other edge features is O(md), but m still can be ω(n), and it will be the most computationally-intensive part. Assume we have a large network with L layers, where L is O(1), and hidden dimension D, we will show that there is a similar network with L layers where the attention score calculation matrices WQ, WK ∈ RD×d, and all other matrices are of the same size and d is O(CL log n ϵ2 ), where C is a constant based on α and β. For this proof we use the distributional Johnson-Lindenstrauss transform lemma (Johnson, 1984): Lemma E.1 (Johnson-Lindenstrauss Transform Lemma ( JLT)). Assume 0 < ϵ, δ <1 2 and any positive integer D, if d = O(log(1/δ) ϵ2 ), there exist a distribution over matrices M ∈ Rd×D that for any x ∈ RD and ∥x∥ = 1: Pr(∥Mx∥ −1 > ϵ) < δ The following corollary is an immediate conclusion from the JLT. Corollary E.2. Assume 0 < ϵ, δ <1 2 and any positive integer D, if d = O(log(1/δ) ε2 ), there exist a distribution over matrices M ∈ Rd×D that for any x, y∈ RD: 22Pr((1 − ε)∥x − y∥ < ∥Mx − My∥ < (1 + ε)∥x − y∥) < δ This can derived by replacing x from JLT with x−y ∥x−y∥. From this, we can derive another corollary about the dot product of the vectors in low-dimensional space. Corollary E.3 (JLT-dot product). Assume 0 < ϵ, δ <1 2 and any positive integer D, if d = O(log(1/δ) ε2 ), there exist a distribution over matrices M ∈ Rd×D that for any x, y∈ RD, and ∥x∥, ∥y∥ ≤√α: Pr((1 − εα)xTy < xTMTMy <(1 + εα)xTy) < δ For the proof see (Kakade and Shakhnarovich, 2009, Corollary 2.1). As a result of this corollary, if we have m pairs of vectors (xi, yi), and for each i ∥xi∥2, ∥yi∥2 ≤ √α of √α, and d = O(log(m) ε2 ), there exists an M such that for all these pairs |xT i MTMyi − xT i yi| < εα. The proof can be done using a union bound over the error from Corollary E.3. Also, in our case where m is the number of edges, we know that m ≤ n2, thus we can also say d = O(log(n) ε2 ). Theorem E.4. Assume we have a Transformer network T with arbitrary large hidden dimension D, L = O(1) layers, and in this network, in all layers, we have ∥h·∥2 ≤ √α, and ∥W·∥op ≤ β. There exists a Transformer bT , that for any layer WQ and WK are in Rd×D for a d = O(log n ε2 ), with a sufficiently small ε, and for all i ∈ [n], ∥T (X)i − bT (X)i∥2 = O(ε). And furthermore, for any attention score a(ℓ) ij ba(ℓ) ij = 1 + O(ε). Proof. In the proof we use hat notation, b□, for the vectors and matrices from bT , for example, ˆh(ℓ) are the outputs of layer ℓ, and cW· are the weight matrices for this network. In all layers for both networks WV , W1, and W2, are of the same size, so we set cWV = WV , cW1 = W1, and cW2 = W2. For the proof, we want to findε(0), ··· , ε(L) in a way that for anyv in layer ℓ, |h(ℓ) v −ˆh(ℓ) v | < ε(ℓ). We will find these bounds inductively, starting from the first layer. We haveε(0) = 0, as both networks have the same input, and we want to bound ε(ℓ+1) based on ε(ℓ). We have Q(ℓ) = W(ℓ) Q H(ℓ), K(ℓ) = W(ℓ) K H(ℓ) and assume ¯Q(ℓ) = W(ℓ) Q bH(ℓ), ¯K(ℓ) = W(ℓ) K bH(ℓ). Because of the operator norm of matricesWQ and WK, for each i we have ∥q(ℓ) i −¯q(ℓ) i ∥ ≤ε(ℓ)β and ∥k(ℓ) i − ¯k(ℓ) i ∥ ≤ε(ℓ)β. Also, we have ∥q(ℓ) i ∥, ∥k(ℓ) i ∥ ≤β√α, thus ∥¯q(ℓ) i ∥, ∥¯k(ℓ) i ∥ ≤β(ε(ℓ) + √α). Now, for each pair of i and j, we have: |q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · ¯k(ℓ) j | = |q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · k(ℓ) j + ¯q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · ¯k(ℓ) j | ≤ |q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · k(ℓ) j | + |¯q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · ¯k(ℓ) j | = |(q(ℓ) i − ¯q(ℓ) i ) · k(ℓ) j | + |¯q(ℓ) i · (k(ℓ) j − ¯k(ℓ) j )| ≤ ∥q(ℓ) i − ¯q(ℓ) i ∥∥k(ℓ) j ∥ + ∥¯q(ℓ) i ∥∥k(ℓ) j − ¯k(ℓ) j ∥ ≤ √αβε(ℓ) + (√α + βε(ℓ))βε(ℓ) = 2√αβε(ℓ) + (βε(ℓ))2 On the other hand, according to the E.3, for a 0 < ε < 1/2 and d = O(log(n) ε2 ) there exists a matrix MQK ∈ Rd×D, such that if we define bQ(ℓ) = MQK ¯Q(ℓ) and bK(ℓ) = MQK ¯K(ℓ), |¯q(ℓ) i · ¯k(ℓ) j − ˆq(ℓ) i · ˆk(ℓ) j | < β2(α + (ε(ℓ))2 + 2√αε(ℓ))ε for all (i, j) pairs in the attention pattern. Note that we can define cW(ℓ) Q = M(ℓ) QKW(ℓ) Q , and cW(ℓ) K = M(ℓ) QKW(ℓ) K , both in Rd×D, as weights 23for the narrow attention score estimator network. With a triangle inequality we have |q(ℓ) i · k(ℓ) i − ˆq(ℓ) i · ˆk(ℓ) i | < β2(α + (ε(ℓ))2 + 2√αε(ℓ))ε + 2√αβε(ℓ) + (βε(ℓ))2. By setting ε(ℓ) ≤ 1, we have |q(ℓ) i · k(ℓ) i − ˆq(ℓ) i · ˆk(ℓ) i | < β2(α + 1 + 2√α)ε + β(2√α + β)ε(ℓ). Let us define εa = β2(α + 1 + 2√α)ε + β(2√α + β)ε(ℓ), we have: ba(ℓ) ij = exp(ˆq(ℓ) i · ˆk(ℓ) j ) P u∈NH(i) exp(ˆq(ℓ) i · ˆk(ℓ) u ) ≤ exp(q(ℓ) i · k(ℓ) j + εa) P u∈NH(i) exp(q(ℓ) i · k(ℓ) j − εa) ≤ a(ℓ) ij exp(2εa) ba(ℓ) ij = exp(ˆq(ℓ) i · ˆk(ℓ) j ) P u∈NH(i) exp(ˆq(ℓ) i · ˆk(ℓ) u ) ≥ exp(q(ℓ) i · k(ℓ) j − εa) P u∈NH(i) exp(q(ℓ) i · k(ℓ) u + εa) ≥ a(ℓ) ij exp(−2εa) Now we bound ∥h(ℓ+1/2) i − ˆh(ℓ+1/2) i ∥: ∥h(ℓ+1/2) i − ˆh(ℓ+1/2) i ∥ = ∥ X j∈Nei(i) a(ℓ) ij v(ℓ) j − baij ˆv(ℓ+1/2) j ∥ = ∥ X j∈Nei(i) a(ℓ) ij v(ℓ) j − ba(ℓ) ij v(ℓ) j + ba(ℓ) ij v(ℓ) j − baij ˆv(ℓ) j ∥ = ∥ X j∈Nei(i) (a(ℓ) ij − ba(ℓ) ij )v(ℓ) j + ba(ℓ) ij (v(ℓ) j − ˆv(ℓ) j )∥ = ∥(v(ℓ) j − ˆv(ℓ) j ) + v(ℓ) j X j∈Nei(i) (a(ℓ) ij − ba(ℓ) ij )∥ ≤ ∥v(ℓ) j − ˆv(ℓ) j ∥ + ∥v(ℓ) j ∥ X |a(ℓ) ij − ba(ℓ) ij | ≤ ε(ℓ)β + √α X max(1 − exp(−2εa), exp(2εa) − 1)a(ℓ) ij ≤ ε(ℓ)β + √α(exp(2εa) − 1), and since 1 + x <exp(x) < 1 + 2x for 0 < x <1, if we have εa < 1, we have ∥h(ℓ+1/2) i − ˆh(ℓ+1/2) i ∥ ≤βε(ℓ) + 4√αεa (1) For the feed-forward network part, we know that this network is β2-Lipschitz because W(ℓ) 1 and W(ℓ) 2 have maximum operator norm β and σ is a 1-Lipschitz activation function. Thus we have ∥h(ℓ+1) i − ˆh(ℓ+1) i ∥ ≤β2(βε(ℓ) + 4√αεa) = (β3 + 8βα + 4β2√α)ε(ℓ) + 4β2(α√α + 2α + √α)ε. Both β3 + 8βα + 4β2√α and 4β2(α√α + 2α + √α) are constants, and if we define them as c1 and c2, we have ε(ℓ+1) ≤ c1ε(ℓ) + c2ε Given ε(0) = 0, as both networks get the same input, we have ε(L) ≤ c1ε(L−1) + c2ε ≤ c1(c1ε(L−2) + c2ε) + c2ε ··· ≤ c2ε(cL−1 1 + ··· + c1) = c1(cL 2 − 1) c2 − 1 ε 24While the error increases exponentially with the number of layers, when we have L = O(1), then the error is bounded by a constant factor of chosen ε. Now, we know that ∥T (X)i − bT (X)i∥2 ≤ ε(L) = O(ε). While from the theorem it seems that the error is increasing exponentially by the layers, in practice the maximum number of layers used in this work is four with most large graph experiments using just two layers. Thus the constant factor will not be as large as it might look. Also, in real-world graphs usually, the columns of X are not quite n distinct vectors and many vectors would be equal or very similar to each other if we have κ unique vectors in the first layer the complexity for the d can be reduced to O(log κ ε2 ). In the homophily graphs the representations h(ℓ) tend to converge to each other and thus again the number of unique vectors will be reduced letting us have smaller d, but these assumptions are not considered in the proof as we keep it general. Although we have proved the existence of the bT , this does not mean that training with a gradient- based algorithm will necessarily lead to the introduced weights, but this gives at least the guarantee that such a network exists. However, on the other hand, it is also possible that the training process finds a set of weights that work better than the weights constructed in this proof. Theorem E.4, by narrowing the attention score calculation part, reduced the complexity fromO(mD+ nD2) to O(md + nD2), and for dense graphs or in scenarios we add denser expander graphs, where m ≫ n, already the introduced network has a much lower complexity. However, our narrow network uses narrow hidden dimensions in all steps and has complexity O(md + nd2). Proving the same guarantee along the whole network is not easy, if not impossible, without any further assumptions on X and the large network. Shirzad et al. (2024) explores these settings further, in the presence of various additional assumptions. E.2 Analysis of the Sampling Process After training a network with a smaller width d, we sample the edges from the original graph and use them in the second-phase training with a large hidden width D. In this section, we shall analyze our sampling process. Formally, we model our process as follows. Suppose that A is the attention score matrix with hidden width D, then we sample and rescale s entries of A to form a sparse matrix B where the goal is the matrix B can approximate A well, i.e., ∥A − B∥2 ≤ ε∥A∥2. However, recall that we can not access the entries of A precisely. Instead, we consider another attention score matrix A′, which corresponds to hidden width d. The first question is how many samples we indeed need to form the matrix B that approximates A well? To answer this, we have the following lemma for the attention score matrix A. Theorem E.5. Suppose that an n × n matrix A satisfies the following conditions: 1. For each i, we have ∥A(i)∥1 = 1. 2. maxj∥A(j)∥1 = K 3. Each column A(j) is ℓ-sparse. Then, consider the sampling procedure that samples s ≥ s0 = O(nK log n/(ε2∥A∥2 2)) = O(nℓ log n/(ε2K)) entries of A with replacement: 1. For each sampleBt, the probability thatBt samples entry Aij is pij = 1 n · |Aij| ∥A(i)∥1 = 1 n |Aij| (with a rescale factor 1/pij, i.e., Bt[i, j] = Aij/pij), and each Bt only samples one entry of A. 2. Form the matrix B = (B1 + B2 + ··· + Bs)/s. Then, we have that with probability at least 9/10, ∥A − B∥2 ≤ ε∥A∥2. To prove this lemma, we need the following matrix Bernstein inequality. 25Lemma E.6 (Matrix Bernstein inequality). Consider a finite sequence Xi of i.i.d. random m × n matrices, with E[Xi] = 0 and Pr(∥Xi∥2 ≤ R) = 1. Let σ2 = max{∥E[XiXT i ]∥2, ∥E[XT i Xi]∥2}. For some fixed s ≥ 1, let X = (X1 + X2 + ··· + Xs)/s, then we have that Pr[∥X∥2 ≥ ε] ≤ (m + n) · exp \u0012 sε2 −σ2 + Rε/3 \u0013 . Proof. We follow a similar proof strategy to that of Achlioptas et al. (2013). At a high level, the work of Achlioptas et al. (2013) considers the matrix Bernstein inequality, whose tail bound is dependent on the following two quantities: σ2 = max{∥E[(A − B1)(A − B1)T ]∥, ∥E[(A − B1)T (A − B1)]∥} and R = max∥A − B1∥ over all possible realizations of B1. Here B1 is the matrix that only samples one entry, and the final output isB = (B1 +B2 +··· +Bs)/s. Instead, we consider the following quantities, ˜σ2 = max   max i X j A2 ij/pij, max j X i A2 ij/pij    ˜R = max ij |Aij|/pij. It is shown in Lemma A.2 of Achlioptas et al. (2013) that |σ/˜σ − 1| ≤ ∥A∥2 2P i ∥A(i)∥2 1 and |R/ ˜R − 1| ≤ ∥A∥2 ∥A∥1 . From our condition on the matrix A, both of the upper bounds are at most 1. Hence, we only need to consider ˜σ and ˜R. Back to our case, we have that pij = 1 n · |Aij| ∥A(1)∥1 = 1 n · |Aij|, from this and the assumption of A we have ˜σ2 = n · max   max i X j |Aij|, max j X i |Aij|    ≤ n · K ˜R = max ij |Aij|/pij = n. Hence, to make δ ≤ 0.1, we only need to set ε′ = ε∥A∥2 in the Matrix Bernstein inequality and then we have s ≥ O(nK log n/(ε2∥A∥2 2)). Finally, note that if ∥A(j)∥1 = K, then we have ∥A∥2 ≥ ∥Aej∥2 = ∥A(j)∥2 ≥ K/ √ ℓ, which means that nK log n/(ε2∥A∥2 2) ≤ nℓ log n/(ε2K). However, as mentioned, we can not access the value of the entries ofA but the entries of A′ (which corresponds to the trained network with a small hidden width d). We next show that even in the case where we sample the entries of A from A′, we can still get the same order of the bound if the entries of A are not under-estimated seriously in A′. Proposition E.7. Suppose that the matrices A and A′ satisfy the condition in Theorem E.5 and for every i, jwe have |A′ ij| ≥1 α|Aij| for some sufficiently large constant α. Then consider the same sampling procedure in Theorem E.5 but sampling the entries of A from the value of A′. Then, the guarantee in Theorem E.5 still holds. Proof. We only need to note that from the assumption, the actual sampling probability p′ ij ≥ 1 α · pij in Theorem E.5, hence it will increase the ˜σ2 and ˜R by at most α times, which means that we can increase s by an α factor to make the error probability at most 0.1. 26F Attention Score Analysis In Figure 3, we observed that the attention scores are relatively close to the reference attention scores. In this section, we provide further details on these experiments and offer additional analysis of the attention scores. For our experiments, we used an implementation of the Exphormer model with normalization on V mappings and temperature adjustment for the attention scores. For each random seed, we selected the model with the best result on the validation set. We used an expander degree of 10 for the Actor dataset and 30 for Amazon-Photos. The difference in expander degrees is due to the significant variation in the average degree of nodes across the datasets. We aimed to balance the number of expander edges and graph edges since it has an impact on some of the experiments. In addition to the expander edges, we also included self-loops, which are necessary for the universal approximation theorem outlined by Shirzad et al. (2023). All networks in these experiments were trained with four layers. For each hidden dimension, we ran 100 experiments with different initializations. The learning rate was adjusted for each hidden dimension to ensure more stable convergence. However, for smaller hidden dimensions, some experiments led to drastically lower accuracy results, which we did not exclude from the analysis. All results, including those with lower accuracy, were considered in our analysis. F.1 Preliminaries Before presenting further experimental results, we provide a brief introduction to the metrics used. For two random variables X ∼ Pand Y ∼ Q, both defined in Rd (or equivalently, defined by their cumulative distribution functions (CDFs) F and G), we can define the following metrics: Energy Distance Energy distance is a metric used to measure the distance between two distributions (Székely and Rizzo, 2013; Sejdinovic et al., 2013; Rizzo and Székely, 2016), and is defined as: D2(F, G) = 2E[X − Y ] − E[X − X′] − E[Y − Y ′], where X, X′ ∼ Pand Y, Y′ ∼ Q, with all variables being independent of each other. This value is shown to be twice the Harald Cramer’s distance (Cramér, 1928), which is defined as: Z (F(x) − G(x))2 dx. This metric is non-negative; however, an unbiased estimator based on samples may yield negative results. Although the energy distance is a useful metric for identifying the distance between two probability distributions, it may not fully capture the variations between them. This issue becomes particularly relevant when measuring the performance of generative models, as it helps assess whether the generative model correctly approximates the real distribution. The following pairs of metrics provide finer-grained understanding of two different types of approximation. Precision & Recall (Sajjadi et al., 2018) These metrics assess generative models by constructing a manifold for both real and generated data. This is done by forming a hypersphere around each data point, extending its radius to the k-th nearest neighbor, and then aggregating these hyperspheres. Precision measures the proportion of generated samples that fall within the real data manifold, while recall quantifies the fraction of real samples covered by the generated data manifold. These metrics correlate well with human judgments in the visual domain and are effective in detecting issues like mode collapse and mode dropping. Density & Coverage (Naeem et al., 2020) These metrics, similar to Precision and Recall, evaluate generative models by considering individual hyperspheres rather than aggregating them into a manifold. Density measures the average number of real hyperspheres that each generated sample falls into, while Coverage quantifies the fraction of real samples that fall into at least one generated hypersphere. These metrics have been shown to be more robust than the Precision and Recall metrics in certain scenarios. 274 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.003 0.017 0.013 0.020 0.017 0.163 0.003 0.000 0.009 0.006 0.012 0.029 0.145 0.017 0.009 0.000 0.002 0.003 0.054 0.108 0.013 0.006 0.002 0.000 0.003 0.050 0.121 0.020 0.012 0.003 0.003 0.000 0.059 0.109 0.017 0.029 0.054 0.050 0.059 0.000 0.219 0.163 0.145 0.108 0.121 0.109 0.219 0.000 Actor Dataset without Expanders 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.004 0.008 0.018 0.187 0.115 0.000 0.000 0.000 0.007 0.018 0.200 0.102 0.004 0.000 0.000 0.001 0.009 0.214 0.120 0.008 0.007 0.001 0.000 0.017 0.208 0.125 0.018 0.018 0.009 0.017 0.000 0.228 0.154 0.187 0.200 0.214 0.208 0.228 0.000 0.360 0.115 0.102 0.120 0.125 0.154 0.360 0.000 Actor Dataset with Expanders 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.028 0.004 0.067 0.037 0.027 0.197 0.028 0.000 0.029 0.035 0.043 0.095 0.108 0.004 0.029 0.000 0.057 0.025 0.039 0.203 0.067 0.035 0.057 0.000 0.024 0.144 0.137 0.037 0.043 0.025 0.024 0.000 0.091 0.198 0.027 0.095 0.039 0.144 0.091 0.000 0.306 0.197 0.108 0.203 0.137 0.198 0.306 0.000 Amazon-Photo Dataset without Expanders 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.009 0.008 0.040 0.068 0.225 0.095 0.009 0.000 0.022 0.080 0.116 0.217 0.065 0.008 0.022 0.000 0.024 0.053 0.276 0.124 0.040 0.080 0.024 0.000 0.011 0.340 0.225 0.068 0.116 0.053 0.011 0.000 0.362 0.271 0.225 0.217 0.276 0.340 0.362 0.000 0.208 0.095 0.065 0.124 0.225 0.271 0.208 0.000 Amazon-Photo Dataset with Expanders 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (d) Figure 9: Pairwise energy distance across networks with different hidden dimensions, uniform distribution, and randomly generated attention scores. For all these metrics, we first consider the distribution of attention scores for each individual node’s neighborhood in a single layer, trained with a specific hidden dimension, represented as a vector. We then compare these distributions across different hidden dimensions or among different layers. Finally, we average the results over all nodes. F.2 Pairwise Distances While we demonstrated the energy distances from the reference hidden dimension of 64 in Figure 3, it is also valuable to examine all pairwise distances. We present these pairwise distances in Figure 9. Additionally, these distances may vary layer by layer, so it is insightful to explore how these distances change across different layers of the network. To this end, we provide the results in Figures 10 to 13. These experiments consistently show that attention scores obtained from different hidden dimension sizes are close to each other. In contrast to uniform sampling, or randomly generated attention scores, this distribution provides a much better reference for drawing neighborhood samples when the goal is to select nodes based on their attention score importance. ♂lightbulbInsight 1 Attention scores from a network with a smaller hidden dimension serve as a good estimator for the attention scores in a network with a higher hidden dimension. 284 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.218 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.219 0.219 0.219 0.219 0.218 0.219 0.000 Actor Dataset without Expanders, Layer 0 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.004 0.020 0.020 0.029 0.022 0.148 0.004 0.000 0.009 0.009 0.016 0.039 0.121 0.020 0.009 0.000 0.002 0.004 0.064 0.078 0.020 0.009 0.002 0.000 0.003 0.069 0.086 0.029 0.016 0.004 0.003 0.000 0.078 0.069 0.022 0.039 0.064 0.069 0.078 0.000 0.219 0.148 0.121 0.078 0.086 0.069 0.219 0.000 Actor Dataset without Expanders, Layer 1 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.004 0.026 0.016 0.028 0.021 0.148 0.004 0.000 0.011 0.005 0.014 0.040 0.120 0.026 0.011 0.000 0.003 0.003 0.073 0.071 0.016 0.005 0.003 0.000 0.004 0.060 0.092 0.028 0.014 0.003 0.004 0.000 0.077 0.071 0.021 0.040 0.073 0.060 0.077 0.000 0.219 0.148 0.120 0.071 0.092 0.071 0.219 0.000 Actor Dataset without Expanders, Layer 2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.002 0.023 0.015 0.024 0.026 0.136 0.002 0.000 0.014 0.008 0.017 0.039 0.121 0.023 0.014 0.000 0.003 0.006 0.080 0.066 0.015 0.008 0.003 0.000 0.005 0.070 0.086 0.024 0.017 0.006 0.005 0.000 0.080 0.075 0.026 0.039 0.080 0.070 0.080 0.000 0.219 0.136 0.121 0.066 0.086 0.075 0.219 0.000 Actor Dataset without Expanders, Layer 3 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200  (d) Figure 10: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Actor dataset, without the expander graph, on individual layers. This dataset has a very low average degree and it appears that almost always the first layer’s attention scores are very similar to the uniform distribution. F.3 Entropy of Attention Scores In this experiment, we analyze the entropy of the attention scores to examine how they change across layers. When the scores approach a one-hot vector, we refer to them as sharp attentions, while more smooth scores resemble a uniform distribution over the neighbors. The goal is to assess how sharp or smooth the attention scores are, on average, across the nodes. To achieve this, we use the entropy metric. Higher entropy indicates more smooth attention scores, while entropy is zero for one-hot vectors. We calculate the entropy for each node’s neighborhood and then average the entropies across all nodes and all random seeds in the layer. The results are presented in Figure 14. An insightful observation from this experiment is that the first layer, across all four datasets, con- sistently exhibits smoother attention scores, while the scores become sharper in subsequent layers. Generally, however, the attention scores are not very sharp in experiments without expander graphs, suggesting that all neighbors are likely similarly informative. This does not necessarily imply that all these nodes are equally important. If identical nodes with the same neighborhoods surround a node, all of them will receive equal attention scores, which indicates no selection in this case. Thus, this does not contradict the idea that a sparse matrix can estimate the same results. Sharpness varies across different hidden dimensions, which may be due to factors such as training dy- namics, learning rate, and the varying temperature setup for different hidden dimensions. Regardless, in all datasets and across all hidden dimensions, the first layer consistently has higher entropy. This 294 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.009 0.027 0.035 0.168 0.153 0.000 0.000 0.001 0.015 0.029 0.147 0.159 0.009 0.001 0.000 0.003 0.013 0.139 0.185 0.027 0.015 0.003 0.000 0.014 0.111 0.216 0.035 0.029 0.013 0.014 0.000 0.187 0.228 0.168 0.147 0.139 0.111 0.187 0.000 0.360 0.153 0.159 0.185 0.216 0.228 0.360 0.000 Actor Dataset with Expanders, Layer 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.005 0.004 0.010 0.175 0.104 0.000 0.000 0.000 0.000 0.014 0.209 0.082 0.005 0.000 0.000 0.000 0.009 0.229 0.099 0.004 0.000 0.000 0.000 0.013 0.218 0.090 0.010 0.014 0.009 0.013 0.000 0.245 0.125 0.175 0.209 0.229 0.218 0.245 0.000 0.360 0.104 0.082 0.099 0.090 0.125 0.360 0.000 Actor Dataset with Expanders, Layer 1 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.001 0.001 0.004 0.017 0.214 0.100 0.001 0.000 0.002 0.018 0.013 0.205 0.087 0.001 0.002 0.000 0.007 0.009 0.244 0.102 0.004 0.018 0.007 0.000 0.035 0.269 0.104 0.017 0.013 0.009 0.035 0.000 0.238 0.134 0.214 0.205 0.244 0.269 0.238 0.000 0.360 0.100 0.087 0.102 0.104 0.134 0.360 0.000 Actor Dataset with Expanders, Layer 2 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.001 0.000 0.000 0.009 0.192 0.101 0.001 0.000 0.000 0.000 0.015 0.239 0.079 0.000 0.000 0.000 0.000 0.003 0.242 0.093 0.000 0.000 0.000 0.000 0.007 0.232 0.089 0.009 0.015 0.003 0.007 0.000 0.243 0.128 0.192 0.239 0.242 0.232 0.243 0.000 0.360 0.101 0.079 0.093 0.089 0.128 0.360 0.000 Actor Dataset with Expanders, Layer 3 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (d) Figure 11: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Actor dataset, with the expander graph, on individual layers. suggests that for sampling, larger neighborhood sizes may be needed in the first layer, while smaller neighborhood sizes could suffice in the subsequent layers. ♂lightbulbInsight 2 Attention scores are smoother in the first layer, and become sharper in subsequent layers. F.4 Inter-layer Attention Scores Similarity After observing that the entropy is higher in the first layer and similar across the subsequent layers, it is worth examining the distance between the attention scores of each pair of layers. The experimental results are presented in Figure 15. All values are relatively small compared to the previous ones, so they are multiplied by 100 for better presentation. The results show that, consistently, the first layer has some distance from all other layers, but the layers following it exhibit very similar attention scores. This suggests that the initial network may be trained using fewer layers, and further layer sampling could be achieved by repeating the attention scores from the final layer to train a deeper Spexphormer model. 304 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.004 0.002 0.038 0.016 0.012 0.239 0.004 0.000 0.012 0.022 0.013 0.029 0.207 0.002 0.012 0.000 0.049 0.021 0.006 0.270 0.038 0.022 0.049 0.000 0.010 0.079 0.197 0.016 0.013 0.021 0.010 0.000 0.040 0.213 0.012 0.029 0.006 0.079 0.040 0.000 0.306 0.239 0.207 0.270 0.197 0.213 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.031 0.004 0.075 0.041 0.034 0.176 0.031 0.000 0.030 0.033 0.047 0.114 0.074 0.004 0.030 0.000 0.061 0.026 0.050 0.175 0.075 0.033 0.061 0.000 0.031 0.170 0.094 0.041 0.047 0.026 0.031 0.000 0.108 0.181 0.034 0.114 0.050 0.170 0.108 0.000 0.306 0.176 0.074 0.175 0.094 0.181 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 1 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.037 0.005 0.082 0.044 0.031 0.189 0.037 0.000 0.034 0.040 0.054 0.114 0.075 0.005 0.034 0.000 0.062 0.026 0.052 0.183 0.082 0.040 0.062 0.000 0.033 0.171 0.116 0.044 0.054 0.026 0.033 0.000 0.108 0.196 0.031 0.114 0.052 0.171 0.108 0.000 0.306 0.189 0.075 0.183 0.116 0.196 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 2 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.041 0.005 0.072 0.045 0.031 0.183 0.041 0.000 0.040 0.043 0.057 0.125 0.075 0.005 0.040 0.000 0.055 0.028 0.049 0.187 0.072 0.043 0.055 0.000 0.023 0.156 0.142 0.045 0.057 0.028 0.023 0.000 0.108 0.204 0.031 0.125 0.049 0.156 0.108 0.000 0.306 0.183 0.075 0.187 0.142 0.204 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 3 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (d) Figure 12: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Amazon-Photo dataset, without the expander graph, on individual layers. ♂lightbulbInsight 3 The attention scores in the layers after the first are consistently very similar to one another, but distinct from the attention scores in the first layer. F.5 Precision, Recall, Density, & Coverage (PRDC) Since the energy distance may not fully capture how distributions match in some cases, alternative metrics have been proposed, primarily for assessing the performance of generative models (Sajjadi et al., 2018; Naeem et al., 2020). In this work, we apply these metrics by considering the attention scores from the network with a hidden dimension of 64 as the reference distribution, assuming that all other dimensions aim to generate the same distribution. We use violin plots to illustrate the distribution of PRDC values across the nodes in each layer. The results are presented in Figures 16 to 19. The plots show the kernel density estimate of the corresponding metrics across all nodes, layers, and random initializations. Precision & Recall and Density & Coverage are pairs of metrics that together describe how well the distribution has been learned. Excelling in just one of these metrics does not necessarily imply that the samples are close to each other. As shown in the results, attention scores from other hidden dimensions consistently achieve high values across all metrics, while uniform distribution and random attention scores fall short in at least one of the metrics from each pair. 314 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.009 0.014 0.042 0.064 0.241 0.146 0.009 0.000 0.041 0.079 0.104 0.226 0.122 0.014 0.041 0.000 0.008 0.020 0.327 0.232 0.042 0.079 0.008 0.000 0.004 0.396 0.307 0.064 0.104 0.020 0.004 0.000 0.437 0.348 0.241 0.226 0.327 0.396 0.437 0.000 0.208 0.146 0.122 0.232 0.307 0.348 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.012 0.004 0.036 0.060 0.229 0.088 0.012 0.000 0.015 0.087 0.117 0.222 0.042 0.004 0.015 0.000 0.031 0.058 0.260 0.087 0.036 0.087 0.031 0.000 0.012 0.327 0.205 0.060 0.117 0.058 0.012 0.000 0.324 0.230 0.229 0.222 0.260 0.327 0.324 0.000 0.208 0.088 0.042 0.087 0.205 0.230 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 1 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.003 0.006 0.037 0.071 0.212 0.074 0.003 0.000 0.007 0.054 0.098 0.220 0.058 0.006 0.007 0.000 0.024 0.063 0.258 0.087 0.037 0.054 0.024 0.000 0.015 0.314 0.187 0.071 0.098 0.063 0.015 0.000 0.338 0.246 0.212 0.220 0.258 0.314 0.338 0.000 0.208 0.074 0.058 0.087 0.187 0.246 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 2 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.012 0.008 0.045 0.078 0.216 0.072 0.012 0.000 0.026 0.100 0.145 0.200 0.037 0.008 0.026 0.000 0.031 0.070 0.261 0.090 0.045 0.100 0.031 0.000 0.012 0.323 0.202 0.078 0.145 0.070 0.012 0.000 0.348 0.258 0.216 0.200 0.261 0.323 0.348 0.000 0.208 0.072 0.037 0.090 0.202 0.258 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 3 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (d) Figure 13: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Amazon-Photo dataset, with the expander graph, on individual layers. F.6 Top-k Attention Sum Another way to assess the sharpness of the attention scores is by examining the sum of the top- k attention scores. If the top-k attention scores for a small k almost sum to one for all nodes, then using the top-k scores can closely approximate the representations of the larger network. However, this is not always the case. In this experiment, we analyze the sum of the top- k attention scores for k ranging from one to ten, across all nodes for hidden dimensions of 64 and 4. While the top-k attention score distributions are similar, the assumption that the sum will be close to one is unrealistic and does not occur frequently. The results, shown in Figure 20, include mean, median, and interquartile range, which indicate the spread of the middle 50% of the results. These results suggest that top-k attention selection may not be fully representative in transductive learning on graphs. This could be due to the presence of many similar nodes, causing the attention to be distributed across these nodes rather than being concentrated on a small subset, which affects the ability to approximate the larger network effectively using just the top-k scores. ♂lightbulbInsight 4 The attention scores in the layers after the first are consistently very similar to one another, but distinct from the attention scores in the first layer. 324 8 16 32 64 uniformrandom 0123 0.799 0.799 0.799 0.799 0.799 0.799 0.281 0.767 0.740 0.653 0.682 0.609 0.799 0.280 0.768 0.738 0.629 0.692 0.619 0.799 0.281 0.754 0.738 0.619 0.681 0.624 0.799 0.280 Actor Dataset without Expanders (a) 4 8 16 32 64 uniformrandom 0123 2.100 2.217 2.311 2.464 2.314 2.644 1.060 1.830 1.752 1.749 1.738 1.655 2.644 1.061 1.714 1.786 1.709 1.615 1.675 2.644 1.060 1.768 1.658 1.658 1.665 1.615 2.644 1.060 Actor Dataset with Expanders (b) 4 8 16 32 64 uniformrandom 0123 2.817 2.788 2.855 2.743 2.755 2.879 1.379 2.668 2.224 2.647 2.069 2.572 2.879 1.379 2.695 2.215 2.677 2.158 2.602 2.879 1.379 2.677 2.188 2.679 2.276 2.616 2.879 1.379 Amazon-Photo Dataset without Expanders (c) 4 8 16 32 64 uniformrandom 0123 2.896 2.767 2.882 2.908 2.902 4.015 2.263 2.566 2.263 2.303 2.668 2.856 4.015 2.264 2.596 2.387 2.316 2.635 2.879 4.015 2.263 2.549 2.361 2.323 2.680 2.885 4.015 2.264 Amazon-Photo Dataset with Expanders (d) Figure 14: Average entropy of attention scores for nodes across different layers. F.7 Attention Scores by Edge Type An interesting question is to examine the ratio of attention scores coming from graph edges versus expander edges and self-loops. Figure 21 illustrates how much of the attention score, on average, is attributed to each edge type across different hidden dimensions. We average the values over all nodes in all layers and random initializations. As expected, for a homophilic dataset like Amazon-Photo, the graph edges are more important. As the model’s hidden dimension increases, the model learns to place more attention on these edges. However, for a heterophilic dataset like Actor, the story is different, with graph edges playing a lower role. In Figure 22, we present a normalized version showing the average attention score by edge type. G Discussion Graph datasets arise from various domains, meaning that they might have differing inductive biases. More expressive methods may not necessarily yield better results on all datasets (Franks et al., 2024). Depending on the architecture and the task, more complex models can even lead to poorer results. Here, we discuss possible scenarios in which our model can be a good fit as well as the shortcomings of other classes of models that are overcome by our model. Graph Structure The relevance of the structure of the graph to the task can vary. For the simple synthetic task introduced in 1, the structure of the graph does not matter. So Transformers without inductive biases of the graphs are expressive enough to solve this problem; however message-passing networks will be restricted to the graph edges and rely on enough number of layers and may be challenged by oversquashing and oversmoothing problems. On the other hand, if the structure of the graph matters, such as counting the number of neighbor nodes with the same color for each node, the structure and the edges will be an important part. Transformers without expressive enough encodings to identify the graph edges will fail in this task. On the other hand, MPNNs even with one layer can easily solve this problem. Our approach enables solving problems in either case, by having both expander graphs for universal information propagation and the actual graph edges for inductive bias, 330 1 2 1 2 3 2.146 2.104 0.000 2.641 0.017 0.039 hidden dim = 4 0 1 2 1 2 3 3.910 3.960 0.010 3.865 -0.007 -0.022 hidden dim = 8 0 1 2 1 2 3 6.401 7.322 0.016 7.950 0.059 -0.002 hidden dim = 16 0 1 2 1 2 3 6.916 6.002 0.035 6.940 -0.027 0.025 hidden dim = 32 0 1 2 1 2 3 7.719 7.605 -0.060 7.931 0.067 0.077 hidden dim = 64 (a) Actor Dataset without Expander 0 1 2 1 2 3 1.003 0.497 0.387 0.498 0.085 -0.388 hidden dim = 4 0 1 2 1 2 3 1.760 1.609 -0.304 2.618 -0.048 0.210 hidden dim = 8 0 1 2 1 2 3 2.865 3.632 -0.002 3.758 -0.287 -0.231 hidden dim = 16 0 1 2 1 2 3 5.256 8.981 0.761 6.172 -0.379 0.273 hidden dim = 32 0 1 2 1 2 3 4.602 3.182 -0.058 3.917 -0.061 -0.250 hidden dim = 64 (b) Actor Dataset with Expander 0 1 2 1 2 3 0.679 0.515 0.011 0.554 -0.037 -0.032 hidden dim = 4 0 1 2 1 2 3 4.414 4.533 -0.077 5.172 0.043 -0.045 hidden dim = 8 0 1 2 1 2 3 2.562 2.683 -0.016 2.527 0.034 -0.035 hidden dim = 16 0 1 2 1 2 3 5.920 5.883 0.146 5.077 0.746 0.299 hidden dim = 32 0 1 2 1 2 3 2.933 3.048 0.025 3.112 0.257 0.024 hidden dim = 64 (c) Amazon-Photo Dataset without Expander 0 1 2 1 2 3 0.943 1.295 -0.069 1.563 0.005 -0.190 hidden dim = 4 0 1 2 1 2 3 2.930 1.836 0.047 2.976 -0.067 0.371 hidden dim = 8 0 1 2 1 2 3 5.029 5.319 -0.156 5.619 -0.051 -0.142 hidden dim = 16 0 1 2 1 2 3 3.754 4.692 0.024 4.066 -0.024 -0.004 hidden dim = 32 0 1 2 1 2 3 7.902 6.771 0.075 5.896 0.427 0.025 hidden dim = 64 (d) Amazon-Photo Dataset with Expander Figure 15: Inter-layer energy distances (×100) for different hidden dimensions. allowing the model to decide the subset of edges that suit the task better — only graph edges, only expander edges or a combination of both. Short-range Vs. Long-range Dependencies If the neighboring nodes tend to be from the same class, i.e., high homophily, MPNNs and methods such as NAGphormer (Chen et al., 2022a), which summarize the neighborhood have good inductive biases; whereas Transformers without proper identification for the neighborhoods may not be as fit for this task. Heterophily may not necessarily mean long-range dependencies, label of each node may just depend on the neighbor nodes, but still label of the neighbor nodes may be different most of the time. For example, for finding the grammatical function of the words in a sentence from a very long text, neighboring words are usually enough for this identification, and nearby words would be from different classes. On the other hand, some tasks may require long-range dependencies — identifying if there are other people in a social network with similar interests or the synthetic task introduced in 1 are some examples. Local models such as MPNNs would require deeper networks for modeling long-range dependencies that makes them prone to common problems such as oversquashing and oversmoothing (Topping et al., 2021; Di Giovanni et al., 2023b,a; Rusch et al., 2023). Our approach can be reduced to MPNN by giving lower attention scores to the expander edges, for learning on the tasks with short-range dependencies only. And also lets the long-range dependency modeling using expander edges. While models 344 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Precision 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Recall 4 8 16 32 uniform random 0.0 2.5 5.0 7.5 10.0Density 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Coverage Actor Dataset without Expanders Figure 16: Violin plots of Precision, Recall, Density, and Coverage metrics for the Actor dataset without expander graphs. 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Precision 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Recall 4 8 16 32 uniform random 0 2 4 6 8Density 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Coverage Actor Dataset with Expanders Figure 17: Violin plots of Precision, Recall, Density, and Coverage metrics for the Actor dataset with expander graphs. 354 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Precision 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Recall 4 8 16 32 uniform random 0 2 4 6 8 10Density 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Coverage Amazon-Photo Dataset without Expanders Figure 18: Violin plots of Precision, Recall, Density, and Coverage metrics for the Amazon-Photo dataset without expander graphs. 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Precision 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Recall 4 8 16 32 uniform random 0 1 2 3 4Density 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Coverage Amazon-Photo Dataset with Expanders Figure 19: Violin plots of Precision, Recall, Density, and Coverage metrics for the Amazon-Photo dataset with expander graphs. 361 2 3 4 5 6 7 8 9 10 k 0.2 0.4 0.6 0.8 1.0Top-k Attention Scores Sum Amazon-Photo without Expander Dataset  1 2 3 4 5 6 7 8 9 10 k 0.0 0.2 0.4 0.6 0.8Top-k Attention Scores Sum Amazon-Photo with Expander Dataset  1 2 3 4 5 6 7 8 9 10 k 0.4 0.6 0.8 1.0Top-k Attention Scores Sum Actor without Expander Dataset  1 2 3 4 5 6 7 8 9 10 k 0.2 0.3 0.4 0.5 0.6 0.7Top-k Attention Scores Sum Actor with Expander Dataset  Top-k Attention Scores Sum, Across Datasets dim=4 Mean dim=4 Median dim=4 Interquartile Range dim=64 Mean dim=64 Median dim=64 Interquartile Range Figure 20: Top-k attention scores sum for k values between 1 to 10. 4 8 16 32 64 Hidden Dimension 0.0 0.2 0.4 0.6 0.8 1.0Ratio Amazon-Photo Dataset 4 8 16 32 64 Hidden Dimension Actor Dataset Ratio of Attention Scores by Edge Type Graph Edges Expander Edges Self-loops Figure 21: Average sum of attention scores for different edge types—graph edges, expander edges, and self- loops—per node neighborhood. The total sum of attention scores per node is one. 374 8 16 32 64 Hidden Dimension 0.00 0.02 0.04 0.06 0.08Average Attention Score Amazon-Photo 4 8 16 32 64 Hidden Dimension 0.0 0.1 0.2 0.3 0.4 0.5Average Attention Score Actor Average Attention Scores by Edge Type Graph Edges Expander Edges Self-loops Figure 22: Average attention scores for different edge types across two datasets and for different hidden dimensions. designed specifically for some of these tasks may have the advantage of reduced complexity. But our approach lets learning without concern about the nature of the problem or having domain knowledge for the task or graph. Subsampling Graphs Many approaches break the graph into sections or subsample nodes or neighbors for training. This approach has shown promising results in many works such as (Zeng et al., 2020; Hamilton et al., 2017; Liu et al., 2021). However, there are many cases in which these approaches are not expressive enough. Clustering the nodes or batching and subsampling based on the neighborhood will not have the required inductive biases to solve the tasks with long-range dependencies. Approaches such as neighbor sampling or connected-subgraph sampling not only inherit the limits of the MPNN networks, but may even miss short-range dependencies. For example, Example (c) in 1 by merely random selection of the neighbors or subgraphs without considering the task. Random subset of node selection that has been used in several promising papers such as Wu et al. (2022, 2023, 2024) gives a chance for nodes from the same label to appear in the same batch, but the batch-size should increase with the graph size accordingly. Very small ratio of batch size to graph size would mean many edges or possible pair of nodes will never be appear in any batch and depending on the task this can limit the power of these models. Also, these models are usually not memory efficient, as graph size grows, they can not keep the batches small, and the required memory grows accordingly. On the other hand, our approach (1) makes smarter selection of neighbors based on the small network’s attention scores; (2) our sampling allows making k-hop neighborhood subgraphs from the extended graph connectivity, and (3) allows the training by trading off memory and time, without critical harm to the model’s expressive power. Unline the GraphSAGE and SGFormer, which use the full graph for the inference time our model uses the same sampling and batching techniques, letting efficient inference beside the efficient training. 38",
      "meta_data": {
        "arxiv_id": "2411.16278v1",
        "authors": [
          "Hamed Shirzad",
          "Honghao Lin",
          "Balaji Venkatachalam",
          "Ameya Velingker",
          "David Woodruff",
          "Danica Sutherland"
        ],
        "published_date": "2024-11-25T10:59:25Z",
        "pdf_url": "https://arxiv.org/pdf/2411.16278v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the scalability issues of Graph Transformers, which typically suffer from quadratic memory complexity, making them impractical for large graphs. It proposes Spexphormer, a two-stage training procedure to scale sparse graph Transformers (like Exphormer) to larger datasets with significantly reduced memory consumption while maintaining competitive accuracy. Key contributions include: 1) Experimental and theoretical analysis demonstrating the similarity of attention scores across networks of varying widths, proposing architectural changes (V normalization, variable temperature) to improve this similarity. 2) Introduction of a layer-wise sparsification method based on learned attention scores, coupled with theoretical analysis of its guarantees. 3) A two-phase training process that enables efficient scaling of Graph Transformers by drastically reducing memory requirements.",
        "methodology": "Spexphormer employs a two-phase training process. The first phase involves an 'Attention Score Estimator Network,' a narrow-width (e.g., 4 or 8) network with one attention head, trained on the full augmented graph (including higher-degree expander graphs, 30-200, and self-loops). This estimator network, less memory-intensive, is designed to identify important neighbors. Two architectural changes are introduced: normalizing V mapping with a learnable scale and gradually annealing the attention temperature (τ from 1.0 to 0.05) to sharpen attention scores. In the second phase, the learned attention scores from the estimator are used to construct sparse, layer-wise interaction graphs for the final, wider network. For each layer, a fixed degree of neighbors is sampled without replacement using GPU-friendly reservoir sampling, enabling standard matrix multiplications. This method also introduces a batching technique that selects neighbors based on their task importance, recursively sampling backwards through layers. Theoretically, the paper provides conditions under which a narrow network's attention scores can approximate those of a wide network (Theorem E.4) and analyzes the sufficiency of sampling for approximating the full attention matrix (Theorem E.5, Proposition E.7).",
        "experimental_setup": "Experiments were conducted on various graph datasets. For attention score estimation, smaller datasets like Actor and Amazon Photo were used, training networks 100 times with hidden dimensions from 4 to 64 to analyze energy distance and PRDC metrics. For model quality, medium-sized graphs included six homophilic datasets (CS, Physics, Photo, Computer, WikiCS, ogbn-arxiv) and six heterophilic datasets (Minesweeper, Tolokers, Roman-empire, Amazon-ratings, Questions, Actor). Large graph datasets tested were ogbn-proteins, Amazon2M, and Pokec. Datasets used standard train/validation/test splits (e.g., 60/20/20, 50/25/25, 10/10/80 depending on the dataset). Performance metrics included accuracy and ROC-AUC. Baselines included GCN, GraphSAGE, GAT, GraphSAINT, Nodeformer, Difformer, SGFormer, GraphGPS, GOAT, GloGNN, SGC, NAGphormer, and Exphormer. Ablation studies evaluated the impact of uniform sampling, max attention sampling, constant temperature, and lack of layer normalization. Training utilized A100 (40GB) and V100 (32GB, 16GB) GPUs, ensuring GPU memory usage below 8GB (whole graph) or 4GB (batching). CPU devices (Intel Xeon E5-2680 v4 with 500GB RAM) were used for large graph attention score estimation. Hyperparameters (e.g., number of layers, hidden dimension, learning rate, dropout) were tuned per dataset.",
        "limitations": "The primary limitation is the reliance on large CPU memory (up to 500GB, typically 200GB sufficient) for the attention score estimation phase when dealing with very large datasets. This makes the approach infeasible without highly distributed computation for extremely massive graphs. Although the theoretical analysis of compressibility (Theorem E.4) shows that error increases exponentially with the number of layers, the practical impact is mitigated by using a small number of layers in real-world applications. The approach also noted that top-k attention selection might not be fully representative in transductive learning due to attention being distributed across many similar nodes rather than concentrated on a few.",
        "future_research_directions": "Future research could focus on dynamically combining sampling with simultaneous attention score estimation to scale this initial estimation to even larger graphs. Another direction involves exploring the potential for training deeper Spexphormer models by reusing or repeating attention scores from the final layer of the initial estimator network. Further investigation into how the constant factor in theoretical error bounds behaves in real-world graphs, especially concerning non-distinct or highly similar feature vectors, could also be beneficial."
      }
    },
    {
      "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations",
      "abstract": "The training of graph neural networks (GNNs) is extremely time consuming\nbecause sparse graph-based operations are hard to be accelerated by hardware.\nPrior art explores trading off the computational precision to reduce the time\ncomplexity via sampling-based approximation. Based on the idea, previous works\nsuccessfully accelerate the dense matrix based operations (e.g., convolution\nand linear) with negligible accuracy drop. However, unlike dense matrices,\nsparse matrices are stored in the irregular data format such that each\nrow/column may have different number of non-zero entries. Thus, compared to the\ndense counterpart, approximating sparse operations has two unique challenges\n(1) we cannot directly control the efficiency of approximated sparse operation\nsince the computation is only executed on non-zero entries; (2) sub-sampling\nsparse matrices is much more inefficient due to the irregular data format. To\naddress the issues, our key idea is to control the accuracy-efficiency trade\noff by optimizing computation resource allocation layer-wisely and\nepoch-wisely. Specifically, for the first challenge, we customize the\ncomputation resource to different sparse operations, while limit the total used\nresource below a certain budget. For the second challenge, we cache previous\nsampled sparse matrices to reduce the epoch-wise sampling overhead. Finally, we\npropose a switching mechanisms to improve the generalization of GNNs trained\nwith approximated operations. To this end, we propose Randomized Sparse\nComputation, which for the first time demonstrate the potential of training\nGNNs with approximated operations. In practice, rsc can achieve up to\n$11.6\\times$ speedup for a single sparse operation and a $1.6\\times$ end-to-end\nwall-clock time speedup with negligible accuracy drop.",
      "full_text": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zirui Liu 1 Shengyuan Chen 2 Kaixiong Zhou 1 Daochen Zha 1 Xiao Huang 2 Xia Hu 1 Abstract Training graph neural networks (GNNs) is ex- tremely time-consuming because sparse graph- based operations are hard to be accelerated by community hardware. Prior art successfully reduces the computation cost of dense matrix based operations (e.g., convolution and linear) via sampling-based approximation. However, unlike dense matrices, sparse matrices are stored in an irregular data format such that each row/column may have a different number of non-zero entries. Thus, compared to the dense counterpart, approx- imating sparse operations has two unique chal- lenges (1) we cannot directly control the effi- ciency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sampling sparse matrices is much more ineffi- cient due to the irregular data format. To address the issues, our key idea is to control the accuracy- efficiency trade-off by optimizing computation re- source allocation layer-wisely and epoch-wisely. For the first challenge, we customize the com- putation resource to different sparse operations, while limiting the total used resource below a cer- tain budget. For the second challenge, we cache previously sampled sparse matrices to reduce the epoch-wise sampling overhead. To this end, we propose Randomized Sparse Computation. In practice, RSC can achieve up to 11.6× speedup for a single sparse operation and 1.6× end-to- end wall-clock time speedup with almost no ac- curacy drop. Codes are available at https:// github.com/warai-0toko/RSC-ICML. 1Department of Computer Science, Rice University, Houston, TX, USA 2Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong SAR. Correspondence to: Xia Hu <xia.hu@rice.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 1. Introductions Graph Neural Networks (GNNs) have achieved great suc- cess across different graph-related tasks (Hamilton et al., 2017; Hu et al., 2020; Ying et al., 2018; Jiang et al., 2022; Zhou et al., 2022; 2023). However, despite its effective- ness, the training of GNNs is very time-consuming. Specifi- cally, GNNs are characterized by an interleaved execution that switches between the aggregation and update phases. Namely, in the aggregation phase, every node aggregates messages from its neighborhoods at each layer, which is implemented based on sparse matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In the update phase, each node will update its embedding based on the aggre- gated messages, where the update function is implemented with dense matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In Figure 1, SpMM and MatMul are the sparse and dense operations in the aggregation and update phases, respectively. Through profiling, we found that the aggregation phase may take more than 90% running time for GNN training. This is because the sparse matrix opera- tions in the aggregation phase have many random memory accesses and limited data reuse, which is hard to be acceler- ated by community hardwares (e.g., CPUs and GPUs) (Duan et al., 2022b; Han et al., 2016; Duan et al., 2022a). Thus, training GNNs with large graphs is often time-inefficient. ognb-proteins Reddit ogbn-product0 20 40 60 80 100Percentage of Time Consumption Other MatMul(forward) MatMul(backward) SpMM(forward) SpMM(backward) Figure 1: The time profiling of a two-layer GCNs on dif- ferent datasets. SpMM may take 70% ∼ 90% of the total time. We measure the time on a single NVIDIA RTX3090 (24GB). The detailed software and hardware information can be found in Appendix D. Existing works towards this problem can be roughly divided into three categories. First, some works propose distributed GNNs training systems, which focus on minimizing the 1 arXiv:2210.10737v2  [cs.LG]  2 Jul 2023RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations communication cost among hardware (Zheng et al., 2020; Ramezani et al., 2022; Wan et al., 2022b; Md et al., 2021; Wan et al., 2022a). Second, another research line optimizes the memory access pattern of sparse operations via coalesc- ing the memory access and fusing consecutive operations (Zhang et al., 2022; Huang et al., 2020a; Rahman et al., 2021; Wang et al., 2021). Third, some other works try to accelerate the training process from the optimization aspect, i.e., using fewer iterations to converge (Narayanan et al., 2022; Cong et al., 2020; Xu et al., 2021; Cai et al., 2021). In parallel, an orthogonal direction is to replace the ex- pensive operations with their faster-approximated versions (Adelman et al., 2021; Drineas et al., 2006b). The key idea is to sub-sample tensors onto low dimensional spaces and perform the original operations here. For example, for the linear operation between two matrices A ∈ Rn×m and B ∈ Rm×q, we first obtain A′ ∈ Rn×k and B′ ∈ Rk×q (k < m) by picking k representative columns of A and the corresponding rows of B (Drineas et al., 2006b). Then we approximate AB ≈ A′B′. With this procedure, the number of floating-point operations (FLOPs) and memory access are both reduced. Based on the idea, previous work success- fully accelerates the dense matrix based operations, such as convolution and linear operations (Adelman et al., 2021). The approximated operation can plug-and-play replace the exact operation to improve per-operation efficiency, and thus is compatible with most of the efficient training methods. Despite the potential, this perspective however has not been explored for the sparse operations in GNNs. The approximation method reduces the computational com- plexity at the cost of giving noisy outputs. Thus, there naturally exists an accuracy-efficiency trade-off. Com- pared to approximating dense matrix operations, there are two unique challenges to optimizing the trade-off for ap- proximated sparse operations. First, unlike the previous example of approximating linear operation, k cannot di- rectly control the efficiency (FLOPs) for sparse operations. This is because, for dense matrices, each row/column has the same amount of parameters. Thus the reduction of FLOPs in approximated dense operations is determined by the dimensions of the sub-sampled matrices (i.e., k). How- ever, in sparse operations, each row/column in the sparse adjacency matrix has different numbers of non-zero en- tries, and the computation is only executed on non-zero entries (i.e., irregular data format). Thus, the reduction of FLOPs in the sparse operations is decided by the selection of representative rows/columns. It lacks a mechanism to directly control the efficiency-accuracy trade-off for each sparse operation. Second, compared to the dense counter- part, sub-sampling (i.e., slicing) the sparse matrix is much more time-consuming due to its irregular data format (Han et al., 2016; Fey & Lenssen, 2019), which counteracts the acceleration from the FLOPs reduction. To this end, we propose Randomized Sparse Computation, dubbed RSC , the first approximation framework tailored for efficient GNN training. Our core idea is to control the trade-off by optimizing the computation resource alloca- tion at the “global” level. Specifically, to tackle the first challenge, at the layer-wise level, we propose to customize the FLOPs of each sparse operation while limiting the total FLOPs under a certain budget. The rationale behind this strategy is that each operation may have a different contribu- tion to the model accuracy. Thus, we could to assign more computational resources to “important” operations under a certain budget. More concretely, we frame it as a constraint optimization problem. Then we propose a greedy algorithm to solve it efficiently. To tackle the second challenge, at the epoch-wise level, we found that the selection of represen- tative row/columns tends to remain similar across nearby iterations. Based on this finding, we develop a caching mechanism to reuse the previously sampled sparse matrix across nearby iterations to reduce per-epoch sampling time. Finally, inspired by the recent finding that the final stage of training usually needs smaller noise to help convergence (Li et al., 2019; Dao et al., 2022), we propose to use approxi- mated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. This switching mechanism significantly reduces the accuracy drop, at the cost of slightly less speedup. We summarize our contributions as follows: • We accelerate the training of GNNs from a new perspec- tive, namely, replacing the expensive sparse operations with their faster-approximated versions. • Instead of focusing on balancing the efficiency-accuracy trade-off at the operation level, we control the trade-off through optimizing resource allocation at the layer-wise and epoch-wise levels. • We propose a caching mechanism to reduce the cost of sampling sparse matrices by reusing previous results. • Extensive experiments have demonstrated the effective- ness of the proposed method. Particularly, RSC can achieve up to 11.6× speedup for a single sparse opera- tion and a 1.6× end-to-end wall-clock time speedup with negligible (≈ 0.3%) accuracy drop. 2. Background and Preliminary 2.1. Graph Neural Networks Let G = ( V, E) be an undirected graph with V = (v1, ··· , v|V|) and E = (e1, ··· , e|E|) being the set of nodes and edges, respectively. Let X ∈ R|V|×d be the node feature matrix. A ∈ R|V|×|V| is the graph adjacency matrix, where Ai,j = 1 if (vi, vj) ∈ Eelse Ai,j = 0. 2RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ˜A = ˜D−1 2 (A + I) ˜D−1 2 is the normalized adjacency ma- trix, where ˜D is the degree matrix of A + I. GNNs re- cursively update the embedding of a node by aggregating embeddings of its neighbors. For example, the forward pass of the lth Graph Convolutional Network (GCN) layer (Kipf & Welling, 2017) can be defined as: H(l+1) = ReLU( ˜AH(l)Θ(l)), (1) where H(l) is the node embedding matrix at the lth layer and H(0) = X. Θ(l) is the weight matrix of the lth layer. In practice, ˜A is often stored in the sparse matrix format, e.g., compressed sparse row (CSR) (Fey & Lenssen, 2019). From the implementation aspect, the computation of Equa- tion (1) can be described as: H(l+1) = ReLU   SpMM \u0012 ˜A, MatMul(H(l), Θ(l)) \u0013! , where SpMM(·, ·) is the Sparse-Dense Matrix Multiplica- tion and MatMul(·, ·) is the Dense Matrix Multiplication. Sparse operations, such as SpMM , have many random mem- ory accesses and limited data reuse. Thus they are much slower than the dense counterpart (Han et al., 2016; Duan et al., 2022b). To get a sense of the scale, we show in Figure 1 that for GCNs, SpMM may take roughly 70% ∼ 90% of the total training time. 2.2. Fast Approximated MatMul with Sampling Let X ∈ Rn×m, Y ∈ Rm×q. The goal is to efficiently esti- mate the matrix production XY . Truncated Singular Value Decomposition (SVD) outputs provably optimal low-rank estimation of XY (Adelman et al., 2021). However, SVD is almost as expensive as matrix production itself. Instead, the sampling algorithm is proposed to approximate the matrix product XY by sampling k columns of X and correspond- ing rows of Y to form smaller matrices, which are then multiplied as usual (Drineas et al., 2006b). This algorithm reduces the computational complexity from O(mnq) to O(knq). Specifically, XY = mX i=1 X:,iYi,: ≈ kX t=1 1 st X:,itYit,: = approx(XY ), (2) where X:,i ∈ Rn×1 and Yi,: ∈ R1×q are the ith column and row of X and Y , respectively. In this paper, we call (X:,i, Yi,:) the ith column-row pair. k is the number of sam- ples (1 ≤ k ≤ m). {pi}m i=1 is a probability distribution over the column-row pairs. it ∈ {1, ··· m} is the index of the sampled column-row pair at the tth trial. st is the scale fac- tor. Theoretically, (Drineas et al., 2006b) shows that if we set st = 1 kpit , then we have E[approx(XY )] =XY . Fur- ther, the approximation errorE[||XY −approx(XY )||F ] is minimized when the sampling probabilities {pi}m i=1 are proportional to the product of the column-row Euclidean norms (Drineas et al., 2006b): pi = ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 . (3) 2.2.1. T OP-k SAMPLING The above sampling-based method is originally developed for accelerating the general application ofMatMul (Drineas et al., 2006b). Directly applying it to neural networks may be sub-optimal since it does not consider the characteristic of neural network weights. Based on the empirical observation that the distribution of weights remains centered around zero during training (Glorot & Bengio, 2010; Han et al., 2015), (Adelman et al., 2021) proposes a top-k sampling algorithm: Picking k column-row pairs with the largest ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 deterministically without scaling. Equivalently, it means pi of column-row pairs with the k- largest value in Equation (3) equals 1, otherwise it equals 0. And sit is a constant 1. Albeit without the scaling while sampling column-row pairs deterministically, under on the assumption of zero-centered weight distribution, (Adelman et al., 2021) theoretically show that top-k sampling still yields an unbiased estimation of XY with minimal approx- imation error. Consequently, the top-k sampling algorithm empirically shows a significantly lower accuracy drop when approximating the convolution and linear operations in the neural networks (Adelman et al., 2021). In the next section, we explore how to approximate the expensive sparse operation via the top-k sampling. 3. The Proposed Framework The overview of RSC is shown in Figure 2, where we use the computation graph of GCN as an example. We first explore which SpMM in the computation graph can be re- placed with its approximated version (Section 3.1). Then since GNNs have multiple SpMM and each of them may have different importance to the model performance, we then automatically allocate computation resources to dif- ferent SpMM (Section 3.2). Finally, we explore two simple and effective tricks for improvingRSC , including a caching mechanism to reduce the overhead of sampling sparse ma- trices (Section 3.3.1) and a switching mechanism to reduce the accuracy drop (Section 3.3.2). 3.1. Where to Apply the Approximation 3.1.1. E XPERIMENTAL ANALYSIS Each sparse operation is executed twice at each train- ing step, i.e., one in the forward pass and the other one 3RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations SPMM MatMul Approx SpMM MatMul Forward Pass Backward Pass Down Sampl ing Cache Caching (Sec 3.3.1) Down Sampl ing Constraint  Optimization Eq. 5 𝑘!  Resource Allocation (Sec 3.2) Θ(!) 𝑯(!$%) 𝛁𝑯(!$%) 𝛁Θ(!) 𝛁𝑯(!)𝑯(!) 𝑱(!) 𝛁𝑱(!) 𝑨' Figure 2: Overview of RSC . For convenience, ReLU is ignored. RSC only replace the SpMM in the backward pass with its approximated version using top-k sampling (Section 3.1). kl is the number of samples for top-k sampling at the lth layer, which is automatically allocated (Section 3.2). To reduce the overhead of sampling, we also cache the sampled graph and reuse it across nearby iterations (Section 3.3). in the backward pass. As shown in Figure 2, here we take SpMM in the lth GCN layer as an example, the for- ward one is H(l+1) = ReLU(SpMM( ˜A, J(l))), where J(l) = MatMul(H(l), Θ(l)) is the intermediate node representations. And the backward one is ∇J(l) = SpMM( ˜A⊤, ∇H(l+1)). ∇J(l) and ∇H(l) are the gradient with respect to J(l) and H(l), respectively. Even though the approximation method itself is statisti- cally unbiased, replacing the exact sparse operation with their faster-approximated versions still injects noise to the computation graph. As we analyzed above, each SpMM is executed twice in the training step. Below we first exper- imentally analyze the impact of the injected noise in the forward pass and the backward pass. As shown in Table 1, we apply top-k sampling to approximate the SpMM in the forward pass, backward pass, or both, respectively. Table 1: Preliminary results on approximatingSpMM via top- k sampling. The model is a two-layer GCN, and the dataset is Reddit. Here we set thek as 0.1|V| across different layers. Method Reddit without approximation 95.39±0.04 only forward 16.45±0.39 only backward 95.25±0.03 forward and backward 80.74±1.00 From Table 1, the accuracy drop is negligible if we only replace SpMM in the backward pass. Notably, if we apply ap- proximation in both the forward and backward pass, the re- sult is significantly better than only applying top-k sampling in the forward pass. The reason is that when only apply- ing approximation in the forward pass, some row/columns are not included in the computation graph, so intuitively these row/columns should be excluded in the backward pass. “forward and backward” result in Table 1 is built based on this intuition such that in the backward pass, we use the column-row pairs sampled in the forward pass to compute the gradient (Adelman et al., 2021). However, it is still not comparable to the result of applying approximation only in the backward pass. Below we mathematically analyze the reason behind the results in Table 1. 3.1.2. T HEORETICAL ANALYSIS We first analyze the case of approximating the sparse opera- tions in the forward pass. Namely, replacingSpMM( ˜A, J(l)) with approx( ˜AJ(l)). We note that we have E[f(x)] ̸= f(E[x]) for any non-linear function f(·), e.g., E[x2] ̸= E2[x]. Thus, even when the approximation method gives an unbiased estimation, i.e., E[approx( ˜AJ(l))] = ˜AJ(l), the node embeddings H(l+1) are still biased since the acti- vation function is non-linear. To see this, E[H(l+1)] =E[ReLU(approx( ˜AJ(l))]) ̸= ReLU(E[approx( ˜AJ(l))]) =H(l+1). Thus, if we apply the approximation for the SpMM in the forward pass, the bias will be propagated layer-by-layer and cause significantly worse results. For the case of only approximating the sparse operation in the backward pass, we have the following proposition: Proposition 3.1 (Proof in Appendix A). If the approxima- tion method is itself unbiased, and we only replace theSpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. The high-level idea is that the gradient of the activation function in the backward pass is only related to the pre- activations in the forward pass, and thus is independent of the approximation error introduced in the backward pass. Due to the page limit, we also discuss why sampling-based approximation is suitable for accelerating GNNs in Ap- pendix A. As suggested by our theoretical and empirical analysis, as shown in Figure 2, we only approximate the sparse operations in the backward pass, while leaving all other operations unchanged. 3.2. How to Apply the Approximation As we mentioned, for sparse operations, the acceleration is decided by the selection of sampled column-row pairs. To see this, as shown in Figure 3, suppose we use top- k sampling to approximate SpMM( ˜A⊤, ∇H). Since the computations are only executed on the non-zero entries, so selecting the orange pairs (i.e., pair 1 and 3) will result in 3 7 × less computational cost (FLOPs) compared to selecting 4RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1 11 111111132100123 3210∇𝐻!!∇𝐻!\"∇𝐻!#∇𝐻\"! ∇𝐻#!∇𝐻$! ∇𝐻\"\" ∇𝐻#\"∇𝐻$\" ∇𝐻\"# ∇𝐻##∇𝐻$# Nodeembeddinggradients∇𝐻∈ℝ!×#,with𝑑=3Sparseadjacencymatrix𝐴$∈ℝ!×!,with𝑁=4 × Figure 3: For approximated sparse operations, the accelera- tion is decided by the selection of column-row pairs. the blue pair (i.e., pair 0 and 2). For both the orange and blue cases, we have k = 2. Thus, the number of samples k cannot directly constrain the FLOPs for each individual operation. Moreover, a GNN has multiple operations (or layers), and the model accuracy has a different sensitivity to the approximation error at different layers. To optimize the accuracy-efficiency trade-off, our key idea is to customize the computation resources (i.e., FLOPs) for each layer by adjusting the number of samples kl in the l-th layer. In this way, we minimize the impact of approximation, while limiting the overall FLOPs under a certain budget. Based on the idea, we frame the resource allocation problem as the following constrained optimization problem: min {kl} − LX l=1 X i∈Topkl ∥ ˜A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥ ˜A∥F ∥∇H(l+1)∥F , (4a) s.t. LX l=1 X i∈Topkl #nnzi ∗ dl ≤ C LX i=1 |E|dl, (4b) where C is the budget (0 < C <1) that controls the overall reduced FLOPs. kl is the number of samples for the top-k sampling at the l-th layer. dl is the hidden dimensions ofl-th layer, and #nnzi is the number of non-zero entries at the i-th column of ˜A⊤. Topkl is the set of indices associated with the kl largest ∥ ˜A⊤ :,i∥2∥∇H(l+1) i,: ∥2. Equation (4a) is equivalent to minimizing the relative ap- proximation error E[|| ˜A⊤∇H(l+1)−approx( ˜A⊤∇H(l+1))||F ∥ ˜A∥F ∥∇H(l+1)||F ] summarized over all layers (Adelman et al., 2021). Also, different sparse operations are weighted summation by the magnitude of gradient ∥∇H(l+1)∥2, which implicitly en- codes the importance of different operations. Equation (4b) is the constraint that controls the overall FLOPs. Specifically, the FLOPs of SpMM between ˜A and the gradient ∇H ∈ RN×d is O(|E|d) and P j∈V #nnzj = |E|. We note that Equation (4b) also bounds the number of memory access of SpMM . 3.2.1. GREEDY SOLUTION The above combination optimization objective is NP-hard, albeit it can be solved by dynamic programming. However, dynamic programming is very slow, which somehow con- tradicts our purpose of being efficient. Thus, we propose to use a greedy algorithm to solve it. Specifically, it starts with the highest kl = |V| for all layers. In each move, it chooses a kl among {kl}L l=1 to reduce by a step size (e.g., 0.02|V|), such that the increment of errors in Equation (4a) is mini- mal. The greedy algorithm will stop when the current total FLOPs fits in the budget in Equation (4b). This algorithm runs super fast, and we found that it has minimal impact on efficiency. We provide the pseudo-code of our greedy algorithm in Algorithm 1 of Appendix B. 3.3. When to Apply the Approximation 3.3.1. CACHE THE SAMPLED SPARSE MATRICES We first give the details about the Compressed Sparse Row (CSR) format for representing the sparse matrix here. CSR stores nonzero values in a matrix and their position in three arrays: index array Rowptr, column array Col, and value array Val. The elements in Rowptr act as the starting indices of the elements in Col and Val that correspond to each row. Specifically, the elements of row i are stored in indices Rowptr[i] to Rowptr[i+ 1] − 1 of Col and Val . The elements in Col and Val are the column index and value in that column, respectively. Figure 5 shows the CSR format of the matrix shown in Figure 3. We ignore the Val array here for illustration convenience. Executing the top- k sampling contains two steps: First, it decides the indices corresponding to the top- k largest column row norms in Equation (3). Second, slicing the matrices according to the indices. In practice, the overhead of the first step can be ignored. However, unlike dense matrices, slicing the adjacency matrix is much slower due to its irregular data format. To see this, suppose the top-k indices of the sparse matrix in Figure 3 correspond to the orange column-row pairs. Figure 5 shows the process of slicing the adjacency matrix in CSR format by reserving only the orange columns. Slicing sparse matrices requires to re-process the graph to build the new Rowptr and Col (Fey & Lenssen, 2019), which introduces significant time overhead, especially for large graphs. For the full graph training, we use the same adjacency matrix across different epochs1. We made a crucial observation that the top-k indices in the adjacency matrix tend to be the same across iterations. In Figure 4, we plot the AUC score of top- k indices between every iteration t and iteration t + 10for 1For sub-graph based training, we can first sample all of the sub-graphs offline. Then during the training, we apply the caching mechanism to each sampled graph. 5RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Reddit GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Yelp GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score ogbn-proteins GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer Figure 4: For each layer, the selected column-row pairs tend to be very similar across iterations. Models here are two-layer GCN and GraphSAGE. Here we show the matching scores (AUC) of top-k indices between every 10 steps. Figure 5: The process of slicing the sparse matrix in Figure 3 by only reserving orange columns (in CSR format). each layer throughout the whole training process. Here we note that AUC score is a commonly used ranking measure and a 1.0 AUC score means the ranking of column-row pairs is identical across iterations. The results in Figure 4 indicate that the top-k indices won’t change significantly within a few iterations. Thus, as shown in Figure 2, we propose to reuse the sampled adjacency matrix for each layer across nearby iterations. Discussion. The rationale behind the success of caching is the slow rate of change in the learned embeddings within GNNs (Fey et al., 2021; Wan et al., 2022a). Prior research has leveraged this “staleness” of embeddings to enhance the efficiency of GNN training [1, 2]. The success of caching can also be explained by the staleness: if embeddings (and their gradients) across consecutive steps remain nearly iden- tical, the sampled sparse matrix will also exhibit minimal variation. Later we experimentally show that the caching mechanism does not impact the model performance a lot, but leads to a significant speedup. 3.3.2. SWITCH BACK AT THE END When training neural networks, the common practice is to use a large learning rate for exploration and anneal to a small one for final convergence (Li et al., 2019). The ratio- nale behind this strategy is that, at the end of the training process, we need to fine-tune our model with small noise for convergence. Since our approximation sparse operations will bring extra noise to the gradient, intuitively, we can switch back to the original sparse operations to help con- vergence. More formally, we propose to use approximated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. We experimentally show that this switching mecha- nism significantly reduces the accuracy drop at the cost of slightly less acceleration effect. We note that the switching mechanism is not proposed in this paper. The switching mechanism takes inspiration from previous work Dao et al. (2022), and both our work and Dao et al. (2022) utilize the switching mechanism to minimize the impact of approximation. 4. Related work and Discussion Due to the page limit, we first discuss the related work on approximated matrix multiplication. Other related topics, i.e., subgraph-based training, randomized GNN training, and non-approximated GNN acceleration, can be found in Appendix C. Approximated Matrix Multiplication.The approximated matrix production can be roughly divided into three cat- egories. However, only a few of them can be used for accelerating GNN training. Specifically, (1) Random walk- based methods (Cohen & Lewis, 1999) performs random walks on a graph representation of the dense matrices, but is only applicable to non-negative matrices; (2) Butterfly- based methods (Chen et al., 2021; Dao et al., 2022) replace dense matrices with butterfly matrices. It is not applicable to SpMM in GNNs because the adjacency matrix often cannot be reduced to a butterfly matrix. (3) Column-row sampling methods(Drineas et al., 2006a; Drineas & Kannan, 2001) sample the input matrices with important rows and columns, then perform the production on the sampled matrix as usual. 5. Limitations First, to guarantee the model accuracy, we only replace the sparse operation in the backward pass. Thus the upper bound of RSC ’s speedup is limited. However, we note that the backward pass usually is more time-consuming than the forward pass, which is also empirically shown in Table 2. Second, some GNNs rely on the scatter-and-gather instead of SpMM (and its variant) to perform the aggregation, such as GAT (Veliˇckovi´c et al., 2017). They are not covered in this paper. However, scatter-and-gather based GNNs can also 6RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations be accelerated by RSC because the column-row sampling is also applicable to scatter and gather operation. Similarly, the caching and switching mechanisms are also applicable to them. However, for the resource allocation Algorithm 1, the scatter and gather operations require tailored error bound and the computation cost modeling in Equation (4). We leave it as future work. 6. Experiments We verify the effectiveness of our proposed framework via answering the following research questions: Q1: How ef- fective is RSC in terms of accuracy with reduced training time? Q2: How effective is our proposed allocation strategy compared to the uniform allocation strategy? Q3: What is the layer-wise ratio assigned by RSC ? Q4: How effec- tive is the caching and switching mechanism in terms of the trade-off between efficiency and accuracy? If without explicitly mentioned, all reported results are averaged over ten random trials 6.1. Experimental Settings Datasets and Baselines. To evaluateRSC , we adopt four common large-scale graph benchmarks from different do- mains, i.e., Reddit (Hamilton et al., 2017), Yelp (Zeng et al., 2020), ogbn-proteins (Hu et al., 2020), and ogbn- products (Hu et al., 2020). We evaluate RSC under both the mini-batch training and full-batch training settings. For the mini-batch training setting, we integrate RSC with one of the state-of-the-art sampling methods, GraphSAINT (Zeng et al., 2020). For the full-batch training setting, we inte- grate RSC with three popular models: two commonly used shallow models, namely, GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017), and one deep model GCNII (Chen et al., 2020). To avoid creating confusion, GCN, GraphSAGE, and GCNII are all trained with the whole graph at each step. For a fair comparison, we use the MEAN aggregator for GraphSAGE and GraphSAINT throughout the paper. Details about the hyperparameters and datasets are in Appendix D. Hyperparameter settings. RSC contains three parts. First, the allocation strategy. We choose the overall budget C in Equation (4b) from {0.1, 0.3, 0.5}. We run the resource allocation strategy every ten steps. The step size α in Algo- rithm 1 is set as 0.02|V|. Second, the caching mechanism. According to Figure 4, we sample the adjacency matrix every ten steps and reuse the sampled matrices for nearby steps. Third, the switching mechanism, where we apply RSC for 80% of the total epochs, while switching back to the original operations for the rest of the 20% epochs. Due to the page limit, We present a detailed hyperparameter study in Appendix E Figure 11 and Figure 12. Evaluation metrics. To evaluate the practical usage of RSC , we report the wall clock time speedup measured on GPUs. Specifically, the speedup equalsTbaseline/Trsc, where Tbaseline and Trsc are the wall clock training time of baseline and RSC , respectively. We note that the Trsc includes the running time of the greedy algorithm, and the effects of caching and switching. 6.2. Performance Analysis 6.2.1. A CCURACY -EFFICIENCY TRADE -OFF To answer Q1, we summarize the speedup and the test accuracy/F1-micro/AUC of different methods in Table 3. Since RSC accelerates the sparse operation in the backward pass, we also provide the detailed efficiency analysis in Table 2. In summary, we observe: ❶ At the operation level, RSC can accelerate the sparse operation in the backward pass by up to 11.6×. For end- to-end training, the accuracy drop of applying RSC over baselines is negligible (0.3%) across different models and datasets, while achieving up to 1.6× end-to-end wall clock time speedup. The gap between the operation speedup and the end-to-end speedup is due to the following two reasons. First, we focus on accelerating the sparse computations in GNNs, which is the unique bottleneck to GNNs. The other dense computations can certainly be accelerated by approximation methods, but this is beyond the scope of this paper. Second, we only accelerate the sparse computation in the backward pass instead of the forward one to guaran- tee performance. We note that for approximation methods that accelerate the training process at operation level, a 1.2 ≈ 1.3× wall-clock speedup with negligible accuracy drop can be regarded as non-trivial (for details, please see Table 1 in (Adelman et al., 2021)), especially considering that these approximation methods are orthogonal to most of the existing efficient training methods. For GraphSAINT, the speedup of RSC is around 1.1×, which is smaller than the full graph training. This is because for subgraph-based training, the equivalent “batch size” is much smaller than the full graph counterparts. As a result, the GPU utility is low since it does not assign each processor a sufficient amount of work and the bottleneck is the mini-batch transfer time (Kaler et al., 2022). We note that the mini-batch sampling and transfer time can be optimized from the system perspec- tive (Kaler et al., 2022), which is orthogonal to our work. The speedup is expected to be larger when the mini-batch sampling time is optimized. 6.2.2. A BLATION ON RESOURCE ALLOCATION . Due to the page limit, we first show the running time of the greedy algorithm in Appendix E Table 11. We conclude that the overhead of the greedy algorithm is negligible com- pared to the acceleration effect of RSC . To answer Q2, we 7RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 2: Comparison on the efficiency at the operation level. fwd/bwd is the wall-clock time for a single forward/backward pass (ms). SpMM MEAN corresponds to the MEAN aggregator used in GraphSAGE (Appendix A.3). Reddit Yelp ogbn- proteins ogbn- products fwd bwd fwd bwd fwd bwd fwd bwd SpMM Baseline 36.28 44.23 26.88 34.38 31.72 42.99 261.03 316.80 +RSC - 3.81 (11.6 ×) - 9.86 (3.49 ×) - 14.87 (2.89 ×) - 35.28 (8.98 ×) SpMMMEAN (Appendix A.3) Baseline 36.21 44.27 26.78 34.38 31.80 43.11 261.03 316.84 +RSC - 7.47 (5.92 ×) - 19.62 (1.75 ×) - 5.22 (8.26 ×) - 71.59 (4.43 ×) Table 3: Comparison on the test accuracy/F1-micro/AUC and speedup on four datasets. Bold faces indicate the accuracy drop is negligible (≈ 0.3%) or the result is better compared to the baseline.The hardware here is a RTX3090 (24GB). # nodes # edges 230K 11.6M 717K 7.9M 132K 39.5M 2.4M 61.9M Model Methods Reddit Yelp ogbn- proteins ogbn- products Acc. Budget C Speedup F1-microBudget C Speedup AUC Budget C Speedup Acc. Budget C Speedup Graph- SAINT Baseline 96.40±0.03 1 1 × 63.30±0.14 1 1 × — — — 79.01±0.21 1 1 × +RSC 96.24±0.030.1 1.11 × 63.34±0.180.1 1.09 × — — — 78.99±0.32 0.3 1.04 × GCN Baseline 95.33±0.03 1 1 × 44.28±1.04 1 1 × 71.99±0.66 1 1 × 75.74±0.11 1 1 × +RSC 95.13±0.050.1 1.47 × 46.09±0.540.1 1.17 × 71.60±0.450.3 1.51 × 75.44±0.21 0.3 1.35 × GraphSAGE (full batch) Baseline 96.61±0.05 1 1 × 63.06±0.18 1 1 × 76.09±0.77 1 1 × 78.73 ± 0.12 1 1 × +RSC 96.52±0.040.1 1.32 × 62.89±0.190.1 1.13 × 76.30±0.420.3 1.60 × 78.50± 0.090.1 1.53 × GCNII Baseline 96.71±0.07 1 1 × 63.45±0.17 1 1 × 73.79±1.32 1 1 × — — — +RSC 96.50±0.120.3 1.45 × 63.57±0.210.1 1.19 × 75.20±0.540.5 1.41 × — — — /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013 /uni0000001c/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000014/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000015/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000015/uni00000018/uni00000014/uni00000011/uni00000015/uni00000018/uni00000013/uni00000014/uni00000011/uni00000015/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000016/uni00000013/uni00000013/uni00000014/uni00000011/uni00000016/uni00000015/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000019/uni00000011/uni00000018/uni0000001b /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000015 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000017/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000018/uni00000011/uni00000018 /uni0000001c/uni00000019/uni00000011/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 Figure 6: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. Here we disabled the caching and switch mechanism for a fair comparison. More results can be found in Appendix E Table 4: Ablation on the caching and switching mechanism. Experiments are conducted on ogbn-proteins. All results are averaged over five random trials. Ablation on Caching Switching AUC Speedup GCN ✗ ✗ 71.60 ± 0.66 1.19 × ✗ ✓ 72.19 ± 0.79 1.14 × ✓ ✗ 69.80 ± 0.60 1.60 × ✓ ✓ 71.60 ± 0.45 1.51 × GraphSAGE ✗ ✗ 75.23 ± 0.79 1.37 × ✗ ✓ 76.39 ± 0.39 1.32 × ✓ ✗ 75.53 ± 0.60 1.78 × ✓ ✓ 76.30 ± 0.42 1.60 × GCNII ✗ ✗ 74.07 ± 0.83 1.10 × ✗ ✓ 74.50 ± 0.52 1.04 × ✓ ✗ 72.47 ± 0.75 1.46 × ✓ ✓ 75.20 ± 0.54 1.41 × compare RSC with the uniform allocation strategy, i.e., set- ting kl = C|V| for all sparse operations in the backward pass. As shown in Figure 6, we plot the Pareto frontier of the accuracy-efficiency trade-off on the Reddit dataset for RSC and the uniform strategy with different C. For a fair comparison, we disabled the caching and switching mechanism. Due to page limit, more results are shown in Appendix E. We observe that: ❷ RSC exhibits a supe- rior trade-off between accuracy and efficiency compared to the uniform allocation, especially under high speedup regime. Namely, compared to the uniform allocation, RSC can achieve higher model accuracy under the same speedup. This can be explained by the fact that each operation has a different importance to the model performance. RSC can au- tomatically allocate more resources to important operations under a given total budget. To answer Q3, due to the page limit, we visualize the al- located kl for each layer across iterations in Appendix E Figure 7, and the degree of picked nodes in Appendix E Figure 8. We observe: ❸ The kl assigned by RSC evolves along with the training. 6.2.3. A BLATION ON CACHING AND SWITCHING . In section 6.2.2, we have shown the superior results of the proposed resource allocation strategy. As we mentioned in Section 3.3, we also introduce two simple tricks to for improving RSC , i.e., the caching and switching mechanism. To verify the effect of each of them (Q4), we conduct incre- mental evaluations on GCN, GraphSAGE and GCNII with 8RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ogbn-proteins, which are summarized in Table 4. The row without caching and switching in Table 4 corresponds to the results with the proposed resource allocation strategy. We observe: ❹ Switching mechanism significantly improves the model performance, at the cost of slightly less acceleration effect. As we analyzed in Section 3.3.2, the improvement can be explained by the fact that the final training stage requires smaller gradient noise to help convergence. ❺ Caching mechanism significantly improves the wall-clock time speedup, at the cost of worse model performance. Al- though caching mechanism can reduce the overhead of sam- pling, the performance drop is too large (> 1%). Intuitively, the accuracy drop of caching also implies that we could not use a “static” down-sampled graph throughout the training process. ❻ Surprisingly, jointly applying the caching and switching, the performance drop can be minimized. 7. Acknowledgements The authors thank the anonymous reviewers for their helpful comments. The work is in part supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. 8. Conclusions and Future work We propose RSC , which replaces the sparse computations in GNNs with their fast approximated versions. RSC can be plugged into most of the existing training frameworks to improve their efficiency. Future work includes exploring RSC for GNNs that rely on scatter-and-gather operations. References Adelman, M., Levy, K., Hakimi, I., and Silberstein, M. Faster neural network training with approximate tensor operations. Advances in Neural Information Processing Systems, 34:27877–27889, 2021. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-y., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, pp. 1204–1215. PMLR, 2021. Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. In Inter- national conference on machine learning. PMLR, 2017. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Sim- ple and deep graph convolutional networks. In Interna- tional Conference on Machine Learning, pp. 1725–1735. PMLR, 2020. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019. Cohen, E. and Lewis, D. D. Approximating matrix multi- plication for pattern recognition tasks. Journal of Algo- rithms, 30(2):211–252, 1999. Cong, W., Forsati, R., Kandemir, M., and Mahdavi, M. Minimal variance sampling with provable guarantees for fast training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 1393–1403, 2020. Dao, T., Chen, B., Sohoni, N. S., Desai, A., Poli, M., Gro- gan, J., Liu, A., Rao, A., Rudra, A., and R´e, C. Monarch: Expressive structured matrices for efficient and accurate training. In International Conference on Machine Learn- ing, pp. 4690–4721. PMLR, 2022. Drineas, P. and Kannan, R. Fast monte-carlo algorithms for approximate matrix multiplication. In Proceedings 42nd IEEE Symposium on Foundations of Computer Science, pp. 452–459. IEEE, 2001. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132– 157, 2006a. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132– 157, 2006b. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethink- ing. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022a. URL https://openreview.net/forum? id=2QrFr_U782Z. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethinking. 2022b. Feng, W., Zhang, J., Dong, Y ., Han, Y ., Luan, H., Xu, Q., Yang, Q., Kharlamov, E., and Tang, J. Graph random neural networks for semi-supervised learning on graphs. 9RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Advances in neural information processing systems, 33: 22092–22103, 2020. Feng, W., Dong, Y ., Huang, T., Yin, Z., Cheng, X., Khar- lamov, E., and Tang, J. Grand+: Scalable graph random neural networks. In Proceedings of the ACM Web Confer- ence 2022, pp. 3248–3258, 2022. Fey, M. and Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Repre- sentation Learning on Graphs and Manifolds, 2019. Fey, M., Lenssen, J. E., Weichert, F., and Leskovec, J. Gn- nautoscale: Scalable and expressive graph neural net- works via historical embeddings. In International confer- ence on machine learning, 2021. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. InProceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016. Han, X., Jiang, Z., Liu, N., and Hu, X. G-mixup: Graph data augmentation for graph classification. arXiv preprint arXiv:2202.07179, 2022. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. Huang, G., Dai, G., Wang, Y ., and Yang, H. Ge-spmm: General-purpose sparse matrix-matrix multiplication on gpus for graph neural networks. In SC20: International Conference for High Performance Computing, Network- ing, Storage and Analysis, pp. 1–12. IEEE, 2020a. Huang, Q., He, H., Singh, A., Lim, S.-N., and Benson, A. R. Combining label propagation and simple models out-performs graph neural networks. In International Conference on Learning Representations, 2020b. Huang, W., Zhang, T., Rong, Y ., and Huang, J. Adap- tive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, 2018. Jiang, Z., Han, X., Fan, C., Liu, Z., Zou, N., Mostafavi, A., and Hu, X. Fmp: Toward fair graph message passing against topology bias. arXiv preprint arXiv:2202.04187, 2022. Jin, W., Ma, Y ., Liu, X., Tang, X., Wang, S., and Tang, J. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 66–74, 2020. Kaler, T., Stathas, N., Ouyang, A., Iliopoulos, A.-S., Schardl, T., Leiserson, C. E., and Chen, J. Accelerating training and inference of graph neural networks with fast sampling and pipelining. Proceedings of Machine Learning and Systems, 4:172–189, 2022. Kipf, T. N. and Welling, M. Semi-supervised classi- fication with graph convolutional networks. In In- ternational Conference on Learning Representations , 2017. URL https://openreview.net/forum? id=SJU4ayYgl. Klicpera, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2018. Li, Y ., Wei, C., and Ma, T. Towards explaining the regu- larization effect of initial large learning rate in training neural networks. Advances in Neural Information Pro- cessing Systems, 32, 2019. Liu, Z., Jin, H., Wang, T.-H., Zhou, K., and Hu, X. Di- vaug: Plug-in automated data augmentation with explicit diversity maximization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4762– 4770, 2021. Martinsson, P.-G. and Tropp, J. Randomized numerical linear algebra: foundations & algorithms (2020). arXiv preprint arXiv:2002.01387, 2020. Md, V ., Misra, S., Ma, G., Mohanty, R., Georganas, E., Heinecke, A., Kalamkar, D., Ahmed, N. K., and Avancha, S. Distgnn: Scalable distributed training for large-scale graph neural networks. In Proceedings of the Interna- tional Conference for High Performance Computing, Net- working, Storage and Analysis, pp. 1–14, 2021. Narayanan, S. D., Sinha, A., Jain, P., Kar, P., and SEL- LAMANICKAM, S. Iglu: Efficient GCN training via lazy updates. In International Conference on Learning 10RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Representations, 2022. URL https://openreview. net/forum?id=5kq11Tl1z4. Qiu, J., Dhulipala, L., Tang, J., Peng, R., and Wang, C. Lightne: A lightweight graph processing system for net- work embedding. In Proceedings of the 2021 interna- tional conference on management of data, pp. 2281–2289, 2021. Rahman, M. K., Sujon, M. H., and Azad, A. Fusedmm: A unified sddmm-spmm kernel for graph embedding and graph neural networks. In 2021 IEEE International Par- allel and Distributed Processing Symposium (IPDPS), pp. 256–266. IEEE, 2021. Ramezani, M., Cong, W., Mahdavi, M., Kandemir, M., and Sivasubramaniam, A. Learn locally, correct globally: A distributed algorithm for training graph neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=FndDxSz3LxQ. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas- sification. arXiv preprint arXiv:1907.10903, 2019. Savas, B. and Dhillon, I. S. Clustered low rank approxi- mation of graphs in information science applications. In Proceedings of the 2011 SIAM International Conference on Data Mining, pp. 164–175. SIAM, 2011. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2017. Wan, C., Li, Y ., Kim, N. S., and Lin, Y . {BDS}- {gcn}: Efficient full-graph training of graph convolu- tional nets with partition-parallelism and boundary sam- pling, 2021. URL https://openreview.net/ forum?id=uFA24r7v4wL. Wan, C., Li, Y ., Li, A., Kim, N. S., and Lin, Y . Bns-gcn: Efficient full-graph training of graph convolutional net- works with partition-parallelism and random boundary node sampling. Proceedings of Machine Learning and Systems, 4:673–693, 2022a. Wan, C., Li, Y ., Wolfe, C. R., Kyrillidis, A., Kim, N. S., and Lin, Y . Pipegcn: Efficient full-graph training of graph convolutional networks with pipelined feature communi- cation. arXiv preprint arXiv:2203.10428, 2022b. Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X., Zhou, J., Ma, C., Yu, L., Gai, Y ., Xiao, T., He, T., Karypis, G., Li, J., and Zhang, Z. Deep graph library: A graph- centric, highly-performant package for graph neural net- works. arXiv preprint arXiv:1909.01315, 2019. Wang, Y ., Feng, B., and Ding, Y . Tc-gnn: Accelerating sparse graph neural network computation via dense tensor core on gpus. arXiv preprint arXiv:2112.02052, 2021. Wang, Z., Wu, X. C., Xu, Z., and Ng, T. E. Cupcake: Acom- pression optimizer for scalable communication-efficient distributed training. Wang, Z., Xu, Z., Wu, X., Shrivastava, A., and Ng, T. E. Dragonn: Distributed randomized approximate gradients of neural networks. In International Conference on Ma- chine Learning, pp. 23274–23291. PMLR, 2022. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein- berger, K. Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861– 6871. PMLR, 2019. Xu, K., Zhang, M., Jegelka, S., and Kawaguchi, K. Op- timization of graph neural networks: Implicit acceler- ation by skip connections and more depth. In Inter- national Conference on Machine Learning, pp. 11592– 11602. PMLR, 2021. Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., and Leskovec, J. Graph convolutional neural net- works for web-scale recommender systems. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–983, 2018. Yu, L., Shen, J., Li, J., and Lerer, A. Scalable graph neu- ral networks for heterogeneous graphs. arXiv preprint arXiv:2011.09679, 2020. Yuan, B., Wolfe, C. R., Dun, C., Tang, Y ., Kyril- lidis, A., and Jermaine, C. Distributed learning of fully connected neural networks using independent sub- net training. Proc. VLDB Endow. , 15(8):1581–1590, 2022. URL https://www.vldb.org/pvldb/ vol15/p1581-wolfe.pdf. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V . Graphsaint: Graph sampling based in- ductive learning method. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=BJe8pkHFwS. Zha, D., Feng, L., Tan, Q., Liu, Z., Lai, K.-H., Bhushanam, B., Tian, Y ., Kejariwal, A., and Hu, X. Dreamshard: Gen- eralizable embedding table placement for recommender systems. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Pro- cessing Systems, 2022. URL https://openreview. net/forum?id=_atSgd9Np52. 11RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zha, D., Feng, L., Luo, L., Bhushanam, B., Liu, Z., Hu, Y ., Nie, J., Huang, Y ., Tian, Y ., Kejariwal, A., and Hu, X. Pre- train and search: Efficient embedding table sharding with pre-trained neural cost models. CoRR, abs/2305.01868, 2023. doi: 10.48550/arXiv.2305.01868. URL https: //doi.org/10.48550/arXiv.2305.01868. Zhang, H., Yu, Z., Dai, G., Huang, G., Ding, Y ., Xie, Y ., and Wang, Y . Understanding gnn computational graph: A coordinated computation, io, and memory perspective. Proceedings of Machine Learning and Systems, 4:467– 484, 2022. Zheng, D., Ma, C., Wang, M., Zhou, J., Su, Q., Song, X., Gan, Q., Zhang, Z., and Karypis, G. Distdgl: distributed graph neural network training for billion-scale graphs. In 2020 IEEE/ACM 10th Workshop on Irregular Appli- cations: Architectures and Algorithms (IA3), pp. 36–44. IEEE, 2020. Zhong, S., Zhang, G., Huang, N., and Xu, S. Revisit kernel pruning with lottery regulated grouped convolutions. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=LdEhiMG9WLO. Zhou, K., Liu, Z., Chen, R., Li, L., Choi, S., and Hu, X. Table2graph: Transforming tabular data to unified weighted graph. In Raedt, L. D. (ed.), Proceedings of the Thirty-First International Joint Conference on Ar- tificial Intelligence, IJCAI 2022, Vienna, Austria, 23- 29 July 2022 , pp. 2420–2426. ijcai.org, 2022. doi: 10.24963/ijcai.2022/336. URL https://doi.org/ 10.24963/ijcai.2022/336. Zhou, K., Choi, S.-H., Liu, Z., Liu, N., Yang, F., Chen, R., Li, L., and Hu, X. Adaptive label smoothing to regularize large-scale graph training. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pp. 55–63. SIAM, 2023. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. arXiv preprint arXiv:1911.07323, 2019. 12RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations A. Mathematical Analysis A.1. Why Sampling-based Approximation for GNN? In the main text, we mentioned SpMM is the main speed bottleneck for GNNs. Below we illustrate why the column- row sampling is suitable for accelerating SpMM in GNNs, from the approximation error perspective. Here we analyze ˜AJ(l) = SpMM( ˜A, J(l)) for illustration convenience. For the backward pass of SpMM , the analysis is similar, except that we are approximating ∇J(l) = SpMM( ˜A⊤, ∇H(l+1)). Column-row sampling approximates the matrix production by excluding some “unimportant” columns and rows in the original matrix. So intuitively, the approximation error E[|| ˜AJ(l) − approx( ˜AJ(l))||F ] is low if the “unimportant” columns/rows are correlated in the selected one. Namely, ˜A and J(l) are low-rank. Formally, we have the following theorem: Theorem A.1 ((Martinsson & Tropp, 2020)) . Suppose we approximate ˜AJ(l) using column-row sampling, and pi is obtained by Equation (3). Then for any positive number ϵ, if the number of samples k satisfies k ≥ ϵ−2(srank( ˜A) + srank(J(l))) log(|V| + d), we have E[|| ˜AJ(l) − approx( ˜AJ(l))||F ] ≤ 2ϵ, where srank in Theorem A.1 is called the stable rank, which is the continuous surrogate measure for the rank that is largely unaffected by tiny singular values. Formally for any matrix Y , srank(Y ) =||Y ||2 F ||Y ||2 ≤ rank(Y ). Fortunately, most real-world graphs are cluster-structured, which means the adjacency matrix ˜A is low-rank (Qiu et al., 2021; Savas & Dhillon, 2011). The low-rank property of real-world graphs is also wildly reported in previous work (Jin et al., 2020; Qiu et al., 2021). Moreover, the intermediate activations J(l) and the activation gradients are also low-rank, due to the aggregation. Namely, low-rank means “correlation” in the row/column space. The embedding (i.e., rows in the activation matrix) of connected nodes tend to close due to the graph propagation, which resulting in the low-rank property of the activation matrix. Thus for GNNs, the approximation error is low with a relatively small number of sample k. This perspective is also experimentally verified in the experiment section. A.2. Proof of Proposition 1 Proposition A.2 (Proof in Appendix A). If the approximation method is itself unbiased, and we only replace the SpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. Here we note that in the main text, for the notation convenience, we ignore the backward pass of ReLU. However, the proof here will consider the non-linear activation function to prove the unbiasedness. Let H(l+1) pre = SpMM( ˜A, J(l)) be the pre-activation. The backward pass of ReLU is: E[∇H(l+1) pre ] =E[1 H(l+1) pre >0 ⊙ ∇H(l+1)] = 1 H(l+1) pre >0 ⊙ E[∇H(l+1)], (5) where ⊙ is the element-wise product and 1 is the indicator function. The element-wise product is linear operation and 1 H(l+1) pre >0 is only related to the pre-activation in the forward pass, we only apply the approximation during the backward pass so 1 H(l+1) pre >0 can be extracted from the expectation. We know that for the last layer, we have E[∇H(L)] = H(L) since we do not apply ReLU at the output layer. We then can prove by induction that E[∇H(l+1)] = H(l+1) and E[∇J(l)] =E[approx( ˜A⊤∇H(l+1) pre )] =∇J(l) for any layer l. A.3. Analysis of MEAN aggregator For GraphSAGE, one commonly used aggregator is the MEAN aggregator, which can be expressed as follows: H(l+1) = W1H(l) + W2SpMM MEAN(A, H(l)), (6) 13RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations where SpMM MEAN is one variant of the vanilla SpMM , which replace the reducer function from sum(·) to mean(·). We note that in popular GNN packages, the MEAN aggregator usually is implemented based on SpMM MEAN (Fey & Lenssen, 2019; Wang et al., 2019) to reduce the memory usage. Here we give an example of SpMM MEAN to illustrate how it works: SpMM MEAN(   1 0 0 4 5 6  , \u00147 8 9 10 \u0015 ) = \"1 2 (1 × 7 + 0× 9) 1 2 (1 × 8 + 0× 10) 1 2 (0 × 7 + 4× 9) 1 2 (0 × 8 + 4× 10) 1 2 (5 × 7 + 6× 9) 1 2 (5 × 8 + 6× 10) # , Equivalently, the SpMM MEAN can also be expressed as: SpMM MEAN(A, H(l)) =D−1AH(l), where D is the degree matrix ofA. Thus, although we did not normalize the adjacency matrix in GraphSAGE, when applying the top-k sampling to approximate SpMM MEAN, the column norm of A:,ji is actually 1√ Degji due to the normalization. Also, for GraphSAGE, the inputs to the first SpMM MEAN operation are A and X. They do not require gradient since they are not trainable. Thus, the first SAGE layer is not presented in Figure 8 and Figure 7. B. Pseudo code of the greedy algorithm Algorithm 1 The greedy algorithm Inputs: Gradients of node embeddings{∇H(1), ···∇ H(L)}, adjacency matrix A, graph G = (V, E), hidden dimensions {d1, ··· dL}. Parameters: The step size α, the overall budget C. Outputs: The layer-wise {k1, ··· kL} associated with the top-k sampling. B ← PL l=1 |E|dl. ∀i, kl ← |V|, Topkl ← {1, ···|V|} . while B ≥ C PL l=1 |E|dl do m ← arg minl∈{1,···L}(P i∈Topkl ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥A∥F ∥∇H(l+1)∥F − P i∈Topkl−α|V| ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2) ∥A∥F ∥∇H(l+1)∥F /* Choose the layer m to reduce by a step size α|V|, such that the increment of errors is minimal. */ B ← B − dm P i∈Topkm∩i/∈Topkm−α|V| #nnzi /*Since we exclude some column-row pairs for layer m, here we reduce the budget B accordingly. */ km ← km − α|V| /* Update km accordingly. */ Topkm ← the set of indices i associated with km largest ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥A∥F ∥∇H(l+1)∥F /* Update Topkm accordingly. */ end while Return {k1, ··· , kL} In algorithm 1, here we provide the pseudo code of our greedy algorithm for solving the constrained optimization problem. In Table 11, we show the run time of the greedy algorithm, which is negligible compared to the acceleration effect. C. Extended Related works Connections to Graph Data Augmentation Data augmentation (Liu et al., 2021; Han et al., 2022) is wildly adopted in the graph learning for improving model generalization, including dropping nodes (Feng et al., 2020), dropping edges (Rong et al., 2019), and graph mixup (Han et al., 2022). As shown in Figure 5, the top- k sampling drops the entire columns in the adjacency matrix, while keeping the number of rows unchanged. That means RSC drops all of the out edges for a set of nodes. This can be viewed as the “structural dropedge” for improving the efficiency. Since we only apply the top-k sampling in the backward pass and top- k indices are different for each operation, RSC essentially forward pass with the whole graph, backward pass with different subgraphs at each layer. This structural dropedge and heterogeneous backward propagation introduce the regularization effect. Thus as shown in the experiment section, RSC may also improve the model accuracy over the baseline. 14RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Subgraph-based GNN training. The key idea of this line of work is to improve the scalability of GNNs by separating the graph into overlapped small batches, then training models with sampled subgraphs (Hamilton et al., 2017; Huang et al., 2018; Zou et al., 2019; Chiang et al., 2019; Zeng et al., 2020). Based on this idea, various sampling techniques have been proposed, including the node-wise sampling (Hamilton et al., 2017; Chen et al., 2017), layer-wise sampling (Huang et al., 2018; Zou et al., 2019), and subgraph sampling (Chiang et al., 2019; Zeng et al., 2020). However, this approach reduces the memory footprint but results in extra time cost to compute the overlapping nodes between batches. Generally, methods in this category are orthogonal to RSC , and they can be combined. Graph precomputation. The graph precomputation methods decouple the message passing from the model training, either as a preprocessing step (Wu et al., 2019; Klicpera et al., 2018; Yu et al., 2020) or post-processing step (Huang et al., 2020b), where the model is simplified as the Multi-Layer Perceptron (MLP). We did consider this line of work in this paper since the backbone model is not GNN anymore. Distributed GNN training. The distributed training leverages extra hardwares to increase the memory capacity and training efficiency (Zha et al., 2023; 2022; Yuan et al., 2022; Wang et al., 2022; Wang et al.). However, the graph data cannot be trivially divided into independent partitions due to the node connectivity. Thus, the graph distributed training frameworks propose to split graph into related partitions and minimize the communication overhead (Wan et al., 2021; 2022b; Ramezani et al., 2022). Our methods are orthogonal to this line of work. Other randomized GNN training. Dropedge (Rong et al., 2019) randomly drops edges to avoid the over-smoothing problem. Graph Random Neural Networks (Grand) (Feng et al., 2020) randomly drop nodes to generate data augmentation for improving model generalization. Grand+ improves the scalability over Grand by pre-computing a general propagation matrix and employ it to perform data augmentation (Feng et al., 2022). As shown in Section C, the key difference between GRAND(+) and RSC is that RSC does not drop any node. Instead RSC drops all of the out edges for a set of nodes only during backward pass. Moreover, the drop pattern are evolving during the training process. This can be viewed as the “structural dropedge”. However, unlike Dropedge (Rong et al., 2019), RSC drop the column-row pairs according to the euclidean norm instead of uniformly dropping. D. Experimental Settings D.1. Software and Hardware Descriptions All experiments are conducted on a server with four NVIDIA 3090 GPUs, four AMD EPYC 7282 CPUs, and 252GB host memory. We implement all models based on Pytorch and Pytorch Geometric. During our experiments, we found that the version of Pytorch, Pytorch Sparse, and Pytorch Scatter can significantly impact the running speed of the baseline. Here we list the details of our used packages in all experiments in Table 5. Table 5: Package configurations of our experiments. Package Version CUDA 11.1 pytorch sparse 0.6.12 pytorch scatter 2.0.8 pytorch geometric 1.7.2 pytorch 1.9.0 OGB 1.3.2 D.2. Statistics of benchmark datasets The statistics for all used datasets are shown in Table 6. We follow the standard data splits and all datasets are directly downloaded from Pytorch Geometric or the protocol of OGB (Hu et al., 2020). D.3. Hyperparameter Settings Regarding Reddit and Yelp dataset, we follow the hyperparameter reported in the respective papers as closely as possible. Regarding ogbn-proteins and ogbn-products dataset, we follow the hyperparameter configurations and codebases provided 15RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 6: Dataset Statistics. Dataset Task Nodes Edges Classes Label Rates Reddit multi-class 232,965 11,606,919 41 65.86% Yelp multi-label 716,847 6,977,409 100 75.00% ogbn-proteins binary-Class 132,534 39,561,252 2 65.00% ogbn-products multi-class 2,449,029 61,859,076 47 8.03% on the OGB (Hu et al., 2020) leader-board. Please refer to the OGB website for more details. The optimizer is Adam for all these models. All methods terminate after a fixed number of epochs. We report the test accuracy associated with the highest validation score. Table 10 summarize the hyperparameter configuration of GraphSAINT. Table 7, Table 8, and Table 9 summarize the hyperparameter configuration of full-Batch GCN, GraphSAGE, and GCNII, respectively. Table 7: Configuration of Full-Batch GCN. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 8: Configuration of Full-Batch GraphSAGE. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 9: Configuration of Full-Batch GCNII. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 4 256 Yelp 0.01 500 0.1 Yes 4 256 ogbn- proteins 0.01 1000 0.5 No 4 256 E. More experiment results The running time of the greedy algorithm is shown in 11. We also visualize the allocated kl for each layer across iterations in Figure 7, and the degree of picked nodes in Figure 8. Here we use Reddit dataset for the case study. We observe that the kl assigned by RSC evolves along with the training. 16RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 10: Training configuration of GraphSAINT. Dataset RandomWalk Sampler Training Archtecture Walk length Roots Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 4 8000 0.01 40 0.1 Yes 3 128 Yelp 2 8000 0.01 75 0.1 Yes 3 512 ogbn- products 3 60000 0.01 20 0.5 No 3 256 Table 11: The running time (second) of the greedy algorithm. Reddit Yelp ogbn- proteins ogbn- products GCN 0.03 0.03 0.03 0.03 GraphSAGE 0.02 0.02 0.03 0.03 GCNII 0.05 0.05 0.06 - /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000015/uni00000013 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000017/uni00000057/uni0000004b/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 Figure 7: The allocated layer-wise kl for GCN, GraphSAGE and GCNII on Reddit, where budget C is set as 0.1. The input of the SpMM in the first GraphSAGE layer does not require gradient and thus absent in the Figure (Appendix A.3). 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCN 1st layer GCN 2nd layer GCN 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 20 40 60 80 100 120Node degrees GraphSAGE 2nd layer GraphSAGE 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCNII 1st layer GCNII 2nd layer GCNII 3rd layer GCNII 4th layer Figure 8: The averaged degrees of nodes picked by top-k sampling along the whole training process, where the applied dataset is Reddit and overall budget C is set as 0.1. E.1. Additional Ablation Results to the Resource Allocation Algorithm (Figure 6) Due to the page limit, we present more ablation study on the resource allocation algorithm here. Specifically, in Figure 9, we compare RSC to the uniform allocation on ogbn-proteins dataset with GCN, GraphSAGE, and GCNII, respectively. In Figure 10, we compare RSC to the uniform allocation on Yelp dataset with GCN, GraphSAGE, and GCNII, respectively. We conclude that RSC generally outperforms the uniform allocation strategy. E.2. Hyperparameter Sensitivity Analysis Here we analyze the impacts of the main hyperparameters of RSC : (1) the budget C, which controls the efficiency-accuracy trade-off; (2) the step size α in the greedy Algorithm 1; (3) when switching back to the original sparse operations. In Figure 12, we vary only one of them with the others fixed. We conclude (1) larger budget C leads to better accuracy with smaller speedup, since we are using more computational resources to approximate the full operation. (2) larger step size α leads 17RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1.01.21.41.61.8 Speedup 67 68 69 70 71 72Accuracy (%) GCN (ogbn-proteins) Uniform Allocation RSC 1.21.41.61.82.02.2 Speedup 70 71 72 73 74 75 76Accuracy (%) GraphSAGE (ogbn-proteins) Uniform Allocation RSC 1.01.21.41.61.8 Speedup 64 66 68 70 72 74Accuracy (%) GCNII (ogbn-proteins) Uniform Allocation RSC Figure 9: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is ogbn-proteins. Here we disabled the caching and switch mechanism for a fair comparison. 0.960.981.001.021.041.061.081.10 Speedup 42 44 46 48Accuracy (%) GCN (Yelp) Uniform Allocation RSC 1.0001.0251.0501.0751.1001.1251.1501.175 Speedup 63.0 63.2 63.4 63.6 63.8 64.0Accuracy (%) GraphSAGE (Yelp) Uniform Allocation RSC 1.101.121.141.161.181.201.221.241.26 Speedup 64.0 64.1 64.2 64.3 64.4Accuracy (%) GCNII (Yelp) Uniform Allocation RSC Figure 10: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is Yelp. Here we disabled the caching and switch mechanism for a fair comparison. to marginally larger speedup since the greedy algorithm will terminate earlier. Also the step size α does not affect the model accuracy a lot. In practice, we set α = 0.02|V|. (3) The later we switch back to the original operation, the larger the accuracy drop and the smaller the speedup, it is equivalent to using less resources to approximate the full operation epoch-wisely. Thus, we apply RSC for 80% of the total epochs to balance the trade-off. 0 100 200 300 400 Epochs 20 40 60 80Validation Accuracy GCN (Reddit) Baseline C=0.1 C=0.2 C=0.3 0 100 200 300 400 Epochs 20 40 60 80 100Validation Accuracy GCNII (Reddit) Baseline C=0.1 C=0.2 C=0.3 Figure 11: Learning curves for validation accuracy under different overall budget C on Reddit dataset. Here we disabled the caching and switching mechanism for ablating the effect of C. 0.1 0.2 0.3 0.4 0.5 (a) Budget C 73 74 75 76Test AUC Baseline AUC RSC AUC RSC Speedup 1.5 1.6 1.7 1.8 Speedup GraphSAGE (ogbn-proteins) 0.01| |  0.02| |  0.05| |  0.1| |  0.2| | (b) step size  75.9 76.0 76.1 76.2 76.3Test AUC Baseline AUC RSC AUC RSC Speedup 1.58 1.60 1.62 1.64 Speedup GraphSAGE (ogbn-proteins) At 60%  total epochs At 70%  total epochs At 80%  total epochs At 90%  total epochs At 95%  total epochs (c) When switching back to the original 75.50 75.75 76.00 76.25 76.50Test AUC Baseline AUC RSC AUC RSC Speedup 1.45 1.50 1.55 1.60 1.65 1.70 Speedup GraphSAGE (ogbn-proteins) Figure 12: Hyperparameter analysis w.r.t. the budget C, the step size α in Algorithm 1, and when switching back to the original operations. The model is GraphSAGE and the applied dataset is ogbn-proteins. 18",
      "meta_data": {
        "arxiv_id": "2210.10737v2",
        "authors": [
          "Zirui Liu",
          "Shengyuan Chen",
          "Kaixiong Zhou",
          "Daochen Zha",
          "Xiao Huang",
          "Xia Hu"
        ],
        "published_date": "2022-10-19T17:25:33Z",
        "pdf_url": "https://arxiv.org/pdf/2210.10737v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Randomized Sparse Computation (RSC) to accelerate Graph Neural Network (GNN) training, which is typically time-consuming due to sparse graph-based operations (SpMM) that are difficult for hardware to accelerate. RSC addresses two unique challenges of approximating sparse operations: efficiently controlling the accuracy-efficiency trade-off despite irregular data formats, and mitigating the high overhead of sampling sparse matrices. The core idea is to optimize computation resource allocation layer-wisely and epoch-wisely. RSC achieves up to 11.6x speedup for single sparse operations and 1.6x end-to-end wall-clock time speedup with negligible accuracy drop (approx. 0.3%). A key finding is that approximating only the sparse operations in the backward pass yields provably unbiased gradients and maintains high accuracy.",
        "methodology": "RSC is an approximation framework that replaces expensive sparse operations in GNNs with faster-approximated versions. It specifically targets the Sparse-Dense Matrix Multiplication (SpMM) in the backward pass, as approximating the forward pass leads to biased node embeddings due to non-linear activation functions. The approximation uses a top-k sampling algorithm, picking column-row pairs with the largest Euclidean norms. RSC introduces three main components: 1) A layer-wise resource allocation strategy, formulated as a constrained optimization problem, minimizes approximation error across layers by customizing FLOPs (via adjusting 'k' samples) for each sparse operation under a total computational budget. This is solved efficiently using a greedy algorithm. 2) An epoch-wise caching mechanism reuses previously sampled sparse matrices across nearby iterations, significantly reducing the overhead of slicing sparse matrices, based on the observation that top-k indices remain similar. 3) A switching mechanism applies approximated sparse operations for most of the training epochs (e.g., 80%) but switches back to original, exact sparse operations in the final stages to fine-tune the model and minimize accuracy drop.",
        "experimental_setup": "RSC was evaluated on four large-scale graph benchmarks: Reddit, Yelp, ogbn-proteins, and ogbn-products. Experiments were conducted under both mini-batch training (integrating RSC with GraphSAINT) and full-batch training (integrating RSC with GCN, GraphSAGE, and GCNII). The hardware setup involved a server with four NVIDIA RTX 3090 GPUs, AMD EPYC 7282 CPUs, and 252GB host memory. Software included PyTorch and PyTorch Geometric with specific versions of PyTorch Sparse, PyTorch Scatter, and OGB. Key hyperparameters included the overall FLOPs budget (C from {0.1, 0.3, 0.5}), the greedy algorithm's step size (alpha=0.02|V|), caching frequency (sampling every 10 steps), and the switching point (RSC used for 80% of total epochs). Performance was measured by wall-clock time speedup (T_baseline / T_RSC) and test accuracy/F1-micro/AUC, with results averaged over ten random trials.",
        "limitations": "The primary limitation is that RSC only approximates sparse operations in the backward pass to guarantee model accuracy, which consequently limits the maximum achievable speedup. Although the backward pass is often the most time-consuming part, accelerating the forward pass would yield further gains. Another limitation is that the current framework does not directly cover GNNs that rely on scatter-and-gather operations (e.g., GAT) instead of Sparse Matrix-Matrix Multiplication (SpMM) or its variants for aggregation. While the core sampling, caching, and switching mechanisms could potentially apply to these GNNs, their resource allocation algorithm (Algorithm 1) would require tailored error bounds and computation cost modeling specific to scatter-and-gather operations.",
        "future_research_directions": "Future work will explore extending RSC to GNNs that rely on scatter-and-gather operations for aggregation. This would involve developing tailored error bounds and computation cost modeling within the resource allocation algorithm (Equation 4) to effectively apply RSC's principles to these different operational structures."
      }
    },
    {
      "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
      "abstract": "Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.",
      "full_text": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort Qualcomm AI Research∗ Amsterdam, The Netherlands {ybond, markusn, tijmen}@qti.qualcomm.com Abstract Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI signif- icantly. Due to their size, the capability of these networks has increased tremen- dously, but this has come at the cost of a significant increase in necessary com- pute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a “no-op” or just a partial update of the residual. To achieve the exact zeros needed in the attention matrix for a no-update, the input to the softmax is pushed to be larger and larger during training, causing outliers in other parts of the network. Based on these observations, we propose two sim- ple (independent) modifications to the attention mechanism - clipped softmax and gated attention . We empirically show that models pre-trained using our methods learn significantly smaller outliers while maintaining and sometimes even improving the floating-point task performance. This enables us to quantize transformers to full INT8 quantization of the activations without any additional effort. We demonstrate the effectiveness of our methods on both language models (BERT, OPT) and vision transformers. Our source code is available at https: //github.com/qualcomm-ai-research/outlier-free-transformers . 1 Introduction Quantization has been one of the most impactful ways to reduce the computational complexity of transformer networks. Previous work has shown that quantizing networks to 4-bit weights is possible without losing too much accuracy [66, 69]. Some research even shows 4-bit weights might be optimal when trading off model size and bit-width [12]. However, quantizing transformers is not always trivial. When quantizing the activations of a trans- former, significant problems arise with outliers in specific layers. This has been noted by several researchers that suggest fixes to transformers after training to ameliorate their effect [13, 67]. These methods are frequently tedious and either require retraining the network, require implementing specific hardware for input-channel quantization [13] or require parts of the activations to still be in higher bit-widths, reducing the effectiveness of the activation quantization [67]. In this paper, we set out to solve the transformer outlier problem entirely by changing the architecture of the network itself. We hope to make transformers easy to quantize from the get-go without needing ∗Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.12929v2  [cs.LG]  9 Nov 2023any post-processing. To do so, we thoroughly analyze why these outliers appear. Previous work has found the existence of these outliers [4, 13], but in our work, we come to a fuller understanding of these outlying values. We find that the outliers occur because attention heads are trying not to update the hidden state, and in the process, strong outliers appear due to the softmax function. This happens for language and vision transformers and different specific transformer architectures. This understanding is the foundation for two new tweaks we suggest to transformer architectures that can remove the problem of the outliers entirely. 2 Background and related work In this section, we briefly cover the basics of neural network quantization and discuss why modern transformers are difficult to quantize. Quantization One of the most powerful ways to decrease the computational time and memory consumption of neural networks is quantization, which uses low-bit representations for the weights and activation tensors. On top of that, using low-bit fixed-point representations, such as INT8, one can further reduce energy consumption since the fixed-point operations are more efficient than their floating-point counterparts [23, 59]. We simulate the quantization process in floating-point according to Jacob et al. [26]. We use the following definition of the quantization function: bx := q (x; s, z, b) =s · \u0010 clip \u0010jx s m + z; 0, 2b − 1 \u0011 − z \u0011 , (1) where x denotes the quantizer input (i.e., network weights or activations), s ∈ R+ the scale factor or the step-size, z ∈ Z the zero point, and b ∈ N the bitwidth. ⌊·⌉ denotes the round-to-nearest-integer operator. This quantization scheme is called uniform affine or asymmetric quantization [24, 32, 76] and it is one of the most commonly used quantization schemes because it allows for efficient implementation of fixed-point arithmetic. In the case of symmetric quantization, we restrict the quantization grid to be symmetric around z = 0. In this work, we focus on post-training quantization (PTQ) methods, which take a pre-trained FP32 network and convert it directly into a fixed-point network without the need for the original training pipeline [2, 5, 7, 25, 32, 35, 41, 43, 44, 75]. These methods require either no data or only a small calibration dataset and are easier to use compared to quantization-aware training (QAT, Bhalgat et al. 3, Esser et al. 16, Gupta et al. 21, Jacob et al. 26, Krishnamoorthi 32) methods that have you train the entire network for more epochs. For more details on neural network quantization, we refer the reader to [19, 46]. Outliers in Transformers Multiple studies have shown that modern transformer-based language models tend to learn outliers in weights and activations [4, 13, 31]. These outliers are present only in a small fixed set of embedding dimensions, but they appear regularly and consistently across multiple layers and data sequences. It was also shown that those outliers play a crucial role in the model predictions and clipping them or by setting to zero the corresponding parameters significantly degrades the model task performance [31, 49]. The strongest in magnitude outliers typically appear at the output of the feed-forward network, FFN, although Dettmers et al.[13] showed that for big enough transformer-based language models they start appearing after every linear layer, including query, key, and value projection layers. This phenomenon holds for many tasks, training objectives and models (both encoder and decoder transformers), including BERT [14], RoBERTa [37], DistilBERT [53], MobileBERT [55], ELECTRA [9], BART [33], XLNet [68], GPT-2 [50], and OPT [74]. Because of these strong outliers, applying per-tensor PTQ for the FFN’s output and the residual sum will likely cause a notable error because of the following trade-off between the range and the precision. On the one hand, using a large quantization range for small-ranged values leads to a loss in representation (high rounding error). On the other hand, a small quantization range for large values leads to a very high clipping error. For the case of significant transformer outliers, frequently, no good trade-off can be found between the rounding and clipping error, resulting in an overall high error. There have been numerous attempts to fix the issue of transformer quantization [4, 12, 13, 17, 27, 28, 51, 54, 62, 63, 69, 71]. Most of these approaches resort to finer quantization granularity (row-wise, 2(a) FFN output in layer #10  (b) FFN output in layer #11 Figure 1: Histograms of outlier counts vs. token positions (blue) and hidden dimensions (green), recorded from the MNLI-m validation set on BERT-base. We use zero-based indexing for dimensions. channel-wise, group-wise weight and activation quantization), use higher bitwidth and/or different numeric format to represent those outliers better or require extra fine-tuning (in the form of QAT and/or knowledge distillation). In other words, they adapt quantization to work with outliers, which often comes at the expense of general applicability or extra inference overhead. In contrast, in this work, we want to address the root cause of the problem and understand why outliers are learned in the first place and suggest a new pre-training protocol that significantly reduces the magnitude of outliers yielding way more quantization-friendly models that can be effortlessly quantized using PTQ without strong degradation of performance. 3 Outlier analysis Outliers in BERT models In Section 2 we discussed that outliers are present only in a few designated embedding dimensions but they appear regularly and consistently across multiple layers and data sequences. We also discussed that the strongest magnitude outliers in BERT typically appear at the output of FFN in the last encoder layers. We start by taking the pre-trained BERT-base-uncased checkpoint from HuggingFace [65] and fine- tune it on MNLI dataset from the well-known GLUE benchmark [ 61] (see experimental details in C.1). To identify the outlier dimensions, we pass the MNLI-m validation set through the network and record all outliers1 at the FFN output in layers #10 and #112. As we can see in Figure 1, there are indeed only a few hidden dimensions where outliers ever occur. We also notice that the majority of outliers (> 97%) correlate with the position of delimiter tokens – [SEP], “.”, and “,”. To better understand the role of those outliers, we analyze the attention patterns of the corresponding attention heads. BERT-base uses multi-head attention with nheads = 12and each head operating on a consecutive subset of dhead = 64features. Therefore, the hidden dimension #180, which happens to have the highest outlier count in both layers #10 and #11, corresponds to attention head #3. In Figure 2 (and more examples in Appendix A.1) we show examples of the attention matrices, values and their product for that head. A common pattern we found is that the attention head assigns almost all of its probability mass to [SEP] tokens, and other less informative tokens like dots/commas, while these tokens also have small values in V associated with those tokens. This results in a small magnitude product between the two (see Figure 2a). This effectively corresponds to a (soft) no-update of the hidden representation, where only small noise is added after the residual. In other cases (Figure 2b and 2c), we observe that a significant portion of attention probability is still spent on delimiter tokens. However, by allocating some of the probability mass on other tokens (together with the small values for the delimiter tokens), this results in a (soft) selective update of the hidden representation. These patterns in self-attention seem to be a learned “workaround” for the limitations of having the softmax and the residual connections in cases where the attention head does not want to update the representation of some or all of the tokens. These observations are in line with Clark et al. [8], Kovaleva et al. [30] that also argued that attending exclusively or almost exclusively to delimiter tokens such as [SEP], periods/commas acts as a “no-op” when the attention head’s function is not applicable. 1We follow Bondarenko et al. [4] and consider outliers as values that exceed 6 standard deviations from the mean of the corresponding activation tensor. 2We use 1-based indexing for encoder layers and attention heads throughout the paper. 3(a) Attention layer #11, data sequence #1 (b) Attention layer #11, data sequence #5 (c) Attention layer #10, data sequence #5 Figure 2: Visualization of the patterns in the self-attention, specifically the attention probabilities, values, and their product (left, middle and right columns, respectively), in attention head #3 for BERT-base, computed on several data sequences from MNLI-m validation set. (a)  (b)  (c)  (d)  (e) Figure 3: A summary of our outlier analysis for ViT demonstrated on a random image from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values for outlier and non-outlier patches. Outliers in ViT We conduct a similar analysis for Vision transformer [15] trained on ImageNet [52]. For this study, we use a pre-trained checkpoint following our experimental setup from Section 5. We highlight our findings in Figure 3 and provide more examples in Appendix A.2. Our analysis shows many similarities to the BERT case. Instead of delimiter tokens, the majority of outliers seem to correlate with some random uninformative patches (e.g., in the background). We also see that the corresponding attention head in the next layer allocates the majority of attention probabilities to the same patches. Finally, those outlier patches on average have a distinctly smaller magnitude of values compared to non-outlier ones, leading to similar no-update behavior. The fact that those values are not as close to zero as it was in the BERT case might be related to the smaller model capacity3, or a relatively shorter training procedure. Hypothesis Based on these observations, we pose the following hypothesis on how this behavior of attention heads is related to outliers: 1. In order for an attention block to not update a representation of a token on the residual, some attention heads want to allocate most of their attention probability mass to some fixed and common set of tokens that have a low information content (e.g., delimiter tokens or background patches) that can be learned to have a small value function output. 3We use ViT/S-16 configuration that has only 22M parameters. 42. From the definition of the softmax function4, it is easy to see that this would require an input of the softmax to have a relatively big dynamic range (Figure 4, 1 ). In fact, in the limit case where softmax is exactly zero, this would require an infinite dynamic range: softmax (x)i = 0 ⇔ ∃ j ̸= i, xj − xi = +∞ (2) 3. Since Layer Normalization ([1], 2 ) normalizes the outliers, the magnitude of the FFN output in the previous layer ( 3 ) has to be very high to still produce a sufficiently big dynamic range after the LayerNorm. Note, that this is also applicable for the transformer models with LayerNorm applied prior to the self-attention or linear transformations instead, a variant adopted by GPT, OPT, and many vision transformers [15, 38, 57, 58]. 4. Finally, as softmax will never output exact zeros, it will always back-propagate a gradient signal to grow bigger outliers5. The outliers will thus tend to become stronger in magnitude, the longer the network is trained. 4 Method Figure 4: A schematic illus- tration of the attention layer in BERT. Hidden activation tensor is denoted by x. ⊕ is an element-wise addition. A problematic output of the FFN that generates largest in magni- tude outliers is highlighted in red. Notice how those outliers in the previous layer influence the behavior in the attention mechanism in the next layer. In this section, we introduce our proposed modifications for the softmax attention mechanism. Based on our insights from Section 3, the core idea of these modifications is to grant the model the ability to produce very small the magnitude (or even exact zeros) output of attention function, without producing outliers. Recall that the self-attention [60] is defined as follows: Attention(x) := softmax \u0012Q(x)K(x)T √dhead \u0013 V (x) (3) where Q, K and V are learnable linear projections of the input x. Most modern transformer models employ a multi-headed variant of self-attention, where dmodel features are partitioned into nheads groups of dhead features, and the final output is the concatenation of the outputs of (3) applied to each group. 4.1 Clipped softmax First, we propose to replace softmax function in (3) with the follow- ing clipped softmax: clipped_softmax(x; ζ, γ) := clip ((ζ − γ) · softmax(x) +γ, 0, 1) . (4) Here x is the input and ζ ≥ 1, γ ≤ 0 are the stretch factors which are hyper-parameters of the method. Similar formulation was used before for sigmoid function [40, 45]. We can view (4) as stretching the output of the softmax from(0, 1) to (γ, ζ) and then clipping back to (0, 1) so that we can represent exact zeros if γ <0 and exact ones if ζ > 1. Specifically, the values of the softmax larger than 1−γ ζ−γ are rounded to one whereas values smaller than −γ ζ−γ are rounded to zero. With this drop-in replacement, we can achieve exact zeros (and ones) with a finite range for the softmax input. In addition to that, whenever values are clipped they will not give a gradient, preventing the outliers to grow further. 4softmax (x)i = exp (xi) / Pd j=1 exp (xj) 5Let y = softmax (x). Then ∂yi ∂xj ̸= 0∀i, j. 54.2 Gated attention An alternative way of architecting the model to have a small attention output without outliers is to equip it with an explicit conditional gating mechanism, as shown in Figure 5. The idea is that the model can use the gating to either keep or nullify the update to the representation of certain tokens and not rely on the attention probabilities and values to achieve the same outcome. Specifically, we propose the following modification to the attention function: Gated_attention(x) := sigmoid (G(x)) ⊙ softmax \u0012Q(x)K(x)T √dhead \u0013 V (x). (5) Here G is the gating function,⊙ is an element-wise multiplication across the token axis and everything else remains the same as in (3). The gating function G is parameterized by a small neural network that is learned jointly with the rest of the model. We replace the attention formulation with the proposed variant in every layer on the transformer network. Figure 5: A schematic il- lustration of our proposed gated attention. Gating module design Recall that the input to the attention layer x has shape (T, dmodel) that is reshaped into (nheads, T, dhead) for the multi-headed self-attention, where T is the sequence length. We chose to define the gating function on a per-head basis. For each head i ∈ {1, . . . , nheads}, we specify Gi : Rdhead → R and the output of the gating module is πi ∈ RT that is computed as follows: bπi,t = Gi(xi,t,:) ∀t ∈ {1, . . . , T} (6) πi,: = sigmoid(bπi,:), (7) note that gating modules are shared between different token positions but not shared across attention heads. We want our gating module to be as lightweight as possible. To start with, we experiment with Gi’s parameterized by a single linear layer. This gives us a gating module that is computationally inexpensive and has a memory overhead of just nheads · (dhead + 1)∼ dmodel extra parameters (which is equivalent to 1 extra token) per attention layer6. We also investigate the effect of using several other gating functions in Appendix B.1. 5 Experiments In this section, we evaluate the proposed modifications to self-attention on several language models (BERT, OPT) and the vision transformers (ViT). We first test the different hyperparameters for the methods and provide insight into how they work. Then we set out to test our method in terms of accuracy, and the difference in quantization improvement after training. All detailed hyperparameters of our experiments are in Appendix C. BERT We experiment with BERT-base-uncased (109M parameters) pre-training using the masked language modeling (MLM) objective. Following [ 14], we use the concatenation of the training sets of BookCorpus [77] and English Wikipedia7. We implement our methods in PyTorch [48] and use training and evaluation pipelines from HuggingFace libraries [ 20, 34, 65]. We follow closely the pre-training procedure from [ 14]. To speed up training and experimentation, we train with a maximum sequence length of 128 for the whole duration of the training. We evaluate on Wikipedia validation set and report the MLM perplexity. OPT We experiment with a 125M sized variant of OPT [74] pre-training using the causal language modeling (CLM) objective. Due to compute constraints, we train the model on the same dataset that was used for BERT pre-training (BookCorpus + Wikipedia) with a maximum sequence length of 512 6For instance, in case of BERT-base, this amounts to less than 0.009% of the total model size. 7Specifically, we use the English subset of Wiki-40b,https://huggingface.co/datasets/wiki40b, that contains cleaned-up text of English Wikipedia and training/validation splits. 6γ ζ FP16 ppl.↓ Max inf. norm Avg. kurtosis W8A8 ppl. ↓ 0 1 4.49±0.01 735±55 3076±262 1294±1046 (= Vanilla) 0 1 .003 4.48±0.01 715±335 2159±238 451±57 0 1 .03 4.49±0.00 741±66 1707±1249 1469±646 −0.003 1 4.46±0.00 688±64 2149±110 636±566 −0.03 1 4.41±0.01 20±1 80±6 4.55±0.01 −0.003 1 .003 4.47±0.00 683±23 2494±1205 268±120 −0.03 1 .03 4.43±0.03 22±3 73±8 4.56±0.05 Table 1: The impact of clipped softmax hyperparameters on BERT-base. and batch size of 192. Similar to our BERT experiments, we use training and evaluation pipelines from HuggingFace libraries. We evaluate on Wikipedia validation set and report the CLM perplexity. ViT Finally, we explore the effectiveness of proposed techniques on vision transformer [15] (ViT- S/16 configuration, 22M parameters) trained on ImageNet-1K [11, 52]. For these experiments, we adopt the training and validation pipelines from PyTorch Image models library [64]. We report top-1 accuracy on the validation set of ImageNet. Quantization setup In all experiments, after the model is trained, we apply 8-bit PTQ. We use uniform affine quantization – symmetric weights, asymmetric activations – with the static activation range setting, as discussed in Section 2. We quantize all weights and activations (both input and output), except the final linear layer for BERT and OPT models. We explore several choices of range estimation (see Appendix C.4) and report the best configuration for each experiment, based on the model performance. We repeat each PTQ experiment 3 times with different random seeds8 and report mean and standard deviation for accuracy/perplexity. We train each network two times with different random seeds and report mean and standard deviation. To assess the amount of outliers in the trained model, we use two metrics: the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. These metrics have been shown to correlate well with the model quantizability [4, 6]. 5.1 The impact of clipped softmax hyperparameters (γ and ζ) We investigate the effect of different values of the clipped softmax stretch parameters and present the results in Table 1. We can see that most of the improvement happens when we use γ < 0 (clipping at zero). For instance, using the value of γ = −0.03 leads to a significantly smaller infinity norm, kurtosis, and quantized model perplexity, compared to the baseline. It is also clear that in the limit |γ| →0 we approach the vanilla softmax attention. Using ζ >1 (clipping at one) yields similar results to the vanilla softmax. Finally, when we combine both γ <0 and ζ >1, for which the results seem similar to just clipping at 0. We, therefore, conclude that for dampening outliers, only the lower-range clipping allows exact zeros matter. Going forward we use only γ <0 and in Appendix B.5 we confirm that ζ >1 is not required for ViT. These observations are in line with our hypothesis that by giving the model the mechanism for representing exact zeros in the attention, we don’t need to learn the strong outliers. 5.2 Clipped softmax γ vs. sequence length As having an extra hyper-parameter that needs to be tuned per model or setup is generally not desirable, we study the sensitivity of the stretch factor γ and its relation with the sequence length T. Recall that the matrix of attention probabilities P has dimensions T × T and each row sums up to one. Because of that, the average value in P is 1/T. It is reasonable to assume that if we define 8Different random subsets of training data are used for quantizer range estimation. 7(a) Relative FP16 log-perplexity  (b) Maximum infinity norm Figure 6: The performance of clipped softmax using γ = −α/T parameterization on BERT-6L. (a) Relative (compared to vanilla softmax pre-training) FP16 log-perplexity ↑ on Wikitext validation set. (b) Maximum infinity norm of the attention layer output (note the logarithmic y-axis). (a) BERT-6L  (b) ViT Figure 7: The performance of Linear gated attention using different bias initialization settings. γ := −α T , where α >0 is a new hyperparameter, there might be a set or a range of values of α that works well across different sequence lengths. To study this, we train a 6-layer variant of BERT-base (BERT-6L) for 500000 steps on WikiText- 103 [ 42] with a batch size of 128 with several values of maximum sequence lengths T ∈ {32, 64, 128, 192, 256} and values of α ∈ {1/4, 1/2, 1, 2, 4, 8}. As we can see from Figure 6, using a clipped softmax with α ∈ [2, 4] significantly dampens the magnitude of outliers while maintaining good FP16 perplexity across all explored sequence lengths. 5.3 The impact of bias initialization in gated attention In all our gated attention experiments, we randomly initialize the weights of G, following [22]. By initializing the bias to a specific value, however, we can set gates to be more open or more closed initially. More open at the start means we initialize closer to the original network, but given the exponential nature of the gate it might take many iterations for the gate to learn to close. Similarly, if the gates are all closed at the start, we deviate too far from the original model training, causing a potential decrease in performance. Assuming Linear Gi’s with small initial weights, if we set the bias to the value of binit, then Gi(·) ≈ binit and πi(·) = sigmoid(Gi(·)) ≈ sigmoid(binit) =:πinit, at the start of training. We study the effect of different values of binit for Linear gated attention on BERT-6L and ViT. We set the bias for all Gi’s to the same value of binit. For BERT-6L, we use the same setup as in Section 5.2, with a fixed sequence length of 128. For ViT, we use the main setup, except we train it for 150 epochs instead of 300. In Figure 7 we see in both BERT and ViT cases that using bias with very highπinit generally performs similarly to the vanilla attention (comparable floating-point performance but strong outliers and poor quantized performance) while setting bias to have very low πinit dampens outliers quite well but leads to strong degradation in the floating-point and quantized performance. The reasonable ranges of πinit seems to be around [0.25, 0.9] for BERT and [0.1, 0.5] for ViT. The wide range indicates the relative robustness of our method to this hyperparameter. 8Model Method FP16/32 Max inf. norm Avg. kurtosis W8A8 BERT (ppl.↓) Vanilla 4.49 ±0.01 735±55 3076±262 1294±1046 Clipped softmax 4.39±0.00 21.5±1.5 80±6 4.52±0.01 Gated attention 4.45 ±0.03 39.2±26.0 201±181 4.65±0.04 OPT (ppl.↓) Vanilla 15.84 ±0.05 340±47 1778±444 21.18±1.89 Clipped softmax 16.29 ±0.07 63.2±8.8 19728±7480 37.20±2.40 Gated attention 15.55±0.05 8.7±0.6 18.9±0.9 16.02±0.07 ViT (acc.↑) Vanilla 80.75 ±0.10 359±81 1018±471 69.24±6.93 Clipped softmax 80.89 ±0.13 73.7±14.9 22.9±1.6 79.77±0.25 Gated attention 81.01±0.06 79.8±0.5 19.9±0.3 79.82±0.11 Table 2: A summary of results for our proposed methods applied on BERT, OPT-125m, and ViT. Model Method FP16 Max inf. norm Avg. kurtosis W8A8 OPT-350m (ppl.↓) Vanilla 13.19 253 2689 37.52 ±3.84 Gated attention 13.01 65.4 261 14.42±0.06 OPT-1.3B (ppl.↓) Vanilla 12.13 428 2756 989.6 ±175 Gated attention 12.21 67.2 444 29.95±0.42 Table 3: The performance of gated attention applied on bigger variants of OPT model. 5.4 Main results We summarize our main set of results in Table 2. As we can see, in almost all cases, both of our proposed techniques dampen the outliers’ magnitude to a great extent, reduce the kurtosis, and yield models with significantly higher quantized performance, which is close to the original FP16/32 performance. In addition to that, for each model, at least one of our methods also improves the floating-point task performance. We hypothesize this is because the network is helped with learning the “no-op” updates more easily. However, we are cautious about the improved performance as this is not consistent across all hyper-parameters and it is unclear if it generalizes to more architectures and larger models. The only case where our method failed to perform well was the clipped softmax applied to OPT. At the moment, we do not have an explanation of why this is the case and leave it for future work. We list selected hyper-parameters and show extended results in Appendix B. We also show the results of our proposed methods quantized to lower bitwidths in Appendix B.7. Results for bigger modelsWe study the question of scalability of our methods to larger models. In Table 3 we show the gated attention results for 350m and 1.3B variants of OPT. Due to compute constraints, we trained networks for 105 steps with batch size of 256 and the rest is the same as in our main pre-training setup. As we can see, our proposed gated attention is also very effective at dampening the outliers and significantly improving the quantized model performance when applied to bigger models. We further study in Appendix B.6 how gated attention can decrease outliers when fine-tuning bigger pre-trained models with outliers. 5.5 Qualitative results In Figure 8 we compare the learned attention patterns using vanilla softmax and our proposed methods (more examples in Appendix A.1). As we can see, both methods can represent a partial/soft no-op behavior, but in case of our methods this does not require strong outliers elsewhere in the network. Note that we found similar patterns in multiple attention heads, but the exact head indices where we observed such patterns depend on random initialization. In the case of clipped softmax, smaller attention weights are generally more diffused while higher weights are more saturated (which comes from the stretching and clipping). In the case of gated attention, the output of the softmax is significantly different since the update of the hidden representation is now further modulated by gating probabilities. 9(a) Vanilla softmax (Attention layer #11, head #3) (b) Clipped softmax (Attention layer #11, head #8) (c) Gated attention (Attention layer #11, head #5) Figure 8: Visualization of the self-attention patterns for BERT-base trained using vanilla and our proposed techniques, computed on data sequence #5 from MNLI-m validation set. (a), (b): attention probabilities, values, and their product. (c): gating probabilities π = sigmoid (G(x)), attention probabilities (output of softmax), values, and their combined product. 6 Discussion “No-op” behavior It is interesting to note that the identified “no-op” behavior is likely not limited to transformers and that convolutional architectures likely learn something similar. We also see that despite the network trying to learn a full “no-op”, still a small amount of noise is added to each residual, which may constitute a form of network regularization. Investigating this further might give us a clue as to why neural networks generalize despite being significantly overparametrized if many parameters are rendered unused by not updating the representation in later layers [72]. Limitations While we studied the scalability of our method for models up to 1.3B size, we haven’t explored the case of very large transformers that are trained for way longer. Given the fundamental understanding of the issue underlying our solutions, we expect the same effect on larger-scale models. We show a very small improvement in FP16/FP32 performance due to our methods, but we do not deem our results exhaustive enough to claim that this will hold in general. Lastly, our methods do have a hyperparameter each, although we show that both methods are relatively robust to its hyperparameter, having one is never optimal. Impact As our methods help transformers to be more efficient, we expect only positive outcomes of our work. Making neural networks more efficient will help with their high power consumption at inference. It further helps to move inference from the cloud to edge devices which can overcome potential privacy concerns. We cannot fathom any negative impact from our work that is not severely construed. 7 Conclusions We have thoroughly analyzed the activation outlier problem that makes transformers difficult to quantize. We showed that transformer networks try to learn not to update residuals and that by doing so, through the combination of the softmax, residual connections and LayerNorm, significant outliers appear in transformers. Based on this insight, we proposed two methods to address this at the core – clipped softmax and gated attention. These structural changes to transformers give similar, if not better, floating-point performance after training but significantly improve the post-training quantization results. We hope that with these two architectural changes to transformers, anyone can train high-performance transformers that are easy to quantize and can benefit from efficient integer inference. 10References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018. [3] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the chal- lenges of efficient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7947–7969, Online and Punta Cana, Dominican Republic, Novem- ber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.627. URL https://aclanthology.org/2021.emnlp-main.627. [5] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169–13178, 2020. [6] Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser, et al. Robust quantization: One model to rule them all. Advances in neural information processing systems , 33: 5308–5317, 2020. [7] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efficient inference. In ICCV Workshops, pages 3009–3018, 2019. [8] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology.org/ W19-4828. [9] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020. [10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702–703, 2020. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [12] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022. [13] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplica- tion for transformers at scale. In Advances in Neural Information Processing Systems, 2022. [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [16] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization. In International Conference on Learning Representations (ICLR), 2020. 11[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Hervé Jégou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020. [18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [19] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021. [20] Sylvain Gugger, Lysandre Debu, Thomas Wolf, Philipp Schmid, Zachary Mueller, and Sourab Mangrulkar. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/ huggingface/accelerate, 2022. [21] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pages 1737–1746. PMLR, 2015. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. [23] M. Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10–14, 2014. doi: 10.1109/ ISSCC.2014.6757323. [24] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research, 18(1):6869–6898, 2017. [25] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020. [26] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer- arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2704–2713, 2018. [27] Minsoo Kim, Sihwa Lee, Sukjin Hong, Du-Seong Chang, and Jungwook Choi. Understanding and improving knowledge distillation for quantization-aware training of large transformer encoders. arXiv preprint arXiv:2211.11014, 2022. [28] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. arXiv preprint arXiv:2101.01321, 2021. [29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [30] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365–4374, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10. 18653/v1/D19-1445. URL https://aclanthology.org/D19-1445. [31] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimen- sions that disrupt transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3392–3405, 2021. [32] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. [33] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871–7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/ 2020.acl-main.703. 12[34] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysan- dre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstra- tions, pages 175–184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-demo.21. [35] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [36] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [40] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312, 2017. [41] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Re- covering neural network quantization error through weight factorization. In International Conference on Machine Learning, pages 4486–4495. PMLR, 2019. [42] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [43] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325–1334, 2019. [44] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020. [45] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197–7206. PMLR, 2020. [46] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Blankevoort Tijmen. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. [47] Vinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of the 27th International Conference on Machine Learning, pages 807–814. Omnipress, 2010. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems (NeuRIPS). 2019. [49] Giovanni Puccetti, Alessio Miaschi, and Felice Dell’Orletta. How do BERT embeddings organize linguistic knowledge? In Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.deelio-1.6. URL https://aclanthology.org/ 2021.deelio-1.6. 13[50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [51] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger. Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point. In Neural Information Processing Systems (NeurIPS 2020). ACM, November 2020. [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. [53] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815–8821, 2020. [55] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileBERT: a compact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2158–2170, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.195. URL https://aclanthology.org/ 2020.acl-main.195. [56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015. [57] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32–42, 2021. [58] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV, pages 516–533. Springer, 2022. [59] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, and Tijmen Blankevoort. Fp8 versus int8 for efficient deep learning inference. 2023. [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000–6010, 2017. [61] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446. [62] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. arXiv preprint arXiv:2209.13325, 2022. [63] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [64] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019. [65] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. 14[66] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases. 2023. [67] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In CVPR, 2022. [68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xl- net: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Pro- cessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf. [69] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transform- ers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Ad- vances in Neural Information Processing Systems , volume 35, pages 27168–27183. Curran Asso- ciates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf. [70] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032, 2019. [71] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019. [72] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. 2017. [73] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [75] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543–7552. PMLR, 2019. [76] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. [77] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015. 15Supplementary materials A Additional graphs from outlier analysis In this section, we present additional graphs from our outlier investigation in Section 3 for BERT and vision transformer. (a)  (b)  (c) Figure 9: A summary of several outlier statistics recorded from ImageNet validation set on ViT. (a) Average infinity norm of the output of each attention layer. (b) A histogram of outlier counts in attention layer #10 vs. hidden dimensions. We use zero-based indexing for dimensions. (c) A heatmap of outlier counts in attention layer #10 vs. patch positions. A.1 BERT Recall from Figure 1 that all the outliers are only present in hidden dimensions #123, #180, #225, #308, #381, #526, #720 (with the majority of them in #180, #720). These hidden dimensions correspond to attention heads #2, #3, #4, #5, #6, #9, and #12. In Figures 10 and 11 we show more examples of the discovered self-attention patterns for attention heads #3 and #12 (↔ hidden dim #180 and #720, respectively). We also show self-attention patterns in attention heads and layers which are not associated with the outliers in Figures 12 and 13, respectively. Finally, in Figures 14 and 15 we show more examples of the attention patterns learned in the network trained with clipped softmax and gated attention. A.2 ViT Figure 9 further shows that there are a lot of similarities in the outlier behavior in the vision transformer, compared to BERT. The strongest magnitude outliers generally happen in the later layers, peaking at layers #10 and #11. The majority of outliers (> 99%) are only ever happening in only 10 hidden dimensions, primarily in dimensions #48 and #43, which corresponds to the attention head #1. Finally, averaged across the entire ImageNet validation set, the outliers seem to be concentrated at the boundaries of the image, which suggest a strong correlation with the background (and a negative correlation with the object, which is usually in the center of the image in the ImageNet dataset). In Figures 16 and 17, we show more examples of outlier and self-attention patterns in the attention head #1 (↔ hidden dimensions #48, #43) for a random subset of images from the ImageNet validation set (in layers #10 and #11, respecively). B Detailed results In this section, we provide extended results for each model, including the used hyperparameters and other design choices. We also present some additional ablation studies. 16Configuration G Memory overhead (per attention layer) # extra parameters # extra tokens Linear nheads × Linear(dhead → 1) nheads(dhead + 1) ∼ 1 MLP nheads × MLP(dhead → nhid → 1) nheads(nhid(dhead + 2) + 1) ∼ nhid All-heads-linear Linear(dmodel → nheads) nheads(dmodel + 1) ∼ nheads Table 4: An overview of the gating function parameterizations explored in this paper and their memory overhead. B.1 Gating architectures We investigate the choice of several gating functions, summarized in Table 4. The configuration “MLP” parameterizes each Gi with a feed-forward net with one hidden layer of size nhid and a ReLU non-linearity [47]. We also explore what happens if we allow the mixing of the representation from different attention heads in the “All-heads-linear” setting, where we use a single linear layer to produce the gating probabilities for all attention heads at once. All three options are tested below. Unless explicitly stated otherwise, we initialize the bias of the gating function to zero (i.e., binit = 0 ↔ πinit = 0.5). B.2 BERT Method FP16 ppl. ↓ Max inf norm Avg. Kurtosis W8A8 ppl. ↓ Vanilla 4.49 ±0.01 735.0±54.9 3076±262 1294±1046 CS (γ = −0.005) 4.44 ±0.02 406.6±35.2 1963±753 75.27±39.57 CS (γ = −0.01) 4.35 ±0.01 198.3±78.7 1581±839 7.06±2.37 CS (γ = −0.015) 4.37 ±0.01 38.9±7.9 165±34 4.54±0.01 CS (γ = −0.02) 4.39 ±0.02 31.7±6.3 90±20 4.56±0.02 CS (γ = −0.025) 4.39±0.00 21.5±1.5 80±6 4.52±0.01 CS (γ = −0.03) 4.41 ±0.01 20.4±0.2 79±6 4.55±0.01 CS (γ = −0.04) 4.51 ±0.05 19.8±9.0 85±7 4.65±0.06 GA, Linear (πinit = 0.25) 4.49 ±0.00 139.8±62.3 739±412 5.05±0.27 GA, Linear (πinit = 0.5) 4.48 ±0.00 177.3±33.2 652±81 5.13±0.15 GA, Linear (πinit = 0.75) 4.49 ±0.00 71.4±49.9 262±147 4.88±0.22 GA, Linear (πinit = 0.9) 4.49 ±0.00 171.5±8.8 559±141 5.15±0.03 GA, MLP (nhid = 4) 4.45±0.03 39.2±26.0 201±181 4.65±0.04 GA, MLP (nhid = 64) 4.49 ±0.01 117.0±48.3 507±167 4.77±0.01 GA, All-heads-linear 4.49 ±0.01 58.3±41.2 334±321 4.67±0.03 Table 5: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to BERT-base. We report the masked language modeling perplexity (ppl. for short) on the English Wikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. Detailed results for BERT-base are summarized in Table 5. As we can see, across most of the settings, both of our methods significantly dampen the outliers’ magnitude, reduce the kurtosis, drastically improve the quantized performance, while maintaining and sometimes improving the FP16 perplexity. B.3 OPT Detailed results for OPT-125m are summarized in Table 6. In our early experiments on a smaller OPT model, we found that applying the weight decay on LayerNorm weights γ (which isn’t the case, by default) has a strong effect on reducing the outliers’ magnitude while yielding the comparable FP16 performance. Therefore, we present the results of applying our gated attention approach in both cases, with and without applying weight decay on LNγ. As we can see in Table 6, in both cases gated attention (further) dampens the outliers’ magnitude to a 17Method LN γ wd FP16 ppl. ↓ Max inf norm Avg. Kurtosis W8A8 ppl. ↓ Vanilla ✕ 15.84±0.05 339.6±47.2 1777±444. 21.18±1.89 GA, Linear (πinit = 0.1) ✕ 15.61±0.05 35.6±4.5 42.4±22.9 16.41±0.18 GA, Linear (πinit = 0.25) ✕ 15.50±0.04 35.8±0.5 59.0±48.3 16.25±0.08 GA, Linear (πinit = 0.5) ✕ 15.54±0.01 46.5±5.0 40.6±8.9 16.30±0.01 GA, All-heads-linear ✕ 15.43±0.01 32.8±1.7 24.2±3 16.30±0.12 Vanilla ✓ 15.96±0.03 87.7±31.9 2080±1460 39.46±16.59 CS (γ = −1/512) ✓ 15.99±0.02 106.4±7.0 5764±2150 185.23±220.00 CS (γ = −2/512) ✓ 15.90±0.02 102.0±27.0 11290±4372 60.90±52.70 CS (γ = −4/512) ✓ 15.86±0.01 83.1±20.6 17174±7791 84.64±10.55 CS (γ = −8/512) ✓ 16.13±0.09 61.5±9.9 19204±4284 42.62±3.64 CS (γ = −12/512) ✓ 16.29±0.07 63.2±8.8 19727±7479 37.22±2.39 GA, Linear (πinit = 0.1) ✓ 15.69±0.05 7.3±0.4 25.4±10 16.23±0.08 GA, Linear (πinit = 0.25) ✓ 15.55±0.05 8.7±0.6 18.9±1 16.02±0.07 GA, Linear (πinit = 0.5) ✓ 15.63±0.00 10.8±0.7 42.0±19 16.20±0.01 GA, All-heads-linear ✓ 15.53±0.01 7.9±0.3 13.8±1 16.09±0.08 Table 6: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to OPT-125m. We report the causal language modeling perplexity (ppl. for short) on the English Wikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. great extent, reduces the kurtosis, and yields models with significantly higher quantized performance, which is close to the original FP16 performance. B.4 ViT Method Patch. Embd. LN FP32 acc. Max inf norm Avg. Kurtosis W8A8 acc. Vanilla ✕ 80.75±0.10 358.5±81.2 1018.3±471.5 69.24±6.93 CS (γ = −0.003) ✕ 80.24±0.05 69.3±20.7 25.6±8.6 78.71±0.33 CS (γ = −0.004) ✕ 80.38±0.01 74.9±10.6 30.6±4.9 78.66±0.49 GA, Linear (πinit = 0.25) ✕ 80.62±0.01 86.0±8.0 23.4±2.7 79.16±0.05 GA, Linear (πinit = 0.5) ✕ 80.32±0.02 88.4±17.9 27.9±14.0 78.90±0.25 GA, MLP (nhid = 4) ✕ 80.62±0.05 118.2±40.5 47.8±29.8 78.79±0.29 Vanilla ✓ 80.98±0.08 81.1±2.5 24.5±1.8 79.62±0.06 CS (γ = −0.0001) ✓ 80.89±0.13 73.7±14.9 22.9±1.6 79.77±0.25 CS (γ = −0.0003) ✓ 80.92±0.07 78.9±5.5 23.8±0.5 79.63±0.05 CS (γ = −0.0005) ✓ 80.95±0.08 72.9±11.8 24.4±0.7 79.73±0.08 CS (γ = −0.001) ✓ 80.95±0.16 80.8±2.1 24.1±0.7 79.69±0.03 CS (γ = −0.002) ✓ 80.80±0.07 78.0±0.5 25.8±0.7 79.32±0.07 CS (γ = −0.003) ✓ 80.79±0.02 75.6±7.9 28.1±4.0 79.00±0.10 GA, Linear (πinit = 0.5) ✓ 81.01±0.06 79.8±0.5 19.9±0.3 79.82±0.11 GA, Linear (πinit = 0.75) ✓ 81.01±0.05 77.8±0.3 21.8±1.9 79.80±0.08 GA, Linear (πinit = 0.9) ✓ 80.92±0.11 70.6±8.0 23.2±3.7 79.64±0.09 Table 7: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to ViT-S/16. We report the top-1 accuracy on ImageNet-1K validation set for floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of the attention layer. Detailed results for ViT-S/16 are summarized in Table 7. After our preliminary experiments on ViT, we noticed that distinct outliers already originate after the patch embeddings. Therefore, we experimented with adding the LayerNorm after the patch 18embeddings (which was absent in the model definition, by default). As we can see in Table 6, together with this change, both of our proposed methods greatly dampens the outliers’ magnitude, reduces the kurtosis, and yields models with significantly higher quantized performance, which is within 1% of the original FP32 accuracy. B.5 The impact of clipped softmax hyperparameters (γ and ζ) on ViT γ ζ FP32 acc. Max inf norm W8A8 acc. 0 1 78.80±0.42 426±69 71.27±0.88 (= Vanilla) 0 1 .001 78.78±0.29 411±88 71.24±0.59 0 1 .002 78.90±0.17 420±47 70.74±0.34 0 1 .004 78.80±0.45 377±67 72.31±0.06 0 1 .01 78.81±0.30 419±77 71.35±0.26 −0.00001 1 78.81±0.21 432±76 69.02±0.19 −0.0001 1 78.81±0.36 380±64 64.04±10.8 −0.001 1 78.42±0.63 282±105 68.43±6.50 −0.003 1 78.26±0.06 99±36 76.49±0.48 −0.01 1 78.10±0.14 391±21 75.83±1.12 −0.03 1 70.26±1.46 197±2 65.80±1.41 −0.001 1 .001 78.45±0.53 283±82 65.03±8.54 −0.003 1 .003 78.25±0.14 119±17 76.37±0.45 Table 8: The impact of clipped softmax hyperparameters on ViT-S/16. We investigate the effect of different values of the clipped softmax stretch parameters applied to the vision transformer and present the results in Table 8. To speed up training, for this experiment we trained ViT for 150 epochs instead of the usual 300 epochs. For this experiment, we did not apply LayerNorm after the patch embeddings. We found similar observations compared to BERT. Specifically, most of the improvement happens when we use γ <0 (clipping at zero) whereas using ζ >1 (clipping at one) yields similar results to the vanilla softmax and combining both γ <0 and ζ >1 yields similar results compared to just clipping at zero. B.6 Fine-tuning experiment Method FP16 ppl. ↓ Max inf norm Avg. Kurtosis Vanilla fine-tuning 29.46 79.3 2086 Fine-tuning w/ Gated attention 29.18 50.9 665 Table 9: OPT-1.3B fine-tuning results with vanilla softmax and gated attention. We report the causal language modeling perplexity (ppl. for short) on the English Wikipedia validation set. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. One of the drawbacks of our proposed framework is that it requires training from scratch, which could be expensive when applied to very large models. To address this, we explored whetherfine-tuning using gated attention can still lead to improved performance and decreased outliers for larger models. We used OPT-1.3B pre-trained checkpoint from HuggingFace and fine-tuned it on Bookcorpus + Wikipedia for 4000 steps with batch size 256, maximum sequence length 512, maximum learning rate 10−5, and linear LR schedule with 400 warmup steps. We use the same LR for both model parameters and gating module parameters. The rest of hyper-parameters are the same as for our pre-training setup. We adapted our gating approach as follows. We initialized bias as binit = 0, which corresponds to the expected initial gating probability output of πinit = 0.5. We multiply the gating probability by 2 so that the expected gate output is 1 and we approximate the attention output of 19the vanilla softmax at the start of fine-tuning. We add a small activation regularization term at the output of each FFN to further encourage the reduction in the magnitude of activations, as unlike when training from scratch outliers are already present in the pre-trained model and need to be suppressed. As we can see from Table 9, fine-tuning with our proposed gated attention results in a better perplexity and also reduced maximum infinity norm and the average kurtosis compared to fine-tuning with vanilla softmax. B.7 Low-bit quantization results Bitwidths Weight range estimation Vanilla Clipped softmax Gated attention FP16 − 4.49±0.01 4.39±0.00 4.45±0.03 W8A8 min-max 1294 ±1046 4.52±0.01 4.65±0.04 W6A8 min-max 598 ±254 4.64±0.01 4.79±0.03 W6A8 MSE 6.49 ±0.38 4.56±0.01 4.71±0.03 W4A8 MSE 6.52 ±0.02 4.90±0.02 5.02±0.03 W6A6 MSE 42.8 ±11.7 6.64±0.14 5.90±0.11 Table 10: A summary of results for our proposed methods applied to BERT-base and quantized to different bitwidthds for weights and activations (using the same PTQ setup as in all previous experi- ments). We report the masked language modeling perplexity on the English Wikipedia validation set. Note that our proposed methods are not limited to 8-bit quantization only and in general can be combined with other more advanced quantization and weight compression methods, including [18, 35, 36, 45, 63, 67]. In Table 10, we show the results of our proposed methods applied to BERT-base and quantized to different bitwidths using our simple post-training quantization setup. Unless stated otherwise, for low-bit (<8-bit) weights and activations we use MSE range estimator as recommended by [2, 7] since it gives better results. As we can see, in all cases both of our methods significantly improve the perplexity compared to the vanilla softmax pre-training. We also notice that generally the performance progressively degrades as we decrease the bitwidths, which is to be expected. Achieving good results with low-bit activation quantization in general is a challenging problem. Further, we notice that the perplexity of the vanilla model significantly improves whenever we consider a low-bit weight quantization with MSE ranges compared to the INT8 case. This can be explained by the fact that using MSE range estimation for weights leads to an implicit clipping of activations (in the same and all subsequent layers in the network), which happen to be of the right amount so that it doesn’t hurt the perplexity. We found that by going from W8A8 to W6A8 the average kurtosis is reduced from 3406±547 to 631±94 and the maximum infinity norm is reduced from 577±80 to 158±40. However, in all cases the resulting model still has significantly larger outliers and a worse performance than both of our proposed methods. Finally, as said before, if achieving good low-bit quantization performance is the goal, it is recommended to combine our methods with more advanced quantization techniques. C Experimental details C.1 BERT Fine-tuning on MNLI dataset We use pre-trained checkpoint BERT-base-uncased (109M param- eters) from HuggingFace repository. We follow standard fine-tuning practices from [14] and [ 65] Each data sequence is tokenized and truncated to the maximum sequence length of 128. Shorter sequences are padded to the same length of 128 using a special [PAD] token. We fine-tune for 3 epochs using Adam [29] with a batch size of 16 and no weight decay. The learning rate is initially set to its maximum value of of 2 · 10−5 and is linearly decayed to zero by the end of fine-tuning. Pre-training from scratch We follow closely the pre-training procedure from [14]. We concate- nate, tokenize, and split the training set into sequences of length 128 (to speed up training and experimentation, we do not fine-tune on longer sequences of 512). We use the masked language modeling objective with the probability of masking p = 0.15. We train with a batch size of 256 20sequences for 106 steps, using AdamW optimizer [ 39] with the maximum learning rate of 10−4, learning rate warm up over the first 104 steps, following by a linear decay to zero by the end of training. We use L2 weight decay of 0.01, L2 gradient norm clipping of 1.0, and dropout probability of 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20]. C.2 OPT pre-training To speed up experimentation, we train OPT-125m sized model on the concatenation of Wikipedia and BookCorpus (same as BERT pre-training). We train with a batch size of 48 and 4 gradient accumulation steps (which results in the effective batch size of 192), so that we can perform pre- training on a single A100 80GB GPU. We concatenate, tokenize, and split the training set into sequences of length 512 and train for 125000 steps (500000 forward passes). We use the rest of the hyper-parameters and follow pre-training practices from [74] and [65]. We initialize weights using a normal distribution with zero mean and a standard deviation of 0.006. All bias terms are initialized to zero. We use AdamW optimizer with (β1, β2) = (0.9, 0.95). We use the linear learning rate schedule, warming up from 0 to the maximum value† of 4 · 10−4 over the first 2000 steps, following by a linear decay to zero by the end of training. We use L2 weight decay of 0.1, L2 gradient norm clipping of 1.0, and dropout probability of 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20]. Note that in our experiments for all model sizes we use the consistent LayerNorm placement before the attention block, unlike OPT-350m checkpoint from HuggingFace that places LayerNorm after the attention block. C.3 ViT pre-training We use the model definition for ViT-S/16 and the training pipeline from PyTorch Image models library [64]. All training is done on resolution 224 ×224 and 16 ×16 patches. For data augmentation, we use RandAugment [10], Mixup [73], CutMix [70], random image cropping [56], horizontal flip, label smoothing ε = 0.1, color jitter 0.4, and random (between bilinear and bicubic) interpolation during training. We train with a batch size of 512 for 300 epochs, using AdamW optimizer and the L2 weight decay of 0.03. We use the cosine learning rate schedule, warming up from 10−6 to the maximum value of 10−3 over the first 20 epochs, followed by a LR decay by a factor of 10 every 30 epochs, until it reaches the minimum value of 10−5. C.4 Quantization settings Weights In all cases, we use symmetric uniform quantization of weights. We use min-max weight quantization for all models except the OPT model, for which we found the MSE estimator to perform better in all cases. Activations We adopt static range estimation approach, which determines quantization parameters for the network by passing a few batches of calibration data through the model before inference. Specifically, we use a running min-max estimator [32], which uses an exponential moving average of the min and max over multiple batches. In all cases, we use running min-max with 0.9 momentum over 16 batches randomly sampled from respective training sets. For OPT model, we also experiment with using 99.99% and 99.999% percentiles instead of actual min and max. We select the best configuration for each experiment (including baseline), based on the model performance. In almost all cases, we found that setting activation quantization ranges using 99.999% percentiles gives the lowest W8A8 perplexity. D Compute cost We compare the runtime of our proposed methods in Table 11. As we can see, the clipped softmax is only marginally more expensive compared to using the vanilla softmax attention. The gated attention †In our experiments, we found this value to perform better compared to the value of 6 · 10−4 listed in the paper. 21Model Vanilla Clipped softmax Gated attention (Linear / MLP) BERT 92.8 ±1.2 93.6±0.8 97.7 / 119.1 OPT 53.6 ±0.4 54.4±0.4 55.7 / 64.7 ViT 101.8 ±0.3 104.0±0.7 110.8 / 122.9 Table 11: An overview of the runtime of the proposed methods, compared to the vanilla pre-training, measured in hours on Nvidia-A100 GPUs. using the linear G adds the compute overhead between 3% and 8%, depending on the model. We found that adding weight decay on LayerNorm γ for OPT and adding the LayerNorm after the patch embeddings for ViT had a negligible effect on the runtime. We estimated that the compute cost of producing the main results in the paper is about 320 GPU days (on A100) and the total cost of the project (including preliminary experiments and ablation studies) to be about 1400 GPU days. 22(a) Attention layer #10, data sequence #16 (b) Attention layer #11, data sequence #16 (c) Attention layer #10, data sequence #21 (d) Attention layer #11, data sequence #21 (e) Attention layer #10, data sequence #61 (f) Attention layer #11, data sequence #61 (g) Attention layer #10, data sequence #88 (h) Attention layer #11, data sequence #88 Figure 10: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 (↔ channel dim #180) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set. 23(a) Attention layer #10, data sequence #16 (b) Attention layer #11, data sequence #16 (c) Attention layer #10, data sequence #21 (d) Attention layer #11, data sequence #21 (e) Attention layer #10, data sequence #61 (f) Attention layer #11, data sequence #61 (g) Attention layer #10, data sequence #88 (h) Attention layer #11, data sequence #88 Figure 11: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #12 ( ↔ channel dim #720) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set. 24(a) Attention layer #10, attention head #1 (b) Attention layer #11, attention head #1 (c) Attention layer #10, attention head #7 (d) Attention layer #11, attention head #7 (e) Attention layer #10, attention head #8 (f) Attention layer #11, attention head #8 (g) Attention layer #10, attention head #10 (h) Attention layer #11, attention head #10 Figure 12: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention heads that are not associated with the strong outliers for BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set. 25(a) Attention layer #1 (b) Attention layer #2 (c) Attention layer #3 (d) Attention layer #4 (e) Attention layer #5 (f) Attention layer #6 (g) Attention layer #7 (h) Attention layer #8 Figure 13: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 (↔ channel dim #180) and the first eight layers of BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set. 26(a) Attention layer #10, Attention head #3, data sequence #1 (b) Attention layer #11, Attention head #3, data sequence #1 (c) Attention layer #10, Attention head #3, data sequence #5 (d) Attention layer #11, Attention head #3, data sequence #5 (e) Attention layer #10, Attention head #3, data sequence #7 (f) Attention layer #11, Attention head #3, data sequence #7 (g) Attention layer #10, Attention head #12, data sequence #1 (h) Attention layer #11, Attention head #12, data sequence #1 Figure 14: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) for BERT-base trained with Clipped softmax, computed on several random data sequences from MNLI-m validation set. 27(a) Attention layer #10, Attention head #3, data sequence #1 (b) Attention layer #11, Attention head #3, data sequence #1 (c) Attention layer #10, Attention head #3, data sequence #5 (d) Attention layer #11, Attention head #3, data sequence #5 (e) Attention layer #10, Attention head #3, data sequence #7 (f) Attention layer #11, Attention head #3, data sequence #7 (g) Attention layer #10, Attention head #12, data sequence #1 (h) Attention layer #11, Attention head #12, data sequence #1 Figure 15: Visualization of the self-attention patterns (from left to right: gating probabilities π = sigmoid (G(x)), output of softmax, values, and their combined product) for BERT-base trained with gated attention, computed on several random data sequences from MNLI-m validation set. 28(a)  (b)  (c)  (d)  (e) Figure 16: A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #10. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #11. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values (V ) for outlier and non-outlier patches. 29(a)  (b)  (c)  (d)  (e) Figure 17: A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values (V ) for outlier and non-outlier patches. 30",
      "meta_data": {
        "arxiv_id": "2306.12929v2",
        "authors": [
          "Yelysei Bondarenko",
          "Markus Nagel",
          "Tijmen Blankevoort"
        ],
        "published_date": "2023-06-22T14:39:04Z",
        "pdf_url": "https://arxiv.org/pdf/2306.12929v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the problem of strong activation outliers in modern transformer models, which makes them difficult to quantize to lower bitwidths (e.g., INT8) without significant performance degradation. The authors show that these outliers arise because attention heads attempt to learn 'no-op' or partial updates to the residual connection, pushing softmax inputs to larger values. The main contributions are two novel, independent modifications to the attention mechanism: clipped softmax and gated attention. These methods are shown to significantly reduce outlier magnitudes and kurtosis, enabling full INT8 quantization of activations without additional effort, while maintaining or even improving floating-point performance across language (BERT, OPT) and vision (ViT) transformers.",
        "methodology": "The core methodology is based on the hypothesis that outliers emerge from the attention mechanism's attempt to perform a 'no-op' update on the residual, which necessitates large dynamic ranges for softmax inputs. To counter this, two main modifications are proposed: 1. **Clipped Softmax**: Replaces the standard softmax with `clipped_softmax(x; ζ, γ) := clip ((ζ − γ) · softmax(x) + γ, 0, 1)`. This allows the attention matrix to produce exact zeros (if γ < 0) or ones (if ζ > 1) with a finite input range, preventing outlier growth by halting gradient propagation when values are clipped. 2. **Gated Attention**: Introduces an explicit conditional gating mechanism `sigmoid(G(x)) ⊙ softmax(Q(x)K(x)T/√dhead)V(x)`. A lightweight neural network `G(x)` (e.g., a single linear layer or MLP) learns to modulate the attention output element-wise, allowing the model to explicitly nullify or keep updates without relying on extreme softmax values. This function operates on a per-head basis, with optional bias initialization strategies.",
        "experimental_setup": "The methods were evaluated on three model architectures: BERT-base-uncased (109M parameters) pre-trained with Masked Language Modeling (MLM) on BookCorpus and English Wikipedia, OPT-125m/350m/1.3B pre-trained with Causal Language Modeling (CLM) on the same dataset, and ViT-S/16 (22M parameters) trained on ImageNet-1K. Evaluation metrics included MLM perplexity (BERT), CLM perplexity (OPT), Top-1 accuracy (ViT), maximum infinity norm, and average kurtosis of attention layer outputs (to quantify outliers). For quantization, 8-bit Post-Training Quantization (PTQ) was applied using uniform affine quantization (symmetric weights, asymmetric activations) with static activation range estimation (running min-max with 0.9 momentum over 16 batches, often using 99.999% percentiles for ranges). Experiments included ablation studies on clipped softmax hyperparameters (γ, ζ) and gated attention bias initialization (πinit) and gating architectures (Linear, MLP, All-heads-linear). Fine-tuning of pre-trained OPT-1.3B with gated attention was also explored.",
        "limitations": "The scalability of the proposed methods was primarily tested on models up to 1.3 billion parameters, and their effectiveness on much larger transformers trained for significantly longer durations was not explicitly explored, although the authors expect similar effects due to the fundamental understanding of the issue. While some experiments showed minor improvements in FP16/FP32 performance, the authors are cautious about claiming this as a general benefit across all architectures and larger models. Both proposed methods introduce a new hyperparameter each, which, despite empirical robustness, is not optimal.",
        "future_research_directions": "Future research directions include further investigation into the identified 'no-op' behavior, which the authors suggest might not be limited to transformers and could be present in convolutional architectures, potentially offering insights into network regularization and generalization in overparameterized models. Another specific area for future work is to understand why the clipped softmax method failed to perform well when applied to OPT models."
      }
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Graph Attention Networks (GATs) struggle to switch off task-irrelevant neighborhood aggregation, leading to over-smoothing and hindering performance, especially on heterophilic datasets. This paper proposes GATE, a GAT extension that addresses this limitation by allowing flexible control over neighborhood aggregation. GATE offers several advantages: it alleviates over-smoothing, benefits from higher depth by utilizing layers for non-linear feature transformations even with minimal aggregation, and often outperforms GATs on real-world heterophilic datasets by down-weighting connections to unrelated neighbors. The research provides a theoretical explanation for GAT's limitation based on gradient flow dynamics and constructs a synthetic test bed to analyze a model's ability to adapt neighborhood aggregation. GATE achieves state-of-the-art test accuracy on the OGB-arxiv dataset.",
        "methodology": "The paper identifies a structural limitation of GATs rooted in a conservation law of GAT gradient flow dynamics, which imposes norm constraints on attention parameters, making it difficult to achieve the large parameter norms required to switch off neighborhood aggregation. To overcome this, GATE modifies the GAT attention mechanism. While a standard GAT layer calculates attention coefficients αuv based on el uv = a⊤ · ϕ(Wshl−1 u + Wthl−1 v ), GATE introduces separate attention parameters for the node (at) and neighborhood (as) contributions, defining el uv = (1u̸=vas + 1u=vat)⊤ · ϕ(Ulhl−1 u + Vlhl−1 v ). This modification allows GATE to learn separate attention parameters for self and neighbor contributions, enabling flexible switching of neighborhood aggregation in a well-trainable parameter regime, as indicated by an updated conservation law for GATE gradients. The approach uses ReLU as the non-linear activation function in GATE, allowing interpretation of the sign of attention parameters.",
        "experimental_setup": "The experimental setup includes both synthetic and real-world graphs for node classification tasks. The synthetic test bed comprises two types of problems: self-sufficient learning (label-relevant information only in node's own features, using Erdős–Rényi graphs and Cora structure with original/randomized labels) and neighbor-dependent learning (label-relevant information in k-hop neighbors' features, using ER graphs and K-means clustering for labels). Real-world evaluations are conducted on five heterophilic benchmark datasets (roman-empire, amazon-ratings, questions, minesweeper, tolokers) and three OGB datasets (OGB-arxiv, OGB-products, OGB-mag), as well as five smaller-scale datasets (Cora, Citeseer, Actor, Texas, Wisconsin). Performance metrics include test accuracy and AUC-ROC. Validation methods involve studying the distribution of αvv values, and over-smoothing is quantitatively measured using a modified Dirichlet energy (EGAT). Models are trained using the Adam optimizer with specified learning rates for different datasets, without additional weight decay or dropout regularization to isolate architectural effects. Network width is typically fixed to 64, and depth varies. Parameter matrices are initialized with an orthogonal looks-linear structure, and GATE's attention parameters are initialized to zero.",
        "limitations": "The paper notes that determining the optimal degree of smoothing and the threshold for 'over'-smoothing is task-dependent and merits further in-depth analysis and curation of task-dependent smoothness measures, which is beyond the scope of this work. For small-scale real-world datasets, models are prone to over-fitting in deeper networks without regularization or skip connections. The neighbor-dependent synthetic task, while illustrative, does not achieve 100% test accuracy due to data points close to a non-crisply defined decision boundary. While GATE introduces more parameters than GAT for its 'sep' variant (GATE-sep adds d^2 parameters compared to GAT-sep, though the base GATE adds only d additional parameters compared to GAT), its base version is parameter-efficient.",
        "future_research_directions": "The paper suggests that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks, implying further research in understanding graph structure utility. The capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE, suggesting a possible combination of both architectures. It would also be interesting to derive conservation laws inherent to other GNN architectures such as FAGCN and GraphSAGE and study how they govern parameter behavior."
      }
    },
    {
      "title": "GraphQNTK: Quantum Neural Tangent Kernel for Graph Data"
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research investigates the optimization and learning dynamics of Graph Attention Networks (GATs), particularly addressing the problem of performance degradation in deeper GATs. The main contributions include: 1) Deriving a conservation law of GAT gradient flow dynamics, which explains why a significant portion of parameters in GATs with standard initialization struggle to change during training, an effect amplified in deeper networks. 2) Proposing a novel balanced initialization scheme that alleviates this problem by enabling more effective propagation of gradients, allowing for the trainability of deeper GATs. 3) Demonstrating that this balanced initialization scheme achieves considerable speedup in training and convergence time compared to standard initialization. The work also establishes an empirical causal link between parameter balancedness and trainability, and the derived theorem serves as a foundation for studying learning dynamics of positive homogeneous models with attention mechanisms.",
        "methodology": "The study employs a theoretical approach to derive a conservation law for GAT gradient flow dynamics. This involves leveraging concepts of neuron-wise balancedness and rescale invariance from traditional deep neural networks, specifically applying Lemma 6.2 and Definition 6.1 to GATs with positive homogeneous activation functions (e.g., ReLU, LeakyReLU). The theory is extended to GAT variations with shared feature weights and multiple attention heads. Based on these theoretical insights, a balanced initialization scheme (Procedure 2.6) is devised. This procedure involves setting attention parameters to zero, and then scaling feature weights for incoming and outgoing connections across layers to satisfy the derived norm preservation law (c=0 in Eq. 5). Additionally, a 'Balanced Orthogonal Initialization' is introduced, where feature weights are initially set using a looks-linear (LL) mirrored block structure (known for promoting dynamical isometry in DNNs), before applying the balancing procedure.",
        "experimental_setup": "Experiments were conducted on GAT (GATv2) models with ReLU activation, weight sharing, and no biases. Performance was evaluated using four initialization schemes: standard Xavier (Xav), Xavier with zero attention (XavZ), Balanced Xavier (BalX), and Balanced Orthogonal (BalO). The optimizers used were SGD and Adam, with learning rates fine-tuned for different depths and datasets. Networks were trained for 5000 epochs (or until a training loss of ≤ 10^-4) on nine benchmark datasets for semi-supervised node classification: Cora, Citeseer, Pubmed, Cornell, Texas, Wisconsin, Squirrel, Chameleon, and Actor. Standard train/validation/test splits were used, with isolated nodes removed from Citeseer. Validation accuracy was used to select the best model state, and results (mean ±95% confidence interval) were reported over five runs. Hardware included Nvidia T4 Tensor Core GPU (15 GB RAM) or Nvidia GeForce RTX 3060 Laptop GPU (6 GB RAM). Additional experiments considered architectural variations (ELU activation, multiple attention heads, no weight sharing, dropout, weight decay), comparisons with Lipschitz normalization, and the applicability to GCNs and ωGAT.",
        "limitations": "The derived conservation law is specifically applicable to the self-attention mechanisms found in the original GAT, GATv2, and architectural variations like ωGAT. It does not directly apply to different types of self-attention, such as the dot-product self-attention used in models like SuperGAT, which would require modifications to the law. While the balanced initialization scheme significantly improves trainability, its effectiveness can be somewhat hindered by non-positively homogeneous activation functions like ELU, especially for the orthogonal initialization. Furthermore, techniques such as dropout, while potentially aiding optimization, were not consistently helpful for deeper networks in the context of this study. For some datasets and very deep configurations, even balanced models showed reduced performance, which might indicate a lack of convergence within the allotted 5000 epochs rather than a fundamental trainability issue.",
        "future_research_directions": "The main theorem serves as a stepping stone for future work on studying the learning dynamics of positive homogeneous models that incorporate attention mechanisms, such as transformers and vision transformers. Another promising direction is to investigate how dynamical isometry can be achieved or approximated in general Graph Neural Networks (GNNs). Future research could also focus on deriving modifications to the conservation law for other attention-based models, particularly those utilizing dot-product self-attention mechanisms (e.g., SuperGAT) and other Transformer-based architectures commonly found in Large Language Models and graph learning applications. Additionally, the observed phenomenon of increased width aiding generalization in deeper models, even with unbalanced initialization, suggests an independent area of interest in understanding how overparameterization may be helpful in GNNs."
      }
    },
    {
      "title": "Generalizing CNNs to graphs with learnable neighborhood quantization"
    }
  ]
}