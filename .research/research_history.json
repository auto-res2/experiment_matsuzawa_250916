{
  "research_topic": "KVキャッシュ量子化 × 精度保持を改善したい（2bit域）",
  "queries": [
    "transformer KV cache quantization",
    "2-bit transformer quantization",
    "post-training 2-bit quantization",
    "accuracy-preserving quantization",
    "mixed-precision KV cache quantization"
  ],
  "research_study_list": [
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization"
    },
    {
      "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization"
    },
    {
      "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers"
    },
    {
      "title": "BiT: Robustly Binarized Multi-distilled Transformer"
    },
    {
      "title": "BiT: Robustly Binarized Multi-distilled Transformer"
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks"
    },
    {
      "title": "Binarized Neural Machine Translation"
    },
    {
      "title": "Towards Accurate Post-training Network Quantization via Bit-Split and Stitching"
    },
    {
      "title": "Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization"
    },
    {
      "title": "Accurate Post Training Quantization With Small Calibration Sets"
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks"
    },
    {
      "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"
    },
    {
      "title": "PowerQuant: Automorphism Search for Non-Uniform Quantization"
    },
    {
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators"
    },
    {
      "title": "Post-Training Sparsity-Aware Quantization"
    },
    {
      "title": "Extreme Compression of Large Language Models via Additive Quantization"
    },
    {
      "title": "Redistribution of Weights and Activations for AdderNet Quantization"
    },
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
    },
    {
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models"
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization"
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"
    }
  ]
}