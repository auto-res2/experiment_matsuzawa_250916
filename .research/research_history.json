{
  "research_topic": "Continuous Learningのメモリ効率に関して改善したい",
  "queries": [
    "memory efficient continual learning",
    "continual learning model compression",
    "rehearsal buffer compression",
    "sparse experience replay",
    "continual learning quantization"
  ],
  "research_study_list": [
    {
      "title": "Continual Learning in Low-rank Orthogonal Subspaces",
      "abstract": "In continual learning (CL), a learner is faced with a sequence of tasks,\narriving one after the other, and the goal is to remember all the tasks once\nthe continual learning experience is finished. The prior art in CL uses\nepisodic memory, parameter regularization or extensible network structures to\nreduce interference among tasks, but in the end, all the approaches learn\ndifferent tasks in a joint vector space. We believe this invariably leads to\ninterference among different tasks. We propose to learn tasks in different\n(low-rank) vector subspaces that are kept orthogonal to each other in order to\nminimize interference. Further, to keep the gradients of different tasks coming\nfrom these subspaces orthogonal to each other, we learn isometric mappings by\nposing network training as an optimization problem over the Stiefel manifold.\nTo the best of our understanding, we report, for the first time, strong results\nover experience-replay baseline with and without memory on standard\nclassification benchmarks in continual learning. The code is made publicly\navailable.",
      "full_text": "Continual Learning in Low-rank Orthogonal Subspaces Arslan Chaudhry1, Naeemullah Khan1, Puneet K. Dokania1,2, Philip H. S. Torr1 University of Oxford1 & Five AI Ltd., UK2 arslan.chaudhry@eng.ox.ac.uk Abstract In continual learning (CL), a learner is faced with a sequence of tasks, arriving one after the other, and the goal is to remember all the tasks once the continual learning experience is ﬁnished. The prior art in CL uses episodic memory, parameter regularization or extensible network structures to reduce interference among tasks, but in the end, all the approaches learn different tasks in a joint vector space. We believe this invariably leads to interference among different tasks. We propose to learn tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Further, to keep the gradients of different tasks coming from these subspaces orthogonal to each other, we learn isometric mappings by posing network training as an optimization problem over the Stiefel manifold. To the best of our understanding, we report, for the ﬁrst time, strong results over experience-replay baseline with and without memory on standard classiﬁcation benchmarks in continual learning.1 1 Introduction In continual learning, a learner experiences a sequence of tasks with the objective to remember all or most of the observed tasks to speed up transfer of knowledge to future tasks. Learning from a diverse sequence of tasks is useful as it allows for the deployment of machine learning models that can quickly adapt to changes in the environment by leveraging past experiences. Contrary to the standard supervised learning setting, where only a single task is available, and where the learner can make several passes over the dataset of the task, the sequential arrival of multiple tasks poses unique challenges for continual learning. The chief one among which is catastrophic forgetting [McCloskey and Cohen, 1989], whereby the global update of model parameters on the present task interfere with the learned representations of past tasks. This results in the model forgetting the previously acquired knowledge. In neural networks, to reduce the deterioration of accumulated knowledge, existing approaches modify the network training broadly in three different ways. First, regularization-based approaches [Kirk- patrick et al., 2016, Zenke et al., 2017, Aljundi et al., 2018, Chaudhry et al., 2018, Nguyen et al., 2018] reduce the drift in network parameters that were important for solving previous tasks. Second, modular approaches [Rusu et al., 2016, Lee et al., 2017] add network components as new tasks arrive. These approaches rely on the knowledge of correct module selection at test time. Third, and perhaps the strongest, memory-based approaches [Lopez-Paz and Ranzato, 2017, Hayes et al., 2018, Isele and Cosgun, 2018, Riemer et al., 2019], maintain a small replay buffer, called episodic memory, and mitigate catastrophic forgetting by replaying the data in the buffer along with the new task data. One common feature among all the three categories is that, in the end, all the tasks are learned in the same 1Code: https://github.com/arslan-chaudhry/orthog_subspace 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.11635v2  [cs.LG]  8 Dec 2020Stiefel Euclidean proj Figure 1: ORTHOG -SUBSPACE . Each blob, with the three ellipses, represents a vector space and its subspaces at a certain layer. The projection operator in the layer Lkeeps the subspaces orthogonal (no overlap). The overlap in the intermediate layers is minimized when the weight matrices are learned on the Stiefel manifold. vector space where a vector space is associated with the output of a hidden layer of the network. We believe this restriction invariably leads to forgetting of past tasks. In this work, we propose to learn different tasks in different vector subspaces. We require these subspaces to be orthogonal to each other in order to prevent the learning of a task from interfering catastrophically with previous tasks. More speciﬁcally, for a point in the vector space inRm, typically the second last layer of the network, we project each task to a low-dimensional subspace by a task-speciﬁc projection matrix P ∈Rm×m, whose rank is r, where r≪m. The projection matrices are generated ofﬂine such that for different tasks they are mutually orthogonal. This simple projection in the second last layer reduces the forgetting considerably in the shallower networks – the average accuracy increases by up to 13% and forgetting drops by up to 66% compared to the strongest experience replay baseline [Chaudhry et al., 2019b] in a three-layer network. However, in deeper networks, the backpropagation of gradients from the different projections of the second last layer do not remain orthogonal to each other in the earlier layers resulting in interference in those layers. To reduce the interference, we use the fact that a gradient on an earlier layer is a transformed version of the gradient received at the projected layer – where the transformation is linear and consists of the product of the weight matrix and the diagonal Jacobian matrix of the non-linearity of the layers in between. Reducing interference then requires this transformation to be an inner-product preserving transformation, such that, if two vectors are orthogonal at the projected layer, they remain close to orthogonal after the transformation. This is equivalent to learning orthonormal weight matrices – a well-studied problem of learning on Stiefel manifolds [Absil et al., 2009, Bonnabel, 2013]. Our approach, dubbed ORTHOG -SUBSPACE , generates two projected orthogonal vectors (gradients) – one for the current task and another for one of the previous tasks whose data is stored in a tiny replay buffer – and updates the network weights such that the weights remain on a Stiefel manifold. We visually describe our approach in Fig. 1. For the same amount of episodic memory, ORTHOG -SUBSPACE , improves upon the strong experience replay baseline by8% in average accuracy and50% in forgetting on deeper networks. 2 Background In this section, we describe the continual learning setup followed by necessary preliminaries for our approach. 2.1 Continual Learning Setup We assume a continual learner experiencing a stream of data triplets(xi,yi,ti) containing an input xi, a target yi, and a task identiﬁer ti ∈T = {1,...,T }. Each input-target pair (xi,yi) ∈X×Y ti is an identical and independently distributed example drawn from some unknown distribution Pti(X,Y ), representing the ti-th learning task. We assume that the tasks are experienced in order ti ≤tj for all i ≤j, and the learner cannot store any but a few samples from Pti in a tiny replay buffer Mi. Under this setup, our goal is to estimate a predictor f = (w◦Φ) : X×T →Y , composed of a 2feature extractor ΦΘ : X→H , which is an L-layer feed-forward neural network parameterized by Θ = {Wl}L l=1, and a classiﬁer wθ : H→Y , that minimizes the multi-task error 1 T T∑ t=1 E(x,y)∼Pt [ ℓ(f(x,t),y) ], (1) where H∈ Rm is an inner product space, Y= ∪t∈TYt, and ℓ: Y×Y→ R≥0 is a loss function. To further comply with the strict sequential setting, similar to prior work [Lopez-Paz and Ranzato, 2017, Riemer et al., 2019], we consider streams of data that areexperienced only once. We only focus on classiﬁcation tasks where either the input or output distribution changes over time. We assume that a task descriptor, identifying the correct classiﬁcation head, is given at both train and test times. Metrics Once the continual learning experience is ﬁnished, we measure two statistics to evaluate the quality of the algorithm: average accuracy, and average maximum forgetting. First, the average accuracy is deﬁned as Accuracy = 1 T T∑ j=1 aT,j, (2) where ai,j denotes the test accuracy on taskjafter the model has ﬁnished experiencing taski. Second, the average maximum forgetting is deﬁned as Forgetting = 1 T −1 T−1∑ j=1 max l∈{1,...,T−1} (al,j −aT,j), (3) that is, the decrease in performance for each of the tasks between their peak accuracy and their accuracy after the continual learning experience is ﬁnished. 2.2 Preliminaries Let the inner product in Hbe denoted by ⟨·,·⟩, and v be an element of H. A matrix O ∈Rm×r, where r ≪m, parameterizes an m×m dimensional orthogonal projection matrix P, given by P = O(O⊤O)−1O⊤, where rank(P) = r. A vector u = Pv, will be the projection of v in a subspace U⊂H with dim(U) = r. Furthermore, if the columns of Oare assumed to be orthonormal, then the projection matrix is simpliﬁed to P = OO⊤. Deﬁnition 2.1 (Orthogonal Subspace). Subspaces Uand Wof a vector space Hare orthogonal if ⟨u,w⟩= 0, ∀u∈U,w ∈W. Deﬁnition 2.2 (Isometry). A linear transformation T : V→V is called an isometry if it is distance preserving i.e. ∥T(v) −T(w)∥= ∥v−w∥, ∀v,w ∈V. A linear transformation that preserves distance must preserve angles and vice versa. We record this in the following theorem. Theorem 2.1. T is an isometry iff it preserves inner products. The proof is fairly standard and given in Appendix Appendix A. Corollary 2.1.1. If T1 and T2 are two isometries then their composition T1 ◦T2 is also an isometry. An orthogonal matrix preserves inner products and therefore acts as an isometry of Euclidean space. Enforcing orthogonality1 during network training corresponds to solving the following constrained 1Note, an orthogonal matrix is always square. However, the matrices we consider can be nonsquare. In this work, the orthogonal matrix is used in the sense of W⊤W = I. 3Stiefel Manifold Tangent plane at Wt Wt Wt+1 Gradient of loss function at Wt Projection of the gradient onto the tangent plane Retraction to the manifold  Figure 2: Gradient computed at a given point (Wt) in the manifold is ﬁrst projected to the tangent plane. There exists a closed form for this step. This projected gradient is then retracted to a point in the manifold giving the ﬁnal update Wt+1. optimization problem: min θ,Θ={Wl,bl}L l=1 ℓ(f(x,t),y), s.t. W⊤ l Wl = I, ∀l∈{1,··· ,L}, (4) where I is an identity matrix of appropriate dimensions. The solution set of the above problem is a valid Riemannian manifold when the inner product is deﬁned. It is called the Stiefel manifold, deﬁned as ¯Ml = {Wl ∈Rnl×nl−1 |W⊤ l Wl = I}, where nl is the number of neurons in layer l, and it is assumed that nl ≥nl−1. For most of the neural network architectures, this assumption holds. For a convolutional layer Wl ∈Rcout×cin×h×w, we reshape it to Wl ∈Rcout×(cin·h·w). The optimization of a differentiable cost function over a Stiefel manifold has been extensively studied in literature [Absil et al., 2009, Bonnabel, 2013]. Here, we brieﬂy summarize the two main steps of the optimization process and refer the reader to Absil et al. [2009] for further details. For a given point Wl on the Stiefel manifold, let TWl represent the tangent space at that point. Further, let gl, a matrix, be the gradient of the loss function with respect to Wl. The ﬁrst step of optimization projects gl to TWl using a closed form ProjTWl (gl) = AWl, where ‘A’ is a skew-symmetric matrix given by (see Appendix B for the derivation): A= glW⊤ l −Wlg⊤ l . (5) Once the gradient projection in the tangent space is found, the second step is to generate a descent curve of the loss function in the manifold. The Cayley transform deﬁnes one such curve using a parameter τ ≥0, specifying the length of the curve, and a skew-symmetric matrix U [Nishimori and Akaho, 2005]: Y(τ) = ( I+ τ 2 U )−1( I−τ 2 U ) Wl, (6) It can be seen that the curve stays on the Stiefel manifold i.e. Y(τ)⊤Y(τ) = I and Y(0) = Wl, and that its tangent vector at τ = 0 is Y′(0) = −UWl. By setting U = A= glW⊤ l −Wlg⊤ l , the curve will be a descent curve for the loss function. Li et al. [2020] showed that one can bypass the expensive matrix inversion in (6) by following the ﬁxed-point iteration of the Cayley transform, Y(τ) = Wl −τ 2 A(Wl + Y(τ)). (7) Li et al. [2020] further showed that under some mild continuity assumptions (7) converges to the closed form (6) faster than other approximation algorithms. The overall optimization on Stiefel manifold is shown in 2. 3 Continual Learning in Orthogonal Subspaces We now describe our continual learning approach. Consider a feature extractor in the form of a feed-forward neural network consisting of Lhidden layers, that takes an input x∈Rd and passes it through the following recursions: hl = σ(Wlhl−1 + bl), where σ(·) is a non-linearity, h0 = x, and 4hL = φ∈Rm. The network is followed by an application-speciﬁc head ( e.g.) a classiﬁer in case of a classiﬁcation task. The network can be thought of as a mapping, Φ : X→H , from one vector space (X∈ Rd) to another (H∈ Rm). When the network is trained for more than one tasks, a shared vector space Hcan be learned if the model has a simultaneous access to all the tasks. In continual learning, on the other hand, when tasks arrive in a sequence, learning a new task can interfere in the space where a previous task was learned. This can result in the catastrophic forgetting of the previous task if the new task is different from the previous one(s). In this work, we propose to learn tasks in orthogonal subspaces such that learning of a new task has minimal interference with already learned tasks. We assume that the network is sufﬁciently parameterized, which often is the case with deep networks, so that all the tasks can be learned in independent subspaces. We deﬁne a family of sets Vthat partitions H, such that, a) Vdoes not contain the empty set ( ∅ /∈ V), b) the union of sets in Vis equal to H(∪Vt∈VVt = H), and c) the intersection of any two distinct sets in Vis empty ((∀Vi,Vj ∈V) i̸= j =⇒ Vi ∩Vj = ∅)). A set Vt ∈V deﬁnes a subspace for task t. We obtain such a subspace by projecting the feature map φ= hL ∈Rm into an r-dimensional space, where r ≪m, via a projection matrix Pt ∈Rm×m of rank r, i.e. we obtain the features for task tby φt = PthL, while ensuring: P⊤ t Pt = I, P⊤ t Pk = 0, ∀k̸= t. (8) The said projection matrix can be easily constructed by ﬁrst generating a set ofmrandom orthonormal basis1 in Rm, then picking r = ⌊m/T⌋of those basis (matrix columns) to form a matrix Ot, and, ﬁnally, obtaining the projections as Pt = OtO⊤ t . For different tasks these basis form a disjoint set P= {P1,··· ,PT}. If the total number of tasks exceeds T, then one can potentially dynamically resize the m×morthogonal matrix while maintaining the required properties. For example, to make space for 2T tasks one can resize the original matrix to 2m×2mwith zero padding, and backup the previous matrix. This would entail dynamically resizing the second last layer of the network. The set Pcan be computed ofﬂine and stored in a hash table that can be readily fetched given a task identiﬁer. The projection only adds a single matrix multiplication in the forward pass of the network making it as efﬁcient as standard training. Next, lets examine the effect of the projection step on the backward pass of the backpropagation algorithm. For a task t, the gradient of the objective ℓ(·,·) on any intermediate layer hl can be decomposed using the chain rule as, gt l = ∂ℓ ∂hl = ( ∂ℓ ∂hL )∂hL ∂hl = (∂φt ∂hL ∂ℓ ∂φt )∂hL ∂hl , = ( Pt ∂ℓ ∂φt )L−1∏ k=l ∂hk+1 ∂hk = gt L L−1∏ k=l Dk+1Wk+1, (9) where Dk+1 is a diagonal matrix representing the Jacobian of the pointwise nonlinearity σk+1(·). For a ReLU nonlinearity and assuming that the non-linearity remains in the linear region during the training [Serra et al., 2017, Arora et al., 2019], we assume the Jacobian matrix to be an identity. It can be seen that for the projected layer L(the second last layer), the gradients of different tasks are orthogonal by construction i.e. gt L ⊥gk̸=t L (c.f . (8)). Hence the gradient interference will be zero at the layer L. However, according to (9), as the gradients are backpropogated to the previous layers they start to become less and less orthogonal (c.f . Fig. 4). This results in interference among different tasks in earlier layers, especially when the network is relatively deep. Let us rewrite the gradients at the intermediate layer l during the training of task t as a linear transformation of the gradient at the layer Li.e. gt l = T(gt L). According to (9), and assuming the Jacobian matrix of the non-linearity to be identity (Dk = I), this transformation is given by T(u) = u L−1∏ k=l Wk+1. (10) 1We generate a random matrix and apply the Gram–Schmidt process ofﬂine, before the continual learning experience begins. 5Algorithm 1 Training of ORTHOG -SUBSPACE on sequential data D= {D1,··· ,DT}, with Θ = {Wl}L l=1 initialized as orthonormalized matrices, P= {P1,··· ,PT}orthogonal projections, learning rate ‘α’,s= 2, q= 0.5, ϵ= 10−8. 1: procedure ORTHOG -SUBSPACE (D,P,α,s,q,ϵ ) 2: M←{}∗ T 3: for t∈{1,··· ,T}do 4: for (xt,yt) ∼Dt do 5: k∼{1,··· ,t −1} ⊿Sample a past task from the replay buffer 6: (xk,yk) ∼Mk ⊿Sample data from the episodic memory 7: gt ←∇Θ,θ(ℓ(f(xt,yt),Pt)) ⊿Compute gradient on the current task 8: gk ←∇Θ,θ(ℓ(f(xk,yk),Pk)) ⊿Compute gradient on the past task 9: g←gt + gk 10: for l= {1,··· ,L}do ⊿Layer-wise update on Stiefel manifold 11: A←glW⊤ l −Wlg⊤ l 12: U ←AWl ⊿Project the gradient onto the tangent space 13: τ ←min(α,2q/(||Wl||+ ϵ)) ⊿Select adaptive learning rate Li et al. [2020] 14: Y0 ←Wl −τU ⊿ Iterative estimation of the Cayley Transform 15: for i= {1,··· ,s}do 16: Yi ←Wl −τ 2 A(Wl + Yi−1) 17: end for 18: Wl ←Ys 19: end for 20: θ←θ−α·gL+1 ⊿Update the classiﬁer head 21: Mt ←(xt,yt) ⊿Add the sample to a ring buffer 22: end for 23: end for 24: return Θ,θ 25: end procedure As noted earlier, gt L ⊥gk̸=t L by construction, then to reduce the interference between any gt l and gk̸=t l , the transformation T(·) in (10) needs to be such that it preserves the inner-product between T(gt L) and T(gk̸=t L ). In other words, T(·) needs to be an isometry 2.2. As discussed in Sec. 2.2, this is equivalent to ensuring that weight matrices {Wl}L l=1 are orthonormal matrices. We learn orthonormal weights of a neural network by posing the network training as an optimization problem over a Stiefel manifold [Absil et al., 2009, Bonnabel, 2013]. More speciﬁcally, the network is initialized from random orthonormal matrices. A tiny replay buffer, storing the examples of past tasks (k < t), is maintained to compute the gradients {gk l }L l=1. The gradients on the current task t, {gt l}L l=1, are computed and weights in each layer lare updated as follows: a) ﬁrst the effective gradient gl = gt l + gk l is projected onto the tangent space at the current estimate of the weight matrix Wl, b) the iterative Cayley Transform (7) is used to retract the update to the Stiefel manifold. The projection onto the tangent space is carried out using the closed-form described in (5). The resulting algorithm keeps the network weights orthonormal throughout the continual learning experience while reducing the loss using the projected gradients. Fig. 3 shows this orthonormality reduces the inner product between the gradients of different tasks. We denote our approach as ORTHOG -SUBSPACE and provide the pseudo-code in Alg. 1. 4 Experiments We now report experiments on continual learning benchmarks in classiﬁcation tasks. 4.1 Continual Learning Benchmarks We evaluateaverage accuracy (2) and forgetting (3) on four supervised classiﬁcation benchmarks. Permuted MNIST is a variant of the MNIST dataset of handwritten digits [LeCun, 1998] where each task applies a ﬁxed random pixel permutation to the original dataset. Rotated MNIST is another variant of MNIST, where each task applies a ﬁxed random image rotation (between0 and 180 degrees) to the original dataset. Both of the MNIST benchmark contain 23 tasks, each with 10000 samples from 10 different classes. Split CIFAR is a variant of the CIFAR-100 dataset [Krizhevsky 6and Hinton, 2009, Zenke et al., 2017], where each task contains the data pertaining 5 random classes (without replacement) out of the total 100 classes. Split miniImageNet is a variant of the ImageNet dataset [Russakovsky et al., 2015, Vinyals et al., 2016], containing a subset of images and classes from the original dataset. Similar to Split CIFAR, in Split miniImageNet each task contains the data from 5 random classes (without replacement) out of the total 100 classes. Both CIFAR-100 and miniImageNet contain 20 tasks, each with 250 samples from each of the 5 classes. Similar to Chaudhry et al. [2019a], for each benchmark, the ﬁrst 3 tasks are used for hyper-parameter tuning (grids are available in Appendix D). The learner can perform multiple passes over the datasets of these three initial tasks. We assume that the continual learning experience begins after these initial tasks and ignore them in the ﬁnal evaluation. 4.2 Baselines We compare ORTHOG -SUBSPACE against several baselines which we describe next. Finetune is a vanilla model trained on a data stream, without any regularization or episodic memory. ICARL [Rebufﬁ et al., 2017] is a memory-based method that uses knowledge-distillation [Hinton et al., 2014] and episodic memory to reduce forgetting. EWC [Kirkpatrick et al., 2016] is a regularization-based method that uses the Fisher Information matrix to record the parameter importance. VCL [Nguyen et al., 2018] is another regularization-based method that uses variational inference to approximate the posterior distribution of the parameters which is regularized during the continual learning experience. AGEM [Chaudhry et al., 2019a] is a memory-based method similar to [Lopez-Paz and Ranzato, 2017] that uses episodic memory as an optimization constraint to reduce forgetting on previous tasks. MER [Riemer et al., 2019] is another memory-based method that uses ﬁrst-order meta-learning formulation [Nichol and Schulman, 2018] to reduce forgetting on previous tasks.ER-Ring [Chaudhry et al., 2019b] is the strongest memory-based method that jointly trains new task data with that of the previous tasks. Finally, Multitask is an oracle baseline that has access to all data to optimize (1). It is useful to estimate an upper bound on the obtainable Accuracy (2). 4.3 Code, Architecture and Training Details Except for VCL, all baselines use the same uniﬁed code base which is made publicly available. For VCL [Nguyen et al., 2018], the ofﬁcial implementation is used which only works on fully-connected networks. All baselines use the same neural network architectures: a fully-connected network with two hidden layers of 256 ReLU neurons in the MNIST experiments, and a standard ResNet18 [He et al., 2016] in CIFAR and ImageNet experiments. All baselines do a single-pass over the dataset of a task, except for episodic memory that can be replayed multiple times. The task identiﬁers are used to select the correct output head in the CIFAR and ImageNet experiments. Batch size is set to 10 across experiments and models. A tiny ring memory of 1 example per class per task is stored for the memory-based methods. For ORTHOG -SUBSPACE , episodic memory is not used for MNIST experiments, and the same amount of memory as other baselines is used for CIFAR100 and miniImageNet experiments. All experiments run for ﬁve different random seeds, each corresponding to a different dataset ordering among tasks, that are ﬁxed across baselines. Averages and standard deviations are reported across these runs. 4.4 Results Tab. 1 shows the overall results on all benchmarks. First, we observe that on relatively shallower networks (MNIST benchmarks), even without memory and preservation of orthogonality during the network training, ORTHOG -SUBSPACE outperform the strong memory-based baselines by a large margin: +7.1% and +9.2% absolute gain in average accuracy, 66% and 42% reduction in forgetting compared to the strongest baseline (ER-Ring), on Permuted and Rotated MNIST, respectively. This shows that learning in orthogonal subspaces is an effective strategy in reducing interference among different tasks. Second, for deeper networks, when memory is used and orthogonality is preserved, ORTHOG -SUBSPACE improves upon ER-Ring considerably: 4.7% and 1.6% absolute gain in average accuracy, 50% and 16.6% reduction in forgetting, on CIFAR100 and miniImageNet, respectively. While we focus on tiny episodic memory, in Tab. 3 of Appendix C, we provide results for larger episodic memory sizes. Our conclusions on tiny memory hold, however, the gap between the performance of ER-Ring and ORTHOG -SUBSPACE gets reduced as the episodic memory size is increased. The network can sufﬁciently mitigate forgetting by relearning on a large episodic memory. 7Table 1: Accuracy (2) and Forgetting (3) results of continual learning experiments. When used, episodic memories contain up to one example per class per task. Last row is a multi-task oracle baseline. METHOD PERMUTEDMNIST R OTATEDMNIST MEMORY ACCURACY FORGETTING ACCURACY FORGETTING FINETUNE \u0017 50.6 (±2.57) 0.44 (±0.02) 43.1 (±1.20) 0.55 (±0.01) EWC [KIRKPATRICK ET AL., 2016] \u0017 68.4 (±0.76) 0.25 (±0.01) 43.6 (±0.81) 0.53 (±0.01) VCL [NGUYEN ET AL., 2018] \u0017 51.8 (±1.54) 0.44 (±0.01) 48.2 (±0.99) 0.50 (±0.01) VCL-RANDOM[NGUYEN ET AL., 2018] \u0013 52.3 (±0.66) 0.43 (±0.01) 54.4 (±1.44) 0.44 (±0.01) AGEM [CHAUDHRY ET AL., 2019A] \u0013 78.3 (±0.42) 0.15 (±0.01) 60.5 (±1.77) 0.36 (±0.01) MER [RIEMER ET AL., 2019] \u0013 78.6 (±0.84) 0.15 (±0.01) 68.7 (±0.38) 0.28 (±0.01) ER-RING [CHAUDHRY ET AL., 2019B] \u0013 79.5 (±0.31) 0.12 (±0.01) 70.9 (±0.38) 0.24 (±0.01) ORTHOG-SUBSPACE(OURS) \u0017 86.6 (±0.91) 0.04 (±0.01) 80.1 (±0.95) 0.14 (±0.01) MULTITASK 91.3 0.0 94.3 0.0 METHOD SPLIT CIFAR S PLIT MINIIMAGENET MEMORY ACCURACY FORGETTING ACCURACY FORGETTING FINETUNE \u0017 42.6 (±2.72) 0.27 (±0.02) 36.1 (±1.31) 0.24 (±0.03) EWC [KIRKPATRICK ET AL., 2016] \u0017 43.2 (±2.77) 0.26 (±0.02) 34.8 (±2.34) 0.24 (±0.04) ICARL [REBUFFI ET AL., 2017] \u0013 46.4 (±1.21) 0.16 (±0.01) - - AGEM [CHAUDHRY ET AL., 2019A] \u0013 51.3 (±3.49) 0.18 (±0.03) 42.3 (±1.42) 0.17 (±0.01) MER [RIEMER ET AL., 2019] \u0013 49.7 (±2.97) 0.19 (±0.03) 45.5 (±1.49) 0.15 (±0.01) ER-RING [CHAUDHRY ET AL., 2019B] \u0013 59.6 (±1.19) 0.14 (±0.01) 49.8 (±2.92) 0.12 (±0.01) ORTHOG-SUBSPACE(OURS) \u0013 64.3 (±0.59) 0.07 (±0.01) 51.4 (±1.44) 0.10 (±0.01) MULTITASK 71.0 0.0 65.1 0.0 Table 2: Systematic evaluation of Projection, Memory and Orthogonalization in ORTHOG -SUBSPACE . METHOD SPLIT CIFAR S PLIT MINI IMAGE NET PROJECTION ER S TIEFEL ACCURACY FORGETTING ACCURACY FORGETTING \u0013 \u0017 \u0017 50.3 (±2.21) 0.21 (±0.02) 40.1 (±2.16) 0.20 (±0.02) \u0017 \u0013 \u0017 59.6 (±1.19) 0.14 (±0.01) 49.8 (±2.92) 0.12 (±0.01) \u0013 \u0013 \u0017 61.2 (±1.84) 0.10 (±0.01) 49.5 (±2.21) 0.11 (±0.01) \u0013 \u0013 \u0013 64.3 (±0.59) 0.07 (±0.01) 51.4 (±1.44) 0.10 (±0.01) Tab. 2 shows a systematic evaluation of projection(8), episodic memory and orthogonalization (7) in ORTHOG -SUBSPACE . First, without memory and orthogonalization, while a simple projection yields competitive results compared to various baselines (c.f . Tab. 1), the performance is still a far cry from ER-Ring. However, when the memory is used along with subspace projection one can already see a better performance compared to ER-Ring. Lastly, when the orthogonality is ensured by learning on a Stiefel manifold, the model achieves the best performance both in terms of accuracy and forgetting. Finally, Fig. 3 shows the distribution of the inner product of gradients between the current and previous tasks, stored in the episodic memory. Everything is kept the same except in one case the weight matrices are learned on the Stiefel manifold while in the other no such constraint is placed on the weights. We observe that when the weights remain on the Stiefel manifold, the distribution is more peaky around zero. This empirically validates our hypothesis that by keeping the transformation (10) isometric, the gradients of different tasks remain near orthogonal to one another in all the layers. 5 Related work In continual learning [Ring, 1997], also called lifelong learning [Thrun, 1998], a learner faces a sequence of tasks without storing the complete datasets of these tasks. This is in contrast to multitask learning [Caruana, 1997], where the learner can simultaneously access data from all tasks. The objective in continual learning is to avoid catastrophic forgetting The main challenge in continual learning is to avoid catastrophic forgetting [McCloskey and Cohen, 1989, McClelland et al., 1995, Goodfellow et al., 2013] on already seen tasks so that the learner is able to learn new tasks quickly. Existing literature in continual learning can be broadly categorized into three categories. 80.00 0.05 0.10 0.15 0.20 Inner product b\\w Task and Memory gradients 0 100 200 300Count No Orthogonality Stiefel (Orthogonality) Layer 15 0.0 0.1 0.2 0.3 0.4 Inner product b\\w Task and Memory gradients 0 50 100 150 200Count No Orthogonality Stiefel (Orthogonality) Layer 10 Figure 3: Histogram of inner product of current task and memory gradients in different layers in Split CIFAR. The more left the distribution is the more orthogonal the gradients are and the less the interference is between the current and previous tasks. First, regularization approachesreduce the drift in parameters important for past tasks [Kirkpatrick et al., 2016, Aljundi et al., 2018, Nguyen et al., 2018, Zenke et al., 2017]. For the large number of tasks, the parameter importance measures suffer from brittleness as the locality assumption embedded in the regularization-based approaches is violated [Titsias et al., 2019]. Furthermore, Chaudhry et al. [2019a] showed that these approaches can only be effective when the learner can perform multiple passes over the datasets of each task – a scenario not assumed in this work. Second,modular approaches use different network modules that can be extended for each new task [Fernando et al., 2017, Aljundi et al., 2017, Rosenbaum et al., 2018, Chang et al., 2018, Xu and Zhu, 2018, Ferran Alet, 2018]. By construction, modular approaches have zero forgetting, but their memory requirements increase with the number of tasks [Rusu et al., 2016, Lee et al., 2017]. Third, memory approaches maintain and replay a small episodic memory of data from past tasks. In some of these methods [Li and Hoiem, 2016, Rebufﬁ et al., 2017], examples in the episodic memory are replayed and predictions are kept invariant by means of distillation [Hinton et al., 2014]. In other approaches [Lopez-Paz and Ranzato, 2017, Chaudhry et al., 2019a, Aljundi et al., 2019] the episodic memory is used as an optimization constraint that discourages increases in loss at past tasks. Some works [Hayes et al., 2018, Riemer et al., 2019, Rolnick et al., 2018, Chaudhry et al., 2019b, 2020] have shown that directly optimizing the loss on the episodic memory, also known as experience replay, is cheaper than constraint-based approaches and improves prediction performance. Recently, Prabhu et al. [2020] showed that training at test time, using a greedily balanced collection of episodic memory, improved performance on a variety of benchmarks. Similarly, Javed and White [2019], Beaulieu et al. [2020] showed that learning transferable representations via meta-learning reduces forgetting when the model is trained on sequential tasks. Similar in spirit to our work is OGD Farajtabar et al. [2019] where the gradients of each task are learned in the orthogonal space of all the previous tasks’ gradients. However, OGD differs signiﬁcantly from our work in terms of memory and compute requirements. Unlike OGD, where the memory of previous task gradients is maintained, which is equivalent to storing nt ×Sdimensional matrix for each task, where nt are the number of examples in each task and Sis the network size, we only store m×rdimensional matrix Ot, where mis the dimension of the feature vector (m≪S) and ris the rank of the subspace, and a tiny replay buffer for each task. For large network sizes, OGD is impractical to use. Furthermore, at each training step OGD subtracts the gradient projections from the space spanned by the gradients in memory, whereas we only project the feature extraction layer to a subspace and maintain orthogonality via learning on Stiefel manifolds. Finally, learning orthonormal weight matrices has been extensively studied in literature. Orthogonal matrices are used in RNNs for avoiding exploding/ vanishing gradient problem [Arjovsky et al., 2016, Wisdom et al., 2016, Jing et al., 2017]. While the weight matrices are assumed to be square in the earlier works, works including [Huang et al., 2018] considered learning non-square orthogonal matrices (called orthonormal matrices in this work) by optimizing over the Stiefel manifolds. More recently, Li et al. [2020] proposed an iterative version of Cayley transform [Nishimori and Akaho, 2005], a key component in optimizing over Stiefel manifolds. Whereas optimizing over the Stiefel manifold ensure strict orthogonality in the weights, [Jia et al., 2019] proposed an algorithm, Singular Value Bounding (SVB), for soft orthogonality constraints. We use strict orthogonality in this work and leave the exploration of soft orthogonality for future research. 96 Conclusion We presented ORTHOG -SUBSPACE , a continual learning method that learns different tasks in orthogo- nal subspaces. The gradients in the projected layer are kept orthogonal in earlier layers by learning isometric transformations. The isometric transformations are learned by posing the network training as an optimization problem over the Stiefel manifold. The proposed approach improved considerably over strong memory replay-based baselines in standard continual learning benchmarks of image classiﬁcation. 7 Broader Impact Continual learning methods like the one we propose allow machine learning models to efﬁciently learn on new data without requiring constant retraining on previous data. This type of learning can be useful when the model is expected to perform in multiple environments and a simultaneous retraining on all the environments is not feasible. However, one of the core assumptions in continual learning is that model should have zero forgetting on previous data. In some scenarios, partially forgetting old data may be acceptable or even preferable, for example if older data was more biased (in any sense) than more recent data. A machine learning practitioner should be aware of this fact and use continual learning approaches only when suitable. Acknowledgment This work was supported by EPSRC/MURI grant EP/N019474/1, Facebook (DeepFakes grant), Five AI UK, and the Royal Academy of Engineering under the Research Chair and Senior Research Fellowships scheme. AC is funded by the Amazon Research Award (ARA) program. References P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009. R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, pages 7120–7129, 2017. R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. R. Aljundi, M. Lin, B. Goujaud, and Y . Bengio. Online continual learning with no task boundaries. arXiv preprint arXiv:1903.08671, 2019. M. Arjovsky, A. Shah, and Y . Bengio. Unitary evolution recurrent neural networks. InInternational Conference on Machine Learning, pages 1120–1128, 2016. S. Arora, S. S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and gen- eralization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019. S. Beaulieu, L. Frati, T. Miconi, J. Lehman, K. O. Stanley, J. Clune, and N. Cheney. Learning to continually learn. arXiv preprint arXiv:2002.09571, 2020. S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58(9):2217–2229, 2013. R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997. M. Chang, A. Gupta, S. Levine, and T. L. Grifﬁths. Automatically composing representation transformations as a means for generalization. In ICML workshop Neural Abstract Machines and Program Induction v2, 2018. A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018. 10A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efﬁcient lifelong learning with a-gem. In ICLR, 2019a. A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019b. A. Chaudhry, A. Gordo, P. K. Dokania, P. Torr, and D. Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 2020. M. Farajtabar, N. Azizan, A. Mott, and A. Li. Orthogonal gradient descent for continual learning. arXiv preprint arXiv:1910.07104, 2019. C. Fernando, D. Banarse, C. Blundell, Y . Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wier- stra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. L. P. K. Ferran Alet, Tomas Lozano-Perez. Modular meta-learning. arXiv preprint arXiv:1806.10166v1, 2018. I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y . Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. T. L. Hayes, N. D. Cahill, and C. Kanan. Memory efﬁcient experience replay for streaming learning. arXiv preprint arXiv:1809.05922, 2018. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS, 2014. L. Huang, X. Liu, B. Lang, A. W. Yu, Y . Wang, and B. Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. D. Isele and A. Cosgun. Selective experience replay for lifelong learning. arXiv preprint arXiv:1802.10269, 2018. K. Javed and M. White. Meta-learning representations for continual learning. In Advances in Neural Information Processing Systems, pages 1820–1830, 2019. K. Jia, S. Li, Y . Wen, T. Liu, and D. Tao. Orthogonal deep neural networks.IEEE transactions on pattern analysis and machine intelligence, 2019. L. Jing, Y . Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y . LeCun, M. Tegmark, and M. Soljaˇci´c. Tunable efﬁcient unitary neural networks (eunn) and their application to rnns. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1733–1741. JMLR. org, 2017. J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. PNAS, 2016. A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/cifar.html, 2009. Y . LeCun. The mnist database of handwritten digits.http://yann.lecun.com/exdb/mnist/, 1998. J. Lee, J. Yun, S. Hwang, and E. Yang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017. J. Li, F. Li, and S. Todorovic. Efﬁcient riemannian optimization on the stiefel manifold via the cayley transform. In International Conference on Learning Representations, 2020. Z. Li and D. Hoiem. Learning without forgetting. In ECCV, pages 614–629, 2016. D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continuum learning. In NIPS, 2017. 11J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995. M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989. C. V . Nguyen, Y . Li, T. D. Bui, and R. E. Turner. Variational continual learning.ICLR, 2018. A. Nichol and J. Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018, 2018. Y . Nishimori and S. Akaho. Learning algorithms utilizing quasi-geodesic ﬂows on the stiefel manifold. Neurocomputing, 67:106–135, 2005. A. Prabhu, P. H. S. Torr, and P. K. Dokania. GDumb: A simple approach that questions our progress in continual learning. In ECCV, 2020. S.-V . Rebufﬁ, A. Kolesnikov, and C. H. Lampert. iCaRL: Incremental classiﬁer and representation learning. In CVPR, 2017. M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y . Tu, and G. Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR, 2019. M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104, 1997. D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, and G. Wayne. Experience replay for continual learning. CoRR, abs/1811.11682, 2018. URL http://arxiv.org/abs/1811.11682. C. Rosenbaum, T. Klinger, and M. Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In ICLR, 2018. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211–252, 2015. A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding and counting linear regions of deep neural networks. arXiv preprint arXiv:1711.02114, 2017. H. D. Tagare. Notes on optimization on stiefel manifolds. In Technical report, Technical report. Yale University, 2011. S. Thrun. Lifelong learning algorithms. In Learning to learn, pages 181–209. Springer, 1998. M. K. Titsias, J. Schwarz, A. G. d. G. Matthews, R. Pascanu, and Y . W. Teh. Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356, 2019. O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. In NIPS, pages 3630–3638, 2016. S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent neural networks. In Advances in neural information processing systems, pages 4880–4888, 2016. J. Xu and Z. Zhu. Reinforced continual learning. In arXiv preprint arXiv:1805.12369v1, 2018. F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 12Appendix Section A provides a proof that isometry preserves angles. Section B derives the closed-form of the gradient projection on the tangent space at a point in the Stiefel manifold. Section C gives further experimental results. Section D lists the grid considered for hyper-parameters. A Isometry Preserves Angles Theorem A.1. T is an isometry iff it preserves inner products. Proof. Suppose T is an isometry. Then for any v,w ∈V, ∥T(v) −T(w)∥2 = ∥v−w∥2 ⟨T(v) −T(w),T(v) −T(w)⟩= ⟨v−w,v −w⟩ ∥T(v)∥2 + ∥T(w)∥2 −2⟨T(v),T(w)⟩= ∥v∥2 + ∥w∥2 −2⟨v,w⟩. Since ∥T(u)∥= ∥u∥for any uin V, all the length squared terms in the last expression above cancel out and we get ⟨T(v),T(w)⟩= ⟨v,w⟩. Conversely, if T preserves inner products, then ⟨T(v−w),T(v−w)⟩= ⟨v−w,v −w⟩, which implies ∥T(v−w)∥= ∥v−w∥, and since T is linear, ∥T(v) −T(w)∥= ∥v−w∥. This shows that T preserves distance. B Closed-form of Projection in Tangent Space This section closely follows the arguments of Tagare [2011]. Let {X ∈Rn×p|X⊤X = I}deﬁnes a manifold in Euclidean space Rn×p, where n > p. This manifold is called the Stiefel manifold. Let TX denotes a tangent space at X. Lemma B.1. Any Z ∈TX satisﬁes: Z⊤X+ X⊤Z = 0 i.e. Z⊤X is a skew-symmetric p×pmatrix. Note, that Xconsists of porthonormal vectors in Rn. Let X⊥be a matrix consisting of the additional n−porthonormal vectors in Rn i.e. X⊥lies in the orthogonal compliment of X, X⊤X⊥= 0. The concatenation of X and X⊥, [XX⊥] is n×northonormal matrix. Then, any matrix U ∈Rn×p can be represented as: U = XA+ X⊥B, where Ais a p×pmatrix, and Bis a (n−p) ×pmatrix. Lemma B.2. A matrix Z = XA+ X⊥Bbelongs to the tangent space at a point on Stiefel manifold TX iff A is skew-symmetric. Let G∈Rn×p be the gradient computed at X. Let the projection of the gradient on the tangent space is denoted by πTX (G). Lemma B.3. Under the canonical inner product, the projection of the gradient on the tangent space is given by πTX (G) = AX, where A= GX⊤−XG⊤. 13Proof. Express G = XGA + X⊥GB. Let Z be any vector in the tangent space, expressed as Z = XZA + X⊥ZB, where ZA is a skew-symmetric matrix according to B.2. Therefore, πTX (G) = tr(G⊤Z), = tr((XGA + X⊥GB)⊤(XZA + X⊥ZB)), = tr(G⊤ AZA + G⊤ BZB). (11) Writing GA as GA = sym(GA) + skew(GA), and plugging in (11) gives, πTX (G) = tr(skew(GA)⊤ZA + G⊤ BZB). (12) Let U = XA+ X⊥B is the vector that represents the projection of Gon the tangent space at X. Then, ⟨U,Z⟩c = tr(U⊤(I−1 2XX⊤)Z), = tr((XA+ X⊥B)⊤(I−1 2XX⊤)(XZA + X⊥ZB)), = tr(1 2A⊤ZA + B⊤ZB) (13) By comparing (12) and (13), we get A= 2skew(GA) and B = GB. Thus, U = 2Xskew(GA) + X⊥GB, = X(GA −G⊤ A) + X⊥GB, ∵ skew(GA) = 1 2(GA −G⊤ A) = XGA −XG⊤ A + G−XGA, ∵ G= XGA + X⊥GB = G−XG⊤ A, = G−XG⊤X, ∵ GA = X⊤G, = GX⊤X−XG⊤X, = (GX⊤−XG⊤)X C More Results Table 3: Accuracy (2) and Forgetting(3) results of continual learning experiments for larger episodic memory sizes. 2, 3 and 5 samples per class per task are stored, respectively. Top table is for Split CIFAR. Bottom table is for Split miniImageNet. METHOD ACCURACY FORGETTING 2 3 5 2 3 5 AGEM 52.2 (±2.59) 56.1 (±1.52) 60.9 (±2.50) 0.16 (±0.01) 0.13 (±0.01) 0.11 (±0.01) ER-RING 61.9 (±1.92) 64.8 (±0.77) 67.2 (±1.72) 0.11 (±0.02) 0.08 (±0.01) 0.06 (±0.01) ORTHOG-SUBSPACE 64.7 (±0.53) 66.8 (±0.83) 67.3 (±0.98) 0.07 (±0.01) 0.05 (±0.01) 0.05 (±0.01) METHOD ACCURACY FORGETTING 2 3 5 2 3 5 AGEM 45.2 (±2.35) 47.5 (±2.59) 49.2 (±3.35) 0.14 (±0.01) 0.13 (±0.01) 0.10 (±0.01) ER-RING 51.2 (±1.99) 53.9 (±2.04) 56.8 (±2.31) 0.10 (±0.01) 0.09 (±0.02) 0.06 (±0.01) ORTHOG-SUBSPACE 53.4 (±1.23) 55.6 (±0.55) 58.2 (±1.08) 0.07 (±0.01) 0.06 (±0.01) 0.05 (±0.01) D Hyper-parameter Selection In this section, we report the hyper-parameters grid considered for experiments. The best values for different benchmarks are given in parenthesis. 14• Multitask – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • Finetune – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • EWC – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] – regularization: [0.1, 1, 10 (MNIST perm, rot, CIFAR, miniImageNet), 100, 1000] • AGEM – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • MER – learning rate: [0.003, 0.01, 0.03 (MNIST, CIFAR, miniImageNet), 0.1, 0.3, 1.0] – within batch meta-learning rate: [0.01, 0.03, 0.1 (MNIST, CIFAR, miniImageNet), 0.3, 1.0] – current batch learning rate multiplier: [1, 2, 5 (CIFAR, miniImageNet), 10 (MNIST)] • ER-Ring – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • ORTHOG -SUBSPACE – learning rate: [0.003, 0.01, 0.03, 0.1 (MNIST perm, rot), 0.2 (miniImageNET), 0.4 (CIFAR), 1.0] 150.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 20 40 60 80Count No Orthogonality Stiefel (Orthogonality) (a) L1 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 25 50 75 100Count No Orthogonality Stiefel (Orthogonality) (b) L2 0.0 0.2 0.4 0.6 Inner product b\\w Task and Memory gradients 0 25 50 75 100Count No Orthogonality Stiefel (Orthogonality) (c) L3 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 25 50 75 100 125Count No Orthogonality Stiefel (Orthogonality) (d) L4 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (e) L5 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (f) L6 0.0 0.2 0.4 0.6 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (g) L7 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (h) L8 0.0 0.1 0.2 0.3 0.4 0.5 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (i) L9 0.0 0.1 0.2 0.3 0.4 0.5 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (j) L10 0.0 0.1 0.2 0.3 0.4 Inner product b\\w Task and Memory gradients 0 50 100 150 200Count No Orthogonality Stiefel (Orthogonality) (k) L11 0.0 0.1 0.2 0.3 0.4 Inner product b\\w Task and Memory gradients 0 100 200Count No Orthogonality Stiefel (Orthogonality) (l) L12 0.0 0.1 0.2 0.3 Inner product b\\w Task and Memory gradients 0 100 200 300Count No Orthogonality Stiefel (Orthogonality) (m) L13 0.00 0.05 0.10 0.15 0.20 Inner product b\\w Task and Memory gradients 0 50 100 150 200Count No Orthogonality Stiefel (Orthogonality) (n) L14 0.000 0.025 0.050 0.075 0.100 0.125 0.150 Inner product b\\w Task and Memory gradients 0 100 200Count No Orthogonality Stiefel (Orthogonality) (o) L15 0.00 0.05 0.10 0.15 0.20 Inner product b\\w Task and Memory gradients 0 100 200 300Count No Orthogonality Stiefel (Orthogonality) (p) L16 Figure 4: Histogram of inner product of current task and memory gradients in all layers in Split CIFAR. 16",
      "meta_data": {
        "arxiv_id": "2010.11635v2",
        "authors": [
          "Arslan Chaudhry",
          "Naeemullah Khan",
          "Puneet K. Dokania",
          "Philip H. S. Torr"
        ],
        "published_date": "2020-10-22T12:07:43Z",
        "venue": "NeurIPS, 2020",
        "pdf_url": "https://arxiv.org/pdf/2010.11635v2.pdf",
        "github_url": "https://github.com/arslan-chaudhry/orthog_subspace"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses catastrophic forgetting in continual learning by proposing a novel method called ORTHOG-SUBSPACE. It introduces the idea of learning different tasks in distinct, low-rank orthogonal vector subspaces to minimize interference. A key contribution is ensuring that backpropagated gradients from these subspaces remain orthogonal throughout deeper network layers by learning isometric transformations, achieved through optimization over the Stiefel manifold. The method demonstrates strong improvements in average accuracy and significant reductions in forgetting compared to powerful experience-replay baselines, both with and without episodic memory, on standard classification benchmarks.",
        "methodology": "The ORTHOG-SUBSPACE methodology involves two main components. First, for each task, the feature map of the second-last layer of the neural network is projected into a task-specific, low-dimensional orthogonal subspace using pre-constructed projection matrices (Pt). These matrices ensure orthogonality of features and gradients at this layer. Second, to maintain orthogonality of gradients as they backpropagate through earlier layers, the network weights are constrained to be orthonormal. This is achieved by posing network training as an optimization problem on the Stiefel manifold. Weights are initialized as orthonormal matrices, and during training, the combined gradients from the current task and a tiny replay buffer (for past tasks) are projected onto the tangent space of the Stiefel manifold, and then retracted onto the manifold using an iterative Cayley transform to update weights. The approach leverages a tiny episodic memory (1 example per class per task) for computing past task gradients.",
        "experimental_setup": "The method was evaluated on four supervised classification benchmarks: Permuted MNIST, Rotated MNIST (both with 23 tasks), Split CIFAR-100, and Split miniImageNet (both with 20 tasks). Network architectures used were a fully-connected network with two hidden layers for MNIST and a ResNet18 for CIFAR/ImageNet experiments. Baselines included Finetune, EWC, VCL, AGEM, MER, iCaRL, and ER-Ring (a strong memory-based baseline), along with a Multitask oracle. Performance was measured using average accuracy and average maximum forgetting. Hyper-parameters were tuned on the first three tasks of each benchmark. A tiny ring buffer storing one example per class per task was used for memory-based methods (except for ORTHOG-SUBSPACE on MNIST, where no memory was used). All experiments were run for five different random seeds with fixed dataset ordering.",
        "limitations": "The paper implicitly notes several limitations. The benefits of ORTHOG-SUBSPACE, particularly over strong experience-replay baselines, tend to decrease as the size of the episodic memory increases, suggesting its greatest impact is with tiny memory buffers. The approach assumes that the network is 'sufficiently parameterized' to learn tasks in independent subspaces. It also assumes that the non-linearity (ReLU) remains in its linear region during training for Jacobian calculations, simplifying gradient transformations. The method requires a task identifier at both train and test times. Furthermore, if the total number of tasks exceeds the pre-allocated capacity (T) for orthogonal projection matrices, it would necessitate dynamically resizing the orthogonal matrix and potentially the network's second-last layer.",
        "future_research_directions": "A clear direction for future research mentioned in the paper is the exploration of soft orthogonality constraints, in contrast to the strict orthogonality enforced by optimizing over the Stiefel manifold in this work. Another potential extension involves dynamically resizing the orthogonal projection matrix (and implicitly the second-last layer of the network) if the total number of tasks exceeds the initial capacity, to accommodate a larger sequence of tasks.",
        "experimental_code": "def weight_variable(shape, name='fc', init_type='default'):    with tf.variable_scope(name):        if init_type == 'default':            if shape[1] == 10:                weights = tf.get_variable('classifier', shape, tf.float32, initializer=tf.initializers.orthogonal())            else:                weights = tf.get_variable('kernel', shape, tf.float32, initializer=tf.initializers.orthogonal())    return weights\n\ndef fc_feedforward(self, h, weights, biases, apply_dropout=False):\n    if apply_dropout:        h = tf.nn.dropout(h, 1)    for ii, (w, b) in enumerate(list(zip(weights, biases))[:-1]):        if self.imp_method in {'SUBSPACE-PROJ', 'ER-SUBSPACE', 'PROJ-ANCHOR', 'PROJ-SUBSPACE-GP'} and ii == len(weights) - 2: # Apply projection at the second last layer only            h = create_fc_layer(h, w, b, P=self.subspace_proj, OWN=False)        else:            h = create_fc_layer(h, w, b, OWN=False)        if apply_dropout:            h = tf.nn.dropout(h, 1)    self.features = h    self.image_feature_dim = h.get_shape().as_list()[-1]    return create_fc_layer(h, weights[-1], biases[-1], apply_relu=False)\n\ndef resnet18_conv_feedforward(self, h, kernels, filters, strides):\n    self.trainable_vars = []\n    init_orthog =  True if (self.imp_method == 'ER-SUBSPACE') else False    h = _conv(h, kernels[0], filters[ff], strides[ff], self.trainable_vars, orthog_init=init_orthog, name='conv_1')    h = _bn(h, self.trainable_vars, self.train_phase, name='bn_1')    h = _residual_block(h, self.trainable_vars, self.train_phase, orthog_init=init_orthog, name='conv2_1')    h = tf.reduce_mean(h, [1, 2])    self.features = h    self.image_feature_dim = h.get_shape().as_list()[-1]\n    if self.imp_method in {'SUBSPACE-PROJ', 'ER-SUBSPACE', 'PROJ-ANCHOR', 'PROJ-SUBSPACE-GP', 'ER-SUBSPACE-GP'}:\n        h = tf.nn.relu(tf.matmul(h, self.subspace_proj))    logits = _fc(h, self.total_classes, self.trainable_vars, orthog_init=init_orthog, name='classifier/fc_1')    return logits\n\ndef create_er_subspace_ops(self):\n    self.kernel_grads = [grad for grad, var in self.reg_gradients_vars if 'kernel' in var.name and 'shortcut' not in var.name]\n    self.accum_er_subspace_grads = [self.accum_grads[i].assign_add(grad/ self.task_id) for i, (grad, var) in enumerate(self.reg_gradients_vars)]\n    self.reset_er_subspace_grads = [v.assign(tf.zeros_like(v)) for v in self.accum_grads]\n    with tf.control_dependencies(self.accum_er_subspace_grads):\n        self.train_er_subspace = self.opt.apply_gradients(zip(self.accum_grads, self.trainable_vars))\n\ndef create_stiefel_ops(self):\n    def matrix_one_norm(a):\n        return tf.reduce_max(tf.norm(a, ord=1, axis=1))    update_stiefel_vars = []    if self.imp_method == 'ER-SUBSPACE':        grad_vars = zip(self.accum_grads, self.trainable_vars)    elif self.imp_method == 'SUBSPACE-PROJ':        grad_vars = self.reg_gradients_vars    for i, (grad, var) in enumerate(grad_vars):\n        if 'kernel' in var.name:\n            shape = var.get_shape().as_list()\n            W = tf.reshape(var, [-1, shape[-1]])\n            G = tf.reshape(grad, [-1, shape[-1]])\n            if STIEFEL_INNP_EUC:\n                gxT = tf.matmul(G, W, transpose_b=True)\n                xTgxT = tf.matmul(W, gxT, transpose_a=True)\n                A_hat = gxT - 0.5*tf.matmul(W, xTgxT)\n                A = A_hat - tf.transpose(A_hat)            else: # Canonical inner prod                A = tf.matmul(G, W, transpose_b=True) - tf.matmul(W, G, transpose_b=True)\n            U = tf.matmul(A, W) # [In x Out] # Tangent vector\n            t = 0.5 * 2 / (matrix_one_norm(A) + 1e-8)\n            alpha = tf.minimum(self.learning_rate, t)\n            Y_0 = W - alpha*U\n            Y_1 = W - alpha*tf.matmul(A, 0.5*(W+Y_0))\n            Y_2 = W - alpha*tf.matmul(A, 0.5*(W+Y_1))\n            update_stiefel_vars.append(var.assign(tf.reshape(Y_2, shape)))\n        else:\n            apply_grads = self.opt.apply_gradients(zip([grad], [var]))            update_stiefel_vars.append(apply_grads)    self.train_stiefel = tf.group(*update_stiefel_vars)\n\ndef train_task_sequence(model, sess, datasets, args):\n    proj_matrices = generate_projection_matrix(model.num_tasks, feature_dim=model.subspace_proj.get_shape()[0], share_dims=args.subspace_share_dims, qr=QR)\n    unit_test_projection_matrices(proj_matrices)\n    # ... (inside loop for each task) ...\n    if model.imp_method == 'SUBSPACE-PROJ':\n        feed_dict[model.output_mask] = logit_mask\n        feed_dict[model.subspace_proj] = proj_matrices[task]\n        if args.maintain_orthogonality:\n            _, loss = sess.run([model.train_stiefel, model.reg_loss], feed_dict=feed_dict)\n        else:\n            _, loss = sess.run([model.train, model.reg_loss], feed_dict=feed_dict)\n    elif model.imp_method == 'ER-SUBSPACE':\n        sess.run([model.reset_er_subspace_grads])\n        if task > 0:\n            tt = np.squeeze(np.random.choice(np.arange(task), 1, replace=False))\n            er_mem_indices = np.arange(mem_offset, mem_offset+args.mem_size*classes_per_task)\n            np.random.shuffle(er_mem_indices)\n            er_train_x_batch = episodic_images[er_mem_indices]\n            er_train_y_batch = episodic_labels[er_mem_indices]\n            feed_dict = {model.x: er_train_x_batch, model.y_: er_train_y_batch,            # ...            feed_dict[model.subspace_proj] = proj_matrices[tt]\n            _, mem_grads = sess.run([model.accum_er_subspace_grads, model.kernel_grads], feed_dict=feed_dict)\n        feed_dict = {model.x: train_x[offset:offset+residual], model.y_: train_y[offset:offset+residual],            # ...            feed_dict[model.subspace_proj] = proj_matrices[task]\n        if args.maintain_orthogonality:\n            # ...            sess.run(model.train_stiefel, feed_dict={model.learning_rate: args.learning_rate})\n        else:\n            _, _, task_grads, kernel_isometry, loss = sess.run([model.train_er_subspace, model.accum_er_subspace_grads, model.kernel_grads, model.kernel_isometry, model.reg_loss], feed_dict=feed_dict)\n        update_fifo_buffer(train_x[offset:offset+residual], train_y[offset:offset+residual], episodic_images, episodic_labels,            task_labels[task], args.mem_size, count_cls, episodic_filled_counter)\n",
        "experimental_info": "The ORTHOG-SUBSPACE method is implemented using TensorFlow. It encompasses two main components: task-specific orthogonal subspace projection for the second-last layer's feature map, and orthonormal weight constraints enforced by optimizing on the Stiefel manifold using an iterative Cayley transform. For `ER-SUBSPACE`, a tiny episodic memory (FIFO ring buffer) is used for replaying past task gradients, which are accumulated and then used in the Stiefel manifold optimization. Weights are initialized as orthonormal matrices.\n\nKey experimental settings/hyperparameters:\n- **Method (`imp_method`):** `SUBSPACE-PROJ` (for subspace projection only) or `ER-SUBSPACE` (for subspace projection with episodic replay).\n- **Orthogonality Constraint (`--maintain-orthogonality`):** A boolean flag to activate Stiefel manifold optimization for network weights.\n- **Projection Matrix Generation:** Matrices `proj_matrices` are generated per task using `generate_projection_matrix`. This function takes `num_tasks`, `feature_dim` (dimension of the second-last layer's output), `share_dims` (number of dimensions to share across tasks), and `qr` (boolean for QR decomposition-based generation) as input.\n- **Subspace Dimension (`--subspace-share-dims`):** Number of dimensions to share across tasks, affecting the rank of the task-specific orthogonal projection.\n- **Episodic Memory (for `ER-SUBSPACE`):**\n  - **Memory Size (`--mem-size`):** Total size of episodic memory (e.g., 1 example per class per task).\n  - **Episodic Memory Batch Size (`--eps-mem-batch`):** Number of samples from episodic memory to use in a batch for computing past task gradients.\n- **Network Architecture (`--arch`):** e.g., 'RESNET-S' or 'FC-S'.\n- **Optimizer (`--optim`):** e.g., 'SGD' (Stochastic Gradient Descent) with momentum.\n- **Learning Rate (`--learning-rate`):** e.g., 0.1 or 1e-3.\n- **Number of Tasks (`--num-tasks`):** Total number of sequential tasks (e.g., 10 or 23).\n- **Random Seed (`--random-seed`):** For reproducibility across runs.\n- **Feature Dimension (`feature_dim`):** The dimension of the feature map before the classification layer, dynamically determined from the model's architecture, e.g., 256 or 512."
      }
    },
    {
      "title": "Contextual Transformation Networks for Online Continual Learning"
    },
    {
      "title": "SparCL: Sparse Continual Learning on the Edge",
      "abstract": "Existing work in continual learning (CL) focuses on mitigating catastrophic\nforgetting, i.e., model performance deterioration on past tasks when learning a\nnew task. However, the training efficiency of a CL system is\nunder-investigated, which limits the real-world application of CL systems under\nresource-limited scenarios. In this work, we propose a novel framework called\nSparse Continual Learning(SparCL), which is the first study that leverages\nsparsity to enable cost-effective continual learning on edge devices. SparCL\nachieves both training acceleration and accuracy preservation through the\nsynergy of three aspects: weight sparsity, data efficiency, and gradient\nsparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a\nsparse network throughout the entire CL process, dynamic data removal (DDR) to\nremove less informative training data, and dynamic gradient masking (DGM) to\nsparsify the gradient updates. Each of them not only improves efficiency, but\nalso further mitigates catastrophic forgetting. SparCL consistently improves\nthe training efficiency of existing state-of-the-art (SOTA) CL methods by at\nmost 23X less training FLOPs, and, surprisingly, further improves the SOTA\naccuracy by at most 1.7%. SparCL also outperforms competitive baselines\nobtained from adapting SOTA sparse training methods to the CL setting in both\nefficiency and accuracy. We also evaluate the effectiveness of SparCL on a real\nmobile phone, further indicating the practical potential of our method.",
      "full_text": "SparCL: Sparse Continual Learning on the Edge Zifeng Wang1,†, Zheng Zhan1,†, Yifan Gong1, Geng Yuan1, Wei Niu2, Tong Jian1, Bin Ren2, Stratis Ioannidis1, Yanzhi Wang1, Jennifer Dy1 1 Northeastern University, 2 College of William and Mary {zhan.zhe, gong.yifa, geng.yuan, yanz.wang}@northeastern.edu, {zifengwang, jian, ioannidis, jdy}@ece.neu.edu, wniu@email.wm.edu, bren@cs.wm.edu Abstract Existing work in continual learning (CL) focuses on mitigating catastrophic for- getting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efﬁciency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning (SparCL), which is the ﬁrst study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dy- namic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efﬁciency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efﬁciency of existing state-of-the-art (SOTA) CL methods by at most 23×less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most1.7%. SparCL also outperforms competitive base- lines obtained from adapting SOTA sparse training methods to the CL setting in both efﬁciency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method. Source code will be released. 1 Introduction The objective of Continual Learning (CL) is to enable an intelligent system to accumulate knowledge from a sequence of tasks, such that it exhibits satisfying performance on both old and new tasks (31). Recent methods mostly focus on addressing the catastrophic forgetting (42) problem – learning model tends to suffer performance deterioration on previously seen tasks. However, in the real world, when the CL applications are deployed in resource-limited platforms ( 47) such as edge devices, the learning efﬁciency, w.r.t. both training speed and memory footprint, are also crucial metrics of interest, yet they are rarely explored in prior CL works. Existing CL methods can be categorized into regularization-based (2; 31; 36; 67), rehearsal-based (8; 11; 49; 60), and architecture-based (30; 41; 51; 57; 58; 69). Both regularization- and rehearsal-based methods directly train a dense model, which might even be over-parametrized for the union of all tasks (18; 38); Though several architecture-based methods (50; 56; 63) start with a sparse sub-network from the dense model, they still grow the model size progressively to learn emerging tasks. The aforementioned methods, although striving for greater performance with less forgetting, still introduce signiﬁcant memory and computation overhead during the whole CL process. †Both authors contributed equally to this work 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2209.09476v1  [cs.LG]  20 Sep 2022Figure 1: Left: Overview of SparCL. SparCL consists of three complementary components: task-aware dynamic masking (TDM) for weight sparsity, dynamic data removal (DDR) for data efﬁciency, and dynamic gradient masking (DGM) for gradient sparsity. Right: SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, with different sparsity ratios on the Split Tiny-ImageNet (15) dataset. Recently, another stream of work, sparse training (4; 19; 34) has emerged as a new training trend to achieve training acceleration, which embraces the promising training-on-the-edge paradigm. With sparse training, each iteration takes less time with the reduction in computation achieved by sparsity, under the traditional i.i.d. learning setting. Inspired by these sparse training methods, we naturally think about introducing sparse training to the ﬁeld of CL. A straightforward idea is to directly combine existing sparse training methods, such as SNIP ( 34), RigL (19), with a rehearsal buffer under the CL setting. However, these methods fail to consider key challenges in CL to mitigate catastrophic forgetting, for example, properly handling transition between tasks. As a result, these sparse training methods, though enhancing training efﬁciency, cause signiﬁcant accuracy drop (see Section 5.2). Thus, we would like to explore a general strategy, which is orthogonal to existing CL methods, that not only leverages the idea of sparse training for efﬁciency, but also addresses key challenges in CL to preserve (or even improve) accuracy. In this work, we propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, aiming at enabling practical CL on edge devices. As shown in Figure 1 (left), SparCL achieves both learning acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, to maintain a small dynamic sparse network during the whole CL process, we develop a novel task-aware dynamic masking (TDM) strategy to keep only important weights for both the current and past tasks, with special consideration during task transitions. Moreover, we propose a dynamic data removal (DDR) scheme, which progressively removes “easy-to-learn” examples from training iterations, which further accelerates the training process and also improves accuracy of CL by balancing current and past data and keeping more informative samples in the buffer. Finally, we provide an additional dynamic gradient masking (DGM) strategy to leverage gradient sparsity for even better efﬁciency and knowledge preservation of learned tasks, such that only a subset of sparse weights are updated. Figure 1 (right) demonstrates that SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, under different sparsity ratios. SparCL is simple in concept, compatible with various existing rehearsal-based CL methods, and efﬁcient under practical scenarios. We conduct comprehensive experiments on multiple CL bench- marks to evaluate the effectiveness of our method. We show that SparCL works collaboratively with existing CL methods, greatly accelerates the learning process under different sparsity ratios, and even sometimes improves upon the state-of-the-art accuracy. We also establish competing baselines by combining representative sparse training methods with advanced rehearsal-based CL methods. SparCL again outperforms these baselines in terms of both efﬁciency and accuracy. Most importantly, we evaluate our SparCL framework on real edge devices to demonstrate the practical potential of our method. We are not aware of any prior CL works that explored this area and considered the constraints of limited resources during training. In summary, our work makes the following contributions: • We propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, which achieves learning acceleration through the synergy of weight sparsity, data efﬁ- ciency, and gradient sparsity. To the best of our knowledge, our work is the ﬁrst to introduce the idea of sparse training to enable efﬁcient CL on edge devices. 2• SparCL shows superior performance compared to both conventional CL methods and CL-adapted sparse training methods on all benchmark datasets, leading to at most 23×less training FLOPs and, surprisingly, 1.7% improvement over SOTA accuracy. • We evaluate SparCL on a real mobile edge device, demonstrating the practical potential of our method and also encouraging future research on CL on-the-edge. The results indicate that our framework can achieve at most 3.1×training acceleration. 2 Related work 2.1 Continual Learning The main focus in continual learning (CL) has been mitigating catastrophic forgetting. Existing methods can be classiﬁed into three major categories. Regularization-based methods (2; 31; 36; 67) limit updates of important parameters for the prior tasks by adding corresponding regularization terms. While these methods reduce catastrophic forgetting to some extent, their performance deteriorates under challenging settings ( 39), and on more complex benchmarks ( 49; 60). Rehearsal-based methods (12; 13; 24) save examples from previous tasks into a small-sized buffer to train the model jointly with the current task. Though simple in concept, the idea of rehearsal is very effective in practice and has been adopted by many state-of-the-art methods ( 8; 10; 48). Architecture-based methods (41; 50; 56; 58; 62) isolate existing model parameters or assign additional parameters for each task to reduce interference among tasks. As mentioned in Section 1, most of these methods use a dense model without consideration of efﬁciency and memory footprint, thus are not applicable to resource-limited settings. Our work, orthogonal to these methods, serves as a general framework for making these existing methods efﬁcient and enabling a broader deployment, e.g., CL on edge devices. A limited number of works explore sparsity in CL, however, for different purposes. Several methods (40; 41; 52; 56) incorporate the idea of weight pruning ( 23) to allocate a sparse sub-network for each task to reduce inter-task interference. Nevertheless, these methods still reduce the full model sparsity progressively for every task and ﬁnally end up with a much denser model. On the contrary, SparCL maintains a sparse network throughout the whole CL process, introducing great efﬁciency and memory beneﬁts both during training and at the output model. A recent work ( 14) aims at discovering lottery tickets (20) under CL, but still does not address efﬁciency. However, the existence of lottery tickets in CL serves as a strong justiﬁcation for the outstanding performance of our SparCL. 2.2 Sparse Training There are two main approaches for sparse training: ﬁxed-mask sparse training and dynamic sparse training. Fixed-mask sparse training methods ( 34; 53; 55; 59) ﬁrst apply pruning, then execute traditional training on the sparse model with the obtained ﬁxed mask. The pre-ﬁxed structure limits the accuracy performance, and the ﬁrst stage still causes huge computation and memory consumption. To overcome these drawbacks, dynamic mask methods (4; 16; 19; 44; 45) adjust the sparsity topology during training while maintaining low memory footprint. These methods start with a sparse model structure from an untrained dense model, then combine sparse topology exploration at the given sparsity ratio with the sparse model training. Recent work (66) further considers to incorporate data efﬁciency into sparse training for better training accelerations. However, all prior sparse training works are focused on the traditional training setting, while CL is a more complicated and difﬁcult scenario with inherent characteristics not explored by these works. In contrast to prior sparse training methods, our work explores a new learning paradigm that introduces sparse training into CL for efﬁciency and also addresses key challenges in CL, mitigating catastrophic forgetting. 3 Continual Learning Problem Setup In supervised CL, a model fθ learns from a sequence of tasks D= {D1,..., DT}, where each task Dt = {(xt,i,yt,i)}nt i=1 consists of input-label pairs, and each task has a disjoint set of classes. Tasks arrive sequentially, and the model must adapt to them. At the t-th step, the model gains access to data from the t-th task. However, a small ﬁx-sized rehearsal buffer Mis allowed to save data from prior tasks. At test time, the easiest setting is to assume task identity is known for each coming test example, named task-incremental learning (Task-IL). If this assumption does not hold, we have the 3...... T ask-aware Dynamic Masking  (TDM)  (a) -and- Expand Shrink Dynamic Data Removal  (DDR) Dynamic Gradient Masking  (DGM) Remove  “easier” samples Update  important gradients  (b) -and- Shrink Expand T ask 1 T ask t (a) TDM DDR DGM (b) T ask T    Epochs Figure 2: Illustration of the SparCL workﬂow. Three components work synergistically to improve training efﬁciency and further mitigate catastrophic forgetting for preserving accuracy. more difﬁcult class-incremental learning (Class-IL) setting. In this work, we mainly focus on the more challenging Class-IL setting, and only report Task-IL performance for reference. The goal of conventional CL is to train a model sequentially that performs well on all tasks at test time. The main evaluation metric is average test accuracy on all tasks. In real-world resource- limited scenarios, we should further consider training efﬁciency of the model. Thus, we measure the performance of the model more comprehensively by including training FLOPs and memory footprint. 4 Sparse Continual Learning (SparCL) Our method, Sparse Continual Learning, is a uniﬁed framework composed of three complementary components: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. The entire framework is shown in Figure 2. We will illustrate each component in detail in this section. 4.1 Task-aware Dynamic Masking To enable cost-effective CL in resource limited scenarios, SparCL is designed to maintain a dynamic structure when learning a sequence of tasks, such that it not only achieves high efﬁciency, but also intelligently adapts to the data stream for better performance. Speciﬁcally, we propose a strategy named task-aware dynamic masking (TDM), which dynamically removes less important weights and grows back unused weights for stronger representation power periodically by maintaining a single binary weight mask throughout the CL process. Different from typical sparse training work, which only leverages the weight magnitude ( 44) or the gradient w.r.t. data from a single training task (19; 66), TDM considers also the importance of weights w.r.t. data saved in the rehearsal buffer, as well as the switch between CL tasks. Speciﬁcally, TDM strategy starts from a randomly initialized binary mask Mθ = M0, with a given sparsity constraint ∥Mθ∥0/∥θ∥0 = 1−s, where s∈[0,1] is the sparsity ratio. Moreover, it makes different intra- and inter-task adjustments to keep a dynamic sparse set of weights based on their continual weight importance (CWI). We summarize the process of task-aware dynamic masking in Algorithm 1 and elaborate its key components in detail below. Continual weight importance (CWI).For a model fθ parameterized by θ, the CWI of weight w⊂θis deﬁned as follows: CWI(w) =∥w∥1 + α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1, (1) where Dt denotes the training data from the t-th task, Mis the current rehearsal buffer, and α, βare coefﬁcients to control the inﬂuence of current and buffered data, respectively. Moreover,Lrepresents the cross-entropy loss for classiﬁcation, while ˜Lis the single-head (1) version of the cross-entropy loss, which only considers classes from the current task by masking out the logits of other classes. Intuitively, CWI ensures we keep (1) weights of larger magnitude for output stability, (2) weights important for the current task for learning capacity, and (3) weights important for past data to mitigate catastrophic forgetting. Moreover, inspired by the classiﬁcation bias in CL (1), we use the single-head cross-entropy loss when calculating importance score w.r.t. the current task to make the importance estimation more accurate. 4Algorithm 1:Task-aware Dynamic Masking (TDM) Input: Model weight θ, number of tasks T, training epochs of the t-th task Kt, binary sparse mask Mθ, sparsity ratio s, intra-task adjustment ratio pintra, inter-task adjustment ratio pinter, update interval δk Initialize: θ, Mθ, s.t. ∥Mθ∥0/∥θ∥0 = 1−s for t= 1,...,T do for e= 1,...,K T do if t> 1 then /* Inter-task adjustment */ Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s−pinter) if e= δkthen Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end end if e mod δk = 0then /* Intra-task adjustment */ Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s+ pintra) Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end Update θ⊙Mθ via backpropagation end end Intra-task adjustment.When training the t-th task, a natural assumption is that the data distribution is consistent inside the task, thus we would like to update the sparse model in a relatively stable way while keeping its ﬂexibility. Thus, in Algorithm 1, we choose to update the sparsity mask Mθ in a shrink-and-expand way every δkepochs. We ﬁrst remove pintra of the weights of least CWI to retain learned knowledge so far. Then we randomly select unused weights to recover the learning capacity for the model and keep the sparsity ratio sunchanged. Inter-task adjustment.When tasks switches, on the contrary, we assume data distribution shifts immediately. Ideally, we would like the model to keep the knowledge learned from old tasks as much as possible, and to have enough learning capacity to accommodate the new task. Thus, instead of the shrink-and-expand strategy for intra-task adjustment, we follow an expand-and-shrink scheme. Speciﬁcally, at the beginning of the (t+ 1)-th task, we expand the sparse model by randomly adding a proportion of pinter unused weights. Intuitively, the additional learning capacity facilitates fast adoption of new knowledge and reduces interference with learned knowledge. We allow our model to have smaller sparsity (i.e., larger learning capacity) temporarily for the ﬁrst δk epochs as a warm- up period, and then remove the pinter weights with least CWI, following the same process in the intra-task case, to satisfy the sparsity constraint. 4.2 Dynamic Data Removal In addition to weight sparsity, decreasing the amount of training data can be directly translated into the saving of training time without any requirements for hardware support. Thus, we would also like to explore data efﬁciency to reduce the training workload. Some prior CL works select informative examples to construct the rehearsal buffer ( 3; 6; 64). However, the main purpose of them is not training acceleration, thus they either introduce excessive computational cost or consider different problem settings. By considering the features of CL, we present a simple yet effective strategy, dynamic data removal (DDR), to reduce training data for further acceleration. We measure the importance of each training example by the occurrence of misclassiﬁcation (54; 66) during CL. In TDM, the sparse structure of our model updates periodically every δkepochs, so we align our data removal process with the update of weight mask for further efﬁciency and training 5stability. In Section 4.1, we have partitioned the training process for the t-th task into Nt = Kt/δk stages based on the dynamic mask update. Therefore, we gradually remove training data at the end of i-th stage, based on the following policy: 1) Calculate the total number of misclassiﬁcations fi(xj) for each training example during the i-th stage. 2) Remove a proportion of ρi training samples with the least number of misclassiﬁcations. Although our main purpose is to keep the “harder” examples to learn to consolidate the sparse model, we can get further beneﬁts for better CL result. First, the removal of “easier” examples increases the probability that “harder” examples to be saved to the rehearsal buffer, given the common strategy,e.g. reservoir sampling (13), to buffer examples. Thus, we construct a more informative buffer in a implicit way without heavy computation. Moreover, since the buffer size is much smaller than the training set size of each task, the data from the buffer and the new task is highly imbalanced, dynamic data removal also relieves the data imbalance issue. Formally, we set the data removal proportion for each task as ρ∈[0,1], and a cutoff stage, such that: cutoff∑ i=1 ρi = ρ, Nk∑ i=cutoff+1 ρi = 0 (2) The cutoff stage controls the trade-off between efﬁciency and accuracy: When we set the cutoff stage earlier, we reduce the training time for all the following stages; however, when the cutoff stage is set too early, the model might underﬁt the removed training data. Note that when we set ρi = 0for all i= 1,2,...,N t and cutoff = Nt, we simply recover the vanilla setting without any data efﬁciency considerations. In our experiments, we assume ρi = ρ/cutoff, i.e., removing equal proportion of data at the end of every stage, for simplicity. We also conduct comprehensive exploration study for ρ and the selection of the cutoff stage in Section 5.3 and Appendix D.3. 4.3 Dynamic Gradient Masking With TDM and DDR, we can already achieve bi-level efﬁciency during training. To further boost training efﬁciency, we explore sparsity in gradient and propose dynamic gradient masking (DGM) for CL. Our method focuses on reducing computational cost by only applying the most important gradients onto the corresponding unpruned model parameters via a gradient mask. The gradient mask is also dynamically updated along with the weight mask deﬁned in Section 4.1. Intuitively, while targeting for better training efﬁciency, DGM also promotes the preservation of past knowledge by preventing a fraction of weights from update. Formally, our goal here is to ﬁnd a subset of unpruned parameters (or, equivalently, a gradient mask MG) to update over multiple training iterations. For a model fθ parameterized by θ, we have the corresponding gradient matrix Gcalculated during each iteration. To prevent the pruned weights from updating, the weight mask Mθ will be applied onto the gradient matrix Gas G⊙Mθ during backpropagation. Besides the gradients of pruned weights, we in addition consider to remove less important gradients for faster training. To achieve this, we introduce the continual gradient importance (CGI) based on the CWI to measure the importance of weight gradients. CGI(w) =α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1. (3) We remove a proportion q of non-zero gradients from Gwith less importance measured by CGI and we have ∥MG∥0/∥θ∥0 = 1−(s+ q). The gradient mask MG is then applied onto the gradient matrix G. During the entire training process, the gradient mask MG is updated with a ﬁxed interval. 5 Experiment 5.1 Experiment Setting Datasets. We evaluate our SparCL on two representative CL benchmarks, Split CIFAR-10 ( 32) and Split Tiny-ImageNet (15) to verify the efﬁcacy of SparCL. In particular, we follow (8; 67) by splitting CIFAR-10 and Tiny-ImageNet into 5 and 10 tasks, each of which consists of 2 and 20 classes respectively. Dataset licensing information can be found in Appendix C. Comparing methods. In particular, we select CL methods of all kinds including regularization- based (EWC ( 31), LwF ( 36)), architecture-based (PackNet ( 41), LPS ( 56)), and rehearsal-based 6Table 1: Comparison with CL methods. SparCL consistently improves training efﬁciency of the corresponding CL methods while preserves (or even improves) accuracy on both class- and task-incremental settings. Method Sparsity Buffer size Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) Task-IL (↑) FLOPs Train×1015(↓) Class-IL (↑) Task-IL (↑) FLOPs Train×1016(↓) EWC (31) 0.00 – 19.49±0.12 68.29±3.92 8.3 7.58±0.10 19.20±0.31 13.3LwF (36) 19.61±0.05 63.29±2.35 8.3 8.46±0.22 15.85±0.58 13.3 PackNet (41) 0.50† – - 93.73 ±0.55 5.0 – 61.88 ±1.01 7.3LPS (56) - 94.50 ±0.47 5.0 – 63.37 ±0.83 7.3 A-GEM (12) 0.00 200 20.04±0.34 83.88±1.49 11.1 8.07±0.08 22.77±0.03 17.8iCaRL (49) 49.02±3.20 88.99±2.13 11.1 7.53±0.79 28.19±1.47 17.8FDR (5) 30.91±2.74 91.01±0.68 13.9 8.70±0.19 40.36±0.68 22.2ER (13) 44.79±1.86 91.19±0.94 11.1 8.49±0.16 38.17±2.00 17.8DER++ (8) 64.88±1.17 91.92±0.60 13.9 10.96±1.17 40.87±1.16 22.2 SparCL-ER75 46.89±0.68 92.02±0.72 2.0 8.98±0.38 39.14±0.85 3.2SparCL-DER++75 0.75 66.30±0.98 94.06±0.45 2.5 12.73±0.40 42.06±0.73 4.0SparCL-ER90 45.81±1.05 91.49±0.47 0.9 8.67±0.41 38.79±0.39 1.4SparCL-DER++90 0.90 200 65.79±1.33 93.73±0.24 1.1 12.27±1.06 41.17±1.31 1.8SparCL-ER95 44.59±0.23 91.07±0.64 0.5 8.43±0.09 38.20±0.46 0.8SparCL-DER++95 0.95 65.18±1.25 92.97±0.37 0.6 10.76±0.62 40.54±0.98 1.0 A-GEM (12) 0.00 500 22.67±0.57 89.48±1.45 11.1 8.06±0.04 25.33±0.49 17.8iCaRL (49) 47.55±3.95 88.22±2.62 11.1 9.38±1.53 31.55±3.27 17.8FDR (5) 28.71±3.23 93.29±0.59 13.9 10.54±0.21 49.88±0.71 22.2ER (13) 57.74±0.27 93.61±0.27 11.1 9.99±0.29 48.64±0.46 17.8DER++ (8) 72.70±1.36 93.88±0.50 13.9 19.38±1.41 51.91±0.68 22.2 SparCL-ER75 60.80±0.22 93.82±0.32 2.0 10.48±0.29 50.83±0.69 3.2SparCL-DER++75 0.75 74.09±0.84 95.19±0.34 2.5 20.75±0.88 52.19±0.43 4.0SparCL-ER90 59.34±0.97 93.33±0.10 0.9 10.12±0.53 49.46±1.22 1.4SparCL-DER++90 0.90 500 73.42±0.95 94.82±0.23 1.1 19.62±0.67 51.93±0.36 1.8SparCL-ER95 57.75±0.45 92.73±0.34 0.5 9.91±0.17 48.57±0.50 0.8SparCL-DER++95 0.95 72.14±0.78 94.39±0.15 0.6 19.01±1.32 51.26±0.78 1.0 †PackNet and LPS actually have a decreased sparsity after learning every task, we use 0.50 to roughly represent the average sparsity. (A-GEM (12), iCaRL (43), FDR (5), ER (13), DER++ (8)) methods. Note that PackNet and LPS are only compatible with task-incremental learning. We also adapt representative sparse training methods (SNIP (34), RigL (19)) to the CL setting by combining them with DER++ (SNIP-DER++, RigL-DER++). Variants of our method.To show the generality of SparCL, we combine it with DER++ (one of the SOTA CL methods), and ER (simple and widely-used) as SparCL-DER++ and SparCL-ER, respectively. We also vary the weight sparsity ratio (0.75,0.90,0.95) of SparCL for a comprehensive evaluation. Evaluation metrics.We use the average accuracy on all tasks to evaluate the performance of the ﬁnal model. Moreover, we evaluate the training FLOPs (19), and memory footprint (66) (including feature map pixels and model parameters during training) to demonstrate the efﬁciency of each method. Please see Appendix D.1 for detailed deﬁnitions of these metrics. Experiment details.For fair comparison, we strictly follow the settings in prior CL work (8; 28). We sets the per task training epochs to 50 and 100 for Split CIFAR-10 and Tiny-ImageNet, respectively, with a batch size of 32. For the model architecture, We follow (8; 49) and adopt the ResNet-18 (25) without any pre-training. We also use the best hyperparameter setting reported in ( 8; 56) for CL methods, and in (19; 34) for CL-adapted sparse training methods. For SparCL and its competing CL-adapted sparse training methods, we adopt a uniform sparsity ratio for all convolutional layers. Please see Appendix D for other details. 5.2 Main Results Comparison with CL methods.Table 1 summarizes the results on Split CIFAR-10 and Tiny- ImageNet, under both class-incremental (Class-IL) and task-incremental (Task-IL) settings. From Table 1, we can clearly tell that SparCL signiﬁcantly improves ER and DER++, while also outperforms other CL baselines, in terms of training efﬁciency (measured in FLOPs). With higher sparsity ratio, SparCL leads to less training FLOPs. Notably, SparCL achieves23×training efﬁciency improvement upon DER++ with a sparsity ratio of 0.95. On the other hand, our framework also improves the average accuracy of ER and DER++ consistently under all cases with a sparsity ratio of 0.75 and 0.90, and only slight performance drop when sparsity gets larger as 0.95. In particular, SparCL-DER++ 7Table 2: Comparison with CL-adapted sparse training methods. All methods are combined with DER++ with a 500 buffer size. SparCL outperforms all methods in both accuracy and training efﬁciency, under all sparsity ratios. All three methods here can save 20% ∼ 51% memory footprint, please see Appendix D.2 for details. Method Spasity Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) FLOPs Train ×1015(↓) Class-IL (↑) FLOPs Train ×1016(↓) DER++ (8) 0.00 72.70±1.36 13.9 19.38±1.41 22.2 SNIP-DER++ (34) 69.82±0.72 1.6 16.13±0.61 2.5RigL-DER++ (19)0.90 69.86±0.59 1.6 18.36±0.49 2.5SparCL-DER++90 73.42±0.95 1.1 19.62±0.67 1.8 SNIP-DER++ (34) 66.07±0.91 0.9 14.76±0.52 1.5RigL-DER++ (19)0.95 66.53±1.13 0.9 15.88±0.63 1.5SparCL-DER++95 72.14±0.78 0.6 19.01±1.32 1.0 Table 3: Ablation study on Split-CIFAR10 with 0.75 sparsity ratio. All components contributes to the overall performance, in terms of both accuracy and efﬁciency (training FLOPs and memory footprint). TDM DDR DGMClass-IL (↑) FLOPs Train ×1015(↓) MemoryFootprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB\u0013 \u0017 \u0017 73.37 3.6 180MB\u0013 \u0013 \u0017 73.80 2.8 180MB\u0013 \u0017 \u0013 73.97 3.3 177MB\u0013 \u0013 \u0013 74.09 2.5 177MB with 0.75 sparsity ratio sets new SOTA accuracy, with all buffer sizes under both benchmarks. The outstanding performance of SparCL indicates that our proposed strategies successfully preserve accuracy by further mitigating catastrophic forgetting with a much sparser model. Moreover, the improvement that SparCL brings to two different existing CL methods shows the generalizability of SparCL as a uniﬁed framework, i.e., it has the potential to be combined with a wide array of existing methods. We would also like to take a closer look at PackNet and LPS, which also leverage the idea of sparsity to split the model by different tasks, a different motivation from training efﬁciency. Firstly, they are only compatible with the Task-IL setting, since they leverage task identity at both training and test time. Moreover, the model sparsity of these methods reduces with the increasing number of tasks, which still leads to much larger overall training FLOPs than that of SparCL. This further demonstrates the importance of keeping a sparse model without permanent expansion throughout the CL process. Comparison with CL-adapted sparse training methods.Table 2 shows the result under the more difﬁcult Class-IL setting. SparCL outperforms all CL-adapted sparse training methods in both accuracy and training FLOPs. The performance gap between SparCL-DER++ and other methods gets larger with a higher sparsity. SNIP- and RigL-DER++ achieve training acceleration at the cost of compromised accuracy, which suggests that keeping accuracy is a non-trivial challenge for existing sparse training methods under the CL setting. SNIP generates the static initial mask after network initialization which does not consider the structure suitability among tasks. Though RigL adopts a dynamic mask, the lack of task-aware strategy prevents it from generalizing well to the CL setting. 5.3 Effectiveness of Key Components Ablation study.We provide a comprehensive ablation study in Table 3 using SparCL-DER++ with 0.75 sparsity on Split CIFAR10. Table 3 demonstrates that all components of our method contribute to both efﬁciency and accuracy improvements. Comparing row 1 and 2, we can see that the majority of FLOPs decrease results from TDM. Interestingly, TDM leads to an increase in accuracy, indicating TDM generates a sparse model that is even more suitable for learning all tasks than then full dense model. Comparing row 2 and 3, we can see that DDR indeed further accelerates training by removing less informative examples. As discussed in Section 4.2, when we remove a certain number of samples (30% here), we achieve a point where we keep as much informative samples as we need, and also balance the current and buffered data. Comparing row 2 and 4, DGM reduce both training FLOPs and memory footprint while improve the performance of the network. Finally, the last row demonstrates the collaborative performance of all components. We also show the same ablation study with 0.90 sparsity in Appendix D.4 for reference. Detail can be found in Appendix D.1. 8Figure 3: Comparison between DDR and One- shot (66) data removal strategy w.r.t. different data removal proportion ρ. DDR outperforms One-shot and also achieves better accuracy when ρ ≤ 30%. Figure 4: Comparison with CL-adapted sparse training methods in training acceleration rate and accuracy results. The radius of circles are measured by memory footprint. Exploration on DDR.To understand the inﬂuence of the data removal proportionρ, and the cutoff stage for each task, we show corresponding experiment results in Figure 3 and Appendix D.3, respectively. In Figure 3, we ﬁx cutoff = 4, i.e., gradually removing equal number of examples every 5 epochs until epoch 20, and vary ρfrom 10% to 90%. We also compare DDR with One-shot removal strategy (66), which removes all examples at once at cutoff. DDR outperforms One-shot consistently with different ρin average accuracy. Also note that since DDR removes the examples gradually before the cutoff stage, DDR is more efﬁcient than One-shot. When ρ≤30%, we also observe increased accuracy of DDR compared with the baseline without removing any data. When ρ≥40%, the accuracy gets increasingly lower for both strategies. The intuition is that when DDR removes a proper amount of data, it removes redundant information while keeps the most informative examples. Moreover, as discussed in Section 4.2, it balances the current and buffered data, while also leave informative samples in the buffer. When DDR removes too much data, it will also lose informative examples, thus the model has not learned these examples well before removal. Exploration on DGM.We test the efﬁcacy of DGM at different sparsity levels. Detailed exploratory experiments are shown in Appendix D.5 for reference. The results indicate that by setting the proportion qwithin an appropriate range, DGM can consistently improve the accuracy performance regardless of the change of weight sparsity. 5.4 Mobile Device Results The training acceleration results are measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone, which has the Qualcomm Snapdragon 865 mobile platform with a Qualcomm Kryo 585 Octa-core CPU. We run each test on a batch of 32 images to denote the training speed. The detail of on-mobile compiler-level optimizations for training acceleration can be found in Appendix E.1. The acceleration results are shown in Figure 4. SparCL can achieve approximately 3.1×and 2.3× training acceleration with 0.95 sparsity and 0.90 sparsity, respectively. Besides, our framework can also save 51% and 48% memory footprint when the sparsity is 0.95 and 0.90. Furthermore, the obtained sparse models save the storage consumption by using compressed sparse row (CSR) storage and can be accelerated to speed up the inference on-the-edge. We provide on-mobile inference acceleration results in Appendix E.2. 6 Conclusion This paper presents a uniﬁed framework named SparCL for efﬁcient CL that achieves both learning acceleration and accuracy preservation. It comprises three complementary strategies: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. Extensive experiments on standard CL benchmarks and real-world edge device evaluations demonstrate that our method signiﬁcantly improves upon existing CL methods and outperforms CL-adapted sparse training methods. We discuss the limitations and potential negative social impacts of our method in Appendix A and B, respectively. 9References [1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In CVPR, 2021. 4 [2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte- laars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. 1, 3 [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. NeurIPS, 2019. 5 [4] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. In ICLR, 2018. 2, 3 [5] Ari S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in function space. arXiv preprint arXiv:1805.08289, 2018. 7 [6] Zalán Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. NeurIPS, 2020. 5 [7] Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garﬁnkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artiﬁcial intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018. 15 [8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In NeurIPS, 2020. 1, 2, 3, 6, 7, 8 [9] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678, 2016. 16 [10] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In ICCV, 2021. 3 [11] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip Torr, and David Lopez-Paz. Us- ing hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 2(7), 2020. 1 [12] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018. 3, 7 [13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019. 3, 6, 7 [14] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Long live the lottery: The existence of winning tickets in lifelong learning. In ICLR, 2020. 3 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR. Ieee, 2009. 2, 6 [16] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019. 3 [17] Peiyan Dong, Siyue Wang, Wei Niu, Chengming Zhang, Sheng Lin, Zhengang Li, Yifan Gong, Bin Ren, Xue Lin, and Dingwen Tao. Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition. In DAC, pages 1–6. IEEE, 2020. 17 [18] Xuanyi Dong and Yi Yang. Network pruning via transformable architecture search. In NeurIPS, pages 759–770, 2019. 1 [19] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In ICML, pages 2943–2952. PMLR, 2020. 2, 3, 4, 7, 8, 16 10[20] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2019. 3 [21] Yifan Gong, Geng Yuan, Zheng Zhan, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia Liu, Bin Ren, Xue Lin, et al. Automatic mapping of the best-suited dnn pruning schemes for real-time mobile acceleration. ACM Transactions on Design Automation of Electronic Systems (TODAES), 27(5):1–26, 2022. 18 [22] Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, et al. A privacy-preserving-oriented dnn pruning and mobile acceleration framework. In GLSVLSI, pages 119–124, 2020. 17 [23] Song Han, Jeff Pool, et al. Learning both weights and connections for efﬁcient neural network. In NeurIPS, pages 1135–1143, 2015. 3 [24] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efﬁcient experience replay for streaming learning. In ICRA, 2019. 3 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 7 [26] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. 17 [27] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In CVPR, pages 4340–4349, 2019. 17 [28] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating con- tinual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. 7 [29] Tong Jian, Yifan Gong, Zheng Zhan, Runbin Shi, Nasim Soltani, Zifeng Wang, Jennifer G Dy, Kaushik Roy Chowdhury, Yanzhi Wang, and Stratis Ioannidis. Radio frequency ﬁngerprinting on the edge. IEEE Transactions on Mobile Computing, 2021. 17 [30] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar and dissimilar tasks. NeurIPS, 2020. 1 [31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017. 1, 3, 6, 7 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf, 2009. 6, 15 [33] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 15 [34] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. ICLR, 2019. 2, 3, 7, 8 [35] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing convolutional neural networks via factorized convolutional ﬁlters. In CVPR, pages 3977–3986, 2019. 17 [36] Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 40(12):2935–2947, 2017. 1, 3, 6, 7 [37] Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, and Yanzhi Wang. Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execution on mobile devices. In AAAI, pages 5117–5124, 2020. 17 11[38] Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie Wen, Xiang Chen, Jian Tang, Kaisheng Ma, et al. An image enhancing pattern-based sparsity for real-time inference on mobile devices. In ECCV, pages 629–645. Springer, 2020. 1 [39] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classiﬁcation: An empirical survey. arXiv preprint arXiv:2101.10423, 2021. 3 [40] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 67–82, 2018. 3 [41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR, 2018. 1, 3, 6, 7 [42] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989. 1 [43] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. ICML Workshop, 2021. 7 [44] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018. 3, 4 [45] Hesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural networks by dynamic sparse reparameterization. In ICML, pages 4646–4655. PMLR, 2019. 3 [46] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight pruning. arXiv preprint arXiv:2001.00138, 2020. 18 [47] Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Grafﬁeti, and Davide Maltoni. Con- tinual learning at the edge: Real-time training on smartphone devices. arXiv preprint arXiv:2105.13127, 2021. 1 [48] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. NeurIPS, 2021. 3 [49] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In CVPR, pages 2001–2010, 2017. 1, 3, 7 [50] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. 1, 3 [51] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, 2018. 1 [52] Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Spacenet: Make free space for continual learning. Neurocomputing, 439:1–11, 2021. 3 [53] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. NeurIPS, 33:6377–6389, 2020. 3 [54] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. 5 [55] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient ﬂow. In ICLR, 2019. 3 [56] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis Ioannidis. Learn-prune-share for lifelong learning. In ICDM, 2020. 1, 3, 6, 7 12[57] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. ECCV, 2022. 1 [58] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to prompt for continual learning.CVPR, 2022. 1, 3 [59] Paul Wimmer, Jens Mehnert, and Alexandru Condurache. Freezenet: Full performance by reduced storage costs. In ACCV, 2020. 3 [60] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In CVPR, pages 374–382, 2019. 1, 3 [61] Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, and Yanzhi Wang. Compiler-aware neural architecture search for on-mobile real-time super-resolution. ECCV, 2022. 18 [62] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In CVPR, pages 3014–3023, 2021. 3 [63] Li Yang, Sen Lin, Junshan Zhang, and Deliang Fan. Grown: Grow only when necessary for continual learning. arXiv preprint arXiv:2110.00908, 2021. 1 [64] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang. Online coreset selection for rehearsal-based continual learning. arXiv preprint arXiv:2106.01085, 2021. 5 [65] Kun-Hsing Yu, Andrew L Beam, and Isaac S Kohane. Artiﬁcial intelligence in healthcare. Nature biomedical engineering, 2(10):719–731, 2018. 15 [66] Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse training framework on the edge. NeurIPS, 34, 2021. 3, 4, 5, 7, 9, 16, 17 [67] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 1, 3, 6 [68] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, et al. Achieving on-mobile real-time super-resolution with neural architecture and pruning search. In ICCV, pages 4821–4831, 2021. 17 [69] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised lifelong learning. Neural Networks, 149:95–106, 2022. 1 Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] The claims match the experimental results and it is expected to generalize according to the diverse experiments stated in our paper. We include all of our code, data, and models in the supplementary materials, which can reproduce our experimental results. (b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6 and Appendix B. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read the ethics review guidelines and ensured that our paper conforms to them. 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 13(b) Did you include complete proofs of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] See Section 5.1, Section 5.4 and we provide code to reproduce the main experimental results. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5.1 and Section 5.4. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See Table 1, Table 2, ﬁg 1, ﬁg 3. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.1, Section 5.4. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We mentioned and cited the datasets (Split CIFAR-10 and Tiny-ImageNet), and all comparing methods with their paper and github in it. (b) Did you mention the license of the assets? [Yes] The licences of used datasets/models are provided in the cited references and we state them explicitly in Appendix C. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We provide code for our proposed method in the supplement. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Limitations One limitation of our method is that we assume a rehearsal buffer is available throughout the CL process. Although the assumption is widely-accepted, there are still situations that a rehearsal buffer is not allowed. However, as a framework targeting for efﬁciency, our work has the potential to accelerate all types of CL methods. For example, simply removing the terms related to rehearsal buffer in equation 1 and equation 3 could serve as a naive variation of our method that is compatible with other non-rehearsal methods. It is interesting to further improve SparCL to be more generic for all kinds of CL methods. Moreover, the benchmarks we use are limited to vision domain. Although using vision-based benchmarks has been a common practice in the CL community, we believe evaluating our method, as well as other CL methods, on datasets from other domains such as NLP will lead to a more comprehensive and reliable conclusion. We will keep track of newer CL benchmarks from different domains and further improve our work correspondingly. B Potential Negative Societal Impact Although SparCL is a general framework to enhance efﬁciency for various CL methods, we still need to be aware of its potential negative societal impact. For example, we need to be very careful about the trade-off between accuracy and efﬁciency when using SparCL. If one would like to pursue efﬁciency by setting the sparsity ratio too high, then even SparCL will result in signiﬁcant accuracy drop, since the over-sparsiﬁed model does not have enough representation power. Thus, we should pay much attention when applying SparCL on accuracy-sensitive applications such as healthcare (65). Another example is that, SparCL as a powerful tool to make CL methods efﬁcient, can also strengthen models for malicious applications ( 7). Therefore, we encourage the community to come up with more strategies and regulations to prevent malicious use of artiﬁcial intelligence. C Dataset Licensing Information • CIFAR-10 (32) is licensed under the MIT license. • The licensing information of Tiny-ImageNet ( 33) is not available. However, the data is available for free to researchers for non-commercial use. D Additional Experiment Details and Results We set α = 0.5,β = 1 in equation 1 and equation 3. We also set δk = 5, pinter = 0.01, pintra = 0.005. We also match different weight sparsity with gradient sparsity for best performance. We sample 20% data from Split CIFAR-10 training set for validation, and we use grid-search on this validation set to help us select the mentioned best hyperparameters. We use the same set of hyperparameters for both datasets. For accurate evaluation, we repeat each experiments 3 times using different random seeds and report the average performance. During our experiments, we adopt unstructured sparsity type and uniform sparsity ratio (0.75,0.90,0.95) for all convolutional layers in the models. D.1 Evaluation Metrics Explanation Training FLOPsThe FLOPs of a single forward pass is calculated by taking the sum of the number of multiplications and additions in each layer lfor a given layer sparsity sl. Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. The goal of the forward pass is to calculate the loss of the current set of parameters on a given batch of data. It can be formulated as al = σ(zl) =σ(wl ∗al−1 + bl) for each layer lin the model. Here, w, b, and zrepresent the weights, biases, and output before activation, respectively; σ(.) denotes the activation function; ais the activations; ∗means convolution operation. The formulation indicates that the layer activations are calculated in sequence using the previous activations and the parameters of the layer. Activation of layers are stored in memory for the backward pass. 15As for the backward propogation, the objective is to back-propagate the error signal while calculating the gradients of the parameters. The two main calculation steps can be represented as: δl = δl+1 ∗rotate180°(wl) ⊙σ′(zl), (4) Gl = al−1 ∗δl, (5) where δl is the error associated with the layer l, Gl denotes the gradients, ⊙represents Hadamard product, σ′(.) denotes the derivative of activation, androtate180°(.) means rotating the matrix by180° is the matrix transpose operation. During the backward pass, each layer lcalculates two quantities, i.e., the gradient of the activations of the previous layer and the gradient of its parameters. Thus, the backward passes are counted astwice the computation expenses of the forward pass (19). We omit the FLOPs needed for batch normalization and cross entropy. In our work, the total FLOPs introduced by TDM, DDR, and DGM on split CIFAR-10 is approximately 4.5 ×109 which is less than 0.0001% of total training FLOPs. For split Tiny-ImageNet, the total FLOPs of them is approximately 1.8 ×1010, which is also less than 0.0001% of total training FLOPs. Therefore, the computation introduced by TDM, DDR, and DGM is negligible. Memory FootprintsFollowing works ( 9; 66), the deﬁnition of memory footprints contain two parts: 1) activations (feature map pixels) during training phase, and 2) model parameters during training phase. For experiments, activations, model weights, and gradients are stored in 32-bit ﬂoating-point format for training. The memory footprint results are calculated with an approximate summation of them. D.2 Details of Memory Footprint The memory footprint is composed of three parts: activations, model weights, and gradients. They are all represented as bw-bit numbers for training. The number of activations in the model is the sum of the activations in each layer. Suppose that the output feature of the l-th layer with a batch size of Bis represented as al ∈RB×Ol×Hl×Wl , where Ol is the number of channels and Hl ×Wl is the feature size. The total number of activations of the model is thus B∑ lOlHlWl. As for the model weights, our SparCL training a sparse model with a sparsity ratio s∈[0,1] from scratch. The sparse model is obtained from a dense model with a total of N weights. A higher value of sindicates fewer non-zero weights in the sparse model. Compressed sparse row (CSR) format is commonly used for sparse storage, which greatly reduces the number of indices need to be stored for sparse matrices. As our SparCL adopt only one sparsity type and we use a low-bit format to store the indices, we omit the indices storage here. Therefore, the memory footprint for model representation is (1 −s)Nbw. Similar calculations can be applied for the gradient matrix. Besides the sparsity ratio s, additional q gradients are masked out from the gradient matrix, resulting a sparsity ratio s+ q. Therefore, the storage of gradients can be approximated as (1 −(s+ q))Nbw. Combining the activations, model representation, and gradients, the total memory footprint in SparCL can be represented as (2B∑ lOlHlWl + (1−s)N + (1−(s+ q))N)bw. DDR requires store indices for the easier examples during the training process. The number of training examples for Split CIFAR-10 and Split Tiny-ImageNet on each task is 10000. In our work, we only need about 3KB (remove 30% training data) for indices storage (in the int8 format) and the memory cost is negligible compared with the total memory footprint. D.3 Effect of Cutoff Stage Table A1: Effect of cutoff. cutoff 1 2 3 4 5 6 7 8 9 Class-IL (↑) 71.54 72.38 72.74 73.20 73.10 73.32 73.27 73.08 73.23 To evaluate the effect of the cutoff stage, we use the same setting as in Figure 3 by setting the sparsity ratio to 0.90. We keep the data removal proportion ρ = 30%, and only change cutoff. 16Table A1 shows the relationship between cutoff and the Class-IL average accuracy. Note that from the perspective of efﬁciency, we would like thecutoff stage as early as possible, so that the remaining epochs will have less examples. However, from Table A1, we can see that if we set it too early, i.e., cutoff ≤3, the accuracy drop is signiﬁcant. This indicate that even for the “easy-to-learn” examples, removing them too early results in underﬁtting. As a balance point between accuracy and efﬁciency, we choose cutoff = 4in our ﬁnal version. D.4 Supplementary Ablation Study Table A2: Ablation study on Split-CIFAR10 with 0.90 sparsity. TDM DDR DGM Class-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB \u0013 \u0017 \u0017 72.98 1.6 166MB \u0013 \u0013 \u0017 73.20 1.2 166MB \u0013 \u0017 \u0013 73.30 1.5 165MB \u0013 \u0013 \u0013 73.42 1.1 165MB Similar to Table 3, we show ablation study with 0.90 sparsity ratio in Table A2. Under a larger sparsity ratio, the conclusion that all components contribute to the ﬁnal performance still holds. However, we can observe that the accuracy increase that comes from DDR and DGM is less than what we show in Table 3. We assume that larger sparsity ratio makes it more difﬁcult for the model to retain good accuracy in CL. Similar results has also been observed in (66) under the usual i.i.d. learning setting. D.5 Exploration on DGM Table A3: Ablation study of the gradient sparsity ratio on Split-CIFAR10. weight sparsity gradient sparsityClass-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) 0.75 0.78 74.08 3.4 178MB 0.75 0.80 73.97 3.3 177MB 0.75 0.82 73.79 3.3 177MB 0.75 0.84 73.26 3.2 176MB 0.90 0.91 73.33 1.6 166MB 0.90 0.92 73.30 1.5 165MB 0.90 0.93 72.64 1.5 165MB We conduct further experiments to demonstrate the inﬂuence of gradient sparsity, and the results are shown in Table A3. There are two sets of the experiments with different weight sparsity settings: 0.75 and 0.90. Within each set of the experiments (the weight sparsity is ﬁxed), we vary the gradient sparsity values. From the results we can see that increasing the gradient sparsity can decrease the FLOPs and memory footprint. However, the accuracy performance degrades more obvious when the gradient sparsity is too much for the weight sparsity. The results indicate that suitable gradient sparsity setting can bring further efﬁciency to the training process while boosting the accuracy performance. In the main results, the gradient sparsity is set as 0.80 for 0.75 weight sparsity, and set as 0.92 for 0.90 weight sparsity. E On-Mobile Compiler Optimizations and Inference Results E.1 Compiler Optimizations Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. Prior works (17; 22; 26; 27; 29; 35; 37; 68) have proved that sparse weight matrices (tensors) can provide inference acceleration via reducing the number of multiplications in convolution operation. Therefore, the forward propagation phase, which is the same as inference, 17can be accelerated by the sparsity inherently. As for backward pass, both of the calculation steps are based on convolution, i.e., matrix multiplication. Equation 4 uses sparse weight matrix (tensor) as the operand, thus can be accelerated in the same way as the forward propagation. Equation 5 allows a sparse output result since the gradient matrix is also sparse. Thus, both two steps have reduced computations, which are roughly proportional to the sparsity ratio, providing the acceleration for the backward propagation phase. Compiler optimizations are used to accelerate the inference in prior works (21; 46; 61). In this work, we extend the compiler optimization techniques for accelerating the forward and backward pass during training on the edge devices. Our compiler optimizations are general, support both sparse model training and inference accelerations on mobile platforms. The optimizations include 1) the supports for sparse models; 2) an auto-tuning process to determine the best-suited conﬁgurations of parameters for different mobile CPUs. The details of our compiler optimizations are presented as follows. E.1.1 Supports for Sparse Models Our framework supports sparse model training and inference accelerations with unstructured pruning. For the sparse (pruned) model, the framework ﬁrst compacts the model storage with a compression format called Compressed Sparse Row (CSR) format, and then performs computation reordering to reduce the branches within each thread and eliminates the load imbalance among threads. A row reordering optimization is also included to further improve the regularity of the weight matrix. After this reordering, the continuous rows with identical or similar numbers of non-zero weights are processed by multi-threads simultaneously, thus eliminating thread divergence and achieving load balance. Each thread processes more than one rows, thus eliminating branches and improving instruction-level parallelism. Moreover, a similar optimization ﬂow (i.e., model compaction and computation reorder and other optimizations) is employed to support all compiler optimizations for sparsity as PatDNN (46). E.1.2 Auto-Tuning for Different Mobile CPUs During DNN sparse training and inference execution, there are many tuning parameters, e.g., matrix tiling sizes, loop unrolling factors, and data placement on memory, that inﬂuence the performance. It is hard to determine the best-suited conﬁguration of these parameters manually. To alleviate this problem, our compiler incorporates an auto-tuning approach for sparse (pruned) models. The Genetic Algorithm is leveraged to explore the best-suited conﬁgurations automatically. It starts the parameter search process with an arbitrary number of chromosomes and explores the parallelism better. Acceleration codes for different DNN models and different mobile CPUs can be generated efﬁciently and quickly through this auto-tuning process. E.2 Inference Acceleration Results On Mobile 60 12 18 2 4 A ccelation R esult s of R esNet -18 on  Split CIF AR-10 0 . 00 0 . 7 5 0 . 90 0 . 95 100 20 30 40 A ccelation R esult s of R esNet -18 on Split Tin y-ImageNet 0 . 00 0 . 7 5 0 . 90 0 . 95 Figure 5: Inference results of sparse models obtained from SparCL under different sparsity ratio compared with dense models obtained from traditional CL methods (sparsity ratio 0.00). Besides accelerating the training process, SparCL also possesses the advantages of providing a sparse model as the output for faster inference. To demonstrate this, we show the inference acceleration results of SparCL with different sparsity ratio settings on mobile in Figure 5. The inference time is measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone. Each test takes 50 runs on different inputs with 8 threads on CPU. As different runs do not vary greatly, only the average time is reported. From the results we can see that the obtained sparse model from SparCL can signiﬁcantly accelerate the inference on both Split-CIFAR-10 and Tiny-ImageNet dataset compared to the model obtained by traditional CL training. For ResNet-18 on Split-CIFAR-10, the model obtained by traditional CL training, which is a dense model, takes 18.53ms for inference. The model provided by SparCL can achieve an inference time of 14.01ms, 8.30ms, and 5.85ms with sparsity 18ratio of 0.75, 0.90, and 0.95, respectively. The inference latency of the dense ResNet-18 obtained by traditional CL training on Tiny-ImageNet is 39.64 ms. While the sparse models provided by SparCL with sparsity ratio settings as 0.75, 0.90, and 0.95 reach inference speed of 33.06ms, 20.37ms, and 15.49ms, respectively, on Tiny-ImageNet. 19",
      "meta_data": {
        "arxiv_id": "2209.09476v1",
        "authors": [
          "Zifeng Wang",
          "Zheng Zhan",
          "Yifan Gong",
          "Geng Yuan",
          "Wei Niu",
          "Tong Jian",
          "Bin Ren",
          "Stratis Ioannidis",
          "Yanzhi Wang",
          "Jennifer Dy"
        ],
        "published_date": "2022-09-20T05:24:48Z",
        "pdf_url": "https://arxiv.org/pdf/2209.09476v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes SparCL, a novel framework for Sparse Continual Learning (CL) that addresses the under-investigated problem of training efficiency in CL systems, especially for resource-limited edge devices. It is the first study to leverage sparsity for cost-effective CL on edge devices. SparCL achieves significant training acceleration (up to 23x less training FLOPs) while preserving and even improving state-of-the-art accuracy (up to 1.7% improvement). It demonstrates superior performance compared to both conventional CL methods and CL-adapted sparse training methods, and its practical potential is shown through evaluation on a real mobile edge device, achieving up to 3.1x training acceleration.",
        "methodology": "SparCL combines three complementary components for weight sparsity, data efficiency, and gradient sparsity. 1) Task-aware Dynamic Masking (TDM): maintains a dynamic sparse network throughout the CL process by dynamically removing less important weights and growing back unused weights. It uses a Continual Weight Importance (CWI) metric, considering weight magnitude, importance for current tasks, and importance for past data, with different intra-task (shrink-and-expand) and inter-task (expand-and-shrink) adjustment strategies. 2) Dynamic Data Removal (DDR): progressively removes less informative training examples by measuring the occurrence of misclassifications, aligning with the weight mask updates. This reduces training data, accelerates the process, and improves accuracy by implicitly constructing a more informative rehearsal buffer and relieving data imbalance. 3) Dynamic Gradient Masking (DGM): further boosts efficiency by only applying the most important gradients to unpruned parameters via a gradient mask, based on a Continual Gradient Importance (CGI) metric. This also helps preserve past knowledge by preventing a fraction of weights from updating.",
        "experimental_setup": "The effectiveness of SparCL is evaluated on two representative CL benchmarks: Split CIFAR-10 (5 tasks, 2 classes each) and Split Tiny-ImageNet (10 tasks, 20 classes each). The model architecture used is ResNet-18 without pre-training. SparCL is compared against a variety of CL methods including regularization-based (EWC, LwF), architecture-based (PackNet, LPS), and rehearsal-based (A-GEM, iCaRL, FDR, ER, DER++). It also competes against baselines adapted from state-of-the-art sparse training methods (SNIP, RigL) combined with DER++. Evaluation metrics include average test accuracy (Class-IL and Task-IL), training FLOPs, and memory footprint. Experiments are repeated 3 times with different random seeds. Real-world evaluation is performed on the CPU of a Samsung Galaxy S20 smartphone to measure training acceleration.",
        "limitations": "One limitation is the assumption of a rehearsal buffer being available throughout the CL process. While widely accepted, it restricts applicability to scenarios where a buffer is not allowed. The benchmarks used are limited to the vision domain; evaluating on datasets from other domains like NLP would provide a more comprehensive conclusion. Additionally, there's a critical trade-off between accuracy and efficiency; setting the sparsity ratio too high can lead to significant accuracy degradation due to insufficient representation power, which is crucial for accuracy-sensitive applications like healthcare. The framework could also potentially strengthen models for malicious applications.",
        "future_research_directions": "Future research could focus on improving SparCL to be more generic and compatible with non-rehearsal-based CL methods, for instance, by modifying the importance metrics (equations 1 and 3) to remove rehearsal buffer-related terms. Expanding evaluations to new CL benchmarks from diverse domains like Natural Language Processing (NLP) is also suggested for a more comprehensive and reliable assessment. The paper also encourages further research on CL on-the-edge, building upon SparCL's demonstrated practical potential."
      }
    },
    {
      "title": "MECTA: Memory-Economic Continual Test-Time Model Adaptation"
    },
    {
      "title": "Improving Task-free Continual Learning by Distributionally Robust Memory Evolution",
      "abstract": "Task-free continual learning (CL) aims to learn a non-stationary data stream\nwithout explicit task definitions and not forget previous knowledge. The widely\nadopted memory replay approach could gradually become less effective for long\ndata streams, as the model may memorize the stored examples and overfit the\nmemory buffer. Second, existing methods overlook the high uncertainty in the\nmemory data distribution since there is a big gap between the memory data\ndistribution and the distribution of all the previous data examples. To address\nthese problems, for the first time, we propose a principled memory evolution\nframework to dynamically evolve the memory data distribution by making the\nmemory buffer gradually harder to be memorized with distributionally robust\noptimization (DRO). We then derive a family of methods to evolve the memory\nbuffer data in the continuous probability measure space with Wasserstein\ngradient flow (WGF). The proposed DRO is w.r.t the worst-case evolved memory\ndata distribution, thus guarantees the model performance and learns\nsignificantly more robust features than existing memory-replay-based methods.\nExtensive experiments on existing benchmarks demonstrate the effectiveness of\nthe proposed methods for alleviating forgetting. As a by-product of the\nproposed framework, our method is more robust to adversarial examples than\nexisting task-free CL methods. Code is available on GitHub\n\\url{https://github.com/joey-wang123/DRO-Task-free}",
      "full_text": "Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Zhenyi Wang1 Li Shen 2 Le Fang 1 Qiuling Suo 1 Tiehang Duan 3 Mingchen Gao 1 Abstract Task-free continual learning (CL) aims to learn a non-stationary data stream without explicit task deﬁnitions and not forget previous knowledge. The widely adopted memory replay approach could gradually become less effective for long data streams, as the model may memorize the stored examples and overﬁt the memory buffer. Second, existing methods overlook the high un- certainty in the memory data distribution since there is a big gap between the memory data dis- tribution and the distribution of all the previous data examples. To address these problems, for the ﬁrst time, we propose a principled memory evolution framework to dynamically evolve the memory data distribution by making the mem- ory buffer gradually harder to be memorized with distributionally robust optimization (DRO). We then derive a family of methods to evolve the memory buffer data in the continuous probabil- ity measure space with Wasserstein gradient ﬂow (WGF). The proposed DRO is w.r.t the worst-case evolved memory data distribution, thus guarantees the model performance and learns signiﬁcantly more robust features than existing memory-replay- based methods. Extensive experiments on ex- isting benchmarks demonstrate the effectiveness of the proposed methods for alleviating forget- ting. As a by-product of the proposed framework, our method is more robust to adversarial exam- ples than existing task-free CL methods. Code is available on GitHub https://github.com/ joey-wang123/DRO-Task-free 1Department of Computer Science and Engineering, Univer- sity at Buffalo, NY , USA2JD Explore Academy, Beijing, China 3Meta, Seattle, WA, USA. Correspondence to: Zhenyi Wang <zhenyiwa@buffalo.edu>, Li Shen <mathshenli@gmail.com>, Mingchen Gao <mgao8@buffalo.edu>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1. Introduction Continual learning (CL) is to learn on a sequence of tasks without forgetting previous ones. Most CL methods assume knowing task identities and boundaries during training. In- stead, this work focuses on a more general and challeng- ing setup, i.e., task-free continual learning (Aljundi et al., 2019b). This learning scenario does not assume explicit task deﬁnition, and data distribution gradually evolves with- out clear task boundaries, making it applicable to a broader range of real-world problems. The current widely adopted memory replay approach stores a small portion of previous tasks in memory and replays them with the new mini-batch data. The CL model would overﬁt the memory buffer, and this approach could be grad- ually less effective for mitigating forgetting as the model repeatedly learns the memory buffer (Jin et al., 2021), il- lustrated in Figure 1 (a). The model could memorize the memory buffer, and previous knowledge will be quickly lost. Furthermore, there is a big gap between memory data distribution and the distribution of all the previous data ex- amples. These methods ignore the high uncertainty of the memory data distribution since a limited memory buffer cannot accurately reﬂect the stationary distribution of all examples seen so far in the data stream, illustrated in Figure 2 (a). Most existing CL works ignore this issue. To address the above issues, we propose a distributionally robust optimization framework (DRO) to evolve the memory data distribution dynamically. This learning objective makes the memory data increasingly harder to be memorized and helps the model alleviate memory overﬁtting and improve generalization, illustrated in Figure 1 (b) and (c). In addition, the proposed DRO framework considers the underlying high uncertainty of memory data distribution. It evolves memory data distribution to ﬁll the gap between the memory data distribution and the ideal stationary distribution of all the previous data, illustrated in Figure 2 (b). We optimize the model performance under the worst-case evolved memory data distribution since we cannot know the exact distribu- tion of all the previous examples. The model can thus learn substantially more robust features with performance guaran- tees than previous work. For the image domain, adversarial examples can have the same appearance as natural images arXiv:2207.07256v2  [cs.LG]  20 Aug 2022Improving Task-free Continual Learning by Distributionally Robust Memory Evolution but have different classiﬁcation results. Our proposed DRO memory evolution framework is robust to such adversarial examples due to the worst-case performance optimization on the evolved memory data distribution. (a) t= t0 (easy to memorize and overﬁt) (b) t= t1 (diverse and hard) (c) t= t2 (diverse and harder to classify) Figure 1.T-SNE visualization of evolved memory embedded by ResNet18 on CIFAR10 at different CL timestamps. Each dot repre- sents a data point’s feature extracted by the last layer of ResNet18. Each color denotes one class of memory data. Initially, the classes are very easy to memorize and overﬁt. Memory evolution makes the memory more diverse and harder to classify and memorize. (a) Experience replay (raw memory data) (b) Our method (evolved memory data) Figure 2.The blue line denotes the stationary distribution of all the previous data, and the red line denotes the memory buffer data distribution. (a): Standard experience replay (ER) (left) replays on the raw memory data. There is a big gap between the raw memory data distribution µ0 and the stationary distribution of all the data in the data stream ν. (b): Our method (right) ﬁlls this gap by evolving the memory data distribution to narrow the gap between the evolved memory data distribution µT and ν. Formally, we ﬁrst design energy functional that measures the degree of forgetting on a speciﬁc memory data distribu- tion and the distributional distance between the evolved and raw memory distribution. The goal is to minimize the energy functional F(µ) for the worse-case performance probability measure (density) πthat is within the neighbor probability measures of the raw memory data probability measure µ0. We name this optimization as task-free DRO. However, it is extremely challenging to solve this optimization problem in an inﬁnite-dimensional probability measure space (function space) which contains an inﬁnite number of probability mea- sures. To efﬁciently solve the task-free DRO, we formulate it from a new continuous dynamics perspective and convert it into a gradient ﬂow system. Speciﬁcally, the memory data distribution evolves in probability measure space as Wasserstein gradient ﬂow (WGF), and model parameters evolve in Euclidean space. We name it as Dynamic DRO, which makes it convenient to use function gradient-descent in probability measure space to solve the task-free DRO. Also, it facilitates the derivation of different methods for memory data distribution evolution. We then introduce three speciﬁc memory evolution methods to solve the Dynamic DRO, including Langevin Dynamics (Welling & Teh, 2011), Stein Variational Gradient Descent (Liu & Wang, 2016), and Hamiltonian ﬂow (Ma et al., 2015; Liu et al., 2019). The proposed memory evolution framework is general, ﬂexible, and easily extendable, with many potential extensions for fu- ture research. We evaluate the effectiveness of the proposed framework by performing comprehensive experiments on several datasets and comparing them to various strong base- lines. We summarize our contributions as follows: • We propose the ﬁrst principled, general, and ﬂexible memory evolution framework for task-free CL from the perspective of distributionally robust optimization, named task-free DRO. Our proposed method is sub- stantially more effective for mitigating forgetting. As a by-product of the proposed DRO framework, our method is more robust to adversarial examples than previous works. • We formulate the task-free DRO from a new contin- uous dynamics perspective and cast it as a gradient ﬂow system, named Dynamic DRO. We propose a fam- ily of memory evolution methods with different ways to efﬁciently solve the Dynamic DRO, opening up a new research direction for presenting new strategies to evolve the memory data. • Extensive experiments on several datasets demonstrate the effectiveness of the proposed method for reducing forgetting and increasing the robustness to adversarial examples. Our framework is versatile and can be seam- lessly integrated with existing memory-replay-based methods to improve their performance. 2. Related Work Continual learning (CL) aims to maintain previous knowledge when learning on sequential tasks with data distribution shift. Most existing CL methods (Lopez-Paz & Ranzato, 2017; Nguyen et al., 2018; Kirkpatrick et al., 2017; Zenke et al., 2017; von Oswald et al., 2019; Wang et al., 2021; Saha et al., 2021; Pham et al., 2021; Wang et al., 2022) are only applicable to the task-aware setting, where there are clear task deﬁnitions and boundaries among the sequentially learned task sequences. Task-free continual learning (He et al., 2019; Zeno et al., 2019; Aljundi et al., 2019b; Chrysakis & Moens, 2020) instead focuses on the more general case where the data distribution could changeImproving Task-free Continual Learning by Distributionally Robust Memory Evolution arbitrarily without explicit task splits. This learning sce- nario has become increasingly important due to the broader application scenarios, and more challenging problem nature (Aljundi et al., 2019b; Lee et al., 2020). Task-free CL Existing approaches for task-free CL can be categorized into two classes. The ﬁrst (majority) one is memory-based methods (Aljundi et al., 2019c;a), which store a small number of data from the previous data stream and replay them with the new mini-batch data later. The sec- ond type is the expansion-based method, such as CN-DPM (Lee et al., 2020). However, CN-DPM needs to increase the memory and computation overhead with the network structure’s expansion and the increase of the network param- eters. Hence, this work focuses on memory-replay-based methods without expanding the network structure since it is simple and effective. Most existing works in this category (Chaudhry et al., 2019b;a) directly perform replay on the raw data without any adaptation. MIR (Aljundi et al., 2019a) pro- poses to replay the samples with which are most interfered. GEN-MIR (Aljundi et al., 2019a) further uses generative models to synthesize the memory examples. The heuristic method, GMED (Jin et al., 2021), proposes to edit memory examples so that the examples are more likely forgotten and harder to be memorized. Our methods share similar moti- vations with GMED, which individually edits the memory data without considering memory data distribution uncer- tainty and population-level statistics. In contrast, our DRO framework focuses on population-level and distribution- level evolution. Orthogonal to our work, Gradient-based Sample Selection (GSS) (Aljundi et al., 2019c) focuses on storing diverse examples. These methods lack theoretical guarantees. In contrast, our framework is principled and focuses on evolving memory data distributions. Distributionally robust optimization (DRO) is an effec- tive optimization framework to handle decision-making un- der uncertainty (Rahimian & Mehrotra, 2019). The basic idea of DRO is ﬁrst to construct a set of probability distri- butions as an ambiguity set and minimize the worst-case performance in this ambiguity set, thus guaranteeing the model performance. There are various applications of DRO in machine learning problems, including tackling the group- shift (Sagawa et al., 2020), subpopulation shift (Zhai et al., 2021), and class imbalances (Xu et al., 2020). To our best knowledge, our work is the ﬁrst principled method with DRO for memory evolution in task-free CL, named task-free DRO. It dynamically evolves memory data distributions to avoid forgetting and learn robust features. In addition, we formulate the proposed task-free DRO from a new continuous dynamics perspective, making it convenient to handle the evolving memory data distribution with differ- ent evolution dynamics. Besides, we propose three ways to evolve the memory data, opening up a new research direc- tion to explore more effective memory evolution methods. 3. Method In this section, we ﬁrst present the problem setup in Section 3.1 for the task-free CL setting. Then, we propose our task- free DRO framework in Section 3.2. Next, we view the task-free DRO from a new continuous dynamics perspective and formulate the task-DRO in an equivalent gradient ﬂow system, named Dynamic DRO, in Section 3.3. In Section 3.4, we present three speciﬁc memory evolution methods to solve the Dynamic DRO efﬁciently. 3.1. Problem Setup A sequence of mini-batch labeled data (xk,yk,hk) sequen- tially arrives at each timestampkand forms a non-stationary data stream. Each data point is associated with a latent task identity hk, where xk denotes the mini-batch data received at timestamp k, yk is the data label associated with xk. Ac- cording to (Aljundi et al., 2019b), a more general deﬁnition of task-free CL is that data distribution shift could happen at any time without explicit task splits. Our method can be directly applied to those more general cases as well. Dur- ing both the training and testing time, the task identity hk is not available to the learner. At the same time, a small memory buffer Mis maintained, and replay the data in M when learning the new task to avoid forgetting the previ- ously learned knowledge. The memory bufferMis updated by reservoir sampling (RS), similar to (Riemer et al., 2019). The goal is to learn a model f(x,θ) to perform well on all the tasks seen until timestamp k. Standard memory replay for CL (Chaudhry et al., 2019b) is to optimize an objective under a known probability distribution µ0. Formally speak- ing, CL with standard memory replay can be expressed as: min ∀θ∈Θ [L(θ,xk,yk) + E x∼µ0 L(θ,x,y)], (1) where θare model parameters and xis the raw memory buffer data with probability measure (density)µ0, i.e., ∀x∈ M,x∼µ0. L(θ,x,y) is the loss function associated with the data (x,y). In the following, we temporally omit the term L(θ,xk,yk) due to the fact that (xk,yk) is the mini- batch data arrived at timestamp kand does not depend on the memory data distribution. 3.2. DRO for Task-free CL Distributionally Robust Optimization (DRO) (Rahimian & Mehrotra, 2019) is a systematic and elegant framework to model the decision-making with ambiguity in the under- lying probability distribution. For task-free CL, different from the standard memory-replay methods in Section 3.1, implicitly assuming µ0 is known. In contrast, our proposedImproving Task-free Continual Learning by Distributionally Robust Memory Evolution DRO framework takes that the underlying actual probability distribution of memory data µis unknown and lies in an am- biguity set of probability distributions. Modeling the mem- ory data uncertainty is particularly useful when the memory buffer is small compared to the whole dataset since the memory has limited coverage to approximate the stationary distribution of all examples seen so far, illustrated in Figure 2 (a). Thus, there is signiﬁcant uncertainty in modeling the multi-task learning scenarios (the performance upper bound) with only a small memory buffer. The proposed DRO frame- work optimizes the worst-case performance in the ambiguity set of probability distributions since we cannot access the distribution of all the previous data. Therefore, it helps the model generalize to previous tasks since it can potentially narrow the gap between the memory data distribution and the distribution of all the previous data, illustrated in Figure 2 (b). As a by-product of this optimization framework, it also helps learn features robust to data distribution pertur- bations. On the other hand, the memory buffer is updated slowly during CL (e.g., by reservoir sampling), and standard memory-replay repeatedly trains the memory buffer, and the CL model can easily overﬁt the memory buffer, as illustrated in Figure 1 (a). Thus, the memory buffer could become less effective for mitigating forgetting. By optimizing the worst- case evolved memory data distribution at each iteration, our proposed DRO can also alleviate the memory overﬁtting problem by transforming the memory data to make them more difﬁcult to memorize. This is illustrated in Figure 1 (b) and (c). Mathematically speaking, the proposed DRO for task-free CL can be expressed as: min ∀θ∈Θ sup µ∈P EµL(θ,x,y) (2) s.t. P= {µ: D(µ||π) ≤D(µ0||π) ≤ϵ}, (3) E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y) ≥λ, (4) where the inner sup optimization is to gradually make the memory data distribution increasingly harder to be memo- rized. Pin Eq. (3) denotes the ambiguity set of probability measures (distributions or densities) for the memory data distribution to characterize its uncertainty. One common choice to deﬁne Pis through Kullback-Leibler (KL) di- vergence. D(µ0||π) denotes the KL divergence between probability measure µ0 and π, where πis the target worst- case evolved memory data distribution, i.e., the probability distribution π that achieves supµ∈PEµL(θ,x,y). ϵ is a constant threshold to characterize the closeness between µ0 and π to ensure the worst-case evolved memory data distribution πdoes not deviate from the raw memory data distribution µ0 too much. Eq. (4) constrains the gradi- ent dot product between the worst-case evolved memory data distribution and raw memory data distribution, i.e., ∇θL(θ,x,y)·∇θL(θ,x′,y), to avoid the evolved memory data deviate from the raw memory data too much. Intu- itively, if the gradient dot product is positive, it means the evolved memory data has a similar update direction com- pared to the raw data. λ is a threshold to determine the constraint magnitude. To solve the optimization, i.e., Eq. (2-4), the worst-case optimization that involves the sup optimization within the KL-divergence ball is generally computationally intractable since it involves the optimization over inﬁnitely many prob- ability distributions. To address this problem, by Lagrange duality (Boyd & Vandenberghe, 2004), we convert Eq. (2- 4) into the following unconstrained optimization problem (detailed derivations are put in Appendix B): min ∀θ∈Θ sup µ [EµL(θ,x,y) −γD(µ||π)+ β E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y)], (5) Where γ and β control the magnitude of regularization. Since the KL-divergence term D(µ||π) is implicitly han- dled by gradient ﬂow (discussed in the following sections), we set γ = 1 for simplicity throughout this paper. The gradient of the third term (gradient dot product term) can be efﬁciently approximated by ﬁnite difference in practice. The optimization Eq. (5) is still computationally hard to solve because the inner sup optimization is over probability measure space, which is an inﬁnite-dimensional function space. We name Eq. (5) as task-free DRO. 3.3. Task-free DRO: A Continuous Dynamics View To make it tractable to solve the task-free DRO Eq. (5), we formulate it from a new continuous dynamics perspective. This new perspective brings signiﬁcant advantages over di- rectly solving Eq. (5): 1) the optimization of Eq. (5) over probability measure space can be converted into a continu- ous probability distribution evolution, equivalent as a WGF, enabling gradient-based solutions in probability measure space (function space); 2) we can efﬁciently solve the WGF by various methods, which provide potential derivations of many different memory evolution methods. We then pro- pose a novel solution by decomposing the task-free DRO Eq. (5) into a gradient ﬂow system. Speciﬁcally, we solve the inner sup optimization problem for memory evolution with WGF in Wasserstein space of probability measures and solve the outer optimization problem for the model parame- ters with gradient ﬂow in Euclidean space. We convert the task-free DRO into a gradient ﬂow system that alternately updates the memory evolution and model parameters. Given a memory buffer at timestamp k, the raw memory data is M= {(x1 0,y1),(x2 0,y2),··· ,(xN 0 ,yN)}, where N is the memory buffer size. We perform a similar memory evolution procedure at each CL timestamp and omit the CL timestamp kfor notation clarity. We denotexi tas the ithdat- apoint in the evolved memory buffer after tevolution steps.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution The raw memory data is assumed to be i.i.d. sampled from the random variable X0, i.e., {x1 0,x2 0,··· ,xN 0 }∼ X0. X0 follows the probability distribution µ0, i.e., X0 ∼µ0. At evolution time t, the memory data Mis evolved as random variable Xt, i.e., {x1 t,x2 t,··· ,xN t } ∼Xt and probabil- ity distribution of random variable Xt follows the prob- ability measure µt, i.e., Xt ∼µt. The empirical mea- sure on the evolved memory buffer at time tis deﬁned as ˆµt = 1 N ∑i=N i=1 δ(xi t) and δis the Dirac measure. We model the memory evolution process as continuous WGF in proba- bility measure space, i.e., use (µt)t≥0 to model the proba- bility distribution evolution of memory data. The evolving (µt)t≥0 will in turn determine the memory evolution process (Xt)t≥0 in Euclidean space. Below, we deﬁne and present the gradient ﬂow in Wasser- stein space. Let P2(Rd) denote the space of probability measures on Rd with ﬁnite second-order moments. Each el- ement µ∈P2(Rd) is a probability measure represented by its density function µ: Rd →R with respect to Lebesgue measure dx. The Wasserstein distance between two proba- bility measures µ1,µ2 ∈P2(Rd) is deﬁned as: W2(µ1,µ2) = ( min ω∈∏(µ1,µ2) ∫ ||x−x′||2dω(x,x′)) )1/2 , where ∏(µ1,µ2) = {ω|ω(A ×Rd) = µ1(A),ω(Rd × B) = µ2(B)}. ω is the joint probability measure with marginal measure of µ1 and µ2 respectively. Thus, W2 = (P2(Rd),W2) forms a metric space. Deﬁnition 3.1 (Wasserstein Gradient Flow) . (Ambrosio et al., 2008). Suppose we have a Wasserstein space W2 = (P2(Rd),W2). A curve of (µt)t≥0 of probability measures is a Wasserstein gradient ﬂow for functional F if it satisﬁes ∂tµt = ∇W2 F(µt) :=div ( µt∇δF δµ(µt) ) (6) where := means deﬁned as, and div(r) :=∑d i=1 ∂zi ri(z) is the divergence operator of a vector-valued function r : Rd →Rd, where zi and ri are the ith element of zand r; ∇is the gradient of a scalar-valued function.∇W2 F(µt) := div(µt∇δF δµ(µt)) is the Wasserstein gradient of functional F at µt, where δF δµ(µt) is the ﬁrst variation of F at µt. The ﬁrst variation of a functional in function space is analogous to the ﬁrst-order gradient of a function in Euclidean distance. We put more detailed deﬁnition in Appendix D. Intuitively, the WGF describes that the probability measure µt follows the steepest curve of the functional F(µ) in Wasserstein space of probability measures (function space) to gradually move towards the target probability measure. We deﬁne the energy functionalF(µ) for memory evolution as the following: F(µ) =V(µ) +D(µ||π) (7) V(µ) =−EµL(θ,x,y)−βEµ∇θL(θ,x,y)·∇θL(θ,x′,y). (8) By deﬁning such energy functional F(µ), the Eq. (5) can be equivalently solved by the following gradient ﬂow system Eq. (9, 10), we name it as Dynamic DRO.   ∂tµt = div ( µt∇δF δµ(µt) ) ; (9) dθ dt = −∇θEµt L(θ,x,y), (10) where Eq. (9) solves the inner sup problem in Eq. (5) with WGF in Wasserstein space and Eq. (10) solves the outer minimization problem in Eq. (5) for parameter update with gradient ﬂow in Euclidean space. In the following, we focus on solving Eq. (9) and Eq. (10). 3.4. Training Algorithm for Dynamic DRO We propose three different methods for efﬁciently solving the Dynamic DRO in Eq. (9)-Eq. (10). The ﬁrst solution is Langevin dynamics with a diffusion process to evolve the memory data distribution; we name this method WGF-LD. Next, different from WGF-LD, which relies on randomness to transform the memory data, we kernelize the WGF in Eq. (9) by solving the WGF in reproducing kernel Hilbert space (RKHS). It deterministically transforms the memory data; we name this method WGF-SVGD. Furthermore, we generalize the above WGF and improve their ﬂexibility to incorporate prior knowledge or geometry information. One instantiation of this general WGF uses Hamiltonian dynamics; we name it WGF-HMC. More novel memory evolution methods are worth exploring in future work. Before introducing the proposed solutions, we ﬁrst present some preliminaries for solving the WGF. By calculus of variation (Miersemann, 2012), the ﬁrst variation for the KL divergence and V(µ) are (Ambrosio et al., 2008): δD(µ||π) δµ = logµ π + 1; δV(u) δµ =U(x,θ)= −L(θ,x,y)−β∇θL(θ,x,y)·∇θL(θ,x′,y). Langevin Dynamics for Dynamic DRO. If we directly use the energy functional F(µ) (Eq. (7)) in Eq. (9), solving gradient ﬂow Eq. (9) corresponds to the Langevin dynamics with the following stochastic differential equation (Welling & Teh, 2011): dX = −∇XU(X,θ)dt+ √ 2dWt, (11)Improving Task-free Continual Learning by Distributionally Robust Memory Evolution (a) t= t0  (b) t= t1  (c) t= t2 Figure 3.Illustration of WGF-LD for memory evolution at differ- ent time t0 <t1 <t2. Initially (t= t0), the raw memory data is easy to overﬁt. From t= t1 to t= t2, the memory data becomes harder to classify and overﬁt. The black arrow (corresponds to the ﬁrst term (∇xU(xi t,θ))), drives the memory data to become harder to classify. The white circle (corresponds to the second term (Brownian motion)) serves as a random force such that the memory data becomes more diverse. where X = (Xt)t≥0 is the memory evolution process as previously deﬁned. W = (Wt)t≥0 is the standard Brownian motion in Rn (Bass, 2011). If Xt ∼µt evolves according to the Langevin dynamics Eq. (11) in Euclidean space, then µ(x,t) =µt(x) evolves according to gradient ﬂow Eq. (9) in the space of probability measures (Jordan et al., 1998). If we discretize the above equation and view each datapoint in memory as one particle, the memory buffer data evolves with Langevin dynamics to obtain diverse memory data by the following updates: xi t+1 −xi t = −α(∇xU(xi t,θ)) + √ 2αξt. (12) As illustrated in Figure 3, the ﬁrst term in Eq. (12) drives the particles (memory data) towards the worst-case mem- ory data distribution πto make the memory data gradually harder to be memorized by dynamically increasing the en- ergy functional. For the second term, it adds noise to en- courage diversity in the transformed memory data, where ξt is standard Gaussian noise, and αis step size, i.e., a proper amount of added Gaussian noise tailored to the used step size. We name this memory evolution method as WGF-LD. Kernelized Method for Dynamic DRO. We replace the Wasserstein gradient ∇W2 F(µt) by the transforma- tion Kµ∇W2 F(µt) under the integral operator Kµf(x) =∫ K(x,x′)f(x′)dµ(x′); where the RKHS space induced by the kernel Kis denoted by H. The probability measure in the kernelized Wasserstein space actually follows the kernelized WGF (Liu, 2017): ∂tµt = div(µtKµt ∇δF δµ(µt)). (13) Eq. (13) can be viewed as the WGF Eq. (9) in RKHS. It indicates that the random variable Xt which describes the evolved memory data at time t evolves as the following (a) t= t0  (b) t= t1  (c) t= t2 Figure 4.Illustration of WGF-SVGD for memory evolution at dif- ferent time t0 <t1 <t2. Initially (t= t0), the raw memory data is easy to overﬁt. From t= t1 to t= t2, the memory data becomes harder to classify and overﬁt. The black arrow (corresponds to the ﬁrst term ( k(xi t,xj t)∇xj t U(xj t,θ))) drives the memory data to become harder to classify. The orange arrow (corresponds to the second term (∇xj t k(xi t,xj t))) serves as repulsive force such that the memory data becomes more diverse. differential equation (Liu, 2017): dX dt = −[Kµ∇δF δµ(µt)](X). (14) This kernelized version is the deterministic approximation of the WGF in Eq. (9) (Liu & Wang, 2016). If we discretize the above equation and view each datapoint in memory as one particle, we can obtain the following memory evolution update equation: xi t+1−xi t = −α N j=N∑ j=1 [k(xi t,xj t)∇xj t U(xj t,θ)    smoothed gradient + ∇xj t k(xi t,xj t)    repulsive term ], (15) As illustrated in Figure 4, the ﬁrst term drives the memory data towards the worst-case memory data distribution by increasing the energy functional. The update is driven by the kernel weighted sum of the gradients from the memory data points, thus smoothing the memory data gradients. The second term serves as a repulsive force that prevents the memory data points from collapsing into a single mode, thus diversifying the memory data population. In this pa- per, we use Gaussian kernel k(xi,xj) =exp(−(xi−xj)2 2σ2 ). We name this memory evolution method as WGF-SVGD. We put detailed derivations of this evolution equation in Appendix E. General Memory Evolution for Dynamic DRO. (Ma et al., 2015) found that any continuous Markov process that provides samples from the target distribution can be written in a very general sampler form. The corresponding general WGF for memory evolution can be written as: ∂tµt = div(µt(D+ Q)∇δF δµ(µt)), (16)Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Where Dis a positive semideﬁnite diffusion matrix, Qis a skew-symmetric curl matrix representing the deterministic traversing effect (Ma et al., 2015). One particular case is: D= ( 0 0 0 C ) ,Q= ( 0 −I I 0 ) , Where Cis the friction term, and Iis the identity matrix. This WGF corresponds to Hamiltonian dynamics (Chen et al., 2014) and can be solved by the following evolution: { xt+1 −xt = vt, vt+1 −vt = −α∇U(x,θ) −τv+ √ 2ταξt, (17) Where vis the momentum variable, and τ is the momentum weight. We name this method as WGF-HMC. The most attractive property of this WGF for memory distri- bution evolution is that we can freely specify the matrix Q and Dtailored to practical requirements. We can consider prior knowledge or geometry information by designing tai- lored Qand D, or develop the kernelized version of this general WGF. These research directions specialized for CL are left as future work to explore. Our framework is quite ﬂexible and general due to the energy functional design and WGF speciﬁcation. The proposed memory evolution algorithm is shown in Algo- rithm 1, with the ﬂexibility to use various evolution methods. Line 3-4 describes that a mini-batch data arrives at time k and samples a mini-batch data from the memory buffer. Line 5-7 is to evolve the mini-batch memory data with T steps depending on using which evolution methods. Line 8 updates the model parameters with the evolved memory data and mini-batch data received at time k. Line 9 updates the memory buffer with the mini-batch data received at time kusing reservoir sampling. Note that we do not replace the raw memory data with the evolved memory data for imple- mentation simplicity in the current version. If we replace the raw memory data with the evolved one, we can reduce the number of evolution steps needed at each CL timestamp to improve efﬁciency since an example can be adequately evolved after accumulating many timestamps. Earlier ex- amples can be evolved less frequently than the later ones. But replacing the raw memory data with the evolved one could slightly reduce the performance in our experiment. Our method needs to store a mini-batch of evolved memory data. This memory cost is negligible compared to the entire data stream memory buffer. We can view memory evolution from two perspectives. First, local evolutionevolves the cur- rent raw memory data distribution with several adaptation steps. Second, global evolutionevolves the memory data at different CL timestamps due to different model parameters. Discrete input. For the application of our methods in the discrete input domain (such as text), text can be embedded in continuous space (e.g., word embedding) and then evolve on embedding with our methods. Algorithm 1 Distributionally Robust Memory Evolution. 1: REQUIRE: model parameters θ, learning rate η, evolution rate (step size)α, number of evolution stepsT at each iteration, memory buffer M; Kis the number of mini-batch data in the data stream. 2: for k= 1to Kdo 3: a new mini-batch data (xk,yk) arrives. 4: sample mini-batch from memory buffer, i.e., (x,y) ∼M 5: for t= 1to T do 6: (x,y) = Evolve((x,y)) by WGF-LD (Eq. (12)) or WGF-SVGD (Eq. (15))) or WGF-HMC (Eq. (17)). 7: end for 8: θk+1 = θk −η∇θ[L(θk,x,y) +L(θk,xk,yk)] 9: update memory buffer by reservoir sampling, M = reservoirsampling(M,(xk,yk)) 10: end for 4. Experiments To evaluate the effectiveness of our memory evolution meth- ods, we compared a variety of state-of-the-art baseline ap- proaches. We ﬁrst describe the benchmark datasets and baselines in Section 4.1. Then, we compare various base- lines on several datasets in Section 4.2, and evaluate the methods in terms of robustness to adversarial examples in Section 4.3. We perform ablation study in Section 4.4. 4.1. Experiment Setup CIFAR10, following (Aljundi et al., 2019a), we split the CIFAR-10 dataset into 5 disjoint tasks with the same train- ing, validation, and test sets. MiniImagenet, following (Aljundi et al., 2019a), we split MiniImagenet (Vinyals et al., 2016) dataset with 100 image classes into 20 disjoint tasks. Each task has 5 classes. CIFAR-100 (Krizhevsky, 2009), contains 100 image classes. We also split it into 20 disjoint tasks. Baselines. We performed comprehensive experiments by comparing to the following strong baselines, including Ex- perience Replay (ER) (Chaudhry et al., 2019b), Maximally Interfering Retrieval (MIR) (Aljundi et al., 2019a), AGEM (Chaudhry et al., 2019a), Gradient-Based Sample Selection (GSS-Greedy) (Aljundi et al., 2019c) and GMED (Jin et al., 2021). Furthermore, following (Jin et al., 2021), we also compare data augmentation, such as random rotations, scal- ing, and horizontal ﬂipping applied to memory buffer data in ER and name this baseline as ERaug. Following (Aljundi et al., 2019a), we also compare (1) ﬁne-tuning, which trains on each latent task sequentially when new batches of each task arrive without any forgetting mitigation mechanism; (2) iid online: which trains the model with a single-pass through the iid sampled data on the same set of samples;Improving Task-free Continual Learning by Distributionally Robust Memory Evolution (3) iid ofﬂine: (upper-bound) which trains the model with multiple passes through the iid sampled data. We train the model with 5 epochs for this baseline. Detailed descriptions of the compared baselines are placed in Appendix A. In addition, our proposed methods are orthogonal to existing memory-replay-based CL methods. Thus, they are versatile, and we can seamlessly combine the proposed methods with them. For illustration, we combine our proposed methods with ER, MIR, and GMED to show the effectiveness. We name the combination methods ER+WGF-LD, ER+WGF- SVGD, ER+WGF-HMC, MIR+WGF-LD, GMED+WGF- LD, etc. Combining with other memory-replay-based meth- ods is straightforward. Implementation Details. We use the Resnet-18 as (Aljundi et al., 2019a). The number of evolution steps T is set to be 5, the evolution rate αis set to be 0.01 for CIFAR10, 0.05 for CIFAR100 and 0.001 for MiniImagenet, β = 0.003 and momentum τ = 0.1. We set the memory buffer size to be 500 for CIFAR-10 dataset, 5K for CIFAR-100 and 10Kfor MiniImagenet. All other hyperparameters are the same as (Aljundi et al., 2019a). All reported results in our experiments are the average accuracy of 10 runs with standard deviation. 4.2. Comparison to Continual Learning We compare the proposed methods to various task-free CL baselines. Table 1 shows the effectiveness of combining the proposed method with existing memory-replay methods, e.g., ER, MIR and GMED. We can observe that our method outperforms these strong baselines. In particular, for ER and ER + WGF methods, our method outperforms baselines by 4.5%, 1.4%, and 2.8% on CIFAR10, CIFAR-100, and Mini- ImageNet, respectively. For MIR and MIR + WGF methods, our method outperforms baselines by3.8%, 1.6%, and 2.1% on CIFAR10, CIFAR-100, and MiniImageNet, respectively. For GMED and GMED + WGF methods, our method out- performs baselines by 3.6%, 0.9%, and 1.4% on CIFAR10, CIFAR-100, and MiniImageNet, respectively. Our methods outperform baselines because they dynamically evolve the memory data distribution and sufﬁciently explore the input space in a principled way. The proposed methods generate more diverse memory data, and the evolved memory be- comes more difﬁcult for the CL model to memorize than baseline methods. In addition, WGF-SVGD generally per- forms better than WGF-SGLD and WGF-HMC. We believe this is because, in RKHS, the evolved memory data is en- couraged to be far apart by the kernel repulsive forces; the evolved memory data may better represent the distribution of all the previous data. Table 1.Comparison to continual learning baselines on CIFAR10, CIFAR-100 and MiniImagenet by combing our proposed method with existing CL methods Algorithm CIFAR10 CIFAR-100 MiniImagenet ﬁne-tuning 18.9±0.1 3 .1±0.2 2 .9±0.5 A-GEM 19.0±0.3 2 .4±0.2 3 .0±0.4 GSS-Greedy 29.9±1.5 19 .5±1.3 17 .4±0.9 ER 33.3±2.8 20 .1±1.2 24 .8±1.0 ER + WGF-LD 37.6±1.5 21.5±1.3 27.3±1.0 ER + WGF-SVGD 36.5±1.4 21 .3±1.5 27.6±1.3 ER + WGF-HMC 37.8±1.3 21.2±1.4 27 .2±1.1 MIR 34.4±2.5 20 .0±1.7 25 .3±1.7 MIR + WGF-LD 38.2±1.2 21.6 ±1.2 26.9±1.0 MIR + WGF-SVGD37.0±1.4 21 .2±1.5 27.4±1.2 MIR + WGF-HMC 37.9±1.5 21 .3±1.4 27 .1±1.3 GMED (ER) 34.8±2.2 20 .9±1.6 27 .3±1.8 GMED + WGF-LD 38.4±1.6 21.7±1.7 28 .3±1.9 GMED + WGF-SVGD37.6±1.7 21.8±1.5 28.7 ±1.5 GMED + WGF-HMC37.8±1.2 21 .5±1.9 28 .4±1.3 ERaug+ ER 46.3±2.7 18 .3±1.9 30 .8±2.2 ERaug+ WGF-LD 47.6±2.4 19 .8±2.2 31 .9±1.8 ERaug+ WGF-SVGD47.9±2.5 19.9±2.3 32.2±1.5 ERaug+ WGF-HMC 47.8±2.6 20.3±2.1 31.7±2.0 iid online 60.3±1.4 18 .7±1.2 17 .7±1.5 iid ofﬂine 78.7±1.1 44 .9±1.5 39 .8±1.4 Table 2.Carlini & Wagner attack model performance for the CIFAR-100 and Mini-Imagenet dataset Algorithm CIFAR-10 CIFAR-100 Mini-Imagenet ER 2.0±0.1 0 .0 0 .0 GMED 2.1±0.1 0 .0 0 .0 ER + WGF-LD 8.0±0.2 3.0±0.2 3.1 ±0.1 ER + WGF-SVGD 4.2±0.1 0 .0 2 .2±0.2 ER + WGF-HMC 8.2±0.3 2.5±0.2 3 .0±0.1 4.3. Robustness to Adversarial Perturbations In this section, we evaluate the robustness of the CL model to adversarial perturbed examples. Given a clas- siﬁer f(x,θ), for an image x, the goal is to ﬁnd another example x′that is close enough to xmeasured by some distance function D(x.x′) ≤ϵsuch that the classiﬁer clas- siﬁes it into another different class, i.e. f(x,θ) ̸= f(x′,θ). In this paper, we focus on the most commonly used ℓ∞and ℓ2 norm as the distance function. Our memory evolution methods optimize on the worst-case evolved memory data distribution during CL; the model would be thus naturally robust to adversarial perturbations. For ℓ∞norm attack, we evaluate the robustness under the strong PGD ℓ∞ attack (Madry et al., 2018), which con- structs adversarial examples with projected gradient descent and ℓ∞ norm constraints. The adversarial perturbation magnitude ranges from [1/255,2/255,··· ,10/255] with 20 steps attack and stepsize of 2 255 on CIFAR-100 and Mini- ImageNet. For ℓ2 norm attack, we adopt the strong Carlini & Wagner attack (Carlini & Wagner, 2017). For illustra-Improving Task-free Continual Learning by Distributionally Robust Memory Evolution tion, we evaluate the model robustness to adversarial exam- ples after training with ER, GMED, ER + WGF-LD, ER + WGF-SVGD, and ER + WGF-HMC, respectively. Fig- ure 5 shows the PGD ℓ∞attack results on CIFAR-100 and Mini-ImageNet. Our WGF-HMC and WGF-LD memory evolution signiﬁcantly outperforms naive ER baseline by 4% −12% depending on the perturbation magnitude. In addition, WGF-HMC and WGF-LD perform more robust than WGF-SVGD. We believe this is because the random- ness introduced in WGF-HMC and WGF-LD can better ex- plore the input space and thus can generate harder evolved memory data. WGF-SVGD smooths the function gradient by the kernel function, thus may generate less hard exam- ples. Table 2 shows the Carlini & Wagner attack results. We can see that under the strong Carlini & Wagner attack, ER baseline accuracy becomes zero, and our methods still outperform baselines ranging from 6.1%,3.0%,3.1% on CIFAR-10, CIFAR-100, Mini-ImageNet, respectively. Both results demonstrate the robustness of our proposed methods to adversarial examples. Figure 5.PGD ℓ∞ attack results on two datasets: CIFAR-100 (left) and Mini-ImageNet (right). 4.4. Ablation Study Effects of different memory size. To investigate the effect of different smaller memory sizes on the model performance, we evaluate the effects on Mini-ImageNet with memory sizes of 3000, 5000, and 10000. We evaluate the effects on CIFAR-100 with the memory sizes of 2000, 3000, and 5000. We show the results in Table 3. In most cases, our WGF memory evolution substantially outperforms the baselines with different memory buffer sizes. Effect of number of evolution steps . To investigate the effect of different evolution steps, we compare 3, 5, and 7 evolution steps, respectively. Table 4 shows the performance variation of different number of evolution steps. We can ﬁnd that the performance improves slightly with an increasing number of evolution steps. For efﬁciency and sufﬁciently ex- ploring the input space to evolve harder memory examples, we choose the evolution step of 5. Hyperparameter sensitivity. Due to space limitation, we put hyperparameter sensitivity analysis, including the regu- larizer weight βand evolution rate α, in Appendix C. Computation cost. We compare the proposed ER + WGF to ER to evaluate its running efﬁciency. Table 5 shows the Table 3.Effect of different memory size on the model performance for the CIFAR-100 and Mini-Imagenet datasets. CIFAR-100 Memory Size 2000 3000 5000 ER 11.2±1.0 15 .0±0.9 20 .1±1.2 ER + WGF-LD 12.9±1.2 17.0±1.1 21.5±1.3 ER + WGF-SVGD 12.3±1.1 16 .0±1.2 21 .3±1.5 ER + WGF-HMC 12.7±1.0 17.2±1.0 21.2±1.4 MIR 11.6±0.8 15 .6±1.0 20 .0±1.7 MIR + WGF-LD 13.1±0.9 17 .3±1.2 21.6±1.2 MIR + WGF-SVGD 12.7±1.0 16 .5±1.3 21 .2±1.5 MIR + WGF-HMC 13.2±1.2 17.5 ±1.1 21.3±1.4 Mini-Imagenet Memory Size 3000 5000 10000 ER 13.4±1.4 17 .9±1.6 24 .8±0.9 ER + WGF-LD 16.2±1.2 20.8±1.2 27 .3±1.0 ER + WGF-SVGD 15.7±1.2 21.3±1.0 27.6 ±1.3 ER + WGF-HMC 15.9±1.5 20 .6±1.4 27 .2±1.1 MIR 12.6±1.5 17 .4±1.2 25 .3±1.7 MIR + WGF-LD 15.5±1.4 20 .5±1.1 26 .9±1.0 MIR + WGF-SVGD 15.3±1.2 20.7±1.6 27.4 ±1.2 MIR + WGF-HMC 15.8±1.7 20.3±1.5 27 .1±1.3 Table 4.Effect of number of evolution steps on Mini-ImageNet. Evolution Steps 3 5 7 ER + WGF-LD 27.0±0.9 27 .3±1.0 27 .5±1.4 ER + WGF-SVGD27.2±1.2 27.6±1.3 27.2±1.2 ER + WGF-HMC 27.1±1.3 27 .2±1.1 27 .6±1.0 efﬁciency comparison results. We set the simple baseline ER with a running time unit of 1. Our method has 3-4 times the computational cost compared to a simple ER baseline. In future work, we will improve its computational efﬁciency. Table 5.Computation efﬁciency (relative training time) of the pro- posed method compared to baseline. Algorithm running time ER 1.0 ER + WGF-LD 3.4 ER + WGF-SVGD 4.1 ER + WGF-HMC 3.5 5. Conclusion This paper proposes a novel concept of DRO memory evolu- tion for task-free continual learning to dynamically evolve the memory data distribution to mitigate the memory over- ﬁtting issue and ﬁll the gap between the memory data dis- tribution and the distribution of all the previous data. We propose a family of memory evolution methods with WGF. The proposed principled framework is general, ﬂexible, and easily expandable. Future work includes designing more informative functional and novel gradient ﬂow dynamics to incorporate physical intuitions and geometry constraints.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Extensive experiments compared to various state-of-the- art methods demonstrate the effectiveness of the proposed method. Interestingly, our methods are more robust to ad- versarial examples than compared baselines. 6. Acknowledgement We thank all the anonymous reviewers for their insightful and thoughtful comments. This research was supported in part by NSF through grant IIS-1910492.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution References Aljundi, R., Belilovsky, E., Tuytelaars, T., Charlin, L., Cac- cia, M., Lin, M., and Page-Caccia, L. Online continual learning with maximal interfered retrieval. Advances in Neural Information Processing Systems 32, pp. 11849– 11860, 2019a. Aljundi, R., Kelchtermans, K., and Tuytelaars, T. Task-free continual learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019b. Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems 30, 2019c. Ambrosio, L., Gigli, N., and Savare, G. Gradient ﬂows: In metric spaces and in the space of probability measures. (Lectures in Mathematics. ETH), 2008. Bass, R. F. Stochastic Processes. Cambridge Series in Sta- tistical and Probabilistic Mathematics. Cambridge Uni- versity Press, 2011. doi: 10.1017/CBO9780511997044. Boyd, S. and Vandenberghe, L. Convex optimization. Cam- bridge university press, 2004. Carlini, N. and Wagner, D. Towards evaluating the ro- bustness of neural networks. 2017 IEEE Symposium on Security and Privacy (SP), 2017. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. Proceedings of the International Conference on Learning Representa- tions, 2019a. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H. S., and Ranzato, M. Continual learning with tiny episodic memories. https://arxiv.org/abs/1902.10486, 2019b. Chen, T., Fox, E., and Guestrin, C. Stochastic gradient hamiltonian monte carlo. In Proceedings of the 31st International Conference on Machine Learning, 2014. Chewi, S., Le Gouic, T., Lu, C., Maunu, T., and Rigollet, P. Svgd as a kernelized wasserstein gradient ﬂow of the chi- squared divergence. In Advances in Neural Information Processing Systems, 2020. Chrysakis, A. and Moens, M.-F. Online continual learning from imbalanced data. Proceedings of the 37th Interna- tional Conference on Machine Learning, 119:1952–1961, 2020. He, X., Sygnowski, J., Galashov, A., Rusu, A. A., Teh, Y . W., and Pascanu, R. Task agnostic continual learning via meta learning. https://arxiv.org/abs/1906.05201, 2019. Jin, X., Sadhu, A., Du, J., and Ren, X. Gradient-based edit- ing of memory examples for online task-free continual learning. Advances in Neural Information Processing Systems, 2021. Jordan, R., Kinderlehrer, D., , and Otto., F. The variational formulation of the fokker–planck equation.SIAM Journal on Mathematical Analysis, 1998. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009. Lee, S., Ha, J., Zhang, D., and Kim, G. A neural dirichlet process mixture model for task-free continual learning. In Proceedings of the 17th International Conference on Machine Learning, 2020. Liu, C., Zhuo, J., and Zhu, J. Understanding mcmc dynam- ics as ﬂows on the wasserstein spac. Proceedings of the International Conference on Machine Learning, 2019. Liu, Q. Stein variational gradient descent as gradient ﬂow. Advances in Neural Information Processing Sys- tems, 2017. Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. Advances in Neural Information Processing Systems, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. Advances in Neural Information Processing Systems, 2017. Ma, Y .-A., Chen, T., and Fox, E. B. A complete recipe for stochastic gradient mcmc. Advances in Neural Informa- tion Processing Systems, 2015. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. Proceedings of the International Con- ference on Learning Representations, 2018. Miersemann, E. Calculus of Variations, Lecture Notes. Leipzig University, 2012. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Vari- ational continual learning. Proceedings of the Interna- tional Conference on Learning Representations, 2018. Pham, Q., Liu, C., Sahoo, D., and HOI, S. Contextual transformation networks for online continual learning. Proceedings of the International Conference on Learning Representations, 2021.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Rahimian, H. and Mehrotra, S. Distributionally robust opti- mization: A review. 2019. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forget- ting by maximizing transfer and minimizing interference. International Conference on Learning Representations, 2019. Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case gener- alization. International Conference on Learning Repre- sentations, 2020. Saha, G., Garg, I., and Roy, K. Gradient projection memory for continual learning. Proceedings of the International Conference on Learning Representations, 2021. Vinyals, O., Blundell, C., Lillicrap, T., kavukcuoglu, k., and Wierstra, D. Matching networks for one shot learning. 29, 2016. von Oswald, J., Henning, C., Sacramento, J., and Grewe, B. F. Continual learning with hypernetworks. https://arxiv.org/abs/1906.00695, 2019. Wang, Z., Duan, T., Fang, L., Suo, Q., and Gao, M. Meta learning on a sequence of imbalanced domains with difﬁ- culty awareness. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pp. 8947–8957, 2021. Wang, Z., Shen, L., Duan, T., Zhan, D., Fang, L., and Gao, M. Learning to learn and remember super long multi- domain task sequence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7982–7992, 2022. Welling, M. and Teh, Y . W. Bayesian learning via stochastic gradient langevin dynamics. Proceedings of the Interna- tional Conference on Machine Learning, 2011. Xu, Z., Dan, C., Khim, J., and Ravikumar, P. Class-weighted classiﬁcation: Trade-offs and robust approaches. Pro- ceedings of the International Conference on Machine Learning, 2020. Zenke, F., Poole, B., and Ganguli, S. Con- tinual learning through synaptic intelligence. https://arxiv.org/abs/1703.04200, 2017. Zeno, C., Golan, I., Hoffer, E., and Soudry, D. Task ag- nostic continual learning using online variational bayes. https://arxiv.org/abs/1803.10123, 2019. Zhai, R., Dan, C., Kolter, J. Z., and Ravikumar, P. Doro: Dis- tributional and outlier robust optimization. Proceedings of the International Conference on Machine Learning, 2021.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution A. Baselines The detailed descriptions of baselines are the following: Experience Replay (ER) (Chaudhry et al., 2019b), which stores a small set of examples from previous tasks with reservoir sampling (Chaudhry et al., 2019b). At later time, we randomly sample a set of examples from the memory buffer to train with the received mini-batch data together to avoid forgetting. Maximally Interfering Retrieval (MIR) (Aljundi et al., 2019a), the goal of MIR is to select the examples that are easily forgettable for replay. The idea of MIR is not using randomly selected data from the memory buffer, but instead replaying the samples that would be (maximally) interfered by the new received data. Following (Aljundi et al., 2019a), we evaluate the model forgetting on 25 examples for Mini-ImageNet dataset, and 50 examples for other datasets. AGEM (Chaudhry et al., 2019a), AGEM tries to ensure that at every training step the average episodic memory loss over the previous tasks does not increase. They project the gradient update direction such that they are less interfered with current data by projecting the gradient to the closest in L2 norm gradient that keeps the angle within 90 degree. Gradient-Based Sample Selection (GSS-Greedy) (Aljundi et al., 2019c) is to encourage diverse examples in the memory buffer. We use GSS-Greedy, which is efﬁcient and performs the best. GMED (Jin et al., 2021). GMED is the recent memory-replay methods, which edits the memory data so that they are harder to be memorized, shares the similar hypothesis as (Aljundi et al., 2019a). B. Lagrangian Duality Derivation The basic idea in Lagrangian duality (Boyd & Vandenberghe, 2004) is to consider the constraints by augmenting the objective function with a weighted sum of the constraint functions. We ﬁrst convert the optimization Eq. (2), Eq. (3) and Eq. (4) into the following optimization. min ∀θ∈Θ sup µ∈P EµL(θ,x,y) (18) s.t. P= {µ: D(µ||π) ≤D(µ0||π) ≤ϵ} (19) − E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y) ≤−σ. (20) Then, by Lagrangian duality (Boyd & Vandenberghe, 2004), we can get the equivalent constrained optimization: min ∀θ∈Θ sup µ [EµL(θ,x,y) −γD(µ||π)+ β E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y)]. (21) where γand βare the Lagrange multiplier associated with the corresponding inequality constraints. C. More Experiments Results Hyperparameter Sensitivity Analysis We use ER+WGF-LD as ablation study, similar results for other memory evolution methods. We perform sensitivity analysis for the regularization weight βof gradient dot product between perturbed memory data and raw data. The results are shown in Table 6. In particular, we show the results thatβ = 0.0, i.e., without gradient dot product regularization. Also, we perform sensitivity analysis on memory evolution rate αin Table 7, 8 and 9.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Table 6.Sensitivity analysis of regularization weight β. Hyperparameter CIFAR-10 CIFAR-100 Mini-Imagenet β= 0.0 37 .2±1.7 21 .2±1.5 27 .1±1.4 β= 0.001 37 .3±1.8 21 .6±1.2 27 .5±1.8 β= 0.003 37 .6±1.5 21 .5±1.3 27 .3±1.0 β= 0.005 37 .5±1.1 21 .3±1.1 27 .6±1.4 Table 7.Sensitivity analysis of evolution rate αon CIFAR10. α= 0.005 α= 0.01 α= 0.03 37.5±1.2 37 .6±1.5 37 .9±1.6 Table 8.Sensitivity analysis of evolution rate αon CIFAR100. α= 0.01 α= 0.05 α= 0.1 21.6±1.2 21 .5±1.3 21 .3±1.2 Table 9.Sensitivity analysis of evolution rate αon miniImageNet. α= 0.0001 α= 0.001 α= 0.005 27.5±1.2 27 .3±1.0 26 .9±1.5 D. Foundations of Calculus of Variations In calculus of variations, the ﬁrst variation is deﬁned as following: Deﬁnition D.1 (First Variations). The ﬁrst variation of a functional F(µ) is the functional at µ δF δµ(µ) = lim ϵ→0 F(µ+ ϵψ) −F(µ) ϵ , (22) where ψis arbitrary function. For more detailed background knowledge of calculus of variations, we recommend the reader refer to (Miersemann, 2012). E. Derivations of the memory evolution methods Speciﬁcally, we derive the WGF-SVGD as an example: δD(µ||π) δµ = logµ π + 1; δV(u) δµ =U(x,θ)= −L(θ,x,y)−β∇θL(θ,x,y)·∇θL(θ,x′,y). We deﬁne the target worst-case evolved memory data distribution asπ∝ e−U by the energy function U(x,θ) deﬁned above. We replace the Wasserstein gradient∇W2 F(µt) by the transformation Kµ∇W2 F(µt) under the integral operator Kµf(x) =∫ K(x,x′)f(x′)dµ(x′);Improving Task-free Continual Learning by Distributionally Robust Memory Evolution The probability measure in the kernelized Wasserstein space actually follows the kernelized WGF (Liu, 2017): ∂tµt = div(µtKµt ∇δF δµ(µt)). (23) Eq. (23) can be viewed as the WGF Eq. (9) in RKHS. It indicates that the random variable Xt which describes the evolved memory data at time tevolves as the following differential equation (Liu, 2017; Chewi et al., 2020): dX dt = −[Kµ∇δF δµ(µt)](X). (24) First, we derive the kernelized Wasserstein gradient of Kµt ∇log µt π (x) := ∫ K(x,·)∇log µt π = ∫ K(x,·)∇Udµt − ∫ ∇2K(x,·)dµt (25) where, we apply integration by parts in the second identity. ∇2 means the gradient of the kernel w.r.t. the second argument. It indicates that the random variable Xt which describes the evolved memory data at time t evolves as the following differential equation (Liu, 2017): We plug Eq. 25 into Eq. 24, we can obtain the following equations. dX dt = − ∫ K(x,·)∇Udµt + ∫ ∇2K(x,·)dµt (26) If we discretize the above Eq. 26 and view each datapoint in memory as one particle, we can obtain the following memory evolution update equation: xi t+1 −xi t = −α N j=N∑ j=1 [k(xi t,xj t)∇xj t U(xj t,θ)    smoothed gradient + ∇xj t k(xi t,xj t)    repulsive term ], (27)",
      "meta_data": {
        "arxiv_id": "2207.07256v2",
        "authors": [
          "Zhenyi Wang",
          "Li Shen",
          "Le Fang",
          "Qiuling Suo",
          "Tiehang Duan",
          "Mingchen Gao"
        ],
        "published_date": "2022-07-15T02:16:09Z",
        "pdf_url": "https://arxiv.org/pdf/2207.07256v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the limitations of memory replay in task-free continual learning (CL), specifically overfitting the memory buffer and the discrepancy between memory data distribution and the true distribution of past data. It proposes the first principled memory evolution framework using Distributionally Robust Optimization (DRO) to dynamically evolve memory data, making it harder to memorize and better representative of previous knowledge. This framework formulates task-free DRO as a continuous gradient flow system (Dynamic DRO) and introduces a family of memory evolution methods based on Wasserstein gradient flow (WGF), including Langevin Dynamics, Stein Variational Gradient Descent, and Hamiltonian flow. The method demonstrates significant effectiveness in mitigating forgetting and, as a by-product, enhances robustness to adversarial examples.",
        "methodology": "The core methodology involves a Distributionally Robust Optimization (DRO) framework to evolve the memory data distribution. The DRO objective minimizes performance under the worst-case evolved memory data distribution, considering an ambiguity set defined by KL divergence from the raw memory distribution and a constraint on the gradient dot product. To solve this intractable optimization problem in infinite-dimensional probability measure space, it is reformulated as a continuous gradient flow system, named Dynamic DRO. Memory evolution is modeled as Wasserstein Gradient Flow (WGF), with model parameters updated via gradient flow in Euclidean space. Three specific methods are derived from WGF for memory evolution: WGF-LD (Langevin Dynamics) uses stochastic differential equations with a drift term for 'harder to memorize' data and a diffusion term for diversity; WGF-SVGD (Stein Variational Gradient Descent) kernelizes WGF in RKHS, leading to deterministic transformations with smoothed gradients and repulsive forces; and WGF-HMC (Hamiltonian Monte Carlo) generalizes WGF using Hamiltonian dynamics with momentum and friction. Memory update uses reservoir sampling.",
        "experimental_setup": "The methods were evaluated on three benchmark datasets: CIFAR-10 (split into 5 disjoint tasks), MiniImagenet (100 classes split into 20 disjoint tasks of 5 classes each), and CIFAR-100 (split into 20 disjoint tasks). Baselines included Experience Replay (ER), Maximally Interfering Retrieval (MIR), A-GEM, Gradient-Based Sample Selection (GSS-Greedy), GMED, and ERaug (ER with data augmentation). Upper and lower bounds were set by fine-tuning, iid online, and iid offline training. The proposed methods were combined with existing memory-replay baselines (ER, MIR, GMED) to demonstrate their versatility. A ResNet-18 model was used. Hyperparameters included 5 evolution steps (T), evolution rates (α) of 0.01 (CIFAR10), 0.05 (CIFAR100), and 0.001 (MiniImagenet), β=0.003, and momentum τ=0.1. Memory buffer sizes were 500 (CIFAR-10), 5K (CIFAR-100), and 10K (MiniImagenet). Results were averaged over 10 runs with standard deviations. Adversarial robustness was evaluated using PGD L-infinity attack and Carlini & Wagner L2 norm attack.",
        "limitations": "One identified limitation is the increased computational cost, with the proposed method being 3-4 times slower than a simple ER baseline. Additionally, for implementation simplicity, the current version does not replace raw memory data with evolved memory data, and initial experiments showed that replacing it could slightly reduce performance, despite potentially reducing the number of evolution steps needed for efficiency.",
        "future_research_directions": "Future research directions include improving the computational efficiency of the proposed framework. There is also potential to design more informative functional forms and novel gradient flow dynamics to incorporate physical intuitions and geometry constraints, further specializing memory evolution for continual learning. Exploring more effective memory evolution methods, such as designing tailored Q and D matrices for the general WGF framework, is another promising avenue. Investigating alternative strategies for replacing raw memory data with evolved memory data to reduce evolution steps and enhance efficiency, while mitigating the observed slight performance reduction, is also a future direction."
      }
    },
    {
      "title": "SparCL: Sparse Continual Learning on the Edge",
      "abstract": "Existing work in continual learning (CL) focuses on mitigating catastrophic\nforgetting, i.e., model performance deterioration on past tasks when learning a\nnew task. However, the training efficiency of a CL system is\nunder-investigated, which limits the real-world application of CL systems under\nresource-limited scenarios. In this work, we propose a novel framework called\nSparse Continual Learning(SparCL), which is the first study that leverages\nsparsity to enable cost-effective continual learning on edge devices. SparCL\nachieves both training acceleration and accuracy preservation through the\nsynergy of three aspects: weight sparsity, data efficiency, and gradient\nsparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a\nsparse network throughout the entire CL process, dynamic data removal (DDR) to\nremove less informative training data, and dynamic gradient masking (DGM) to\nsparsify the gradient updates. Each of them not only improves efficiency, but\nalso further mitigates catastrophic forgetting. SparCL consistently improves\nthe training efficiency of existing state-of-the-art (SOTA) CL methods by at\nmost 23X less training FLOPs, and, surprisingly, further improves the SOTA\naccuracy by at most 1.7%. SparCL also outperforms competitive baselines\nobtained from adapting SOTA sparse training methods to the CL setting in both\nefficiency and accuracy. We also evaluate the effectiveness of SparCL on a real\nmobile phone, further indicating the practical potential of our method.",
      "full_text": "SparCL: Sparse Continual Learning on the Edge Zifeng Wang1,†, Zheng Zhan1,†, Yifan Gong1, Geng Yuan1, Wei Niu2, Tong Jian1, Bin Ren2, Stratis Ioannidis1, Yanzhi Wang1, Jennifer Dy1 1 Northeastern University, 2 College of William and Mary {zhan.zhe, gong.yifa, geng.yuan, yanz.wang}@northeastern.edu, {zifengwang, jian, ioannidis, jdy}@ece.neu.edu, wniu@email.wm.edu, bren@cs.wm.edu Abstract Existing work in continual learning (CL) focuses on mitigating catastrophic for- getting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efﬁciency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning (SparCL), which is the ﬁrst study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dy- namic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efﬁciency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efﬁciency of existing state-of-the-art (SOTA) CL methods by at most 23×less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most1.7%. SparCL also outperforms competitive base- lines obtained from adapting SOTA sparse training methods to the CL setting in both efﬁciency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method. Source code will be released. 1 Introduction The objective of Continual Learning (CL) is to enable an intelligent system to accumulate knowledge from a sequence of tasks, such that it exhibits satisfying performance on both old and new tasks (31). Recent methods mostly focus on addressing the catastrophic forgetting (42) problem – learning model tends to suffer performance deterioration on previously seen tasks. However, in the real world, when the CL applications are deployed in resource-limited platforms ( 47) such as edge devices, the learning efﬁciency, w.r.t. both training speed and memory footprint, are also crucial metrics of interest, yet they are rarely explored in prior CL works. Existing CL methods can be categorized into regularization-based (2; 31; 36; 67), rehearsal-based (8; 11; 49; 60), and architecture-based (30; 41; 51; 57; 58; 69). Both regularization- and rehearsal-based methods directly train a dense model, which might even be over-parametrized for the union of all tasks (18; 38); Though several architecture-based methods (50; 56; 63) start with a sparse sub-network from the dense model, they still grow the model size progressively to learn emerging tasks. The aforementioned methods, although striving for greater performance with less forgetting, still introduce signiﬁcant memory and computation overhead during the whole CL process. †Both authors contributed equally to this work 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2209.09476v1  [cs.LG]  20 Sep 2022Figure 1: Left: Overview of SparCL. SparCL consists of three complementary components: task-aware dynamic masking (TDM) for weight sparsity, dynamic data removal (DDR) for data efﬁciency, and dynamic gradient masking (DGM) for gradient sparsity. Right: SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, with different sparsity ratios on the Split Tiny-ImageNet (15) dataset. Recently, another stream of work, sparse training (4; 19; 34) has emerged as a new training trend to achieve training acceleration, which embraces the promising training-on-the-edge paradigm. With sparse training, each iteration takes less time with the reduction in computation achieved by sparsity, under the traditional i.i.d. learning setting. Inspired by these sparse training methods, we naturally think about introducing sparse training to the ﬁeld of CL. A straightforward idea is to directly combine existing sparse training methods, such as SNIP ( 34), RigL (19), with a rehearsal buffer under the CL setting. However, these methods fail to consider key challenges in CL to mitigate catastrophic forgetting, for example, properly handling transition between tasks. As a result, these sparse training methods, though enhancing training efﬁciency, cause signiﬁcant accuracy drop (see Section 5.2). Thus, we would like to explore a general strategy, which is orthogonal to existing CL methods, that not only leverages the idea of sparse training for efﬁciency, but also addresses key challenges in CL to preserve (or even improve) accuracy. In this work, we propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, aiming at enabling practical CL on edge devices. As shown in Figure 1 (left), SparCL achieves both learning acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, to maintain a small dynamic sparse network during the whole CL process, we develop a novel task-aware dynamic masking (TDM) strategy to keep only important weights for both the current and past tasks, with special consideration during task transitions. Moreover, we propose a dynamic data removal (DDR) scheme, which progressively removes “easy-to-learn” examples from training iterations, which further accelerates the training process and also improves accuracy of CL by balancing current and past data and keeping more informative samples in the buffer. Finally, we provide an additional dynamic gradient masking (DGM) strategy to leverage gradient sparsity for even better efﬁciency and knowledge preservation of learned tasks, such that only a subset of sparse weights are updated. Figure 1 (right) demonstrates that SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, under different sparsity ratios. SparCL is simple in concept, compatible with various existing rehearsal-based CL methods, and efﬁcient under practical scenarios. We conduct comprehensive experiments on multiple CL bench- marks to evaluate the effectiveness of our method. We show that SparCL works collaboratively with existing CL methods, greatly accelerates the learning process under different sparsity ratios, and even sometimes improves upon the state-of-the-art accuracy. We also establish competing baselines by combining representative sparse training methods with advanced rehearsal-based CL methods. SparCL again outperforms these baselines in terms of both efﬁciency and accuracy. Most importantly, we evaluate our SparCL framework on real edge devices to demonstrate the practical potential of our method. We are not aware of any prior CL works that explored this area and considered the constraints of limited resources during training. In summary, our work makes the following contributions: • We propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, which achieves learning acceleration through the synergy of weight sparsity, data efﬁ- ciency, and gradient sparsity. To the best of our knowledge, our work is the ﬁrst to introduce the idea of sparse training to enable efﬁcient CL on edge devices. 2• SparCL shows superior performance compared to both conventional CL methods and CL-adapted sparse training methods on all benchmark datasets, leading to at most 23×less training FLOPs and, surprisingly, 1.7% improvement over SOTA accuracy. • We evaluate SparCL on a real mobile edge device, demonstrating the practical potential of our method and also encouraging future research on CL on-the-edge. The results indicate that our framework can achieve at most 3.1×training acceleration. 2 Related work 2.1 Continual Learning The main focus in continual learning (CL) has been mitigating catastrophic forgetting. Existing methods can be classiﬁed into three major categories. Regularization-based methods (2; 31; 36; 67) limit updates of important parameters for the prior tasks by adding corresponding regularization terms. While these methods reduce catastrophic forgetting to some extent, their performance deteriorates under challenging settings ( 39), and on more complex benchmarks ( 49; 60). Rehearsal-based methods (12; 13; 24) save examples from previous tasks into a small-sized buffer to train the model jointly with the current task. Though simple in concept, the idea of rehearsal is very effective in practice and has been adopted by many state-of-the-art methods ( 8; 10; 48). Architecture-based methods (41; 50; 56; 58; 62) isolate existing model parameters or assign additional parameters for each task to reduce interference among tasks. As mentioned in Section 1, most of these methods use a dense model without consideration of efﬁciency and memory footprint, thus are not applicable to resource-limited settings. Our work, orthogonal to these methods, serves as a general framework for making these existing methods efﬁcient and enabling a broader deployment, e.g., CL on edge devices. A limited number of works explore sparsity in CL, however, for different purposes. Several methods (40; 41; 52; 56) incorporate the idea of weight pruning ( 23) to allocate a sparse sub-network for each task to reduce inter-task interference. Nevertheless, these methods still reduce the full model sparsity progressively for every task and ﬁnally end up with a much denser model. On the contrary, SparCL maintains a sparse network throughout the whole CL process, introducing great efﬁciency and memory beneﬁts both during training and at the output model. A recent work ( 14) aims at discovering lottery tickets (20) under CL, but still does not address efﬁciency. However, the existence of lottery tickets in CL serves as a strong justiﬁcation for the outstanding performance of our SparCL. 2.2 Sparse Training There are two main approaches for sparse training: ﬁxed-mask sparse training and dynamic sparse training. Fixed-mask sparse training methods ( 34; 53; 55; 59) ﬁrst apply pruning, then execute traditional training on the sparse model with the obtained ﬁxed mask. The pre-ﬁxed structure limits the accuracy performance, and the ﬁrst stage still causes huge computation and memory consumption. To overcome these drawbacks, dynamic mask methods (4; 16; 19; 44; 45) adjust the sparsity topology during training while maintaining low memory footprint. These methods start with a sparse model structure from an untrained dense model, then combine sparse topology exploration at the given sparsity ratio with the sparse model training. Recent work (66) further considers to incorporate data efﬁciency into sparse training for better training accelerations. However, all prior sparse training works are focused on the traditional training setting, while CL is a more complicated and difﬁcult scenario with inherent characteristics not explored by these works. In contrast to prior sparse training methods, our work explores a new learning paradigm that introduces sparse training into CL for efﬁciency and also addresses key challenges in CL, mitigating catastrophic forgetting. 3 Continual Learning Problem Setup In supervised CL, a model fθ learns from a sequence of tasks D= {D1,..., DT}, where each task Dt = {(xt,i,yt,i)}nt i=1 consists of input-label pairs, and each task has a disjoint set of classes. Tasks arrive sequentially, and the model must adapt to them. At the t-th step, the model gains access to data from the t-th task. However, a small ﬁx-sized rehearsal buffer Mis allowed to save data from prior tasks. At test time, the easiest setting is to assume task identity is known for each coming test example, named task-incremental learning (Task-IL). If this assumption does not hold, we have the 3...... T ask-aware Dynamic Masking  (TDM)  (a) -and- Expand Shrink Dynamic Data Removal  (DDR) Dynamic Gradient Masking  (DGM) Remove  “easier” samples Update  important gradients  (b) -and- Shrink Expand T ask 1 T ask t (a) TDM DDR DGM (b) T ask T    Epochs Figure 2: Illustration of the SparCL workﬂow. Three components work synergistically to improve training efﬁciency and further mitigate catastrophic forgetting for preserving accuracy. more difﬁcult class-incremental learning (Class-IL) setting. In this work, we mainly focus on the more challenging Class-IL setting, and only report Task-IL performance for reference. The goal of conventional CL is to train a model sequentially that performs well on all tasks at test time. The main evaluation metric is average test accuracy on all tasks. In real-world resource- limited scenarios, we should further consider training efﬁciency of the model. Thus, we measure the performance of the model more comprehensively by including training FLOPs and memory footprint. 4 Sparse Continual Learning (SparCL) Our method, Sparse Continual Learning, is a uniﬁed framework composed of three complementary components: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. The entire framework is shown in Figure 2. We will illustrate each component in detail in this section. 4.1 Task-aware Dynamic Masking To enable cost-effective CL in resource limited scenarios, SparCL is designed to maintain a dynamic structure when learning a sequence of tasks, such that it not only achieves high efﬁciency, but also intelligently adapts to the data stream for better performance. Speciﬁcally, we propose a strategy named task-aware dynamic masking (TDM), which dynamically removes less important weights and grows back unused weights for stronger representation power periodically by maintaining a single binary weight mask throughout the CL process. Different from typical sparse training work, which only leverages the weight magnitude ( 44) or the gradient w.r.t. data from a single training task (19; 66), TDM considers also the importance of weights w.r.t. data saved in the rehearsal buffer, as well as the switch between CL tasks. Speciﬁcally, TDM strategy starts from a randomly initialized binary mask Mθ = M0, with a given sparsity constraint ∥Mθ∥0/∥θ∥0 = 1−s, where s∈[0,1] is the sparsity ratio. Moreover, it makes different intra- and inter-task adjustments to keep a dynamic sparse set of weights based on their continual weight importance (CWI). We summarize the process of task-aware dynamic masking in Algorithm 1 and elaborate its key components in detail below. Continual weight importance (CWI).For a model fθ parameterized by θ, the CWI of weight w⊂θis deﬁned as follows: CWI(w) =∥w∥1 + α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1, (1) where Dt denotes the training data from the t-th task, Mis the current rehearsal buffer, and α, βare coefﬁcients to control the inﬂuence of current and buffered data, respectively. Moreover,Lrepresents the cross-entropy loss for classiﬁcation, while ˜Lis the single-head (1) version of the cross-entropy loss, which only considers classes from the current task by masking out the logits of other classes. Intuitively, CWI ensures we keep (1) weights of larger magnitude for output stability, (2) weights important for the current task for learning capacity, and (3) weights important for past data to mitigate catastrophic forgetting. Moreover, inspired by the classiﬁcation bias in CL (1), we use the single-head cross-entropy loss when calculating importance score w.r.t. the current task to make the importance estimation more accurate. 4Algorithm 1:Task-aware Dynamic Masking (TDM) Input: Model weight θ, number of tasks T, training epochs of the t-th task Kt, binary sparse mask Mθ, sparsity ratio s, intra-task adjustment ratio pintra, inter-task adjustment ratio pinter, update interval δk Initialize: θ, Mθ, s.t. ∥Mθ∥0/∥θ∥0 = 1−s for t= 1,...,T do for e= 1,...,K T do if t> 1 then /* Inter-task adjustment */ Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s−pinter) if e= δkthen Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end end if e mod δk = 0then /* Intra-task adjustment */ Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s+ pintra) Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end Update θ⊙Mθ via backpropagation end end Intra-task adjustment.When training the t-th task, a natural assumption is that the data distribution is consistent inside the task, thus we would like to update the sparse model in a relatively stable way while keeping its ﬂexibility. Thus, in Algorithm 1, we choose to update the sparsity mask Mθ in a shrink-and-expand way every δkepochs. We ﬁrst remove pintra of the weights of least CWI to retain learned knowledge so far. Then we randomly select unused weights to recover the learning capacity for the model and keep the sparsity ratio sunchanged. Inter-task adjustment.When tasks switches, on the contrary, we assume data distribution shifts immediately. Ideally, we would like the model to keep the knowledge learned from old tasks as much as possible, and to have enough learning capacity to accommodate the new task. Thus, instead of the shrink-and-expand strategy for intra-task adjustment, we follow an expand-and-shrink scheme. Speciﬁcally, at the beginning of the (t+ 1)-th task, we expand the sparse model by randomly adding a proportion of pinter unused weights. Intuitively, the additional learning capacity facilitates fast adoption of new knowledge and reduces interference with learned knowledge. We allow our model to have smaller sparsity (i.e., larger learning capacity) temporarily for the ﬁrst δk epochs as a warm- up period, and then remove the pinter weights with least CWI, following the same process in the intra-task case, to satisfy the sparsity constraint. 4.2 Dynamic Data Removal In addition to weight sparsity, decreasing the amount of training data can be directly translated into the saving of training time without any requirements for hardware support. Thus, we would also like to explore data efﬁciency to reduce the training workload. Some prior CL works select informative examples to construct the rehearsal buffer ( 3; 6; 64). However, the main purpose of them is not training acceleration, thus they either introduce excessive computational cost or consider different problem settings. By considering the features of CL, we present a simple yet effective strategy, dynamic data removal (DDR), to reduce training data for further acceleration. We measure the importance of each training example by the occurrence of misclassiﬁcation (54; 66) during CL. In TDM, the sparse structure of our model updates periodically every δkepochs, so we align our data removal process with the update of weight mask for further efﬁciency and training 5stability. In Section 4.1, we have partitioned the training process for the t-th task into Nt = Kt/δk stages based on the dynamic mask update. Therefore, we gradually remove training data at the end of i-th stage, based on the following policy: 1) Calculate the total number of misclassiﬁcations fi(xj) for each training example during the i-th stage. 2) Remove a proportion of ρi training samples with the least number of misclassiﬁcations. Although our main purpose is to keep the “harder” examples to learn to consolidate the sparse model, we can get further beneﬁts for better CL result. First, the removal of “easier” examples increases the probability that “harder” examples to be saved to the rehearsal buffer, given the common strategy,e.g. reservoir sampling (13), to buffer examples. Thus, we construct a more informative buffer in a implicit way without heavy computation. Moreover, since the buffer size is much smaller than the training set size of each task, the data from the buffer and the new task is highly imbalanced, dynamic data removal also relieves the data imbalance issue. Formally, we set the data removal proportion for each task as ρ∈[0,1], and a cutoff stage, such that: cutoff∑ i=1 ρi = ρ, Nk∑ i=cutoff+1 ρi = 0 (2) The cutoff stage controls the trade-off between efﬁciency and accuracy: When we set the cutoff stage earlier, we reduce the training time for all the following stages; however, when the cutoff stage is set too early, the model might underﬁt the removed training data. Note that when we set ρi = 0for all i= 1,2,...,N t and cutoff = Nt, we simply recover the vanilla setting without any data efﬁciency considerations. In our experiments, we assume ρi = ρ/cutoff, i.e., removing equal proportion of data at the end of every stage, for simplicity. We also conduct comprehensive exploration study for ρ and the selection of the cutoff stage in Section 5.3 and Appendix D.3. 4.3 Dynamic Gradient Masking With TDM and DDR, we can already achieve bi-level efﬁciency during training. To further boost training efﬁciency, we explore sparsity in gradient and propose dynamic gradient masking (DGM) for CL. Our method focuses on reducing computational cost by only applying the most important gradients onto the corresponding unpruned model parameters via a gradient mask. The gradient mask is also dynamically updated along with the weight mask deﬁned in Section 4.1. Intuitively, while targeting for better training efﬁciency, DGM also promotes the preservation of past knowledge by preventing a fraction of weights from update. Formally, our goal here is to ﬁnd a subset of unpruned parameters (or, equivalently, a gradient mask MG) to update over multiple training iterations. For a model fθ parameterized by θ, we have the corresponding gradient matrix Gcalculated during each iteration. To prevent the pruned weights from updating, the weight mask Mθ will be applied onto the gradient matrix Gas G⊙Mθ during backpropagation. Besides the gradients of pruned weights, we in addition consider to remove less important gradients for faster training. To achieve this, we introduce the continual gradient importance (CGI) based on the CWI to measure the importance of weight gradients. CGI(w) =α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1. (3) We remove a proportion q of non-zero gradients from Gwith less importance measured by CGI and we have ∥MG∥0/∥θ∥0 = 1−(s+ q). The gradient mask MG is then applied onto the gradient matrix G. During the entire training process, the gradient mask MG is updated with a ﬁxed interval. 5 Experiment 5.1 Experiment Setting Datasets. We evaluate our SparCL on two representative CL benchmarks, Split CIFAR-10 ( 32) and Split Tiny-ImageNet (15) to verify the efﬁcacy of SparCL. In particular, we follow (8; 67) by splitting CIFAR-10 and Tiny-ImageNet into 5 and 10 tasks, each of which consists of 2 and 20 classes respectively. Dataset licensing information can be found in Appendix C. Comparing methods. In particular, we select CL methods of all kinds including regularization- based (EWC ( 31), LwF ( 36)), architecture-based (PackNet ( 41), LPS ( 56)), and rehearsal-based 6Table 1: Comparison with CL methods. SparCL consistently improves training efﬁciency of the corresponding CL methods while preserves (or even improves) accuracy on both class- and task-incremental settings. Method Sparsity Buffer size Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) Task-IL (↑) FLOPs Train×1015(↓) Class-IL (↑) Task-IL (↑) FLOPs Train×1016(↓) EWC (31) 0.00 – 19.49±0.12 68.29±3.92 8.3 7.58±0.10 19.20±0.31 13.3LwF (36) 19.61±0.05 63.29±2.35 8.3 8.46±0.22 15.85±0.58 13.3 PackNet (41) 0.50† – - 93.73 ±0.55 5.0 – 61.88 ±1.01 7.3LPS (56) - 94.50 ±0.47 5.0 – 63.37 ±0.83 7.3 A-GEM (12) 0.00 200 20.04±0.34 83.88±1.49 11.1 8.07±0.08 22.77±0.03 17.8iCaRL (49) 49.02±3.20 88.99±2.13 11.1 7.53±0.79 28.19±1.47 17.8FDR (5) 30.91±2.74 91.01±0.68 13.9 8.70±0.19 40.36±0.68 22.2ER (13) 44.79±1.86 91.19±0.94 11.1 8.49±0.16 38.17±2.00 17.8DER++ (8) 64.88±1.17 91.92±0.60 13.9 10.96±1.17 40.87±1.16 22.2 SparCL-ER75 46.89±0.68 92.02±0.72 2.0 8.98±0.38 39.14±0.85 3.2SparCL-DER++75 0.75 66.30±0.98 94.06±0.45 2.5 12.73±0.40 42.06±0.73 4.0SparCL-ER90 45.81±1.05 91.49±0.47 0.9 8.67±0.41 38.79±0.39 1.4SparCL-DER++90 0.90 200 65.79±1.33 93.73±0.24 1.1 12.27±1.06 41.17±1.31 1.8SparCL-ER95 44.59±0.23 91.07±0.64 0.5 8.43±0.09 38.20±0.46 0.8SparCL-DER++95 0.95 65.18±1.25 92.97±0.37 0.6 10.76±0.62 40.54±0.98 1.0 A-GEM (12) 0.00 500 22.67±0.57 89.48±1.45 11.1 8.06±0.04 25.33±0.49 17.8iCaRL (49) 47.55±3.95 88.22±2.62 11.1 9.38±1.53 31.55±3.27 17.8FDR (5) 28.71±3.23 93.29±0.59 13.9 10.54±0.21 49.88±0.71 22.2ER (13) 57.74±0.27 93.61±0.27 11.1 9.99±0.29 48.64±0.46 17.8DER++ (8) 72.70±1.36 93.88±0.50 13.9 19.38±1.41 51.91±0.68 22.2 SparCL-ER75 60.80±0.22 93.82±0.32 2.0 10.48±0.29 50.83±0.69 3.2SparCL-DER++75 0.75 74.09±0.84 95.19±0.34 2.5 20.75±0.88 52.19±0.43 4.0SparCL-ER90 59.34±0.97 93.33±0.10 0.9 10.12±0.53 49.46±1.22 1.4SparCL-DER++90 0.90 500 73.42±0.95 94.82±0.23 1.1 19.62±0.67 51.93±0.36 1.8SparCL-ER95 57.75±0.45 92.73±0.34 0.5 9.91±0.17 48.57±0.50 0.8SparCL-DER++95 0.95 72.14±0.78 94.39±0.15 0.6 19.01±1.32 51.26±0.78 1.0 †PackNet and LPS actually have a decreased sparsity after learning every task, we use 0.50 to roughly represent the average sparsity. (A-GEM (12), iCaRL (43), FDR (5), ER (13), DER++ (8)) methods. Note that PackNet and LPS are only compatible with task-incremental learning. We also adapt representative sparse training methods (SNIP (34), RigL (19)) to the CL setting by combining them with DER++ (SNIP-DER++, RigL-DER++). Variants of our method.To show the generality of SparCL, we combine it with DER++ (one of the SOTA CL methods), and ER (simple and widely-used) as SparCL-DER++ and SparCL-ER, respectively. We also vary the weight sparsity ratio (0.75,0.90,0.95) of SparCL for a comprehensive evaluation. Evaluation metrics.We use the average accuracy on all tasks to evaluate the performance of the ﬁnal model. Moreover, we evaluate the training FLOPs (19), and memory footprint (66) (including feature map pixels and model parameters during training) to demonstrate the efﬁciency of each method. Please see Appendix D.1 for detailed deﬁnitions of these metrics. Experiment details.For fair comparison, we strictly follow the settings in prior CL work (8; 28). We sets the per task training epochs to 50 and 100 for Split CIFAR-10 and Tiny-ImageNet, respectively, with a batch size of 32. For the model architecture, We follow (8; 49) and adopt the ResNet-18 (25) without any pre-training. We also use the best hyperparameter setting reported in ( 8; 56) for CL methods, and in (19; 34) for CL-adapted sparse training methods. For SparCL and its competing CL-adapted sparse training methods, we adopt a uniform sparsity ratio for all convolutional layers. Please see Appendix D for other details. 5.2 Main Results Comparison with CL methods.Table 1 summarizes the results on Split CIFAR-10 and Tiny- ImageNet, under both class-incremental (Class-IL) and task-incremental (Task-IL) settings. From Table 1, we can clearly tell that SparCL signiﬁcantly improves ER and DER++, while also outperforms other CL baselines, in terms of training efﬁciency (measured in FLOPs). With higher sparsity ratio, SparCL leads to less training FLOPs. Notably, SparCL achieves23×training efﬁciency improvement upon DER++ with a sparsity ratio of 0.95. On the other hand, our framework also improves the average accuracy of ER and DER++ consistently under all cases with a sparsity ratio of 0.75 and 0.90, and only slight performance drop when sparsity gets larger as 0.95. In particular, SparCL-DER++ 7Table 2: Comparison with CL-adapted sparse training methods. All methods are combined with DER++ with a 500 buffer size. SparCL outperforms all methods in both accuracy and training efﬁciency, under all sparsity ratios. All three methods here can save 20% ∼ 51% memory footprint, please see Appendix D.2 for details. Method Spasity Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) FLOPs Train ×1015(↓) Class-IL (↑) FLOPs Train ×1016(↓) DER++ (8) 0.00 72.70±1.36 13.9 19.38±1.41 22.2 SNIP-DER++ (34) 69.82±0.72 1.6 16.13±0.61 2.5RigL-DER++ (19)0.90 69.86±0.59 1.6 18.36±0.49 2.5SparCL-DER++90 73.42±0.95 1.1 19.62±0.67 1.8 SNIP-DER++ (34) 66.07±0.91 0.9 14.76±0.52 1.5RigL-DER++ (19)0.95 66.53±1.13 0.9 15.88±0.63 1.5SparCL-DER++95 72.14±0.78 0.6 19.01±1.32 1.0 Table 3: Ablation study on Split-CIFAR10 with 0.75 sparsity ratio. All components contributes to the overall performance, in terms of both accuracy and efﬁciency (training FLOPs and memory footprint). TDM DDR DGMClass-IL (↑) FLOPs Train ×1015(↓) MemoryFootprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB\u0013 \u0017 \u0017 73.37 3.6 180MB\u0013 \u0013 \u0017 73.80 2.8 180MB\u0013 \u0017 \u0013 73.97 3.3 177MB\u0013 \u0013 \u0013 74.09 2.5 177MB with 0.75 sparsity ratio sets new SOTA accuracy, with all buffer sizes under both benchmarks. The outstanding performance of SparCL indicates that our proposed strategies successfully preserve accuracy by further mitigating catastrophic forgetting with a much sparser model. Moreover, the improvement that SparCL brings to two different existing CL methods shows the generalizability of SparCL as a uniﬁed framework, i.e., it has the potential to be combined with a wide array of existing methods. We would also like to take a closer look at PackNet and LPS, which also leverage the idea of sparsity to split the model by different tasks, a different motivation from training efﬁciency. Firstly, they are only compatible with the Task-IL setting, since they leverage task identity at both training and test time. Moreover, the model sparsity of these methods reduces with the increasing number of tasks, which still leads to much larger overall training FLOPs than that of SparCL. This further demonstrates the importance of keeping a sparse model without permanent expansion throughout the CL process. Comparison with CL-adapted sparse training methods.Table 2 shows the result under the more difﬁcult Class-IL setting. SparCL outperforms all CL-adapted sparse training methods in both accuracy and training FLOPs. The performance gap between SparCL-DER++ and other methods gets larger with a higher sparsity. SNIP- and RigL-DER++ achieve training acceleration at the cost of compromised accuracy, which suggests that keeping accuracy is a non-trivial challenge for existing sparse training methods under the CL setting. SNIP generates the static initial mask after network initialization which does not consider the structure suitability among tasks. Though RigL adopts a dynamic mask, the lack of task-aware strategy prevents it from generalizing well to the CL setting. 5.3 Effectiveness of Key Components Ablation study.We provide a comprehensive ablation study in Table 3 using SparCL-DER++ with 0.75 sparsity on Split CIFAR10. Table 3 demonstrates that all components of our method contribute to both efﬁciency and accuracy improvements. Comparing row 1 and 2, we can see that the majority of FLOPs decrease results from TDM. Interestingly, TDM leads to an increase in accuracy, indicating TDM generates a sparse model that is even more suitable for learning all tasks than then full dense model. Comparing row 2 and 3, we can see that DDR indeed further accelerates training by removing less informative examples. As discussed in Section 4.2, when we remove a certain number of samples (30% here), we achieve a point where we keep as much informative samples as we need, and also balance the current and buffered data. Comparing row 2 and 4, DGM reduce both training FLOPs and memory footprint while improve the performance of the network. Finally, the last row demonstrates the collaborative performance of all components. We also show the same ablation study with 0.90 sparsity in Appendix D.4 for reference. Detail can be found in Appendix D.1. 8Figure 3: Comparison between DDR and One- shot (66) data removal strategy w.r.t. different data removal proportion ρ. DDR outperforms One-shot and also achieves better accuracy when ρ ≤ 30%. Figure 4: Comparison with CL-adapted sparse training methods in training acceleration rate and accuracy results. The radius of circles are measured by memory footprint. Exploration on DDR.To understand the inﬂuence of the data removal proportionρ, and the cutoff stage for each task, we show corresponding experiment results in Figure 3 and Appendix D.3, respectively. In Figure 3, we ﬁx cutoff = 4, i.e., gradually removing equal number of examples every 5 epochs until epoch 20, and vary ρfrom 10% to 90%. We also compare DDR with One-shot removal strategy (66), which removes all examples at once at cutoff. DDR outperforms One-shot consistently with different ρin average accuracy. Also note that since DDR removes the examples gradually before the cutoff stage, DDR is more efﬁcient than One-shot. When ρ≤30%, we also observe increased accuracy of DDR compared with the baseline without removing any data. When ρ≥40%, the accuracy gets increasingly lower for both strategies. The intuition is that when DDR removes a proper amount of data, it removes redundant information while keeps the most informative examples. Moreover, as discussed in Section 4.2, it balances the current and buffered data, while also leave informative samples in the buffer. When DDR removes too much data, it will also lose informative examples, thus the model has not learned these examples well before removal. Exploration on DGM.We test the efﬁcacy of DGM at different sparsity levels. Detailed exploratory experiments are shown in Appendix D.5 for reference. The results indicate that by setting the proportion qwithin an appropriate range, DGM can consistently improve the accuracy performance regardless of the change of weight sparsity. 5.4 Mobile Device Results The training acceleration results are measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone, which has the Qualcomm Snapdragon 865 mobile platform with a Qualcomm Kryo 585 Octa-core CPU. We run each test on a batch of 32 images to denote the training speed. The detail of on-mobile compiler-level optimizations for training acceleration can be found in Appendix E.1. The acceleration results are shown in Figure 4. SparCL can achieve approximately 3.1×and 2.3× training acceleration with 0.95 sparsity and 0.90 sparsity, respectively. Besides, our framework can also save 51% and 48% memory footprint when the sparsity is 0.95 and 0.90. Furthermore, the obtained sparse models save the storage consumption by using compressed sparse row (CSR) storage and can be accelerated to speed up the inference on-the-edge. We provide on-mobile inference acceleration results in Appendix E.2. 6 Conclusion This paper presents a uniﬁed framework named SparCL for efﬁcient CL that achieves both learning acceleration and accuracy preservation. It comprises three complementary strategies: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. Extensive experiments on standard CL benchmarks and real-world edge device evaluations demonstrate that our method signiﬁcantly improves upon existing CL methods and outperforms CL-adapted sparse training methods. We discuss the limitations and potential negative social impacts of our method in Appendix A and B, respectively. 9References [1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In CVPR, 2021. 4 [2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte- laars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. 1, 3 [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. NeurIPS, 2019. 5 [4] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. In ICLR, 2018. 2, 3 [5] Ari S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in function space. arXiv preprint arXiv:1805.08289, 2018. 7 [6] Zalán Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. NeurIPS, 2020. 5 [7] Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garﬁnkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artiﬁcial intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018. 15 [8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In NeurIPS, 2020. 1, 2, 3, 6, 7, 8 [9] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678, 2016. 16 [10] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In ICCV, 2021. 3 [11] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip Torr, and David Lopez-Paz. Us- ing hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 2(7), 2020. 1 [12] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018. 3, 7 [13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019. 3, 6, 7 [14] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Long live the lottery: The existence of winning tickets in lifelong learning. In ICLR, 2020. 3 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR. Ieee, 2009. 2, 6 [16] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019. 3 [17] Peiyan Dong, Siyue Wang, Wei Niu, Chengming Zhang, Sheng Lin, Zhengang Li, Yifan Gong, Bin Ren, Xue Lin, and Dingwen Tao. Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition. In DAC, pages 1–6. IEEE, 2020. 17 [18] Xuanyi Dong and Yi Yang. Network pruning via transformable architecture search. In NeurIPS, pages 759–770, 2019. 1 [19] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In ICML, pages 2943–2952. PMLR, 2020. 2, 3, 4, 7, 8, 16 10[20] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2019. 3 [21] Yifan Gong, Geng Yuan, Zheng Zhan, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia Liu, Bin Ren, Xue Lin, et al. Automatic mapping of the best-suited dnn pruning schemes for real-time mobile acceleration. ACM Transactions on Design Automation of Electronic Systems (TODAES), 27(5):1–26, 2022. 18 [22] Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, et al. A privacy-preserving-oriented dnn pruning and mobile acceleration framework. In GLSVLSI, pages 119–124, 2020. 17 [23] Song Han, Jeff Pool, et al. Learning both weights and connections for efﬁcient neural network. In NeurIPS, pages 1135–1143, 2015. 3 [24] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efﬁcient experience replay for streaming learning. In ICRA, 2019. 3 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 7 [26] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. 17 [27] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In CVPR, pages 4340–4349, 2019. 17 [28] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating con- tinual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. 7 [29] Tong Jian, Yifan Gong, Zheng Zhan, Runbin Shi, Nasim Soltani, Zifeng Wang, Jennifer G Dy, Kaushik Roy Chowdhury, Yanzhi Wang, and Stratis Ioannidis. Radio frequency ﬁngerprinting on the edge. IEEE Transactions on Mobile Computing, 2021. 17 [30] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar and dissimilar tasks. NeurIPS, 2020. 1 [31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017. 1, 3, 6, 7 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf, 2009. 6, 15 [33] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 15 [34] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. ICLR, 2019. 2, 3, 7, 8 [35] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing convolutional neural networks via factorized convolutional ﬁlters. In CVPR, pages 3977–3986, 2019. 17 [36] Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 40(12):2935–2947, 2017. 1, 3, 6, 7 [37] Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, and Yanzhi Wang. Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execution on mobile devices. In AAAI, pages 5117–5124, 2020. 17 11[38] Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie Wen, Xiang Chen, Jian Tang, Kaisheng Ma, et al. An image enhancing pattern-based sparsity for real-time inference on mobile devices. In ECCV, pages 629–645. Springer, 2020. 1 [39] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classiﬁcation: An empirical survey. arXiv preprint arXiv:2101.10423, 2021. 3 [40] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 67–82, 2018. 3 [41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR, 2018. 1, 3, 6, 7 [42] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989. 1 [43] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. ICML Workshop, 2021. 7 [44] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018. 3, 4 [45] Hesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural networks by dynamic sparse reparameterization. In ICML, pages 4646–4655. PMLR, 2019. 3 [46] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight pruning. arXiv preprint arXiv:2001.00138, 2020. 18 [47] Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Grafﬁeti, and Davide Maltoni. Con- tinual learning at the edge: Real-time training on smartphone devices. arXiv preprint arXiv:2105.13127, 2021. 1 [48] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. NeurIPS, 2021. 3 [49] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In CVPR, pages 2001–2010, 2017. 1, 3, 7 [50] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. 1, 3 [51] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, 2018. 1 [52] Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Spacenet: Make free space for continual learning. Neurocomputing, 439:1–11, 2021. 3 [53] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. NeurIPS, 33:6377–6389, 2020. 3 [54] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. 5 [55] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient ﬂow. In ICLR, 2019. 3 [56] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis Ioannidis. Learn-prune-share for lifelong learning. In ICDM, 2020. 1, 3, 6, 7 12[57] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. ECCV, 2022. 1 [58] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to prompt for continual learning.CVPR, 2022. 1, 3 [59] Paul Wimmer, Jens Mehnert, and Alexandru Condurache. Freezenet: Full performance by reduced storage costs. In ACCV, 2020. 3 [60] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In CVPR, pages 374–382, 2019. 1, 3 [61] Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, and Yanzhi Wang. Compiler-aware neural architecture search for on-mobile real-time super-resolution. ECCV, 2022. 18 [62] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In CVPR, pages 3014–3023, 2021. 3 [63] Li Yang, Sen Lin, Junshan Zhang, and Deliang Fan. Grown: Grow only when necessary for continual learning. arXiv preprint arXiv:2110.00908, 2021. 1 [64] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang. Online coreset selection for rehearsal-based continual learning. arXiv preprint arXiv:2106.01085, 2021. 5 [65] Kun-Hsing Yu, Andrew L Beam, and Isaac S Kohane. Artiﬁcial intelligence in healthcare. Nature biomedical engineering, 2(10):719–731, 2018. 15 [66] Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse training framework on the edge. NeurIPS, 34, 2021. 3, 4, 5, 7, 9, 16, 17 [67] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 1, 3, 6 [68] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, et al. Achieving on-mobile real-time super-resolution with neural architecture and pruning search. In ICCV, pages 4821–4831, 2021. 17 [69] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised lifelong learning. Neural Networks, 149:95–106, 2022. 1 Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] The claims match the experimental results and it is expected to generalize according to the diverse experiments stated in our paper. We include all of our code, data, and models in the supplementary materials, which can reproduce our experimental results. (b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6 and Appendix B. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read the ethics review guidelines and ensured that our paper conforms to them. 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 13(b) Did you include complete proofs of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] See Section 5.1, Section 5.4 and we provide code to reproduce the main experimental results. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5.1 and Section 5.4. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See Table 1, Table 2, ﬁg 1, ﬁg 3. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.1, Section 5.4. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We mentioned and cited the datasets (Split CIFAR-10 and Tiny-ImageNet), and all comparing methods with their paper and github in it. (b) Did you mention the license of the assets? [Yes] The licences of used datasets/models are provided in the cited references and we state them explicitly in Appendix C. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We provide code for our proposed method in the supplement. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Limitations One limitation of our method is that we assume a rehearsal buffer is available throughout the CL process. Although the assumption is widely-accepted, there are still situations that a rehearsal buffer is not allowed. However, as a framework targeting for efﬁciency, our work has the potential to accelerate all types of CL methods. For example, simply removing the terms related to rehearsal buffer in equation 1 and equation 3 could serve as a naive variation of our method that is compatible with other non-rehearsal methods. It is interesting to further improve SparCL to be more generic for all kinds of CL methods. Moreover, the benchmarks we use are limited to vision domain. Although using vision-based benchmarks has been a common practice in the CL community, we believe evaluating our method, as well as other CL methods, on datasets from other domains such as NLP will lead to a more comprehensive and reliable conclusion. We will keep track of newer CL benchmarks from different domains and further improve our work correspondingly. B Potential Negative Societal Impact Although SparCL is a general framework to enhance efﬁciency for various CL methods, we still need to be aware of its potential negative societal impact. For example, we need to be very careful about the trade-off between accuracy and efﬁciency when using SparCL. If one would like to pursue efﬁciency by setting the sparsity ratio too high, then even SparCL will result in signiﬁcant accuracy drop, since the over-sparsiﬁed model does not have enough representation power. Thus, we should pay much attention when applying SparCL on accuracy-sensitive applications such as healthcare (65). Another example is that, SparCL as a powerful tool to make CL methods efﬁcient, can also strengthen models for malicious applications ( 7). Therefore, we encourage the community to come up with more strategies and regulations to prevent malicious use of artiﬁcial intelligence. C Dataset Licensing Information • CIFAR-10 (32) is licensed under the MIT license. • The licensing information of Tiny-ImageNet ( 33) is not available. However, the data is available for free to researchers for non-commercial use. D Additional Experiment Details and Results We set α = 0.5,β = 1 in equation 1 and equation 3. We also set δk = 5, pinter = 0.01, pintra = 0.005. We also match different weight sparsity with gradient sparsity for best performance. We sample 20% data from Split CIFAR-10 training set for validation, and we use grid-search on this validation set to help us select the mentioned best hyperparameters. We use the same set of hyperparameters for both datasets. For accurate evaluation, we repeat each experiments 3 times using different random seeds and report the average performance. During our experiments, we adopt unstructured sparsity type and uniform sparsity ratio (0.75,0.90,0.95) for all convolutional layers in the models. D.1 Evaluation Metrics Explanation Training FLOPsThe FLOPs of a single forward pass is calculated by taking the sum of the number of multiplications and additions in each layer lfor a given layer sparsity sl. Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. The goal of the forward pass is to calculate the loss of the current set of parameters on a given batch of data. It can be formulated as al = σ(zl) =σ(wl ∗al−1 + bl) for each layer lin the model. Here, w, b, and zrepresent the weights, biases, and output before activation, respectively; σ(.) denotes the activation function; ais the activations; ∗means convolution operation. The formulation indicates that the layer activations are calculated in sequence using the previous activations and the parameters of the layer. Activation of layers are stored in memory for the backward pass. 15As for the backward propogation, the objective is to back-propagate the error signal while calculating the gradients of the parameters. The two main calculation steps can be represented as: δl = δl+1 ∗rotate180°(wl) ⊙σ′(zl), (4) Gl = al−1 ∗δl, (5) where δl is the error associated with the layer l, Gl denotes the gradients, ⊙represents Hadamard product, σ′(.) denotes the derivative of activation, androtate180°(.) means rotating the matrix by180° is the matrix transpose operation. During the backward pass, each layer lcalculates two quantities, i.e., the gradient of the activations of the previous layer and the gradient of its parameters. Thus, the backward passes are counted astwice the computation expenses of the forward pass (19). We omit the FLOPs needed for batch normalization and cross entropy. In our work, the total FLOPs introduced by TDM, DDR, and DGM on split CIFAR-10 is approximately 4.5 ×109 which is less than 0.0001% of total training FLOPs. For split Tiny-ImageNet, the total FLOPs of them is approximately 1.8 ×1010, which is also less than 0.0001% of total training FLOPs. Therefore, the computation introduced by TDM, DDR, and DGM is negligible. Memory FootprintsFollowing works ( 9; 66), the deﬁnition of memory footprints contain two parts: 1) activations (feature map pixels) during training phase, and 2) model parameters during training phase. For experiments, activations, model weights, and gradients are stored in 32-bit ﬂoating-point format for training. The memory footprint results are calculated with an approximate summation of them. D.2 Details of Memory Footprint The memory footprint is composed of three parts: activations, model weights, and gradients. They are all represented as bw-bit numbers for training. The number of activations in the model is the sum of the activations in each layer. Suppose that the output feature of the l-th layer with a batch size of Bis represented as al ∈RB×Ol×Hl×Wl , where Ol is the number of channels and Hl ×Wl is the feature size. The total number of activations of the model is thus B∑ lOlHlWl. As for the model weights, our SparCL training a sparse model with a sparsity ratio s∈[0,1] from scratch. The sparse model is obtained from a dense model with a total of N weights. A higher value of sindicates fewer non-zero weights in the sparse model. Compressed sparse row (CSR) format is commonly used for sparse storage, which greatly reduces the number of indices need to be stored for sparse matrices. As our SparCL adopt only one sparsity type and we use a low-bit format to store the indices, we omit the indices storage here. Therefore, the memory footprint for model representation is (1 −s)Nbw. Similar calculations can be applied for the gradient matrix. Besides the sparsity ratio s, additional q gradients are masked out from the gradient matrix, resulting a sparsity ratio s+ q. Therefore, the storage of gradients can be approximated as (1 −(s+ q))Nbw. Combining the activations, model representation, and gradients, the total memory footprint in SparCL can be represented as (2B∑ lOlHlWl + (1−s)N + (1−(s+ q))N)bw. DDR requires store indices for the easier examples during the training process. The number of training examples for Split CIFAR-10 and Split Tiny-ImageNet on each task is 10000. In our work, we only need about 3KB (remove 30% training data) for indices storage (in the int8 format) and the memory cost is negligible compared with the total memory footprint. D.3 Effect of Cutoff Stage Table A1: Effect of cutoff. cutoff 1 2 3 4 5 6 7 8 9 Class-IL (↑) 71.54 72.38 72.74 73.20 73.10 73.32 73.27 73.08 73.23 To evaluate the effect of the cutoff stage, we use the same setting as in Figure 3 by setting the sparsity ratio to 0.90. We keep the data removal proportion ρ = 30%, and only change cutoff. 16Table A1 shows the relationship between cutoff and the Class-IL average accuracy. Note that from the perspective of efﬁciency, we would like thecutoff stage as early as possible, so that the remaining epochs will have less examples. However, from Table A1, we can see that if we set it too early, i.e., cutoff ≤3, the accuracy drop is signiﬁcant. This indicate that even for the “easy-to-learn” examples, removing them too early results in underﬁtting. As a balance point between accuracy and efﬁciency, we choose cutoff = 4in our ﬁnal version. D.4 Supplementary Ablation Study Table A2: Ablation study on Split-CIFAR10 with 0.90 sparsity. TDM DDR DGM Class-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB \u0013 \u0017 \u0017 72.98 1.6 166MB \u0013 \u0013 \u0017 73.20 1.2 166MB \u0013 \u0017 \u0013 73.30 1.5 165MB \u0013 \u0013 \u0013 73.42 1.1 165MB Similar to Table 3, we show ablation study with 0.90 sparsity ratio in Table A2. Under a larger sparsity ratio, the conclusion that all components contribute to the ﬁnal performance still holds. However, we can observe that the accuracy increase that comes from DDR and DGM is less than what we show in Table 3. We assume that larger sparsity ratio makes it more difﬁcult for the model to retain good accuracy in CL. Similar results has also been observed in (66) under the usual i.i.d. learning setting. D.5 Exploration on DGM Table A3: Ablation study of the gradient sparsity ratio on Split-CIFAR10. weight sparsity gradient sparsityClass-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) 0.75 0.78 74.08 3.4 178MB 0.75 0.80 73.97 3.3 177MB 0.75 0.82 73.79 3.3 177MB 0.75 0.84 73.26 3.2 176MB 0.90 0.91 73.33 1.6 166MB 0.90 0.92 73.30 1.5 165MB 0.90 0.93 72.64 1.5 165MB We conduct further experiments to demonstrate the inﬂuence of gradient sparsity, and the results are shown in Table A3. There are two sets of the experiments with different weight sparsity settings: 0.75 and 0.90. Within each set of the experiments (the weight sparsity is ﬁxed), we vary the gradient sparsity values. From the results we can see that increasing the gradient sparsity can decrease the FLOPs and memory footprint. However, the accuracy performance degrades more obvious when the gradient sparsity is too much for the weight sparsity. The results indicate that suitable gradient sparsity setting can bring further efﬁciency to the training process while boosting the accuracy performance. In the main results, the gradient sparsity is set as 0.80 for 0.75 weight sparsity, and set as 0.92 for 0.90 weight sparsity. E On-Mobile Compiler Optimizations and Inference Results E.1 Compiler Optimizations Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. Prior works (17; 22; 26; 27; 29; 35; 37; 68) have proved that sparse weight matrices (tensors) can provide inference acceleration via reducing the number of multiplications in convolution operation. Therefore, the forward propagation phase, which is the same as inference, 17can be accelerated by the sparsity inherently. As for backward pass, both of the calculation steps are based on convolution, i.e., matrix multiplication. Equation 4 uses sparse weight matrix (tensor) as the operand, thus can be accelerated in the same way as the forward propagation. Equation 5 allows a sparse output result since the gradient matrix is also sparse. Thus, both two steps have reduced computations, which are roughly proportional to the sparsity ratio, providing the acceleration for the backward propagation phase. Compiler optimizations are used to accelerate the inference in prior works (21; 46; 61). In this work, we extend the compiler optimization techniques for accelerating the forward and backward pass during training on the edge devices. Our compiler optimizations are general, support both sparse model training and inference accelerations on mobile platforms. The optimizations include 1) the supports for sparse models; 2) an auto-tuning process to determine the best-suited conﬁgurations of parameters for different mobile CPUs. The details of our compiler optimizations are presented as follows. E.1.1 Supports for Sparse Models Our framework supports sparse model training and inference accelerations with unstructured pruning. For the sparse (pruned) model, the framework ﬁrst compacts the model storage with a compression format called Compressed Sparse Row (CSR) format, and then performs computation reordering to reduce the branches within each thread and eliminates the load imbalance among threads. A row reordering optimization is also included to further improve the regularity of the weight matrix. After this reordering, the continuous rows with identical or similar numbers of non-zero weights are processed by multi-threads simultaneously, thus eliminating thread divergence and achieving load balance. Each thread processes more than one rows, thus eliminating branches and improving instruction-level parallelism. Moreover, a similar optimization ﬂow (i.e., model compaction and computation reorder and other optimizations) is employed to support all compiler optimizations for sparsity as PatDNN (46). E.1.2 Auto-Tuning for Different Mobile CPUs During DNN sparse training and inference execution, there are many tuning parameters, e.g., matrix tiling sizes, loop unrolling factors, and data placement on memory, that inﬂuence the performance. It is hard to determine the best-suited conﬁguration of these parameters manually. To alleviate this problem, our compiler incorporates an auto-tuning approach for sparse (pruned) models. The Genetic Algorithm is leveraged to explore the best-suited conﬁgurations automatically. It starts the parameter search process with an arbitrary number of chromosomes and explores the parallelism better. Acceleration codes for different DNN models and different mobile CPUs can be generated efﬁciently and quickly through this auto-tuning process. E.2 Inference Acceleration Results On Mobile 60 12 18 2 4 A ccelation R esult s of R esNet -18 on  Split CIF AR-10 0 . 00 0 . 7 5 0 . 90 0 . 95 100 20 30 40 A ccelation R esult s of R esNet -18 on Split Tin y-ImageNet 0 . 00 0 . 7 5 0 . 90 0 . 95 Figure 5: Inference results of sparse models obtained from SparCL under different sparsity ratio compared with dense models obtained from traditional CL methods (sparsity ratio 0.00). Besides accelerating the training process, SparCL also possesses the advantages of providing a sparse model as the output for faster inference. To demonstrate this, we show the inference acceleration results of SparCL with different sparsity ratio settings on mobile in Figure 5. The inference time is measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone. Each test takes 50 runs on different inputs with 8 threads on CPU. As different runs do not vary greatly, only the average time is reported. From the results we can see that the obtained sparse model from SparCL can signiﬁcantly accelerate the inference on both Split-CIFAR-10 and Tiny-ImageNet dataset compared to the model obtained by traditional CL training. For ResNet-18 on Split-CIFAR-10, the model obtained by traditional CL training, which is a dense model, takes 18.53ms for inference. The model provided by SparCL can achieve an inference time of 14.01ms, 8.30ms, and 5.85ms with sparsity 18ratio of 0.75, 0.90, and 0.95, respectively. The inference latency of the dense ResNet-18 obtained by traditional CL training on Tiny-ImageNet is 39.64 ms. While the sparse models provided by SparCL with sparsity ratio settings as 0.75, 0.90, and 0.95 reach inference speed of 33.06ms, 20.37ms, and 15.49ms, respectively, on Tiny-ImageNet. 19",
      "meta_data": {
        "arxiv_id": "2209.09476v1",
        "authors": [
          "Zifeng Wang",
          "Zheng Zhan",
          "Yifan Gong",
          "Geng Yuan",
          "Wei Niu",
          "Tong Jian",
          "Bin Ren",
          "Stratis Ioannidis",
          "Yanzhi Wang",
          "Jennifer Dy"
        ],
        "published_date": "2022-09-20T05:24:48Z",
        "pdf_url": "https://arxiv.org/pdf/2209.09476v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces SparCL (Sparse Continual Learning), the first framework to leverage sparsity for cost-effective continual learning on resource-limited edge devices. SparCL addresses both training efficiency (speed and memory) and catastrophic forgetting, a challenge largely unexplored in prior CL work. It achieves significant training acceleration (up to 23x less FLOPs, 3.1x faster on mobile) and surprisingly improves state-of-the-art accuracy by up to 1.7% by integrating weight sparsity, data efficiency, and gradient sparsity.",
        "methodology": "SparCL comprises three complementary components: 1) Task-aware Dynamic Masking (TDM) learns and maintains a dynamic sparse network throughout the CL process. It uses a Continual Weight Importance (CWI) metric based on weight magnitude and gradients from both current and buffered past data. It applies intra-task (shrink-and-expand) and inter-task (expand-and-shrink) adjustments to the sparse mask. 2) Dynamic Data Removal (DDR) reduces training data by progressively removing less informative (easy-to-learn) examples based on misclassification frequency, which also helps balance current and buffered data and keeps more informative samples in the rehearsal buffer. 3) Dynamic Gradient Masking (DGM) further boosts efficiency by sparsifying gradient updates, applying only the most important gradients onto unpruned parameters based on a Continual Gradient Importance (CGI) metric, preventing a fraction of weights from updating.",
        "experimental_setup": "SparCL was evaluated on two representative CL benchmarks: Split CIFAR-10 (5 tasks, 2 classes each) and Split Tiny-ImageNet (10 tasks, 20 classes each), using ResNet-18 as the model architecture. Comparisons were made against various state-of-the-art CL methods (regularization-based: EWC, LwF; architecture-based: PackNet, LPS; rehearsal-based: A-GEM, iCaRL, FDR, ER, DER++) and CL-adapted sparse training methods (SNIP-DER++, RigL-DER++). Evaluation metrics included average test accuracy (Class-IL and Task-IL), training FLOPs, and memory footprint. Hyperparameters were selected via grid-search on a validation set. Practical evaluation was conducted on a real mobile device, a Samsung Galaxy S20 smartphone (Qualcomm Snapdragon 865 CPU), measuring training acceleration and inference speed.",
        "limitations": "One limitation is the assumption that a rehearsal buffer is always available throughout the CL process, which might not hold in all scenarios. While the framework has potential to adapt to non-rehearsal methods by modifying the importance metrics, this specific extension needs further improvement. Additionally, the benchmarks used are limited to the vision domain; evaluating SparCL on datasets from other domains like Natural Language Processing (NLP) is necessary for a more comprehensive conclusion. Potential negative societal impacts include the trade-off between accuracy and efficiency, especially in accuracy-sensitive applications like healthcare, and the possibility of strengthening models for malicious uses.",
        "future_research_directions": "Future research could focus on improving SparCL to be more generic and compatible with all kinds of CL methods, including those that do not use a rehearsal buffer. Expanding the evaluation to include datasets from other domains, such as NLP, would also provide a more comprehensive and reliable assessment of the method's effectiveness. Further exploration into the trade-offs between efficiency and accuracy for various sparsity ratios is encouraged, particularly for accuracy-sensitive applications. Developing strategies and regulations to prevent the malicious use of strengthened AI models is also suggested."
      }
    },
    {
      "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
      "abstract": "We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.",
      "full_text": "Online Learned Continual Compression with Adaptive Quantization Modules Lucas Caccia 1 2 3 Eugene Belilovsky 2 4 Massimo Caccia 2 4 5 Joelle Pineau 1 2 3 Abstract We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a rep- resentative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting en- counters a major challenge: representations de- rived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the com- pression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and cur- rent progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to sig- niﬁcant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learn- ing agents. 1. Introduction Interest in machine learning in recent years has been fueled by the plethora of data being generated on a regular basis. Effectively storing and using this data is critical for many applications, especially those involving continual learning. In general, compression techniques can greatly improve data storage capacity, and, if done well, reduce the memory and compute usage in downstream machine learning tasks (Gueguen et al., 2018; Oyallon et al., 2018). Thus, learned compression has become a topic of great interest (Theis 1McGill 2Mila 3Facebook AI Research 4University of Montreal 5ElementAI. Correspondence to: Lucas Caccia <lucas.page-caccia@mail.mcgill.ca>, Eugene Belilovsky <eu- gene.belilovsky@umontreal.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). et al., 2017; Ball´e et al., 2016; Johnston et al., 2018). Yet its application in reducing the size of datasets bound for machine learning applications has been limited. This work focuses on the following familiar setting: new training data arrives continuously for a learning algorithm to exploit, however this data might not be iid, and further- more there is insufﬁcient storage capacity to preserve all the data uncompressed. We may want to train classiﬁers, reinforcement learning policies, or other models continu- ously from this data as it’s being collected, or use samples randomly drawn from it at a later point for a downstream task. For example, an autonomous vehicle (with bounded memory) collects large amounts of high-dimensional train- ing data (video, 3D lidar) in a non-stationary environment (e.g. changing weather conditions), and overtime applies an ML algorithm to improve its behavior using this data. This data might be transferred at a later point for use in down- stream learning. Current learned compression algorithms, e.g. (Torfason et al., 2018), are not well designed to deal with this case, as their convergence speed is too slow to be usable in an online setting. In the ﬁeld of continual/lifelong learning (Thrun & Mitchell, 1995), which has for now largely focused on classiﬁcation, approaches based on storing memories for later use have emerged as some of the most effective in online settings (Lopez-Paz et al., 2017; Aljundi et al., 2018; Chaudhry et al.; 2019; Aljundi et al., 2019). These memories can be stored as is, or via a generative model (Shin et al., 2017). Then, they can either be used for rehearsal (Chaudhry et al., 2019; Aljundi et al., 2019) or for constrained optimization (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2018). Indeed many continual learning applications would be greatly improved with replay approaches if one could afford to store all samples. These approaches are however inherently limited by the amount of data that can be stored Learning a generative model to compress the previous data stream thus seems like an appealing idea. However, learn- ing generative models, particularly in the online and non- stationary setting, continues to be challenging, and can greatly increase the complexity of the continual learning task. Furthermore, such models are susceptible to catas- trophic forgetting (Aljundi et al., 2019). An alternate ap- proach is to learn a compressed representation of the data, arXiv:1911.08019v3  [cs.LG]  20 Aug 2020Online Learned Continual Compression with Adaptive Quantization Modules Online Continual Compression z_1 z_1 z_2 z_3z_1 z_2 X ~ non-iid t=1 t=2 t=3 query  samples query  samples Learner Learner Encoder Encoder Encoder Decoder     t=3 Decoder     t=1 Decoder     t=2 storage storage storage  Decoder     t=3 z_1 Decoder     t=1 Representation Drift z_1 X_3 X_2 X_1 Figure 1.Illustration of the challenges in the Online Continual Compression problem. A model must be able to decode representations encoded by previous versions of the autoencoder, permitting anytime access to data for the learner. This must be accomplished while dealing with a time-varying data distribution and ﬁxed memory constraints which can be more stable than learning generative models. Learned compression in the online and non-stationary set- ting itself introduces several challenges illustrated in Fig 1. Firstly the learned compression module must be able to de- code representations encoded by earlier versions of itself, introducing a problem we refer to as representation drift. Secondly, the learned compressor is itself susceptible to catastrophic forgetting. Finally, the learned compression needs to be adaptive to maintain a prescribed level of re- construction quality even if it has not fully adapted to the current distribution. In this work we demonstrate that the VQ-V AE framework (van den Oord et al., 2017; Razavi et al., 2019), originally introduced in the context of generative modeling and density estimation, can be used online to effectively address repre- sentation drift while achieving high compression. Further- more, when augmented with an internal replay mechanism it can overcome forgetting. Finally we use propose to use multiple gradient-isolated compression levels to allow the compressor to adaptively store samples at different compres- sion scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. The main contributions of this work are: (a) we intro- duce and highlight the online learned continual compres- sion (OCC) problem and its challenges. (b) We show how representation drift, one of the key challenges, can be tackled by effective use of codebooks in the VQ-V AE framework. (c) We propose an architecture using multi- ple VQ-V AE’s, adaptive compression scheme, stream sam- pling scheme, and self-replay mechanism that work together to effectively tackle the OCC problem. (d) We demon- strate this can yield state-of-the-art performance in stan- dard online continual image classiﬁcation benchmarks and demonstrate the applications of our OCC solution in a va- riety of other contexts. The code to reproduce our results is available at https://github.com/pclucas14/ adaptive-quantization-modules 2. Related Work Learned compression has been recently studied for the case of image compression. Works by Theis et al. (2017); Ball´e et al. (2016); Johnston et al. (2018) have shown learned compressions can outperform standard algorithms like JPEG. These methods however are difﬁcult to adapt for online settings as they do not directly address the challenges of the OCC problem (e.g. representation drift). Continual Learning research currently focuses on over- coming catastrophic forgetting (CF) in the supervised learn- ing setting, with some limited work in the generative mod- eling and reinforcement learning settings. Most continual learning methods can be grouped into three major families. Some algorithms dynamically change the model’s architec- ture to incorporate learning from each task separately (Rusu et al., 2016; Li & Hoiem, 2018; Fernando et al., 2017). Al- though these methods can perform well in practice, their introduction of task-speciﬁc weights requires growing com- pute and memory costs which are problematic for the online setting. Another set of techniques employ regularization to constrain weights updates in the hope of maintaining knowl- edge from previous tasks. Notable methods in this class include (Kirkpatrick et al., 2017; Husz´ar, 2017; Zenke et al., 2017; Nguyen et al., 2017; Chaudhry et al., 2018). However, this set of approaches has been shown to be inefﬁcient in the online setting (Chaudhry et al., 2019). The last family of methods encapsulates all that have a mech- anism to store information about the previous data distribu- tions. This memory then serves as a tool for the continual learner to rehearse previous tasks. The simplest instantia- tion of this method is to keep and sample from a buffer of old data to retrain the model after every update (Chaudhry et al., 2019). This approach is widely used in RL where it is known as Experience Replay (ER) (Lin, 1993; Mnih et al., 2013; Andrychowicz et al., 2016). Another method, known as Generative Replay (GR) (Shin et al., 2017), uses generative modeling to store past task distributions. The con- tinual learner then trains on generated samples to alleviateOnline Learned Continual Compression with Adaptive Quantization Modules CF. Other notable examples are Gradient Episodic Memory (GEM) (Lopez-Paz et al., 2017), iCarl (Rebufﬁ et al., 2017), and Maximally Interfered Retrieval (MIR) (Aljundi et al., 2019), as well as (Aljundi et al., 2018; Hu et al., 2018). Most closely related to our work, Riemer et al. (2017) consider compressing memories for use in the continual classiﬁcation task. They also employ a discrete latent variable model but with the Gumbel approximation, which shows to be less effective than our approach. Furthermore a separate ofﬂine iid pre-training step for the learned compression is required in order to surpass the ER baseline, distinctly different from the online continual compression we consider. Lidar compression is considered in (Tu et al., 2019) and (Caccia et al., 2018). Both approaches use a similar projec- tion from 3D (x,y,z ) coordinates to 2D cylindrical coordi- nates, and leverage deep generative models to compress the data. However, neither accounts for potential distribution shift, nor for online learning. In this work we show that us- ing this 2D projection in conjunction with our model allows us to mitigate the two issues above for lidar data. 3. Methodology In this section we outline our approach to the online con- tinual compression problem. First we will review the VQ- V AE and highlight the properties making it effective for representational drift. Then we will describe our adaptive architecture, storage, and sampling scheme. 3.1. Problem Setting: Online Continual Compression We consider the problem setting where a stream of samples x ∼Dt arrives from different distributions Dt changing over time t= 1...T . We have a ﬁxed storage capacity ofC bytes where we would like to store the most representative information from all data distributions D1,...DT . There is notably a trade-off in quality of information versus the amount of samples stored. We propose to use a learned compression model, and most crucially, this model must also be stored within the Cbytes, to encode and decode the data samples. Another critical requirement is that at anytime tthe content of the storage (data and/or compression model) be usable for downstream applications. An important chal- lenge, illustrated in Figure 1, is that the learned compression module will change over time, while we still need to be able to decode the memories in storage. 3.2. Vector Quantized V AE for Online Compression The VQ-V AE is a discrete auto-encoder which relies on a vector quantization step to obtain discrete latent repre- sentations. An embedding table, E ∈RK×D consisting of K vectors of size D, is used to quantize encoder out- puts. Given an input (e.g. an RGB image), the encoder ﬁrst encodes it as a Hh ×Wh ×D tensor, where Hh Encoder     t=1      Encoder     t=3      same after VQ Vector Quantization different codebook Figure 2.Illustration of reduced representation drift from Vector Quantization and Wh denote the height and width of the latent repre- sentation. Then, every Ddimensional vector is quantized using a nearest-neighbor lookup on the embedding table. Speciﬁcally, zij = arg mine∈E ||enc(x)ij −e||2, where i,j refers to a spatial location. The output of the quanti- zation step is then fed through the decoder. The gradient of this non-differentiable step is approximated using the straight-through estimator. An important property to notice is that to reconstruct the input, only the Hh ×Wh indices are required, thus yielding high compression (van den Oord et al., 2017). Critically, the embedding tables are updated independently from the encoder and decoder, namely by minimizing mine ||sg[enc(x)ij] −e||, where sg is the stop gradient operator. Observe in the case of online compression, if the embedding table is ﬁxed, then a change in the encoder parameters and therefore a change in the encoder output for a given input will not change the ﬁnal quantized representation z, unless it is sufﬁciently large, thus we can observe that if the em- beddings change slowly or are ﬁxed we can greatly improve our control of the representational drift. This effect is illus- trated in Figure 2. On the other hand we do need to adapt the embedding table, since randomly selected embeddings would not cover well the space of encoder outputs. 3.3. Adaptive Quantization Modules To address issues of how to optimize storage and sampling in the context of Online Continual Compression we intro- duce Adaptive Quantization Modules (AQM). We use AQM to collectively describe the architecture, adaptive multi-level storage mechanism, and data sampling method used. To- gether they allow effectively constraining individual sample quality and memory usage while keeping in storage a faith- ful representation of the overall distribution.Online Learned Continual Compression with Adaptive Quantization Modules Algorithm 1: AQM L EARNING WITH SELF -REPLAY Input: Learning rate α, EXTERNAL LEARNER 1 Initialize: AQM Memory M; AQM Parameters θaqm 2 for t∈1..T do 3 % Fetch data from current task 4 for Binc ∼Dt do 5 for n∈1..N do 6 B←Binc 7 if t> 1 then 8 % Fetch data from buffer 9 Bre ∼SAMPLE (M,θaqm) 10 B ←(Binc,Bre) 11 end 12 % Update AQM 13 θaqm ←ADAM(θaqm,B,α ) 14 % Send data to external learner 15 EXTERNAL LEARNER (B) 16 if t> 1 then UPDATE BUFFER REP(M,θaqm) 17 %Save current indices 18 ADDTOMEMORY (M,Binc,θaqm) 19 end 20 end 21 end AQM uses an architecture consisting of a sequence of VQ- V AEs, each with a buffer. The AQM approach to online continual compression is overall summarized in Algorithm 1 (note ADDTOMEMORY is described in Appendix A). In- coming data is added to storage using an adaptive compres- sion scheme described in Algorithm 2. Incoming data is also used along with randomly sampled data from storage (self- replay) to update the current AQM model. The randomly sampled data also updates its representation in storage as per Algorithm 3. As illustrated by the optional lines in blue, Algorithm 1 can run concurrently with a downstream learn- ing task (e.g. online continual classiﬁcation) which would use the same batch order. It can also be run independently as part of a data collection. In the sequel we give further details on all these elements 3.3.1. A RCHITECTURE AND TRAINING Each AQM module contains a VQ-V AE and a corresponding buffer of adaptive capacity. A diagram of the architecture is given in Figure 3. We will denote the output after quan- tization of each module i as zi q and the set of codebook indexes used to obtain zi q as ai. Note that ai are the discrete representations we actually store. Each subsequent module produces and stores an ai requiring fewer bits to represent. For RGB images, the compression rate at a given level is given by H×W×3×log2(256) Nc×Hhi×Whi×⌈log2 (Ki)⌉. Here Ki is the number of embeddings in the codebooks, ( Hhi, Whi) the spatial dimension of the latent representation and Nci the number of codebooks. Algorithm 2: ADAPTIVE COMPRESS Input: datapoint x, AQM with Lmodules, threshold dth 1 % Forward all modules, store encodings 2 {zi q,ai}i=1..L= ENCODE (x) 3 for i∈L...1 do 4 % Decode from level i to output space 5 ˆx= DECODE (zi q) 6 % Check reconstruction error 7 if MSE(ˆx,x) <dth then return ai, i 8 end 9 % Otherwise, return original input 10 return x,0 Algorithm 3: UpdateBufferRep Input: Memory M, AQM with Llevels, data D, distortion threshold dth 1 for x∈Ddo 2 hidx, blockid = ADAPTIVE COMPRESS (x, AQM, dth) 3 % Delete Old Repr. 4 DELETE (M[x]) 5 % Add new one 6 ADD(M, hidx) 7 end VQV AE-2 (Razavi et al., 2019) also uses a multi-scale hier- archical organization, where unlike our AQM, the top level models global information such as shape, while the bottom level, conditioned on the top one, models local information. While this architecture is tailored for generative modeling, it is less attractive for compression, as both the bottom and top quantized representations must be stored for high qual- ity reconstructions. Furthermore in AQM each module is learned in a greedy manner using the current estimate of z(i−1) q without passing gradients between modules similar to (Belilovsky et al., 2019; Nøkland & Eidnes, 2019). A subsequent module is not required to build representations which account for all levels of compression, thus minimiz- ing interference across resolutions. This allows the modules to each converge as quickly as possible with minimal drift at their respective resolution, particularly important in the online continual learning case. 3.3.2. M ULTI -LEVEL STORAGE Our aim is to store the maximum number of samples in an allotted Cbytes of storage, while assuring their quality, and our ability to reconstruct them. Samples are thus stored at different levels based on the compressors’ current ability. The process is summarized in Algorithm 2. Such an approach is particularly helpful in the online non- stationary setting, allowing knowledge retention before the compressor network has learned well the current distribution. Note in Alg. 2 samples can be completely uncompressed until the ﬁrst module is able to effectively encode them. This can be crucial in some cases, if the compressor has not yet converged, to avoid storing poorly compressed rep-Online Learned Continual Compression with Adaptive Quantization Modules Figure 3.Architecture of Adaptive Quantization Modules. Each level uses its own loss and maintains its own replay buffer. Yello dotted lines indicate gradient isolation between modules resentations. Further taking into account that compression difﬁculty is not the same for all datapoints, this allows use of more capacity for harder data, and fewer for easier. We also note, since we maintain stored samples at each mod- ule and the modules are decoupled, that such an approach allows to easily distribute training in an asynchronous man- ner as per Belilovsky et al. (2019). 3.3.3. S ELF -REPLAY AND STREAM SAMPLING As shown in Alg. 1 our AQM is equipped with an internal experience replay mechanism (Mnih et al., 2013), which reconstructs a random sample from storage and uses it to perform an update to the AQM modules, while simultane- ously freeing up overall memory if the sample can now be compressed at a later AQM module. This has the effect of both reducing forgetting and freeing memory. In practice we replay at the same rate as incoming samples arrive and thus replay will not increase asymptotic complexity of the online learning. Finally, for efﬁciency the replay can be coupled to an external online learner querying for random samples from the overall memory. Since we would like the AQM to work in cases of a ﬁxed memory capacity it must also be equipped with a mecha- nism for selecting which samples from the stream to store and which to delete from memory. Reservoir Sampling (RS) is a simple yet powerful approach to this problem, used successful in continual learning (Chaudhry et al., 2019). It adds a sample from the stream with prob. p= buffer capacity points seen so far while remove a random sample. However, RS is not directly compatible with AQM primarily because the amount of sam- ples that can be stored varies over time. This is because samples at different levels have different memory usage and memory can be freed by replay. We thus propose an alternative scheme, which maximally ﬁlls available memory and selects non-uniformly samples for deletion. Speciﬁ- cally when a larger amount of samples are added at one point in the stream, they become more likely to be removed. The details of this stream sampling method are provided in Appendix A. 3.3.4. DRIFT CONTROL VIA CODEBOOK STABILIZATION As mentioned previously, a good online compressor must control its representational drift, which occurs when up- dates in the auto-encoder parameters creates a mismatch with the static representations in the buffer. Throughout the paper we measure representational drift by compar- ing the following time varying quantity: DRIFT t(z) = RECON ERR ( Decode(θt; z),x ) where θt the model pa- rameters at timetand (z,x) is a stored compressed represen- tation and its original uncompressed datapoint respectively. For all experiments on images, RECON ERR is simply the mean squared error. As illustrated in Sec 3.2 a slow changing codebook can allow to control drifting representations. This can be in part accomplished by updating the codebook with an exponential moving average as described in (van den Oord et al., 2017, Appendix A), where it was used to reduce the variance of codebook updates. This alone is insufﬁcient to fully control drift, thus once a given module yields satisfactory compressions on the data stream, we freeze the module’s embedding matrix but leave encoder and decoder parameters free to change and adapt to new data. Moreover, we note that ﬁxing the codebook for a given module does not affect the reconstruction performance of subsequent modules, as they only need access to the current module’s decoder which can still freely change. 4. Experiments We evaluate the efﬁcacy of the proposed methods on a suite of canonical and new experiments. In Section 4.1 we present results on standard supervised continual learning benchmarks on CIFAR-10. In Section 4.2 we evaluate other downstream tasks such as standard iid training applied on the storage at the end of online continual compression. For this evaluation we consider larger images from Imagenet, as well as on lidar data. Finally we apply AQM on observations of an agent in an RL environment.Online Learned Continual Compression with Adaptive Quantization Modules Accuracy (↑) M= 20 M= 50 iid online 60.8 ±1.0 60 .8 ±1.0 iid ofﬂine 79.2 ±0.4 79 .2 ±0.4 GEM (Lopez-Paz et al., 2017) 16.8 ±1.1 17 .1 ±1.0 iCarl (5 iter) (Rebufﬁ et al., 2017) 28.6 ±1.2 33 .7 ±1.6 ﬁne-tuning 18.4 ±0.3 18 .4 ±0.3 ER 27.5 ±1.2 33 .1 ±1.7 ER-MIR (Aljundi et al., 2019) 29.8 ±1.1 40 .0 ±1.1 ER-JPEG 33.9 ±1.0 43 .1 ±0.6 Gumbel AE (Riemer et al., 2018) 25.5 ±2.0 28 .8 ±2.9 AQM (ours) 43.5 ±0.7 47 .0 ±0.8 Forgetting (↓) M= 20 M= 50 N/A N/A N/A N/A 73.5 ±1.7 70 .7 ±4.5 49 ±2.4 40 .6 ±1.1 85.4 ±0.7 85 .4 ±0.7 50.5 ±2.4 35 .4 ±2.0 50.2 ±2.0 30 .2 ±2.3 54.8 ±1.2 44 .3 ±0.9 71.5 ±2.8 67 .2 ±3.9 23.0 ±1.0 19 .0 ±1.4 Table 1.Shared head results on disjoint CIFAR-10. Total memory per class M measured in sample memory size. We report (a) Accuracy, (b) Forgetting (lower is better). 4.1. Online Continual Classiﬁcation Although CL has been studied in generative modeling (Ramapuram et al., 2017; Lesort et al., 2018; Zhai et al., 2019; Lesort et al., 2019) and reinforcement learning (Kirk- patrick et al., 2017; Fernando et al., 2017; Riemer et al., 2018), supervised learning is still the standard for evalua- tion of new methods. Thus, we focus on the online continual classiﬁcation of images for which our approach can provide a complement to experience replay. In this setting, a new task consists of new image classes that the classiﬁer must learn, while not forgetting the previous ones. The model is only allowed one pass through the data (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2019; Chaudhry et al., 2019). The online compression here takes the role of replay buffer in replay based methods such as (Chaudhry et al., 2019; Aljundi et al., 2019). We thus run Algorithm 1, with an additional online classiﬁer being updated performed at line 15. Here we consider the more challenging continual classiﬁca- tion setting often referred to as using ashared-head (Aljundi et al., 2019; Farquhar & Gal, 2018; Aljundi et al., 2018). Here the model is not informed of the task (and thereby the subset of classes within it) at test time. This is in contrast to other (less realistic) CL classiﬁcation scenarios where the task, and therefore subset of classes, is provided explicitly to the learner (Farquhar & Gal, 2018; Aljundi et al., 2019). For this set of experiments, we report accuracy, i.e. 1 T ∑T i=1 RT,i, and forgetting, i.e. 1 T−1 ∑T−1 i=1 max(R:,i) − RT,i with R ∈ RT×T representing the accuracy matrix where Ri,j is the test classiﬁcation accuracy on task jwhen task iis completed. Baselines A basic baseline for continual supervised learn- ing is Experience Replay ( ER). It consists of storing old data in a buffer to replay old memories. Although relatively simple recent research has shown it is a critical baseline to consider, and in some settings is actually state-of-the- art (Chaudhry et al., 2019; Aljundi et al., 2019; Rolnick et al., 2018). AQM can be used as an add-on to ER that in- corporates online continual compression. We also compare against ER with standard JPEG compression. In addition we consider the following baselines. iid online (upper-bound) trains the model with a single-pass through the data on the same set of samples, but sampled iid. iid ofﬂine (upper- bound) evaluates the model using multiple passes through the data, sampled iid. We use 5 epochs in all the experiments for this baseline. ﬁne-tuning trains continuously upon ar- rival of new tasks without any forgetting avoidance strategy. iCarl (Rebufﬁ et al., 2017) incrementally classiﬁes using a nearest neighbor algorithm, and prevents catastrophic for- getting by using stored samples. GEM (Lopez-Paz et al., 2017) uses stored samples to avoid increasing the loss on previous task through constrained optimization. It has been shown to be a strong baseline in the online setting. It gives similar results to the recent A-GEM (Chaudhry et al.). ER- MIR (Aljundi et al., 2019) controls the sampling of the replays to bias sampling towards samples that will be for- gotten. We note that the ER-MIR critera is orthogonal to AQM, and both can be applied jointly.Gumbel AE (Riemer et al., 2018) learns an autoencoder for ER using the Gumbel softmax to obtain discrete representations. We evaluate with the standard CIFAR-10 split (Aljundi et al., 2018), where 5 tasks are presented sequentially, each adding two new classes. Evaluations are shown in Table 1. Due to our improved storage of previous data, we observe sig- niﬁcant improvement over other baselines at various mem- ory sizes. We can contrast AQM’s performance with ER’s to understand the net impact of our compression scheme. Speciﬁcally, AQM improves over ER by 16.0% and 13.9% in the M=20 and M=50 case, highlighting the effectiveness of online compression. Our approach is also superior in forgetting by a signiﬁcant margin in both memory settings. To compare directly to reporting in (Riemer et al., 2018) we also benchmarked our implementation on the Incremental CIFAR-100 multi-head experiment (Lopez-Paz et al., 2017) with the same settings as in (Riemer et al., 2018). By using AQM we were able to get 65.3 vs the reported 43.7 using a buffer of size 200. To speciﬁcally isolate the advantageOnline Learned Continual Compression with Adaptive Quantization Modules Accuracy RS 5.2 ±0.2 2 Module AQM (ours) 23.2 ± 1.1 Ablate 2nd Module 20.5 ±1.3 Ablate Fixing Codebook 19.2 ±0.6 Ablate Decoupled Training 16.5 ±0.7 Ablate Adaptive Compression 13.1 ±3.2 Table 2.Imagenet ofﬂine training evaluation from online continual compression. We see a clear gain over a standard Reservoir sam- pling approach. We then ablate each component of our proposal showing each component is important. Note storage used in each experiment is identical (including accounting for model sizes). of gumbel softmax versus the vector quantization for drift, we replaced the vector quantization approach with gumbel softmax in an AQM. We observed signﬁcantly less drift in the case where vector quantization is used. Full details of this experiment are described in the supplementary materials along with visualizations. The CIFAR-10 dataset has a low resolution (3 ×32 ×32) and uses a lot of data per task (10K samples). These two characteristics might leave the online compression problem easier than in a real-life scenario. Speciﬁcally, if the ﬁrst tasks are long enough and the compression rate is not too large, the model can quickly converge and thus not incur too much representation drift. Indeed, we found that us- ing a single module is already sufﬁcient for this task. For these reasons, we now study the AQM in more challenging settings presented in the next section. 4.2. Ofﬂine Evaluation on Larger Images Besides the standard continual classiﬁcation setup, we pro- pose several other evaluations to determine the effectiveness of the stored data and compression module after learning online compression. We also perform a detailed ablation to study the efﬁcacy of each component in AQM. Ofﬂine training on Imagenet We compare the effective- ness of the stored memories of AQM after a certain amount of online continual compression. We do this by training in a standard iid way an ofﬂine classiﬁcation model using only reconstructions obtained from the storage sampled after on- line continual compression has progressed for a period of time. In each case we would have the same sized storage available. We note that simply having more stored memo- ries does not amount to better performance as their quality may be severely degraded and affected by drift. Using this evaluation we ﬁrst compare a standard reservoir sampling approach on uncompressed data to a 2 module AQM using the same size storage. We observe that perfor- mance is drastically increased using the compressed sam- ples. We then use this to perform a series of ablations to Figure 4.Impact of codebook freezing. Vertical black line indi- cates freezing point. We see that AQM is still able to adapt and reduce its reconstruction loss, while having stable compressed representations. Results averaged over 5 runs demonstrate each component of our proposal is important. Speciﬁcally (a) we restrict AQM to have only one module, (b) instead of decoupled training we train modules end-to- end, (c) we remove adaptive compression, thus all samples are stored in the most compressed block, regardless of qual- ity, and (d) we do not stabilize the codebook, the embedding matrices of every block are never ﬁxed. We observe that all these elements contribute to successfully storing a represen- tative set of data for the distribution online. Drift Ablation We have seen the importance of codebook freezing when dealing with high dimensional datasets. How- ever, judging solely from the ﬁnal downstream task perfor- mance it’s difﬁcult to see if the model continues adapting after freezing. As alluded in Sec 3.2 there is a tradeoff be- tween keeping recoverable representations and a model’s ability to continue to adapt. To shed some light on this, we run the following experiment: we run a vanilla VQ-V AE on the same 20 task mini-imagenet stream, without storing any samples. When it reaches a pre-speciﬁed performance threshold, we ﬁx the codebook, and store compressed held- out data from the ﬁrst task. We then continue to update the VQ-V AE parameters, and the memory is kept ﬁxed for the rest of the stream. We apply self-replay but no other AQM mechanisms (e.g. no sampling from the input stream and no adaptive compression). We monitor how well the VQ-V AE can adapt by looking at the streaming reconstruction cost, measured on the in- coming data before an update. We also monitor the drift of samples stored in the buffer. Results are presented in Figure 4. They demonstrate that drift is controlled by stabilizing the codebook, while the model can still improve at nearly the same rate. Further analysis, along with an additional ex- periment showcasing the robustness of vector quantization to small perturbations is included in the Appendix.Online Learned Continual Compression with Adaptive Quantization Modules Figure 5.Top: Sample decoded from the buffer at the end of train- ing from scratch (32x compression rate). Bottom: Original lidar LiDAR Range data enables autonomous vehicles to scan the topography of their surrounding, giving precise mea- surements of an obstacle’s relative location. In its raw form, range data can be very large, making it costly to transmit in real time, or for long term storage. Equipping self-driving cars with a good lidar compressor can enable fast vehicle- to-vehicle (V2V) communication, leading to safer driving (Eckelmann et al., 2017). Moreover, since data collected by autonomous vehicles can be highly non-stationary (new objects on the road, changing weather or trafﬁc conditions), having a compressor which can quickly adapt to this distri- bution change will reduce the required memory for storage (or bandwidth for real time transmission). We proceed to train AQM on the Kitti Dataset (Geiger et al., 2013), which contains 61 LiDAR scan recordings, each belonging to either the “residential”, “road”, “city” envi- ronments. The data is processed as in (Caccia et al., 2018), where points from the same elevation angle are sorted in increasing order of azimuth angle along the same row. This yield a 2D grid, making it compatible with the same architec- ture used in the previous experiments. As in (Caccia et al., 2018; Tu et al., 2019), we report the reconstruction cost in Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE) which allows to compare two point clouds. Note AQM can also be adapted to use task relevant criteria besides MSE. We consider two settings. In the ﬁrst, we train AQM from scratch on a data stream consisting of recordings from all three environments. We present (once) all the recordings of an environment before moving on to another, in order to maximise the distribution shift. We show qualitative results in Figure 5 and in the supplementary materials. Ob- serve that we are able to effectively reconstruct the LiDAR samples and can easily tradeoff quality with compression. Overall we obtain 18.8 cm SNNRMSE with 32×compres- Size in Mb Raw 1326.8 Gzip 823.0 AQM 35.5 ±.06 AQM + ﬁnetune 33.0 ±.07 AQM + ﬁnetune + PNG 27.9 ±.01 Table 3.Compression results for the data transmission of the city lidar recordings. We require that each compressed scan has an SNNRMSE under 15 cm. sion, which lies in a range that has been shown in (Tu et al., 2019) to be sufﬁcient to enable SLAM localization with very minimal error. In the second setting, we wish to simulate a scenario where some data is available a priori for the model to leverage. However, this data is limited and does not cover all the possible modalities to which an autonomous vehicle could be exposed. To this end, we pretrain AQM in a fully ofﬂine iid manner on the road and residential recordings. We then simulate the deployment of the compressor on a vehicle, where it must compress and transmit in real time the lidar data feed from a new distribution. We therefore stream the held-out city recordings and show that AQM can be ﬁne-tuned on the ﬂy to reduce the required bandwidth for data transmission. Quantitative results are presented in table 3. We ensure that the reconstructed lidar scans have a SNNRMSE smaller than 15.0 cm. Moreover, since the stored representations in AQM are 2D and discrete, we can apply lossless compression schemes such as Portable Network Graphics (PNG). 4.3. Atari RL Environments Another application of online continual compression is for preserving the states of an reinforcement learning agent op- erating online. These agents may often learn new tasks or enter new rooms thus the observations will often be highly non-iid. Furthermore many existing reinforcement learning algorithms already rely on potentially large replay buffers which can be prohibitive (Mnih et al., 2014; Rolnick et al., 2018) to run and may greatly beneﬁt from an approach such as the AQM to run concurrently with reinforcement learning algorithms. We thus perform a proof of concept for the AQM for storing the state sequence encountered by an RL learner in the atari environment(Bellemare et al., 2013). We use the dataset and tasks introduced in (Anand et al., 2019), which runs a random or learned policy in the atari environments and provides a set of classiﬁcation tasks to evaluate whether key information about the state is preserved. Results are shown Table 4. We run the online learning with the AQM on the data stream observed by the random agent. We use the same observations and optimiza- tion as in (Anand et al., 2019) and report the F1 resultsOnline Learned Continual Compression with Adaptive Quantization Modules Figure 6.Top: original. Bottom: reconstructed from AQM Game Cls Input F1 Pong Orig. State 86.7 AQM Recon 86.8 Ms Pacman Orig. State 89.4 AQM Recon 88.3 Pitfall Orig. State 68.2 AQM Recon 66.7 Table 4.Results on RL probing tasks from (Anand et al., 2019) with linear probe applied to original observation and to reconstruc- tions from AQM after online compression. Acc is averaged for each game over game speciﬁc prediction. of a linear probe directly on states for our reconstructions after online compression and the originals. Results for 3 environments are shown in Table 4 and examples in in Fig 6 and the Appendix. We ﬁnd that AQM can well preserve the critical information while compressing the state by 16x. The reference accuracies achieved by our classiﬁer are sim- ilar to those in (Anand et al., 2019). However, we do not control for the representation size unlike those evaluations of various unsupervised models. 5. Conclusion We have introduced online continual compression. We demonstrated vector quantization can be used to control drift and how to create mechanisms that allow maintain- ing quality and maximizing memory usage. These allowed learning compression while compressing. We have shown effectiveness of this online compression approach on stan- dard continual classiﬁcation benchmarks, as well as for compressing larger images, lidar, and atari data. We believe future work can consider dealing with temporal correlations for video and reinforcement learning tasks, as well as im- proved prioritization of samples for storage. References Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Online continual learning with no task boundaries. In arXiv, 2018. Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Charlin, L., and Tuytelaars, T. Online continual learning with maximally interfered retrieval. In Advances in Neural Information Processing (NeurIPS), 2019. Anand, A., Racah, E., Ozair, S., Bengio, Y ., C ˆot´e, M.- A., and Hjelm, R. D. Unsupervised state representation learning in atari. arXiv preprint arXiv:1906.08226, 2019. Andrychowicz, M., Denil, M., G´omez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N. Learning to learn by gradient descent by gradient descent. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Sys- tems 29, pp. 3981–3989. Curran Associates, Inc., 2016. Ball´e, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2016. Belilovsky, E., Eickenberg, M., and Oyallon, E. De- coupled greedy learning of cnns. arXiv preprint arXiv:1901.08164, 2019. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Caccia, L., van Hoof, H., Courville, A., and Pineau, J. Deep generative modeling of lidar data. arXiv preprint arXiv:1812.01180, 2018. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. In ICLR 2019. Chaudhry, A., Dokania, P. K., Ajanthan, T., and Torr, P. H. Riemannian walk for incremental learning: Un- derstanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H., and Ranzato, M. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. Eckelmann, S., Trautmann, T., Ußler, H., Reichelt, B., and Michler, O. V2v-communication, lidar system and posi- tioning sensors for future fusion algorithms in connected vehicles. Transportation research procedia, 27:69–76, 2017. Farquhar, S. and Gal, Y . Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733 , 2018.Online Learned Continual Compression with Adaptive Quantization Modules Fernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra, D. Pathnet: Evolu- tion channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, pp. 0278364913491297, 2013. Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J. Faster neural networks straight from jpeg. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31 , pp. 3933–3944. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7649-faster-neural-networks-straight-from-jpeg. pdf. Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., Zhao, D., and Yan, R. Overcoming catastrophic forgetting for continual learning via model adaptation. 2018. Husz´ar, F. On quadratic penalties in elastic weight consoli- dation. arXiv preprint arXiv:1712.03847, 2017. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Jin Hwang, S., Shor, J., and Toderici, G. Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4385–4393, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. Lesort, T., Caselles-Dupr´e, H., Garcia-Ortiz, M., Stoian, A., and Filliat, D. Generative models from the perspective of continual learning. arXiv preprint arXiv:1812.09111, 2018. Lesort, T., Gepperth, A., Stoian, A., and Filliat, D. Marginal replay vs conditional replay for continual learning. In International Conference on Artiﬁcial Neural Networks, pp. 466–480. Springer, 2019. Li, Z. and Hoiem, D. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 40(12):2935–2947, 2018. Lin, L.-J. Reinforcement learning for robots using neu- ral networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. Lopez-Paz, D. et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 2014. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. arXiv preprint arXiv:1901.06656, 2019. Oyallon, E., Belilovsky, E., Zagoruyko, S., and Valko, M. Compressing the input for cnns with the ﬁrst-order scatter- ing transform. In The European Conference on Computer Vision (ECCV), September 2018. Ramapuram, J., Gregorova, M., and Kalousis, A. Lifelong generative modeling. arXiv preprint arXiv:1705.09847, 2017. Razavi, A., Oord, A. v. d., and Vinyals, O. Generating diverse high-ﬁdelity images with vq-vae-2.arXiv preprint arXiv:1906.00446, 2019. Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. Riemer, M., Franceschini, M., and Klinger, T. Genera- tion and consolidation of recollections for efﬁcient deep lifelong learning. CoRR, abs/1711.06761, 2017. URL http://arxiv.org/abs/1711.06761. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and Wayne, G. Experience replay for continual learning, 2018. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had- sell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.Online Learned Continual Compression with Adaptive Quantization Modules Shin, H., Lee, J. K., Kim, J., and Kim, J. Continual learn- ing with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017. Theis, L., Shi, W., Cunningham, A., and Husz ´ar, F. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017. Thrun, S. and Mitchell, T. M. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25–46, 1995. Torfason, R., Mentzer, F., ´Ag´ustsson, E., Tschannen, M., Timofte, R., and Gool, L. V . Towards image under- standing from deep compression without decoding. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkXWCMbRW. Tu, C., Takeuchi, E., Carballo, A., and Takeda, K. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 Interna- tional Conference on Robotics and Automation (ICRA), pp. 3274–3280. IEEE, 2019. van den Oord, A., Vinyals, O., et al. Neural discrete repre- sentation learning. In Advances in Neural Information Processing Systems, pp. 6306–6315, 2017. Zenke, F., Poole, B., and Ganguli, S. Continual learn- ing through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017. Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G. Lifelong gan: Continual learning for conditional im- age generation. ArXiv, abs/1907.10107, 2019.Online Learned Continual Compression with Adaptive Quantization Modules A. Buffer Memory Management We consider two settings to manage the samples inside the memory. In the ﬁrst setting, we do not perform any codebook freezing. Therefore, as the model trains, the amount of compressed representations AQM can store increases smoothly. In this setting, the current amount of samples stored by AQM is a good approximation of the model’s capacity. Therefore, we simply use this estimate instead of the total buffer size in the regular reservoir sampling scheme. The algorithm is presented in Alg 4. Algorithm 4: AddToMemory Input: Memory Mwith capacity C(bytes), sample x 1 Nreg = C BY TES(x) 2 capacity = max( Nreg, NUM SAMPLES (M) ) 3 %Probability of adding x 4 add ∼B( capacity SAMPLE AMT SEEN SO FAR ) %Bernoulli 5 if add then 6 hidx, blockid = ADAPTIVE COMPRESS (x, AE, dth) 7 while BITS(hidx) - FREE SPACE(M) >0 do 8 DELETE RANDOM (M) 9 end 10 end This is not the case when we perform codebook freezing. In the latter setting, consider the moment when the ﬁrst codebook is ﬁxed; suddenly, the amount of samples the model can store has increased by a factor equal to the compression rate of the ﬁrst block. Therefore, at this moment, the amount of samples currently stored by AQM is not a good approximation for the model’s capacity. Moreover, when performing codebook freezing, since the capacity suddenly spikes, we must decide between a) having an imbalance in the buffer, where certain temporal regions of the streams are not equally represented, or not utilising all available memory and storing less incoming samples so they are in similar quantities as previous samples. We opt for the former approach, and propose a procedure that allows the buffer to rebalance itself as new training data becomes available. We illustrate the procedure with an example. Consider an AQM where the distribution of samples in the buffer is the one plotted in Fig 10. Speciﬁcally, we show the number of samples stored for each minibatch processed by the model. In this example, very few (<20) samples are stored from the earliest part of the stream, while a much larger number comes from the more recent part of the stream. Assuming that the model is over its memory capacity, we need to remove samples until the memory requirement is met. Ideally, we would like to remove more samples from parts of the stream where samples are abundant. In order to do so, we use Kernel Density Estimation on the histogram in Fig 10. Doing so gives us the line labelled iter0 in Fig 10. We then sample points according to the distribution given by iter0, remove them, and ﬁt a new KDE with the remaining points (labelled iter1). In this example we repeat this procedure 10 times, until iter9. As we can see, the distribution of stored samples becomes closer to the uniform distribution, i.e. the setting where all parts of the streeam are equally represented. Therefore, in the setting where codebook freezing is performed, we ﬁrst add all incoming points to the buffer. Then, points are removed according to the procedure described above. This allows for maximal memory usage while ensuring that the buffer self balances over time. Note that we need to store the timestamp alongside each sample, which has a negligible cost when dealing with high-dimensional inputs. B. Further Details of Experiments We include here further details regarding the models used in our experiments. For all reported results, almost all hyperpa- rameters are kept the same. we set D the size of the embedding table equal to 100, we use a decay value of 0.6 for the Embedding EMA update, and the same architectural blocks. Across problems, we mainly vary the reconstruction threshold, as well how the blocks are stacked and their compression rates (by e.g. changing the number of codebooks per block).Online Learned Continual Compression with Adaptive Quantization Modules Figure 7.(left) histogram of samples in AQM where no buffer balancing was performed. (right) iterative buffer balancing procedure B.1. Cifar For CIFAR-10, we use a 1 block AQM, latent size (16 x 16 x 100) is quantized with (16 x 16 x 1) indices where the last index represents the number of codebooks. The codebook here contains 128 embeddings, giving a compression factor of 13.7×. Due to the simplistic nature of the task and low resolution of the images, AQM already yields good compression before the end of the ﬁrst task, hence adaptive compression and codebook freezing are not required. For the (Riemer et al., 2018) baseline, we ran a hyperparameter search to vary the compression size. Speciﬁcally, we ran a grid search for the number of categories per latent variable, as well as for the number of latent variables. We found the gumbel softmax much less stable during training and harder to cross-validate than vector quantization. Below we show an example of the image quality of our approach compared to (Riemer et al., 2018). We ran both AQM and (Riemer et al., 2018) on the split CIFAR-10 task, then extracted images which happened to be in the buffer of both methods. Figure 8.Bottom row: random buffer reconstructions using (Riemer et al., 2018). Middle row: random buffer reconstructions using SQM. Top row: corresponding original image. Columns are ordered w.r.t their entry time in the buffer, from oldest to newest. All samples above were obtained from the disjoint CIFAR-10 task, and are 12×smaller than their original image. B.2. Imagenet For the Ofﬂine 128 x 128 Imagenet experiment, we use the following three blocks to build AQM, with the following latent sizes : 1. (64 x 64 x 100) quantized using (64 x 64 x 1) indices, with a codebook of 16 vectors, giving a 24 ×compression. 2. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 256 vectors, giving a 48 ×compression. 3. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 32 vectors, giving a 76.8 ×compression. For the 2 block AQM, we searched over using blocks (1-2) (2-3) and (1-3). For 1 block AQM we simply tried all three blocks independently. When stacking two blocks with the same latent sizes (e.g. block 2 and 3) the encoder and decoder functions for the second block are the identity. In other words, the second block simply learns another embedding matrix.Online Learned Continual Compression with Adaptive Quantization Modules C. Drift Ablation Here we provide additional results for the drift ablation studied in 4. We repeat the same experiment for different values of the reconstruction threshold parameter, which controls when the codebook freezing occurs. The table below shows that the same conclusion holds across multiple values for this parameter: codebook freezing yields little to no drift, while only negligibly hindering the model’s ability to adapt to a distribution shift. It is worth noting that in cases of severe drift (e.g. with recon th = 7.5) the model diverges because it is rehearsing on samples of poor quality. In this setting, codebook freezing performs better on both the streaming MSE and the drift MSE. recon th Freezing Streaming MSE Drift MSE Streaming + Drift MSE 1 No 0.59 ±0.03 1.01 ±0.14 1.60 ±0.17 1 Yes 0.60 ±0.03 0.49 ±0.02 1.09±0.05 2.5 No 0.63 ±0.06 13.24 ±5.36 13.87 ±5.42 2.5 Yes 0.81 ±0.02 1.07 ±0.05 1.88±1.12 5 No 0.65 ±0.04 55.31 ±36.82 55.96 ±36.86 5 Yes 0.92 ±0.03 1.69 ±0.18 2.61±0.21 7.5 nan nan nan nan 7.5 Yes 0.98 ±0.11 2.10 ±0.28 3.08±0.39 Table 5.Here we provide additional results for the drift ablation discussion in section shown in Fig 4. For clarity all results are multiplied by 100. (e.g. recon th of 2.5 corresponds to 0.025). Results averaged over 5 runs. Figure 9.Visualization of results reported in Table C. We kept the scale of the y axis consistent across the four graphs for easy comparison.Online Learned Continual Compression with Adaptive Quantization Modules D. Atari Below are sample reconstructions used in the Atari experiments. For each game, reconstructions are in the ﬁrst row and the original uncompressed samples in the second row. Reconstructions are 16×smaller than the original RGB images. Figure 10.Pong Figure 11.Pitfall Figure 12.Ms PacmanOnline Learned Continual Compression with Adaptive Quantization Modules E. Lidar Samples Here we show reveral lidar compressions (left) and their original counterpart (right). The compression rate is 32 ×. We note that unlike RGB images, raw lidar scans are stored using ﬂoating points. We calculate the compression rate from the (smaller) polar projection instead of the 3 channel cartesian representation. Figure 13.Lidar reconstruction (left) vs original (right)Online Learned Continual Compression with Adaptive Quantization Modules Figure 14.Lidar reconstruction (left) vs original (right)",
      "meta_data": {
        "arxiv_id": "1911.08019v3",
        "authors": [
          "Lucas Caccia",
          "Eugene Belilovsky",
          "Massimo Caccia",
          "Joelle Pineau"
        ],
        "published_date": "2019-11-19T00:43:16Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08019v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces and defines the Online Continual Compression (OCC) problem, focusing on learning to compress and store a representative dataset from a non-i.i.d data stream while observing each sample only once and without pretraining. It highlights key challenges such as representation drift (ensuring older compressed data remains decodable by evolving models), catastrophic forgetting, and maintaining adaptive reconstruction quality under memory constraints. The core contribution is the proposal of Adaptive Quantization Modules (AQM) within the VQ-VAE framework, which effectively uses codebooks to mitigate representation drift. AQM integrates multiple VQ-VAEs, an adaptive compression scheme, a self-replay mechanism, and a novel stream sampling scheme. The work demonstrates state-of-the-art performance on standard online continual image classification benchmarks and showcases its applicability to larger images, LiDAR data, and reinforcement learning agents.",
        "methodology": "The methodology is based on the Vector Quantized Variational Autoencoder (VQ-VAE) framework to achieve online continual compression. The key innovation is the Adaptive Quantization Modules (AQM), an architecture consisting of a sequence of VQ-VAEs, each with its own buffer and trained with gradient isolation between modules. Representation drift is controlled by leveraging VQ-VAE's codebooks, updating embedding tables independently, and eventually freezing them after satisfactory compression, sometimes using an exponential moving average for stabilization. AQM incorporates a multi-level adaptive storage mechanism (Algorithm 2) that stores samples at different compression rates based on their reconstruction quality (MSE), allowing samples to remain uncompressed if initial quality is poor. A self-replay mechanism (Algorithm 1) uses reconstructed samples from storage to update AQM modules, reducing forgetting and potentially freeing memory. Finally, a stream sampling method, detailed in Appendix A, uses Kernel Density Estimation-based balancing to dynamically manage the buffer, ensuring maximal memory usage while prioritizing the deletion of overrepresented samples.",
        "experimental_setup": "The effectiveness of AQM is evaluated across several domains. For online continual classification, experiments are conducted on CIFAR-10 (5 tasks, 2 new classes each) and Incremental CIFAR-100, measuring accuracy and forgetting against baselines including Experience Replay (ER), GEM, iCarl, fine-tuning, ER-MIR, ER-JPEG, and Gumbel AE. For offline evaluation on larger images, 128x128 Imagenet data is used to train a standard iid classification model on AQM's reconstructions, accompanied by an ablation study on AQM's components (number of modules, training coupling, adaptive compression, codebook stabilization). Representation drift is specifically ablated by measuring `RECON_ERR` (Mean Squared Error) for images, focusing on the impact of codebook freezing. LiDAR compression is tested on the Kitti Dataset (61 recordings from 'residential', 'road', 'city' environments), processed into 2D grids, with reconstruction quality measured by Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE). Evaluations cover training from scratch and pretraining followed by fine-tuning on new data streams, also integrating lossless PNG compression on discrete representations. For reinforcement learning, AQM's ability to preserve critical state information is evaluated on Atari environments (Pong, Ms Pacman, Pitfall) by comparing the F1 score of a linear probe on AQM-reconstructed (16x compressed) states against original states.",
        "limitations": "The evaluation on the CIFAR-10 dataset, which features low resolution and a high volume of data per task, may simplify the online compression problem, potentially leading to an underestimation of the challenges AQM faces in more complex, real-world scenarios. The authors note that a single AQM module was often sufficient for this task. Additionally, in the Atari RL experiments, the representation size was not controlled for, which could hinder direct comparative analysis with other unsupervised models.",
        "future_research_directions": "Future research can focus on extending AQM to handle temporal correlations more effectively, particularly in video and reinforcement learning tasks. Another promising direction involves developing improved methods for prioritizing samples for storage within the memory buffer to optimize resource allocation and knowledge retention."
      }
    },
    {
      "title": "Towards Backward-Compatible Continual Learning of Image Compression",
      "abstract": "This paper explores the possibility of extending the capability of\npre-trained neural image compressors (e.g., adapting to new data or target\nbitrates) without breaking backward compatibility, the ability to decode\nbitstreams encoded by the original model. We refer to this problem as continual\nlearning of image compression. Our initial findings show that baseline\nsolutions, such as end-to-end fine-tuning, do not preserve the desired backward\ncompatibility. To tackle this, we propose a knowledge replay training strategy\nthat effectively addresses this issue. We also design a new model architecture\nthat enables more effective continual learning than existing baselines.\nExperiments are conducted for two scenarios: data-incremental learning and\nrate-incremental learning. The main conclusion of this paper is that neural\nimage compressors can be fine-tuned to achieve better performance (compared to\ntheir pre-trained version) on new data and rates without compromising backward\ncompatibility. Our code is available at\nhttps://gitlab.com/viper-purdue/continual-compression",
      "full_text": "Towards Backward-Compatible Continual Learning of Image Compression Zhihao Duan1 Ming Lu2 Justin Yang1 Jiangpeng He1† Zhan Ma2 Fengqing Zhu1 1 Purdue University, West Lafayette, Indiana, U.S.A. 2 Nanjing University, Nanjing, Jiangsu, China {duan90, yang1834, he416, zhu0}@purdue.edu, {minglu, mazhan}@nju.edu.cn Abstract This paper explores the possibility of extending the capa- bility of pre-trained neural image compressors (e.g., adapt- ing to new data or target bitrates) without breaking back- ward compatibility, the ability to decode bitstreams encoded by the original model. We refer to this problem as continual learning of image compression. Our initial findings show that baseline solutions, such as end-to-end fine-tuning, do not preserve the desired backward compatibility. To tackle this, we propose a knowledge replay training strategy that effectively addresses this issue. We also design a new model architecture that enables more effective continual learning than existing baselines. Experiments are conducted for two scenarios: data-incremental learning and rate-incremental learning. The main conclusion of this paper is that neural image compressors can be fine-tuned to achieve better per- formance (compared to their pre-trained version) on new data and rates without compromising backward compati- bility. Our code is available at this link. 1. Introduction Recent years have witnessed the rapid development of deep learning-based image compression. Most existing research in this field considers theoffline learning setting, i.e., once a neural network model is trained, its parameters are fixed and kept unchanged when deployed. However, real-world appli- cations are often complex and dynamic, and an ideal com- pressor should be capable of being incrementally learned and adapted to various scenarios. For example, consider an image storage application with a compressor pre-trained on natural images for certain predefined target bitrates. As new image sources ( e.g., microscopy, remote sensing, and human face images) are encountered, one may want to up- date the compressor to improve its performance on the new data and to support different target rates. This raises an in- teresting question: can neural network-based image com- †Corresponding author & Project lead Enc. Dec. New  Dec. Pre-trained model Fine-tuning New  Enc. New  Dec. Bits Backward compatibility (a) Continual learning of image compression Original  image Reconstruction,  before fine - tuning Reconstruction, after fine - tuning Vanilla fine -tuning With our method (b) The backward compatibility problem. Once a neural compressor (in this experiment, [41]) is fine-tuned, it can no longer decode the bitstream produced by its original version. Our method addresses this issue. Figure 1. The goal of this work is to fine-tune pre-trained learned image compressors with new data or new rates while preserving backward compatibility (Fig. 1a). Baseline methods are backward incompatible, while ours is effective (Fig. 1b). pressors be learned continually, and if so, would it bring performance benefits compared to the pre-trained model? One might assume that simply fine-tuning a pre-trained model would be sufficient. Yet, doing so disrupts the model’s backward compatibility (Fig. 1b), i.e., the ability to decode bitstreams produced by the original model, due to a mismatch between the encoder (pre-trained model) and decoder (fine-tuned model). Maintaining backward com- patibility is crucial, as failing to do so renders existing bit- streams in the storage (or sent from other devices) inac- cessible. It is worth noting that this backward compatibil- ity problem is different from the well-known problem of catastrophic forgetting in neural networks [28]. The unique properties of compression, including the sender-receiver re- lationship and the existence of entropy coding, set it apart from other image processing/vision tasks. Therefore, ex- isting continual learning methods for vision tasks [13, 51] cannot be applied as is, and new strategies must be devel- oped for compression to maintain the decoder’s backward arXiv:2402.18862v1  [eess.IV]  29 Feb 2024compatibility when adapting to new data and rates. To achieve backward compatibility, the most straight- forward way is to modify only the encoding process of a pre-trained model when adapting to new data or new rates [7, 19, 56]. This strategy resembles the common prac- tice in traditional codecs, where there is often a standardized decoder, but various encoders can be designed to accommo- date different applications. Despite its simplicity, keeping the decoder unchanged hinders the model’s ability to adapt to new data and rates, leading to suboptimal performance. This work shows that it is possible to continually train both the encoder and decoder of neural compressors while maintaining backward compatibility. We begin by noticing that as long as the entropy model of a compressor is kept unchanged, it can decode the old bitstreams and obtain the latent representations. Based on this observation, we pro- pose a knowledge replay training scheme that can be used to train the encoder and decoder networks without break- ing backward compatibility. We also design a model ar- chitecture where the entropy model consumes only a small amount of parameters, allowing most model parameters to be learnable in fine-tuning. We formulate two experimental scenarios: data-incremental learning and rate-incremental learning. Experimental results demonstrate that our pro- posed methods enable neural image compressors to obtain improved performance on new data and new rates without breaking backward compatibility. To summarize, our contributions are as follows: • We propose a knowledge replay-based training strategy that can be used to train neural image compressors incre- mentally without breaking backward compatibility; • We design a neural network architecture targeting effec- tive continual learning of image compression; • We formulate two continual learning scenarios for im- age compression: data-incremental learning and rate- incremental learning. Experimental results show that our method outperforms baseline approaches in both cases. 2. Background and Related Work 2.1. Learned lossy image compression (LIC) Most learning-based methods for lossy image compression can be interpreted using the entropy-constrained non-linear transform coding framework [4]. Let X ∼ pdata denote data samples with an underlying data distribution. In this frame- work, a neural network encoderfenc maps X to a latent vari- able Z ≜ fenc(X), and a neural network decoder fdec maps Z back to a reconstruction ˆX ≜ fdec(Z). A learned prob- ability distribution pZ, also known as the entropy model, is used to model the marginal distribution of Z. The learning objective is to minimize the rate-distortion (R-D) loss: min EX∼pdata h −log2 pZ(Z) + λ · d(X, ˆX) i , (1) where d is a distortion metric (e.g., mean squared error),λ is the Lagrange multiplier trading off between rate and distor- tion, and the minimization is over the network parameters of fenc, fdec, and pZ. This framework has also been extended to variable-rate compression [12, 14], where the encoder, decoder, and entropy model are conditioned on λ. During variable-rate training, the model parameters are optimized for various λ sampled from a distribution pΛ: min EX∼pdata,Λ∼pΛ h −log2 pZ(Z|Λ) + Λ· d(X, ˆX) i (2) where Z ≜ fenc(X; Λ), ˆX ≜ fdec(Z; Λ). (3) Existing research in LIC can be categorized into several groups. A major group focuses on designing expressive architectures for fenc and fdec, such as convolutional and transformer-based ones [10, 11, 14, 20, 23, 27, 33, 37–39, 43, 52, 54, 59]. Another line of research lies in designing the entropy model pZ, such as autoregressive [22, 40, 41, 44] and hierarchical models [3, 14, 15, 25]. Other research includes, e.g., quantization methods [17, 18, 21, 55, 57] and variable-rate compression methods [6, 9, 12, 31, 48, 53]. To our knowledge, none of these existing methods are designed to work with continual learning as in our work. The most related line of research to this paper iscontent- adaptive image compression, where the goal is to adapt a compressor to new images or new target rates in a per-image fashion. Solutions to this problem include encoder-side op- timization and decoder-side adaptation. Encoder-side opti- mization methods [7, 19, 56] directly optimize the R-D ob- jective w.r.t. Z during encoding. Decoder-side adaptation methods [42, 47, 50], on the other hand, include parameter- efficient neural network modules in the bitstream, which are executed on the decoder side to improve decoding. Among them, many methods require computationally expensive, it- erative optimization during encoding. The scope of this paper is distinct from content-adaptive image compression in several ways: (a) our goal is to in- crementally train the compressor parameters in-place with- out introducing additional parameters, and (b) as opposed to per-image optimization during testing, our method employs a one-time training procedure and induces no additional computational cost at test time. Our research is comple- mentary to content-adaptive image compression, and they can be combined to further improve the performance. 2.2. Knowledge replay in continual learning Continual learning has been widely studied for image clas- sification [13] and semantic segmentation [8], which aim to learn a sequence of new tasks incrementally without forgetting the previously learned knowledge. Among ex- isting methods for continual learning, replay-based meth- ods [34, 36, 45] have emerged as particularly effective,𝑓enc (0) 𝑓dec (0) Pre-trained model 𝑋test 0 𝑏test 0𝜆 ∈ [𝜆low 0 ,𝜆high (0) ] Fine-tuning 𝑓enc (1) 𝑓dec (1) 𝑋train 1 𝜆 ∈ [𝜆low 1 ,𝜆high (1) ] (a) Continual learning Goal 1: Backward compatibility 𝑏test 0 Goal 2: New data & rate performance 𝑓dec (1) 𝑓enc (1) 𝑓dec (1) 𝑋test 1 𝜆 ∈ [𝜆low 1 ,𝜆high (1) ] 𝑋test 0෠𝑋test 0 Distortion Rate Rate Distortion (b) Evaluation Figure 2. Problem definition. Fig. 2a shows the fine-tuning pro- cess of a pre-trained model, and Fig. 2b shows the two evaluation criteria: backward compatibility and new data/rate performance. Entropy models are kept frozen and omitted in the figures. which applies an additional memory buffer to store exem- plar data from learned tasks and then integrate with new task data to perform knowledge rehearsal during continual learn- ing. Our proposed knowledge replay method adopts a simi- lar principle. However, image compression contains unique challenges distinct from typical computer vision tasks, mak- ing existing continual learning strategies inapplicable to de- ploy in image compression. Thus, a tailored knowledge re- play strategy is required for our problem scenario. 3. Problem Description In this section, we formulate the problem (Sec. 3.1), discuss the backward compatibility issue (Sec. 3.2), and motivate the design of our method (Sec. 3.3). 3.1. Continual learning for compression Assume that we have a pre-trained variable-rate model with supported λ ∈ [λ(0) low, λ(0) high]. We use f(0) enc, f(0) dec, p(0) Z to de- note the pre-trained model’s encoder, decoder, and entropy model, respectively. Also, assume that we have used the pre-trained model to compress a test set of images X(0) test and obtained the corresponding bitstreams b(0) test, as illustrated in Fig. 2a (top). This situation well-silumates a typical image storage application with a learned image compressor. We now aim at fine-tuning the model with new dataX(1) train and a new rate range determined by [λ(1) low, λ(1) high]. Note that the new data and rate range may or may not be the same as the old ones. Similarly, let f(1) enc, f(1) dec, p(1) Z denote the new model components, as illustrated in Fig. 2a (bottom). We expect the new model to achieve two goals: 1. Backward compatibility: The new model should be ca- pable of decoding the bitstreams produced by the old model (Fig. 2b, top). 2. New-data (or new-rate) performance: The new model should perform better than the old one on new data and new rates (Fig. 2b, bottom). Layers𝑥 Layers Layers Layers 𝑧1𝑧2 𝑝𝑍1𝑝𝑍2|𝑍1 ො𝑥 𝑓enc 𝑓dec 𝑝𝑍 Figure 3. The Hyperprior model architecture [3], where the layers are categorized into three groups: fenc, fdec, and pZ. Number of parameters Total fenc fdec pZ M-S Hyp. [41] 17.6M 5.9M 2.5M 8.2M (47%) GMA [11] 26.6M 5.8M 11.7M 9.0M (34%) ELIC [23] 33.8M 9.7M 7.3M 16.7M (49%) STF [59] 99.9M 11.3M 7.1M 81.4M (81%) TCM [33] 45.2M 3.3M 6.8M 35.2M (78%) MLIC++ [26] 116.7M 6.3M 12.0M 98.4M (84%) Our model 35.5M 18.7M 11.9M 4.9M (14%) Table 1. Many existing methods employ a parameter-expensive entropy model. We propose an architecture with a lightweight en- tropy model, which makes continual learning more effective (since more parameters are learnable in the fine-tuning phase). 3.2. Backward compatibility of entropy decoding As mentioned in Fig. 1, fine-tuning the model end-to-end breaks the backward compatibility of the decoder. The pri- mary reason lies in entropy coding: range-based entropy coding algorithms [16, 46], which are commonly used in modern neural compressors, are known to be sensitive to the probability distribution of the encoded symbols. As the new entropy modelp(1) Z is different from the old onep(0) Z , the new model is not able to perform entropy coding correctly using the old bitstreams b(0) test, and thus the correct (quan- tized) latent variables Z(0) test cannot be obtained. In fact, re- cent research [2, 29, 49] has shown that even a small change (e.g., a floating point round-off error) in the entropy model can lead to failure in entropy decoding. 3.3. Freezing the entropy model during fine-tuning To avoid the aforementioned issue, the entropy model pZ needs to be kept unchanged throughout fine-tuning. Then, the latent variablesZ(0) test are guaranteed to be recovered loss- lessly from the old bitstreams, and the problem reduces to continually learning the decoder fdec without forgetting the old knowledge ( i.e., decoding Z(0) test). With our pro- posed training strategy (Sec. 4.1), we show that this can be achieved for many existing neural compressors. We also notice that the entropy model pZ is often the largest component in many existing compressors, most of which are based on the Hyperprior model [3, 41] (Fig. 3). We found that their entropy model takes up a significant portion (e.g., 84% for MLIC++[26]) of the model parame- ters, as shown in Tab. 1. Consequently, a large proportion of model parameters need to be fixed during continual learn-ing, which may impair the new-data (or new-rate) perfor- mance. Motivated by this, we design a model architecture (Sec. 4.2) that employs a lightweight entropy model (Tab. 1, last row), which makes continual learning more effective. 4. Method As just mentioned, our methods include two independent components: the knowledge replay-based training strategy (Sec. 4.1) and a neural network architecture specifically de- signed for continual learning (Sec. 4.2). We now present them sequentially in detail. 4.1. Continual learning with knowledge replay Following the notation introduced in Sec. 3.1, we denote a pre-trained model as f(0) enc , f(0) dec , pZ, which are trained on data X(0) train with λ ∈ [λ(0) low, λ(0) high]. We drop the superscript for pZ since it is kept frozen throughout fine-tuning. In- spired by the idea of knowledge rehearsal with exemplars in class-incremental learning methods [45], we use the old training data X(0) train and the old encoder f(0) enc to perform “knowledge replay” in the fine-tuning process. To this end, we store f(0) enc along with X(0) train within a dedicated memory buffer before the fine-tuning stage. Note that we do not pose restrictive assumptions on training resources and allow ac- cess to the entire training set X(0) train during fine-tuning. In the fine-tuning process, our knowledge replay-based training objective contains two terms. The first term, ℓnew, is the standard R-D loss for the new training dataX(1) train with the new λ value range [λ(1) low, λ(1) high]: ℓnew ≜ E h R(1) + Λ(1) · D(1) i , where (4) R(1) ≜ −log2 pZ(Z(1)|Λ(1)) (5) D(1) ≜ d(X(1) train, ˆX(1) train). (6) In (4), the expectation is w.r.t. X(1) train and Λ(1), where Λ(1) is a random variable with the support being [λ(1) low, λ(1) high]. Its probability density, p(1) Λ , controls how λ is sampled during training. Intuitively, minimizing ℓnew adapts the model pa- rameters to new data and new rates, but it is not sufficient to maintain backward compatibility. The other term in our loss function encourages back- ward compatibility of the model parameters through knowl- edge replay of the old data X(0) train and the old encoder f(0) enc . Specifically, we use f(0) enc to encode X(0) train, which gives the corresponding (quantized) latent variables Z(0) train. Then, the current decoder f(1) dec decodes Z(0) train, and the reconstruction ˆX(0) train is compared with X(0) train to compute the knowledge re- 𝑓enc (0) 𝜆 ∈ [𝜆low 0 ,𝜆high (0) ] 𝐷(0) ෠𝑋train 0 𝑓enc (1) 𝑓dec (1) 𝑋train 1 𝜆 ∈ [𝜆low 1 ,𝜆high (1) ] 𝑅(1) 𝐷(1) ෠𝑋train 1 𝑋train 0 Figure 4. Our knowledge replay-based training strategy contains two components: a distortion loss that encourages backward com- patibility (top, dashed lines), and the standard R-D loss for new data and new rates (bottom, solid lines). The entropy model pZ is kept frozen and is omitted in the figure. play loss function, ℓKR, defined as: ℓKR ≜ E h Λ(0) · D(0) i , where (7) D(0) ≜ d \u0010 X(0) train, f(1) dec \u0010 f(0) enc (X(0) train) \u0011\u0011 . (8) The expectation is w.r.t. X(0) train and Λ(0), where Λ(0) ∼ p(0) Λ controls how λ is sampled during knowledge replay. In our experiments, we choose p(0) Λ to be the same as the one used in pre-training. Note that there is no rate term in ℓKR since the replayed encoder f(0) enc is fixed, and thus the rate term is constant w.r.t. the model parameters being trained. By replaying the old data and the old encoder, the decoder net- work effectively retains backward compatibility, as shown in our experiments (Sec. 5.3). Fig. 4 illustrates our knowledge replay strategy for one training iteration. Our training objective is to minimize a weighted summation of the two terms, ℓnew and ℓKR, with a scalar hyperparameter α ∈ [0, 1]: min (1− α) · ℓnew + α · ℓKR. (9) The hyperparameter α controls the weight of replayed data in the training objective and can be tuned to achieve a de- sired trade-off between backward compatibility and new data/rate performance (shown in Sec. 5.4). Our knowl- edge replay strategy is general and can be applied to various model architectures, enabling backward-compatible contin- ual learning of those models (shown in Sec. 5.3). 4.2. Model architecture Since freezing the entropy model is necessary for backward compatibility, we propose a model architecture that em- ploys a lightweight entropy model by design so that most parameters in the model can stay learnable. An overview of the architecture is shown in Fig. 5. Our model is inspired by the hierarchical residual coding architecture [14, 18, 58], but we decouple the entropy model ( pZ) and the decoder(fdec) into two separate branches, resulting in a reduced en- tropy model size. We now describe each component of the model in detail. Encoding: The encoding process involves a bottom-up pass through the encoder fenc and a top-down pass through the entropy model pZ. Given an input image x, encod- ing begins with fenc, a neural network consisting of a se- quence of downsampling and residual layers that produces a hierarchy of image features (denoted as hi in the figure). Specifically, for an input image with 256 × 256 pixels, fenc produces N = 4 features with spatial dimentions 32 × 32, 16 × 16, 8 × 8, and 4 × 4. All layers are convolutional, so the spatial dimensions scale accordingly for images of dif- ferent sizes. Then, the entropy model pZ starts with a con- stant e0 and iteratively updates it using the features hi from fenc. In each stage, ei−1 is upsampled to the same spatial di- mension as hi and concatenated with hi. The concatenated features are then fed into a sequence of layers to produce zi, the latent variable (which will be entropy coded) for the i-th stage. zi is then aggregated with the upsampled ei−1 through a linear layer and addition operation, and the result is denoted as ei and passed to the next stage. Note that in each stage, the entropy model also estimates the probabil- ity distribution of zi given z<i (with z<i ≜ {z1, ..., zi−1}), which is used for entropy coding. Probabilistic model, quantization, and entropy cod- ing are performed in the same way as for the discretized Gaussian model in Hyperprior-based methods [3, 41]. As a brief recap, the entropy model predicts a mean µi and a scale σi for each latent variable zi, and the probability model for zi is a discretized Gaussian distribution: pi(zi) ≜ pZi|Z<i(zi | z<i) (10) = Z zi+0.5 zi−0.5 N(t; µi, σ2 i ) dt, (11) where the dependence on z<i is through µi and σi. Dur- ing testing, the residual between zi and µi is quantized to the nearest integer, and during training, quantization is sim- ulated by additive uniform noise. Each stage i produces a separate bitstream corresponding to zi (using the asym- metric numeral systems [16] algorithm), and all stages are executed sequentially for i = 1, ..., N. Decoding: Given encoded bitstreams, the decoding pro- cess mirrors the encoding process in reverse. Firstly, the entropy model is executed top-down to iteratively predict pZi|Z<i, based on which the bitstreams are entropy-decoded to obtain zi. Then, the decoder fdec is executed top-down to reconstruct the image. Starting with a constant r0, the de- coder iteratively updates ri using zi and ei in each stage, as shown in the right pane of Fig. 5. In each stage and the final layer, residual layers and upsampling layers are applied to restore the image to its original resolution. Down ↓ 𝑥 ො𝑥 Encoder 𝑓enc Decoder 𝑓decEntropy model 𝑝𝑍 Layers Down ↓ Layers Concat. Layers +𝑧𝑖 𝑝𝑍𝑖|𝑍<𝑖 Layers Up ↑ 𝑒𝑖 Up ↑ Layers Layers Concat. 𝑟𝑖 𝑟𝑖−1 𝑒𝑖 𝑧𝑖 𝑒𝑖−1 Proj. Repeat for 𝑖 = 1,2,…,𝑁 Layers Up ↑ ℎ𝑖 𝑒0 𝑟0 Figure 5. Our model adopts the hierarchical residual coding archi- tecture [14, 18, 58] but decouples the entropy model (pZ) and the decoder (fdec) into two branches. Up ↑ denotes upsampling, Down ↓ denotes down-sampling, and Proj. denotes a linear projection layer. Detailed layer configurations are in Appendix, Sec. 7 DW Conv Feature LayerNorm MLP (𝐻,𝑊,𝐶) Feature 𝜆 Embedding (𝐶,) (𝐶,) MLP +× + Figure 6. Each layer in our model is a ConvNeXt block [35] conditioned on λ through an affine transformation. In the figure, (H, W, C) denotes height, width, and channel dimensions. Rate-conditional network layers: Fig. 6 shows the de- tails of each layer ( i.e., residual block) in our model. Our model employs the ConvNeXt module [35] as the basic building blocks. To achieve variable-rate compression, we adopt the conditional convolution technique [12, 14], which applies an adaptive affine transformation to the convolu- tional layer output (after layer normalization) to control the rate based on the input λ. Variable-rate training: The (pre-)training objective is to minimize the standard R-D loss for variable rate com- pression (Eq. (2)), except that the rate term consists of the sum of the rates for all latent variables: min EX,Λ \" NX i=1 −log2 pi(Zi|Λ) + Λ· d(X, ˆX) # , (12) where X follows the training data distribution, and Λ fol- lows pΛ, a continuous probability distribution that con- trols the sampling strategy of λ during training. After the pre-training phase, we freeze pZ and apply our knowledge replay training strategy presented in Sec. 4.1 to achieve backward-compatible fine-tuning.5. Experiments Our experiments compare various fine-tuning strategies as well as various model architectures for continual learning of image compression. We begin by describing the setup (Sec. 5.1) and baseline methods (Sec. 5.2). Then, Sec. 5.3 presents the main experimental results for our proposed methods. Finally, we provide additional experiments to an- alyze the effectiveness of knowledge replay (Sec. 5.4) and our model architecture (Sec. 5.5). 5.1. Experiment setup We consider two continual learning scenarios in image com- pression: data-incremental learning and rate-incremental learning. In the former, pre-trained models are fine-tuned on a new dataset, and in the latter, models are fine-tuned with a larger rate range (either going higher or lower). De- tailed configurations are shown in Tab. 2, and the datasets and metrics used are listed below. Datasets: we use the COCO [32] dataset train2017 split for pre-training all models. The dataset contains 118,287 images with around 640 × 420 pixels. We randomly crop the images to 256 × 256 patches during training. For data- incremental learning, we adopt the CelebA-HQ dataset [30] at 256 × 256 pixels, a commonly-used human face image dataset for generative image modeling [24]. The dataset consists of 30,000 images, where 24,000 are for training, 3,000 for validation, and the remaining 3,000 for testing. Metrics: We use bits per pixel (bpp), peak signal-to- noise ratio (PSNR, computed for the RGB space), and BD- Rate [5] to measure compression performance, all of which are standard metrics for image compression. As described in Sec. 3, we evaluate each method for two objectives: • Backward compatibility: We use the fine-tuned model to decode b(0) test , the bitstreams encoded by the pre-trained model, to obtain reconstructions ˆX(0) test . The bpp is com- puted for b(0) test (which is a constant independent of fine- tuning strategies), and the PSNR is computed between the reconstructions ˆX(0) test and the original images X(0) test . • New data & rate performance: We compress the new data X(1) test with the new rates, determined by the λ value range [λ(1) low, λ(1) high], to obtain bpp and PSNR metrics. We report all results in terms of BD-Rate in the main paper (due to space constraints), and we provide the correspond- ing PSNR-bpp curves in the Appendix. 5.2. Methods in comparison In addition to our model, we choose Mean & Scale Hyper- prior [41] (MSH) and Gaussian Mixture & Attention [11] (GMA) as two base models for our experiments. These two are commonly used and representative models for learned image compression, and we believe the experimental con- clusions based on them can be generalized to other existing [λ(0) low , λ(0) high] X(0) train X(0) test Pre-training [32, 1024] COCO Kodak [λ(1) low , λ(1) high] X(1) train X(1) test Data-incremental [32, 1024] CelebA CelebA Rate-incremental (low → high) [32, 4096] COCO Kodak Rate-incremental (high → low) [4, 1024] COCO Kodak Table 2. Experiment configurations. We start with a pre-trained model (pre-training) and fine-tune it either with new data ( data- incremental) or new rates (rate-incremental). BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams (Kodak) New data (CelebA) Avg. MSH-VR, pre-trained 26.7 17.1 21.9 MSH-VR w/ FT Enc. 26.7 14.8 20.8 MSH-VR w/ FT Enc. & Dec. 419.1 8.95 214.0 MSH-VR w/ KR (ours) 20.1 9.57 14.8 GMA-VR, pre-trained 4.43 -9.23 -2.40 GMA-VR w/ FT Enc. 4.43 -10.5 -3.04 GMA-VR w/ FT Enc. & Dec. 354.6 -17.3 168.7 GMA-VR w/ KR (ours) 4.33 -14.4 -5.04 Our model, pre-trained 1.87 -13.2 -5.67 Our model w/ FT Enc. 1.87 -14.6 -6.37 Our model w/ FT Enc. & Dec. 262.9 -19.0 122.0 Our model w/ KR 0.87 -16.6 -7.87 Table 3. Data-incremental learning (COCO → CelebA) results. PSNR-bpp curves are provided in Appendix, Fig. 8. models. Since variable-rate compression is required to per- form rate-incremental learning, we construct a variable-rate version for each of them and train it in the same way as for our model. The resulting models are referred to asMSH-VR and GMA-VR, respectively. Sec. 8 in the Appendix provides pre-training and fine-tuning hyperparameters, and Sec. 9 in the Appendix provides details on the variable-rate baselines compared to their fixed-rate models. We apply the following fine-tuning strategies for each model and compare their performance: • Pre-traiend model: Using the pre-trained model without fine-tuning with new data or rates is the simplest baseline. • Fine-tuning the encoder only (FT Enc.) : We fine-tune the encoder with new data while keeping other parameters frozen. Since the entropy model and the decoder are never changed, backward compatibility is guaranteed. • Fine-tuning both the encoder and decoder (FT Enc. & Dec.): We fine-tune all model parameters except for the entropy model parameters. Since the decoder changes, backward compatibility may be lost. • Our approach: knowledge replay (KR). We fine-tune the model’s encoder and decoder with the proposed knowledge replay (KR) strategy applied. 5.3. Experimental results Data-incremental learning. Tab. 3 show the results (pre- trained on COCO, fine-tuned on CelebA). We begin byKodak BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams: bpp ≈ (0.1,0.9) New rate: bpp ≈ (0.1,1.6) Avg. MSH-VR, pre-trained 26.7 - - MSH-VR w/ FT Enc. & Dec. 282.2 23.4 152.80 MSH-VR w/ KR (ours) 19.6 17.3 18.45 GMA-VR, pre-trained 4.43 - - GMA-VR w/ FT Enc. & Dec. 64.1 4.56 34.33 GMA-VR w/ KR (ours) 3.39 2.34 2.87 Our model, pre-trained 1.87 - - Our model w/ FT Enc. & Dec. 17.7 1.45 9.58 Our model w/ KR 0.96 0.70 0.83 Table 4. Rate-incremental learning (low → high) results. PSNR- bpp curves are provided in Appendix, Fig. 9. Kodak BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams: bpp ≈ (0.1,0.9) New rate: bpp ≈ (0.03,0.9) Avg. MSH-VR, pre-trained 26.7 - - MSH-VR w/ FT Enc. & Dec. 35.1 37.6 36.4 MSH-VR w/ KR (ours) 18.9 29.9 24.4 GMA-VR, pre-trained 4.43 - - GMA-VR w/ FT Enc. & Dec. 10.0 9.11 9.56 GMA-VR w/ KR (ours) 1.75 6.92 4.34 Our model, pre-trained 1.87 - - Our model w/ FT Enc. & Dec. 14.33 5.18 9.76 Our model w/ KR 0.86 4.26 2.56 Table 5. Rate-incremental learning (high → low) results. PSNR- bpp curves are provided in Appendix, Fig. 10. comparing the fine-tuning strategy for the MSH-VR model. Firstly, fine-tuning the encoder (FT Enc.) does not provide a significant improvement for new data BD-Rate (17.1% → 14.8%), which is expected since fine-tuning the encoder only reduces the amortization gap [56] and does not im- prove the model’s capacity. When fine-tuning both the en- coder and decoder (FT Enc. & Dec.), the new data BD-Rate is improved by a much larger margin (17.1%→ 8.95%), in- dicating the importance of updating the decoder. However, this came at the cost of backward compatibility, as shown by the significant increase in BD-Rate for old bitstreams (26.7% → 419.1%). This indicates that, while the decoder fits the new data well, it becomes incompatible with the old bitstreams. With our knowledge replay strategy ( MSH-VR w/ KR), we are able to achieve a competitive new data per- formance (9.57% BD-Rate) without sacrificing backward compatibility. Notably, fine-tuning with our strategy also improves the performance on old bitstreams, which is not the case for the other strategies. On average, the knowledge replay strategy clearly outperforms the other ones. These observations are consistent with the results forGMA-VR and our model, demonstrating the effectiveness of knowledge replay in data-incremental learning. Also, when comparing different model architectures, our model achieves the best performance overall in terms of all metrics. Rate-incremental learning . Tab. 4 presents the re- Fine-tuning BD-Rate w.r.t. VTM 22.0 Config. Data KR loss Old bits. New data Avg. 0 - - 1.87 -13.2 -5.67 1 CA 262.9 -19.0 121.9 2 CA + COCO 23 -16.9 3.05 3 CA ✓ 3.67 -16.3 -6.32 4 (ours) CA + COCO ✓ 0.87 -16.6 -7.87 Table 6. Ablative analysis of our knowledge replay-based training strategy for data-incremental learning (COCO→ CelebA). For the “data” column, CA denotes the CelebA dataset. sults for rate-incremental learning (from low rates to higher rates). In rate-incremental experiments, we omit the FT Enc. baseline, since fine-tuning the encoder alone cannot effectively extend the rate range of any considered mod- els (see Appendix, Sec. 10.2 for details). Starting with the MSH-VR model, we observe that fine-tuning both the en- coder and decoder ( FT Enc. & Dec. ) is able to extend the operational rate range of the model with a similar BD-Rate w.r.t. VTM for the new rates. However, the performance on old bitstreams is significantly degraded, similar to the obser- vation in data-incremental learning experiments. By apply- ing our knowledge replay strategy (MSH-VR w/ KR), in con- trast, the model is able to achieve a competitive BD-Rate for the new rates (17.3%) while maintaining backward compat- ibility. Again, the results for GMA-VR and our model show a similar pattern. Overall, our model with KR outperforms the baselines, validating its effectiveness in rate-incremental learning. For rate-incremental learning from high rates to low rates (Tab. 5), the above observations stay the same. 5.4. Experimental analysis: knowledge replay The effectiveness of the proposed knowledge replay strat- egy has already been verified in the previous experiments. We now provide additional experiments to answer the fol- lowing questions that aim to analyze the individual compo- nents of our knowledge replay strategy. What contributes to the backward compatibility? In data-incremental learning, there are two components in our knowledge replay strategy that may help backward compat- ibility: the replayed training data and the knowledge replay loss. To analyze the contribution of each component, we freeze the entropy model parameters of our model and fine- tune it with the two components separately. To analyze the contribution of each component, we start from “Our model w/ FT Enc. & Dec. ” and apply the two components one by one. Tab. 6 shows the results. Config. 0 is the pre- trained model, and Config. 1 is the “FT Enc. & Dec. ”base- line that does not retain backward compatibility. With re- played data ( Config. 2 ), backward compatibility is largely improved (262.9% → 23% BD-Rate) but is still worse than the pre-trained model. When the knowledge replay loss is applied (Config. 3 and Config. 4 ), the performance on old bitstreams becomes comparable to the pre-trained model.α 0.0 0.25 0.5 0.75 1.0 BD-rate: old bitstreams 262.9 1.40 0.87 0.72 0.63 BD-rate: new data -19.0 -16.8 -16.6 -16.3 -14.4 Avg. BD-rate 122.0 -7.7 -7.9 -7.8 -6.9 Table 7. Data-incremental learning (COCO→ CelebA) results for our model with varying α. The scalar α ∈ [0, 1] controls the ratio of replayed data in fine-tuning, where α = 0.0 means no replay, and α = 1.0 means no new data. BD-rate is w.r.t. VTM 22.0. Latency (in seconds) Params. Entropy Coding Network (CPU) Network (GPU) Enc. Dec. Enc. Dec. Enc. Dec. MSH + VR 19.2M 0.026 0.068 0.318 0.325 0.004 0.010 GMA + VR 33.4M 3.072 6.302 0.941 1.221 0.006 0.022 Ours 35.5M 0.046 0.052 0.474 0.393 0.026 0.011 *Hardware: Intel 10700K CPU (using four threads) and Nvidia Quadro 6000 GPU. *Latency is the average time to encode/decode a Kodak image (768 ×512 pixels), averaged over all 24 images. Time includes entropy coding. Table 8. Computational complexity of the model architectures used in our experiments. We conclude that both the replayed data and the loss func- tion contribute to backward compatibility, and the knowl- edge replay loss is more important among the two. What is the impact of α in the knowledge replay loss function (Eq. (9))? We train our model for data- incremental learning with varyingα, the scalar that controls the ratio of replayed data in each training iteration. Results are shown in Tab. 7. Firstly, it is clear that as α grows from 0 to 1, the performance on old bitstreams monotonically improves (i.e., the BD-Rate decreases). Recall that when α = 0 , no replay is performed, and the model is trained with only the new data; when α = 1, the model is trained with only the replayed data. The results are thus consis- tent with the intuition that more replay leads to better back- ward compatibility. When it comes to the new data perfor- mance, the trend is reversed: as α grows, the performance on new data monotonically degrades, which again matches the intuition. On average, the performance is comparable for α ∈ [0.25, 0.75]. We conclude that our approach is in- sensitive to the choice of α and can achieve a good trade- off between backward compatibility and new data perfor- mance. Our experiments choose α = 0.5 by default, but in practice, the choice of α can be treated as a hyperparameter and determined by the application requirements. 5.5. Experimental analysis: model architecture Computational complexity. Tab. 8 shows the computa- tional complexity of our model and the two baselines. The metrics include the number of parameters, entropy coding latency, and neural network forward pass latency. Except for a few exceptions (the parameter count of MSH-VR, the entropy coding latency of GMA-VR, and the GPU encoding latency of our model), all methods are mostly comparable in terms of computational complexity. Thus, we conclude that the performance improvement of our model is not due BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams (Kodak) New data (CelebA) Avg. Sequential pZ and fdec 4.42 -12.8 -4.19 Sequential pZ and fdec w/ KR 3.18 -15.8 -6.31 Parallel pZ and fdec 1.87 -13.2 -5.67 Parallel pZ and fdec w/ KR 0.87 -16.6 -7.87 Table 9. Comparing architecture variants of our model in terms of data-incremental learning (COCO → CelebA). to the increase in computational complexity. Impact of decoupling the entropy model and the de- coder. Recall that in order to reduce the parameters of pZ, our architecture decouples pZ and fdec into two par- allel branches. To analyze the impact of doing so, we con- struct a variant of our model wherepZ and fdec are executed in sequential (like in Hyperprior-based models; see Fig. 3). For a fair comparison, the sequential version has the same number of latent feature channels and the same number of parameters as the parallel model, and training is performed in the same way as for all previous experiments. We show the data-incremental learning results in Tab. 9. We observe that our model (Parallel pZ and fdec w/ KR) achieves a bet- ter performance than the sequential version on both old bit- streams and new data, which verifies our design. 5.6. Discussion Our experiments show that neural image compressors can adapt to new data and rates in a backward-compatible man- ner by using the proposed training strategy. In addition to continual learning applications, this observation offers in- sights into related research such as the standardization of learned image compression. Despite recent attempts toward this goal [1, 39], it remains an open question about which part of a selected neural compressor needs to be standard- ized. Our findings suggest a possible direction: only the entropy model (instead of the entire model architecture and parameters) needs to be standardized, and other components (e.g., the decoder network) could be fine-tuned over time with backward-compatible training strategies. 6. Conclusion This paper presents two approaches: a knowledge replay- based training strategy and a neural network architecture, for continual learning of image compression. Our knowl- edge replay strategy enables existing compressors to adapt to new data and target rates while ensuring that previ- ously compressed bitstreams remain decodable. Through extensive experiments, we conclusively answer the ques- tion raised at the beginning of the paper: neural network- based image compressors can be learned continually in a backward-compatible manner, achieving improved perfor- mance on new data, new rates, and old bitstreams.Limitations and future work. Our work serves as a preliminary study on continual learning for image compres- sion. Despite its effectiveness, our knowledge replay strat- egy assumes unconstrained training resources, which may not be true in practice. Also, we focus on the decoder’s backward compatibility, while the same problem can be studied for the encoder. For future work, a possible direc- tion is to extend our two-step method (pre-training and fine- tuning) to multi-step continual learning scenarios. References [1] Jo ˜ao Ascenso, Elena Alshina, and Touradj Ebrahimi. The jpeg ai standard: Providing efficient human and machine visual data con- sumption. IEEE MultiMedia, 30(1):100–111, 2023. 8 [2] Johannes Ball ´e, Nick Johnston, and David Minnen. Integer networks for data compression with latent-variable models.International Con- ference on Learning Representations, 2018. 3 [3] J. Ball ´e, D. Minnen, S. Singh, S. Hwang, and N. Johnston. Vari- ational image compression with a scale hyperprior. International Conference on Learning Representations, 2018. 2, 3, 5 [4] J. Ball ´e, P. A. Chou, D. Minnen, S. Singh, N. Johnston, E. Agustsson, S. Hwang, and Toderici G. Nonlinear transform coding. IEEE Jour- nal of Selected Topics in Signal Processing , 15(2):339–353, 2021. 2 [5] Gisle Bjontegaard. Calculation of average psnr differences between rd-curves. Video Coding Experts Group - M33, 2001. 6 [6] Shilv Cai, Zhijun Zhang, Liqun Chen, Luxin Yan, Sheng Zhong, and Xu Zou. High-fidelity variable-rate image compression via invert- ible activation transformation. Proceedings of the ACM International Conference on Multimedia, pages 2021–2031, 2022. 2 [7] Joaquim Campos, Simon Meierhans, Abdelaziz Djelouah, and Christopher Schroers. Content adaptive optimization for neural im- age compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition Workshops, 2019. 2 [8] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling the background for incremen- tal learning in semantic segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9233–9242, 2020. 2 [9] Tong Chen and Zhan Ma. Variable bitrate image compression with quality scaling factors. IEEE International Conference on Acoustics, Speech and Signal Processing, pages 2163–2167, 2020. 2 [10] T. Chen, H. Liu, Z. Ma, Q. Shen, X. Cao, and Y . Wang. End-to-end learnt image compression via non-local attention optimization and improved context modeling. IEEE Transactions on Image Process- ing, 30:3179–3191, 2021. 2 [11] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto. Learned image com- pression with discretized gaussian mixture likelihoods and attention modules. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7936–7945, 2020. 2, 3, 6, 4 [12] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Variable rate deep image compression with a conditional autoencoder. Proceed- ings of the IEEE/CVF International Conference on Computer Vision, pages 3146–3154, 2019. 2, 5 [13] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale ˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44 (7):3366–3385, 2022. 1, 2 [14] Zhihao Duan, Ming Lu, Jack Ma, Yuning Huang, Zhan Ma, and Fengqing Zhu. Qarv: Quantization-aware resnet vae for lossy image compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1–15, 2023. 2, 4, 5 [15] Zhihao Duan, Ming Lu, Zhan Ma, and Fengqing Zhu. Lossy image compression with quantized hierarchical vaes. Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 198–207, 2023. 2 [16] Jarek Duda, Khalid Tahboub, Neeraj J. Gadgil, and Edward J. Delp. The use of asymmetric numeral systems as an accurate replacement for huffman coding. Picture Coding Symposium, pages 65–69, 2015. 3, 5 [17] Alaaeldin El-Nouby, Matthew J. Muckley, Karen Ullrich, Ivan Laptev, Jakob Verbeek, and Herve Jegou. Image compression with product quantized masked image modeling. Transactions on Ma- chine Learning Research, 2023. 2 [18] Runsen Feng, Zongyu Guo, Weiping Li, and Zhibo Chen. Nvtc: Nonlinear vector transform coding. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101–6110, 2023. 2, 4, 5 [19] Chenjian Gao, Tongda Xu, Dailan He, Yan Wang, and Hongwei Qin. Flexible neural image compression via code editing. Advances in Neural Information Processing Systems, 35:12184–12196, 2022. 2 [20] G. Gao, P. You, R. Pan, S. Han, Y . Zhang, Y . Dai, and H. Lee. Neural image compression via attentional multi-scale back projection and frequency decomposition. Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision , pages 14677–14686, 2021. 2 [21] Zongyu Guo, Zhizheng Zhang, Runsen Feng, and Zhibo Chen. Soft then hard: Rethinking the quantization in neural image compression. Proceedings of the International Conference on Machine Learning , 139:3920–3929, 2021. 2 [22] Dailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, and Hongwei Qin. Checkerboard context model for efficient learned image com- pression. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14766–14775, 2021. 2 [23] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang. Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 5708–5717, 2022. 2, 3 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 6 [25] Yueyu Hu, Wenhan Yang, Zhan Ma, and Jiaying Liu. Learning end-to-end lossy image compression: A benchmark. IEEE Trans- actions on Pattern Analysis and Machine Intelligence , 44(8):4194– 4211, 2022. 2 [26] Wei Jiang and Ronggang Wang. MLIC$ˆ {++}$: Linear complex- ity multi-reference entropy modeling for learned image compression. ICML Workshop Neural Compression: From Information Theory to Applications, 2023. 3 [27] Wei Jiang, Jiayu Yang, Yongqi Zhai, Peirong Ning, Feng Gao, and Ronggang Wang. Mlic: Multi-reference entropy model for learned image compression. Proceedings of the ACM International Confer- ence on Multimedia, pages 7618–7627, 2023. 2 [28] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcom- ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. 1 [29] Esin Koyuncu, Timofey Solovyev, Elena Alshina, and Andr ´e Kaup. Device interoperability for learned image compression with weights and activations quantization.Picture Coding Symposium, pages 151– 155, 2022. 3 [30] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation.Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5548–5557, 2020. 6, 1[31] Jooyoung Lee, Seyoon Jeong, and Munchurl Kim. Selective com- pression learning of latent representations for variable-rate image compression. Advances in Neural Information Processing Systems , 35:13146–13157, 2022. 2 [32] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in con- text. Proceedings of the European Conference on Computer Vision, pages 740–755, 2014. 6, 1 [33] Jinming Liu, Heming Sun, and Jiro Katto. Learned image compres- sion with mixed transformer-cnn architectures. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14388–14397, 2023. 2, 3 [34] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and Qianru Sun. Mnemonics training: Multi-class incremental learning without for- getting. Proceedings of the IEEE/CVF conference on Computer Vi- sion and Pattern Recognition, pages 12245–12254, 2020. 2 [35] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11966–11976, 2022. 5, 2 [36] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information pro- cessing systems, 30, 2017. 2 [37] Ming Lu and Zhan Ma. High-efficiency lossy image coding through adaptive neighborhood information aggregation. arXiv preprint arXiv:2204.11448, 2022. 2 [38] Ming Lu, Peiyao Guo, Huiqing Shi, Chuntong Cao, and Zhan Ma. Transformer-based image compression. Data Compression Confer- ence, pages 469–469, 2022. [39] Haichuan Ma, Dong Liu, Ning Yan, Houqiang Li, and Feng Wu. End-to-end optimized versatile image compression with wavelet-like transform. IEEE Transactions on Pattern Analysis and Machine In- telligence, 44(3):1247–1263, 2022. 2, 8 [40] D. Minnen and S. Singh. Channel-wise autoregressive entropy mod- els for learned image compression. Proceedings of the IEEE Inter- national Conference on Image Processing, pages 3339–3343, 2020. 2 [41] D. Minnen, J. Ball ´e, and G. Toderici. Joint autoregressive and hier- archical priors for learned image compression. Advances in Neural Information Processing Systems, 31:10794–10803, 2018. 1, 2, 3, 5, 6, 4 [42] Guanbo Pan, Guo Lu, Zhihao Hu, and Dong Xu. Content adaptive latents and decoder for neural image compression.Proceedings of the European Conference on Computer Vision, pages 556–573, 2022. 2 [43] Yichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhen- hong Sun, Li Hao, and Rong Jin. Learning accurate entropy model with global reference for image compression. International Confer- ence on Learning Representations, 2021. 2 [44] Yichen Qian, Xiuyu Sun, Ming Lin, Zhiyu Tan, and Rong Jin. Entro- former: A transformer-based entropy model for learned image com- pression. International Conference on Learning Representations , 2022. 2 [45] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and represen- tation learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5533–5542, 2017. 2, 4 [46] J. Rissanen and G. G. Langdon. Arithmetic coding. IBM Journal of Research and Development, 23(2):149–162, 1979. 3 [47] Sheng Shen, Huanjing Yue, and Jingyu Yang. Dec-adapter: Explor- ing efficient decoder-side adapter for bridging screen content and nat- ural image compression. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12887–12896, 2023. 2 [48] Myungseo Song, Jinyoung Choi, and Bohyung Han. Variable-rate deep image compression through spatially-adaptive feature trans- form. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2360–2369, 2021. 2 [49] Kuan Tian, Yonghang Guan, Jinxi Xiang, Jun Zhang, Xiao Han, and Wei Yang. Effortless cross-platform video codec: A codebook-based method. arXiv preprint arXiv:2310.10292, 2023. 3 [50] Koki Tsubota, Hiroaki Akutsu, and Kiyoharu Aizawa. Univer- sal deep image compression via content-adaptive optimization with adapters. IEEE/CVF Winter Conference on Applications of Com- puter Vision, pages 2528–2537, 2023. 2 [51] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A compre- hensive survey of continual learning: Theory, method and applica- tion. arXiv preprint arXiv:2302.00487, 2023. 1 [52] Yueqi Xie, Ka Leong Cheng, and Qifeng Chen. Enhanced invertible encoding for learned image compression. Proceedings of the ACM International Conference on Multimedia, pages 162–170, 2021. 2 [53] Fei Yang, Luis Herranz, Yongmei Cheng, and Mikhail G. Moze- rov. Slimmable compressive autoencoders for practical neural image compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 4996–5005, 2021. 2 [54] Yibo Yang and Stephan Mandt. Computationally-efficient neu- ral image compression with shallow decoders. Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 530–540, 2023. 2 [55] Yibo Yang, Robert Bamler, and Stephan Mandt. Variational Bayesian quantization. Proceedings of the International Conference on Ma- chine Learning, 119:10670–10680, 2020. 2 [56] Yibo Yang, Robert Bamler, and Stephan Mandt. Improving infer- ence for neural image compression. Advances in Neural Information Processing Systems, 33:573–584, 2020. 2, 7 [57] Xi Zhang and Xiaolin Wu. Lvqac: Lattice vector quantization cou- pled with spatially adaptive companding for efficient learned image compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 10239–10248, 2023. 2 [58] Xiaosu Zhu, Jingkuan Song, Lianli Gao, Feng Zheng, and Heng Tao Shen. Unified multivariate gaussian mixture for efficient neural im- age compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 17591–17600, 2022. 4, 5 [59] Renjie Zou, Chunfeng Song, and Zhaoxiang Zhang. The devil is in the details: Window-based attention for image compression. Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 17471–17480, 2022. 2, 3Towards Backward-Compatible Continual Learning of Image Compression Supplementary Material 7. Appendix: Model Architecture Details In the main paper, Sec. 4.2 provides a high-level overview of the proposed model architecture. This section provides more details about the model architecture, such as the num- ber of channels and stride sizes for each layer. The detailed model architecture is shown in Fig. 7, where the model components are marked in the same way as in Sec. 4.2. Our model contains four phases, all of which have the same structure, while only different in (1) the number of feature channels, (2) the number of ConvNeXt blocks, and (3) the first phase starts from bias e0 and r0 instead of the feature maps from the previous phase. The spatial dimensions (height and width) in the figure are for an input image with 256 × 256 pixels. Since the model is fully convolutional, the spatial dimensions of in- termediate layer outputs scales accordingly with the input image size. Both initial bias features e0 and r0 have a shape of 1 × 1 × 128, and they are repeated spatially to match the spatial dimensions of z1. 8. Appendix: Training and Fine-tuning Details Tab. 10 lists the pre-training and fine-tuning hyperparame- ters used in our experiments. For a fair comparison, we use the same hyperparameters for training all models, including our proposed model and the baseline models ( i.e., MSH- VR and GMA-VR). Note that the fine-tuning dataset varies for different sets of experiments. For data-incremental learning, we use CelebA-HQ [30], and for rate-incremental learning, we use COCO [32], which is the same as the pre- training dataset. Pre-training Fine-tuning Data augmentation Crop, h-flip Crop, h-flip Input size 256x256 256x256 Optimizer Adam Adam Learning rate 2 × 10−4 1 × 10−4 LR schedule Constant + cosine Cosine Weight decay 0.0 0.0 Batch size 32 32 # iterations 500K 100K # images seen 16M 3.2M Gradient clip 2.0 2.0 EMA 0.9999 - GPU 1 × RTX 3090 1 × A40 Time ≈ 51 hours ≈ 11 hours Table 10. Training Hyperparameters. The GPU time is for training our proposed model, and all other hyperparameters are the same for all models. 9. Appendix: Variable-Rate Baseline Models In the main paper (Sec. 5.2), we mentioned that we con- struct variable-rate versions of the two baseline models (i.e., MSH-VR and GMA-VR) in order to use them in the rate- incremental learning experiment. Fig. 11 shows the rate- distortion performance of the variable-rate versions com- pared to the original ones. As shown in the figure, the variable-rate versions achieve similar performance as the original ones, which validates the our experimental setting. 10. Appendix: Experimental Results 10.1. PSNR-Bpp curves for the main experiments Due to the space constraint, we show only BD-rate results without PSNR-bpp curves in the main paper. This section provides the PSNR-bpp curves for the main experiments (Sec. 5.3). Fig. 8 shows the PSNR-bpp curves for data-incremental learning experiments, which includes the backward compat- ibility experiment (Fig. 8a) and the new-data performance experiment (Fig. 8b). For backward compatibility, it is clear that models with fine-tuned encoder and decoder suffer a significant performance drop on the old bitstreams, while other fine-tuned models obtain comparable performance as the pre-trained models. Among them, our proposed knowl- edge replay strategy achieves even better performance than using the pre-trained model directly. For new-data perfor- mance, our method achieves comparable performance as the models with fine-tuned encoder and decoder (which are not backward compatible), and outperforms the pre-trained models by a clear margin. These observations are consis- tent with what we have observed in the BD-rate results in the main paper. We show the PSNR-bpp curves for rate-incremental learning experiments, including the low-to-high experiment (Fig. 9) and the high-to-low experiment (Fig. 10). The re- sults are consistent with previous observations: (1) Fine- tuning the encoder and decoder does not preserve back- ward compatibility, while our approach does; and (2) Our approach even outperforms all other methods in terms of new-rate performance. 10.2. Fine-tuning the encoder does not generalize the model to new rates We mentioned in Sec. 5.3 that fine-tuning the encoder alone cannot effectively extend the rate range of the pre-trained models. We provide an example for showing this in Fig. 12, where we show the rate-incremental learning (low → high)Down 4x ↓ 256 × 256 3 CNX Down 2x ↓ CNX Concat. CNX + CNX CNX 𝑒4 Up 4x ↑ CNX Linear ×6 64 × 64 128 32 × 32 256 ×6 CNXCNX 32 × 32 32 Down 2x ↓ CNX Concat. CNX + 𝑒3 Linear 𝑧3 16 × 16 384 ×6 CNXCNX 16 × 16 256 Down 2x ↓ CNX Concat. CNX + 𝑒2 Linear 𝑧2 8 × 8 256 ×4 CNXCNX 8 × 8 128 𝑒1 Down 2x ↓ CNX Concat. CNX + Linear 𝑧1 4 × 4 128 ×4 CNXCNX 4 × 4 128 4 × 4 128 𝑝𝑍4|𝑍<4 𝑧4 𝑝𝑍3|𝑍<3 𝑝𝑍2|𝑍1 𝑝𝑍1 Up 2x ↑ CNX CNX Up 2x ↑ CNX CNX Up 2x ↑ 4 × 4 128 8 × 8 256 16 × 16 384 32 × 32 256 CNX Concat. 𝑟2 CNX Up 2x ↑ Linear ×2 𝑒2 𝑧2 ×2 CNX Concat. 𝑟1 Linear ×2 𝑒1 𝑧1 4 × 4 128 𝑒0  𝑟0  4 × 4 128 8 × 8 256 8 × 8 256 CNX Concat. 𝑟3 CNX Up 2x ↑ Linear ×3 𝑒3 𝑧3 ×3 16 × 16 384 16 × 16 384 CNX Concat. 𝑟4 CNX Up 2x ↑ Linear ×3 𝑒4 𝑧4 ×3 32 × 32 256 32 × 32 256 Up 2x ↑ ×6 64 × 64 128 256 × 256 3 Phase 4 Phase 3 Phase 2 Phase 1 Encoder 𝑓enc Decoder 𝑓dec Entropy model 𝑝𝑍 Figure 7. Detailed architecture of the proposed model. In the figure, CNX denotes a ConvNeXt block [35] conditioned on lagrange multiplier λ, as described in Fig. 6. Dimensionality of the layer outputs are shown in the format ofheight × width and channels, where the spatial dimensions (height and width) are for a 256 × 256 input image, and they scales linearly with the input image size. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. Our model w/ FT Enc. & Dec. Our model w/ KR (a) Performance on the old bitsreams of Kodak (backward compatibility). Each subfigure shows the performance of a different model. 0.1 0.2 0.3 0.4 0.5 0.6 Bits per pixel (bpp) 30 32 34 36 38RGB PSNR CelebA-HQ, 256x256 MSH-VR, pre-trained MSH-VR w/ FT Enc. MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 Bits per pixel (bpp) 30 32 34 36 38RGB PSNR CelebA-HQ, 256x256 GMA-VR, pre-trained GMA-VR w/ FT Enc. GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 Bits per pixel (bpp) 30 32 34 36 38RGB PSNR CelebA-HQ, 256x256 Our model, pre-trained Our model w/ FT Enc. Our model w/ FT Enc. & Dec. Our model w/ KR (b) Performance on CelebA-HQ (new-data performance). Each subfigure shows the performance of a different model. Figure 8. PSNR-Bpp curves for data-incremental learning experiments. In figure (a), the “models, pre-trained” curves overlap with the “models w/ FT Enc. ”curves because their decoder are the same.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 26 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (ours) (a) Backward compatibility (bpp range is around [0.1, 0.9]). Each subfigure shows the performance of a different model. 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (ours) (b) New-rate performance (bpp range is around [0.1, 1.6]). Each subfigure shows the performance of a different model. Figure 9. PSNR-Bpp curves for rate-incremental learning (low → high) experiments. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (a) Backward compatibility (bpp range is around [0.1, 0.9]). Each subfigure shows the performance of a different model. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (b) New-rate performance (bpp range is around [0.03, 0.9]). Each subfigure shows the performance of a different model. Figure 10. PSNR-Bpp curves for rate-incremental learning (high → low) experiments.0.2 0.4 0.6 0.8 1.0 1.2 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak GMA-VR GMA (Cheng et al., 2020) MSH-VR MSH (Ballé et al., 2018) Figure 11. The variable-rate version of the baseline models that we constructed (MSH-VR and GMA-VR) are comparable to the original ones (MSH [41] and GMA [11]) in terms of PNSR-bpp performance on Kodak. 0.10.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.81.9 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. MSH-VR w/ FT Enc. & Dec. Figure 12. Fine-tuning the encoder does not effectively generalize the pre-train model (MSG-VR, for example) to new rates. performance of the pre-trained MSH-VR, the one with fine- tuned encoder (MSH-VR w/ FT Enc.), and the one with fine- tuned encoder and decoder (MSH-VR w/ FT Enc. & Dec. ). As shown in the figure, fine-tuning the encoder marginally extends the rate range of the pre-trained model, and the PSNR drops visibly when the rate is higher than maximum rate of the pre-trained model. Thus, we do not use this strat- egy in our rate-incremental learning experiments.",
      "meta_data": {
        "arxiv_id": "2402.18862v1",
        "authors": [
          "Zhihao Duan",
          "Ming Lu",
          "Justin Yang",
          "Jiangpeng He",
          "Zhan Ma",
          "Fengqing Zhu"
        ],
        "published_date": "2024-02-29T05:25:04Z",
        "pdf_url": "https://arxiv.org/pdf/2402.18862v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the problem of extending pre-trained neural image compressors to new data or target bitrates without breaking backward compatibility (i.e., the ability to decode bitstreams encoded by the original model). It proposes a knowledge replay training strategy and a new model architecture to achieve this. The main conclusion is that neural image compressors can be fine-tuned to achieve better performance on new data and rates while preserving backward compatibility, outperforming baseline solutions like end-to-end fine-tuning which disrupt compatibility.",
        "methodology": "The methodology comprises two independent components: a knowledge replay-based training strategy and a novel neural network architecture. The training strategy freezes the entropy model (pZ) to ensure backward compatibility and employs a knowledge replay loss (ℓKR) during fine-tuning. This loss is computed by encoding old training data with the pre-trained encoder, then decoding with the current decoder, and comparing the reconstruction to the original. This is combined with a standard Rate-Distortion (R-D) loss (ℓnew) for new data and rates, weighted by a hyperparameter α. The model architecture is inspired by hierarchical residual coding but decouples the entropy model and the decoder into two separate branches, resulting in a lightweight entropy model. It uses ConvNeXt blocks as building units and conditional convolution for variable-rate compression by conditioning on λ via affine transformations.",
        "experimental_setup": "Experiments were conducted in two continual learning scenarios: data-incremental learning (fine-tuning on a new dataset) and rate-incremental learning (fine-tuning with an extended rate range). Models were pre-trained on the COCO train2017 dataset (118,287 images, 256x256 patches). For data-incremental learning, models were fine-tuned on the CelebA-HQ dataset (24,000 training images, 256x256 pixels). For rate-incremental learning, COCO was used. Backward compatibility was evaluated on old bitstreams generated from the Kodak dataset. Performance metrics included bits per pixel (bpp), Peak Signal-to-Noise Ratio (PSNR) (RGB space), and BD-Rate. Baseline models included variable-rate versions of Mean & Scale Hyperprior (MSH-VR) and Gaussian Mixture & Attention (GMA-VR), compared against a pre-trained model, encoder-only fine-tuning, encoder-decoder fine-tuning (without knowledge replay), and the proposed knowledge replay strategy. Training utilized an Adam optimizer, specific learning rates, batch size 32, and gradient clipping, running on RTX 3090 and A40 GPUs.",
        "limitations": "The knowledge replay strategy assumes unconstrained access to old training resources (e.g., the entire old training dataset and the old encoder parameters), which may not be practical in all real-world scenarios. Additionally, the current research primarily focuses on maintaining the decoder's backward compatibility, while the problem of ensuring similar compatibility for the encoder is not explored.",
        "future_research_directions": "Future work could extend the current two-step pre-training and fine-tuning method to multi-step continual learning scenarios, allowing for adaptation over a sequence of tasks or data shifts. Another direction involves studying the problem of maintaining backward compatibility for the encoder, complementing the current focus on the decoder."
      }
    },
    {
      "title": "Forget-free Continual Learning with Winning Subnetworks"
    },
    {
      "title": "Forget-free Continual Learning with Winning Subnetworks"
    },
    {
      "title": "Impossibility Results for Grammar-Compressed Linear Algebra",
      "abstract": "To handle vast amounts of data, it is natural and popular to compress vectors\nand matrices. When we compress a vector from size $N$ down to size $n \\ll N$,\nit certainly makes it easier to store and transmit efficiently, but does it\nalso make it easier to process?\n  In this paper we consider lossless compression schemes, and ask if we can run\nour computations on the compressed data as efficiently as if the original data\nwas that small. That is, if an operation has time complexity\n$T(\\rm{inputsize})$, can we perform it on the compressed representation in time\n$T(n)$ rather than $T(N)$? We consider the most basic linear algebra\noperations: inner product, matrix-vector multiplication, and matrix\nmultiplication. In particular, given two compressed vectors, can we compute\ntheir inner product in time $O(n)$? Or perhaps we must decompress first and\nthen multiply, spending $\\Omega(N)$ time?\n  The answer depends on the compression scheme. While for simple ones such as\nRun-Length-Encoding (RLE) the inner product can be done in $O(n)$ time, we\nprove that this is impossible for compressions from a richer class: essentially\n$n^2$ or even larger runtimes are needed in the worst case (under complexity\nassumptions). This is the class of grammar-compressions containing most popular\nmethods such as the Lempel-Ziv family. These schemes are more compressing than\nthe simple RLE, but alas, we prove that performing computations on them is much\nharder.",
      "full_text": "arXiv:2010.14181v1  [cs.CC]  27 Oct 2020 Impossibility Results for Grammar-Compressed Linear Algebra Amir Abboud∗ Arturs Backurs† Karl Bringmann‡ Marvin K¨ unnemann§ Abstract To handle vast amounts of data, it is natural and popular to co mpress vectors and matrices. When we compress a vector from size N down to size n ! N, it certainly makes it easier to store and transmit eﬃciently, but does it also make it easier to process? In this paper we consider lossless compression schemes, and ask if we can run our computations on the compressed data as eﬃciently as if the original data wa s that small. That is, if an operation has time complexity T pinput-sizeq, can we perform it on the compressed representation in time T pnq rather than T pNq? We consider the most basic linear algebra operations: inne r product, matrix-vector multiplication, and matrix multiplication. In particular , given two compressed vectors, can we compute their inner product in time Opnq? Or perhaps we must decompress ﬁrst and then multiply, spend ing Ω pNq time? The answer depends on the compression scheme. While for simp le ones such as Run-Length-Encoding (RLE) the inner product can be done in Opnq time, we prove that this is impossible for compressions from a richer class: essentially n2 or even larger runtimes are needed in the worst case (under co mplexity assumptions). This is the class of grammar-compressions containing most popular methods such as the Lempel-Ziv family. These schemes are more compressing than the simple RLE, but alas, we prove that performing computations on them is much harder. 1 Introduction The idea of using compression to speed up computations can be found in any domain that deals with large- scale data, and ML is no exception. By exploiting redundancies and va rious forms of structure in a piece of data, compression algorithms such as zip can reduce its size from N down to n, where n ! N. The data becomes cheaper to store, access, transmit, and perhaps also to analyze. Can we run our ML tools on the compressed data, without decompressing it ﬁrst, and make the computation times proportional to n rather than N? Since most ML algorithms boil down to large amounts of basic algebra ic operations such as multiplications of vectors and matrices, with inner product as the atomic operation, the most basic question in this context is: Main Question.Given two N-dimensional vectors, each in a compressed form of size n ! N, can we compute their inner product in ˜Opnq time1 rather than OpNq? The answer, of course, depends on the compression scheme that we use. There seems to be an inherent tension: more complex schemes have higher compression rates but are harder to analyze without decompres- sion. First, let us clarify that our interest is in exact computations and lossless compressions, even though lossy techniques such as dimensionality reduction [16] are widely used by the ML community. In many cases, e.g. when performing a basic algebraic operation within a larger pipeline , even a small amount of error could add up to make the ﬁnal result unintelligible. Recent years has s een a growing interest in exploring the potential of lossless compression for speeding up ML [35, 83, 59 , 65]. An inspiring result was honorably ∗ IBM Almaden Research Center, amir.abboud@gmail.com † Toyota Technological Institute at Chicago, backurs@ttic.edu. Supported by an NSF Grant CCF-2006806. ‡ Saarland University and Max Planck Institute for Informati cs, Saarland Informatics Campus, bringmann @cs.uni-saarland.de. This work is part of the project TIPEA that has received fund ing from the European Research Council (ERC) under the European Unions Horizon 2020 research and in novation programme (grant agreement No. 850979). §Max Planck Institute for Informatics, Saarland Informatic s Campus, marvin@mpi-inf.mpg.de 1We use the notation ˜Opnq “ n ¨ Nop1q for near-linear time, hiding small terms such as log factors . 1Table 1: The potential savings from grammar-compressed linear algebra: Compression rates on real datasets. We compare zip, a standard grammar-compression, with Run Length Encoding (RLE), a simple method that works well on repetitive or sparse data. For more such results, se e [35, Table 1]. Dataset Size RLE (compression rate) zip (compression rate) ISOLET [30] 30.94 MB 29.83 MB (0.96) 7.94 MB (0.26) US Census 1990 [30] 342.26 MB 341.97 MB (0.99) 51.91 MB (0.15) mentioned as an outstanding paper at NeurIPS last year [65]: any N ˆ d matrix A can be compressed down to a matrix of size d ˆ d such that the optimal solutions of Least-Mean-Squares (LMS) ins tances are exactly the same on A and A1 . This is an example where for a speciﬁc task (LMS solvers) a speciﬁc c ompression scheme (designed by the authors) leads to a solution in time T pnq rather than T pNq, giving a 100x speedup on benchmark data; it makes one wonder if this approach can work in a more general setting. For rather simple compression methods, the answer to our question is positive. A recent Communications of the ACM article [35] exhibits Compressed Linear Algebra [32, 33, 34] a compression scheme for vectors and matrices that uses simple techniques such as Run Length Encoding ( RLE) and allows for fast computations on the compressed data with impressive experimental results when integrated into ML systems. The RLE encoding of a vector simply replaces runs of values by tuples indicatin g the value and the length of the run; e.g. the binary vector 00011111000 gets encoded as 0 31503. Given two vectors encoded in this way with size nRLE , a simple one-pass algorithm can compute their inner product in OpnRLE q time. Before that, there were many algorithms for exploiting succinct encodings of sparse vectors [78, 56, 52]; e.g. by simply listing the nonzero locations the binary vector 0100001000 gets encoded as p2, 7q. These encodings allow for a linear time inner product computation as well. However, these simple methods are often not very compressing. At the other end of the spectrum, we have the heavy-duty and time-tested family of Grammar-Compressions [54] that includes the Lempel-Ziv-family (LZ77, LZ78, LZW, etc.) [58, 91, 86], Byte-Pair Encoding [82], diction ary methods, and others [69, 63]. These compressions are used in ubiquitous applications such as zip, S nappy, GIF, PNG, the built-in Unix utility compress, and even in PDF. Their compression rates are often on a whole diﬀer ent level compared to RLE; e.g. the current draft of this paper reduces from 10KB to 4KB with zip but RLE has no eﬀect. See Table 1 and [35, Table 1] for empirical data showing the quantitat ive potential of these methods for some standard ML datasets. What all these more elaborate compr ession techniques have in common is that they essentially (up to low order terms [76]) encode a string (or vect or) by a Straight-Line Program (SLP): a restricted kind of a context-free grammar that can only produc e one string. In more detail, an SLP is deﬁned over some alphabet Σ, say t0, 1u, and it is a set of replacement rules (or productions) of a very simple form: a rule is either a symbol in Σ or it is the concatenation of tw o previous rules (under some ﬁxed ordering of the rules). The last replacement rule is the sequence de ﬁned by the SLP. For example, we can compress the sequence 01010101 with the rules S1 Ñ 0; S2 Ñ 1; S3 Ñ S1 S2; S4 Ñ S3 S3; S5 Ñ S4 S4 and S5 corresponds to the sequence 01010101. See Figure 1. For some s trings this can give an exponential compression, e.g. the sequence p01qN requires only Oplog Nq rules; note that its RLE has size N. While ﬁnding the smallest SLP for a given string is NP-Hard, it can be approx imated either by the above practical methods or provably up to logarithmic factors [76, 20, 79, 48, 50]. Thus, the holy grail in this context is to perform algebraic operation s in T pcompression-sizeq time even when the vectors are compressed with zip or one of the other heavy-du ty grammar compressions; that is, without unzipping them ﬁrst. Ideally, we would implement a “zip-inner-product” function that takes two zip ﬁles encoding vectors and computes the inner product in near-linea r time (which may not even be enough time to unzip them). A recent paper titled “When LZW meets ML” [59] m akes partial progress towards this goal: the inner product can be computed eﬃciently on their tuple oriented coding where each coordinate is grammar-compressed separately, but not the vector as a whole. This makes their method less compressing since, unlike with zip, the size of the encoding is always at least the dime nsionality of the vectors. Main Question (Restated). Given twoN-dimensional vectors, each grammar-compressed down to sizen ! N, can we compute their inner product in ˜Opnq time rather than OpNq? While eﬃciently analyzing these grammars may seem like a daunting task, a large body of works over the 2S1 Ñ 0 S2 Ñ 1 S3 Ñ S1 S2 S4 Ñ S3 S3 S5 Ñ S4 S4 (a) S5 S4 S3 S1 0 S2 1 S3 S1 0 S2 1 S4 S3 S1 0 S2 1 S3 S1 0 S2 1 (b) 0 1 S1 S2 S3 S4 S5 (c) Figure 1: (a) An SLP generating the sequence 01010101. (b) The c orresponding parse tree. (c) The acyclic graph corresponding to the SLP. last three decades has equipped us with an ingenious toolbox exactlyfor this purpose. It turns out that many important problems can indeed be solved surprisingly faster than th e decompress-then-solve bound, e.g. in pattern matching [71, 53, 11, 36, 18, 61, 40, 45, 49]. This gives hope for a positive answer to our question and that many ML computations could be sped up by operating on gramma r-compressions. These algorithms typically look at the parse trees that haveN leaves but onlyn distinctly labelled internal nodes (see Figure 1), and traverse them starting from the root down, while attempting t o only spend time proportional to the depth of the tree per distinct label. Using tricks that restructure the grammar to make the tree balanced, the depth can be upper bounded by Oplog Nq, making the total time Opn log Nq. To learn more about this subﬁeld of Algorithm Design, we refer the reader to the surveys [9 0, 57, 39, 81, 41, 73, 77, 64, 80]. 1.1 Our Results Alas, our main result is a negative resolution to the main question above. We apply the tools of theoretical computer science, and the recently blossoming ﬁeld of ﬁne-grained complexity, in order to shed light into the mathematical foundations of Compressed Linear Algebra. We prov e new hardness reductions showing cases where the time to compute the inner product must be large (under p opular complexity assumptions) even when the vectors have very small grammar compressions. For example, there are N-dimensional vectors with grammar-compressions of size n “ OpN1{3q where the inner product must take ˜Ωpn2q time2 to compute. The consequences to other settings such as matrix-vector multip lication are further explained below. This creates a strong separation between grammar-compressions, w here we prove an ˜Ωpn2q lower bound, and RLE, where an Opnq algorithm exists. This formally justiﬁes the use of simpler methods in M L systems and guides researchers away from searching for an ultra-eﬃcient “zip -inner-product” function. Fine-Grained Complexity Negative results are paramount to the success of any scientiﬁc dis cipline. The most prominent framework for proving such results in compute r science is the theory of NP-Hardness, where one proves that a problem cannot be solved in polynomial time u nless P “ NP which would imply breakthrough algorithms for famously-hard problems such as SAT and Subset Sum. Without this theory, countless hours would have been wasted by algorithm designers trying to come up with provable, worst-case, polynomial time algorithms for NP-Hard problems. Due to the increas e in data sizes of recent years, the ethos of this theory that “eﬃcient = polynomial” has become obsolet e, and a more demanding attitude where “eﬃcient = linear” has arisen. By replacing the polynomial redu ctions of NP-Hardness with more eﬃcient ones (often linear), ﬁne-grained complexity can prove har dness results even for problems that have polynomial time algorithms. Exemplary results show that linear or sub quadratic algorithms for certain problems, which admit quadratic-time algorithms, would refute popu lar assumptions (conjectures that are similar to but stronger than P ‰ NP ) and have breakthrough consequences for famously hard proble ms. 2The more standard notation is n2´ op1q which indicates an Ω pn1.9999q lower bound, no matter how close to 2 we go. That is, only mildly subquadratic algorithms are possible, e.g. by shaving log factors. 3One of the central assumptions in this theory and in this paper is the 3SUM Conjecture: “ No algorithm can decide, in subquadratic Opn2´ εq time, if there are three numbers that sum to zero among a given set of n numbers”. A recent survey on ﬁne-grained complexity [89] cites dozens of papers, mainly in computational geometry [38] but also in other ﬁelds [72, 85, 7, 8, 21, 55, 10, 43], th at prove 3SUM-Hardness results showing that their algorithms are optimal up to a refutation of this conject ure. In this paper, we prove the ﬁrst 3SUM-Hardness results in ML 3 as far as we are aware. The 3SUM assumption and its generalizations that we use in the theorems below are formally deﬁned and discussed in Sec tion 2. Vector Inner Product Our ﬁrst and main result is a reduction from 3SUM to compressed inne r product of two vectors, negatively resolving our main question. Theorem 1.1.Assuming the 3SUM conjecture, the inner product of two N-dimensional vectors that are grammar-compressed to size n “ ΘpN 1 4 q cannot be computed in Opn2´ εq time where ε ą 0. Moreover, we strengthen and generalize this result in several way s. First, we address the dependence between n and N: could it be that for more or less compressed vectors the picture is diﬀerent? Using a stronger variant of the 3SUM conjecture, the same lower bound of n2 holds even when n “ N1{3, and therefore our result can be stated as an ˜ΩpN 2 3 q lower bound which is quite close to the trivial upper bound of OpNq. Moreover, by a (highly nontrivial) boosting of our reduction, in Sec tion 3 we establish an ˜ΩpN 1 3 q lower bound with n “ Nε for any ε ď 1{3. That is, when the vectors are highly compressed even n10 time is not suﬃcient4; this is in stark contrast to the case of RLE-compressed vectors where Opnq is always possible. Matrix-Vector Multiplication Next, we consider the problem of computing the M ¨ v product of an N-dimensional vector v that is compressed to size n with an N ˆ N matrix M where each row is compressed to size Opnq. Perhaps computing these N inner products as a batch can be done faster than computing each separately. Alas, by another signiﬁcant boosting of our redu ction we prove that this is also impossible. While if the encoding is with RLE the product can be computed in OpNnq time, which is linear in the representation size of the matrix and thus optimal, it turns out tha t for grammar compressions ˜ΩpNn2q is required. The proof is in Section 4. Theorem 1.2.Assuming the 3SUM conjecture, the product of an N ˆ N-dimensional matrix, where each row is grammar-compressed to size n “ ΘpN 1 5 q, with an N-dimensional vector that is grammar-compressed to size n cannot be computed in OpNn2´ εq time where ε ą 0. Matrix Multiplication Finally, we consider matrix multiplication of compressed matrices C “ A ¨ B. There are multiple ways to compress an N ˆ N matrix: we might compress each row or each column, so that the compression size is N ¨ n, or treat the whole matrix as an N2-dimensional vector and compress it to size n. Each way may lead to a diﬀerent time complexity, but no matter which way we choose, the ﬁrst question to ask, and that will determine the time we can hop e for, is: what is the output size? The na¨ ıve answer is that the matrix C has size N ˆ N, but since A and B are compressed, shouldn’t we expect C to also be representable with a small grammar of size n ! N2? Unlike the above questions that deal with computation time, this is an information-theoretic questio n, and in Section 5 we give strong and unconditional negative answers: the matrix C cannot be grammar-compressed to size opN2{ log2 Nq even when A and B are strongly compressible. Moreover, some of our results hold eve n when A and B have very small RLE encodings. Therefore, our results should be of interest to the compressed linear algebra project beyond grammar-compressions. 3We remark that some complexity assumption is necessary for proving the kind of r esults we are interested, since uncon- ditionally proving even very weak lower bounds on the time co mplexity such as Ω pn1` εq and even for NP-Hard problems like SAT (not to mention inner product) is far beyond current tech niques [12]. 4Strictly speaking, such a conditional lower bound of Ω pn10q for highly compressible inputs can already be proven by com- bining a known #P-hardness reduction from SubsetSum [60] wi th a ﬁne-grained hardness of SubsetSum under the Exponentia l Time Hypothesis (see, e.g. [47]). However, such an approach yields only a weak lower bound in terms of the uncompressed si ze N, namely a bound of Ω pNǫq for some non-explicit, possibly tiny ǫ. Our lower bounds always give an explicit, reasonably large value for ǫ. 4Technical Remarks While the tools for proving NP-Hardness results for grammar-comp ressed data are old [64], they only apply in the unrealistic setting where n “ log N, and we are interested in more ﬁne- grained results. Only recently, a FOCS paper by the authors [2] int roduced the techniques for proving such lower bounds. This previous work focused on combinatorial patter n matching problems and the current work extends it to the setting of linear algebra. Our results establis h the hardness even of the simplest setting of binary vectors and matrices over t0, 1u. This setting is particularly studied due to its connection to graphs, where grammar compressions have also received a lot of attention [66, 67]. Moreover, we show that even deciding if the inner product is 0 or ě 1 is hard, and so our lower bounds hold against any bounded approximation algorithms. Extending the lower bounds to other functions such as computing the ℓ2 distance between two vectors is also easy. Like almost all results in ﬁne-grain ed complexity [89], our lower bounds are against both deterministic and randomized algorithms. Finally, we remark that our lower bounds are for the most basic setting of worst-case instances. Extending them to average-case results, showing that instances that come from certain natural d istributions are also hard, is an open question. However, notice that even if the original vectors come from a natural distribution, the distribution of the grammar representations will be completely diﬀerent (and probably far from natural). Therefore, exploiting the structure of non-worst-case instances seems far beyond current reach in this context. 1.2 Other Related Works There have been a few recent works showing ﬁne-grained complexity results for machine learning problems. In particular, [14] showed that the classic algorithm of Viterbi that computes the most likely path in a Hidden Markov Model which results in a given sequence of observatio ns is essentially optimal assuming certain complexity theoretical hypotheses. Another work [13] showed conditional hardness results for multiple empirical risk minimization problems such as kernel support vector m achines, kernel ridge regression, and training the ﬁnal layer of a neural network. Furthermore, there are many works that show hardness for problems that are used in machine learning literature. This includes co nditional lower bounds for kernel low- rank approximation [68], closest pair and its variants [9, 75, 88, 24,29, 28], maximum inner product [6, 22, 23], earth mover’s distance (a.k.a. Wasserstein metric) [74], dynamic time warping distance [3, 17]. Further contexts in which lossless compressions are used for ML ap plications, where the primary focus is on other aspects than increasing algorithmic performance, includ e compressing and accelerating models for deployment on resource-constrained devices (see [44, 26]; e.g ., lossless compressions are used to compress weights after a quantization step) or implementing the principle of min imum description length for feature learning (see [70]). Outside of ML, the idea of improving eﬃciency by operating on (lossles sly) compressed data is well- established in databases [1, 25, 87, 46], and is gaining traction also in b ioinformatics [84]. 2 Preliminaries As described in Section 1, a grammar compression of a sequence (ora vector) is an SLP that produces the sequence. In our proofs we will use the following simple observation a bout SLPs. Proposition 2.1. Let G be an SLP with start symbol S that generates a sequence s. For any α P N, we can compute an SLP G1 that generates the α-fold repetition of s, i.e., sα “ s s ¨ ¨ ¨ slooomooon α times , and has size |G| ` Oplog αq in time Op|G1 |q. Proof sketch. Using Oplog αq repeated squaring rules Si Ñ Si´ 1Si´ 1 and S0 Ñ S, we obtain non-terminals S0, . . . , S tlog2 αu generating s2i for i P t 0, . . . , tlog2 αuu. It is straightforward to combine these non-terminals, according to the binary representation of α, to generate sα using only Oplog αq additional non-terminals. Using this property, we can often compress sequences much more eﬃciently than run-length encoding alone could: E.g., repetitive patterns like p010011qn can be encoded using only Θplog nq bits instead of Θpnq. Indeed, our constructions crucially exploit a repeated application of this property to compress hard instances to very small sizes. 5The Complexity Assumptions As discussed in Section 1, the impossibility results in ﬁne-grained com- plexity are based on certain popular conjectures. One of the cent ral ones concerns the 3SUM problem, which has a few equivalent formulations (up to linear time transformations [31]); we will mostly use the following5. Deﬁnition 2.2 (The 3SUM Problem). Given three sets A, B, C of m integers in t1, . . . , U u, decide if there is a triple a P A, b P B, c P C such that a ` b “ c. It is a simple exercise (that is often given in interviews) to come up with an Opm2q time algorithm, and despite decades of eﬀorts, only mildly subquadratic Opm2{ logc mq bounds for a small 0 ă c ă 3 are known [15, 51, 42, 37, 19]. The 3SUM Conjecture.No algorithm can solve the 3SUM problem in Opm2´ εq time, where ε ą 0. A few remarks about this conjecture. First, a folklore trick of tak ing all numbers modulo a random large prime shows that the problem for arbitrary universe U is equivalent to the case where U “ Opm3 log2 mq (see Lemma B.1 in [5] for a proof). Therefore, we will assume this bou nd on U. When U becomes too small, the problem becomes easy due to an Opm ` U log Uq algorithm using Fast Fourier Transform [27]. However, the problem is conjectured to be hard even when U “ Θpm2q and this is referred to as the Strong 3SUM Conjecture [10, 2]. This stronger assumption allows us to strengthen our lower bounds by reducing N. Second, the hardness of the more general kSUM problem is also used as a complexity assumption [4, 2]. In the formulation that we will use, we are given k sets A1, . . . , A k of m integers in t1, . . . , U u where U “ Θpmrk{2s q and are asked to decide if there are k numbers, one from each set, such that a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. The Strong kSUM conjecture states that cannot be done in Opmrk{2s´ εq time, for any ε ą 0. We will use this assumption to prove lower bounds even whenn is much smaller thanN. Third, 3SUM and the other hardness assumptions in ﬁne-grained complexity are conjectured to be true even against randomized algorithms that succeed with high probability. This is important since some of our redu ctions are randomized. 3 Vector Inner Product In this section we present the proof of Theorem 1.1 by giving a reduction from 3SUM to the inner product of compressed vectors. A slightly weaker conditional lower bound o f ˜ΩpN1{2q for vectors compressible to n “ N1{4 can be extracted from the proof of Theorem 5.11 in [2]. We use similar tricks, but a diﬀerent and more optimized construction to obtain a stronger conditional lo wer bound of ˜ΩpN2{3q already on less compressible vectors with n “ N1{3. Technically, the novelty is that we manage to encode two sets ( A and B) into one vector of length mU rather than m2U. This new construction is crucial for the extensions we show – we do not see how to prove any lower bound for matrix-vecto r inner product without building on this new construction. Proof.Given an instance of 3SUM, that is, three sets A, B, C of m integers in t1, . . . , U u, we show how to construct vectors v1 A` B, v1 C P t 0, 1uN with N “ 2mU log2 m such that: (1) v1 A` B ¨ v1 C ě 1 if and only there a P A, b P B, c P C with a ` b “ c, (2) both vectors have a compression of size Opm log Uq, and (3) the construction time is Opm log Uq. This reduction suﬃces for proving Theorem 1.1 due to the following ca lculations. Since (as discussed in Section 2) we can assume that U “ Θpm3 log2 mq, the reduction produces two vectors of dimension N “ Θppm log mq4q and compressed size n “ ΘpN1{4q “ Θpm log mq, such that the inner product reveals the answer to the 3SUM instance. Therefore, an Opn2´ εq-time algorithm would solve the 3SUM instance in time Opm2´ εpolylogmq, refuting the 3SUM conjecture. Note that the Opm log Uq time for the reduction itself is negligible. Moreover, if we assume the Strong 3SUM conjectu re, we can start with 3SUM instances where U “ Opm2q and get vectors of dimension N “ Oppm log mq3q, ruling out inner product algorithms with time OpN 2 3 ´ εq. We now present the construction of the vectors. As a ﬁrst step,we observe that for any setX Ď t 1, ..., U u, we can compress its characteristic vector vX P t 0, 1uU , i.e., vX ris “ 1 iﬀ i P X, to size Op|X| log Uq as follows. We write X “ t x1, . . . , x |X| u with x1 ă x2 ă ¨ ¨ ¨ ă x|X| and observe that vX :“ 0x1´ 1 1 0x2´ x1´ 1 1 . . . 1 0x|X| ´ x|X|´ 1´ 1 1 0U´ x|X| , 5For example, instead of a ` b “ c or a ` b ` c “ 0 we may be given a target t and ask for a ` b ` c “ t. 6where each 0-block has length at most U and can thus be encoded using Oplog U) symbols using Proposi- tion 2.1. In total, we obtain a compression of size Op|X| log Uq, which can be computed in time Op|X| log Uq as well. Let A “ t a1, . . . , a nu. The central idea is to let v1 A` B, v1 C consist of m blocks of size 2 U, where the i-th block in v1 A` B gives the characteristic vector of the set ai ` B “ t ai ` b | b P Bu Ď t 1, . . . , 2Uu and the i-th block in v1 C gives the characteristic vector of C Ď t 1, . . . , 2Uu. Formally, we deﬁne v1 A` B :“ 0a1 vB 0U´ a1 lo oooo omo oooo on v1 a1` B 0a2 vB 0U´ a2 lo oooo omo oooo on v1 a2` B . . . 0am vB0U´ am lo ooooo omo ooooo on v1 am` B 0N´ 2mU , v1 C :“ vC 0U vC 0U . . . v C 0U 0N´ 2mU . (Here, the last block of 0s only serves to get the desired dimension o f N for technical reasons.) We observe that v1 A` B and v1 C have an inner product of at least 1 if and only if the characteristic ve ctors of some block i have a common 1-entry. Thus, consider any block i: We have v1 ai` Brks “ p vC 0U qrks “ 1 if and only if k ´ ai P B and k P C, i.e., ai P A, k ´ ai P B, k P C is a solution of the given 3SUM instance. Thus, v1 A` B ¨ v1 C ě 1 if and only if there is some a P A, b P B, c P C such that a ` b “ c, as desired. It remains to show that a Opm log Uq-sized compression of v1 A` B and v1 C can be computed in time Opm log Uq: Clearly, since vC 0U can be compressed to size Opm log Uq eﬃciently, we can also compress its m-fold repetition using Oplog mq additional symbols using Proposition 2.1, as well 0 N´ 2mU which takes Oplog Nq “ Oplog mUq additional symbols; thus, v1 C can be compressed to size Opm log mUq in time Opm log Uq. Furthermore, recall that we can compress vB to size Opm log Uq eﬃciently, and let G be an SLP with starting symbol SB generating vB. Thus, to compress vai` B, we only need to compress the surrounding blocks 0 ai , 0 U´ ai and can reuse SB to generate vB. Since we can encode the 0-blocks using Oplog Uq additional non-terminals, this yields a compression size of Oplog Uq per block i. Together with a Oplog mUq encoding of the trailing block 0 N´ 2mU , this yields again a compression of size Opm log Uq. Note that reusing a non-terminal generating vB was instrumental in giving a compression of sizeOpm log mq rather than Opm2 log mq and that this compression can indeed be computed in time Opm log Uq and concludes the claim. With more work, the above arguments can be generalized to reduce a kSUM instance with k sets of m integers in t1, . . . , U u to vectors of dimension N “ Θpmk´ 2Uq and compressed size Opm log Uq in time Opm log Uq. The main idea is to encode a shift of Ak´ 1 for each tuple of A1, . . . , A k´ 2 in one vector, and encode mk´ 2 repetitions of the remaining set Ak in the other vector. Under the Strong kSUM conjecture, this yields a conditional lower bound for inner product of ˜ΩpN1{3q where n “ OppN{Uq1{pk´ 2q log Nq. Thus, for any ﬁxed ε ą 0, let k be a suﬃciently large constant integer such that 1 {pk ´ 2q ă ε, then the Strong kSUM conjecture implies that N-dimensional vectors with compressed size n “ OpNεq cannot have an OpN1{3´ δ q algorithm for any constant δ ą 0. We formally prove the result in the appendix. 4 Matrix-Vector Multiplication In this section we sketch how to prove Theorem 1.2 by giving a reduction from 3SUM to Matrix-Vector multiplication on compressed data. We give a complete formal proof in the appendix. A helpful tool for this task is the following self-reduction for 3SUM,which follows from combining a known self-reduction [62] with a standard universe-size reduction techn ique on each produced instance [15, 72, 5]. Lemma 4.1 (Self-Reduction for 3SUM) . Let 1 ď s “ spmq ď m and ǫ ą 0 be arbitrary. If there is an algorithm that, given a target t and L “ Oppm{sq2q sets Aℓ, Bℓ, Cℓ of s integers in t1, . . . , O ps3 log2 squ, determines for all 1 ď ℓ ď L whether there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t in total time Opm2´ ǫq, then the 3SUM conjecture is false. Given the above self-reduction, the basic idea is as follows. We const ruct a matrix M whose rows are indexed by the instance 1 ď ℓ ď L and the aim is to construct the row Mℓ and the vector v such that Mℓ ¨ v ě 1 if and only if the instance Aℓ,Bℓ, Cℓ contains a solution, i.e., a P Aℓ, b P Bℓ, c P Cℓ with a` b` c “ t. Unfortunately, we cannot apply our Vector Inner Product const ruction directly: this would encode the set Aℓ ` Bℓ “ t a ` b | a P Aℓ, b P Bℓu into the row Mℓ and the set Cℓ into the vector v – however, in the matrix 7product Mv , each row Mℓ is multiplied with a ﬁxed vector v, while the Cℓ’s diﬀer for each ℓ. We overcome this issue by adapting our construction to encode the set Aℓ ` Bℓ ` Cℓ “ t a ` b ` c | a P Aℓ, b P Bℓ, c P Cℓu into the row Mℓ, and only the common target t into v. As all instances use the same target t, this is indeed possible. Speciﬁcally, using the ideas of Theorem 1.1, which produces a 2 sU-dimensional vectors encoding the sets A ` B and C, both having compressed size Ops log Uq, we show how to produce 3 s2U-dimensional vectors Mℓ and v encoding the sets Aℓ ` Bℓ ` Cℓ and ttu, both having compressed size Ops log Uq. This yields a pL ˆ 3s2Uq-dimensional matrix M and 3s2U-dimensional vector v. There is a choice s “ Θpm2{7q that leads to a quadratic matrix M with dimension N “ Θpm10{7q (as it has Oppm{sq2q “ Opm10{7q rows and Ops2Uq “ Ops5q “ Opm10{7q columns), with row compressions of size n “ Θps log sq “ Θpm2{7 log mq « N1{5. Thus, any OpNn2´ εq algorithm computing M ¨ v would solve 3SUM instances in time ˜Opm2´ 2ε{7q, refuting the 3SUM conjecture. 5 Matrix-Matrix Multiplication In this section, we consider the problem of computing the matrix product C of two N ˆ N matrices A, B. We consider the following representations of the input matrices: • Convenient compression: A is compressed row-wise, B is compressed column-wise. This repre- sentation allows us to compute any single entry Ci,j by running an inner product algorithm on the compressed row Ai and the compressed column Bj . The size of the input is OpN ¯ninq, where ¯nin is the maximum compressed size of the rows Ai and columns Bj . • Strong compression: For any matrix M, we deﬁne strong compression as a grammar compression of M or MT when viewed as n2-dimensional vector, whichever is shortest. When both A, B are given as strong compression, the resulting representation can have a m uch smaller size (it can be opNq), but to compute a single entry Ci,j , we ﬁrst might need to obtain a representation of the row Ai and the column Bj . Similarly, we have several options for representing C: • Row-wise compression of C. This compression is particularly useful if we aim to compute re- peated matrix products A1pA2p¨ ¨ ¨ p AkBqqq. The output size is OpN ¯noutq, where ¯nout is the maximum compressed size over all rows of C. • Column-wise compression of C. This compression is particularly useful if we aim to compute repeated matrix products pppAB1qB2q ¨ ¨ ¨ q Bk. The output size is OpN ¯noutq, where ¯nout is the maximum compressed size over all columns of C. • Strong compression of C. This compression has the smallest output size, which can be even opNq. We show the following result: Theorem 5.1.For inﬁnitely many N, there are N ˆ N matrices A, B with 1. convenient compression of size OpN log Nq (already under RLE), and 2. strong compression of size Oplog2 Nq, such that 3. the matrix product C “ AB has size ΩpN2{ log2 Nq in any grammar-compression (row-wise, column- wise, or strong). As a consequence, there can be no opN2{ log2 Nq algorithm for matrix-matrix multiplication (for any of our discussed representations), since already writing the output requires time ΩpN2{ log2 Nq. The rough proof strategy is to construct an instance C “ AB such that C and CT , when viewed as N2-dimensional vectors, contain all substrings of length 2 log 2 n. By the following standard lemma, such a string has no grammar compression of size opN2{ log Nq. 8Lemma 5.2 (see, e.g., [20, Lemma 3]) . Let ℓ P N. If a string x is generated by a grammar of size n, then x contains at most nℓ distinct substrings of length ℓ. Proof of Theorem 5.1. Let ℓ P N. We ﬁrst deﬁne the matrices A1 , B1 where A1 is a p2ℓ ˆ 2ℓq matrix with rows indexed by strings x P t 0, 1uℓ in lexicographic order, and B1 is a p2ℓ ˆ 2ℓp2ℓqq matrix with columns indexed by py, kq P t 0, 1uℓ ˆ t 1, . . . , 2ℓu in lexicographic order. For arbitrary z P t 0, 1uℓ, let diag pzq denote the ℓ ˆ ℓ diagonal matrix with z on the diagonal. We deﬁne A1 x :“ p x | 1ℓq, B 1 py,1q,...,py,2ℓq :“ ˆ diagp1ℓq 0 0 diagpyq ˙ . Let C1 “ A1 B1 be the p2ℓ ˆ 2ℓp2ℓqq product matrix of A1 and B1 , with rows and columns indexed by t0, 1uℓ and t0, 1uℓ ˆ t 1, . . . , 2ℓu, respectively. Observe that by deﬁnition, pCx,py,1q , . . . , C x,py,2ℓqq “ p x | yq for any x, y P t 0, 1uℓ. In particular, when we view C1 as a 22ℓp2ℓq-length string, it contains all strings in t0, 1u2ℓ as substrings, thus by Lemma 5.2, any row-wise compression is of siz e at least 2 2ℓ{p2ℓq. It is straightforward to make these matrices quadratic with dimens ion N “ Θpℓ2ℓq (by introducing all-0 columns) and to ensure that also column-wise compression has s ize Ωp22ℓ{ℓq “ ΩpN2{ log2 Nq (using transposed constructions to A1 and B1 ). Finally, we can compress each row of A1 and column of B1 trivially to length Opℓq “ Oplog Nq (already using RLE). In the appendix, we also argue how to grammar -compress the concatenation of the columns of A1 and the rows of B1 to size Opℓ2q “ Oplog2 Nq, which concludes the desired bound on the strong compression. Broader Impact The broader impact of our work is to inform algorithm design for compressed linear algebra, which can lead to faster algorithms for a variety of tasks on large data sets. The ethical consequences depend on the speciﬁc application. We do not see any inherently new concerns raised by our results, beyond those that follow generally from faster algorithms and an increased ability to process data. References [1] Daniel Abadi, Samuel Madden, and Miguel Ferreira. Integrating compression and execution in column- oriented database systems. In Proceedings of the 2006 ACM SIGMOD international conferenc e on Management of data , pages 671–682, 2006. [2] Amir Abboud, Arturs Backurs, Karl Bringmann, and Marvin K¨ un nemann. Fine-grained complexity of analyzing compressed data: Quantifying improvements over dec ompress-and-solve. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2 017, Berkeley, CA, USA, October 15-17, 2017 , pages 192–203, 2017. [3] Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. Tigh t hardness results for lcs and other sequence similarity measures. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 59–78. IEEE, 2015. [4] Amir Abboud and Kevin Lewi. Exact weight subgraphs and the k-su m conjecture. In International Colloquium on Automata, Languages, and Programming , pages 1–12. Springer, 2013. [5] Amir Abboud, Kevin Lewi, and Ryan Williams. Losing weight by gaining edges. In European Symposium on Algorithms , pages 1–12. Springer, 2014. [6] Amir Abboud, Aviad Rubinstein, and Ryan Williams. Distributed pcp th eorems for hardness of ap- proximation in p. In 2017 IEEE 58th Annual Symposium on Foundations of Computer S cience (FOCS), pages 25–36. IEEE, 2017. [7] Amir Abboud and Virginia Vassilevska Williams. Popular conjectures im ply strong lower bounds for dynamic problems. In 2014 IEEE 55th Annual Symposium on Foundations of Computer S cience, pages 434–443. IEEE, 2014. 9[8] Amir Abboud, Virginia Vassilevska Williams, and Oren Weimann. Conseq uences of faster alignment of sequences. In International Colloquium on Automata, Languages, and Prog ramming, pages 39–51. Springer, 2014. [9] Josh Alman and Ryan Williams. Probabilistic polynomials and hamming nea rest neighbors. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Scien ce, pages 136–150. IEEE, 2015. [10] A. Amir, T. M. Chan, M. Lewenstein, and N. Lewenstein. On hard ness of jumbled indexing. In Proc. ICALP, volume 8572, pages 114–125, 2014. [11] Amihood Amir, Gary Benson, and Martin Farach. Let sleeping ﬁles lie: Pattern matching in z- compressed ﬁles. Journal of Computer and System Sciences , 52(2):299–307, 1996. [12] Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach . Cambridge University Press, 2009. [13] Arturs Backurs, Piotr Indyk, and Ludwig Schmidt. On the ﬁne- grained complexity of empirical risk minimization: Kernel methods and neural networks. In Advances in Neural Information Processing Systems, pages 4308–4318, 2017. [14] Arturs Backurs and Christos Tzamos. Improving viterbi is har d: Better runtimes imply faster clique algorithms. In Proceedings of the 34th International Conference on Machin e Learning-Volume 70 , pages 311–321. JMLR. org, 2017. [15] Ilya Baran, Erik D Demaine, and Mihai Patra¸ scu. Subquadratic algorithms for 3sum. In Workshop on Algorithms and Data Structures , pages 409–421. Springer, 2005. [16] Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to im- age and text data. In Proceedings of the seventh ACM SIGKDD international confer ence on Knowledge discovery and data mining , pages 245–250, 2001. [17] Karl Bringmann and Marvin K¨ unnemann. Quadratic conditionallower bounds for string problems and dynamic time warping. In 2015 IEEE 56th Annual Symposium on Foundations of Computer S cience, pages 79–97. IEEE, 2015. [18] Patrick C´ egielski, Irene Guessarian, Yury Lifshits, and Yuri Matiyasevich. Window subsequence prob- lems for compressed texts. In Proc. 1st International Computer Science Symposium in Russ ia (CSR’06), pages 127–136. Springer, 2006. [19] Timothy M Chan. More logarithmic-factor speedups for 3sum,(m edian,+)-convolution, and some geo- metric 3sum-hard problems. ACM Transactions on Algorithms (TALG) , 16(1):1–23, 2019. [20] Moses Charikar, Eric Lehman, Ding Liu, Rina Panigrahy, Manoj Pr abhakaran, Amit Sahai, and Abhi Shelat. The smallest grammar problem. STOC’02 and IEEE Transactions on Information Theory , 51(7):2554–2576, 2005. [21] Kuan-Yu Chen, Ping-Hui Hsu, and Kun-Mao Chao. Approximate matching for run-length encoded strings is 3sum-hard. In Annual Symposium on Combinatorial Pattern Matching , pages 168–179. Springer, 2009. [22] Lijie Chen. On the hardness of approximate and exact (bichrom atic) maximum inner product. arXiv preprint arXiv:1802.02325, 2018. [23] Lijie Chen, Shaﬁ Goldwasser, Kaifeng Lyu, Guy N Rothblum, and A viad Rubinstein. Fine-grained com- plexity meets ip= pspace. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on D iscrete Algorithms, pages 1–20. SIAM, 2019. [24] Lijie Chen and Ryan Williams. An equivalence class for orthogonal v ectors. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithm s, pages 21–40. SIAM, 2019. 10[25] Zhiyuan Chen, Johannes Gehrke, and Flip Korn. Query optimizat ion in compressed database systems. In Proceedings of the 2001 ACM SIGMOD international conferenc e on Management of data , pages 271–282, 2001. [26] Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and Jagann athan Sarangapani. A comprehensive survey on model compression and acceleration. Artif. Intell. Rev. , 53(7):5113–5155, 2020. [27] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Cliﬀord Stein. Introduction to algorithms . MIT press, 2009. [28] Karthik CS and Pasin Manurangsi. On closest pair in euclidean metr ic: Monochromatic is as hard as bichromatic. In 10th Innovations in Theoretical Computer Science Conferen ce (ITCS 2019) . Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018. [29] Roee David and Bundit Laekhanukit. On the complexity of closest pair via polar-pair of point-sets. SIAM Journal on Discrete Mathematics , 33(1):509–527, 2019. [30] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2 017. [31] Bartlomiej Dudek, Pawel Gawrychowski, and Tatiana Starikovs kaya. All non-trivial variants of 3-ldt are equivalent. CoRR, abs/2001.01289, 2020. [32] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Com- pressed linear algebra for large-scale machine learning. Proc. VLDB Endow. , 9(12):960–971, 2016. [33] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Scaling machine learning via compressed linear algebra. SIGMOD Rec. , 46(1):42–49, 2017. [34] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Com- pressed linear algebra for large-scale machine learning. VLDB J. , 27(5):719–744, 2018. [35] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Com- pressed linear algebra for declarative large-scale machine learning. Commun. ACM , 62(5):83–91, 2019. [36] Martin Farach and Mikkel Thorup. String matching in Lempel-Ziv c ompressed strings. In Proc. 27th Annual ACM Symposium on Theory of Computing (STOC’95) , pages 703–712. ACM, 1995. [37] Ari Freund. Improved subquadratic 3sum. Algorithmica, 77(2):440–458, 2017. [38] Anka Gajentaan and Mark H. Overmars. On a class of opn2q problems in computational geometry. Computational Geometry , 5(3):165–185, 1995. [39] Leszek Gasieniec, Marek Karpinski, Wojciech Plandowski, and Wo jciech Rytter. Eﬃcient algorithms for Lempel-Ziv encoding. Proc. 5th Scandinavian Workshop on Algorithm Theory (SWAT’ 96), pages 392–403, 1996. [40] Pawe/suppress l Gawrychowski. Pattern matching in Lempel-Ziv compressed strings: fast, simple, and determin- istic. In Proc. 19th Annual European Symposium on Algorithms (ESA’11 ), pages 421–432. Springer, 2011. [41] Raﬀaele Giancarlo, Davide Scaturro, and Filippo Utro. Textual d ata compression in computational biology: a synopsis. Bioinformatics, 25(13):1575–1586, 2009. [42] Omer Gold and Micha Sharir. Improved bounds for 3SUM, K-SUM, and linear degeneracy. CoRR, abs/1512.05279, 2015. [43] Isaac Goldstein, Tsvi Kopelowitz, Moshe Lewenstein, and Ely Po rat. How hard is it to ﬁnd (honest) witnesses? arXiv preprint arXiv:1706.05815 , 2017. 11[44] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compr essing deep neural network with pruning, trained quantization and huﬀman coding. In Yoshua Bengio and Yann LeCun, editors, Proc. 4th International Conference on Learning Representations , ICLR 2016 , 2016. [45] Danny Hermelin, Gad M Landau, Shir Landau, and Oren Weimann. U niﬁed compression-based accel- eration of edit-distance computation. Algorithmica, 65(2):339–353, 2013. [46] Balakrishna R Iyer and David Wilhite. Data compression support in databases. In VLDB, volume 94, pages 695–704, 1994. [47] Klaus Jansen, Felix Land, and Kati Land. Bounding the running t ime of algorithms for scheduling and packing problems. SIAM J. Discret. Math. , 30(1):343–366, 2016. [48] Artur Je˙ z. Approximation of grammar-based compression via recompression. Theoretical Computer Science, 592:115–134, 2015. [49] Artur Je˙ z. Faster fully compressed pattern matching by rec ompression. ACM Transactions on Algo- rithms (TALG) , 11(3):20, 2015. [50] Artur Je˙ z. A really simple approximation of smallest grammar.Theoretical Computer Science , 616:141– 150, 2016. [51] Allan Grønlund Jørgensen and Seth Pettie. Threesomes, degen erates, and love triangles. In Proc. of the 55th Annual IEEE Symposium on Foundations of Computer Sc ience (FOCS) , pages 621–630, 2014. [52] Vasileios Karakasis, Theodoros Gkountouvas, Kornilios Kourtis , Georgios Goumas, and Nectarios Koziris. An extended compression format for the optimization of sp arse matrix-vector multiplication. IEEE Transactions on Parallel and Distributed Systems , 24(10):1930–1940, 2012. [53] Marek Karpinski, Wojciech Rytter, and Ayumi Shinohara. Patt ern-matching for strings with short descriptions. In Proc. Annual Symposium on Combinatorial Pattern Matching ( CPM’95), pages 205– 214. Springer, 1995. [54] John C. Kieﬀer and En-Hui Yang. Grammar-based codes: A new class of universal lossless source codes. IEEE Trans. Inf. Theory , 46(3):737–754, 2000. [55] Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Higher lower bound s from the 3sum conjecture. In Proceedings of the twenty-seventh annual ACM-SIAM symposi um on Discrete algorithms , pages 1272– 1287. SIAM, 2016. [56] Kornilios Kourtis, Georgios Goumas, and Nectarios Koziris. Optim izing sparse matrix-vector multipli- cation using index and value compression. In Proceedings of the 5th conference on Computing frontiers , pages 87–96, 2008. [57] N Jesper Larsson. Structures of string matching and data compression . Department of Computer Science, Lund University, 1999. [58] Abraham Lempel and Jacob Ziv. On the complexity of ﬁnite seque nces. IEEE Transactions on Infor- mation Theory , 22(1):75–81, 1976. [59] Fengan Li, Lingjiao Chen, Arun Kumar, Jeﬀrey F Naughton, Jign esh M Patel, and Xi Wu. When lempel-ziv-welch meets machine learning: A case study of acceleratin g machine learning using coding. arXiv preprint arXiv:1702.06943 , 2017. [60] Yury Lifshits. Processing compressed texts: A tractability bo rder. In Bin Ma and Kaizhong Zhang, editors, Proc. 18th Annual Symposium on Combinatorial Pattern Match ing (CPM 2007) , volume 4580 of Lecture Notes in Computer Science , pages 228–240. Springer, 2007. [61] Yury Lifshits, Shay Mozes, Oren Weimann, and Michal Ziv-Ukelso n. Speeding up hmm decoding and training by exploiting sequence repetitions. Algorithmica, 54(3):379–399, 2009. 12[62] Andrea Lincoln, Virginia Vassilevska Williams, Joshua R. Wang, and R . Ryan Williams. Deterministic time-space trade-oﬀs for k-sum. In International Colloquium on Automata, Languages, and Prog ram- ming, pages 58:1–58:14, 2016. [63] Qi Liu, Yu Yang, Chun Chen, Jiajun Bu, Yin Zhang, and Xiuzi Ye. R NACompress: Grammar-based compression and informational complexity measurement of RNA sec ondary structure. BMC bioinfor- matics, 9(1):176, 2008. [64] Markus Lohrey. Algorithmics on slp-compressed strings: A sur vey. Groups Complexity Cryptology , 4(2):241–299, 2012. [65] Alaa Maalouf, Ibrahim Jubran, and Dan Feldman. Fast and accur ate least-mean-squares solvers. In Advances in Neural Information Processing Systems 32: Annu al Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada , pages 8305– 8316, 2019. [66] Sebastian Maneth and Fabian Peternek. A survey on methods a nd systems for graph compression. arXiv preprint arXiv:1504.00616 , 2015. [67] Sebastian Maneth and Fabian Peternek. Grammar-based grap h compression. Information Systems , 76:19–45, 2018. [68] Cameron Musco and David Woodruﬀ. Is input sparsity time possible for kernel low-rank approximation? In Advances in Neural Information Processing Systems , pages 4435–4445, 2017. [69] Craig G Nevill-Manning and Ian H Witten. Compression and explanat ion using hierarchical grammars. The Computer Journal , 40(2 and 3):103–116, 1997. [70] Hristo S. Paskov, Robert West, John C. Mitchell, and Trevor J. Hastie. Compressive feature learning. In Christopher J. C. Burges, L´ eon Bottou, Zoubin Ghahramani, a nd Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems , pages 2931–2939, 2013. [71] Wojciech Plandowski. Testing equivalence of morphisms on conte xt-free languages. Proc. 2nd Annual European Symposium on Algorithms (ESA’94) , pages 460–470, 1994. [72] Mihai Pˇ atra¸ scu. Towards polynomial lower bounds for dynamic problems. In Proc. of the 42nd Annual ACM Symposium on Theory Of Computing (STOC) , pages 603–610, 2010. [73] Roberto Radicioni and Alberto Bertoni. Grammatical compress ion: compressed equivalence and other problems. Discrete Mathematics and Theoretical Computer Science , 12(4):109, 2010. [74] Dhruv Rohatgi. Conditional hardness of earth mover distance . arXiv preprint arXiv:1909.11068 , 2019. [75] Aviad Rubinstein. Hardness of approximate nearest neighbor s earch. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing , pages 1260–1268, 2018. [76] Wojciech Rytter. Application of Lempel–Ziv factorization to the approximation of grammar-based compression. Theoretical Computer Science , 302(1-3):211–222, 2003. [77] Wojciech Rytter. Grammar compression, lz-encodings, and st ring algorithms with implicit input. In Proc. 31st International Colloquium on Automata, Language s, and Programming (ICALP’04) , pages 15–27. Springer, 2004. [78] Yousef Saad. Iterative methods for sparse linear systems , volume 82. siam, 2003. [79] Hiroshi Sakamoto. A fully linear-time approximation algorithm for grammar-based compression. Journal of Discrete Algorithms , 3(2):416–430, 2005. [80] Hiroshi Sakamoto. Grammar compression: Grammatical infere nce by compression and its application to real data. In ICGI, pages 3–20, 2014. 13[81] D Sculley and Carla E Brodley. Compression and machine learning: A new perspective on feature space vectors. In Proc. Data Compression Conference (DCC’06) , pages 332–341, 2006. [82] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shi- nohara, and Setsuo Arikawa. Byte pair encoding: A text compress ion scheme that accelerates pattern matching. Technical report, Technical Report DOI-TR-161, Dep artment of Informatics, Kyushu Uni- versity, 1999. [83] Yasuo Tabei, Hiroto Saigo, Yoshihiro Yamanishi, and Simon J Puglisi. Scalable partial least squares re- gression on grammar-compressed data matrices. InProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1875–1884, 2016. [84] Kedar Tatwawadi, Mikel Hernaez, Idoia Ochoa, and Tsachy Weis sman. Gtrac: fast retrieval from compressed collections of genomic variants. Bioinformatics, 32(17):i479–i486, 2016. [85] Virginia Vassilevska and Ryan Williams. Finding, minimizing, and countin g weighted subgraphs. In Proceedings of the forty-ﬁrst annual ACM symposium on Theor y of computing , pages 455–464, 2009. [86] Terry A. Welch. A technique for high-performance data compr ession. Computer, 6(17):8–19, 1984. [87] Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerko tte. The implementation and performance of compressed databases. ACM Sigmod Record , 29(3):55–67, 2000. [88] Ryan Williams. On the diﬀerence between closest, furthest, and orthogonal pairs: Nearly-linear vs barely-subquadratic complexity. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms , pages 1207–1215. SIAM, 2018. [89] Virginia Vassilevska Williams. On some ﬁne-grained questions in algor ithms and complexity. In Pro- ceedings of the ICM , volume 3, pages 3431–3472. World Scientiﬁc, 2018. [90] Ian H Witten, Alistair Moﬀat, and Timothy C Bell. Managing gigabytes: compressing and indexing documents and images . Morgan Kaufmann, 1999. [91] Jacob Ziv and Abraham Lempel. A universal algorithm for sequen tial data compression. IEEE Trans- actions on Information Theory , 23(3):337–343, 1977. A Further Preliminaries For a sequence of vectorsv1, . . . , v ℓ, we let v1 v2 . . . v ℓ “ v1 ˝ v2 ˝ ¨ ¨ ¨ ˝ vℓ “ ⃝ ℓ i“ 1vi denote their concatenation. By the following observation, when proving a lower bound for a compr ession of size Θ pNγ q, the main task is to prove the upper bound n “ OpNγ q; the lower bound n “ ΩpNγ q can be ensured mechanically. Observation A.1. Let 0 ď γ ď 1. Given two N-dimensional vectors u, v of compressed size OpNγ q, we can compute two OpNq-dimensional vectors u1 , v1 of compressed size ΘpNγ q with the same inner product. Proof. Append 0Nγ using ΘpNγq additional rules to the encodings of u and v. The Strong kSUM Assumption To generalize the lower bound of Theorem 1.1 so that it works for an arbitrary relationship between compressed and uncompressed sizes, we will use an assumption about a generalized version of 3SUM. Deﬁnition A.2(The kSUM Problem). Given k sets A1, . . . , A k of m integers in t1, . . . , U u, decide if there are k numbers a1 P A1, . . . , a k P Ak such that a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. For all constant k ě 3 a simple meet-in-the-middle algorithm with hashing solves kSUM in Opmrk{2s q time, and no faster algorithm by mε factors, for any ε ą 0, is known to date, unless the universe size U is smaller than Opmrk{2s´ εq. This is because Fast Fourier Transform gives an Opm ` kU log Uq time algorithm [27]. It is conjectured that substantially faster algorithms do not e xist (e.g. in [4, 2]). 14The Strong kSUM Conjecture. For all constant k ě 3 it holds that: no algorithm can solve the kSUM problem with U “ Opmrk{2s q in Opmrk{2s´ εq time, where ε ą 0. Observe that this assumption is about all k ě 3 and therefore implies the Strong 3SUM conjecture as a special case. Intuitively, the reason this problem helps us give redu ctions where the vectors are much more compressible is that, compared to 3SUM, as k grows the ratio between the time complexity mk{2 and the input size m grows. B Vector Inner Product In this section, we prove the generalization of the lower bound of Theorem 1.1 to arbitrary relationships between compressed and uncompressed sizes of the vectors. Theorem B.1.Let 0 ă ε ă 1{3. Assuming the Strong kSUM conjecture for all constant k, the inner product of two N-dimensional vectors that are grammar-compressed to size n “ ΘpNεq cannot be computed in OpN1{3´ δq time, where δ ą 0. This result follows from the following stronger statement. Theorem B.2. Let k ě 3. Assuming the Strong kSUM conjecture, the inner product of two N-dimensional vectors that are grammar-compressed to size n “ ΘpN1{r 3k´ 4 2 s q cannot be computed in OpNp1{3` γk q´ δq time, where δ ą 0 and γk :“ # 2 3pk´ 1q , if k is odd, 4 9k´ 12 , if k is even. Observe that the above statement implies Theorem B.1: For any 0 ă ε ă 1{3, we choose k suﬃciently large such that 1 {r 3k´ 4 2 s ă ε. Then using Observation A.1, we obtain that any OpN1{3´ δ q-time algorithm for Vector Inner Product with compressed size n “ ΘpNεq would give an OpN1{3` γk ´ δ1 q-time algorithm for Vector Inner Product with compressed size OpN1{r 3k´ 4 2 s q “ OpNεq, where δ1 “ γk ` δ – this would refute the Strong kSUM conjecture by Theorem B.2. Furthermore, observe that if we set k “ 3, we obtain a ˜ΩpN2{3q lower bound for compressed size n “ ΘpN1{3q under the Strong 3SUM conjecture. In the remainder of this section, we give the proof of Theorem B.2. T he central construction is captured by the following lemma. Lemma B.3.Given sets A1, . . . , A k of integers in t1, . . . , U u, we deﬁne v1 A1`¨¨¨` Ak´ 1 :“ ⃝ pa1,...,ak´ 2qPA1ˆ¨¨¨ˆ Ak´ 2 in lexicographic order 0a1`¨¨¨` ak´ 2 vAk´ 1 0pk´ 2qU´ a1´¨¨¨´ ak´ 2 , v1 Ak :“ p vAk 0pk´ 2qU qmk´ 2 , where vAk´ 1 , vAk P t 0, 1uU denote the characteristic vectors of the sets Ak´ 1, Ak. We have the following properties: 1. The inner product of the mk´ 2pk ´ 1qU-dimensional vectors v1 A1`¨¨¨` Ak´ 1 and v1 Ak is nonzero if and only if there is a tuple pa1, . . . , a kq P A1 ˆ ¨ ¨ ¨ ˆ Ak with a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. 2. We can compute compressions of v1 A1`¨¨¨` Ak´ 1 , v1 Ak of size Opkm log Uq “ Opm log Uq in time Opm log Uq. Proof. For 1., observe that by construction,v1 A1`¨¨¨` Ak1 and v1 Ak consist of mk´ 2 blocks, indexed bypa1, . . . , a k´ 2q P A1 ˆ ¨ ¨ ¨ ˆ Ak´ 2 and consisting of the sequence 0a1`¨¨¨` ak´ 2 vAk´ 1 0pk´ 2qU´ a1´¨¨¨´ ak´ 2 and vAk 0pk´ 2qU of length pk ´ 1qU, respectively. In particular, in block pa1, . . . , a k´ 2q there is a common 1-entry t if and only if t “ p a1 ` a2 ` ¨ ¨ ¨ ` ak´ 2q ` a for some a P Ak´ 1 and t “ a1 for some a1 P Ak. Thus, there exists a common 1-entry in v1 A1`¨¨¨` Ak´ 2 and v1 Ak if and only if there are pa1, . . . , a kq P A1 ˆ ¨ ¨ ¨ ˆ Ak with a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. For 2., we ﬁrst recall that as shown in the proof of Theorem 1.1, we c an compute a compression of the characteristic vectors vAk´ 1 and vAk of size Opm log Uq in time Opm log Uq. Thus, using Proposition 2.1, we 15can compute a compression of v1 Ak “ p vAk 0pk´ 2qU qmk´ 2 of size Opm log Uq` Oplogppk´2qUqq` Oplog mk´ 2q “ Opm log Uq in time Opm log Uq. To show the claim for v1 A1`¨¨¨` Ak´ 1 , we proceed inductively and construct the strings v1 Ak´ 1 :“ vAk´ 1 and v1 Ai`¨¨¨` Ak´ 1 :“ ⃝ pai,...,ak´ 2qPAiˆ¨¨¨ˆ Ak´ 2 in lexicographic order 0ai`¨¨¨` ak´ 2 vAk´ 1 0pk´ 1´ iqU´ ai ´¨¨¨´ ak´ 2 , for i “ k ´ 2, . . . , 1. The central observation is that we can write Ai “ t apiq 1 , . . . , a piq m u with apiq 1 ă apiq 2 ă ¨ ¨ ¨ ă apiq m and obtain v1 Ai`¨¨¨` Ak´ 1 “ ⃝ m j“ 10apiq j v1 Ai` 1`¨¨¨` Ak´ 1 0U´ apiq j . Thus, given an SLP Gi` 1 for v1 Ai` 1`¨¨¨` Ak´ 1 with starting symbol Si` 1, we can give an SLP Gi for v1 Ai`¨¨¨` Ak´ 1 of size |Gi` 1| ` Opm log Uq as follows: For each j “ 1, . . . , m , we encode 0 apiq j using Oplog apiq j q “ Oplog Uq additional symbols, re-use Si` 1 to generate v1 Ai` 1`¨¨¨` Ak´ 1 , and encode 0 U´ apiq j using OplogpU ´ apiq j qq “ Oplog Uq additional symbols. Observe that we can obtain this compression in t ime Opm log Uq. Thus, starting from an SLP for v1 Ak´ 1 , after k ´ 2 steps we obtain an SLP G1 for v1 A1`¨¨¨` Ak´ 1 of size Opkm log Uq “ Opm log Uq. The running time of this construction is Opkm log Uq “ Opm log Uq, concluding the proof. Let A1, . . . , A k Ď t 1, . . . , U u be a Strong kSUM instance, i.e., U “ Opmrk{2s q. The reduction given in Lemma B.3 gives two vectors v, v1 of dimension mk´ 2 ¨ p k ´ 1qU such that their inner product allows us to decide the kSUM instance. Furthermore, the vectors have a compressed size of Opm log Uq. We slightly adaptv, v1 by appending 0’s to increase the dimension slightly toN “ mk´ 2¨pk´1qU logrp3k´ 4q{2s U (this does not change their inner product). We verify the following f acts: (1) an OpN1{3` γk ´ δq-time Vector Inner Product algorithm for some δ ą 0 refutes the Strong kSUM conjecture and (2) n “ OpN1{r 3k´ 4 2 s q. Using Observation A.1, this concludes the proof of Theorem B.2. For (1), consider ﬁrst the case that k is odd. Then U “ Opmpk` 1q{2q and N “ Opmk´ 2UpolylogUq “ Opm3pk´ 1q{2polylogmq. Observe that N1{3` γk´ δ “ Opm 3pk´ 1q 2 ¨p 1 3 ` 2 3pk´ 1q ´ δq polylogmq “ Opm k´ 1 2 ` 1´ 3pk´ 1q 2 δq “ Opmr k 2 s´ δ1 q, for any 0 ă δ1 ă 3pk ´ 1qδ{2. Similarly, for even k, we have U “ Opmk{2q and N “ Opmk´ 2UpolylogUq “ Opmp3k´ 4q{2polylogmq. Using 1{3 ` γk “ 1{3 ` 4{p9k ´ 12q “ k{p3k ´ 4q, we obtain that N1{3` γk ´ δ “ Opm 3k´ 4 2 ¨p k 3k´ 4 ´ δq polylogmq “ Opm k 2 ´ δ1 q, for any 0 ă δ1 ă p 3k ´ 4qδ{2. Thus, in both cases, an OpN1{3` γk ´ δq-time Vector Inner Product algorithm refutes the Strong kSUM conjecture by solving the given kSUM instance in time Opmrk{2s´ δ1 q with δ1 ą 0. Finally, for (2), note that N “ Opmk´ 2U logrp3k´ 4q{2s Uq “ Opmrp3k´ 4q{2s logrp3k´ 4q{2s mq. Thus n “ Opm log mq “ OpN1{rp3k´ 4q{2s q, as desired. C Matrix-Vector Product In this section we provide the full proof of Theorem 1.2. We ﬁrst prove a self-reduction for 3SUM as a central tool (using standard techniques), and then proceed to give the ﬁ nal reduction. C.1 Proof of the Self-Reduction Let us restate Lemma 4.1. 16Lemma C.1 (Self-Reduction for 3SUM) . Let 1 ď s “ spmq ď m and ε ą 0 be arbitrary. If there is an algorithm that, given a target t and L “ Oppm{sq2q sets Aℓ, Bℓ, Cℓ of s integers in t1, . . . , O ps3 log2 squ, determines for all 1 ď ℓ ď L whether there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t in total time Opm2´ ǫq, then the 3SUM conjecture is false. In the remainder of this section, we give the proof. LetA, B, C be sets of m integers in t1, . . . , U u. We use a couple of results from earlier work that are stated for the following 3SUM formulation: given three sets A1 , B1 , C1 of m integers in t´U, . . . , U u with U “ Opm3 log2 mq, we are asked to determine whether there are a P A1 , b P B1 , c P C1 such that a ` b ` c “ 0. We ﬁrst reduce our formulation to this formulation by settingA1 :“ A, B1 :“ B, and C1 :“ ´ C “ t´ c | c P Cu. We can now use the following known self-reduction for 3SUM. Lemma C.2(Reformulated from [62, Theorem 13]) . Let s :“ spmq with 1 ď s ď m. Given three sets A1 , B1 , C1 of m integers in t´U, . . . , U u, we can compute, in time Opm2{sq, a list of L “ Oppm{sq2q 3SUM instances, i.e., sets A1 ℓ, B1 ℓ, C1 ℓ with 1 ď ℓ ď L, such that there is an a P A1 , b P B1 , c P C1 with a ` b ` c “ 0 if and only if there is an instance 1 ď ℓ ď L and a triple a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c “ 0. Furthermore, each A1 ℓ, B1 ℓ, C1 ℓ is a subset of s integers of A1 , B1 , C1 , respectively. Proof sketch. We give the high-level arguments (for details, see the proof of The orem 13 in [62]). For a set S, let min S and max S denote the smallest and largest element in S, respectively. We sort A1 , B1 , C1 and split each array into rm{ss consecutive parts A1 1, . . . , A 1 rm{ss , B1 1, . . . , B 1 rm{ss , C1 1, . . . , C 1 rm{ss , each of at most s elements, such that max A1 i ă min A1 i` 1,max B1 i ă min B1 i` 1 and max C1 i ă min C1 i` 1 for all i. Instead of searching for a 3SUM triple a P A1 i, b P B1 j , c P C1 k for each 1 ď i, j, k ď rm{ss (i.e., Θppm{sq3q subproblems with s elements each), one observes that most subproblems can be trivially solved: We say that a subproblem pi, j, k q is trivial, if min Ai ` min Bj ` min Ck ą 0 or max Ai ` max Bj ` max Ck ă 0; these subproblems cannot contain a solution. The key insight is that there are at mostOppm{sq2q non-trivial subproblems (which follows since the domination partial ordering on t1, . . . , u u3 has at most Opu2q incomparable elements); these can be determined in time Oppm{sq2q. Thus, it suﬃces to list all Oppm{sq2q non-trivial subproblems with s integers in each set in time Opm2{sq. The resulting instances A1 ℓ, B1 ℓ, C1 ℓ consist of integers in t´U, . . . , U u with large universe size U “ Opm3 log2 mq. We reduce the universe size to Ops3 log2 sq using a folklore technique (a slightly stronger result with U “ Ops3q can be achieved using the techniques of [15]). To prepare notation, for any set S, we let S mod p :“ t s mod p | s P Su. Lemma C.3 (Adaptation of [5, Lemma B.1]) . There is some α such that U1 :“ αs3 log s log U satisﬁes the following property: Let A, B, C be sets of s integers in t´U, . . . , U u such that no a P A, b P B, c P C satisﬁes a ` b ` c “ 0. Let p be a prime chosen uniformly at random from t2, . . . , U 1 u. Then the probability that there are ap P A mod p, bp P B mod p, cp P C mod p with ap ` bp ` cp ” 0 pmod pq is at most 1{2. Proof. Let a P A, b P B, c P C be arbitrary. Since a`b`c ‰ 0, note that pa mod pq`pb mod pq`pc mod pq ” 0 pmod pq if and only if p divides a ` b ` c. Since a ` b ` c P t´ 3U, . . . , 3Uu, a ` b ` c has at most log 2p3Uq prime factors. Let P denote the number of prime numbers in t2, . . . , U 1 u; by the prime number theorem we can choose α large enough such that P ě 2s3 log2p3Uq. Thus, the probability that p was chosen among these at most log 2p3Uq prime factors is at most log 2p3Uq{P ď 1{p2s3q. Thus, by a union bound over all s3 triples a P A, b P B, c P C, the probability that there are ap P A mod p, bp P B mod p, cp P C mod p with a ` b ` c ” 0 pmod pq is at most 1 {2. Note that if A, B, C contain a triple a, b, c with a ` b ` c “ 0, then also A mod p, B mod p, C mod p contain a triple ap, bp, cp with ap ` bp ` cp ” 0 pmod pq for any p. We can ﬁnally prove Lemma C.1: Assume that there is an algorithm A that given a target t and L “ Oppm{sq2q instances Aℓ, Bℓ, Cℓ, 1 ď ℓ ď L of s integers in t1, . . . , U 1 u, determines for all 1 ď ℓ ď L whether there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t in total time Opm2´ εq with ε ą 0. Observe that since A runs in time Opm2´ εq, we must have s “ Ωpmεq, since otherwise already the size of the input to A of Θpm2{sq would be ωpm2´ εq. Thus, we have U1 “ Ops3 log2 sq. For r “ 1, . . . , γ log m many repetitions, we do the following: We choose a random prime pr P r 2, U 1s and obtain ℓ instances in t0, . . . , p r ´ 1u Ď t 0, . . . , U u by taking the sets modulo pr, i.e., Aprq ℓ :“ A1 ℓ mod pr, 17Bprq ℓ :“ B1 ℓ mod pr, and Cprq ℓ “ C1 ℓ mod pr. Observe that we may determine whether there is some a P Aprq ℓ , b P Bprq ℓ , c P Cprq ℓ with a ` b ` c ” 0 pmod prq by testing for each t P t 0, pr, 2pru, whether there a P Aprq ℓ , b P Bprq ℓ , c P Cprq ℓ with a ` b ` c “ t. Thus, to do this, and additionally ensure that each integer is in t1, . . . , U 1 u, we add 1 to each integer in Aprq ℓ , Bprq ℓ , Cprq ℓ and for each λ P t 0, 1, 2u, call A on the sets Aprq ℓ , Bprq ℓ , Cprq ℓ , 1 ď ℓ ď L with common target tλ :“ 3 ` λpr. Observe that after these 3 γ log m calls to A, we know for each 1 ď ℓ ď L and 1 ď r ď γ log m whether there are a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c ” 0 pmod prq. We declare our original 3SUM instance A, B, C to be a YES instance if and only if there is some ℓ such that for all r we have found a witness a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c ” 0 pmod prq. Note that if A, B, C is a YES instance, we always return YES by Lemma C.2. Otherwise, if A, B, C is a NO instance, consider a ﬁxed ℓ. By Lemmas C.2 and C.3, the probability that for all r, we ﬁnd a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c ” 0 pmod prq is bounded by 2´ γ log m “ m´ γ. Thus, by a union bound over all ℓ, the probability that we incorrectly return YES in this case is at most Lm´ γ “ Oppm{sq2m´ γq “ Opm2´ γ q. We can make this error probability polynomially small by choosing γ ą 2. Observe that the running time of the above process is Oplog mq times the running time of A (note that the running time used for Lemma C.2 is linear in its output size, which is th e input size of A and thus dominated by the running time of A). Thus, we can solve any 3SUM instance in time Opm2´ ε log mq, which would refute the 3SUM conjecture. This concludes the proof of Le mma C.1. C.2 Main Reduction for Matrix-Vector Multiplication We now turn to the proof of Theorem 1.2. Proof.Let s be a parameter to be chosen later. By Lemma 4.1, it suﬃces to solve L “ Oppm{sq2q 3SUM instances Aℓ, Bℓ, Cℓ consisting of s integers in t1, . . . , U u, U “ Ops3 log2 sq with common target 1 ď t ď 3U in time Opm2´ ǫq for some ǫ ą 0 to contradict the 3SUM conjecture. We construct an pL ˆ 3s2Uq matrix M and v P t 0, 1u3s2U as follows. Intuitively, each row Mℓ and the vector v are partitioned into s2 blocks of size 3 U. Each block is indexed by pi, jq with i, j P t 1, . . . , s u in lexicographic order and the block of Mℓ corresponding to pi, jq encodes the characteristic vector of the set ai ` bj ` Cℓ “ t ai ` bj ` c | c P Cℓu Ď t 1, . . . , 3Uu, where ai is the i-th integer in Aℓ and bj is the j-th integer in Bℓ. Correspondingly, every block pi, jq in v encodes the characteristic vector of the singleton set ttu Ď t 1, . . . , 3Uu. Thus, there is a position in block pi, jq in which both Mℓ and v have a 1 if and only if there is a c P Cℓ such that ai ` bj ` c “ t. Formally, for any 1 ď ℓ ď L, we write Aℓ “ t aℓ 1, . . . , a ℓ su, Bℓ “ t bℓ 1, . . . , b ℓ su and deﬁne Mℓ :“ 0a1` b1 vCℓ 03U´ a1´ a2 looooooooooomooooooooooon vaℓ 1` bℓ 1` Cℓ . . . 0ai` bj vCℓ 03U´ ai´ bj looooooooooomooooooooooon vaℓ i` bℓ j` Cℓ . . . 0as` bs vCℓ 03U´ as´ bs looooooooooomooooooooooon vaℓ s` bℓ s` Cℓ , v :“ 0t´ 1103U´ t . . . 0t´ 1103U´ t . . . 0t´ 1103U´ t, where vCℓ P t 0, 1uU denotes the characteristic vector of Cℓ. By this structure, it is clear that Mℓv ě 1 if and only if there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t. We will show that each row Mℓ can be compressed to size Θ ps log sq (as opposed to its RLE of length Θps3 log sq). We thus will set N “ r3s2U log3 ss “ Θps5 log5 sq, and append 0 N´ 3s2U to each row Mℓ and v, so that we obtain an Lˆ N matrix M1 and N-dimensional vector v1 whose product M1 v1 can be used to solve all instances Aℓ, Bℓ, Cℓ in linear time. Observe that each row has a compression of size Θ pN1{5q “ Θps log sq, as desired. Since L “ Oppm{sq2q and N ě s5, we can set s “ Θpm2{7q such that L ď N (we can indeed make L “ N by introducing zero rows, if necessary). Thus, an OpNn2´ ǫq-time algorithm for multiplying M1 and v1 would solve all L 3SUM instances in time OpNn2´ ǫq “ Oppm{sq2ps log sq2´ ǫq “ Oppm2{sǫqpolylogsq “ Opm2´ 2 7 ǫpolylogmq, which would refute the 3SUM conjecture. Analogous to the proof of Theorems 1.1 and B.2, we can compute a co mpression of size Θ ps log sq in time Ops log sq. Indeed, for each Mℓ, this already follows from Lemma B.3 when setting A1 :“ Aℓ, A2 :“ 18Bℓ, A3 :“ Cℓ, which shows how to compress the string v1 A1` A2` A3 “ Mℓ to size Ops log Uq “ Ops log sq in time Ops log Uq “ Ops log sq. For v, we simply apply Proposition 2.1 to the straightforward compression of 0t´ 1103U´ t to size Oplog Uq, which leads to a compression of v of size Oplog U ` log sq “ Oplog sq. Using Observation A.1, we can make all encodings have size Θ ps log sq, which concludes the proof. D Matrix-Matrix Product In this section, we give the full proof of Theorem 5.1. Proof of Theorem 5.1.Let ℓ P N. We ﬁrst deﬁne the matrices A1 , B1 where A1 is a p2ℓ ˆ 2ℓq matrix with rows indexed by strings x P t 0, 1uℓ in lexicographic order, and B1 is a p2ℓ ˆ 2ℓp2ℓqq matrix with columns indexed by py, kq P t 0, 1uℓ ˆ t 1, . . . , 2ℓu in lexicographic order. For arbitrary z P t 0, 1uℓ, let diag pzq denote the ℓ ˆ ℓ diagonal matrix with z on the diagonal. We deﬁne A1 x :“ p x | 1ℓq, B 1 py,1q,...,py,2ℓq :“ ˆ diagp1ℓq 0 0 diagpyq ˙ . Let C1 “ A1 B1 be the p2ℓ ˆ 2ℓp2ℓqq product matrix of A1 and B1 , with rows and columns indexed by t0, 1uℓ and t0, 1uℓ ˆ t 1, . . . , 2ℓu, respectively. Observe that by deﬁnition, pCx,py,1q , . . . , C x,py,2ℓqq “ p x | yq for any x, y P t 0, 1uℓ. In particular, when we view C1 as a 22ℓp2ℓq-length string, it contains all strings in t0, 1u2ℓ as substrings, thus by Lemma 5.2, any row-wise compression is of siz e at least 2 2ℓ{p2ℓq. To also ensure column-wise incompressibility, we slightly extend the construction by analogous transposed constructions: We let N :“ 2ℓp2ℓ ` 1q and deﬁne the ﬁnal pN ˆ Nq matrices A, B as follows: A :“ ˆ A1 0 0 0 B1T 0 ˙ , B :“ ¨ ˝ B1 0 0 A1T 0 0 ˛ ‚ . Since C :“ AB “ ˆ A1 B1 0 0 pA1 B1 qT ˙ contains all length-p2ℓq strings as substrings of the rows (in the A1 B1 part) and as substrings of the columns (in the pA1 B1 qT part), any strong compression of C is of size at least 22ℓ{p2ℓq “ ΩpN{ log2 Nq, proving the third part of the claim. For the ﬁrst two parts, it remains to show that A and B can be well compressed: For the convenient compression, we observe that any row in A is either of the form px1ℓ | 02ℓ | 0N´ 4ℓq, which has a RLE of length at most |x1ℓ| ` Oplog Nq “ Oplog Nq, or it is of the form p02ℓ | 0i´ 1α02ℓ´ i | 0N´ 4ℓq for some α P t 0, 1u, i P t 1, ..., 2ℓu, which also has a RLE of length at most Oplog Nq. Thus, each of the N rows of A can be compressed to size Oplog Nq, as desired. By a symmetric statement, also each column of B has a RLE of size Oplog Nq. Finally, for the strong compression, we show that we compress AT when viewed as a string, i.e., we compress the concatenation of the columns of A. The main insight is the following: Imagine a binary ℓ- bit counter. Using grammar compression, we can compress the seq uence of values of any ﬁxed bit while the counter counts from 0 to 2 ℓ ´ 1 in size Opℓq. Formally, let G0, G1 be grammar compressions of strings s0,s1. For any 1 ď i ď ℓ, we can encode ps2ℓ´ i 0 s2ℓ´ i 1 q2i´ 1 using only Opℓq additional non-terminals in the canonical way. Speciﬁcally, using Opℓ ´ iq new symbols, we may encode s2ℓ´ i 0 s2ℓ´ i 1 ; let ˜S denote the corresponding non-terminal. We then encode ˜S2i´ 1 using Opiq additional new symbols. In total, we only need Oppℓ ´ iq ` iq “ Opℓq additional symbols, as desired. We apply the above idea to encode the concatenation all columns of A as follows: Consider column i. • For 1 ď i ď ℓ, then by the chosen lexicographic order of the row indices x P t 0, 1uℓ of A1 , note that the i-th column of A is of the form p02ℓ´ i 12ℓ´ i q2i´ 1 | 0N´ 2ℓ . Using the above analysis, we can compress it to size Opℓq ` Oplog Nq “ Oplog Nq. • If ℓ ` 1 ď i ď 2ℓ, the i-th column is of the form 1 2ℓ | 0N´ 2ℓ , which we can compress to size Oplog ℓ ` log Nq “ Oplog Nq. 19• If 2 ℓ ` 1 ď i ď 3ℓ, write i “ 2ℓ ` i1 and observe that the i-th column of A is of the form 0 2ℓ | p0i1 ´ 110ℓ´ i1 q2ℓ . Using Opℓq non-terminals to encode 0 i1 ´ 110ℓ´ i1 , it is immediate that we can compress the complete column using Opℓq additional non-terminals, i.e., yielding a total of Opℓq “ Oplog Nq. • If 3ℓ ` 1 ď i ď 4ℓ, write i “ 3ℓ ` i1 and observe that by the chosen lexicographic order of the column indices py, kq P t 0, 1uℓ ˆ t 1, . . . , 2ℓu of B1 , the i-th column of A is of the form 0 2ℓ | p s2ℓ´ i1 0 s2ℓ´ i1 1 q2i1 ´ 1 where sα :“ 0i1 ´ 1α1ℓ´ i1 . We can give trivial grammars of size Opℓq for s0, s1. Then, by the above analysis, we only need Opℓq additional non-terminals for the counter-like part. In total, we on ly need Opℓq “ Oplog Nq non-terminals to encode the i-th column. • Finally, observe that the remaining columns i “ 4ℓ ` 1, . . . , N consist of pN ´ 4ℓqN zeroes, which we can encode together using only Oplog Nq non-terminals. In summary, we can encode the ﬁrst 4 ℓ columns using Oplog Nq non-terminals each, and only Oplog Nq non-terminals for the remaining columns, so we can fully compress th e concatenation of A’s columns to size Oplog2 Nq, as claimed. 20",
      "meta_data": {
        "arxiv_id": "2010.14181v1",
        "authors": [
          "Amir Abboud",
          "Arturs Backurs",
          "Karl Bringmann",
          "Marvin Künnemann"
        ],
        "published_date": "2020-10-27T10:33:01Z",
        "pdf_url": "https://arxiv.org/pdf/2010.14181v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper investigates whether computations can be run efficiently on grammar-compressed data (size n) as opposed to uncompressed data (size N). It proves impossibility results (lower bounds under complexity assumptions) for fundamental linear algebra operations like inner product, matrix-vector multiplication, and matrix multiplication when data is grammar-compressed. Specifically, it shows that for grammar-compressed vectors of size n, computing their inner product can require Ω(n^2) time (e.g., Ω(N^(2/3)) for n=N^(1/3)), contrasting with O(n) for simpler schemes like Run-Length Encoding (RLE). For matrix-vector multiplication, it demonstrates Ω(Nn^2) time. Unconditionally, it shows that matrix multiplication outputs (C=AB) cannot be grammar-compressed to o(N^2 / log^2 N) even if inputs are highly compressible, implying a lower bound for computation. These results establish a strong separation between grammar-compressions and simpler methods, guiding algorithm design in compressed linear algebra.",
        "methodology": "The authors use fine-grained complexity theory, employing hardness reductions from popular complexity assumptions, primarily the 3SUM Conjecture and its generalizations (Strong 3SUM and kSUM conjectures). The core technique involves constructing specific \"hard\" instances of N-dimensional binary vectors and matrices that are highly compressible by Straight-Line Programs (SLPs, the formal model for grammar-compressions). These constructions ensure that solving the compressed linear algebra problem (e.g., inner product) would imply a sub-quadratic algorithm for 3SUM, thereby refuting the conjecture. Key elements include Proposition 2.1 for efficient compression of repeated sequences and self-reductions for 3SUM.",
        "experimental_setup": "Not mentioned",
        "limitations": "The impossibility results are conditional upon the validity of widely believed complexity assumptions, such as the 3SUM conjecture. If these conjectures are false, the lower bounds would not hold. The current lower bounds are established for worst-case instances; extending them to average-case scenarios or natural data distributions is noted as an open and challenging problem. The results specifically apply to binary vectors and matrices over {0,1} and exact, lossless computations, though they are extensible to bounded approximation algorithms and other L2 distance functions.",
        "future_research_directions": "The primary future research direction explicitly stated is to investigate whether the conditional lower bounds can be extended to average-case results, demonstrating that instances derived from certain natural data distributions are also inherently hard to process when grammar-compressed. The paper also implies that general \"zip-inner-product\" functions for arbitrary grammar-compressed vectors are unlikely to be highly efficient, steering future work towards simpler compression schemes or task-specific compression designs."
      }
    },
    {
      "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
      "abstract": "We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.",
      "full_text": "Online Learned Continual Compression with Adaptive Quantization Modules Lucas Caccia 1 2 3 Eugene Belilovsky 2 4 Massimo Caccia 2 4 5 Joelle Pineau 1 2 3 Abstract We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a rep- resentative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting en- counters a major challenge: representations de- rived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the com- pression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and cur- rent progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to sig- niﬁcant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learn- ing agents. 1. Introduction Interest in machine learning in recent years has been fueled by the plethora of data being generated on a regular basis. Effectively storing and using this data is critical for many applications, especially those involving continual learning. In general, compression techniques can greatly improve data storage capacity, and, if done well, reduce the memory and compute usage in downstream machine learning tasks (Gueguen et al., 2018; Oyallon et al., 2018). Thus, learned compression has become a topic of great interest (Theis 1McGill 2Mila 3Facebook AI Research 4University of Montreal 5ElementAI. Correspondence to: Lucas Caccia <lucas.page-caccia@mail.mcgill.ca>, Eugene Belilovsky <eu- gene.belilovsky@umontreal.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). et al., 2017; Ball´e et al., 2016; Johnston et al., 2018). Yet its application in reducing the size of datasets bound for machine learning applications has been limited. This work focuses on the following familiar setting: new training data arrives continuously for a learning algorithm to exploit, however this data might not be iid, and further- more there is insufﬁcient storage capacity to preserve all the data uncompressed. We may want to train classiﬁers, reinforcement learning policies, or other models continu- ously from this data as it’s being collected, or use samples randomly drawn from it at a later point for a downstream task. For example, an autonomous vehicle (with bounded memory) collects large amounts of high-dimensional train- ing data (video, 3D lidar) in a non-stationary environment (e.g. changing weather conditions), and overtime applies an ML algorithm to improve its behavior using this data. This data might be transferred at a later point for use in down- stream learning. Current learned compression algorithms, e.g. (Torfason et al., 2018), are not well designed to deal with this case, as their convergence speed is too slow to be usable in an online setting. In the ﬁeld of continual/lifelong learning (Thrun & Mitchell, 1995), which has for now largely focused on classiﬁcation, approaches based on storing memories for later use have emerged as some of the most effective in online settings (Lopez-Paz et al., 2017; Aljundi et al., 2018; Chaudhry et al.; 2019; Aljundi et al., 2019). These memories can be stored as is, or via a generative model (Shin et al., 2017). Then, they can either be used for rehearsal (Chaudhry et al., 2019; Aljundi et al., 2019) or for constrained optimization (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2018). Indeed many continual learning applications would be greatly improved with replay approaches if one could afford to store all samples. These approaches are however inherently limited by the amount of data that can be stored Learning a generative model to compress the previous data stream thus seems like an appealing idea. However, learn- ing generative models, particularly in the online and non- stationary setting, continues to be challenging, and can greatly increase the complexity of the continual learning task. Furthermore, such models are susceptible to catas- trophic forgetting (Aljundi et al., 2019). An alternate ap- proach is to learn a compressed representation of the data, arXiv:1911.08019v3  [cs.LG]  20 Aug 2020Online Learned Continual Compression with Adaptive Quantization Modules Online Continual Compression z_1 z_1 z_2 z_3z_1 z_2 X ~ non-iid t=1 t=2 t=3 query  samples query  samples Learner Learner Encoder Encoder Encoder Decoder     t=3 Decoder     t=1 Decoder     t=2 storage storage storage  Decoder     t=3 z_1 Decoder     t=1 Representation Drift z_1 X_3 X_2 X_1 Figure 1.Illustration of the challenges in the Online Continual Compression problem. A model must be able to decode representations encoded by previous versions of the autoencoder, permitting anytime access to data for the learner. This must be accomplished while dealing with a time-varying data distribution and ﬁxed memory constraints which can be more stable than learning generative models. Learned compression in the online and non-stationary set- ting itself introduces several challenges illustrated in Fig 1. Firstly the learned compression module must be able to de- code representations encoded by earlier versions of itself, introducing a problem we refer to as representation drift. Secondly, the learned compressor is itself susceptible to catastrophic forgetting. Finally, the learned compression needs to be adaptive to maintain a prescribed level of re- construction quality even if it has not fully adapted to the current distribution. In this work we demonstrate that the VQ-V AE framework (van den Oord et al., 2017; Razavi et al., 2019), originally introduced in the context of generative modeling and density estimation, can be used online to effectively address repre- sentation drift while achieving high compression. Further- more, when augmented with an internal replay mechanism it can overcome forgetting. Finally we use propose to use multiple gradient-isolated compression levels to allow the compressor to adaptively store samples at different compres- sion scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. The main contributions of this work are: (a) we intro- duce and highlight the online learned continual compres- sion (OCC) problem and its challenges. (b) We show how representation drift, one of the key challenges, can be tackled by effective use of codebooks in the VQ-V AE framework. (c) We propose an architecture using multi- ple VQ-V AE’s, adaptive compression scheme, stream sam- pling scheme, and self-replay mechanism that work together to effectively tackle the OCC problem. (d) We demon- strate this can yield state-of-the-art performance in stan- dard online continual image classiﬁcation benchmarks and demonstrate the applications of our OCC solution in a va- riety of other contexts. The code to reproduce our results is available at https://github.com/pclucas14/ adaptive-quantization-modules 2. Related Work Learned compression has been recently studied for the case of image compression. Works by Theis et al. (2017); Ball´e et al. (2016); Johnston et al. (2018) have shown learned compressions can outperform standard algorithms like JPEG. These methods however are difﬁcult to adapt for online settings as they do not directly address the challenges of the OCC problem (e.g. representation drift). Continual Learning research currently focuses on over- coming catastrophic forgetting (CF) in the supervised learn- ing setting, with some limited work in the generative mod- eling and reinforcement learning settings. Most continual learning methods can be grouped into three major families. Some algorithms dynamically change the model’s architec- ture to incorporate learning from each task separately (Rusu et al., 2016; Li & Hoiem, 2018; Fernando et al., 2017). Al- though these methods can perform well in practice, their introduction of task-speciﬁc weights requires growing com- pute and memory costs which are problematic for the online setting. Another set of techniques employ regularization to constrain weights updates in the hope of maintaining knowl- edge from previous tasks. Notable methods in this class include (Kirkpatrick et al., 2017; Husz´ar, 2017; Zenke et al., 2017; Nguyen et al., 2017; Chaudhry et al., 2018). However, this set of approaches has been shown to be inefﬁcient in the online setting (Chaudhry et al., 2019). The last family of methods encapsulates all that have a mech- anism to store information about the previous data distribu- tions. This memory then serves as a tool for the continual learner to rehearse previous tasks. The simplest instantia- tion of this method is to keep and sample from a buffer of old data to retrain the model after every update (Chaudhry et al., 2019). This approach is widely used in RL where it is known as Experience Replay (ER) (Lin, 1993; Mnih et al., 2013; Andrychowicz et al., 2016). Another method, known as Generative Replay (GR) (Shin et al., 2017), uses generative modeling to store past task distributions. The con- tinual learner then trains on generated samples to alleviateOnline Learned Continual Compression with Adaptive Quantization Modules CF. Other notable examples are Gradient Episodic Memory (GEM) (Lopez-Paz et al., 2017), iCarl (Rebufﬁ et al., 2017), and Maximally Interfered Retrieval (MIR) (Aljundi et al., 2019), as well as (Aljundi et al., 2018; Hu et al., 2018). Most closely related to our work, Riemer et al. (2017) consider compressing memories for use in the continual classiﬁcation task. They also employ a discrete latent variable model but with the Gumbel approximation, which shows to be less effective than our approach. Furthermore a separate ofﬂine iid pre-training step for the learned compression is required in order to surpass the ER baseline, distinctly different from the online continual compression we consider. Lidar compression is considered in (Tu et al., 2019) and (Caccia et al., 2018). Both approaches use a similar projec- tion from 3D (x,y,z ) coordinates to 2D cylindrical coordi- nates, and leverage deep generative models to compress the data. However, neither accounts for potential distribution shift, nor for online learning. In this work we show that us- ing this 2D projection in conjunction with our model allows us to mitigate the two issues above for lidar data. 3. Methodology In this section we outline our approach to the online con- tinual compression problem. First we will review the VQ- V AE and highlight the properties making it effective for representational drift. Then we will describe our adaptive architecture, storage, and sampling scheme. 3.1. Problem Setting: Online Continual Compression We consider the problem setting where a stream of samples x ∼Dt arrives from different distributions Dt changing over time t= 1...T . We have a ﬁxed storage capacity ofC bytes where we would like to store the most representative information from all data distributions D1,...DT . There is notably a trade-off in quality of information versus the amount of samples stored. We propose to use a learned compression model, and most crucially, this model must also be stored within the Cbytes, to encode and decode the data samples. Another critical requirement is that at anytime tthe content of the storage (data and/or compression model) be usable for downstream applications. An important chal- lenge, illustrated in Figure 1, is that the learned compression module will change over time, while we still need to be able to decode the memories in storage. 3.2. Vector Quantized V AE for Online Compression The VQ-V AE is a discrete auto-encoder which relies on a vector quantization step to obtain discrete latent repre- sentations. An embedding table, E ∈RK×D consisting of K vectors of size D, is used to quantize encoder out- puts. Given an input (e.g. an RGB image), the encoder ﬁrst encodes it as a Hh ×Wh ×D tensor, where Hh Encoder     t=1      Encoder     t=3      same after VQ Vector Quantization different codebook Figure 2.Illustration of reduced representation drift from Vector Quantization and Wh denote the height and width of the latent repre- sentation. Then, every Ddimensional vector is quantized using a nearest-neighbor lookup on the embedding table. Speciﬁcally, zij = arg mine∈E ||enc(x)ij −e||2, where i,j refers to a spatial location. The output of the quanti- zation step is then fed through the decoder. The gradient of this non-differentiable step is approximated using the straight-through estimator. An important property to notice is that to reconstruct the input, only the Hh ×Wh indices are required, thus yielding high compression (van den Oord et al., 2017). Critically, the embedding tables are updated independently from the encoder and decoder, namely by minimizing mine ||sg[enc(x)ij] −e||, where sg is the stop gradient operator. Observe in the case of online compression, if the embedding table is ﬁxed, then a change in the encoder parameters and therefore a change in the encoder output for a given input will not change the ﬁnal quantized representation z, unless it is sufﬁciently large, thus we can observe that if the em- beddings change slowly or are ﬁxed we can greatly improve our control of the representational drift. This effect is illus- trated in Figure 2. On the other hand we do need to adapt the embedding table, since randomly selected embeddings would not cover well the space of encoder outputs. 3.3. Adaptive Quantization Modules To address issues of how to optimize storage and sampling in the context of Online Continual Compression we intro- duce Adaptive Quantization Modules (AQM). We use AQM to collectively describe the architecture, adaptive multi-level storage mechanism, and data sampling method used. To- gether they allow effectively constraining individual sample quality and memory usage while keeping in storage a faith- ful representation of the overall distribution.Online Learned Continual Compression with Adaptive Quantization Modules Algorithm 1: AQM L EARNING WITH SELF -REPLAY Input: Learning rate α, EXTERNAL LEARNER 1 Initialize: AQM Memory M; AQM Parameters θaqm 2 for t∈1..T do 3 % Fetch data from current task 4 for Binc ∼Dt do 5 for n∈1..N do 6 B←Binc 7 if t> 1 then 8 % Fetch data from buffer 9 Bre ∼SAMPLE (M,θaqm) 10 B ←(Binc,Bre) 11 end 12 % Update AQM 13 θaqm ←ADAM(θaqm,B,α ) 14 % Send data to external learner 15 EXTERNAL LEARNER (B) 16 if t> 1 then UPDATE BUFFER REP(M,θaqm) 17 %Save current indices 18 ADDTOMEMORY (M,Binc,θaqm) 19 end 20 end 21 end AQM uses an architecture consisting of a sequence of VQ- V AEs, each with a buffer. The AQM approach to online continual compression is overall summarized in Algorithm 1 (note ADDTOMEMORY is described in Appendix A). In- coming data is added to storage using an adaptive compres- sion scheme described in Algorithm 2. Incoming data is also used along with randomly sampled data from storage (self- replay) to update the current AQM model. The randomly sampled data also updates its representation in storage as per Algorithm 3. As illustrated by the optional lines in blue, Algorithm 1 can run concurrently with a downstream learn- ing task (e.g. online continual classiﬁcation) which would use the same batch order. It can also be run independently as part of a data collection. In the sequel we give further details on all these elements 3.3.1. A RCHITECTURE AND TRAINING Each AQM module contains a VQ-V AE and a corresponding buffer of adaptive capacity. A diagram of the architecture is given in Figure 3. We will denote the output after quan- tization of each module i as zi q and the set of codebook indexes used to obtain zi q as ai. Note that ai are the discrete representations we actually store. Each subsequent module produces and stores an ai requiring fewer bits to represent. For RGB images, the compression rate at a given level is given by H×W×3×log2(256) Nc×Hhi×Whi×⌈log2 (Ki)⌉. Here Ki is the number of embeddings in the codebooks, ( Hhi, Whi) the spatial dimension of the latent representation and Nci the number of codebooks. Algorithm 2: ADAPTIVE COMPRESS Input: datapoint x, AQM with Lmodules, threshold dth 1 % Forward all modules, store encodings 2 {zi q,ai}i=1..L= ENCODE (x) 3 for i∈L...1 do 4 % Decode from level i to output space 5 ˆx= DECODE (zi q) 6 % Check reconstruction error 7 if MSE(ˆx,x) <dth then return ai, i 8 end 9 % Otherwise, return original input 10 return x,0 Algorithm 3: UpdateBufferRep Input: Memory M, AQM with Llevels, data D, distortion threshold dth 1 for x∈Ddo 2 hidx, blockid = ADAPTIVE COMPRESS (x, AQM, dth) 3 % Delete Old Repr. 4 DELETE (M[x]) 5 % Add new one 6 ADD(M, hidx) 7 end VQV AE-2 (Razavi et al., 2019) also uses a multi-scale hier- archical organization, where unlike our AQM, the top level models global information such as shape, while the bottom level, conditioned on the top one, models local information. While this architecture is tailored for generative modeling, it is less attractive for compression, as both the bottom and top quantized representations must be stored for high qual- ity reconstructions. Furthermore in AQM each module is learned in a greedy manner using the current estimate of z(i−1) q without passing gradients between modules similar to (Belilovsky et al., 2019; Nøkland & Eidnes, 2019). A subsequent module is not required to build representations which account for all levels of compression, thus minimiz- ing interference across resolutions. This allows the modules to each converge as quickly as possible with minimal drift at their respective resolution, particularly important in the online continual learning case. 3.3.2. M ULTI -LEVEL STORAGE Our aim is to store the maximum number of samples in an allotted Cbytes of storage, while assuring their quality, and our ability to reconstruct them. Samples are thus stored at different levels based on the compressors’ current ability. The process is summarized in Algorithm 2. Such an approach is particularly helpful in the online non- stationary setting, allowing knowledge retention before the compressor network has learned well the current distribution. Note in Alg. 2 samples can be completely uncompressed until the ﬁrst module is able to effectively encode them. This can be crucial in some cases, if the compressor has not yet converged, to avoid storing poorly compressed rep-Online Learned Continual Compression with Adaptive Quantization Modules Figure 3.Architecture of Adaptive Quantization Modules. Each level uses its own loss and maintains its own replay buffer. Yello dotted lines indicate gradient isolation between modules resentations. Further taking into account that compression difﬁculty is not the same for all datapoints, this allows use of more capacity for harder data, and fewer for easier. We also note, since we maintain stored samples at each mod- ule and the modules are decoupled, that such an approach allows to easily distribute training in an asynchronous man- ner as per Belilovsky et al. (2019). 3.3.3. S ELF -REPLAY AND STREAM SAMPLING As shown in Alg. 1 our AQM is equipped with an internal experience replay mechanism (Mnih et al., 2013), which reconstructs a random sample from storage and uses it to perform an update to the AQM modules, while simultane- ously freeing up overall memory if the sample can now be compressed at a later AQM module. This has the effect of both reducing forgetting and freeing memory. In practice we replay at the same rate as incoming samples arrive and thus replay will not increase asymptotic complexity of the online learning. Finally, for efﬁciency the replay can be coupled to an external online learner querying for random samples from the overall memory. Since we would like the AQM to work in cases of a ﬁxed memory capacity it must also be equipped with a mecha- nism for selecting which samples from the stream to store and which to delete from memory. Reservoir Sampling (RS) is a simple yet powerful approach to this problem, used successful in continual learning (Chaudhry et al., 2019). It adds a sample from the stream with prob. p= buffer capacity points seen so far while remove a random sample. However, RS is not directly compatible with AQM primarily because the amount of sam- ples that can be stored varies over time. This is because samples at different levels have different memory usage and memory can be freed by replay. We thus propose an alternative scheme, which maximally ﬁlls available memory and selects non-uniformly samples for deletion. Speciﬁ- cally when a larger amount of samples are added at one point in the stream, they become more likely to be removed. The details of this stream sampling method are provided in Appendix A. 3.3.4. DRIFT CONTROL VIA CODEBOOK STABILIZATION As mentioned previously, a good online compressor must control its representational drift, which occurs when up- dates in the auto-encoder parameters creates a mismatch with the static representations in the buffer. Throughout the paper we measure representational drift by compar- ing the following time varying quantity: DRIFT t(z) = RECON ERR ( Decode(θt; z),x ) where θt the model pa- rameters at timetand (z,x) is a stored compressed represen- tation and its original uncompressed datapoint respectively. For all experiments on images, RECON ERR is simply the mean squared error. As illustrated in Sec 3.2 a slow changing codebook can allow to control drifting representations. This can be in part accomplished by updating the codebook with an exponential moving average as described in (van den Oord et al., 2017, Appendix A), where it was used to reduce the variance of codebook updates. This alone is insufﬁcient to fully control drift, thus once a given module yields satisfactory compressions on the data stream, we freeze the module’s embedding matrix but leave encoder and decoder parameters free to change and adapt to new data. Moreover, we note that ﬁxing the codebook for a given module does not affect the reconstruction performance of subsequent modules, as they only need access to the current module’s decoder which can still freely change. 4. Experiments We evaluate the efﬁcacy of the proposed methods on a suite of canonical and new experiments. In Section 4.1 we present results on standard supervised continual learning benchmarks on CIFAR-10. In Section 4.2 we evaluate other downstream tasks such as standard iid training applied on the storage at the end of online continual compression. For this evaluation we consider larger images from Imagenet, as well as on lidar data. Finally we apply AQM on observations of an agent in an RL environment.Online Learned Continual Compression with Adaptive Quantization Modules Accuracy (↑) M= 20 M= 50 iid online 60.8 ±1.0 60 .8 ±1.0 iid ofﬂine 79.2 ±0.4 79 .2 ±0.4 GEM (Lopez-Paz et al., 2017) 16.8 ±1.1 17 .1 ±1.0 iCarl (5 iter) (Rebufﬁ et al., 2017) 28.6 ±1.2 33 .7 ±1.6 ﬁne-tuning 18.4 ±0.3 18 .4 ±0.3 ER 27.5 ±1.2 33 .1 ±1.7 ER-MIR (Aljundi et al., 2019) 29.8 ±1.1 40 .0 ±1.1 ER-JPEG 33.9 ±1.0 43 .1 ±0.6 Gumbel AE (Riemer et al., 2018) 25.5 ±2.0 28 .8 ±2.9 AQM (ours) 43.5 ±0.7 47 .0 ±0.8 Forgetting (↓) M= 20 M= 50 N/A N/A N/A N/A 73.5 ±1.7 70 .7 ±4.5 49 ±2.4 40 .6 ±1.1 85.4 ±0.7 85 .4 ±0.7 50.5 ±2.4 35 .4 ±2.0 50.2 ±2.0 30 .2 ±2.3 54.8 ±1.2 44 .3 ±0.9 71.5 ±2.8 67 .2 ±3.9 23.0 ±1.0 19 .0 ±1.4 Table 1.Shared head results on disjoint CIFAR-10. Total memory per class M measured in sample memory size. We report (a) Accuracy, (b) Forgetting (lower is better). 4.1. Online Continual Classiﬁcation Although CL has been studied in generative modeling (Ramapuram et al., 2017; Lesort et al., 2018; Zhai et al., 2019; Lesort et al., 2019) and reinforcement learning (Kirk- patrick et al., 2017; Fernando et al., 2017; Riemer et al., 2018), supervised learning is still the standard for evalua- tion of new methods. Thus, we focus on the online continual classiﬁcation of images for which our approach can provide a complement to experience replay. In this setting, a new task consists of new image classes that the classiﬁer must learn, while not forgetting the previous ones. The model is only allowed one pass through the data (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2019; Chaudhry et al., 2019). The online compression here takes the role of replay buffer in replay based methods such as (Chaudhry et al., 2019; Aljundi et al., 2019). We thus run Algorithm 1, with an additional online classiﬁer being updated performed at line 15. Here we consider the more challenging continual classiﬁca- tion setting often referred to as using ashared-head (Aljundi et al., 2019; Farquhar & Gal, 2018; Aljundi et al., 2018). Here the model is not informed of the task (and thereby the subset of classes within it) at test time. This is in contrast to other (less realistic) CL classiﬁcation scenarios where the task, and therefore subset of classes, is provided explicitly to the learner (Farquhar & Gal, 2018; Aljundi et al., 2019). For this set of experiments, we report accuracy, i.e. 1 T ∑T i=1 RT,i, and forgetting, i.e. 1 T−1 ∑T−1 i=1 max(R:,i) − RT,i with R ∈ RT×T representing the accuracy matrix where Ri,j is the test classiﬁcation accuracy on task jwhen task iis completed. Baselines A basic baseline for continual supervised learn- ing is Experience Replay ( ER). It consists of storing old data in a buffer to replay old memories. Although relatively simple recent research has shown it is a critical baseline to consider, and in some settings is actually state-of-the- art (Chaudhry et al., 2019; Aljundi et al., 2019; Rolnick et al., 2018). AQM can be used as an add-on to ER that in- corporates online continual compression. We also compare against ER with standard JPEG compression. In addition we consider the following baselines. iid online (upper-bound) trains the model with a single-pass through the data on the same set of samples, but sampled iid. iid ofﬂine (upper- bound) evaluates the model using multiple passes through the data, sampled iid. We use 5 epochs in all the experiments for this baseline. ﬁne-tuning trains continuously upon ar- rival of new tasks without any forgetting avoidance strategy. iCarl (Rebufﬁ et al., 2017) incrementally classiﬁes using a nearest neighbor algorithm, and prevents catastrophic for- getting by using stored samples. GEM (Lopez-Paz et al., 2017) uses stored samples to avoid increasing the loss on previous task through constrained optimization. It has been shown to be a strong baseline in the online setting. It gives similar results to the recent A-GEM (Chaudhry et al.). ER- MIR (Aljundi et al., 2019) controls the sampling of the replays to bias sampling towards samples that will be for- gotten. We note that the ER-MIR critera is orthogonal to AQM, and both can be applied jointly.Gumbel AE (Riemer et al., 2018) learns an autoencoder for ER using the Gumbel softmax to obtain discrete representations. We evaluate with the standard CIFAR-10 split (Aljundi et al., 2018), where 5 tasks are presented sequentially, each adding two new classes. Evaluations are shown in Table 1. Due to our improved storage of previous data, we observe sig- niﬁcant improvement over other baselines at various mem- ory sizes. We can contrast AQM’s performance with ER’s to understand the net impact of our compression scheme. Speciﬁcally, AQM improves over ER by 16.0% and 13.9% in the M=20 and M=50 case, highlighting the effectiveness of online compression. Our approach is also superior in forgetting by a signiﬁcant margin in both memory settings. To compare directly to reporting in (Riemer et al., 2018) we also benchmarked our implementation on the Incremental CIFAR-100 multi-head experiment (Lopez-Paz et al., 2017) with the same settings as in (Riemer et al., 2018). By using AQM we were able to get 65.3 vs the reported 43.7 using a buffer of size 200. To speciﬁcally isolate the advantageOnline Learned Continual Compression with Adaptive Quantization Modules Accuracy RS 5.2 ±0.2 2 Module AQM (ours) 23.2 ± 1.1 Ablate 2nd Module 20.5 ±1.3 Ablate Fixing Codebook 19.2 ±0.6 Ablate Decoupled Training 16.5 ±0.7 Ablate Adaptive Compression 13.1 ±3.2 Table 2.Imagenet ofﬂine training evaluation from online continual compression. We see a clear gain over a standard Reservoir sam- pling approach. We then ablate each component of our proposal showing each component is important. Note storage used in each experiment is identical (including accounting for model sizes). of gumbel softmax versus the vector quantization for drift, we replaced the vector quantization approach with gumbel softmax in an AQM. We observed signﬁcantly less drift in the case where vector quantization is used. Full details of this experiment are described in the supplementary materials along with visualizations. The CIFAR-10 dataset has a low resolution (3 ×32 ×32) and uses a lot of data per task (10K samples). These two characteristics might leave the online compression problem easier than in a real-life scenario. Speciﬁcally, if the ﬁrst tasks are long enough and the compression rate is not too large, the model can quickly converge and thus not incur too much representation drift. Indeed, we found that us- ing a single module is already sufﬁcient for this task. For these reasons, we now study the AQM in more challenging settings presented in the next section. 4.2. Ofﬂine Evaluation on Larger Images Besides the standard continual classiﬁcation setup, we pro- pose several other evaluations to determine the effectiveness of the stored data and compression module after learning online compression. We also perform a detailed ablation to study the efﬁcacy of each component in AQM. Ofﬂine training on Imagenet We compare the effective- ness of the stored memories of AQM after a certain amount of online continual compression. We do this by training in a standard iid way an ofﬂine classiﬁcation model using only reconstructions obtained from the storage sampled after on- line continual compression has progressed for a period of time. In each case we would have the same sized storage available. We note that simply having more stored memo- ries does not amount to better performance as their quality may be severely degraded and affected by drift. Using this evaluation we ﬁrst compare a standard reservoir sampling approach on uncompressed data to a 2 module AQM using the same size storage. We observe that perfor- mance is drastically increased using the compressed sam- ples. We then use this to perform a series of ablations to Figure 4.Impact of codebook freezing. Vertical black line indi- cates freezing point. We see that AQM is still able to adapt and reduce its reconstruction loss, while having stable compressed representations. Results averaged over 5 runs demonstrate each component of our proposal is important. Speciﬁcally (a) we restrict AQM to have only one module, (b) instead of decoupled training we train modules end-to- end, (c) we remove adaptive compression, thus all samples are stored in the most compressed block, regardless of qual- ity, and (d) we do not stabilize the codebook, the embedding matrices of every block are never ﬁxed. We observe that all these elements contribute to successfully storing a represen- tative set of data for the distribution online. Drift Ablation We have seen the importance of codebook freezing when dealing with high dimensional datasets. How- ever, judging solely from the ﬁnal downstream task perfor- mance it’s difﬁcult to see if the model continues adapting after freezing. As alluded in Sec 3.2 there is a tradeoff be- tween keeping recoverable representations and a model’s ability to continue to adapt. To shed some light on this, we run the following experiment: we run a vanilla VQ-V AE on the same 20 task mini-imagenet stream, without storing any samples. When it reaches a pre-speciﬁed performance threshold, we ﬁx the codebook, and store compressed held- out data from the ﬁrst task. We then continue to update the VQ-V AE parameters, and the memory is kept ﬁxed for the rest of the stream. We apply self-replay but no other AQM mechanisms (e.g. no sampling from the input stream and no adaptive compression). We monitor how well the VQ-V AE can adapt by looking at the streaming reconstruction cost, measured on the in- coming data before an update. We also monitor the drift of samples stored in the buffer. Results are presented in Figure 4. They demonstrate that drift is controlled by stabilizing the codebook, while the model can still improve at nearly the same rate. Further analysis, along with an additional ex- periment showcasing the robustness of vector quantization to small perturbations is included in the Appendix.Online Learned Continual Compression with Adaptive Quantization Modules Figure 5.Top: Sample decoded from the buffer at the end of train- ing from scratch (32x compression rate). Bottom: Original lidar LiDAR Range data enables autonomous vehicles to scan the topography of their surrounding, giving precise mea- surements of an obstacle’s relative location. In its raw form, range data can be very large, making it costly to transmit in real time, or for long term storage. Equipping self-driving cars with a good lidar compressor can enable fast vehicle- to-vehicle (V2V) communication, leading to safer driving (Eckelmann et al., 2017). Moreover, since data collected by autonomous vehicles can be highly non-stationary (new objects on the road, changing weather or trafﬁc conditions), having a compressor which can quickly adapt to this distri- bution change will reduce the required memory for storage (or bandwidth for real time transmission). We proceed to train AQM on the Kitti Dataset (Geiger et al., 2013), which contains 61 LiDAR scan recordings, each belonging to either the “residential”, “road”, “city” envi- ronments. The data is processed as in (Caccia et al., 2018), where points from the same elevation angle are sorted in increasing order of azimuth angle along the same row. This yield a 2D grid, making it compatible with the same architec- ture used in the previous experiments. As in (Caccia et al., 2018; Tu et al., 2019), we report the reconstruction cost in Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE) which allows to compare two point clouds. Note AQM can also be adapted to use task relevant criteria besides MSE. We consider two settings. In the ﬁrst, we train AQM from scratch on a data stream consisting of recordings from all three environments. We present (once) all the recordings of an environment before moving on to another, in order to maximise the distribution shift. We show qualitative results in Figure 5 and in the supplementary materials. Ob- serve that we are able to effectively reconstruct the LiDAR samples and can easily tradeoff quality with compression. Overall we obtain 18.8 cm SNNRMSE with 32×compres- Size in Mb Raw 1326.8 Gzip 823.0 AQM 35.5 ±.06 AQM + ﬁnetune 33.0 ±.07 AQM + ﬁnetune + PNG 27.9 ±.01 Table 3.Compression results for the data transmission of the city lidar recordings. We require that each compressed scan has an SNNRMSE under 15 cm. sion, which lies in a range that has been shown in (Tu et al., 2019) to be sufﬁcient to enable SLAM localization with very minimal error. In the second setting, we wish to simulate a scenario where some data is available a priori for the model to leverage. However, this data is limited and does not cover all the possible modalities to which an autonomous vehicle could be exposed. To this end, we pretrain AQM in a fully ofﬂine iid manner on the road and residential recordings. We then simulate the deployment of the compressor on a vehicle, where it must compress and transmit in real time the lidar data feed from a new distribution. We therefore stream the held-out city recordings and show that AQM can be ﬁne-tuned on the ﬂy to reduce the required bandwidth for data transmission. Quantitative results are presented in table 3. We ensure that the reconstructed lidar scans have a SNNRMSE smaller than 15.0 cm. Moreover, since the stored representations in AQM are 2D and discrete, we can apply lossless compression schemes such as Portable Network Graphics (PNG). 4.3. Atari RL Environments Another application of online continual compression is for preserving the states of an reinforcement learning agent op- erating online. These agents may often learn new tasks or enter new rooms thus the observations will often be highly non-iid. Furthermore many existing reinforcement learning algorithms already rely on potentially large replay buffers which can be prohibitive (Mnih et al., 2014; Rolnick et al., 2018) to run and may greatly beneﬁt from an approach such as the AQM to run concurrently with reinforcement learning algorithms. We thus perform a proof of concept for the AQM for storing the state sequence encountered by an RL learner in the atari environment(Bellemare et al., 2013). We use the dataset and tasks introduced in (Anand et al., 2019), which runs a random or learned policy in the atari environments and provides a set of classiﬁcation tasks to evaluate whether key information about the state is preserved. Results are shown Table 4. We run the online learning with the AQM on the data stream observed by the random agent. We use the same observations and optimiza- tion as in (Anand et al., 2019) and report the F1 resultsOnline Learned Continual Compression with Adaptive Quantization Modules Figure 6.Top: original. Bottom: reconstructed from AQM Game Cls Input F1 Pong Orig. State 86.7 AQM Recon 86.8 Ms Pacman Orig. State 89.4 AQM Recon 88.3 Pitfall Orig. State 68.2 AQM Recon 66.7 Table 4.Results on RL probing tasks from (Anand et al., 2019) with linear probe applied to original observation and to reconstruc- tions from AQM after online compression. Acc is averaged for each game over game speciﬁc prediction. of a linear probe directly on states for our reconstructions after online compression and the originals. Results for 3 environments are shown in Table 4 and examples in in Fig 6 and the Appendix. We ﬁnd that AQM can well preserve the critical information while compressing the state by 16x. The reference accuracies achieved by our classiﬁer are sim- ilar to those in (Anand et al., 2019). However, we do not control for the representation size unlike those evaluations of various unsupervised models. 5. Conclusion We have introduced online continual compression. We demonstrated vector quantization can be used to control drift and how to create mechanisms that allow maintain- ing quality and maximizing memory usage. These allowed learning compression while compressing. We have shown effectiveness of this online compression approach on stan- dard continual classiﬁcation benchmarks, as well as for compressing larger images, lidar, and atari data. We believe future work can consider dealing with temporal correlations for video and reinforcement learning tasks, as well as im- proved prioritization of samples for storage. References Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Online continual learning with no task boundaries. In arXiv, 2018. Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Charlin, L., and Tuytelaars, T. Online continual learning with maximally interfered retrieval. In Advances in Neural Information Processing (NeurIPS), 2019. Anand, A., Racah, E., Ozair, S., Bengio, Y ., C ˆot´e, M.- A., and Hjelm, R. D. Unsupervised state representation learning in atari. arXiv preprint arXiv:1906.08226, 2019. Andrychowicz, M., Denil, M., G´omez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N. Learning to learn by gradient descent by gradient descent. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Sys- tems 29, pp. 3981–3989. Curran Associates, Inc., 2016. Ball´e, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2016. Belilovsky, E., Eickenberg, M., and Oyallon, E. De- coupled greedy learning of cnns. arXiv preprint arXiv:1901.08164, 2019. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Caccia, L., van Hoof, H., Courville, A., and Pineau, J. Deep generative modeling of lidar data. arXiv preprint arXiv:1812.01180, 2018. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. In ICLR 2019. Chaudhry, A., Dokania, P. K., Ajanthan, T., and Torr, P. H. Riemannian walk for incremental learning: Un- derstanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H., and Ranzato, M. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. Eckelmann, S., Trautmann, T., Ußler, H., Reichelt, B., and Michler, O. V2v-communication, lidar system and posi- tioning sensors for future fusion algorithms in connected vehicles. Transportation research procedia, 27:69–76, 2017. Farquhar, S. and Gal, Y . Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733 , 2018.Online Learned Continual Compression with Adaptive Quantization Modules Fernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra, D. Pathnet: Evolu- tion channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, pp. 0278364913491297, 2013. Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J. Faster neural networks straight from jpeg. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31 , pp. 3933–3944. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7649-faster-neural-networks-straight-from-jpeg. pdf. Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., Zhao, D., and Yan, R. Overcoming catastrophic forgetting for continual learning via model adaptation. 2018. Husz´ar, F. On quadratic penalties in elastic weight consoli- dation. arXiv preprint arXiv:1712.03847, 2017. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Jin Hwang, S., Shor, J., and Toderici, G. Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4385–4393, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. Lesort, T., Caselles-Dupr´e, H., Garcia-Ortiz, M., Stoian, A., and Filliat, D. Generative models from the perspective of continual learning. arXiv preprint arXiv:1812.09111, 2018. Lesort, T., Gepperth, A., Stoian, A., and Filliat, D. Marginal replay vs conditional replay for continual learning. In International Conference on Artiﬁcial Neural Networks, pp. 466–480. Springer, 2019. Li, Z. and Hoiem, D. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 40(12):2935–2947, 2018. Lin, L.-J. Reinforcement learning for robots using neu- ral networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. Lopez-Paz, D. et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 2014. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. arXiv preprint arXiv:1901.06656, 2019. Oyallon, E., Belilovsky, E., Zagoruyko, S., and Valko, M. Compressing the input for cnns with the ﬁrst-order scatter- ing transform. In The European Conference on Computer Vision (ECCV), September 2018. Ramapuram, J., Gregorova, M., and Kalousis, A. Lifelong generative modeling. arXiv preprint arXiv:1705.09847, 2017. Razavi, A., Oord, A. v. d., and Vinyals, O. Generating diverse high-ﬁdelity images with vq-vae-2.arXiv preprint arXiv:1906.00446, 2019. Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. Riemer, M., Franceschini, M., and Klinger, T. Genera- tion and consolidation of recollections for efﬁcient deep lifelong learning. CoRR, abs/1711.06761, 2017. URL http://arxiv.org/abs/1711.06761. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and Wayne, G. Experience replay for continual learning, 2018. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had- sell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.Online Learned Continual Compression with Adaptive Quantization Modules Shin, H., Lee, J. K., Kim, J., and Kim, J. Continual learn- ing with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017. Theis, L., Shi, W., Cunningham, A., and Husz ´ar, F. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017. Thrun, S. and Mitchell, T. M. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25–46, 1995. Torfason, R., Mentzer, F., ´Ag´ustsson, E., Tschannen, M., Timofte, R., and Gool, L. V . Towards image under- standing from deep compression without decoding. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkXWCMbRW. Tu, C., Takeuchi, E., Carballo, A., and Takeda, K. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 Interna- tional Conference on Robotics and Automation (ICRA), pp. 3274–3280. IEEE, 2019. van den Oord, A., Vinyals, O., et al. Neural discrete repre- sentation learning. In Advances in Neural Information Processing Systems, pp. 6306–6315, 2017. Zenke, F., Poole, B., and Ganguli, S. Continual learn- ing through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017. Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G. Lifelong gan: Continual learning for conditional im- age generation. ArXiv, abs/1907.10107, 2019.Online Learned Continual Compression with Adaptive Quantization Modules A. Buffer Memory Management We consider two settings to manage the samples inside the memory. In the ﬁrst setting, we do not perform any codebook freezing. Therefore, as the model trains, the amount of compressed representations AQM can store increases smoothly. In this setting, the current amount of samples stored by AQM is a good approximation of the model’s capacity. Therefore, we simply use this estimate instead of the total buffer size in the regular reservoir sampling scheme. The algorithm is presented in Alg 4. Algorithm 4: AddToMemory Input: Memory Mwith capacity C(bytes), sample x 1 Nreg = C BY TES(x) 2 capacity = max( Nreg, NUM SAMPLES (M) ) 3 %Probability of adding x 4 add ∼B( capacity SAMPLE AMT SEEN SO FAR ) %Bernoulli 5 if add then 6 hidx, blockid = ADAPTIVE COMPRESS (x, AE, dth) 7 while BITS(hidx) - FREE SPACE(M) >0 do 8 DELETE RANDOM (M) 9 end 10 end This is not the case when we perform codebook freezing. In the latter setting, consider the moment when the ﬁrst codebook is ﬁxed; suddenly, the amount of samples the model can store has increased by a factor equal to the compression rate of the ﬁrst block. Therefore, at this moment, the amount of samples currently stored by AQM is not a good approximation for the model’s capacity. Moreover, when performing codebook freezing, since the capacity suddenly spikes, we must decide between a) having an imbalance in the buffer, where certain temporal regions of the streams are not equally represented, or not utilising all available memory and storing less incoming samples so they are in similar quantities as previous samples. We opt for the former approach, and propose a procedure that allows the buffer to rebalance itself as new training data becomes available. We illustrate the procedure with an example. Consider an AQM where the distribution of samples in the buffer is the one plotted in Fig 10. Speciﬁcally, we show the number of samples stored for each minibatch processed by the model. In this example, very few (<20) samples are stored from the earliest part of the stream, while a much larger number comes from the more recent part of the stream. Assuming that the model is over its memory capacity, we need to remove samples until the memory requirement is met. Ideally, we would like to remove more samples from parts of the stream where samples are abundant. In order to do so, we use Kernel Density Estimation on the histogram in Fig 10. Doing so gives us the line labelled iter0 in Fig 10. We then sample points according to the distribution given by iter0, remove them, and ﬁt a new KDE with the remaining points (labelled iter1). In this example we repeat this procedure 10 times, until iter9. As we can see, the distribution of stored samples becomes closer to the uniform distribution, i.e. the setting where all parts of the streeam are equally represented. Therefore, in the setting where codebook freezing is performed, we ﬁrst add all incoming points to the buffer. Then, points are removed according to the procedure described above. This allows for maximal memory usage while ensuring that the buffer self balances over time. Note that we need to store the timestamp alongside each sample, which has a negligible cost when dealing with high-dimensional inputs. B. Further Details of Experiments We include here further details regarding the models used in our experiments. For all reported results, almost all hyperpa- rameters are kept the same. we set D the size of the embedding table equal to 100, we use a decay value of 0.6 for the Embedding EMA update, and the same architectural blocks. Across problems, we mainly vary the reconstruction threshold, as well how the blocks are stacked and their compression rates (by e.g. changing the number of codebooks per block).Online Learned Continual Compression with Adaptive Quantization Modules Figure 7.(left) histogram of samples in AQM where no buffer balancing was performed. (right) iterative buffer balancing procedure B.1. Cifar For CIFAR-10, we use a 1 block AQM, latent size (16 x 16 x 100) is quantized with (16 x 16 x 1) indices where the last index represents the number of codebooks. The codebook here contains 128 embeddings, giving a compression factor of 13.7×. Due to the simplistic nature of the task and low resolution of the images, AQM already yields good compression before the end of the ﬁrst task, hence adaptive compression and codebook freezing are not required. For the (Riemer et al., 2018) baseline, we ran a hyperparameter search to vary the compression size. Speciﬁcally, we ran a grid search for the number of categories per latent variable, as well as for the number of latent variables. We found the gumbel softmax much less stable during training and harder to cross-validate than vector quantization. Below we show an example of the image quality of our approach compared to (Riemer et al., 2018). We ran both AQM and (Riemer et al., 2018) on the split CIFAR-10 task, then extracted images which happened to be in the buffer of both methods. Figure 8.Bottom row: random buffer reconstructions using (Riemer et al., 2018). Middle row: random buffer reconstructions using SQM. Top row: corresponding original image. Columns are ordered w.r.t their entry time in the buffer, from oldest to newest. All samples above were obtained from the disjoint CIFAR-10 task, and are 12×smaller than their original image. B.2. Imagenet For the Ofﬂine 128 x 128 Imagenet experiment, we use the following three blocks to build AQM, with the following latent sizes : 1. (64 x 64 x 100) quantized using (64 x 64 x 1) indices, with a codebook of 16 vectors, giving a 24 ×compression. 2. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 256 vectors, giving a 48 ×compression. 3. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 32 vectors, giving a 76.8 ×compression. For the 2 block AQM, we searched over using blocks (1-2) (2-3) and (1-3). For 1 block AQM we simply tried all three blocks independently. When stacking two blocks with the same latent sizes (e.g. block 2 and 3) the encoder and decoder functions for the second block are the identity. In other words, the second block simply learns another embedding matrix.Online Learned Continual Compression with Adaptive Quantization Modules C. Drift Ablation Here we provide additional results for the drift ablation studied in 4. We repeat the same experiment for different values of the reconstruction threshold parameter, which controls when the codebook freezing occurs. The table below shows that the same conclusion holds across multiple values for this parameter: codebook freezing yields little to no drift, while only negligibly hindering the model’s ability to adapt to a distribution shift. It is worth noting that in cases of severe drift (e.g. with recon th = 7.5) the model diverges because it is rehearsing on samples of poor quality. In this setting, codebook freezing performs better on both the streaming MSE and the drift MSE. recon th Freezing Streaming MSE Drift MSE Streaming + Drift MSE 1 No 0.59 ±0.03 1.01 ±0.14 1.60 ±0.17 1 Yes 0.60 ±0.03 0.49 ±0.02 1.09±0.05 2.5 No 0.63 ±0.06 13.24 ±5.36 13.87 ±5.42 2.5 Yes 0.81 ±0.02 1.07 ±0.05 1.88±1.12 5 No 0.65 ±0.04 55.31 ±36.82 55.96 ±36.86 5 Yes 0.92 ±0.03 1.69 ±0.18 2.61±0.21 7.5 nan nan nan nan 7.5 Yes 0.98 ±0.11 2.10 ±0.28 3.08±0.39 Table 5.Here we provide additional results for the drift ablation discussion in section shown in Fig 4. For clarity all results are multiplied by 100. (e.g. recon th of 2.5 corresponds to 0.025). Results averaged over 5 runs. Figure 9.Visualization of results reported in Table C. We kept the scale of the y axis consistent across the four graphs for easy comparison.Online Learned Continual Compression with Adaptive Quantization Modules D. Atari Below are sample reconstructions used in the Atari experiments. For each game, reconstructions are in the ﬁrst row and the original uncompressed samples in the second row. Reconstructions are 16×smaller than the original RGB images. Figure 10.Pong Figure 11.Pitfall Figure 12.Ms PacmanOnline Learned Continual Compression with Adaptive Quantization Modules E. Lidar Samples Here we show reveral lidar compressions (left) and their original counterpart (right). The compression rate is 32 ×. We note that unlike RGB images, raw lidar scans are stored using ﬂoating points. We calculate the compression rate from the (smaller) polar projection instead of the 3 channel cartesian representation. Figure 13.Lidar reconstruction (left) vs original (right)Online Learned Continual Compression with Adaptive Quantization Modules Figure 14.Lidar reconstruction (left) vs original (right)",
      "meta_data": {
        "arxiv_id": "1911.08019v3",
        "authors": [
          "Lucas Caccia",
          "Eugene Belilovsky",
          "Massimo Caccia",
          "Joelle Pineau"
        ],
        "published_date": "2019-11-19T00:43:16Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08019v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces the Online Learned Continual Compression (OCC) problem, highlighting challenges like processing non-i.i.d. data streams, limited storage, representation drift, and catastrophic forgetting. Proposes Adaptive Quantization Modules (AQM), an architecture based on multiple Vector Quantized Variational Autoencoders (VQ-VAEs), to address these challenges. Demonstrates how VQ-VAE's codebooks effectively mitigate representation drift. Integrates an adaptive multi-level compression scheme, a self-replay mechanism, and a novel stream sampling scheme within AQM to manage memory and data quality. Achieves state-of-the-art performance on online continual image classification benchmarks (CIFAR-10, CIFAR-100) and successfully applies the approach to larger images (Imagenet), LiDAR data compression, and preserving states in Atari RL environments without requiring pretraining.",
        "methodology": "The core model is built upon the Vector Quantized Variational Autoencoder (VQ-VAE), which uses an embedding table for discrete latent representations via nearest-neighbor quantization. The proposed Adaptive Quantization Modules (AQM) architecture consists of a sequence of VQ-VAEs, each with an adaptive capacity buffer. Modules are trained in a greedy, decoupled manner with gradient isolation to minimize interference across resolutions. An adaptive multi-level storage mechanism (Algorithm 2) stores samples at different compression levels based on their reconstruction quality (MSE threshold), allowing for uncompressed storage if the compressor has not converged. An internal self-replay mechanism (Algorithm 1) reconstructs random samples from storage for AQM updates and memory freeing. A novel stream sampling method is introduced to select samples for storage and deletion, adapted for AQM's varying memory usage. Representation drift is controlled by updating codebooks with an exponential moving average and by freezing a module's embedding matrix once it achieves satisfactory compression, while allowing encoder and decoder parameters to continue adapting.",
        "experimental_setup": "Evaluations are conducted across several domains. For online continual classification, experiments use disjoint CIFAR-10 (5 tasks, 2 new classes per task) and Incremental CIFAR-100, measuring accuracy and forgetting against baselines like Experience Replay (ER), ER-MIR, ER-JPEG, Gumbel AE, iid online/offline, fine-tuning, iCarl, and GEM. For offline evaluation on larger images, Imagenet (128x128) is used, where a standard i.i.d. offline classification model is trained on reconstructions from AQM storage; an ablation study assesses AQM components. LiDAR data compression is tested on the Kitti Dataset (residential, road, city environments), with 3D data projected to 2D cylindrical coordinates, and evaluated using Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE) for point cloud comparison. Two scenarios are explored: training from scratch and pretraining followed by fine-tuning on new distributions. For Atari RL environments, Pong, Ms Pacman, and Pitfall are used, with data from Anand et al. (2019), measuring the F1 score of a linear probe on original versus reconstructed states.",
        "limitations": "Existing learned compression algorithms exhibit slow convergence speeds, making them unsuitable for online settings. Learning generative models in online and non-stationary environments is challenging and susceptible to catastrophic forgetting. Continual learning methods relying on dynamic architectural changes face increasing compute and memory costs, which are problematic for online scenarios. Regularization-based continual learning techniques are shown to be inefficient in online settings. The Gumbel approximation, used in some related work, is found to be less effective and stable during training compared to vector quantization. Standard Reservoir Sampling is incompatible with AQM due to varying memory usage and memory freeing by replay, necessitating a new stream sampling scheme. For simple tasks like CIFAR-10, the low resolution and high data per task can make online compression easier, with a single module often sufficing and quick convergence minimizing representation drift.",
        "future_research_directions": "Future work could explore methods for dealing with temporal correlations, particularly in video and reinforcement learning tasks. Another promising direction is to investigate improved prioritization of samples for storage, beyond the current stream sampling method."
      }
    },
    {
      "title": "Analysis of Stochastic Processes through Replay Buffers",
      "abstract": "Replay buffers are a key component in many reinforcement learning schemes.\nYet, their theoretical properties are not fully understood. In this paper we\nanalyze a system where a stochastic process X is pushed into a replay buffer\nand then randomly sampled to generate a stochastic process Y from the replay\nbuffer. We provide an analysis of the properties of the sampled process such as\nstationarity, Markovity and autocorrelation in terms of the properties of the\noriginal process. Our theoretical analysis sheds light on why replay buffer may\nbe a good de-correlator. Our analysis provides theoretical tools for proving\nthe convergence of replay buffer based algorithms which are prevalent in\nreinforcement learning schemes.",
      "full_text": "arXiv:2206.12848v1  [cs.LG]  26 Jun 2022 Analysis of Stochastic Processes through Replay Buffers Shirli Di Castro Shashua 1 Shie Mannor 1 2 Dotan Di Castro 3 Abstract Replay buffers are a key component in many re- inforcement learning schemes. Y et, their theo- retical properties are not fully understood. In this paper we analyze a system where a stochas- tic processX is pushed into a replay buffer and then randomly sampled to generate a stochastic processY from the replay buffer. W e provide an analysis of the properties of the sampled pro- cess such as stationarity, Markovity and autocor- relation in terms of the properties of the original process. Our theoretical analysis sheds light on why replay buffer may be a good de-correlator. Our analysis provides theoretical tools for prov- ing the convergence of replay buffer based al- gorithms which are prevalent in reinforcement learning schemes. 1. Introduction A Replay buffer (RB) is a mechanism for saving past gen- erated data samples and for sampling data for off-policy reinforcement learning (RL) algorithms ( Lin, 1993). The RB serves a First-In-First-Out (FIFO) buffer with a ﬁxed capacity and it enables sampling mini-batches from previ- ously saved data points. Its structure and sampling mecha- nism provide a unique characteristic: the RB serves asde- correlator of data samples. T ypically, the agent in RL al- gorithms encounters sequences of highly correlated states and learning from these correlated data points may be prob- lematic since many deep learning algorithms suffer from high estimation variance when data samples are dependent. Thus, a mechanism that decorrelates the input such as the RB can improve data efﬁciency and reduce sample com- plexity. Since its usage in the DQN algorithm ( Mnih et al. , 2013), RB mechanism have become popular in many off- 1 T echnion Institute of T echnology , Haifa, Israel 2 NVIDIA Re- search, Israel 3 Bosch Center of AI, Haifa, Israel. Correspondence to: Shirli Di Castro Shashua <sdicastro@gmail.com>. policy RL algorithms ( Lillicrap et al. , 2015; Haarnoja et al. , 2018). Previous work has been done on the empirical beneﬁts of RB usage ( Fedus et al. , 2020; Zhang & Sutton , 2017), but still there is a lack in theoretical understanding of how the RB mechanism works. Understanding the prop- erties of RBs is crucial for convergence and ﬁnite sample analysis of algorithms that use a RB in training. For the best of our knowledge, this is the ﬁrst work to tackle these theoretical aspects. In this work we focus on the following setup. W e de- ﬁne a random processX that is pushed into a N samples size RB and analyze the characteristics of the stochastic process ofK samples that is sampled from the RB. W e analyze if properties of the original random process such as Markovity and stationarity are maintained and quantify the auto-correlation and covariance in the new RB process (later denoted by Y ) when possible. Our motivation comes from RL algorithms that use RB. Speciﬁcally, we focus on the induced Markov chain given a policy but we note that the analysis in this paper is also rel- evant to general random processes that are kept in a FIFO queue. This is relevant for domains such as First Come First Served domains ( Laguna & Marklund , 2013). Our goal is to provide analytical tools for analyzing algorithm s that use RBs. Our results can provide theoretical under- standing of phenomena seen in experiments using RBs that have never been analyzed theoretically before. Our the- ory for RBs provides tools for proving convergence of RB- based RL algorithms. Our main contributions are: 1. Formulating RBs as random processes and analyze their properties such as stationarity, Markovity, ergod- icity, auto-correlation and covariance. 2. Comparing between properties of the original random process that was pushed into the RB and the sampled process at the output of the RB. Particularly we prove that when sampling uniformly from the RB, the RB forms as a de-correlator between the sampled batches. 3. Connecting our RB theory to RL by demonstrating this connection through a RB-based actor critic RL al- gorithm that samplesK transitions from RB with size N for updating its parameters. W e prove, for the ﬁrst time, the asymptotic convergence of such RB-basedAnalysis of Stochastic Processes through Replay Buffers Figure 1. Replay buffer ﬂow diagram: Process X enters the RB which stores {Xt, . . . X t−N+1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transitions are thrown away from the RB. At each time ste p t, a random subset Jt of K time steps is sampled from the RB. W is simply [RB, J ]. Y is the process of averaging a function over X at times from the subset J. Lastly , the process Z is simply a function applied on the variable X. Comp aring Y to Z, we can see that Z can serve as an online update whil e Y can serve as a RB-based update. actor critic algorithm. The paper is structured as follows. W e begin with present- ing the setup in Section 2. W e then state our main results regarding RB properties in Section 3. In Section 4 we con- nect between our RB theory and its use in RL and provide a convergence proof for an RB-based actor critic algorithm. Afterward, in Section 5 we position our work in existing literature and conclude in Section 6. 2. Setup for Replay Buffer Analysis 2.1. Replay Buffer Structure LetX ≜ (Xt)∞ t=0 be a stochastic process where the subscript t indicates time. The samples are dynamically pushed into a Replay Buffer (RB; Lin, 1993; Mnih et al. , 2013) of capacity N, i.e., it is a First-In-First-Out (FIFO) buffer that can hold the N latest samples. W e deﬁne the state of the RB at time t with RBt = {Xt−N+1, . . . , X t}. Suppose that the buffer cells are numbered from 1 to N. The latest observation of X is pushed into cell 1, the ob- servation before into cell 2, etc. When a new observation arrives, the observation in cell n is pushed into cell n + 1 for 1 ≤ n < N , while the observation in cell N is thrown away. The random process RB = ( RBt)∞ t=0 contains the last N samples of X. The random process Y is deﬁned as the av- erage of random K samples (without replacement) out of the N samples and applying a function f(·) : · → RD 1 1 W e note that f(·) may also depend on t but we leave that for the sake of simplicity . where D is the dimension of the algorithm 2. The function f(·) may correspond to a typical RL function that one usu- ally ﬁnd in RL algorithms such as linear function approxi- mation, T emporal Difference, etc. ( Bertsekas & Tsitsiklis , 1996). W e elaborate on possible RL functions in Section 4.2. 2.2. Replay Buffer Sampling Method W e analyze the ”unordered sampling without replacement” strategy from the RB. W e note that other sampling meth- ods may be analyzed, but we chose this speciﬁc sampling due to its popularity in many deep reinforcement learning algorithms 3. Let N be a set of indices: N = {1, . . . , N } and let ¯J be a subset of K indices from N. Given N and K, let CN,K be the set of all possible subsets ¯J for speciﬁc N and K. Then, the probability of sampling subset ¯J is pN,K binom( ¯J) = 1 (N K) ∀ ¯J ∈ CN,K , where (N K ) ≜ N! (N−K)!K! is the binomial coefﬁcient. 2.3. Replay Buffer Related Processes W e denote the set of K temporal indices of the sam- ples from RB by the random process J (corresponds to a ”Batch” in Deep Learning) where Jt = {jk}K k=1 ⊂ {t− N + 1, . . . , t } 4. Similarly, the corresponding K RB in- dices process is ¯J where ¯Jt ⊂ { 1, . . . , N }. W e remark that 2 For example, in linear function approximation of Actor-Cri tic algorithms, D is the dimension of the linear basis used to approx- imate the value function by the critic. 3 In Section 6 we discuss shortly future directions for other sampling schemes. 4 W e note that in the ﬁrst K steps the batch is of size smaller than K and in the ﬁrst N steps the RB is not full.Analysis of Stochastic Processes through Replay Buffers both Jt and ¯Jt contain identical information but one refers to the absolute time, and one to the indices of the RB. W e deﬁne the random processWt ≜ [RBt, Jt] which holds both the information on the RB as well on the sampling from it. For later usage, we deﬁne the processXt going through a function f(·) with Zt ≜ f(Xt). The resulting Yt has the structure of Yt = 1 K ∑ j∈Jt Zj = 1 K ∑ j∈Jt f(Xj). The stochastic processes relations that are described abov e are visualized in Figure 1. 3. Replay Buffer Properties In this section we analyze the properties of a random pro- cessY that is sampled from the RB and used in some RL algorithm. Speciﬁcally, we analyze stationarity, Markovi ty, ergodicity, auto-correlation, and covariance. 3.1. Stationarity, Markovity and Ergodicity The following Lemmas characterize the connection be- tween different properties of X that enter the RB and the properties of the processes RB and Y . Stationarity is not a typical desired RL property since we constantly thrive to improve the policy (and thus the in- duced policy) but we bring it here for the sake of complete- ness. Lemma 1(Stationarity). Let Xt and ¯Jt be stationary pro- cesses. Then, RBt and Yt are stationary. The proof for Lemma 1 is deferred to Section A.1 in the supplementary material. In the next Lemma we analyze when the processRB is Markovian. This property is important in RL analysis. Note thatYt is not necessarily Markovian, however, Wt is Markovian. For this, with some abuse of notation, we de- ﬁneXn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of random variables realizaions from process X stored in the RB at time t from position n1 to n2. Lemma 2 (Markovity). Let Xt be a Markov process. Then: (1) RBt and Wt are Markovian. (2) The transition proba- bilities of RBt for t ≥ N are: P (RBt+1|RBt) =      P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t), 0 otherwise. If Jt is sampled according to ”unordered sampling without replacement”, then the transition probabilities of Wt for t ≥ N are: P (Wt+1|Wt) =        1 ( N K)P (Xt+1|Xt) ∀Jt+1 ∈ CN,K , if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise. The proof for Lemma 2 is deferred to Section A.2 in the supplementary material. In RL, the properties of aperiodicity and irreducible (that together form ergodicity; Norris, 1998) are crucial in many convergence proofs. The following states that these proper - ties are preserved when using RB. W e denote supp (Yt) as the set of all the values Yt can take. Lemma 3 (Ergodicity). Let Xt be a Markov process that is aperiodic and irreducible. Then, RBt and Wt are aperi- odic and irreducible. Moreover , every point y ∈ supp(Yt) is visited inﬁnitely often. The proof for Lemma 3 is deferred to Section A.3 in the supplementary material. 3.2. Auto-Correlation and Covariance In this section we analyze the auto-correlation and covari- ance of the processY expressed by process X properties. When X is stationary, the auto-correlation and covariance functions for X are: RX (τ) = E[XtXt+τ ] CX (τ) = E [(Xt − E [Xt]) (Xt+τ − E [Xt+τ ])] In the same way, the deﬁnition of the auto-correlation and covariance functions for the processY are RY (τ) and CY (τ), respectively. In the following theorem we prove the relationship between the auto-correlation and covari- ance functions of processesY and X. For that, we need to deﬁne the distribution of all time differences between two batches of samples as follows. Deﬁnition 1(Time difference distribution between two batches of samples) . Consider a RB of size N. Consider taking two different random permutations (batches), de- noted by¯Ja and ¯Jb, both of length K in two possibly dif- ferent time points, ta and tb = ta + τ. Let τ′ be a random variable where its distribution Fτ ′ (·) is the probability of each difference between each sample of ¯Ja and ¯Jb. The support of τ′ is τ − N + 1 ≤ τ′ ≤ τ + N − 1. Theorem 1 (Auto-Correlation and Covariance) . Let τ be the difference between two time steps of the processes Y . Let Eτ ′ be an expectation according to the distribution Fτ ′ (·). Then: RY (τ) = Eτ ′ [RZ (τ′)] , CY (τ) = Eτ ′ [CZ (τ′)] . (1)Analysis of Stochastic Processes through Replay Buffers The proof for Theorem 1 is in Section B.1 in the supple- mentary material. W e note two things. First, we note that we did not specify how the sampling is done from the RB and it is expressed by the random variable τ′ from Deﬁni- tion 1, i.e., Eq. ( 1) is a general expression. Second, we note that we express the correlation using process Z and not process X directly, but process Z auto-correlation and covariance can be computed directly in any practical case using the relationZt = f(Xt). For the speciﬁc case of ”unordered sampling without re- placement”, we express the relation between the second moments ofZ and Y explicitly through the distribution of τ′. Lemma 4 (Time difference Distribution for uniform batches). The random variable τ′ distribution for ”un- ordered sampling without replacement” is P (τ′) =      N−|d| N2 τ′ = τ + d, d ∈ {− N + 1, . . . 0, . . . N − 1} 0 τ′ < τ − N + 1 or τ′ > τ + N − 1 . The proof for Lemma 4 is in Section B.2 in the supplemen- tary material. In the following corollary we state the exact dependence in the case of random sampling ofK samples from a RB with size N. Corollary 1. Consider process Z where sampling is ac- cording to ”unordered sampling without replacement”. Then, the auto-correlation and covarinace of the process Y are: RY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)RZ (d + τ) CY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)CZ (d + τ) The proof for Corollary 1 is in Section B.3 in the supple- mentary material. W e see that using a RB reduces the auto- correlation and covaraince of processZ by factor of N. In- terestingly, this reduction is independent of K. This result proves the de-correlation effect of using RBs and provides an explicit expression for that. 4. Replay Buffers in Reinforcement Learning In the previous section we analyzed properties of stochastic processes that go through a RB. In this section we analyze RBs in RL. Stabilizing learning in modern off-policy deep RL algorithms, such as Deep Q-Networks ( Mnih et al. , 2013) or DDPG ( Lillicrap et al. , 2015), is based on saving past observed transitions in a RB. Even though its use is ex- tensive, the theoretical understanding of sampling batches mechanism from a RB is still quite limited. This is our focus in this section. W e begin with describing the setup that will serve us in this section. Then we connect between the random processes as deﬁned in Section 3 and common stochastic updates used in RL. W e then describe an RB-based actor-critic algorithm that uses a batch ofK samples from the RB in each param- eters update step. This type of algorithm serves as a basic example for popular usages of RBs in RL. W e note that other versions of RB-based RL algorithms (such as deep RL algorithms, value-based algorithms, discounted settings of the value function) can be analyzed with the stochastic processes tools we provide in this work. Finally, we present a full convergence proof for the RB-based actor critic algo- rithm. Despite its popularity, to the best of our knowledge, there is only handful of proofs that consider RB in RL al- gorithm analysis (e.g., Di-Castro Shashua et al. , 2021 or Lazic et al. , 2021). Most of the convergence proofs for off- policy algorithms assume that a single sample is sampled from the RB. Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based algorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP environ- ments, and they focused only on a single sample batch from the RB instead ofK samples (which complicates the proof). Therefore, for completeness and focusing on the RB prop- erties, we provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Simi- larly to previous works, we consider here a setup with lin- ear function approximation ( Bertsekas & Tsitsiklis , 1996). 4.1. Setup for Markov Decision Process An environment in RL is modeled as a Markov Deci- sion Process (MDP; Puterman, 1994), where S and A are the state space and action space, respectively. W e let P(S′|S, A) denote the probability of transitioning from state S ∈ S to state S′ ∈ S when applying action A ∈ A . W e consider a probabilistic policy πθ(A|S), parameterized by θ ∈ Θ ⊂ Rd which expresses the probability of the agent to choose an action A given that it is in state S. The MDP measure P (S′|S, A) and the policy measure πθ(A|S) induce together a Markov Chain (MC) measure Pθ(S′|S) (Pθ in matrix form). W e let µθ denote the stationary dis- tribution induced by the policy πθ. The reward function is denoted by r(S, A). In this work we focus on the average reward setting 5 . The goal of the agent is to ﬁnd a policy that maximizes the av- erage reward that the agent receives during its interaction 5 The discount factor settings can be obtained in similar way t o current setup.Analysis of Stochastic Processes through Replay Buffers with the environment. Under an ergodicity assumption, the average reward over time eventually converges to the ex- pected reward under the stationary distribution ( Bertsekas, 2005): ηθ ≜ lim T →∞ ∑ T t=0 r(St, At) T = ES∼µθ ,A∼πθ [r(S, A)]. (2) The state-value function evaluates the overall expected ac - cumulated rewards given a starting state S and a policy πθ V πθ (S) ≜ E [ ∞∑ t=0 (r(St, At) − ηθ) ⏐ ⏐ ⏐ ⏐ ⏐S0 = S, πθ ] , (3) where the actions follow the policy At ∼ πθ(·|St) and the next state follows the transition probability St+1 ∼ P (·|St, At). Let O = {S, A, S ′} be a transition from the environment. Let Ot be a transition at time t. The temporal difference error δ(O) (TD; Bertsekas & Tsitsiklis , 1996) is a random variable based on a single sampled transition from the RB, δ(O) = r(S, A) − η + φ(S′)⊤w − φ(S)⊤w, (4) where ˆV πθ w (S) = φ(S)⊤w is a linear approximation for V πθ (S), φ(S) ∈ Rd is a feature vector for state S and w ∈ Rd is the critic parameter vector. W e denote by Φ ∈ R|S|×d the matrix of all state feature vectors. 4.2. Replay Buffer as a Random Process in RL In Section 3 we compared between properties of general random process X going through a RB and yielding a pro- cess Y . In the RL context we have Xt ≜ Ot, meaning our basic component is a single transition of state-action- next-state observed at timet. In addition, we deﬁned Zt ≜ f(Xt) process where f(·) is a general function. In RL, f(·) is commonly deﬁned as the value function, the Q- function, the TD-error, the empirical average reward, the critic or actor gradients or any other function that com- putes a desirable update, based on an observed transition O. Common RL algorithms that use a single f(Ot) compu- tation in the parameters update step are commonly referred ason-policy algorithms where they update their parameters based only on the last observed transition in the Markov chain. See Figure 1 for a comparison between on-line up- dates and RB-based updates. Using the formulation of ran- dom processes we presented in Section 3, we can character- ize the on-line updates, based on a single last transition as follows: Zreward t = freward(Ot) = r(St, At) − ηt Zcritic t = fcritic(Ot) = δ(Ot)φ(St) Zactor t = factor (Ot) = δ(Ot)∇ log πθ(At|St) Algorithm 1 Linear Actor Critic with RB samples 1: Initialize Replay Buffer RB with size N. 2: Initialize actor parameters θ0, critic parameters w0 and average reward estimator η0. 3: Learning steps {αη t}, {αw t}, {αθ t}. 4: for t = 0 , . . . do 5: Interact with MDP M according to policy πθt and add the transition {St, At, r(St, At), St+1} to RB t. 6: Sample Jt - K random time indices form RBt. De- note the corresponding transitions as {Oj }j∈Jt . 7: δ(Oj ) = r(Sj , Aj ) − ηt + φ(S′ j )⊤wt − φ(Sj )⊤wt 8: Update average reward ηt+1 = ηt + αη t( 1 K ∑ j∈Jt r(Sj , Aj ) − ηt) 9: Update critic wt+1 = wt +αw t 1 K ∑ j∈Jt δ(Oj )φ(Sj ) 10: Update actor θt+1 = Γ ( θt − αθ t 1 K ∑ j∈Jt δ(Oj )∇θ log πθ(Aj |Sj ) ) 11: end for When using RB-based off-policy algorithms, the param- eters updates are computed over an average of K func- tions which are based on K transitions that were sam- pled randomly from the last stored N transitions. This exactly matches our deﬁnition of the process Y : Yt = 1 K ∑ j∈Jt f(Xj ) = 1 K ∑ j∈Jt Zj . The following updates are typical in RB-based off-policy algorithms: Y reward t = 1 K ∑ j∈Jt Zreward j = 1 K ∑ j∈Jt r(Sj , Aj ) − ηt Y critic t = 1 K ∑ j∈Jt Zcritic j = 1 K ∑ j∈Jt δ(Oj )φ(Sj ) Y actor t = 1 K ∑ j∈Jt Zactor j = 1 K ∑ j∈Jt δ(Oj )∇ log πθ(Aj |Sj ) (5) In Algorithm 1, we present a linear actor critic algorithm based on RB samples where the algorithm updates the ac- tor and critic using a random batch of transitions from the RB. In Section 4.5 we show how the results from Section 3 regarding a random process that is pushed into the RB, and the deﬁnitions ofX and Y processes are helpful in proving the asymptotic convergence of this algorithm. 4.3. Linear Actor Critic with RB Samples Algorithm The basic RB-based algorithm we analyze in this work is presented in Algorithm 1. W e propose a two time scale linear actor critic optimization scheme (similarly t o Konda & Tsitsiklis , 2000), which is an RB-based version of Bhatnagar et al. (2008) algorithm. Our algorithm is fully described by the random process Wt = [ RBt, Jt] and by the algorithm updates Y reward t , Y critic t and Y actor t describedAnalysis of Stochastic Processes through Replay Buffers in equation ( 5). See Figure 2 for a visualized ﬂow diagram of Algorithm 1. In Algorithm 1 we consider an environment, modeled as an MDP M, and we maintain a replay buffer RB with capacity N. The agent collects transitions {S, A, r(S, A), S′} from the environment and stores them in the RB. W e train the agent in an off-policy manner. At each time stept, the agent samples Jt – a subset of K random time indices from RBt which deﬁnes the random transitions batch for optimizing the average reward, critic and actor parameters. Note that for the actor updates, we use a projectionΓ(·) that projects any θ ∈ Rd to a compact set Θ whenever θ /∈ Θ . 4.4. Expectations of Critic and Actor Updates in Algorithm 1 W e divide the convergence analysis of Algorithm 1 into two parts. The ﬁrst part, presented in this section, is unique to our paper - we describe in Lemmas 5 and 6 a closed form of the expectations of the actor and critic updates, based on a random batch ofK transitions from the RB. In the second part, presented in Section 4.5, we use Stochas- tic Approximation tools for proving the algorithm updates convergence, based on the results from Lemmas 5 and 6. W e note that Section 4.5 follows the steps of the conver- gence proofs presented by Di-Castro Shashua et al. (2021) and Bhatnagar et al. (2009). For time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the in- duced MC with a corresponding policy parameter θt−n+1. For this parameter, we denote the corresponding state dis- tribution vectorρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 ). Finally, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1) and the reward vector rt−n+1 with elements rt−n+1(S) =∑ A πθt−n+1 (A|S)r(S, A). Based on these deﬁnitions we deﬁne Ct ≜ 1 N N∑ n=1 Dt−n+1 (Pt−n+1 − I) bt ≜ 1 N N∑ n=1 Dt−n+1 (rt−n+1 − ηθe) . (6) where I is the identity matrix and e is a vector of ones. Let Dθ ≜ diag(µθ) and deﬁne Cθ ≜ Dθ (Pθ − I) , b θ ≜ Dθ (rθ − ηθe) . (7) In our RB setting, since we have at time t a RB with the last N samples, Ct and bt in Equation ( 6) represent a superpo- sition of all related elements of these samples. For proving the convergence of the critic, we assume the policy is ﬁxed. Then, when t → ∞ , ρt−n+1 → µθ for all index n. This means that the induced MC is one for all the samples in the RB, so the sum overN disappears for Cθ and bθ. The following two lemmas compute the expectation of the critic and actor updates when using a random batch of K samples. The expectations are over all possible random batches sampled from the RB. Recall that¯Jt ⊂ { 1, . . . , N } and CN,K is the set of all possible subsets ¯J for speciﬁc N and K. These lemmas are the main results for proving con- vergence of RB-based RL algorithms. Lemma 5. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ, where Cθ and bθ are deﬁned in ( 7). Lemma 6. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∇θηθ − ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) , where ¯V πθ (S) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) . The proofs for Lemmas 5 and 6 are in sections C.1 and D.1, respectively, in the supplementary material. 4.5. Asymptotic Convergence of Algorithm 1 W e are now ready to present the convergence theo- rems for the critic and actor in Algorithm 1. In the proof of our theorems we use tools from Stochastic Ap- proximation (SA) ( Kushner & Y in , 2003; Borkar, 2009; Bertsekas & Tsitsiklis , 1996), a standard tool in the liter- ature for analyzing iterations of processes such as two time scale Actor-Critic in the context of RL. W e showed in Lemma 2 that the process Wt = [ RBt, Jt] of sampling K random transitions from the RB is a Markov process. In addition, we showed in Lemma 3 that if the orig- inal Markov chain is irreducible and aperiodic, then also the RB Markov process is irreducible and aperiodic. This property is required for the existence of unique stationary distribution and for proving the convergence of the itera- tions in Algorithm 1 using SA tools. W e note that proving convergence for a general function approximation is hard. W e choose to demonstrate the convergence for a linear func- tion approximation (LF A; Bertsekas & Tsitsiklis , 1996), in order to keep the convergence proof as simple as possibleAnalysis of Stochastic Processes through Replay Buffers Figure 2. Replay buffer in reinforcement learning ﬂow diagram: The ra ndom processes described in Figure 1 are reﬂected in Algorithm 1. Here the random process that enters the RB is O which is a tuple of (S, A, S ′). The RB stores the last N transitions {Ot, . . . O t−N−1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transition are thrown away from the RB. At each time step t, a random subset of K time steps is sampled from the RB and is denoted as Jt. W is simply [RB, J ]. In Algorithm 1 we have three different updates, Y reward t , Y critic t and Y actor t , all are averages over functions of transitions sampled fro m the RB. Then the parameters are updated accordingly . Finally , the policy parameter θt+1 is used to sample the action in transition Ot+1 that later enters to the RB. while focusing in the proof on the RB and random batches aspects of the algorithm. W e present several assumptions that are necessary for prov- ing the convergence of Algorithm 1. Assumption 4 is needed for the uniqueness of the convergence point of the critic. In addition, we choose a stateS∗ to be of value 0, i.e., V πθ (S∗) = 0 (due to Assumption 2, S∗ can be any of S ∈ S ). Assumption 5 is required in order to get a with probability 1 using the SA convergence. In our actor-critic setup we need two time-scales convergence, thus, in this assumption the critic is a ‘faster’ recursion than the actor. Assumption 1. 1. The set Θ is compact. 2. The reward |r(·, ·)| ≤ 1 for all S ∈ S , A ∈ A . Assumption 2. F or any policy πθ, the induced Markov chain of the MDP process {St}t≥0 is irreducible and ape- riodic. Assumption 3. F or any state–action pair (S, A), πθ(A|S) is continuously differentiable in the parameter θ. Assumption 4. 1. The matrix Φ has full rank. 2. The functions φ(S) are Liphschitz in S and bounded. 3. F or every w ∈ Rd, Φ w ̸= e where e is a vector of ones. Assumption 5. The step-sizes {αη t}, {αw t}, {αθ t}, t ≥ 0 satisfy ∑ ∞ tαη t = ∑ ∞ tαw t = ∑ ∞ tαθ t = ∞,∑ ∞ t(αη t)2, ∑ ∞ t(αw t)2, ∑ ∞ t(αθ t)2 < ∞ and αθ t = o(αw t). Now we are ready to prove the following theorems, regard- ing Algorithm 1. W e note that Theorem 2 and 3 state the critic and actor convergence. Theorem 2. (Convergence of the Critic to a ﬁxed point) Under Assumptions 1-5, for any given π and {ηt}, {wt} as in the updates in Algorithm 1, we have ηt → ηθ and wt → wπ with probability 1, where wπ is obtained as a unique solution to Φ ⊤CθΦ w + Φ ⊤bθ = 0 . The proof for Theorem 2 is in Section C in the supple- mentary material. It follows the proof for Lemma 5 in Bhatnagar et al. (2009). For establishing the convergence of the actor updates, we deﬁne additional terms. Let Z de- note the set of asymptotically stable equilibria of the ODE ˙θ = ˆΓ(−∇θηθ) and let Zǫ be the ǫ-neighborhood of Z. W e deﬁne ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Theorem 3. (Convergence of the actor) Under Assumptions 1-5, given ǫ > 0, ∃δ > 0 such that for θt, t ≥ 0 obtained using Algorithm 1, if supθt ∥ξπθ t ∥ < δ , then θt → Z ǫ as t → ∞ with probability one. The proof for Theorem 3 is in Section D in the supple- mentary material. It follows the proof for Theorem 2 in Bhatnagar et al. (2009).Analysis of Stochastic Processes through Replay Buffers 5. Related W ork Actor critic algorithms analysis: The convergence analysis of our proposed RB-based actor critic algo- rithm is based on the Stochastic Approximation method ( Kushner & Clark , 2012). Konda & Tsitsiklis (2000) pro- posed the actor-critic algorithm, and established the asymptotic convergence for the two time-scale actor-critic, with TD( λ) learning-based critic. Bhatnagar et al. (2009) proved the convergence result for the original actor-criti c and natural actor-critic methods. Di Castro & Meir (2010) proposed a single time-scale actor-critic algorithm and proved its convergence. W orks on ﬁnite sample analysis for actor critic algorithms ( Wu et al. , 2020; Zou et al. , 2019; Dalal et al. , 2018) analyze the case of last transition update and do not analyze the RB aspects in these algorithms. Recently, Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based actor critic al- gorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP en- vironments, and they focused only on a single sample batch from the RB instead of K samples. W e provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Replay Buffer analysis: Experience replay ( Lin, 1993) is a central concept for achieving good performance in deep reinforcement learning. Deep RB-based algorithms such as deep Q-learning (DQN, Mnih et al. , 2013), deep de- terministic policy gradient (DDPG; Lillicrap et al. , 2015), actor critic with experience replay (ACER; W ang et al. , 2016), T win Delayed Deep Deterministic policy gradi- ent (TD3, Fujimoto et al. , 2018), Soft Actor Critic (SAC, Haarnoja et al. , 2018) and many others use RBs to improve performance and data efﬁciency. W e focus mainly on works that provide some RB properties analysis. Zhang & Sutton (2017) and Liu & Zou (2018) study the effect of replay buffer size on the agent per- formance . Fedus et al. (2020) investigated through sim- ulated experiments how the learning process is affected by the ratio of learning updates to experience collected. Other works focus on methods to prioritize samples in the RB and provide experimental results to emphasis perfor- mance improvement when using prioritized sampling from RB ( Schaul et al. , 2015; Pan et al. , 2018; Zha et al. , 2019; Horgan et al. , 2018; Lahire et al. , 2021). W e, on the other hand, focus on the theoretical aspects of RB properties and provide convergence results for RB-based algorithms. Lazic et al. (2021) proposed a RB version for a regularized policy iteration algorithm. They provide an additional mo- tivation for using RBs, in addition to the advantage of re- duced temporal correlations: They claim that using RB in online learning in MDPs can approximate well the average of past value functions. Their analysis also suggests a new objective for sub-sampling or priority-sampling transiti ons in the RB, which differs priority-sampling objectives of pr e- vious work ( Schaul et al. , 2015). Regarding RB analysis in Deep RL algorithms, Fan et al. (2020) performed a ﬁnite sample analysis on DQN algo- rithm ( Mnih et al. , 2013). In their analysis, they simpli- ﬁed the technique of RB with an independence assump- tion and they replaced the distribution over random batches with a ﬁxed distribution. These assumptions essentially re - duce DQN to the neural ﬁtted Q-iteration (FQI) algorithm ( Riedmiller, 2005). In our work we focus on asymptotic convergence and analyze explicitly the distribution of ran - dom batches from the RB. 6. Conclusions In this work we analyzed RB and showed some basic ran- dom processes properties of it as ergodicity, stationarity, Markovity, correlation, and covariance. The latter two are of most interest since they can explain the success of mod- ern RL algorithm based on RB. In addition, we developed theoretical tools of stochastic process analysis for replay buffers. W e provided an example of how to use these tools to analyze the convergence of an RB-based actor critic al- gorithm. Similarly, other common RB-based algorithms in reinforcement learning such as DQN ( Mnih et al. , 2013), DDPG ( Lillicrap et al. , 2015), TD3 ( Fujimoto et al. , 2018), SAC ( Haarnoja et al. , 2018) and many others can be ana- lyzed now , using the tools we developed in this work. As a future research, we propose two directions that are of great interest and complete the analysis we provided in this work: 1. Spectrum analysis of the learning processes. Since we adopted an approach of ”Signals and Systems” with random signals ( Oppenheim et al. , 1997; Porat, 2008), one can use spectrum analysis in order to dis- cover instabilities or cycles in the learning process. 2. More complex RBs. There is a large experimental body of work that tries to propose different schemes of RBs. Some of them apply different independent on RL quantities sampling techniques while other apply dependent on RL quantities schemes (e.g., prioritized RB depends on the TD signal; Schaul et al. , 2015). In this work we paved the ﬁrst steps to apply analysis on such schemes (both dependent and independent). 7. Acknowledgements This work was partially supported by the Israel Science Foundation under contract 2199/20.Analysis of Stochastic Processes through Replay Buffers References Bertsekas, D.Dynamic programming and optimal control . Athena scientiﬁc Belmont, MA, 2005. Bertsekas, D. P . and Tsitsiklis, J. N. Neuro-dynamic pro- gramming. Athena Scientiﬁc, 1996. Bhatnagar, S. and Kumar, S. A simultaneous pertur- bation stochastic approximation-based actor-critic algo - rithm for markov decision processes. IEEE T ransactions on Automatic Control , 49(4):592–598, 2004. Bhatnagar, S., Ghavamzadeh, M., Lee, M., and Sutton, R. S. Incremental natural actor-critic algorithms. In Advances in neural information processing systems , pp. 105–112, 2008. Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee, M. Natural actor–critic algorithms. Automatica, 45(11): 2471–2482, 2009. Borkar, V . S. Stochastic approximation: a dynamical sys- tems viewpoint , volume 48. Springer, 2009. Borkar, V . S. and Meyn, S. P . The ode method for con- vergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization , 38 (2):447–469, 2000. Dalal, G., Sz ¨ or ´ enyi, B., Thoppe, G., and Mannor, S. Finite sample analyses for td (0) with function approximation. InProceedings of the AAAI Conference on Artiﬁcial In- telligence, 2018. Di Castro, D. and Meir, R. A convergent online single time scale actor critic algorithm. The Journal of Machine Learning Research , 11:367–410, 2010. Di-Castro Shashua, S., Di Castro, D., and Mannor, S. Sim and real: Better together. Advances in Neural Informa- tion Processing Systems , 34, 2021. Fan, J., W ang, Z., Xie, Y ., and Y ang, Z. A theoretical anal- ysis of deep q-learning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020. Fedus, W ., Ramachandran, P ., Agarwal, R., Bengio, Y ., Larochelle, H., Rowland, M., and Dabney, W . Revisiting fundamentals of experience replay. InInternational Con- ference on Machine Learning , pp. 3061–3071. PMLR, 2020. Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In Interna- tional Conference on Machine Learning , pp. 1587–1596. PMLR, 2018. Haarnoja, T ., Zhou, A., Abbeel, P ., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep rein- forcement learning with a stochastic actor. InInterna- tional conference on machine learning , pp. 1861–1870. PMLR, 2018. Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., V an Hasselt, H., and Silver, D. Dis- tributed prioritized experience replay.arXiv preprint arXiv:1803.00933 , 2018. Konda, V . R. and Tsitsiklis, J. N. Actor-critic algorithms. In Advances in neural information processing systems , pp. 1008–1014. Citeseer, 2000. Kushner, H. and Y in, G. G. Stochastic approximation and recursive algorithms and applications , volume 35. Springer Science & Business Media, 2003. Kushner, H. J. and Clark, D. S. Stochastic approximation methods for constrained and unconstrained systems , vol- ume 26. Springer Science & Business Media, 2012. Laguna, M. and Marklund, J. Business process modeling, simulation, and design . T aylor & Francis, 2013. Lahire, T ., Geist, M., and Rachelson, E. Large batch expe- rience replay. arXiv preprint arXiv:2110.01528 , 2021. Lazic, N., Y in, D., Abbasi-Y adkori, Y ., and Szepesvari, C. Improved regret bound and experience replay in regular- ized policy iteration.arXiv preprint arXiv:2102.12611 , 2021. Lillicrap, T . P ., Hunt, J. J., Pritzel, A., Heess, N., Erez, T ., T assa, Y ., Silver, D., and Wierstra, D. Continuous con- trol with deep reinforcement learning. arXiv preprint arXiv:1509.02971 , 2015. Lin, L.-J. Reinforcement learning for robots using neural networks. T echnical report, Carnegie-Mellon Univ Pitts- burgh P A School of Computer Science, 1993. Liu, R. and Zou, J. The effects of memory replay in re- inforcement learning. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 478–485. IEEE, 2018. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Norris, J. R. Markov chains . Number 2 in 1. Cambridge university press, 1998. Oppenheim, A. V ., Willsky, A. S., Nawab, S. H., Hern ´ andez, G. M., et al. Signals & systems . Pearson Educaci ´ on, 1997.Analysis of Stochastic Processes through Replay Buffers Pan, Y ., Zaheer, M., White, A., Patterson, A., and White, M. Organizing experience: a deeper look at replay mech- anisms for sample-based planning in continuous state do- mains.arXiv preprint arXiv:1806.04624 , 2018. Porat, B. Digital processing of random signals: theory and methods. Courier Dover Publications, 2008. Puterman, M. L. Markov Decision Processes . Wiley and Sons, 1994. Riedmiller, M. Neural ﬁtted q iteration–ﬁrst experi- ences with a data efﬁcient neural reinforcement learning method. InEuropean conference on machine learning , pp. 317–328. Springer, 2005. Schaul, T ., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint arXiv:1511.05952 , 2015. W ang, Z., Bapst, V ., Heess, N., Mnih, V ., Munos, R., Kavukcuoglu, K., and de Freitas, N. Sample efﬁ- cient actor-critic with experience replay.arXiv preprint arXiv:1611.01224 , 2016. Wu, Y ., Zhang, W ., Xu, P ., and Gu, Q. A ﬁnite time analysis of two time-scale actor critic methods. arXiv preprint arXiv:2005.01350 , 2020. Zha, D., Lai, K.-H., Zhou, K., and Hu, X. Experience replay optimization. arXiv preprint arXiv:1906.08387 , 2019. Zhang, S. and Sutton, R. S. A deeper look at experience replay. arXiv preprint arXiv:1712.01275 , 2017. Zou, S., Xu, T ., and Liang, Y . Finite-sample analysis for sarsa with linear function approximation. arXiv preprint arXiv:1902.02234 , 2019.Analysis of Stochastic Processes through Replay Buffers A. Proofs for Lemmas in Section 3 A.1. Proof of Lemma 1 Proof. Stationarity of RBt: Recall that stationarity (in the strong sense) means that fo r m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FX (Xt1+τ , . . . , X tm+τ ) = FX (Xt1 , . . . , X tm ), where FX (Xt1 , . . . , X tm ) is the cumulative distribution. Then, FRB (RBt1+τ , . . . , RB tm+τ ) (1) = FX (Xt1+τ −N+1, . . . , X t1+τ , Xt2+τ −N+1, . . . , X t2+τ , . . . , Xtm+τ −N+1, . . . , X tm+τ ), (2) = FX (Xt1−N+1, . . . , X t1 , Xt2−N+1, . . . , X t2 , . . . , Xtm−N+1, . . . , X tm ) (3) = FRB (RBt1 , . . . , RB tm ), where we use the RB deﬁnition in (1), stationarity of X in (2), and expressing RB based on X in (3). Stationarity of Yt: Similarly, for m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FY (Yt1+τ , . . . , Y tm+τ ) (1) = FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj )   (2) = ∑ ¯Jt1+τ ,..., ¯Jtm+τ F ¯Jt1+τ ,..., ¯Jtm+τ (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (3) = ∑ ¯Jt1 ,..., ¯Jtm F ¯Jt1 ,..., ¯Jtm (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (4) = FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj )   (5) = FY (Yt1 , . . . , Y tm ), where in (1) we use the process Y deﬁnition, in (2) we use iterated expectation, in (3) we use b oth the stationarity of X and ¯J, in (4) we use again iterated expectation, and in (5) we use Y deﬁnition. A.2. Proof of Lemma 2 Proof. W e ﬁrst note that we use some abuse of notation when referring sometimes to random variables from processes X, RB, W and J the same as their realizations. However, to avoid an overhea d of the proof, we keep the notations simple and short.Analysis of Stochastic Processes through Replay Buffers Proving Markovity requires that P (RBt+1|RBt, RBt−1, . . . , RB 0) = P (RBt+1|RBt). (A.1) P (Wt+1|Wt, Wt−1, . . . , W 0) = P (Wt+1|Wt). (A.2) W e start with proving the Markovity of RBt. Let us denote Xn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of realizations of the random variables from process X stored in the RB at time t in positions n1 to n2. Note that when a new transition is pushed to the RB into position n = 1 , the oldest transition in position n = N is thrown away, and all the transitions in the RB move one index forward. W e present h ere some observations regarding the RB that will help us through the proof: RBt = XN 1 (t) = {Xt−N+1, . . . , X t−n+1 . . . , X t} (RB deﬁnition). (A.3) XN 1 (t + 1) = {Xt+1} ∪ XN−1 1 (t) (A.4) XN−1 1 (t) ⊂ XN 1 (t) (A.5) Xt ∈ XN 1 (t) (A.6) P (Xt+1|Xt, . . . X 0) = P (Xt+1|Xt) (Since Xt is assumed to be Markovian). (A.7) P (a, b|c1, c2, . . . ) = P (a|b, c1, c2, . . . ) · P (b|c1, c2, . . . ) (Expressing joint probability (A.8) as a conditional probabilities product). P (a|b) = p(a) (If a and b are independent) . (A.9) Computing the l.h.s. of equation ( A.1) yields P (RBt+1|RBt, . . . , RB 0) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t), . . . , X N 1 (0) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Similarly, computing the r.h.s of ( A.1) yields P (RBt+1|RBt) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Both sides of ( A.1) are equal and therefore RBt is Markovian. In addition we have that for t ≥ N: P (RBt+1|RBt) = { P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise . Next, we prove the Markovity of Wt. Recall that Wt is deﬁned as: Wt = [ RBt, Jt] (A.10) where Jt ⊂ { t − N + 1, . . . , t } is a random subset of K time indices. By their deﬁnition, RBt and Jt are independent for all t. Computing the l.h.s. of equation ( A.2) yields P (Wt+1|Wt, . . . , W 0) (A.10) = P (RBt+1, Jt+1|RBt, Jt, . . . , RB 0, J0) (A.8) = P (RBt+1|Jt+1, RBt, Jt, . . . , RB 0, J0) · P (Jt+1|RBt, Jt, . . . , RB 0, J0) (A.1),(A.9) = P (RBt+1|RBt) · P (Jt+1) (A.9) = P (RBt+1, Jt+1|RBt, Jt) (A.10) = P (Wt+1|Wt)Analysis of Stochastic Processes through Replay Buffers W e have the required result in ( A.2), therefore Wt is Markovian. In addition, If Jt+1 is sampled according to ”unordered sampling without replacement” (deﬁned in Section 2.2), then for t ≥ N: P (Wt+1|Wt) = P (RBt+1|RBt)·P (Jt+1) =        1 (N K) P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) ∀Jt+1 ∈ CN,K , 0 otherwise. . A.3. Proof of Lemma 3 Proof. W e prove by contradiction. Let us assume that the process RB is neither aperiodic nor irreducible. If it is periodic, then one of the indices in the RB is periodic. Without loss of g enerality, let us assume that this is the l delayed time-steps index. But since in this index we have a periodic process, i.e ., it is the process X delayed in l steps, it contradicts the assumption that X is aperiodic. W e prove irreducibility in a similar way. Since the process Y is a deterministic function of the process RB, it must be aperiodic and irreducible as well, otherwise it will contradict the aperiodicity and irreducibility of t he process RB. Finally, since f(·) is a deterministic function, and since for each t, Yt is an image of an ergodic process Xt, i.e., each x ∈ Xt is visited inﬁnitely often, and as a results each point y ∈ supp(Yt) of the image of f(·) is visited inﬁnitely often, otherwise, it contradicts the d eterministic nature of f(·) or the ergodicity of X.Analysis of Stochastic Processes through Replay Buffers B. Auto Correlation and Covariance proofs B.1. Proof of Theorem 1 Proof. Let ¯Jt ⊂ { 1, . . . N } and ¯Jt+τ ⊂ { 1, . . . N } be subsets of K indices each. W e begin with calculating the auto- correlation of process Zt. RY (τ) = E[YtYt+τ ] (1) = E  1 K ∑ n∈ ¯Jt Zt−n+1 · 1 K ∑ m∈ ¯Jt+τ Zt+τ −m+1   (2) = E  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (3) = E ¯Jt, ¯Jt+τ ∼CN,K ,{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (4) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ 1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1) ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (5) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ En∼ ¯Jt,m∼ ¯Jt+τ [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (6) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ EXt−n+1,Xt+τ −m+1 [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (7) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ E [Zt−n+1Zt+τ −m+1] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (8) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ RZ (τ + n − m) ⏐ ⏐¯Jt, ¯Jt+τ ] ] (9) = Eτ ′∼ ˜Jτ [RZ (τ′)] where in (1) we used the deﬁnition of Y using the indices subse ts ¯Jt and ¯Jt+τ . In (2) used the deﬁnition of Z and in (3) we wrote the expectation explicitly. In (4) we used the condi tional expectation and in (5) we wrote 1 K ∑ n∈ ¯Jt f(·) and 1 K ∑ m∈ ¯Jt+τ f(·) as an expectations since given the subsets ¯Jt and ¯Jt+τ , the probability of sampling index n or m from the RB is uniform and equals 1 K . In (6) we are left with the marginal expectations for every p ossible couple of indices. In (7) we used again the deﬁnition of Z and in (8) we used the deﬁ nition of the auto-correlation function of Z. In (9) we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}. The calculation for the covariance CY (τ) follows the same steps as we did for RY (τ). B.2. Proof of Lemma 4 Proof. Let ¯Jt = {¯jk}K k=1 ⊂ { N, . . . , 1} and ¯Jt+τ = {¯lk}K k=1 ⊂ { N, . . . , 1} be subsets of K indices each. Let Jt = {jk}K k=1 ⊂ { t − N + 1, . . . , t } and Jt+τ = {lk}K k=1 ⊂ { t + τ − N + 1, . . . , t + τ} be subsets of K time-steps each. The relations between these two subsets are: ¯jk = n → jk = t − n + 1 and ¯lk = m → lk = t + τ − m + 1. W e saw in Section B.1 that we can move from these two subsets into the set of all poss ible differences ˜Jτ . Recall that we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}.Analysis of Stochastic Processes through Replay Buffers W e now deﬁne some sets and multisets that will help us in the ca lculation of P (τ′). Deﬁnition 2. Let CN,K be the set of all possible permutations of choosing K samples out of N samples without replacement. Observe that: |CN,K | = (N K ) Deﬁnition 3. Let LN,K be the set of all possible tuples of two batches of K samples ch osen from a set of N samples. Observe that: |LN,K | = ((N K )) 2 Deﬁnition 4. Let MN,K be the multiset of all possible two-sample tuple generated f or all possible couple of batches in the set LN,K . Let ¯MN,K be the unique set of MN,K : Observe that: |MN,K | = K2 · ((N K )) 2 | ¯MN,K | = N2 Deﬁnition 5. Let M(n, m) be the number of times the unique tuple (n, m) appears in the multiset MN,K . Observe that: M(n, m) = ((N − 1 K − 1 )) 2 for all n ∈ { 1, . . . , N }, m ∈ { τ, . . . , N + τ} Deﬁnition 6. Let DN,K,τ ′ be the number of unique sample tuples which have the time diff erence such that: n−m = τ′ −τ Observe that: DN,K,τ ′ = N − |τ′ − τ| Here we consider the ”unordered sampling without replaceme nt” (described in Section 2) for sampling ¯Jt and ¯Jt+τ and we would like to calculate the probability distribution for ea ch time difference τ′, that is P (τ′). W e have total of K2 · ((N K ) ) 2 such differences since we have (N K ) possible permutations for each batch and in each permutatio n we have K time elements. W e saw in the deﬁnitions above that from all K2 · ((N K ) ) 2 possible two-sample couples we have N2 unique sample couples, each of which has ((N−1 K−1 ) ) 2 repetitions. From these N2 couples, we have only N − |τ′ − τ| unique sample tuples (n, m) that holds n − m = τ′ − τ. W e deﬁne d = τ′ − τ, therefore −N + 1 ≤ d ≤ N − 1. W e now can calculate P (τ′): P (τ′) = ((N−1 K−1 ) ) 2 · (N − |τ′ − τ|) K2 · ((N K ) ) 2 (1) = ((N−1 K−1 ) ) 2 · (N − |d|) K2 · ((N K ) ) 2 (2) = (N − 1)! · (N − 1)! · K! · K! · (N − K)! · (N − K)! · (N − |d|) K2 · (K − 1)! · (K − 1)! · (N − K)! · (N − K)! · N! · N! (3) = N − |d| N2 where in (1) we substitute τ′ − τ = d. In (2) we wrote explicitly the binomial terms. In (3) we canc eled similar elements in the denominator and numerator. Notice that this probabil ity formula is relevant only for τ − N + 1 ≤ τ′ ≤ τ + N − 1Analysis of Stochastic Processes through Replay Buffers and other values of τ′ can not be reached form combining these two batches. Therefo re, P (τ′) = 0 for τ − N + 1 > τ ′ and τ′ > τ + N − 1. Interestingly, this proof shows how parameter K is canceled out, meaning this time difference distribution is independent on K. In addition, we can observe that the resulting distributio n can be considered as a convolution of two rectangles, which represents the time limits of each batch and the unifor m sampling, and the resulting convolution, a triangle which represents P (τ′). B.3. Proof of Corollary 1 Proof. Combining Theorem 1 and Lemma 4 we get: RY (τ) = Eτ ′ [RZ (τ′)] = ∑ τ ′ P (τ′)RZ (τ′) 1 = N−1∑ d=−N+1 N − |d| N2 RZ (d + τ) where in (1) we used P (τ′) from Lemma 4 and changed the variables: d = τ′ − τ for τ − N + 1 ≤ τ′ ≤ τ + N − 1. Similar development can be done to CY (τ). C. Proof of Theorem 2: A verage reward and critic convergence Proof. Recall that our TD-error update Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj , Aj , r(Sj , Aj ), S′ j }. In the critic update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the critic update is deﬁned as w′ = w + αw 1 K ∑ j∈J δ(Oj )φ(Sj ). where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: w′ = w + αw 1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1). In this proof we follow the proof of Lemma 5 in Bhatnagar et al. (2009). Observe that the average reward and critic updates from Algorithm 1 can be written as ηt+1 = ηt + αη t ( F η t + Mη t+1 ) (C.1) wt+1 = vt + αw t ( F w t + Mw t+1 ) , (C.2) where F η t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mη t+1 ≜  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − ηt  − F η t F w t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mw t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) − F w tAnalysis of Stochastic Processes through Replay Buffers and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , M η τ , M w τ : τ ≤ t}. W e use Theorem 2.2 of Borkar & Meyn (2000) to prove convergence of these iterates. Brieﬂy, this theor em states that given an iteration as in ( C.1) and ( C.2), these iterations are bounded w .p.1 if Assumption 6. 1. F η t and F w t are Lipschitz, the functions F∞(η) = lim σ→∞ F η(ση)/σ and F∞(w) = limσ→∞ F w(σw)/σ are Lipschitz, and F∞(η) and F∞(w) are asymptotically stable in the origin. 2. The sequences Mη t+1 and Mw t+1 are martingale difference noises and for some Cη 0 , Cw 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥wt∥2). W e begin with the average reward update in ( C.1). The ODE describing its asymptotic behavior corresponds t o ˙η = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η  ≜ F η. (C.3) F η is Lipschitz continuous in η. The function F∞(η) exists and satisﬁes F∞(η) = −η. The origin is an asymptotically stable equilibrium for the ODE ˙η = F∞(η) and the related Lyapunov function is given by η2/2. For the critic update, consider the ODE ˙w = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1)  ≜ F w In Lemma 5 we show that this ODE can be written as ˙w = Φ ⊤CθΦ w + Φ ⊤bθ, (C.4) where Cθ and bθ are deﬁned in ( 7). F w is Lipschitz continuous in w and F∞(w) exists and satisﬁes F∞(w) = Φ ⊤CθΦ w. Consider the system ˙w = F∞(w) (C.5) In assumption 4 we assume that Φ w ̸= e for every w ∈ Rd. Therefore, the only asymptotically stable equilibrium fo r ( C.5) is the origin (see the explanation in the proof of Lemma 5 in Bhatnagar et al. (2009)). Therefore, for all t ≥ 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2 + ∥wt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥ηt∥2 + ∥wt∥2) for some Cη 0 , Cw 0 < ∞. Mη t can be directly seen to be uniformly bounded almost surely. T hus, Assumptions (A1) and (A2) of Borkar & Meyn (2000) are satisﬁed for the average reward, TD-error, and critic u pdates. From Theorem 2.1 of Borkar & Meyn (2000), the average reward, TD-error, and critic iterates are uni formly bounded with probability one. Note that when t → ∞ , ( C.3) has ηθ deﬁned as in ( 2) as its unique globally asymptotically stable equilibrium with V2(η) = ( η − ηθ)2 serving as the associated Lyapunov function. Next, suppose that w = wπ is a solution to the system Φ ⊤CθΦ w = 0 . Under Assumption 4, using the same arguments as in the proof of Lemma 5 in Bhatnagar et al. (2009), wπ is the unique globally asymptotically stable equilibrium o f the ODE ( C.4). Assumption 6 is now veriﬁed and under Assumption 5, the claim follows from Theorem 2.2, pp. 450 of (Borkar & Meyn , 2000).Analysis of Stochastic Processes through Replay Buffers C.1. Proof of Lemma 5 Proof. W e compute the expectation of the critic update with linear f unction approximation according to Algorithm 1. In this proof, we focus on the ”Unordered sampling without repl acement” strategy for sampling batch of K transitions from the replay buffer (see Section 2.2 for this strategy probability distribution). Recall that n is a position in the RB and it corresponds to transition Ot−n+1 = ( St−n+1, At−n+1, S′ t−n+1). W e will use the notation of ¯J ⊂ { 1, . . . , n, . . . , N } to refer the K indices sampled batches. In addition we will use the followi ng observations: P (n| ¯J, n ∈ ¯J) = 1 K , P (n| ¯J, n /∈ ¯J) = 0 (C.6) P (n ∈ ¯J) = K N , P (n /∈ ¯J) = 1 − K N P (n| ¯J) = P (n ∈ ¯J) · P (n| ¯J, n ∈ ¯J) + P (n /∈ ¯J) · P (n| ¯J, n /∈ ¯J) = K N 1 K + 0 = 1 N (C.7) P ( ¯J) = 1(N K ) (C.8) |CN,K | = (N K ) (C.9) Now we can compute the desired expectation: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)   = E ¯Jt∼CN,K  E{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt     ( C.6) = E ¯Jt∼CN,K [ E{Ot−n+1}n∈ ¯Jt ∼RBt [ En∼ ¯Jt [δ(Ot−n+1)φ(St−n+1)] ⏐ ⏐¯Jt ] ] 1 = E ¯Jt∼CN,K [ En∼ ¯Jt [ EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ] ⏐ ⏐¯Jt ] 2 = ∑ ¯Jt∈CN,K P ( ¯Jt) N∑ n=1 P (n| ¯Jt)EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ( C.7),(C.8) = ∑ ¯Jt∈CN,K 1(N K ) N∑ n=1 1 N EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] (C.9) = 1 N N∑ n=1 EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] 3 = 1 N N∑ n=1 ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] (C.10) where in (1) we are left with the marginal expectations for ea ch observation, in (2) we wrote expectations explicitly and in (3) we used the deﬁnition of the TD-error in ( 4). Next, for time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the induced MC with a corresponding policy paramet er θt−n+1. For this parameter, we denote the corresponding state distr ibution vector ρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 . In addition, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1). Similarly to ( Bertsekas & Tsitsiklis , 1996) Lemma 6.5, pp.298, we can substitute the inner expectation ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] = Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe), (C.11)Analysis of Stochastic Processes through Replay Buffers where I is the |S| × |S| identity matrix, e in |S| × 1 vector of ones and rt−n+1 is a |S| × 1 vector deﬁned as rt−n+1(s) =∑ a πθt−n+1 (A|S)r(S, A). Combining equations ( 6), ( C.10) and ( C.11) yields 1 N N∑ n=1 ( Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe) ) = Φ ⊤CtΦ w + Φ ⊤bt, (C.12) In the limit, t → ∞ and ρt−n+1 → µθ for all index n. Using Cθ and bθ deﬁned in ( 7), ( C.10) can be expressed as E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ. (C.13)Analysis of Stochastic Processes through Replay Buffers D. Proof of Theorem 3: Actor convergence Proof. Recall that our TD-error update in Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj, Aj , r(Sj , Aj ), S′ j }. In the actor update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the actor update is deﬁned as θ′ = Γ  θ − αθ 1 K ∑ j∈J δ(Oj )∇ log πθ(Aj |Sj )  . where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: θ′ = Γ  θ − αθ 1 K ∑ n∈ ¯J δ(Ot−n+1)∇ log πθ(At−n+1|St−n+1)  . In this proof we follow the proof of Theorem 2 in Bhatnagar et al. (2009). Let O = {S, A, S ′} and let δπ(O) = r(S, A) − η + φ(S′)⊤wπ − φ(S)⊤wπ, where wπ is the convergent parameter of the critic recursion with pro bability one (see its deﬁnition in the proof for Theorem 2). Observe that the actor parameter update from Algorithm 1 can be written as θt+1 = Γ ( θt − αθ t ( δ(O)∇θ log πθ(A|S) + F θ t − F θ t + Nθt t − Nθt t ) ) = Γ ( θt − αθ t ( Mθ t+1 + (F θ t − Nθt t ) + Nθt t ) ) where F θ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mθ t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) − F θ t Nθ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , θτ , M η τ , M w τ , M θ τ : τ ≤ t}. Since the critic converges along the faster timescale, from Theorem 2 it follows that F θ t − Nθt t = o(1). Now , let M2(t) = t−1∑ r=0 αθ rMθ r+1, t ≥ 1. The quantities δ(O) can be seen to be uniformly bounded since from the proof in The orem 2, {ηt} and {wt} are bounded sequences. Therefore, using Assumption 5, {M2(t)} is a convergent martingale sequence ( Bhatnagar & Kumar , 2004). Consider the actor update along the slower timescale corres ponding to αθ tin Algorithm 1. Let w(·) be a vector ﬁeld on a set Θ . Deﬁne another vector ﬁeld: ˆΓ ( w(y) ) = lim 0<η→0 (Γ ( y+ηw(y) ) −y η ) . In case this limit is not unique, we let ˆΓ ( w(y) ) be the set of all possible limit points (see pp. 191 of ( Kushner & Clark , 2012)). Consider now the ODE ˙θ = ˆΓ  −E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)     (D.1)Analysis of Stochastic Processes through Replay Buffers Substituting the result from Lemma 6, the above ODE is analogous to ˙θ = ˆΓ(−∇θηθ + ξπθ ) = ˆΓ ( − Nθ t ) (D.2) where ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Consider also an associated ODE: ˙θ = ˆΓ ( − ∇θηθ ) (D.3) W e now show that h1(θt) ≜ −Nθt t is Lipschitz continuous. Here wπθ t corresponds to the weight vector to which the critic update converges along the faster timescale when the corres ponding policy is πθt (see Theorem 2). Note that µθ(S), S ∈ S is continuously differentiable in θ and have bounded derivatives. Also, ¯ηθt is continuously differentiable as well and has bounded derivative as can also be seen from ( 2). Further, wπθ t can be seen to be continuously differentiable with bounded derivatives. Finally, ∇2πθt (A|S) exists and is bounded. Thus h1(θt) is a Lipschitz continuous function and the ODE ( D.1) is well posed. LetZ denote the set of asymptotically stable equilibria of ( D.3) i.e., the local minima of ηθ, and let Zǫ be the ǫ- neighborhood of Z. T o complete the proof, we are left to show that as supθ ∥ξπθ ∥ → 0 (viz. δ → 0), the trajectories of ( D.2) converge to those of ( D.3) uniformly on compacts for the same initial condition in bot h. This claim follows the same arguments as in the proof of Theorem 2 in Bhatnagar et al. (2009). D.1. Proof of Lemma 6 Proof. W e compute the required expectation with linear function ap proximation according to Algorithm 1. Following the same steps when proving the expectation for the critic in Sec tion C.1, we have: E ¯Jt∼CN,K , {Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπ θ (Ot−n+1)∇θ log πθ (At−n+1|St−n+1)   = 1 N N∑ n=1 ESt−n+1,A t−n+1,S ′ t−n+1 [( r(St−n+1, A t−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) ∇θ log πθ (At−n+1|St−n+1) ] Recall the deﬁnition of the state distribution vector ρt−n+1 in Section 4.4. In the limit, t → ∞ and ρt−n+1 → µθ for all index n, then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∑ S∈S µθ(S) ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ − φ(S)⊤wπθ ) ∇θ log πθ(A|S) W e deﬁne now the following term: ¯V πθ (S) = ∑ A∈A πθ(A|S) ¯Qπθ (S, A) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) , (D.4) where ¯V πθ (S) and ¯Qπθ (S, A) correspond to policy πθ. Note that here, the convergent critic parameter wπθ is used. Let’s look at the gradient of ( D.4):Analysis of Stochastic Processes through Replay Buffers ∇θ ¯V πθ (S) = ∇θ (∑ A∈A πθ(A|S) ¯Qπθ (S, A) ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ A∈A πθ(A|S) ( −∇θηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) − ∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Summing both sides over the stationary distribution µθ ∑ S µθ(S)∇θ ¯V πθ (S) = ∑ S µθ(S) ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ S µθ(S) ( −∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   − ∇θηθ + ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Then: ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) (∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θvπθ − ∇θ ¯V πθ (S) ) . Since µθ is a stationary distribution, ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ = ∑ S µθ(S) ∑ S′∈S Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ ∑ S µθ(S)Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ µθ(S′)φ(S′)⊤∇θwπθ , Then, ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) The result follows immediately.",
      "meta_data": {
        "arxiv_id": "2206.12848v1",
        "authors": [
          "Shirli Di Castro Shashua",
          "Shie Mannor",
          "Dotan Di-Castro"
        ],
        "published_date": "2022-06-26T11:20:44Z",
        "pdf_url": "https://arxiv.org/pdf/2206.12848v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper formulates replay buffers (RBs) as random processes and provides a theoretical analysis of their properties, including stationarity, Markovity, ergodicity, auto-correlation, and covariance. It compares these properties between an original stochastic process (X) pushed into the RB and the sampled process (Y) from the RB, proving that RBs act as a de-correlator for uniformly sampled batches. For the first time, the work connects this RB theory to reinforcement learning (RL) by demonstrating its use in proving the asymptotic convergence of a linear RB-based actor-critic algorithm that samples K transitions from the RB.",
        "methodology": "The methodology involves analyzing stochastic processes (X, RB, Y, W, Z) using concepts from probability theory and Markov chains. The RB is modeled as a First-In-First-Out (FIFO) buffer of fixed capacity N, and the sampling strategy is defined as \"unordered sampling without replacement\" of K samples. The core analytical tools include characterizing the stationary, Markovian, and ergodic properties of the RB and sampled processes, and deriving explicit relationships for auto-correlation and covariance between the input and output processes of the RB. For RL applications, the analysis is framed within Markov Decision Processes (MDPs) in an average reward setting, using linear function approximation. The asymptotic convergence of the RB-based actor-critic algorithm is proven using Stochastic Approximation (SA) tools, specifically focusing on the expectations of critic and actor updates and employing the ODE method for two-time scale convergence.",
        "experimental_setup": "The paper is a theoretical work and does not involve an experimental setup with datasets, benchmarks, or simulations. The analysis and convergence proofs are purely mathematical, demonstrating the theoretical properties and algorithmic convergence under specified assumptions rather than empirical validation.",
        "limitations": "The theoretical analysis of replay buffer properties is primarily focused on the \"unordered sampling without replacement\" strategy, acknowledging that other sampling methods could be analyzed. The convergence proof for the RB-based actor-critic algorithm is restricted to a single MDP environment and employs linear function approximation for simplicity, noting that proving convergence for general function approximation is challenging. The proofs establish asymptotic convergence, which does not address finite-sample performance. The convergence relies on several specific assumptions typical in Stochastic Approximation, such as compactness of parameter sets, boundedness of rewards, differentiability of policy, full rank of feature matrix, and particular step-size conditions.",
        "future_research_directions": "The authors propose two main future research directions: (1) Conducting spectrum analysis of the learning processes, using a \"Signals and Systems\" approach with random signals, to identify potential instabilities or cycles. (2) Extending the analysis to more complex replay buffer schemes, including those with different sampling techniques that are either independent of RL quantities or dependent on them (e.g., prioritized RBs), building upon the foundational steps laid in this work."
      }
    },
    {
      "title": "Analysis of Stochastic Processes through Replay Buffers",
      "abstract": "Replay buffers are a key component in many reinforcement learning schemes.\nYet, their theoretical properties are not fully understood. In this paper we\nanalyze a system where a stochastic process X is pushed into a replay buffer\nand then randomly sampled to generate a stochastic process Y from the replay\nbuffer. We provide an analysis of the properties of the sampled process such as\nstationarity, Markovity and autocorrelation in terms of the properties of the\noriginal process. Our theoretical analysis sheds light on why replay buffer may\nbe a good de-correlator. Our analysis provides theoretical tools for proving\nthe convergence of replay buffer based algorithms which are prevalent in\nreinforcement learning schemes.",
      "full_text": "arXiv:2206.12848v1  [cs.LG]  26 Jun 2022 Analysis of Stochastic Processes through Replay Buffers Shirli Di Castro Shashua 1 Shie Mannor 1 2 Dotan Di Castro 3 Abstract Replay buffers are a key component in many re- inforcement learning schemes. Y et, their theo- retical properties are not fully understood. In this paper we analyze a system where a stochas- tic processX is pushed into a replay buffer and then randomly sampled to generate a stochastic processY from the replay buffer. W e provide an analysis of the properties of the sampled pro- cess such as stationarity, Markovity and autocor- relation in terms of the properties of the original process. Our theoretical analysis sheds light on why replay buffer may be a good de-correlator. Our analysis provides theoretical tools for prov- ing the convergence of replay buffer based al- gorithms which are prevalent in reinforcement learning schemes. 1. Introduction A Replay buffer (RB) is a mechanism for saving past gen- erated data samples and for sampling data for off-policy reinforcement learning (RL) algorithms ( Lin, 1993). The RB serves a First-In-First-Out (FIFO) buffer with a ﬁxed capacity and it enables sampling mini-batches from previ- ously saved data points. Its structure and sampling mecha- nism provide a unique characteristic: the RB serves asde- correlator of data samples. T ypically, the agent in RL al- gorithms encounters sequences of highly correlated states and learning from these correlated data points may be prob- lematic since many deep learning algorithms suffer from high estimation variance when data samples are dependent. Thus, a mechanism that decorrelates the input such as the RB can improve data efﬁciency and reduce sample com- plexity. Since its usage in the DQN algorithm ( Mnih et al. , 2013), RB mechanism have become popular in many off- 1 T echnion Institute of T echnology , Haifa, Israel 2 NVIDIA Re- search, Israel 3 Bosch Center of AI, Haifa, Israel. Correspondence to: Shirli Di Castro Shashua <sdicastro@gmail.com>. policy RL algorithms ( Lillicrap et al. , 2015; Haarnoja et al. , 2018). Previous work has been done on the empirical beneﬁts of RB usage ( Fedus et al. , 2020; Zhang & Sutton , 2017), but still there is a lack in theoretical understanding of how the RB mechanism works. Understanding the prop- erties of RBs is crucial for convergence and ﬁnite sample analysis of algorithms that use a RB in training. For the best of our knowledge, this is the ﬁrst work to tackle these theoretical aspects. In this work we focus on the following setup. W e de- ﬁne a random processX that is pushed into a N samples size RB and analyze the characteristics of the stochastic process ofK samples that is sampled from the RB. W e analyze if properties of the original random process such as Markovity and stationarity are maintained and quantify the auto-correlation and covariance in the new RB process (later denoted by Y ) when possible. Our motivation comes from RL algorithms that use RB. Speciﬁcally, we focus on the induced Markov chain given a policy but we note that the analysis in this paper is also rel- evant to general random processes that are kept in a FIFO queue. This is relevant for domains such as First Come First Served domains ( Laguna & Marklund , 2013). Our goal is to provide analytical tools for analyzing algorithm s that use RBs. Our results can provide theoretical under- standing of phenomena seen in experiments using RBs that have never been analyzed theoretically before. Our the- ory for RBs provides tools for proving convergence of RB- based RL algorithms. Our main contributions are: 1. Formulating RBs as random processes and analyze their properties such as stationarity, Markovity, ergod- icity, auto-correlation and covariance. 2. Comparing between properties of the original random process that was pushed into the RB and the sampled process at the output of the RB. Particularly we prove that when sampling uniformly from the RB, the RB forms as a de-correlator between the sampled batches. 3. Connecting our RB theory to RL by demonstrating this connection through a RB-based actor critic RL al- gorithm that samplesK transitions from RB with size N for updating its parameters. W e prove, for the ﬁrst time, the asymptotic convergence of such RB-basedAnalysis of Stochastic Processes through Replay Buffers Figure 1. Replay buffer ﬂow diagram: Process X enters the RB which stores {Xt, . . . X t−N+1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transitions are thrown away from the RB. At each time ste p t, a random subset Jt of K time steps is sampled from the RB. W is simply [RB, J ]. Y is the process of averaging a function over X at times from the subset J. Lastly , the process Z is simply a function applied on the variable X. Comp aring Y to Z, we can see that Z can serve as an online update whil e Y can serve as a RB-based update. actor critic algorithm. The paper is structured as follows. W e begin with present- ing the setup in Section 2. W e then state our main results regarding RB properties in Section 3. In Section 4 we con- nect between our RB theory and its use in RL and provide a convergence proof for an RB-based actor critic algorithm. Afterward, in Section 5 we position our work in existing literature and conclude in Section 6. 2. Setup for Replay Buffer Analysis 2.1. Replay Buffer Structure LetX ≜ (Xt)∞ t=0 be a stochastic process where the subscript t indicates time. The samples are dynamically pushed into a Replay Buffer (RB; Lin, 1993; Mnih et al. , 2013) of capacity N, i.e., it is a First-In-First-Out (FIFO) buffer that can hold the N latest samples. W e deﬁne the state of the RB at time t with RBt = {Xt−N+1, . . . , X t}. Suppose that the buffer cells are numbered from 1 to N. The latest observation of X is pushed into cell 1, the ob- servation before into cell 2, etc. When a new observation arrives, the observation in cell n is pushed into cell n + 1 for 1 ≤ n < N , while the observation in cell N is thrown away. The random process RB = ( RBt)∞ t=0 contains the last N samples of X. The random process Y is deﬁned as the av- erage of random K samples (without replacement) out of the N samples and applying a function f(·) : · → RD 1 1 W e note that f(·) may also depend on t but we leave that for the sake of simplicity . where D is the dimension of the algorithm 2. The function f(·) may correspond to a typical RL function that one usu- ally ﬁnd in RL algorithms such as linear function approxi- mation, T emporal Difference, etc. ( Bertsekas & Tsitsiklis , 1996). W e elaborate on possible RL functions in Section 4.2. 2.2. Replay Buffer Sampling Method W e analyze the ”unordered sampling without replacement” strategy from the RB. W e note that other sampling meth- ods may be analyzed, but we chose this speciﬁc sampling due to its popularity in many deep reinforcement learning algorithms 3. Let N be a set of indices: N = {1, . . . , N } and let ¯J be a subset of K indices from N. Given N and K, let CN,K be the set of all possible subsets ¯J for speciﬁc N and K. Then, the probability of sampling subset ¯J is pN,K binom( ¯J) = 1 (N K) ∀ ¯J ∈ CN,K , where (N K ) ≜ N! (N−K)!K! is the binomial coefﬁcient. 2.3. Replay Buffer Related Processes W e denote the set of K temporal indices of the sam- ples from RB by the random process J (corresponds to a ”Batch” in Deep Learning) where Jt = {jk}K k=1 ⊂ {t− N + 1, . . . , t } 4. Similarly, the corresponding K RB in- dices process is ¯J where ¯Jt ⊂ { 1, . . . , N }. W e remark that 2 For example, in linear function approximation of Actor-Cri tic algorithms, D is the dimension of the linear basis used to approx- imate the value function by the critic. 3 In Section 6 we discuss shortly future directions for other sampling schemes. 4 W e note that in the ﬁrst K steps the batch is of size smaller than K and in the ﬁrst N steps the RB is not full.Analysis of Stochastic Processes through Replay Buffers both Jt and ¯Jt contain identical information but one refers to the absolute time, and one to the indices of the RB. W e deﬁne the random processWt ≜ [RBt, Jt] which holds both the information on the RB as well on the sampling from it. For later usage, we deﬁne the processXt going through a function f(·) with Zt ≜ f(Xt). The resulting Yt has the structure of Yt = 1 K ∑ j∈Jt Zj = 1 K ∑ j∈Jt f(Xj). The stochastic processes relations that are described abov e are visualized in Figure 1. 3. Replay Buffer Properties In this section we analyze the properties of a random pro- cessY that is sampled from the RB and used in some RL algorithm. Speciﬁcally, we analyze stationarity, Markovi ty, ergodicity, auto-correlation, and covariance. 3.1. Stationarity, Markovity and Ergodicity The following Lemmas characterize the connection be- tween different properties of X that enter the RB and the properties of the processes RB and Y . Stationarity is not a typical desired RL property since we constantly thrive to improve the policy (and thus the in- duced policy) but we bring it here for the sake of complete- ness. Lemma 1(Stationarity). Let Xt and ¯Jt be stationary pro- cesses. Then, RBt and Yt are stationary. The proof for Lemma 1 is deferred to Section A.1 in the supplementary material. In the next Lemma we analyze when the processRB is Markovian. This property is important in RL analysis. Note thatYt is not necessarily Markovian, however, Wt is Markovian. For this, with some abuse of notation, we de- ﬁneXn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of random variables realizaions from process X stored in the RB at time t from position n1 to n2. Lemma 2 (Markovity). Let Xt be a Markov process. Then: (1) RBt and Wt are Markovian. (2) The transition proba- bilities of RBt for t ≥ N are: P (RBt+1|RBt) =      P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t), 0 otherwise. If Jt is sampled according to ”unordered sampling without replacement”, then the transition probabilities of Wt for t ≥ N are: P (Wt+1|Wt) =        1 ( N K)P (Xt+1|Xt) ∀Jt+1 ∈ CN,K , if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise. The proof for Lemma 2 is deferred to Section A.2 in the supplementary material. In RL, the properties of aperiodicity and irreducible (that together form ergodicity; Norris, 1998) are crucial in many convergence proofs. The following states that these proper - ties are preserved when using RB. W e denote supp (Yt) as the set of all the values Yt can take. Lemma 3 (Ergodicity). Let Xt be a Markov process that is aperiodic and irreducible. Then, RBt and Wt are aperi- odic and irreducible. Moreover , every point y ∈ supp(Yt) is visited inﬁnitely often. The proof for Lemma 3 is deferred to Section A.3 in the supplementary material. 3.2. Auto-Correlation and Covariance In this section we analyze the auto-correlation and covari- ance of the processY expressed by process X properties. When X is stationary, the auto-correlation and covariance functions for X are: RX (τ) = E[XtXt+τ ] CX (τ) = E [(Xt − E [Xt]) (Xt+τ − E [Xt+τ ])] In the same way, the deﬁnition of the auto-correlation and covariance functions for the processY are RY (τ) and CY (τ), respectively. In the following theorem we prove the relationship between the auto-correlation and covari- ance functions of processesY and X. For that, we need to deﬁne the distribution of all time differences between two batches of samples as follows. Deﬁnition 1(Time difference distribution between two batches of samples) . Consider a RB of size N. Consider taking two different random permutations (batches), de- noted by¯Ja and ¯Jb, both of length K in two possibly dif- ferent time points, ta and tb = ta + τ. Let τ′ be a random variable where its distribution Fτ ′ (·) is the probability of each difference between each sample of ¯Ja and ¯Jb. The support of τ′ is τ − N + 1 ≤ τ′ ≤ τ + N − 1. Theorem 1 (Auto-Correlation and Covariance) . Let τ be the difference between two time steps of the processes Y . Let Eτ ′ be an expectation according to the distribution Fτ ′ (·). Then: RY (τ) = Eτ ′ [RZ (τ′)] , CY (τ) = Eτ ′ [CZ (τ′)] . (1)Analysis of Stochastic Processes through Replay Buffers The proof for Theorem 1 is in Section B.1 in the supple- mentary material. W e note two things. First, we note that we did not specify how the sampling is done from the RB and it is expressed by the random variable τ′ from Deﬁni- tion 1, i.e., Eq. ( 1) is a general expression. Second, we note that we express the correlation using process Z and not process X directly, but process Z auto-correlation and covariance can be computed directly in any practical case using the relationZt = f(Xt). For the speciﬁc case of ”unordered sampling without re- placement”, we express the relation between the second moments ofZ and Y explicitly through the distribution of τ′. Lemma 4 (Time difference Distribution for uniform batches). The random variable τ′ distribution for ”un- ordered sampling without replacement” is P (τ′) =      N−|d| N2 τ′ = τ + d, d ∈ {− N + 1, . . . 0, . . . N − 1} 0 τ′ < τ − N + 1 or τ′ > τ + N − 1 . The proof for Lemma 4 is in Section B.2 in the supplemen- tary material. In the following corollary we state the exact dependence in the case of random sampling ofK samples from a RB with size N. Corollary 1. Consider process Z where sampling is ac- cording to ”unordered sampling without replacement”. Then, the auto-correlation and covarinace of the process Y are: RY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)RZ (d + τ) CY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)CZ (d + τ) The proof for Corollary 1 is in Section B.3 in the supple- mentary material. W e see that using a RB reduces the auto- correlation and covaraince of processZ by factor of N. In- terestingly, this reduction is independent of K. This result proves the de-correlation effect of using RBs and provides an explicit expression for that. 4. Replay Buffers in Reinforcement Learning In the previous section we analyzed properties of stochastic processes that go through a RB. In this section we analyze RBs in RL. Stabilizing learning in modern off-policy deep RL algorithms, such as Deep Q-Networks ( Mnih et al. , 2013) or DDPG ( Lillicrap et al. , 2015), is based on saving past observed transitions in a RB. Even though its use is ex- tensive, the theoretical understanding of sampling batches mechanism from a RB is still quite limited. This is our focus in this section. W e begin with describing the setup that will serve us in this section. Then we connect between the random processes as deﬁned in Section 3 and common stochastic updates used in RL. W e then describe an RB-based actor-critic algorithm that uses a batch ofK samples from the RB in each param- eters update step. This type of algorithm serves as a basic example for popular usages of RBs in RL. W e note that other versions of RB-based RL algorithms (such as deep RL algorithms, value-based algorithms, discounted settings of the value function) can be analyzed with the stochastic processes tools we provide in this work. Finally, we present a full convergence proof for the RB-based actor critic algo- rithm. Despite its popularity, to the best of our knowledge, there is only handful of proofs that consider RB in RL al- gorithm analysis (e.g., Di-Castro Shashua et al. , 2021 or Lazic et al. , 2021). Most of the convergence proofs for off- policy algorithms assume that a single sample is sampled from the RB. Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based algorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP environ- ments, and they focused only on a single sample batch from the RB instead ofK samples (which complicates the proof). Therefore, for completeness and focusing on the RB prop- erties, we provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Simi- larly to previous works, we consider here a setup with lin- ear function approximation ( Bertsekas & Tsitsiklis , 1996). 4.1. Setup for Markov Decision Process An environment in RL is modeled as a Markov Deci- sion Process (MDP; Puterman, 1994), where S and A are the state space and action space, respectively. W e let P(S′|S, A) denote the probability of transitioning from state S ∈ S to state S′ ∈ S when applying action A ∈ A . W e consider a probabilistic policy πθ(A|S), parameterized by θ ∈ Θ ⊂ Rd which expresses the probability of the agent to choose an action A given that it is in state S. The MDP measure P (S′|S, A) and the policy measure πθ(A|S) induce together a Markov Chain (MC) measure Pθ(S′|S) (Pθ in matrix form). W e let µθ denote the stationary dis- tribution induced by the policy πθ. The reward function is denoted by r(S, A). In this work we focus on the average reward setting 5 . The goal of the agent is to ﬁnd a policy that maximizes the av- erage reward that the agent receives during its interaction 5 The discount factor settings can be obtained in similar way t o current setup.Analysis of Stochastic Processes through Replay Buffers with the environment. Under an ergodicity assumption, the average reward over time eventually converges to the ex- pected reward under the stationary distribution ( Bertsekas, 2005): ηθ ≜ lim T →∞ ∑ T t=0 r(St, At) T = ES∼µθ ,A∼πθ [r(S, A)]. (2) The state-value function evaluates the overall expected ac - cumulated rewards given a starting state S and a policy πθ V πθ (S) ≜ E [ ∞∑ t=0 (r(St, At) − ηθ) ⏐ ⏐ ⏐ ⏐ ⏐S0 = S, πθ ] , (3) where the actions follow the policy At ∼ πθ(·|St) and the next state follows the transition probability St+1 ∼ P (·|St, At). Let O = {S, A, S ′} be a transition from the environment. Let Ot be a transition at time t. The temporal difference error δ(O) (TD; Bertsekas & Tsitsiklis , 1996) is a random variable based on a single sampled transition from the RB, δ(O) = r(S, A) − η + φ(S′)⊤w − φ(S)⊤w, (4) where ˆV πθ w (S) = φ(S)⊤w is a linear approximation for V πθ (S), φ(S) ∈ Rd is a feature vector for state S and w ∈ Rd is the critic parameter vector. W e denote by Φ ∈ R|S|×d the matrix of all state feature vectors. 4.2. Replay Buffer as a Random Process in RL In Section 3 we compared between properties of general random process X going through a RB and yielding a pro- cess Y . In the RL context we have Xt ≜ Ot, meaning our basic component is a single transition of state-action- next-state observed at timet. In addition, we deﬁned Zt ≜ f(Xt) process where f(·) is a general function. In RL, f(·) is commonly deﬁned as the value function, the Q- function, the TD-error, the empirical average reward, the critic or actor gradients or any other function that com- putes a desirable update, based on an observed transition O. Common RL algorithms that use a single f(Ot) compu- tation in the parameters update step are commonly referred ason-policy algorithms where they update their parameters based only on the last observed transition in the Markov chain. See Figure 1 for a comparison between on-line up- dates and RB-based updates. Using the formulation of ran- dom processes we presented in Section 3, we can character- ize the on-line updates, based on a single last transition as follows: Zreward t = freward(Ot) = r(St, At) − ηt Zcritic t = fcritic(Ot) = δ(Ot)φ(St) Zactor t = factor (Ot) = δ(Ot)∇ log πθ(At|St) Algorithm 1 Linear Actor Critic with RB samples 1: Initialize Replay Buffer RB with size N. 2: Initialize actor parameters θ0, critic parameters w0 and average reward estimator η0. 3: Learning steps {αη t}, {αw t}, {αθ t}. 4: for t = 0 , . . . do 5: Interact with MDP M according to policy πθt and add the transition {St, At, r(St, At), St+1} to RB t. 6: Sample Jt - K random time indices form RBt. De- note the corresponding transitions as {Oj }j∈Jt . 7: δ(Oj ) = r(Sj , Aj ) − ηt + φ(S′ j )⊤wt − φ(Sj )⊤wt 8: Update average reward ηt+1 = ηt + αη t( 1 K ∑ j∈Jt r(Sj , Aj ) − ηt) 9: Update critic wt+1 = wt +αw t 1 K ∑ j∈Jt δ(Oj )φ(Sj ) 10: Update actor θt+1 = Γ ( θt − αθ t 1 K ∑ j∈Jt δ(Oj )∇θ log πθ(Aj |Sj ) ) 11: end for When using RB-based off-policy algorithms, the param- eters updates are computed over an average of K func- tions which are based on K transitions that were sam- pled randomly from the last stored N transitions. This exactly matches our deﬁnition of the process Y : Yt = 1 K ∑ j∈Jt f(Xj ) = 1 K ∑ j∈Jt Zj . The following updates are typical in RB-based off-policy algorithms: Y reward t = 1 K ∑ j∈Jt Zreward j = 1 K ∑ j∈Jt r(Sj , Aj ) − ηt Y critic t = 1 K ∑ j∈Jt Zcritic j = 1 K ∑ j∈Jt δ(Oj )φ(Sj ) Y actor t = 1 K ∑ j∈Jt Zactor j = 1 K ∑ j∈Jt δ(Oj )∇ log πθ(Aj |Sj ) (5) In Algorithm 1, we present a linear actor critic algorithm based on RB samples where the algorithm updates the ac- tor and critic using a random batch of transitions from the RB. In Section 4.5 we show how the results from Section 3 regarding a random process that is pushed into the RB, and the deﬁnitions ofX and Y processes are helpful in proving the asymptotic convergence of this algorithm. 4.3. Linear Actor Critic with RB Samples Algorithm The basic RB-based algorithm we analyze in this work is presented in Algorithm 1. W e propose a two time scale linear actor critic optimization scheme (similarly t o Konda & Tsitsiklis , 2000), which is an RB-based version of Bhatnagar et al. (2008) algorithm. Our algorithm is fully described by the random process Wt = [ RBt, Jt] and by the algorithm updates Y reward t , Y critic t and Y actor t describedAnalysis of Stochastic Processes through Replay Buffers in equation ( 5). See Figure 2 for a visualized ﬂow diagram of Algorithm 1. In Algorithm 1 we consider an environment, modeled as an MDP M, and we maintain a replay buffer RB with capacity N. The agent collects transitions {S, A, r(S, A), S′} from the environment and stores them in the RB. W e train the agent in an off-policy manner. At each time stept, the agent samples Jt – a subset of K random time indices from RBt which deﬁnes the random transitions batch for optimizing the average reward, critic and actor parameters. Note that for the actor updates, we use a projectionΓ(·) that projects any θ ∈ Rd to a compact set Θ whenever θ /∈ Θ . 4.4. Expectations of Critic and Actor Updates in Algorithm 1 W e divide the convergence analysis of Algorithm 1 into two parts. The ﬁrst part, presented in this section, is unique to our paper - we describe in Lemmas 5 and 6 a closed form of the expectations of the actor and critic updates, based on a random batch ofK transitions from the RB. In the second part, presented in Section 4.5, we use Stochas- tic Approximation tools for proving the algorithm updates convergence, based on the results from Lemmas 5 and 6. W e note that Section 4.5 follows the steps of the conver- gence proofs presented by Di-Castro Shashua et al. (2021) and Bhatnagar et al. (2009). For time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the in- duced MC with a corresponding policy parameter θt−n+1. For this parameter, we denote the corresponding state dis- tribution vectorρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 ). Finally, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1) and the reward vector rt−n+1 with elements rt−n+1(S) =∑ A πθt−n+1 (A|S)r(S, A). Based on these deﬁnitions we deﬁne Ct ≜ 1 N N∑ n=1 Dt−n+1 (Pt−n+1 − I) bt ≜ 1 N N∑ n=1 Dt−n+1 (rt−n+1 − ηθe) . (6) where I is the identity matrix and e is a vector of ones. Let Dθ ≜ diag(µθ) and deﬁne Cθ ≜ Dθ (Pθ − I) , b θ ≜ Dθ (rθ − ηθe) . (7) In our RB setting, since we have at time t a RB with the last N samples, Ct and bt in Equation ( 6) represent a superpo- sition of all related elements of these samples. For proving the convergence of the critic, we assume the policy is ﬁxed. Then, when t → ∞ , ρt−n+1 → µθ for all index n. This means that the induced MC is one for all the samples in the RB, so the sum overN disappears for Cθ and bθ. The following two lemmas compute the expectation of the critic and actor updates when using a random batch of K samples. The expectations are over all possible random batches sampled from the RB. Recall that¯Jt ⊂ { 1, . . . , N } and CN,K is the set of all possible subsets ¯J for speciﬁc N and K. These lemmas are the main results for proving con- vergence of RB-based RL algorithms. Lemma 5. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ, where Cθ and bθ are deﬁned in ( 7). Lemma 6. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∇θηθ − ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) , where ¯V πθ (S) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) . The proofs for Lemmas 5 and 6 are in sections C.1 and D.1, respectively, in the supplementary material. 4.5. Asymptotic Convergence of Algorithm 1 W e are now ready to present the convergence theo- rems for the critic and actor in Algorithm 1. In the proof of our theorems we use tools from Stochastic Ap- proximation (SA) ( Kushner & Y in , 2003; Borkar, 2009; Bertsekas & Tsitsiklis , 1996), a standard tool in the liter- ature for analyzing iterations of processes such as two time scale Actor-Critic in the context of RL. W e showed in Lemma 2 that the process Wt = [ RBt, Jt] of sampling K random transitions from the RB is a Markov process. In addition, we showed in Lemma 3 that if the orig- inal Markov chain is irreducible and aperiodic, then also the RB Markov process is irreducible and aperiodic. This property is required for the existence of unique stationary distribution and for proving the convergence of the itera- tions in Algorithm 1 using SA tools. W e note that proving convergence for a general function approximation is hard. W e choose to demonstrate the convergence for a linear func- tion approximation (LF A; Bertsekas & Tsitsiklis , 1996), in order to keep the convergence proof as simple as possibleAnalysis of Stochastic Processes through Replay Buffers Figure 2. Replay buffer in reinforcement learning ﬂow diagram: The ra ndom processes described in Figure 1 are reﬂected in Algorithm 1. Here the random process that enters the RB is O which is a tuple of (S, A, S ′). The RB stores the last N transitions {Ot, . . . O t−N−1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transition are thrown away from the RB. At each time step t, a random subset of K time steps is sampled from the RB and is denoted as Jt. W is simply [RB, J ]. In Algorithm 1 we have three different updates, Y reward t , Y critic t and Y actor t , all are averages over functions of transitions sampled fro m the RB. Then the parameters are updated accordingly . Finally , the policy parameter θt+1 is used to sample the action in transition Ot+1 that later enters to the RB. while focusing in the proof on the RB and random batches aspects of the algorithm. W e present several assumptions that are necessary for prov- ing the convergence of Algorithm 1. Assumption 4 is needed for the uniqueness of the convergence point of the critic. In addition, we choose a stateS∗ to be of value 0, i.e., V πθ (S∗) = 0 (due to Assumption 2, S∗ can be any of S ∈ S ). Assumption 5 is required in order to get a with probability 1 using the SA convergence. In our actor-critic setup we need two time-scales convergence, thus, in this assumption the critic is a ‘faster’ recursion than the actor. Assumption 1. 1. The set Θ is compact. 2. The reward |r(·, ·)| ≤ 1 for all S ∈ S , A ∈ A . Assumption 2. F or any policy πθ, the induced Markov chain of the MDP process {St}t≥0 is irreducible and ape- riodic. Assumption 3. F or any state–action pair (S, A), πθ(A|S) is continuously differentiable in the parameter θ. Assumption 4. 1. The matrix Φ has full rank. 2. The functions φ(S) are Liphschitz in S and bounded. 3. F or every w ∈ Rd, Φ w ̸= e where e is a vector of ones. Assumption 5. The step-sizes {αη t}, {αw t}, {αθ t}, t ≥ 0 satisfy ∑ ∞ tαη t = ∑ ∞ tαw t = ∑ ∞ tαθ t = ∞,∑ ∞ t(αη t)2, ∑ ∞ t(αw t)2, ∑ ∞ t(αθ t)2 < ∞ and αθ t = o(αw t). Now we are ready to prove the following theorems, regard- ing Algorithm 1. W e note that Theorem 2 and 3 state the critic and actor convergence. Theorem 2. (Convergence of the Critic to a ﬁxed point) Under Assumptions 1-5, for any given π and {ηt}, {wt} as in the updates in Algorithm 1, we have ηt → ηθ and wt → wπ with probability 1, where wπ is obtained as a unique solution to Φ ⊤CθΦ w + Φ ⊤bθ = 0 . The proof for Theorem 2 is in Section C in the supple- mentary material. It follows the proof for Lemma 5 in Bhatnagar et al. (2009). For establishing the convergence of the actor updates, we deﬁne additional terms. Let Z de- note the set of asymptotically stable equilibria of the ODE ˙θ = ˆΓ(−∇θηθ) and let Zǫ be the ǫ-neighborhood of Z. W e deﬁne ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Theorem 3. (Convergence of the actor) Under Assumptions 1-5, given ǫ > 0, ∃δ > 0 such that for θt, t ≥ 0 obtained using Algorithm 1, if supθt ∥ξπθ t ∥ < δ , then θt → Z ǫ as t → ∞ with probability one. The proof for Theorem 3 is in Section D in the supple- mentary material. It follows the proof for Theorem 2 in Bhatnagar et al. (2009).Analysis of Stochastic Processes through Replay Buffers 5. Related W ork Actor critic algorithms analysis: The convergence analysis of our proposed RB-based actor critic algo- rithm is based on the Stochastic Approximation method ( Kushner & Clark , 2012). Konda & Tsitsiklis (2000) pro- posed the actor-critic algorithm, and established the asymptotic convergence for the two time-scale actor-critic, with TD( λ) learning-based critic. Bhatnagar et al. (2009) proved the convergence result for the original actor-criti c and natural actor-critic methods. Di Castro & Meir (2010) proposed a single time-scale actor-critic algorithm and proved its convergence. W orks on ﬁnite sample analysis for actor critic algorithms ( Wu et al. , 2020; Zou et al. , 2019; Dalal et al. , 2018) analyze the case of last transition update and do not analyze the RB aspects in these algorithms. Recently, Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based actor critic al- gorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP en- vironments, and they focused only on a single sample batch from the RB instead of K samples. W e provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Replay Buffer analysis: Experience replay ( Lin, 1993) is a central concept for achieving good performance in deep reinforcement learning. Deep RB-based algorithms such as deep Q-learning (DQN, Mnih et al. , 2013), deep de- terministic policy gradient (DDPG; Lillicrap et al. , 2015), actor critic with experience replay (ACER; W ang et al. , 2016), T win Delayed Deep Deterministic policy gradi- ent (TD3, Fujimoto et al. , 2018), Soft Actor Critic (SAC, Haarnoja et al. , 2018) and many others use RBs to improve performance and data efﬁciency. W e focus mainly on works that provide some RB properties analysis. Zhang & Sutton (2017) and Liu & Zou (2018) study the effect of replay buffer size on the agent per- formance . Fedus et al. (2020) investigated through sim- ulated experiments how the learning process is affected by the ratio of learning updates to experience collected. Other works focus on methods to prioritize samples in the RB and provide experimental results to emphasis perfor- mance improvement when using prioritized sampling from RB ( Schaul et al. , 2015; Pan et al. , 2018; Zha et al. , 2019; Horgan et al. , 2018; Lahire et al. , 2021). W e, on the other hand, focus on the theoretical aspects of RB properties and provide convergence results for RB-based algorithms. Lazic et al. (2021) proposed a RB version for a regularized policy iteration algorithm. They provide an additional mo- tivation for using RBs, in addition to the advantage of re- duced temporal correlations: They claim that using RB in online learning in MDPs can approximate well the average of past value functions. Their analysis also suggests a new objective for sub-sampling or priority-sampling transiti ons in the RB, which differs priority-sampling objectives of pr e- vious work ( Schaul et al. , 2015). Regarding RB analysis in Deep RL algorithms, Fan et al. (2020) performed a ﬁnite sample analysis on DQN algo- rithm ( Mnih et al. , 2013). In their analysis, they simpli- ﬁed the technique of RB with an independence assump- tion and they replaced the distribution over random batches with a ﬁxed distribution. These assumptions essentially re - duce DQN to the neural ﬁtted Q-iteration (FQI) algorithm ( Riedmiller, 2005). In our work we focus on asymptotic convergence and analyze explicitly the distribution of ran - dom batches from the RB. 6. Conclusions In this work we analyzed RB and showed some basic ran- dom processes properties of it as ergodicity, stationarity, Markovity, correlation, and covariance. The latter two are of most interest since they can explain the success of mod- ern RL algorithm based on RB. In addition, we developed theoretical tools of stochastic process analysis for replay buffers. W e provided an example of how to use these tools to analyze the convergence of an RB-based actor critic al- gorithm. Similarly, other common RB-based algorithms in reinforcement learning such as DQN ( Mnih et al. , 2013), DDPG ( Lillicrap et al. , 2015), TD3 ( Fujimoto et al. , 2018), SAC ( Haarnoja et al. , 2018) and many others can be ana- lyzed now , using the tools we developed in this work. As a future research, we propose two directions that are of great interest and complete the analysis we provided in this work: 1. Spectrum analysis of the learning processes. Since we adopted an approach of ”Signals and Systems” with random signals ( Oppenheim et al. , 1997; Porat, 2008), one can use spectrum analysis in order to dis- cover instabilities or cycles in the learning process. 2. More complex RBs. There is a large experimental body of work that tries to propose different schemes of RBs. Some of them apply different independent on RL quantities sampling techniques while other apply dependent on RL quantities schemes (e.g., prioritized RB depends on the TD signal; Schaul et al. , 2015). In this work we paved the ﬁrst steps to apply analysis on such schemes (both dependent and independent). 7. Acknowledgements This work was partially supported by the Israel Science Foundation under contract 2199/20.Analysis of Stochastic Processes through Replay Buffers References Bertsekas, D.Dynamic programming and optimal control . Athena scientiﬁc Belmont, MA, 2005. Bertsekas, D. P . and Tsitsiklis, J. N. Neuro-dynamic pro- gramming. Athena Scientiﬁc, 1996. Bhatnagar, S. and Kumar, S. A simultaneous pertur- bation stochastic approximation-based actor-critic algo - rithm for markov decision processes. IEEE T ransactions on Automatic Control , 49(4):592–598, 2004. Bhatnagar, S., Ghavamzadeh, M., Lee, M., and Sutton, R. S. Incremental natural actor-critic algorithms. In Advances in neural information processing systems , pp. 105–112, 2008. Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee, M. Natural actor–critic algorithms. Automatica, 45(11): 2471–2482, 2009. Borkar, V . S. Stochastic approximation: a dynamical sys- tems viewpoint , volume 48. Springer, 2009. Borkar, V . S. and Meyn, S. P . The ode method for con- vergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization , 38 (2):447–469, 2000. Dalal, G., Sz ¨ or ´ enyi, B., Thoppe, G., and Mannor, S. Finite sample analyses for td (0) with function approximation. InProceedings of the AAAI Conference on Artiﬁcial In- telligence, 2018. Di Castro, D. and Meir, R. A convergent online single time scale actor critic algorithm. The Journal of Machine Learning Research , 11:367–410, 2010. Di-Castro Shashua, S., Di Castro, D., and Mannor, S. Sim and real: Better together. Advances in Neural Informa- tion Processing Systems , 34, 2021. Fan, J., W ang, Z., Xie, Y ., and Y ang, Z. A theoretical anal- ysis of deep q-learning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020. Fedus, W ., Ramachandran, P ., Agarwal, R., Bengio, Y ., Larochelle, H., Rowland, M., and Dabney, W . Revisiting fundamentals of experience replay. InInternational Con- ference on Machine Learning , pp. 3061–3071. PMLR, 2020. Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In Interna- tional Conference on Machine Learning , pp. 1587–1596. PMLR, 2018. Haarnoja, T ., Zhou, A., Abbeel, P ., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep rein- forcement learning with a stochastic actor. InInterna- tional conference on machine learning , pp. 1861–1870. PMLR, 2018. Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., V an Hasselt, H., and Silver, D. Dis- tributed prioritized experience replay.arXiv preprint arXiv:1803.00933 , 2018. Konda, V . R. and Tsitsiklis, J. N. Actor-critic algorithms. In Advances in neural information processing systems , pp. 1008–1014. Citeseer, 2000. Kushner, H. and Y in, G. G. Stochastic approximation and recursive algorithms and applications , volume 35. Springer Science & Business Media, 2003. Kushner, H. J. and Clark, D. S. Stochastic approximation methods for constrained and unconstrained systems , vol- ume 26. Springer Science & Business Media, 2012. Laguna, M. and Marklund, J. Business process modeling, simulation, and design . T aylor & Francis, 2013. Lahire, T ., Geist, M., and Rachelson, E. Large batch expe- rience replay. arXiv preprint arXiv:2110.01528 , 2021. Lazic, N., Y in, D., Abbasi-Y adkori, Y ., and Szepesvari, C. Improved regret bound and experience replay in regular- ized policy iteration.arXiv preprint arXiv:2102.12611 , 2021. Lillicrap, T . P ., Hunt, J. J., Pritzel, A., Heess, N., Erez, T ., T assa, Y ., Silver, D., and Wierstra, D. Continuous con- trol with deep reinforcement learning. arXiv preprint arXiv:1509.02971 , 2015. Lin, L.-J. Reinforcement learning for robots using neural networks. T echnical report, Carnegie-Mellon Univ Pitts- burgh P A School of Computer Science, 1993. Liu, R. and Zou, J. The effects of memory replay in re- inforcement learning. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 478–485. IEEE, 2018. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Norris, J. R. Markov chains . Number 2 in 1. Cambridge university press, 1998. Oppenheim, A. V ., Willsky, A. S., Nawab, S. H., Hern ´ andez, G. M., et al. Signals & systems . Pearson Educaci ´ on, 1997.Analysis of Stochastic Processes through Replay Buffers Pan, Y ., Zaheer, M., White, A., Patterson, A., and White, M. Organizing experience: a deeper look at replay mech- anisms for sample-based planning in continuous state do- mains.arXiv preprint arXiv:1806.04624 , 2018. Porat, B. Digital processing of random signals: theory and methods. Courier Dover Publications, 2008. Puterman, M. L. Markov Decision Processes . Wiley and Sons, 1994. Riedmiller, M. Neural ﬁtted q iteration–ﬁrst experi- ences with a data efﬁcient neural reinforcement learning method. InEuropean conference on machine learning , pp. 317–328. Springer, 2005. Schaul, T ., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint arXiv:1511.05952 , 2015. W ang, Z., Bapst, V ., Heess, N., Mnih, V ., Munos, R., Kavukcuoglu, K., and de Freitas, N. Sample efﬁ- cient actor-critic with experience replay.arXiv preprint arXiv:1611.01224 , 2016. Wu, Y ., Zhang, W ., Xu, P ., and Gu, Q. A ﬁnite time analysis of two time-scale actor critic methods. arXiv preprint arXiv:2005.01350 , 2020. Zha, D., Lai, K.-H., Zhou, K., and Hu, X. Experience replay optimization. arXiv preprint arXiv:1906.08387 , 2019. Zhang, S. and Sutton, R. S. A deeper look at experience replay. arXiv preprint arXiv:1712.01275 , 2017. Zou, S., Xu, T ., and Liang, Y . Finite-sample analysis for sarsa with linear function approximation. arXiv preprint arXiv:1902.02234 , 2019.Analysis of Stochastic Processes through Replay Buffers A. Proofs for Lemmas in Section 3 A.1. Proof of Lemma 1 Proof. Stationarity of RBt: Recall that stationarity (in the strong sense) means that fo r m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FX (Xt1+τ , . . . , X tm+τ ) = FX (Xt1 , . . . , X tm ), where FX (Xt1 , . . . , X tm ) is the cumulative distribution. Then, FRB (RBt1+τ , . . . , RB tm+τ ) (1) = FX (Xt1+τ −N+1, . . . , X t1+τ , Xt2+τ −N+1, . . . , X t2+τ , . . . , Xtm+τ −N+1, . . . , X tm+τ ), (2) = FX (Xt1−N+1, . . . , X t1 , Xt2−N+1, . . . , X t2 , . . . , Xtm−N+1, . . . , X tm ) (3) = FRB (RBt1 , . . . , RB tm ), where we use the RB deﬁnition in (1), stationarity of X in (2), and expressing RB based on X in (3). Stationarity of Yt: Similarly, for m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FY (Yt1+τ , . . . , Y tm+τ ) (1) = FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj )   (2) = ∑ ¯Jt1+τ ,..., ¯Jtm+τ F ¯Jt1+τ ,..., ¯Jtm+τ (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (3) = ∑ ¯Jt1 ,..., ¯Jtm F ¯Jt1 ,..., ¯Jtm (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (4) = FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj )   (5) = FY (Yt1 , . . . , Y tm ), where in (1) we use the process Y deﬁnition, in (2) we use iterated expectation, in (3) we use b oth the stationarity of X and ¯J, in (4) we use again iterated expectation, and in (5) we use Y deﬁnition. A.2. Proof of Lemma 2 Proof. W e ﬁrst note that we use some abuse of notation when referring sometimes to random variables from processes X, RB, W and J the same as their realizations. However, to avoid an overhea d of the proof, we keep the notations simple and short.Analysis of Stochastic Processes through Replay Buffers Proving Markovity requires that P (RBt+1|RBt, RBt−1, . . . , RB 0) = P (RBt+1|RBt). (A.1) P (Wt+1|Wt, Wt−1, . . . , W 0) = P (Wt+1|Wt). (A.2) W e start with proving the Markovity of RBt. Let us denote Xn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of realizations of the random variables from process X stored in the RB at time t in positions n1 to n2. Note that when a new transition is pushed to the RB into position n = 1 , the oldest transition in position n = N is thrown away, and all the transitions in the RB move one index forward. W e present h ere some observations regarding the RB that will help us through the proof: RBt = XN 1 (t) = {Xt−N+1, . . . , X t−n+1 . . . , X t} (RB deﬁnition). (A.3) XN 1 (t + 1) = {Xt+1} ∪ XN−1 1 (t) (A.4) XN−1 1 (t) ⊂ XN 1 (t) (A.5) Xt ∈ XN 1 (t) (A.6) P (Xt+1|Xt, . . . X 0) = P (Xt+1|Xt) (Since Xt is assumed to be Markovian). (A.7) P (a, b|c1, c2, . . . ) = P (a|b, c1, c2, . . . ) · P (b|c1, c2, . . . ) (Expressing joint probability (A.8) as a conditional probabilities product). P (a|b) = p(a) (If a and b are independent) . (A.9) Computing the l.h.s. of equation ( A.1) yields P (RBt+1|RBt, . . . , RB 0) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t), . . . , X N 1 (0) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Similarly, computing the r.h.s of ( A.1) yields P (RBt+1|RBt) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Both sides of ( A.1) are equal and therefore RBt is Markovian. In addition we have that for t ≥ N: P (RBt+1|RBt) = { P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise . Next, we prove the Markovity of Wt. Recall that Wt is deﬁned as: Wt = [ RBt, Jt] (A.10) where Jt ⊂ { t − N + 1, . . . , t } is a random subset of K time indices. By their deﬁnition, RBt and Jt are independent for all t. Computing the l.h.s. of equation ( A.2) yields P (Wt+1|Wt, . . . , W 0) (A.10) = P (RBt+1, Jt+1|RBt, Jt, . . . , RB 0, J0) (A.8) = P (RBt+1|Jt+1, RBt, Jt, . . . , RB 0, J0) · P (Jt+1|RBt, Jt, . . . , RB 0, J0) (A.1),(A.9) = P (RBt+1|RBt) · P (Jt+1) (A.9) = P (RBt+1, Jt+1|RBt, Jt) (A.10) = P (Wt+1|Wt)Analysis of Stochastic Processes through Replay Buffers W e have the required result in ( A.2), therefore Wt is Markovian. In addition, If Jt+1 is sampled according to ”unordered sampling without replacement” (deﬁned in Section 2.2), then for t ≥ N: P (Wt+1|Wt) = P (RBt+1|RBt)·P (Jt+1) =        1 (N K) P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) ∀Jt+1 ∈ CN,K , 0 otherwise. . A.3. Proof of Lemma 3 Proof. W e prove by contradiction. Let us assume that the process RB is neither aperiodic nor irreducible. If it is periodic, then one of the indices in the RB is periodic. Without loss of g enerality, let us assume that this is the l delayed time-steps index. But since in this index we have a periodic process, i.e ., it is the process X delayed in l steps, it contradicts the assumption that X is aperiodic. W e prove irreducibility in a similar way. Since the process Y is a deterministic function of the process RB, it must be aperiodic and irreducible as well, otherwise it will contradict the aperiodicity and irreducibility of t he process RB. Finally, since f(·) is a deterministic function, and since for each t, Yt is an image of an ergodic process Xt, i.e., each x ∈ Xt is visited inﬁnitely often, and as a results each point y ∈ supp(Yt) of the image of f(·) is visited inﬁnitely often, otherwise, it contradicts the d eterministic nature of f(·) or the ergodicity of X.Analysis of Stochastic Processes through Replay Buffers B. Auto Correlation and Covariance proofs B.1. Proof of Theorem 1 Proof. Let ¯Jt ⊂ { 1, . . . N } and ¯Jt+τ ⊂ { 1, . . . N } be subsets of K indices each. W e begin with calculating the auto- correlation of process Zt. RY (τ) = E[YtYt+τ ] (1) = E  1 K ∑ n∈ ¯Jt Zt−n+1 · 1 K ∑ m∈ ¯Jt+τ Zt+τ −m+1   (2) = E  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (3) = E ¯Jt, ¯Jt+τ ∼CN,K ,{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (4) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ 1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1) ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (5) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ En∼ ¯Jt,m∼ ¯Jt+τ [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (6) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ EXt−n+1,Xt+τ −m+1 [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (7) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ E [Zt−n+1Zt+τ −m+1] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (8) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ RZ (τ + n − m) ⏐ ⏐¯Jt, ¯Jt+τ ] ] (9) = Eτ ′∼ ˜Jτ [RZ (τ′)] where in (1) we used the deﬁnition of Y using the indices subse ts ¯Jt and ¯Jt+τ . In (2) used the deﬁnition of Z and in (3) we wrote the expectation explicitly. In (4) we used the condi tional expectation and in (5) we wrote 1 K ∑ n∈ ¯Jt f(·) and 1 K ∑ m∈ ¯Jt+τ f(·) as an expectations since given the subsets ¯Jt and ¯Jt+τ , the probability of sampling index n or m from the RB is uniform and equals 1 K . In (6) we are left with the marginal expectations for every p ossible couple of indices. In (7) we used again the deﬁnition of Z and in (8) we used the deﬁ nition of the auto-correlation function of Z. In (9) we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}. The calculation for the covariance CY (τ) follows the same steps as we did for RY (τ). B.2. Proof of Lemma 4 Proof. Let ¯Jt = {¯jk}K k=1 ⊂ { N, . . . , 1} and ¯Jt+τ = {¯lk}K k=1 ⊂ { N, . . . , 1} be subsets of K indices each. Let Jt = {jk}K k=1 ⊂ { t − N + 1, . . . , t } and Jt+τ = {lk}K k=1 ⊂ { t + τ − N + 1, . . . , t + τ} be subsets of K time-steps each. The relations between these two subsets are: ¯jk = n → jk = t − n + 1 and ¯lk = m → lk = t + τ − m + 1. W e saw in Section B.1 that we can move from these two subsets into the set of all poss ible differences ˜Jτ . Recall that we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}.Analysis of Stochastic Processes through Replay Buffers W e now deﬁne some sets and multisets that will help us in the ca lculation of P (τ′). Deﬁnition 2. Let CN,K be the set of all possible permutations of choosing K samples out of N samples without replacement. Observe that: |CN,K | = (N K ) Deﬁnition 3. Let LN,K be the set of all possible tuples of two batches of K samples ch osen from a set of N samples. Observe that: |LN,K | = ((N K )) 2 Deﬁnition 4. Let MN,K be the multiset of all possible two-sample tuple generated f or all possible couple of batches in the set LN,K . Let ¯MN,K be the unique set of MN,K : Observe that: |MN,K | = K2 · ((N K )) 2 | ¯MN,K | = N2 Deﬁnition 5. Let M(n, m) be the number of times the unique tuple (n, m) appears in the multiset MN,K . Observe that: M(n, m) = ((N − 1 K − 1 )) 2 for all n ∈ { 1, . . . , N }, m ∈ { τ, . . . , N + τ} Deﬁnition 6. Let DN,K,τ ′ be the number of unique sample tuples which have the time diff erence such that: n−m = τ′ −τ Observe that: DN,K,τ ′ = N − |τ′ − τ| Here we consider the ”unordered sampling without replaceme nt” (described in Section 2) for sampling ¯Jt and ¯Jt+τ and we would like to calculate the probability distribution for ea ch time difference τ′, that is P (τ′). W e have total of K2 · ((N K ) ) 2 such differences since we have (N K ) possible permutations for each batch and in each permutatio n we have K time elements. W e saw in the deﬁnitions above that from all K2 · ((N K ) ) 2 possible two-sample couples we have N2 unique sample couples, each of which has ((N−1 K−1 ) ) 2 repetitions. From these N2 couples, we have only N − |τ′ − τ| unique sample tuples (n, m) that holds n − m = τ′ − τ. W e deﬁne d = τ′ − τ, therefore −N + 1 ≤ d ≤ N − 1. W e now can calculate P (τ′): P (τ′) = ((N−1 K−1 ) ) 2 · (N − |τ′ − τ|) K2 · ((N K ) ) 2 (1) = ((N−1 K−1 ) ) 2 · (N − |d|) K2 · ((N K ) ) 2 (2) = (N − 1)! · (N − 1)! · K! · K! · (N − K)! · (N − K)! · (N − |d|) K2 · (K − 1)! · (K − 1)! · (N − K)! · (N − K)! · N! · N! (3) = N − |d| N2 where in (1) we substitute τ′ − τ = d. In (2) we wrote explicitly the binomial terms. In (3) we canc eled similar elements in the denominator and numerator. Notice that this probabil ity formula is relevant only for τ − N + 1 ≤ τ′ ≤ τ + N − 1Analysis of Stochastic Processes through Replay Buffers and other values of τ′ can not be reached form combining these two batches. Therefo re, P (τ′) = 0 for τ − N + 1 > τ ′ and τ′ > τ + N − 1. Interestingly, this proof shows how parameter K is canceled out, meaning this time difference distribution is independent on K. In addition, we can observe that the resulting distributio n can be considered as a convolution of two rectangles, which represents the time limits of each batch and the unifor m sampling, and the resulting convolution, a triangle which represents P (τ′). B.3. Proof of Corollary 1 Proof. Combining Theorem 1 and Lemma 4 we get: RY (τ) = Eτ ′ [RZ (τ′)] = ∑ τ ′ P (τ′)RZ (τ′) 1 = N−1∑ d=−N+1 N − |d| N2 RZ (d + τ) where in (1) we used P (τ′) from Lemma 4 and changed the variables: d = τ′ − τ for τ − N + 1 ≤ τ′ ≤ τ + N − 1. Similar development can be done to CY (τ). C. Proof of Theorem 2: A verage reward and critic convergence Proof. Recall that our TD-error update Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj , Aj , r(Sj , Aj ), S′ j }. In the critic update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the critic update is deﬁned as w′ = w + αw 1 K ∑ j∈J δ(Oj )φ(Sj ). where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: w′ = w + αw 1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1). In this proof we follow the proof of Lemma 5 in Bhatnagar et al. (2009). Observe that the average reward and critic updates from Algorithm 1 can be written as ηt+1 = ηt + αη t ( F η t + Mη t+1 ) (C.1) wt+1 = vt + αw t ( F w t + Mw t+1 ) , (C.2) where F η t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mη t+1 ≜  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − ηt  − F η t F w t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mw t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) − F w tAnalysis of Stochastic Processes through Replay Buffers and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , M η τ , M w τ : τ ≤ t}. W e use Theorem 2.2 of Borkar & Meyn (2000) to prove convergence of these iterates. Brieﬂy, this theor em states that given an iteration as in ( C.1) and ( C.2), these iterations are bounded w .p.1 if Assumption 6. 1. F η t and F w t are Lipschitz, the functions F∞(η) = lim σ→∞ F η(ση)/σ and F∞(w) = limσ→∞ F w(σw)/σ are Lipschitz, and F∞(η) and F∞(w) are asymptotically stable in the origin. 2. The sequences Mη t+1 and Mw t+1 are martingale difference noises and for some Cη 0 , Cw 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥wt∥2). W e begin with the average reward update in ( C.1). The ODE describing its asymptotic behavior corresponds t o ˙η = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η  ≜ F η. (C.3) F η is Lipschitz continuous in η. The function F∞(η) exists and satisﬁes F∞(η) = −η. The origin is an asymptotically stable equilibrium for the ODE ˙η = F∞(η) and the related Lyapunov function is given by η2/2. For the critic update, consider the ODE ˙w = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1)  ≜ F w In Lemma 5 we show that this ODE can be written as ˙w = Φ ⊤CθΦ w + Φ ⊤bθ, (C.4) where Cθ and bθ are deﬁned in ( 7). F w is Lipschitz continuous in w and F∞(w) exists and satisﬁes F∞(w) = Φ ⊤CθΦ w. Consider the system ˙w = F∞(w) (C.5) In assumption 4 we assume that Φ w ̸= e for every w ∈ Rd. Therefore, the only asymptotically stable equilibrium fo r ( C.5) is the origin (see the explanation in the proof of Lemma 5 in Bhatnagar et al. (2009)). Therefore, for all t ≥ 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2 + ∥wt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥ηt∥2 + ∥wt∥2) for some Cη 0 , Cw 0 < ∞. Mη t can be directly seen to be uniformly bounded almost surely. T hus, Assumptions (A1) and (A2) of Borkar & Meyn (2000) are satisﬁed for the average reward, TD-error, and critic u pdates. From Theorem 2.1 of Borkar & Meyn (2000), the average reward, TD-error, and critic iterates are uni formly bounded with probability one. Note that when t → ∞ , ( C.3) has ηθ deﬁned as in ( 2) as its unique globally asymptotically stable equilibrium with V2(η) = ( η − ηθ)2 serving as the associated Lyapunov function. Next, suppose that w = wπ is a solution to the system Φ ⊤CθΦ w = 0 . Under Assumption 4, using the same arguments as in the proof of Lemma 5 in Bhatnagar et al. (2009), wπ is the unique globally asymptotically stable equilibrium o f the ODE ( C.4). Assumption 6 is now veriﬁed and under Assumption 5, the claim follows from Theorem 2.2, pp. 450 of (Borkar & Meyn , 2000).Analysis of Stochastic Processes through Replay Buffers C.1. Proof of Lemma 5 Proof. W e compute the expectation of the critic update with linear f unction approximation according to Algorithm 1. In this proof, we focus on the ”Unordered sampling without repl acement” strategy for sampling batch of K transitions from the replay buffer (see Section 2.2 for this strategy probability distribution). Recall that n is a position in the RB and it corresponds to transition Ot−n+1 = ( St−n+1, At−n+1, S′ t−n+1). W e will use the notation of ¯J ⊂ { 1, . . . , n, . . . , N } to refer the K indices sampled batches. In addition we will use the followi ng observations: P (n| ¯J, n ∈ ¯J) = 1 K , P (n| ¯J, n /∈ ¯J) = 0 (C.6) P (n ∈ ¯J) = K N , P (n /∈ ¯J) = 1 − K N P (n| ¯J) = P (n ∈ ¯J) · P (n| ¯J, n ∈ ¯J) + P (n /∈ ¯J) · P (n| ¯J, n /∈ ¯J) = K N 1 K + 0 = 1 N (C.7) P ( ¯J) = 1(N K ) (C.8) |CN,K | = (N K ) (C.9) Now we can compute the desired expectation: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)   = E ¯Jt∼CN,K  E{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt     ( C.6) = E ¯Jt∼CN,K [ E{Ot−n+1}n∈ ¯Jt ∼RBt [ En∼ ¯Jt [δ(Ot−n+1)φ(St−n+1)] ⏐ ⏐¯Jt ] ] 1 = E ¯Jt∼CN,K [ En∼ ¯Jt [ EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ] ⏐ ⏐¯Jt ] 2 = ∑ ¯Jt∈CN,K P ( ¯Jt) N∑ n=1 P (n| ¯Jt)EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ( C.7),(C.8) = ∑ ¯Jt∈CN,K 1(N K ) N∑ n=1 1 N EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] (C.9) = 1 N N∑ n=1 EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] 3 = 1 N N∑ n=1 ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] (C.10) where in (1) we are left with the marginal expectations for ea ch observation, in (2) we wrote expectations explicitly and in (3) we used the deﬁnition of the TD-error in ( 4). Next, for time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the induced MC with a corresponding policy paramet er θt−n+1. For this parameter, we denote the corresponding state distr ibution vector ρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 . In addition, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1). Similarly to ( Bertsekas & Tsitsiklis , 1996) Lemma 6.5, pp.298, we can substitute the inner expectation ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] = Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe), (C.11)Analysis of Stochastic Processes through Replay Buffers where I is the |S| × |S| identity matrix, e in |S| × 1 vector of ones and rt−n+1 is a |S| × 1 vector deﬁned as rt−n+1(s) =∑ a πθt−n+1 (A|S)r(S, A). Combining equations ( 6), ( C.10) and ( C.11) yields 1 N N∑ n=1 ( Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe) ) = Φ ⊤CtΦ w + Φ ⊤bt, (C.12) In the limit, t → ∞ and ρt−n+1 → µθ for all index n. Using Cθ and bθ deﬁned in ( 7), ( C.10) can be expressed as E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ. (C.13)Analysis of Stochastic Processes through Replay Buffers D. Proof of Theorem 3: Actor convergence Proof. Recall that our TD-error update in Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj, Aj , r(Sj , Aj ), S′ j }. In the actor update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the actor update is deﬁned as θ′ = Γ  θ − αθ 1 K ∑ j∈J δ(Oj )∇ log πθ(Aj |Sj )  . where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: θ′ = Γ  θ − αθ 1 K ∑ n∈ ¯J δ(Ot−n+1)∇ log πθ(At−n+1|St−n+1)  . In this proof we follow the proof of Theorem 2 in Bhatnagar et al. (2009). Let O = {S, A, S ′} and let δπ(O) = r(S, A) − η + φ(S′)⊤wπ − φ(S)⊤wπ, where wπ is the convergent parameter of the critic recursion with pro bability one (see its deﬁnition in the proof for Theorem 2). Observe that the actor parameter update from Algorithm 1 can be written as θt+1 = Γ ( θt − αθ t ( δ(O)∇θ log πθ(A|S) + F θ t − F θ t + Nθt t − Nθt t ) ) = Γ ( θt − αθ t ( Mθ t+1 + (F θ t − Nθt t ) + Nθt t ) ) where F θ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mθ t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) − F θ t Nθ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , θτ , M η τ , M w τ , M θ τ : τ ≤ t}. Since the critic converges along the faster timescale, from Theorem 2 it follows that F θ t − Nθt t = o(1). Now , let M2(t) = t−1∑ r=0 αθ rMθ r+1, t ≥ 1. The quantities δ(O) can be seen to be uniformly bounded since from the proof in The orem 2, {ηt} and {wt} are bounded sequences. Therefore, using Assumption 5, {M2(t)} is a convergent martingale sequence ( Bhatnagar & Kumar , 2004). Consider the actor update along the slower timescale corres ponding to αθ tin Algorithm 1. Let w(·) be a vector ﬁeld on a set Θ . Deﬁne another vector ﬁeld: ˆΓ ( w(y) ) = lim 0<η→0 (Γ ( y+ηw(y) ) −y η ) . In case this limit is not unique, we let ˆΓ ( w(y) ) be the set of all possible limit points (see pp. 191 of ( Kushner & Clark , 2012)). Consider now the ODE ˙θ = ˆΓ  −E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)     (D.1)Analysis of Stochastic Processes through Replay Buffers Substituting the result from Lemma 6, the above ODE is analogous to ˙θ = ˆΓ(−∇θηθ + ξπθ ) = ˆΓ ( − Nθ t ) (D.2) where ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Consider also an associated ODE: ˙θ = ˆΓ ( − ∇θηθ ) (D.3) W e now show that h1(θt) ≜ −Nθt t is Lipschitz continuous. Here wπθ t corresponds to the weight vector to which the critic update converges along the faster timescale when the corres ponding policy is πθt (see Theorem 2). Note that µθ(S), S ∈ S is continuously differentiable in θ and have bounded derivatives. Also, ¯ηθt is continuously differentiable as well and has bounded derivative as can also be seen from ( 2). Further, wπθ t can be seen to be continuously differentiable with bounded derivatives. Finally, ∇2πθt (A|S) exists and is bounded. Thus h1(θt) is a Lipschitz continuous function and the ODE ( D.1) is well posed. LetZ denote the set of asymptotically stable equilibria of ( D.3) i.e., the local minima of ηθ, and let Zǫ be the ǫ- neighborhood of Z. T o complete the proof, we are left to show that as supθ ∥ξπθ ∥ → 0 (viz. δ → 0), the trajectories of ( D.2) converge to those of ( D.3) uniformly on compacts for the same initial condition in bot h. This claim follows the same arguments as in the proof of Theorem 2 in Bhatnagar et al. (2009). D.1. Proof of Lemma 6 Proof. W e compute the required expectation with linear function ap proximation according to Algorithm 1. Following the same steps when proving the expectation for the critic in Sec tion C.1, we have: E ¯Jt∼CN,K , {Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπ θ (Ot−n+1)∇θ log πθ (At−n+1|St−n+1)   = 1 N N∑ n=1 ESt−n+1,A t−n+1,S ′ t−n+1 [( r(St−n+1, A t−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) ∇θ log πθ (At−n+1|St−n+1) ] Recall the deﬁnition of the state distribution vector ρt−n+1 in Section 4.4. In the limit, t → ∞ and ρt−n+1 → µθ for all index n, then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∑ S∈S µθ(S) ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ − φ(S)⊤wπθ ) ∇θ log πθ(A|S) W e deﬁne now the following term: ¯V πθ (S) = ∑ A∈A πθ(A|S) ¯Qπθ (S, A) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) , (D.4) where ¯V πθ (S) and ¯Qπθ (S, A) correspond to policy πθ. Note that here, the convergent critic parameter wπθ is used. Let’s look at the gradient of ( D.4):Analysis of Stochastic Processes through Replay Buffers ∇θ ¯V πθ (S) = ∇θ (∑ A∈A πθ(A|S) ¯Qπθ (S, A) ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ A∈A πθ(A|S) ( −∇θηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) − ∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Summing both sides over the stationary distribution µθ ∑ S µθ(S)∇θ ¯V πθ (S) = ∑ S µθ(S) ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ S µθ(S) ( −∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   − ∇θηθ + ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Then: ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) (∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θvπθ − ∇θ ¯V πθ (S) ) . Since µθ is a stationary distribution, ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ = ∑ S µθ(S) ∑ S′∈S Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ ∑ S µθ(S)Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ µθ(S′)φ(S′)⊤∇θwπθ , Then, ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) The result follows immediately.",
      "meta_data": {
        "arxiv_id": "2206.12848v1",
        "authors": [
          "Shirli Di Castro Shashua",
          "Shie Mannor",
          "Dotan Di-Castro"
        ],
        "published_date": "2022-06-26T11:20:44Z",
        "pdf_url": "https://arxiv.org/pdf/2206.12848v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper provides a theoretical analysis of replay buffers (RBs) by formulating them as random processes. It analyzes properties of the sampled process (Y) from the RB, such as stationarity, Markovity, ergodicity, auto-correlation, and covariance, in terms of the original input process (X). A key finding is that uniform sampling from an RB acts as a de-correlator, reducing auto-correlation and covariance of the process by a factor of N (RB size), independent of batch size K. The work also connects this RB theory to Reinforcement Learning (RL) by demonstrating its use in proving, for the first time, the asymptotic convergence of a RB-based actor-critic RL algorithm that samples K transitions.",
        "methodology": "The methodology involves defining a stochastic process X entering a First-In-First-Out (FIFO) replay buffer of fixed capacity N, and then a process Y sampled from this RB using 'unordered sampling without replacement' of K samples. The core analytical approach is to characterize the properties (stationarity, Markovity, ergodicity, auto-correlation, covariance) of Y based on those of X and a derived process Z=f(X). For RL, the paper models transitions as X, and updates as Zt=f(Xt) (e.g., TD-error, gradients). The convergence proof for the RB-based linear actor-critic algorithm (Algorithm 1) leverages Stochastic Approximation (SA) tools and linear function approximation (LFA) to simplify the analysis and focus on RB aspects.",
        "experimental_setup": "The paper is purely theoretical, focusing on mathematical analysis and proofs of stochastic process properties and asymptotic convergence for an RL algorithm. No empirical experiments, datasets, benchmarks, or simulators are described or used for validation. The convergence proofs themselves serve as the validation method within the theoretical framework, specifically for a linear actor-critic algorithm with linear function approximation.",
        "limitations": "The theoretical analysis primarily focuses on 'unordered sampling without replacement' from the replay buffer, leaving other sampling methods for future work. The convergence proof for RB-based RL algorithms is for a single MDP environment, contrasting with prior work that considered multiple environments but a single sample batch. Furthermore, the convergence proof relies on a linear function approximation to keep the proof tractable, acknowledging that proving convergence for a general function approximation is challenging. The analysis focuses on asymptotic convergence rather than finite-sample analysis.",
        "future_research_directions": "The authors suggest two main future research directions: (1) Conducting spectrum analysis of the learning processes, adopting a 'Signals and Systems' approach to uncover instabilities or cycles. (2) Extending the analysis to more complex replay buffer schemes, including those with different independent or dependent sampling techniques (e.g., prioritized RBs based on TD signal), building upon the foundational steps laid out in this work."
      }
    },
    {
      "title": "On Efficient Constructions of Checkpoints",
      "abstract": "Efficient construction of checkpoints/snapshots is a critical tool for\ntraining and diagnosing deep learning models. In this paper, we propose a lossy\ncompression scheme for checkpoint constructions (called LC-Checkpoint).\nLC-Checkpoint simultaneously maximizes the compression rate and optimizes the\nrecovery speed, under the assumption that SGD is used to train the model.\nLC-Checkpointuses quantization and priority promotion to store the most crucial\ninformation for SGD to recover, and then uses a Huffman coding to leverage the\nnon-uniform distribution of the gradient scales. Our extensive experiments show\nthat LC-Checkpoint achieves a compression rate up to $28\\times$ and recovery\nspeedup up to $5.77\\times$ over a state-of-the-art algorithm (SCAR).",
      "full_text": "On Efﬁcient Constructions of Checkpoints Yu Chen1 Zhenming Liu 1 Bin Ren 1 Xin Jin 2 Abstract Efﬁcient construction of checkpoints/snapshots is a critical tool for training and diagnosing deep learning models. In this paper, we propose a lossy compression scheme for checkpoint constructions (called LC-Checkpoint). LC-Checkpoint simul- taneously maximizes the compression rate and optimizes the recovery speed, under the assump- tion that SGD is used to train the model. LC- Checkpoint uses quantization and priority promo- tion to store the most crucial information for SGD to recover, and then uses a Huffman coding to leverage the non-uniform distribution of the gra- dient scales. Our extensive experiments show that LC-Checkpoint achieves a compression rate up to 28×and recovery speedup up to 5.77×over a state-of-the-art algorithm (SCAR). 1. Introduction Efﬁcient construction of checkpoints (snapshots) has been increasingly important to deep learning research. In the arms race of developing more accurate models, researchers utilize heavier computing infrastructure and develop deeper and larger models. Without proper infrastructure support, the research process inevitably becomes fragile. For exam- ple, distributed computation fails from time to time, leading to the excessive need to re-train models (Qiao et al., 2018b). Diagnosing deep learning models also evolves to a complex procedure partly because that the community has a better understanding of deep learning models and produces more rules for “debugging” them. Some common errors include gradient explosion (Goodfellow et al., 2016), “divide by zero” (Ioffe & Szegedy, 2015), and dead activation. This calls for the need to construct “breakpoints,” resembling those used in debugging computer programs, so that re- searchers can conveniently jump to the state right before the model “crashes” in the training. 1William & Mary, Williamsburg, Virginia, USA2Johns Hop- kins University, Baltimore, Maryland, USA. Correspondence to: Yu Chen <ychen39@email.wm.edu>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). Producing checkpoints frequently enables failed training process to restart with minimum wasted time, and serves as breakpoints for debugging models. So far the standard practice of constructing checkpoints is primitive. The most common practice is to save the model state directly, counting on that the backend system is sufﬁciently robust so that this operation does not become a bottleneck (Baylor et al., 2017). Attempts of partially storing model states are also examined (Qiao et al., 2018b) but these works usually focus on recovery speed, instead of directly tackling system issues. The most pronounced technical challenge here is that deep models are usually large, so producing frequent checkpoints creates unmanageable burdens to both I/O and storage, even under modern distributed platforms (Abadi et al., 2016; Li et al., 2014; Low et al., 2012). Therefore, this leads to our question: Research Q: How can we compress model checkpoints? We speciﬁcally aim to design a lossy compressing scheme, addressing two criteria simultaneously. First, like standard compression problems, we need to maximize the compres- sion rate. Second, the scheme needs to be optimized for the downstream application of training. When a model restarts from our lossy checkpoints, it needs to efﬁciently resume to the most recent state (e.g., restart from a failed process or reach the state preceding the crash). Compression of model states is a new technical problem that requires addressing cross-cutting constraints from informa- tion theory, learning algorithm, and system design. We need to leverage statistical patterns encoded in the model state and factor in how the model states interact with a learning algo- rithm (more speciﬁcally, stochastic gradient type algorithms in the deep learning setting). This means neither standard lossy compression algorithms nor recently developed model compression algorithms (Han et al., 2015a; Courbariaux et al., 2015; Hong et al., 2016; Leng et al., 2018; Lin et al., 2016) directly work in our setting. Standard lossy compres- sion algorithms aim to minimize reconstruction error but our end goal is to enable a learning algorithm to “quickly recover.” Model compression techniques aim to transform a (static) model into a simpler one while ensuring the fore- casts are not perturbed much whereas in our setting we need a reliable coding scheme that functions well throughout the arXiv:2009.13003v1  [cs.LG]  28 Sep 2020On Efﬁcient Constructions of Checkpoints entire dynamic process of learning, which is an orthogo- nal and perhaps more challenging goal. In addition, our algorithm must be efﬁcient and scalable so that it can be executed frequently. Our solution. To achieve our aims, we focus on a delta- encoding scheme (Mogul et al., 1997), tracking only the information on the difference between two checkpoints. Un- der this scheme, we examine whether we can cut the least useful information (with respect to training) from the model state, and ensure that the remaining information is amenable for compression. A perhaps surprising message here is that ℓ2-norm reconstruction error for the “delta” appears to be an ineffective metric for minimizing the recovery time. In- stead, our algorithm ﬁrst removes all the parameters with inconsequential updates, and then quantizes the remain- der information. These strategies resemble those used in distributed training with the goal of minimizing communi- cation cost (Alistarh et al., 2017). After we obtain the most signiﬁcant information for portion of parameter updates, we represent them in suitable format and apply a Huffman cod- ing to further compress these bits, so that the compression rate can be at the information theoretic limit. This strategy resembles recent techniques for model compression (Han et al., 2015a; Wu et al., 2016; Park et al., 2017; Zhou et al., 2017; Rastegari et al., 2016). The contribution of this paper includes: • Proposal of a fundamental research question on com- pressing model states for training recovery. • Characterization of a family of compression schemes that can efﬁciently track the learning process, based on a stylized model we develop. • Design of a lossy coding scheme with high- compression rate that integrates both classical com- pression techniques and recent ones developed for dis- tributed learning and model compression. • Optimization of training systems that minimizes the overhead of producing checkpoints on the ﬂy. Our extensive evaluation demonstrates that by simultane- ously leveraging techniques from distributed training and model compression, our algorithm delivers a solution (called LC-Checkpoint, LC refers to Lossy Compression) with a compression rate of up to 28x and superior recovering time—achieving up to 5.77×recovery speedup over a state- of-the-art algorithm (SCAR). 2. Our approach We now describe our compression framework. We introduce a stylized model for the learning process to facilitate the analysis of the system design trade-off. Then we explain our design principles, determined by both the stylized model and our extensive experiments. Our model. A “high-dimensional” vector u ∈Rn repre- sents the model state. An iterative algorithm (e.g., stochastic gradient descent) is used to gradually move the model state vector u toward a local optimal point u∗. Let ut be the model state at the t-th round. In our stylized model, we assume ut performs a (drifted) random walk that converges to u∗. Speciﬁcally, we use the following process to model ui’s trajectory. LetL= ∥u0 −u∗∥. ut+1 = u∗ + η(ut −u∗) +ϵt, (1) where η and Ljointly model the convergence rate of the algorithm, and ϵt is a random noise component to reﬂect the stochastic nature of SGD. When ηis set to be a small constant, the model characterizes those algorithms that have linear convergence rate. When η= (1−1/L), this model characterizes those algorithms whose convergence rates are 1 −1/t(Boyd & Vandenberghe, 2004). While our model does not captures the detail of many SGD algorithms, be- cause different SGD algorithms have different convergence rate, designing a unifying model that highlights design trade- offs requires us to make simplifying assumptions. Our design principles. We next describe our design prin- ciples. P1. Minimize irritation to SGD. When we design lossy compression scheme, a portion of information is inevitably lost, causing performance degradation to a learning algo- rithm. We ﬁnd that we should not simply use ℓ2 recon- struction error to measure degradation of SGD. This can be best illustrated by the stylized model. For simplicity, let u∗ = 0, so ut+1 = ut −((1 −η)ut + ϵt). The delta term we want to compress is ((1 −η)ut + ϵt). When we use a lossy compression, it corresponds to adding an ad- ditional noise term that is a function of ut and ϵt. So with the compression scheme, the new learning process becomes ut+1 = ut −((1 −η)ut + ϵt + f(ut,ϵt)). Ob- serving that as long as I E[f(ut,ϵt) | ut,ϵt] = 0, and Var(f(ut,ϵt) | ut,ϵt) is dominated (smaller than) by Var(ϵt), then the convergence quality remains unchanged, by standard results from stochastic approximation (Lai, 2009; Kushner & Yin, 2003). There are many constructs that satisfy the expectation and variance constraints. Let us consider an example of keeping the most signiﬁcant bit of ((1 −η)ut + ϵt) by using stan- dard randomized rounding (Alistarh et al., 2017). Because of the nature of the rounding algorithm, the expectation is 0. In addition, because the most signiﬁcant bit is kept, the information loss in rounding will not be greater than ∥((1 −η)ut + ϵt) ∥2 = O(std(ϵt)) under a mild assump- tion that ϵt’s standard deviation also scales proportionally to ∥ut∥over time. Therefore, this rounding scheme does notOn Efﬁcient Constructions of Checkpoints affect the performance of the training algorithm. In general, the 1-bit encoding is a special case of quantization. A wide family of quantization schemes will satisfy the expectation and variance constraint. Our algorithm will explore this trade-off. Note also when we minimize ℓ2 reconstruction error, this corresponds to keeping top-kheaviest entries in ut+1 −ut. P2. Maximize redundancies in residual information. Our compression scheme also needs to ensure the information we keep exhibits large redundancy, as measured by entropy. This will enable us to use traditional coding schemes such as Huffman code to compress the data at the information theoretic limit. The interplay between P1 and P2 highlights the unique struc- ture of our compression problem. This can be best illustrated by a compression scheme called TOPN. This compression scheme keeps the largest elements in δt. We observe (i) while this scheme minimizes ℓ2 reconstruction error, it does not have superior recovery time. Many other compression schemes that possess the aforementioned properties recover equally fast, as suggested by our stylized model. (ii) It is dif- ﬁcult to perform compression for the TOPN scheme. TOPN scheme usually needs to track 10% of all the entries in δt to be effective. The overhead of tracking the locations of these elements is surprisingly high. This is because in part that the vector is not sufﬁciently sparse so sparse matrix representation does not help. Our solution, on the other hand, carefully complies P1 and circumvents the need to track the locations of the entries we keep and thus achieves signiﬁcantly higher compression rate. P3. Do not use random projections and/or sketches. No- tably, we discover that sketch-based randomized projection techniques (e.g., Woodruff et al. (2014)) harm the compres- sion. Roughly speaking, sketches compress information by projecting multiple numbers into one cell. While this could speed up query time, it only irritates the gradient de- scent algorithm in our setting. Consider a toy example in which ut ∈R2 and the optimal point u∗ = (0,10). Let ut = (5,5) be the current state so the gradient is along the direction (−1,1). When we apply sketches (say CountMin sketches), it collapses the direction (−1,1) into a single point 0. When we make a query, the gradients for both coordinates are incorrect. Sketches are more useful when the entries in the gradient vector are heterogeneous and queries need to be answered at “line rate” (e.g., do not slow down the training Ivkin et al. (2019)). Here, when a model needs to be recovered from a checkpoint, the job is less time-sensitive. Therefore, even we face heterogeneous pa- rameters, it is more effective to carefully disentangle crucial information from inconsequential ones than using arbitrary Algorithm 1 LC-CHECKPOINT-BASED SGD Input: u∗, u0, η 1: Initialize ˜u0 = u0. 2: for t= 1to T do 3: Update model state: ut = u∗ + η(ut−1 −u∗) +ϵ 4: Compute distance: δt = ut −˜ut−1 5: Quantize δt: ˜δt = QUANTIZE (δt) 6: Compress ˜δt by Huffman coding and save to disk 7: Update checkpoint state: ˜ut = ˜ut−1 + ˜δt 8: end for Output: uT, {˜δt |t∈[T]} random projections. 3. LC-Checkpoint-based SGD We now describe our solution LC-Checkpoint (LC refers to Lossy Compression). See Figure 1 for a working example and Algorithm 1 for a workﬂow. For simplicity, we assume that our system maintains a checkpoint ˜δt for each iteration. We slightly abuse δt to refer to both the compressed data and the real vector it represents. It is straightforward to downsample our operations to construct a checkpoint every k-iterations. Our solution consists of two major compo- nents. C1. Approximate tracking by delta-coding. At each step, our system maintains an approximation ˜ut of the ground-truth state. We simply set ˜ut = u0 + ∑ i≤t ˜δi, where u0 is the initial state of the model. Our system con- tinuously maintains and updates ˜ut at the background (line 7 in Algorithm 1). Our major compression task is to prop- erly track the “delta” between the approximate state and ground-truth. Speciﬁcally, the compression task for the t-th iteration is δt = ut −˜ut−1. See 3⃝in Figure 1. C2. Quantization and Huffman coding. This compo- nent compresses δt through two steps, Step 1. Two-stage quantization. We ﬁrst perform an exponent-based quantiza- tion, and then a priority promotion operation. This opera- tion intelligently drops inconsequential information between two consecutive states. Step 2. Lossless compression by Huffman. Finally, the quantized distance vector is further compressed using Huffman coding. One can see that to reconstruct the model state at iteration t from the checkpoints, we may simply compute ut = u0 + ∑t i=1 ˜δt. In what follows, Section 3.1 discusses C2 and Section 3.2 discusses additional system-level optimizations.On Efﬁcient Constructions of Checkpoints - 0.76 -0.48 0.2 0.07 0.18 0.49 0.14 0.39 0.82 0.09 0.070.76 0.82 e=-1,s=0 e=-2,s=0 e=-2,s=1 e=-3,s=0 e=-4,s=0 0.79 0.44 -0.48 0 0.79 -0.48 0 0 0 0.44 0 0.44 0.79 0+ 000 01 10 11 10 0 10 0111011100110 1.34 1.02 0.39 1.37 1.34 1.77 1.03 1.58 1.93 1.03 u0 ˜u0 ut−1 ˜ut−1 ut ˜ut … … … … 0.58 1.5 0.19 1.3 1.16 1.28 0.89 1.19 1.11 0.94 0.18 0.39 0.49 -0.48 0.14 0.2 0.09 1.37 1.02 0.19 1.3 1.16 1.72 0.89 1.63 1.9 0.94 ut ˜ut−1 ˜ut δt ˜δt Exponent-base Quantization Priority Promotion 0.79 0.44 -0.48 0.17 0.08 001 010 011 100 00 Huﬀman Encoding Checkpoint Saving ① ② ③ Figure 1.LC-Checkpoint overview. 3.1. Quantization and Huffman coding 3.1.1. T WO-STAGE QUANTIZATION LC-Checkpoint employs a novel two-stage pipeline to quan- tize δt, which consists of two main sub-steps: exponent- based quantization and priority promotion. Exponent-based Quantization. Recall that a ﬂoating point vis represented by v= (−1)s ×m×2e, where sis the sign, mis the mantissa, and eis the exponent. Recall that δt = ut −˜ut−1 ∈Rn is a high-dimensional vector we aim to encode. Our exponent-based quantization works as follows: ﬁrst, it partitions entries in δinto multiple buckets according to eand s, i.e., it assigns the elements with iden- tical exponents and signs to the same bucket. Our crucial observation from extensive experiments is that entries in ut usually drift towards the same direction, so δt typically have the same sign. Next, our algorithm represents each bucket by the average of maximum and minimum values in the bucket. Figure 1 2⃝shows an example, in which,δtis quantized into ﬁve buckets (marked with ﬁve different colors). All entries in each bucket are then represented by a unique value. Indexing kbuckets requires log2 kbits. Because δt consists of nﬂoating points, each of which usesb(e.g., b∈{32,64}) bits, the compression rate is r= nb nlog2 k+kb. For example, in Figure 1, δhas 10 elements (i.e., n= 10), each of which is represented by a single-precision ﬂoating point (i.e., b= 32). Thus, the original δhas nb, i.e., 320 bits in total. Exponent-based quantization uses 5 buckets (i.e., k= 5). Thus, after quantization, δhas (10×log 5+5×32 = 190) bits. Therefore, the compressing rate (r) is 1.68 (i.e., 320/190). It is critical to control the number of bucketskto achieve an optimal compression ratio. Fortunately, the exponent-based bucketing can control k≤29 for single-precision ﬂoating point elements, and control k ≤212 for double-precision. 1 Our evaluation results (Section 4.3) conﬁrm that usually k <25 sufﬁces. Figure 2(a) plots the distribution of all elements’ exponent parts in the last convolutional layer of AlexNet. Priority Promotion. We further improve the compres- sion ratio by limiting the number of buckets with a priority promotion approach. Our crucial observation is that when δt,i is excessively close to 0 (i.e., ˜ui,t−1 is close ui,t), it is more effective to batch the updates (i.e., do not update the i-th entry of δt until it becomes substantial). Note also this is conceptually different from minimizing construction er- rors. Minimizing construction errors corresponds to exactly keeping track of the heaviest entries in δt, whereas we both remove excessively small entries and quantize large entries 1Single-precision ﬂoating point numbers use 8 bits to store e, and together with a sign bit—that is why k ≤ 29. Similarly, double-precision numbers use 11 bits to store e.On Efﬁcient Constructions of Checkpoints −130 −1250 1 2 3 1e6 −30 −25 −20 −15 1e6 (a) Exponent distribution of δ. -127 -18 -17 -16 -15 -14 -13 -120.00 0.25 0.50 0.75 1.00 1.25 1.50 1e7 (b) Exponent distribution of ˜δ(3-bit promotion). Figure 2.The distribution of all elements’ exponent parts in the last convolutional layer of AlexNet. When eequals −127, the element value is 0. The x-axis denotes the exponent part value, and the y-axis indicates the count of elements with this value. (as done in the previous step). Speciﬁcally, we proposex-bit priority promotion. It keeps 2x −1 buckets with larger e only and merges the rest buckets into one with a unique value of 0. In other words, priority promotion updates ˜wi with a larger distance to wi with a higher priority. It limits the index of buckets within xbits. Figure 1 (Priority Promotion) uses 2-bit priority promotion to control the number of buckets under 4. It merges the green and purple buckets into a red one that is represented by a value 0. Indexing these buckets only needs 2 bits. Figure 2(b) gives a real example of 3-bit priority promotion for the last convolutional layer in AlexNet. 3.1.2. H UFFMAN CODING Finally, observing the number of elements in each bucket is highly non-uniform in most learning processes, we use Huffman coding (Van Leeuwen, 1976) to further compress the bucket. For example, Figure 2(a) plots the distribution of all elements’ exponent parts in the last convolutional layer of AlexNet. This distribution shows a skewed behavior, thus more suitable for Huffman coding. Our crucial ob- servation is that priority promotion further aggravates the skewness of this distribution (Figure 2(b)), thus marrying quantization with Huffman coding produces more than “sum of parts” beneﬁts. Our later evaluation validates it (Sec- tion 4.3). 3.2. System Optimizations LC-Checkpoint also comprises several novel system-level optimizations as follows: • Asynchronous Execution: Because only the ﬁrst step of LC-Checkpoint depends on the model state, the rest steps can run simultaneously with the next iteration of SGD computation. This asynchronous (non-blocking) execution signiﬁcantly reduces the checkpoint overhead, and mitigates the blocking of model execution. • Checkpoint Merging: To further reduce the recovery time, LC-Checkpoint employs a helper process to merge multiple checkpoints into super-step ones, periodically. In case of any system crash, LC-Checkpoint uses these super-step checkpoints for recovery. • Huffman Code Table Caching: The number of buckets may stay the same from one it- eration to another, speciﬁcally after priority promotion. Thus, it is possible to reuse the Huffman code table (with only a simple sort of buckets according to the number of entries in each bucket) among different iterations without any rebuilding. LC-Checkpoint comprises a lightweight cache to store the Huffman code table for each buckets count. 4. Experiments This section evaluates LC-Checkpoint on four typical ML applications with three benchmark datasets, and compares it with previous efforts (SCAR Qiao et al. (2018b) and a TOPN mechanism as mentioned in Section 2) on recovery (rework) cost, compression ratio, and execution overhead, demonstrating the superiority of LC-Checkpoint. 4.1. Methodology Evaluation Objective: This evaluation has four main ob- jectives: (1) comparing LC-Checkpoint’ recovery (rework) cost with previous work; (2) evaluating the compression beneﬁts brought by different approaches mentioned before; (3) speciﬁcally, validating the effectiveness of priority pro- motion; (4) conﬁrming that LC-Checkpoint incurs low over- head by an experiment case study. Our work is mainly compared with two state-of-the-art efforts: SCAR (Qiao et al., 2018b) and a TOPN mechanism. SCAR partitionsOn Efﬁcient Constructions of Checkpoints 5% 10% ckpt size 0 4 8 12rework cost SCAR TOPN LC (a) MLR on MNIST. 5% 10% ckpt size 0 4 8 12rework cost SCAR TOPN LC (b) LeNet on MNIST. 5% 10% ckpt size 0 4 8 12rework cost SCAR TOPN LC (c) AlexNet on MNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (d) MF on MovieLens. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (e) MLR on FashionMNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (f) LeNet on FashionMNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (g) AlexNet on FashionMNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (h) MF on Jester. Figure 3.Rework cost comparison among LC-Checkpoint, SCAR, and TOPN. The x-axis indicates the ratio of the compressed checkpoint size over the full checkpoint size. The y-axis shows the rework iterations. The error bars indicate 95% conﬁdence intervals, calculated by repeating each trial 50 times. the parameters and updates one partition in each iteration to reduce the checkpoint size. The TOPN mechanism only updates the parameters with the top-n largest distances to the previous iteration. The TOPN checkpoint is stored in a compressed sparse row (CSR) format. ML Applications and Datasets: LC-Checkpoint is evaluated on four typical ML applications: Multino- mial Logistic Regression ( MLR), LeNet-5 ( Lenet) (Le- Cun et al., 1998), AlexNet (Krizhevsky et al., 2012) and Matrix Factorization ( MF). The ﬁrst three applica- tions are trained on MNIST (LeCun et al., 1998) and FashionMNIST (Xiao et al., 2017) datasets. The last one, MF is trained on Jester (Goldberg et al., 2001) and MovieLens10M (Harper & Konstan, 2015). Platforms and Evaluation Conﬁgurations: Our experi- ments are conducted on a multi-core server with an Intel Xeon Gold 6138 Skylake CPU with 40 cores, each running at 2.0 GHz, and 192 GB DDR4 memory. The training is per- formed on a Tesla P100 GPU with 16GB High-bandwidth Memory (HBM). 4.2. Recovery/Rework Cost Comparison This section evaluates the recovery (or rework) cost of LC- Checkpoint, particularly comparing it to SCAR (Qiao et al., 2018b) and a TOPN mechanism2. 2Rework (or recovery) cost is deﬁned as the number of itera- tions from ˜ut to ut. All methods share the same SGD computation cost for each iteration. To evaluate their rework costs fairly, we use the same check- point size (update size) for all three methods. Two check- point sizes are tested: 5% and 10% of the full checkpoint size3. These checkpoint sizes can be set directly for SCAR and TOPN. However, LC-Checkpoint’s size is determined by the data distribution and thus changed dynamically. To address this issue, LC-Checkpoint employs 2-bit and 3-bit priority promotion that control its checkpoint size at 5% and 10%. Figure 4 reports more details of LC-Checkpoint’s checkpoint size information. Figure 3 compares the rework cost of three methods, SCAR, TOPN, and LC-Checkpoint, showing that LC-Checkpoint incurs the lowest rework cost for all ML applications and datasets among them. For the 5% checkpoint test case, LC-Checkpoint outperforms SCAR by 2.88×-5.77×, and TOPN by 2.17×-4.06×, respectively. With 10% checkpoint size, LC-Checkpoint outperforms SCAR by 1.9×-4.82×, and outperforms TOPN by 1.52×-2.17×, respectively. In addition, comparing two checkpoint sizes (5% v.s. 10%), LC-Checkpoint results in more stable rework cost as the checkpoint size decreasing. For example, decreasing the checkpoint size from 10% to 5%, LC-Checkpoint has a negligible rework cost increase on LeNet with MNIST (Figure 3(b)) and AlexNet (Figure 3(c), 3(g)). It does not have any rework cost change for other cases. In contrast, SCAR and TOPN increase 1.6×rework cost on average as the checkpoint size changing from 10% to 5%. 3Full checkpoint stores all model parameters after a speciﬁc iteration.On Efﬁcient Constructions of Checkpoints 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (a) MLR on MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (b) LeNet on MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (c) AlexNet on MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (d) MF on MovieLens. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (e) MLR on Fashion-MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (f) LeNet on Fashion-MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (g) AlexNet on Fashion-MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (h) MF on Jester. Figure 4.The compression ratio with different compression methods. The x-axis denotes the bits count used in priority promotion, and the y-axis is the ratio of the checkpoint size after compression over the one before compression. E, P, H denote “exponent-base quantization”, “priority promotion”, and “Huffman coding”, respectively. 4.3. LC-Checkpoint Compression Effect Breakdown This section evaluates and analyzes the compression ef- fect of different approaches mentioned before, exponent- base quantization (E), priority promotion (P), and Huffman coding (H). Figure 4 reports the compression ratios with 2-bit and 3-bit priority promotion. With all compression approaches, the ultimate checkpoint sizes (E+P+H) are all below 5% with 2-bits, and below 10% with 3-bits over the uncompressed full checkpoint, i.e., the compression rates are above 20×and 10×, respectively. Exponent-base quantization yields a compression ratio of 85% on average. It proves that the exponent parts of all pa- rameters in δspan across a small range of all values that can be represented by single precision ﬂoating-point. 15% also indicates that the bucket number k< 25, because the aver- age bucket number can be estimated as k= 2(32×15%=4.8), where 32 is the width of single precision ﬂoating-point. Pri- ority promotion brings 9.26% extra compression ratio on average for 2-bit and 6.23% for 3-bit. For most cases, pri- ority promotion with smaller bits yields more beneﬁts for Huffman coding except MF (Figure 4(d), 4(h)). This is because MF’s parameters are sparse, thus Huffman coding can reach a sufﬁcient compression ratio without aggressive priority promotion. Across all models (and datasets), Huff- man coding brings 2% extra compression ratio with 2-bits priority promotion, and 1.6% with 3-bits one on average. 4.4. The Effectiveness of Priority Promotion This section further discusses the effectiveness of priority promotion. It aims to prove that priority promotion is able to save the majority of high priority parameters. We prove it by showing the exponent buckets result in a larger impact on the model state when their represented unique values are further from 0 (i.e., eis larger). Assume δis calculated from one state uθ to another for m iterations. Then, δi m is created by setting the parameters in the i-th exponent bucket to 0. The ground truth is calculated as Vgt = L(uθ+ δm) where L(x) denotes the loss function. Then the relative error is calculated as: Ei m = Vgt −L(uθ + δi m)  2 Vgt (2) Figure 5 reports the result of MLR with m= 10n,n ∈[1,6]. Both datasets (MNIST and FashionMNIST) on varied m prove that the elements in the buckets with the top-n largest distance impact more on the model (denotes as a higher relative error when the bucket represented value is set to 0). In addition, it is possible to preserve all important buckets with only a small number of index bits. For example, using 2-bit priority promotion (4 buckets with the last bucket storing 0) can easily preserve the most important buckets, and using 3-bit (8 buckets) can preserve all effective buckets. This result explains why priority promotion can compress the checkpoint with negligible accuracy loss.On Efﬁcient Constructions of Checkpoints (a) MLR on MNIST. (b) MLR on FashionMNIST. Figure 5.Evaluation on the priority of each exponent bucket. The x-axis denotes the id of the exponent bucket that is deleted. The y-axis shows the relative error to the ground-truth. Figure 6.MF on MovieLens25M.The x-axis denotes the iteration and the y-axis is the model’s RMSE (Root Mean Square Error). 4.5. A Case Study on LC-Checkpoint’s Overhead This section evaluates LC-Checkpoint’s execution overhead and overall impact on the model execution using a case study, i.e., trainingMF on MovieLens25M (Harper & Kon- stan, 2015) dataset. Each iteration costs 91 seconds on av- erage. LC-Checkpoint employs 3-bit priority promotion, resulting in a checkpoint size below 10% (of the uncom- pressed full checkpoint size). Default approach creates a full checkpoint every 10 iterations. A failure is triggered at the 7-th iteration. Figure 6 reports the result. LC-Checkpoint only incurs one extra iteration than the normal execution without any failure to convergence, and saves 6 iterations compared to the full checkpoint method, i.e., saving 546 seconds execution time. LC-Checkpoint introduces only less than 4 seconds (i.e., around 4%) overhead for each iteration, which is negligible. 5. Related Work Fault-tolerance is a key fundamental support for ML sys- tems. Li et al. (Li et al., 2014) propose a runtime parameter replication approach for recovery. Tensorﬂow (Abadi et al., 2016) employs periodic checkpoint to save the model state. Other efforts like (Harlap et al., 2017; Qiao et al., 2018a) aim to support strong consistency semantics. In contrast, our work relaxes the consistency guarantee of checkpoint based on the self-correcting behavior of ML applications. With a set of lossy compression mechanisms, our work can afford high frequent checkpoints, resulting in low rework cost and ﬁne-grained model state recovery. Similarly, Qiao et al. (Qiao et al., 2018b) also propose a fault-tolerant solu- tion (SCAR in our evaluation) based on weak consistency by partially updating parameters. SCAR is potential to store re- dundant information during checkpointing according to our evaluation, and our work aims to eliminate such redundancy by selectively saving the distance between two states. Model compression has been proposed to reduce model storage space and accelerate model execution time, simulta- neously. Weight pruning and weight quantization are two important categories of model compression. Some popular weight pruning techniques closely related to our work are summarized as follows. Guo et al. (Guo et al., 2016) present a dynamic network surgery approach with on-the-ﬂy connection pruning to reducing the network complexity. Dai et al. (Dai et al., 2019) combine the growth and the pruning phases in training to generate compact DNN architectures. Han et al. (Han et al., 2015b) design Deep Compression, a model compression approach by combining pruning, quantization, and Huffman coding. Mao et al. (Mao et al., 2017) carefully explore the impact of varied pruning granularity on model accuracy and propose a coarse-grained weight pruning approach. All effort above aims to prune model weights without compromising accuracy. Different from them, our work eliminates the redundancy between two checkpoints and reduces the rework cost during recovery by designing a reliable coding scheme working throughout the entire dynamic process of learning. Weight quantization is also widely used for model compres- sion. BinaryConnect (Courbariaux et al., 2015) introduces the binary weight for replacing multiplication by addition and subtraction. Binarized Neural Networks (Courbariaux et al., 2016) also use binary weights and activations to ac- celerate computation. Park et al. (Park et al., 2017) propose a clustering method based on weighted entropy for weight quantization. Leng et al. (Leng et al., 2018) formulate quan- tization as an optimization problem and solve it by ADMM. Our approach also employs quantization to reduce the bits ofOn Efﬁcient Constructions of Checkpoints parameters by designing a novel exponent-based quantiza- tion technique. Moreover, our approach emphasizes ﬁltering the parameters with a new priority promotion method. 6. Conclusion and Future Work This paper presents LC-Checkpoint, the ﬁrst checkpoint scheme based on lossy compression to achieve the maximal compression rate and efﬁcient recovery simultaneously. It employs a novel two-stage quantization method consisting of exponent-based quantization and priority promotion to identify and store the most critical information for SGD to recover, and leverages Huffman coding to further ben- eﬁt from the non-uniform distribution of gradient scales. Our evaluation demonstrates that LC-Checkpoint achieves a compression rate up to 28×and recovery speedup up to 5.77×over the state-of-the-art algorithm (SCAR). In the future, we plan to generalize LC-Checkpoint by re- laxing the assumption of SGD and equipping it with the capability of selecting checkpoint compression rates dynam- ically according to model and data changes. Acknowledgements We thank the anonymous reviewers for their valuable com- ments. This work is supported in part by NSF grants CRII- 1755646, CRII-1755769, OAC-1835821, CNS-1813487 and CCF-1918757, a Google Faculty Research Award, and an AWS Machine Learning Research Award. References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. Alistarh, D., Grubic, D., Li, J., Tomioka, R., and V ojnovic, M. Qsgd: Communication-efﬁcient sgd via gradient quan- tization and encoding. In Advances in Neural Information Processing Systems, pp. 1709–1720, 2017. Baylor, D., Breck, E., Cheng, H.-T., Fiedel, N., Foo, C. Y ., Haque, Z., Haykal, S., Ispir, M., Jain, V ., Koc, L., et al. Tfx: A tensorﬂow-based production-scale machine learn- ing platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1387–1395, 2017. Boyd, S. and Vandenberghe, L. Convex optimization. Cam- bridge university press, 2004. Courbariaux, M., Bengio, Y ., and David, J.-P. Binarycon- nect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123–3131, 2015. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y . Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. Dai, X., Yin, H., and Jha, N. K. Nest: A neural network syn- thesis tool based on a grow-and-prune paradigm. IEEE Transactions on Computers, 68(10):1487–1497, 2019. Goldberg, K., Roeder, T., Gupta, D., and Perkins, C. Eigen- taste: A constant time collaborative ﬁltering algorithm. information retrieval, 4(2):133–151, 2001. Goodfellow, I., Bengio, Y ., and Courville, A.Deep learning. MIT press, 2016. Guo, Y ., Yao, A., and Chen, Y . Dynamic network surgery for efﬁcient dnns. In Advances in neural information processing systems, pp. 1379–1387, 2016. Han, S., Mao, H., and Dally, W. J. Deep compres- sion: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efﬁcient neural network. In Advances in neural information processing systems, pp. 1135–1143, 2015b. Harlap, A., Tumanov, A., Chung, A., Ganger, G. R., and Gibbons, P. B. Proteus: agile ml elasticity through tiered reliability in dynamic resource markets. InProceedings of the Twelfth European Conference on Computer Systems, pp. 589–604, 2017. Harper, F. M. and Konstan, J. A. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015. Hong, M., Luo, Z.-Q., and Razaviyayn, M. Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems. SIAM Journal on Optimization, 26(1):337–364, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Ivkin, N., Rothchild, D., Ullah, E., Stoica, I., Arora, R., et al. Communication-efﬁcient distributed sgd with sketching. In Advances in Neural Information Processing Systems, pp. 13144–13154, 2019.On Efﬁcient Constructions of Checkpoints Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems , pp. 1097–1105, 2012. Kushner, H. and Yin, G. G.Stochastic approximation and re- cursive algorithms and applications, volume 35. Springer Science & Business Media, 2003. Lai, T. L. Martingales in sequential analysis and time series, 1945–1985. Electronic Journal for history of probability and statistics, 5(1), 2009. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998. Leng, C., Dou, Z., Li, H., Zhu, S., and Jin, R. Extremely low bit neural network: Squeeze the last bit out with admm. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y . Scaling distributed machine learning with the parameter server. In 11th {USENIX}Symposium on Operating Systems Design and Implementation ( {OSDI}14), pp. 583–598, 2014. Lin, D., Talathi, S., and Annapureddy, S. Fixed point quan- tization of deep convolutional networks. In International Conference on Machine Learning, pp. 2849–2858, 2016. Low, Y ., Gonzalez, J., Kyrola, A., Bickson, D., Guestrin, C., and Hellerstein, J. M. Distributed graphlab: A frame- work for machine learning in the cloud. arXiv preprint arXiv:1204.6078, 2012. Mao, H., Han, S., Pool, J., Li, W., Liu, X., Wang, Y ., and Dally, W. J. Exploring the regularity of sparse struc- ture in convolutional neural networks. arXiv preprint arXiv:1705.08922, 2017. Mogul, J. C., Douglis, F., Feldmann, A., and Krishnamurthy, B. Potential beneﬁts of delta encoding and data compres- sion for http. In Proceedings of the ACM SIGCOMM’97 conference on Applications, technologies, architectures, and protocols for computer communication, pp. 181–194, 1997. Park, E., Ahn, J., and Yoo, S. Weighted-entropy-based quantization for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5456–5464, 2017. Qiao, A., Aghayev, A., Yu, W., Chen, H., Ho, Q., Gib- son, G. A., and Xing, E. P. Litz: Elastic frame- work for high-performance distributed machine learn- ing. In 2018 {USENIX}Annual Technical Conference ({USENIX}{ATC}18), pp. 631–644, 2018a. Qiao, A., Aragam, B., Zhang, B., and Xing, E. P. Fault tolerance in iterative-convergent machine learning. arXiv preprint arXiv:1810.07354, 2018b. Rastegari, M., Ordonez, V ., Redmon, J., and Farhadi, A. Xnor-net: Imagenet classiﬁcation using binary convo- lutional neural networks. In European conference on computer vision, pp. 525–542. Springer, 2016. Van Leeuwen, J. On the construction of huffman trees. In ICALP, pp. 382–410, 1976. Woodruff, D. P. et al. Sketching as a tool for numerical linear algebra. Foundations and TrendsR⃝in Theoretical Computer Science, 10(1–2):1–157, 2014. Wu, J., Leng, C., Wang, Y ., Hu, Q., and Cheng, J. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4820–4828, 2016. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. Zhou, A., Yao, A., Guo, Y ., Xu, L., and Chen, Y . Incre- mental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.",
      "meta_data": {
        "arxiv_id": "2009.13003v1",
        "authors": [
          "Yu Chen",
          "Zhenming Liu",
          "Bin Ren",
          "Xin Jin"
        ],
        "published_date": "2020-09-28T01:20:15Z",
        "venue": "International Conference on Machine Learning, 2020",
        "pdf_url": "https://arxiv.org/pdf/2009.13003v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes LC-Checkpoint, a novel lossy compression scheme for efficient construction of checkpoints in deep learning models. It addresses the critical problem of compressing large model states to maximize compression rate and optimize recovery speed, specifically for models trained with SGD. LC-Checkpoint achieves this by using a delta-encoding scheme, intelligently quantizing the most significant parameter updates (through exponent-based quantization and priority promotion), and further compressing these updates using Huffman coding. The research introduces a fundamental question on compressing model states for training recovery, characterizes a family of efficient compression schemes for tracking the learning process, and designs a high-compression rate coding scheme that integrates classical compression with techniques from distributed learning and model compression. It also optimizes training systems to minimize the overhead of producing checkpoints. The scheme delivers up to 28x compression rate and 5.77x recovery speedup over the state-of-the-art SCAR algorithm.",
        "methodology": "The LC-Checkpoint framework operates on a delta-encoding scheme, continuously maintaining an approximation of the true model state and compressing the 'delta' (difference) between the true and approximate states. The core compression is a two-stage quantization process: (1) Exponent-based Quantization, which buckets entries in the delta vector based on their exponent and sign, representing each bucket by a unique average value. This leverages the observation that parameter updates often drift in similar directions. (2) Priority Promotion, which further enhances compression by limiting the number of buckets; it merges inconsequential (near-zero) updates into a single '0' value, preserving only a prioritized set of `2^x - 1` buckets with larger exponent values. The resulting quantized delta vector is then subjected to Huffman coding for lossless compression, exploiting the non-uniform distribution of elements across buckets. Key design principles include minimizing perturbation to SGD's convergence, maximizing redundancy in retained information for efficient coding, and deliberately avoiding random projections or sketches due to their potential to harm gradient descent. System optimizations comprise asynchronous execution of compression steps to reduce overhead, periodic checkpoint merging into 'super-step' checkpoints for faster recovery, and caching of Huffman code tables to prevent rebuilding.",
        "experimental_setup": "LC-Checkpoint was evaluated on four representative machine learning applications: Multinomial Logistic Regression (MLR), LeNet-5, AlexNet, and Matrix Factorization (MF). These applications were trained using datasets including MNIST, FashionMNIST, Jester, MovieLens10M, and MovieLens25M (for a case study). The proposed method was compared against two state-of-the-art algorithms: SCAR (a fault-tolerant solution based on partial parameter updates) and a TOPN mechanism (which updates only top-n largest parameter distances, stored in CSR format). Experiments were conducted on a multi-core server featuring an Intel Xeon Gold 6138 Skylake CPU (40 cores, 2.0 GHz) and 192 GB DDR4 memory, with training performed on a Tesla P100 GPU with 16GB HBM. Evaluation objectives included comparing recovery (rework) cost, analyzing compression ratios of its components (exponent-based quantization, priority promotion, Huffman coding), validating priority promotion's effectiveness via relative error measurements, and assessing execution overhead. Checkpoint sizes for comparison were set at 5% and 10% of the full checkpoint size, with LC-Checkpoint dynamically adjusting its size through 2-bit and 3-bit priority promotion to match these targets.",
        "limitations": "The LC-Checkpoint scheme operates under the explicit assumption that Stochastic Gradient Descent (SGD) is used to train the deep learning models. The stylized model used to facilitate system design analysis makes simplifying assumptions and does not capture the detailed convergence behaviors of many diverse SGD algorithms, as different algorithms inherently have varying convergence rates.",
        "future_research_directions": "Future research will focus on extending LC-Checkpoint by relaxing its current dependency on Stochastic Gradient Descent (SGD) as the training optimizer. Additionally, the authors plan to enhance the system with the capability to dynamically select checkpoint compression rates, adapting to changes in the model and data characteristics during training."
      }
    },
    {
      "title": "The State of Sparse Training in Deep Reinforcement Learning",
      "abstract": "The use of sparse neural networks has seen rapid growth in recent years,\nparticularly in computer vision. Their appeal stems largely from the reduced\nnumber of parameters required to train and store, as well as in an increase in\nlearning efficiency. Somewhat surprisingly, there have been very few efforts\nexploring their use in Deep Reinforcement Learning (DRL). In this work we\nperform a systematic investigation into applying a number of existing sparse\ntraining techniques on a variety of DRL agents and environments. Our results\ncorroborate the findings from sparse training in the computer vision domain -\nsparse networks perform better than dense networks for the same parameter count\n- in the DRL domain. We provide detailed analyses on how the various components\nin DRL are affected by the use of sparse networks and conclude by suggesting\npromising avenues for improving the effectiveness of sparse training methods,\nas well as for advancing their use in DRL.",
      "full_text": "The State of Sparse Training in Deep Reinforcement Learning Laura Graesser* 1 2 Utku Evci* 2 Erich Elsen3 Pablo Samuel Castro2 Abstract The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the re- duced number of parameters required to train and store, as well as in an increase in learning efﬁ- ciency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Re- inforcement Learning (DRL). In this work we perform a systematic investigation into applying a number of existing sparse training techniques on a variety of DRL agents and environments. Our results corroborate the ﬁndings from sparse train- ing in the computer vision domain – sparse net- works perform better than dense networks for the same parameter count – in the DRL domain. We provide detailed analyses on how the various com- ponents in DRL are affected by the use of sparse networks and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL1. 1. Introduction Deep neural networks are typically organized as a stack of layers. Each layer consists of multiple neurons, where each neuron is connected to all neurons in the next layer; this is often referred to as a dense network. Alternatively, each neuron can be wired to a subset of the neurons in the next layer, resulting in a sparse, and smaller, network. Such sparse neural networks have been shown to match the performance of their dense counterparts while requiring only 10%-to-20% of the connections in most cases (Han et al., 2015; Gale et al., 2019; Blalock et al., 2020) providing *Equal contribution 1Robotics at Google 2Google Research, Canada 3Adept AI. Correspondence to: Laura Graesser <laura- graesser@google.com>, Utku Evci <evcu@google.com>, Pablo Samuel Castro <psc@google.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl signiﬁcant memory, storage, and latency gains. Deep networks have become a mainstay of scalable rein- forcement learning (RL), key to recent successes such as playing – at superhuman levels – Atari games (Mnih et al., 2015), Go (Silver et al., 2016), Dota 2 (Berner et al., 2019) and as well as controlling complex dynamical systems such as stratospheric balloons (Bellemare et al., 2020) and plasma in real-time (Degrave et al., 2022). Despite their importance, most deep reinforcement learning (DRL) research focuses on improving the algorithmic aspect of DRL, and less on the architecture aspect. Sparse networks in particular have received very little attention, likely due to the belief that net- work over-parameterization helps with learning. However, recent work suggests that RL agents may suffer from im- plicit under-parameterization when training deep networks with gradient descent (Kumar et al., 2021), suggesting that the network’s expressivity is in fact underused. In addition to this, Nikishin et al. (2022) suggests deep RL agents may have a tendency to overﬁt to early training data. Given this, one might expect there is substantial opportunity to com- press RL agents. Further, sparse networks might beneﬁt DRL by reducing the cost of training or aid running them in latency-constrained settings such as controlling plasma (Degrave et al., 2022). One limitation of current research on training sparse neural networks is that it almost solely focuses on image classiﬁca- tion benchmarks (Blalock et al., 2020; Hoeﬂer et al., 2021) creating the risk of over-ﬁtting to a speciﬁc domain. Do ad- vances observed in computer vision (CV) transfer to DRL? A few recent works (Sokar et al., 2021; Arnob et al., 2021) attempt to address this by applying individual sparse train- ing algorithms to DRL agents. However, it is still unknown if the key observation made in CV , thatsparse models per- form better than dense ones for the same parameter count, transfers to DRL. In this work we focus on answering that question and sys- tematically explore the effectiveness of different sparse learning algorithms in the online DRL setting. In order to achieve this, we benchmark four different sparse training algorithms using value-based (DQN (Mnih et al., 2015)) and actor-critic, (SAC (Haarnoja et al., 2018) and PPO (Schul- man et al., 2017)) agents. Our results also include a broad analysis of various components that play a role in the train- arXiv:2206.10369v1  [cs.LG]  17 Jun 2022The State of Sparse Training in Deep Reinforcement Learning ing of these sparse networks: sparsity distribution strategies, weight decay, layer initialization, signal-to-noise ratio for gradients, as well as batch size, topology update strategy and frequency. We summarize our main ﬁndings below: • In almost all cases, sparse neural networks perform bet- ter than their dense counterparts for a given parameter count, demonstrating their potential for DRL. • It is possible to train up to 80 - 90% sparse networks with minimal loss in performance compared to the standard dense networks. • Pruning often obtains the best results, and dynamic sparse training improves over static sparse training sig- niﬁcantly. However gradient based growth (Evci et al., 2020) seems to have a limited effect on performance. We argue this is due to low signal-to-noise ratio in gradients. • The distribution of parameters among the actor and critic networks, as well as among different layers, im- pact training greatly. We observe that the best perfor- mance is obtained by allocating the majority of parame- ters to the critic network and using Erdos Renyi Kernel (ERK) sparsity distributions. • We observe robust performance over various hyper- parameter variations. Somewhat surprisingly, when adding noise to the observations, sparse methods achieve better robustness in most cases. 2. Background 2.1. Sparse training Removing connections from neural networks was suggested at least as early as Mozer & Smolensky (1989), which coined the name “Skeleton” networks for what we today call sparse networks. Techniques for ﬁnding sparse neural networks can be grouped under two main categories. (1) Dense-to-sparse training approaches (Han et al., 2016; Molchanov et al., 2017; Wortsman et al., 2019; Kusu- pati et al., 2020; Peste et al., 2021) start with a dense neural network and gradually reduce the network size by pruning its weights. This approach often achieves state-of-the-art performance amongst sparse networks, however it requires the same (or more) computation as training a large dense net- work. An alternative to pruning is (2) sparse training (Mocanu et al., 2018). This family of methods sparsiﬁes the network at initialization and maintains this sparsity through- out training, thus reducing the training cost proportional to the sparsity of the network. However, training sparse neural networks from scratch is known to be difﬁcult, leading to sub-optimal solutions (Frankle & Carbin, 2019; Liu et al., 2019; Evci et al., 2019). DRL training is notoriously resource hungry, hence we fo- cus on the second family of methods (i.e. sparse training) in this work. There are various approaches to sparse training. One line of work (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020), attempts to prune a dense networkimmediately on iteration 0. The resulting networks are used as an initial- ization for sparse training and kept ﬁxed throughout. These techniques have been shown to have marginal gains over random pruning (Frankle et al., 2020), especially when used in modern training pipelines. Furthermore they may not generalize well in the RL setting as the non-stationarity of the data make it less clear that any decision made at iteration 0 will remain optimal throughout training. Another line of work starts with randomly initialized sparse neural networks (both weights and masks) and focuses on improving sparse training by changing the sparse connec- tivity among neurons (Mocanu et al., 2018; Bellec et al., 2018) throughout the optimization. Known as Dynamic sparse training (DST), such approaches have been shown to match pruning results, making it possible to train sparse net- works efﬁciently without sacriﬁcing performance (Dettmers & Zettlemoyer, 2019; Evci et al., 2020). In this work we benchmark one dense-to-sparse and three sparse training methods, which we brieﬂy describe below: Pruning (Zhu & Gupta, 2018):uses a simple procedure to slowly make a dense network sparse over the course of one training run using weight magnitudes. We start pruning the network from 20% of the training steps and stop when we reach 80%, keeping the ﬁnal sparse network ﬁxed for the remaining of the training. This simple pruning algorithm is shown to exceed or match more complex pruning algorithms (Gale et al., 2019). Despite the fact it requires the same order of magnitude resources as training a dense network, we included this method since it serves as an upper bound on the sparse training performance. Static: prunes a given dense network randomly at initial- ization and the resulting sparse network is trained with a ﬁxed structure. This is an important baseline to show the effectiveness of DST algorithms explained below. Sparse Evolutionary Training (SET) (Mocanu et al., 2018): Similar to Static, SET starts training with a random sparse network. During training, a portion of the connec- tions are changed every N steps (the update interval) by replacing the lowest magnitude connections with new ran- dom ones. The fraction (drop fraction) of updated weights are decayed over the course of training to help the network converge to a minima. We use cosine decay as proposed by Dettmers & Zettlemoyer (2019). Rigged Lottery (RigL) (Evci et al., 2020):is the same as SET, except the new connections are activated using the gradient signal (highest magnitude) instead of at random.The State of Sparse Training in Deep Reinforcement Learning This criteria has been shown to improve results signiﬁcantly in image classiﬁcation and with enough training iterations matches or exceed accuracies obtained by pruning. 2.2. Reinforcement learning Reinforcement learning (RL) aims to design learning algo- rithms for solving sequential decision-making problems. Typically these are framed as an agent interacting with an environment at discrete time-steps by making action choices from a set of possible agent states; the environ- ment in turn responds to the action selection by (possibly) changing the agent’s state and/or providing a numerical reward (or cost); the agent’s objective is to ﬁnd a pol- icy mapping states to actions so as to maximize (mini- mize) the sum of rewards (costs). This is formalized as a Markov decision process (Puterman, 1994) deﬁned as a tuple ⟨X,A,P,R,γ⟩, where X is the state space, A is the action space, P : X ×A →∆(X) deﬁnes the transition dynamics2, R : X ×A →R is the reward function, and γ ∈[0,1) is a discount factor. A policy π : X →∆(A) formalizes an agent’s behaviour and induces a value func- tion Vπ : X →R deﬁned via the well-known Bellman recurrence: Vπ(x) := Ea∼π(x) [ R(x,a) + γEx′∼P(x,a)Vπ(x′) ] (1) It is convenient to deﬁne state-action value functions Qπ : X×A →R as: Qπ(x,a) := R(x,a)+γEx′∼P(x,a)Vπ(x′). The goal of an RL agent is to ﬁnd a policy π∗:= maxπVπ (which is guaranteed to exist); for notational convenience we denote V∗:= Vπ∗ and Q∗:= Qπ∗ . In online RL the agent achieves this by iteratively improving an initial policy π0: {π0,π1,··· ,πt,···} and using these intermediate policies to collect new experience from the environment in the form of transitions (x,a,r,x ′), where a ∼πt(x), r = R(x,a), and x′∼P(x,a). These transitions constitute the dataset the agent uses to improve its policies. In other words, the learning proocess is a type of closed feedback loop : an agent’s policy directly affects the data gathered from the environment, which in turn directly affects how the agent updates its policy. When X is very large, it is impractical to store Vπ and Qπ in a table, so a function approximator Vθ ≈Vπ (where θare the approximator’s parameters) is employed instead. This function approximator is usually one or more deep networks, and this type of RL is known as deep RL (DRL). DRL algorithms can be broadly categorized into two groups: Value-based: The function Qπ is approximated by a deep network Qθ. The policy is directly induced from the value 2∆(X) denotes the set of probability distributions over a ﬁnite set X. estimate via πt(x) = arg maxa∈A Qθt (x,a)3. The parame- ters θare trained using a temporal difference loss (based on Equation 1) from transitions sampled from D: L(θ) = E(x,a,r,x′)∼D [ Qθ(x,a) −(r+ γmax a′∈A Q¯θ(x′,a′)) ] (2) Here, ¯θs a copy of θthat is infrequently synced with θfor more stable training (Mnih et al., 2015). These methods are typically employed for discrete control environments, where there is a ﬁnite (and relatively small) set of actions (e.g. Atari games (Bellemare et al., 2013)). Policy-gradient: In contrast to value-based methods where the policy is implicitly improved by virtue of improving Qθ, policy-gradient methods maintain and directly improve upon a policy πψ parameterized by ψ. These methods typi- cally still make use of a value estimate Qθ as part of their learning process, and are thus often referred to as actor-critic methods (where πψ is the actor and Qθ the critic). Two po- tential advantages of these methods is that they can be more forgiving of errors in the Qθ estimates, and they can handle continuous action spaces (for instance, by having πψ(x) output mean and variance parameters from which actions may be sampled). These methods are typically employed for continuous control environments, where the action space is continuous (e.g. MuJoCo (Todorov et al., 2012)). 3. Experimental setup DRL algorithms We investigate both value-based and policy-gradient methods. We chose DQN (Mnih et al., 2015) as the value-based algorithm, as it is the algorithm that ﬁrst spurred the ﬁeld of DRL, and has thus been extensively stud- ied and extended. We chose two actor-critic algorithms for our investigations: an on-policy algorithm (PPO (Schulman et al., 2017)) and an off-policy one (SAC (Haarnoja et al., 2018)); both are generally considered to be state-of-the-art for many domains. Environments For discrete-control we focus on three classic control environments (CartPole, Acrobot, and Moun- tainCar) as well as 15 games from the ALE Atari suite (Bellemare et al., 2013) (see subsection A.4 for game selec- tion details). For continuous-control we use ﬁve environ- ments of varying difﬁculty from the MuJoCo suite (Todorov et al., 2012) (HalfCheetah, Hopper, Walker2d, Ant, and Humanoid). Rewards obtained by DRL algorithms have notoriously high variance (Agarwal et al., 2021). Therefore we repeat each experiment with at least 10 different seeds 3Although there are other mechanisms for deﬁning a policy, such as using a softmax, and there are exploration strategies to consider, we present only the argmax setup for simplicity, as the other variants are mostly orthogonal to our analyses.The State of Sparse Training in Deep Reinforcement Learning 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score Figure 1.IQM plots for networks at 90% sparsity for various architecture and algorithm combinations. SAC and PPO are averaged over 5 MuJoCo environments, whereas DQN is averaged over 15 Atari environments. Results at different sparsities can be found at Appendix E. ”Dense: 100%” corresponds to the standard dense model. Atari scores were normalized using human performance per game. MuJoCo scores were normalized using the average returns obtained by the Dense: 100% SAC agent per game. and report the average reward obtained over the last 10% of evaluations. We also provide 95% conﬁdence intervals in all plots. See subsection A.1 for additional details. Training For each sparse training algorithm considered ( Pruning , Static, RigL, and SET) we train poli- cies ranging between 50% to 99% sparsity. To ensure a fair comparison between algorithms, we performed a hy- per parameter sweep for each algorithm separately. The exception is DQN experiments on Atari for which it was too computationally expensive to do a full hyper-parameter sweep and we used values found in previous experiments instead. Sparse results in these environments may therefore be conservative compared to the well tuned dense baseline. In addition to training the standard dense networks used in the literature, we also train smaller dense networks by scaling down layer widths to approximately match the pa- rameter counts of the sparse networks, thereby providing a ”parameter-equivalent” dense baseline. We share details of the hyper parameter sweeps and hyper parameters used for each algorithm in subsection A.3. Code Our code is built upon the TF-Agents (Guadarrama et al., 2018), Dopamine (Castro et al., 2018), and RigL (Evci et al., 2020) codebases. We use rliable (Agarwal et al., 2021) to calculate the interquartile mean (IQM) and plot the results. The IQM is calculated by discarding the bottom and top 25% of normalized scores aggregated from multiple runs and environments, then calculating the mean (reported with 95% conﬁdence intervals) over the remaining 50% runs. Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl 4. The State of Sparse Networks in Deep RL We begin by presenting the outcome of our analyses in Figure 1 and Figure 2 for DQN (Atari), PPO and SAC (Mu- JoCo). Figure 1 presents the IQM at 90% sparsity, whilst in Figure 2 we evaluate ﬁnal performance relative to the number of parameters. We share results for classic control, 2 additional MuJoCo and 12 additional Atari environments in Appendix B. Three main conclusions emerge; (1) In most cases performance obtained by sparse networks signiﬁcantly exceeds that of their dense counterparts with a comparable number of parameters. Critically, in more difﬁcult environ- ments requiring larger networks (e.g. Humanoid, Atari), sparse networks can be obtained with efﬁcient sparse train- ing methods. (2) It is possible to train sparse networks with up to 80-90% fewer parameters and without loss in perfor- mance compared to the standard dense model. (3) Gradient based growing (i.e. RigL) seems to have limited impact on the performance of sparse networks. Next, we discuss each of these points in detail. Sparse networks perform better.Inline with previous ob- servations made in speech (Kalchbrenner et al., 2018), natu-The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.700.800.90 0.950.96 0.97 0.98 SAC / Walker2d-v2 104 105 #Params 0 2000 4000 6000 8000 10000 12000 0.500.700.800.90 0.950.96 0.97 0.98 SAC / HalfCheetah-v2 104 105 #Params 1000 2000 3000 4000 5000 0.500.700.800.90 0.95 0.96 0.970.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.98 0.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.70 0.800.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 500 1000 1500 2000 2500 0.50 0.70 0.80 0.90 0.950.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 2.Comparison of the ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (CNN) (row-2) SAC on MuJoCo, (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% (annotated on the pruning curve) for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals. See Appendix B for results on additional environments. 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 1000 2000 3000 4000 5000Reward Walker2d-v2 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 2000 4000 6000 8000 10000 12000 HalfCheetah-v2 algorithm dense rigl % full dense params 100 20 10 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 1000 2000 3000 4000 5000 Humanoid-v2 Figure 3.Evaluating how varying the actor-critic parameter ratio affects performance for a given parameter budget on different environment with policies trained using SAC. % of parameters allocated to the actor network is reported on the x axis. In SAC the parameter count of both critic networks is summed to give the overall critic parameter count. The vertical line corresponds the the standard parameter split and the horizontal line to the full dense training reward.The State of Sparse Training in Deep Reinforcement Learning ral language modelling (Li et al., 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse net- works found by pruning achieve signiﬁcantly higher rewards than the dense baseline. However training these sparse net- works from scratch ( static) performs poorly. DST algo- rithms (RigL and SET) improve over static signiﬁcantly, however often fall short of matching the pruning perfor- mance. Critically, we observe that for more difﬁcult environment requiring larger networks such as Humanoid, MsPacman, Qbert and Pong, sparse networks found by efﬁcient DST algorithms exceed the performance of the dense baseline. How sparse?Next we asked how much sparsity is possible without loss in performance relative to that of the standard dense model (denoted by Dense:100% in Figure 1 and by the horizontal lines in Figure 2). We ﬁnd that on average DST algorithms maintain performance up to 90% sparsity using SAC (Figure 1 top left) or DQN ( Figure 1 bottom row), after which performance drops. However performance is variable. For example, DST algo- rithms maintain performance especially well in MsPacman and Humanoid. Whereas in Qbert none of the methods are able to match the performance of the standard dense model at any of the examined levels of sparsity. In the Atari environments, training a ResNet (He et al., 2015) following the architecture from Espeholt et al. (2018) instead of the standard CNN alone provided about 3x im- provement in IQM scores. We were also surprised to see that pruning at 90% sparsity exceeds the performance of the standard ResNet model. These observations indicate that while sparse training can bring very signiﬁcant efﬁciency gains in some environments, it is not a guaranteed beneﬁt. Unlike supervised learning, expected gains likely depend on both task and network, and merits further inquiry. RigL and SET:For most sparsities (50% - 95%) we ob- serve little difference between these two sparse training algo- rithms. At very high sparsities, RigL may outperform SET. The difference can be large (e.g. MsPacman), but is more often moderate (e.g. Pong) or negligible (e.g. Humanoid, Qbert) with overlapping conﬁdence intervals. This suggests that the gradient signal used by RigL may be less informa- tive in the DRL setting compared to image classiﬁcation, where it obtains state-of-the-art performance and consis- tently outperforms SET. Understanding this phenomenon could be a promising direction for improving sparse training methods for DRL. Perhaps unsurprisingly, the clarity of the differences be- tween sparse and dense training is affected by the stability of the underlying RL algorithm. Our results using SAC, designed for stability, were the clearest, as were the DQN results. In contrast, our results using PPO which has much higher variance, were less stark. For this reason, we used SAC and DQN when studying the different aspects of sparse agents in the rest of this work. 5. Where should sparsity be distributed? When searching for efﬁcient network architectures for DRL it is natural to ask where sparsity is best allocated. To that end, we consider both how to distribute parameters between network types and as well as within them. Actor or Critic? Although in value-based agents such as DQN there is a single network, in actor-critic methods such as PPO and SAC there are at least two: an actor and a critic network. It is believed that the underlying functions these networks approximate (e.g. a value function vs. a policy) may have signiﬁcantly different levels of complexity, and this complexity likely varies across environments. Actor and critic typically have near-identical network architectures. However, for a given parameter budget it is not clear that this is the best strategy, as the complexity of the functions being approximated may vary signiﬁcantly. We thus seek to understand how performance changes as the parameter ratio between the actor and critic is varied for a given parameter budget. In Figure 3 we assess three parameter budgets: 100%, 20% and 10% of the standard dense parameter count, and two training regimes, dense and sparse. Given the observed similarity in performance between RigL and SET in Figure 2, we selected one method, RigL, for this analysis. We observe that assigning a low proportion of parameters to the critic (10 - 20%) incurs a high performance cost across all regimes. When parameters are more scarce, in 20% and 10% of standard dense settings, performance degradation is highest. This effect is not symmetric. Reducing the actor parameters to just 10% rarely affects performance compared to the default actor-critic split of 34:66 (vertical line). Interestingly the default split appears well tuned, achieving the best performance in most settings. However in the more challenging Humanoid environment we see that for smaller dense networks, reducing the actor parameters to just 10% yields the best performance. Sparse networks follow a simi- lar trend, but we notice that they appear to be more sensitive to the parameter ratio, especially at higher sparsities. Overall this suggests that the value function is the more complex function to approximate in these settings, bene- ﬁting from the lion’s share of parameters. It also suggests that tuning the parameter ratio may improve performance. Furthermore, FLOPs at evaluation time is determined only by the actor network. Since the actor appears to be easier to compress, this suggests large potential FLOPs savings for real-time usage of these agents. Finally, this approachThe State of Sparse Training in Deep Reinforcement Learning 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0 10 6  10 5  10 4  10 3  10 2 Weight Decay 0 1000 2000 3000 4000 5000 6000Reward Walker2d Dense Pruning Rigl Set Static Static Rigl Set Sparsity-aware Initiliazation 0 1000 2000 3000 4000 5000 6000Reward Walker-2d False (ERK) True (ERK) False (Uniform) True (Uniform) Figure 4.Sensitivity analysis on policies trained with SAC: (left) uniform vs ERK sparsity distributions, (center) weight decay, (right) sparsity-aware vs dense initialization. We use 80% (ERK) sparse networks in all plots unless noted otherwise. Plots for the remaining hyper-parameters are shared in Appendix D. could be used to better understand the relative complexity of policies and values functions across different environments. Within network sparsity In Figure 4 (left) we turn our attention to the question of distributing parameters within networks and compare two strategies; uniform and ERK (Evci et al., 2020). Given a target sparsity of, say 90%, uniform achieves this by making each layer 90% sparse; ERK distributes them proportional to the sum of its dimen- sion, which has the effect of making large layers relatively more sparse than the smaller ones. Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.2 for further discussion). On the other hand, ERK has no effect on the FLOPs count of fully connected networks used in MuJoCo environments. Our results show that ERK sig- niﬁcantly improves performance over uniform sparsity and thus we use ERK distribution in all of our experiments and share results with uniform distribution in Appendix C. We hypothesize the advantage of ERK is because it leaves input and output layers relatively more dense, since they typ- ically have few incoming our outgoing connections, and this enables the network to make better use of (a) the observation and (b) the learned representations at the highest layers in the network. It is interesting to observe that maintaining a dense output layer is one of the key design decisions made by Sokar et al. (2021) for their proposed algorithm. 6. Sensitivity analysis In this section we assess the sensitivity of some key hyper- parameters for sparse training. Plots and commentary for the remaining hyper-parameters (drop fraction, topology update interval, and batch size) are shared in Appendix D. Weight decay In Figure 4 (center) we evaluate the effect of weight decay and ﬁnd that a small amount of weight decay is beneﬁcial for pruning, RigL, and SET. This is to be expected since network topology choices are made based on weight magnitude, although we do note that the improvements are quite minor. Surprisingly weight decay seems to help dense even though it is not often used in DRL. Findings: We recommend using small weight decay. Sparsity-aware initialization In Figure 4 (right) we eval- uate the effect of adjusting layer weight initialization based on a layer’s sparsity on static, RigL and SET. A common approach to initialization is to scale a weight’s initializa- tion inversely by the square root of the number of incoming connections. Consequently, when we drop incoming con- nections, the initialization distribution should be scaled pro- portionately to the number of incoming connections (Evci et al., 2022). Figure 4 (right) shows that this sparsity-aware initialization consistently improves performance when us- ing uniform distribution over layer sparsities. However the difference disappears when using ERK for RigL and SET and may even harm performance for static. Findings: Performance is not sensitive to sparsity-aware ini- tialization when using ERK and helps when using uniform layer sparsity. For RigL and SET we recommend always using sparsity-aware weight initialization (since it never ap- pears to harm performance) but for static this may depend on layer sparsity. 7. Signal-to-noise ratio in DRL environments Variance reduction is key to training deep models and of- ten achieved through using momentum based optimizers (Schmidt et al., 2011; Kingma & Ba, 2015a). However when new connections are grown such averages are not available, therefore noise in the gradients can provide mis- leading signals. In Figure 5 we share the signal-to-noise ratio (SNR) for the Classic control and MuJoCo environ- ments over the course of training. SNR is calculated as |µ| σ where µis the mean and σis the standard deviation of gradients over a mini-batch. A low SNR means the signalThe State of Sparse Training in Deep Reinforcement Learning 0 20000 40000 60000 80000 100000 Steps 10 3 10 2 10 1 Average SNR train_eval.env_name MountainCar-v0 CartPole-v0 Acrobot-v1 init_masks.sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 2 10 1 Average SNR Actor Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 1 Average SNR Critic Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 Figure 5.Signal-to-noise (SNR) ratio comparison of different gradient based DRL algorithms. We calculate SNR for every parameter in corresponding network (including the inactive/pruned weights) and report the mean SNR value. (left) DQN networks when training on classic control environments. SAC actor (center) and critic (right) networks during the training on MuJoCo environments. is dominated by the variance and thus the mean (the sig- nal) is uninformative. We calculate SNR for all parameters separately and report the mean. Mini-batch gradients can have average SNR values as low as 0.01 starting early in training. Higher sparsities seem to cause lower SNR values. Similarly, actor networks have lower SNR. Findings: We ﬁnd the average SNR for gradients to decrease with sparsity, potentially explaining the difﬁculty of using gradient based growing criteria in sparse training. 8. Are sparse networks robust to noise? Sparse neural networks can improve results on primary metrics such accuracy and rewards, yet they might have some unexpected behaviours in other aspects (Hooker et al., 2020). In Figure 6 we assess the effect of adding in- creasing amounts of noise to the observations and mea- suring their effect on a trained policy. Noise was sampled ∼N(0,σ),σ ∈[0,1,..., 30], quantized to an integer, and added to each observation’s pixel values (∈[0,255]) before normalization. Noise was sampled independently per pixel. We look at three data regimes; 100%, 50% and 10% of the standard dense model parameter count and compare dense and sparse training (RigL and SET). We made an effort to select policies with comparable performance for all the methods, chosen from the set of all policies trained during this work. We observe that (1) smaller models are generally more ro- bust to high noise than larger models, (2) sparse models are more robust to high noise than dense models on average, and (3) in most cases there are minimal differences when the noise is low. We can see that in the very low data regime (10% full pa- rameter count) policies trained using RigL are more robust to high noise compared with their dense counterparts, a fact observed across every environment. In the moderate data regime (50% full parameter count) the ordering is more mixed. In Qbert the dense model is most robust but the picture is reversed for Pong and McPacman. Finally, SET appears less robust to high noise than RigL, although we note this is not the case for Pong at 50% density. Although a preliminary analysis, it does suggest that sparse training can produce networks that are more robust to obser- vational noise, even when experienced post-training. 9. Related work Sparse training Though research on pruning has a rel- atively long story by deep learning standards (Mozer & Smolensky, 1989; Sietsma & Dow, 1988; Han et al., 2015; Molchanov et al., 2017; Louizos et al., 2017), training sparse networks from scratch has only recently gained popularity. Goyal et al. (2017), Frankle & Carbin (2019), and Evci et al. (2019) showed that training sparse networks from a ran- dom initialization is difﬁcult compared to dense neural net- works. Despite this, various approaches have been recently proposed to improve sparse training, most notably lottery tickets (Frankle et al., 2019; Zhou et al., 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; Liu & Zenke, 2020). Solutions that focus on initialization alone have been shown to be ineffective for contemporary models (Evci et al., 2022; Frankle et al., 2020), possibly due to the catapult mechanism observed early in training (Lewkowycz et al., 2020). For an in-depth survey on the topic, please see Hoeﬂer et al. (2021). Sparse networks in RL Livne & Cohen (2020) used pruning as an intermediary step to guide the width of dense neural networks for DQN and A2C agents. Most of the work investigating the use of sparse sparse training in RL are in the context of the lottery ticket hypothesis; Morcos et al. (2019) studied the existence of lucky sparse initializa- tions using pruning and late-rewinding; Hasani et al. (2020) proposed an interesting approach by repurposing the sparseThe State of Sparse Training in Deep Reinforcement Learning 0 5 10 15 20 25 30 Observation noise StdDev 500 750 1000 1250 1500 1750 2000 2250Average return MsPacman 0 5 10 15 20 25 30 Observation noise StdDev 20 15 10 5 0 5 10 15 20 Average return Pong Density 100 50 10 Method Dense RigL SET 0 5 10 15 20 25 30 Observation noise StdDev 0 1000 2000 3000 4000 5000 6000Average return Qbert Figure 6.Robustness to observation noise. We test the robustness of networks trained using sparse and dense methods (denoted by line style) by adding Gaussian noise to the observations. We examine three parameter regimes, 100% (blue), 50% (pink) and 10% (purple) of the standard dense model parameter count. All policies were trained using DQN. circuitry of the C. elegans soil-worm for RL tasks; Vischer et al. (2021) observed that the success of such initializations dependens heavily on selecting correct features for the in- put data, and not on any general qualities of the different initializations. Lee et al. (2021) proposed the use of block- circulant masks during early steps of training to improve the efﬁciency of pruning on TD3 agents, while Arnob et al. (2021) applied one-shot pruning algorithms in an ofﬂine-RL setting. Perhaps the work closest to ours is the algorithm proposed by Sokar et al. (2021), where authors applied the SET algorithm for end-to-end training of sparse networks in two actor-critic algorithms (TD3 and SAC). By a carefully chosen topology update schedule and dynamic architecture design, the proposed algorithm was able to match the dense network with a sparsity of around 50%. Novel architectures in DRL A number of works have focused on evolving network architectures for RL policies. Nadizar et al. (2021) applied pruning together with evo- lution algorithms. Whiteson & Stone (2006) combined NEAT (Stanley & Miikkulainen, 2002) with Q-learning (Watkins, 1989) to evolve better learners, Gaier & Ha (2019) evolved strong architectural priors, resulting in networks that could solve tasks with a single randomly initialized shared weight, whilst Tang et al. (2020) evolve compact self-attention architectures as a form of indirect network encoding. Zambaldi et al. (2019) similarly explored self- attention enabling agents to perform relational reasoning and achieve state-of-the-art performance on the majority of StarCraft II mini-games. Another line of research seeks to improve the stability (Parisotto et al., 2020) and efﬁciency (Parisotto & Salakhutdinov, 2021) of transformers applied to DRL, whilst Shah & Kumar (2021) explore the utility of using features extracted from a pre-trained Resnet in the standard DRL pipeline. Consistent with our observations in this work, Ha & Schmidhuber (2018) showed it is possible to train very compact controllers (i.e. actors) albeit in a the context of model-based instead of the model-free RL setting considered here. 10. Discussion and Conclusion In this work we sought to understand the state of sparse training for DRL by applying pruning, static, SET and RigL to DQN, PPO, and SAC agents trained on a variety of envi- ronments. We found sparse training methods to be a drop-in alternative for their dense counterparts providing better re- sults for the same parameter count. From a practical stand- point we made recommendations regarding hyper-parameter settings and showed that non-uniform sparse initialization combined with tuning actor:critic parameter ratios improves performance. We hope this work establishes a useful foundation for fu- ture research into sparse DRL algorithms and highlights a number of interesting research questions. In contrast to the computer vision domain, we observe that RigL fails to match pruning results. Low SNR in high sparsity regimes offers a clue but more work is needed to understand this phenomena. Our results in section 8 also suggest that sparse networks may aid in generalization and robustness to obser- vational noise; this is an active area of interest and research in the DRL community, so a more thorough understanding could result in important algorithmic advances. Acknowledgements We thank Fabian Pedregosa, Rishabh Agarwal, and Adrien Ali Ta¨ıga for their helpful feedback on the manuscript, Oscar Ramirez for his help with on the TF-Agents codebase, and Trevor Gale and Sara Hooker for inspiring the title of this work. We also thank Brain Montreal RL team for their useful feedback on an early version of this work. Finally, we thank Bram Grooten for pointing out Degrave et al. (2022) and their contribution to the motivation for this work.The State of Sparse Training in Deep Reinforcement Learning References Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 2021. Arnob, S. Y ., Ohib, R., Plis, S., and Precup, D. Single- shot pruning for ofﬂine reinforcement learning. ArXiv, abs/2112.15579, 2021. Bellec, G., Kappel, D., Maass, W., and Legenstein, R. A. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations, 2018. Bellemare, M., Candido, S., Castro, P., Gong, J., Machado, M., Moitra, S., Ponda, S., and Wang, Z. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588:77–82, 12 2020. doi: 10.1038/ s41586-020-2939-8. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The Arcade Learning Environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, June 2013. Berner, C., Brockman, G., Chan, B., Cheung, V ., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., J ´ozefowicz, R., Gray, S., Olsson, C., Pa- chocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. URL http://arxiv.org/ abs/1912.06680. Blalock, D., Ortiz, J. J. G., Frankle, J., and Guttag, J. What is the state of neural network pruning? ArXiv, 2020. URL https://arxiv.org/abs/2003.03033. Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Belle- mare, M. G. Dopamine: A research framework for deep reinforcement learning. CoRR, abs/1812.06110, 2018. URL http://arxiv.org/abs/1812.06110. Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de las Casas, D., Donner, C., Fritz, L., Galperti, C., Hu- ber, A., Keeling, J., Tsimpoukelli, M., Kay, J., Merle, A., Moret, J.-M., Noury, S., Pesamosca, F., Pfau, D., Sauter, O., Sommariva, C., Coda, S., Duval, B., Fasoli, A., Kohli, P., Kavukcuoglu, K., Hassabis, D., and Riedmiller, M. Magnetic control of tokamak plasmas through deep rein- forcement learning. Nature, 2022. Dettmers, T. and Zettlemoyer, L. Sparse networks from scratch: Faster training without losing performance. ArXiv, 2019. URL http://arxiv.org/abs/1907. 04840. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V ., Ward, T., Doron, Y ., Firoiu, V ., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K. IMPALA: scalable dis- tributed deep-rl with importance weighted actor-learner architectures. CoRR, 2018. Evci, U., Pedregosa, F., Gomez, A. N., and Elsen, E. The difﬁculty of training sparse neural networks. ArXiv, 2019. URL http://arxiv.org/abs/1906.10732. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners. In Pro- ceedings of Machine Learning and Systems 2020, 2020. Evci, U., Ioannou, Y . A., Keskin, C., and Dauphin, Y . Gradi- ent ﬂow in sparse neural networks and how lottery tickets win. In AAAI Conference on Artiﬁcial Intelligence, 2022. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th Interna- tional Conference on Learning Representations (ICLR), 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis. ArXiv, 2019. URL https://arxiv.org/abs/1903.01611. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Pruning neural networks at initialization: Why are we missing the mark? ArXiv, 2020. URL https: //arxiv.org/abs/2009.08576. Gaier, A. and Ha, D. Weight agnostic neural networks. 2019. URL https://weightagnostic.github. io. https://weightagnostic.github.io. Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. ArXiv, 2019. URL http: //arxiv.org/abs/1902.09574. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Guadarrama, S., Korattikara, A., Ramirez, O., Castro, P., Holly, E., Fishman, S., Wang, K., Gonina, E., Wu, N., Kokiopoulou, E., Sbaiz, L., Smith, J., Bart ´ok, G., Berent, J., Harris, C., Vanhoucke, V ., and Brevdo, E. TF-Agents: A library for reinforcement learning in tensorﬂow. https://github.com/tensorflow/ agents, 2018. URL https://github.com/ tensorflow/agents. [Online; accessed 25-June- 2019].The State of Sparse Training in Deep Reinforcement Learning Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings. neurips.cc/paper/2018/file/ 2de5d16682c3c35007e4e92982f1a2ba-Paper. pdf. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor. In International Conference on Machine Learning (ICML), 2018. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efﬁcient neural network. In Advances in neural information processing systems, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. EIE: Efﬁcient Inference Engine on compressed deep neural network. In Proceedings of the 43rd International Symposium on Computer Architecture, 2016. Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu, R. A natural lottery ticket winner: Reinforcement learning with ordinary neural circuits. In Proceedings of the 37th International Conference on Machine Learning. PMLR, 2020. URL https://proceedings.mlr.press/ v119/hasani20a.html. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. CoRR, 2015. Hoeﬂer, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A. Sparsity in deep learning: Pruning and growth for efﬁcient inference and training in neural networks. ArXiv, abs/2102.00554, 2021. Hooker, S., Courville, A. C., Clark, G., Dauphin, Y ., and Frome, A. What do compressed deep neural networks forget. arXiv: Learning, 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural au- dio synthesis. In International Conference on Machine Learning (ICML), 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015a. Kingma, D. P. and Ba, J. Adam: A method for stochas- tic optimization. In Bengio, Y . and LeCun, Y . (eds.), 3rd International Conference on Learning Representa- tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015b. URL http: //arxiv.org/abs/1412.6980. Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Im- plicit under-parameterization inhibits data-efﬁcient deep reinforcement learning. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=O9bnihsFfXU. Kusupati, A., Ramanujan, V ., Somani, R., Wortsman, M., Jain, P., Kakade, S., and Farhadi, A. Soft threshold weight reparameterization for learnable sparsity. In Proceedings of the International Conference on Machine Learning , 2020. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.),Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207–1216, Stan- ford, CA, 2000. Morgan Kaufmann. Lee, J., Kim, S., Kim, S., Jo, W., and Yoo, H.-J. Gst: Group- sparse training for accelerating deep reinforcement learn- ing. ArXiv, abs/2101.09650, 2021. Lee, N., Ajanthan, T., and Torr, P. H. S. SNIP: Single-shot Network Pruning based on Connection Sensitivity. In International Conference on Learning Representations (ICLR), 2019, 2019. Lewkowycz, A., Bahri, Y ., Dyer, E., Sohl-Dickstein, J., and Gur-Ari, G. The large learning rate phase of deep learning: the catapult mechanism. Arxiv, 2020. URL https://arxiv.org/pdf/2003.02218. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train large, then compress: Re- thinking model size for efﬁcient training and inference of transformers. ArXiv, abs/2002.11794, 2020. Liu, S., Yin, L., Mocanu, D. C., and Pechenizkiy, M. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In ICML, 2021. Liu, T. and Zenke, F. Finding trainable sparse networks through neural tangent transfer. In ICML, 2020. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. In International Conference on Learning Representations, 2019. Livne, D. and Cohen, K. Pops: Policy pruning and shrink- ing for deep reinforcement learning. IEEE Journal of Selected Topics in Signal Processing, 14:789–801, 2020. Louizos, C., Ullrich, K., and Welling, M. Bayesian compres- sion for deep learning. InAdvances in Neural Information Processing Systems, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- ness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,The State of Sparse Training in Deep Reinforcement Learning Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier- stra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540): 529–533, 2015. Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of arti- ﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 2018. Molchanov, D., Ashukha, A., and Vetrov, D. P. Variational Dropout Sparsiﬁes Deep Neural Networks. In Proceed- ings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 Au- gust 2017, 2017. Morcos, A., Yu, H., Paganini, M., and Tian, Y . One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Information Processing Systems, 2019. Mostafa, H. and Wang, X. Parameter efﬁcient train- ing of deep convolutional neural networks by dynamic sparse reparameterization. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , 2019. URL http://proceedings.mlr.press/ v97/mostafa19a.html. Mozer, M. C. and Smolensky, P. Skeletonization: A tech- nique for trimming the fat from a network via relevance assessment. In Advances in Neural Information Process- ing Systems 1, 1989. Nadizar, G., Medvet, E., Pellegrino, F. A., Zullich, M., and Nichele, S. On the effects of pruning on evolved neural controllers for soft robots. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, 2021. Nikishin, E., Schwarzer, M., D’Oro, P., Bacon, P.-L., and Courville, A. The primacy bias in deep reinforcement learning. In Proceedings of the Thirty-ninth International Conference on Machine Learning (ICML’22), 2022. Parisotto, E. and Salakhutdinov, R. Efﬁcient transformers in reinforcement learning using actor-learner distillation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=uR9LaO_QxF. Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., Botvinick, M., Heess, N., and Hadsell, R. Stabilizing transformers for reinforcement learning. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7487–7498. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/parisotto20a.html. Peste, A., Ioﬁnova, E., Vladu, A., and Alistarh, D. Ac/dc: Alternating compressed/decompressed training of deep neural networks. ArXiv, abs/2106.12379, 2021. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY , USA, 1st edition, 1994. ISBN 0471619779. Schmidt, M., Roux, N. L., and Bach, F. Convergence rates of inexact proximal-gradient methods for convex optimiza- tion. In Proceedings of the 24th International Conference on Neural Information Processing Systems, 2011. Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. In Bengio, Y . and Le- Cun, Y . (eds.),4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506.02438. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. Shah, R. M. and Kumar, V . Rrl: Resnet as representation for reinforcement learning. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research, pp. 9465–9476. PMLR, 18– 24 Jul 2021. URL https://proceedings.mlr. press/v139/shah21a.html. Sietsma, J. and Dow, R. J. Neural net pruning-why and how. In IEEE International Conference on Neural Networks, 1988. doi: 10.1109/ICNN.1988.23864. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V ., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nature, 529:484–503, 2016. URL http://www.nature.com/nature/journal/ v529/n7587/full/nature16961.html. Sokar, G., Mocanu, E., Mocanu, D. C., Pechenizkiy, M., and Stone, P. Dynamic sparse training for deep reinforcement learning. ArXiv, abs/2106.04217, 2021.The State of Sparse Training in Deep Reinforcement Learning Stanley, K. O. and Miikkulainen, R. Evolving neural networks through augmenting topologies. Evol. Com- put., 10(2):99–127, jun 2002. ISSN 1063-6560. doi: 10.1162/106365602320169811. URL https://doi. org/10.1162/106365602320169811. Tanaka, H., Kunin, D., Yamins, D. L. K., and Ganguli, S. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. ArXiv, 2020. URL https: //arxiv.org/abs/2006.05467. Tang, Y ., Nguyen, D., and Ha, D. Neuroevolution of self- interpretable agents. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ’20, pp. 414–424, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 9781450371285. doi: 10. 1145/3377930.3389847. URL https://doi.org/ 10.1145/3377930.3389847. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012. doi: 10.1109/IROS.2012.6386109. Vischer, M. A., Lange, R., and Sprekeler, H. On lottery tickets and minimal task representations in deep rein- forcement learning. ArXiv, abs/2105.01648, 2021. Wang, C., Zhang, G., and Grosse, R. Picking winning tickets before training by preserving gradient ﬂow. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=SkgsACVKPH. Watkins, C. J. C. H. Learning from Delayed Re- wards. PhD thesis, King’s College, Cambridge, UK, May 1989. URL http://www.cs.rhul.ac.uk/ ˜chrisw/new_thesis.pdf. Whiteson, S. and Stone, P. Evolutionary function approxi- mation for reinforcement learning. Journal of Machine Learning Research, 7(31):877–917, 2006. URL http: //jmlr.org/papers/v7/whiteson06a.html. Wortsman, M., Farhadi, A., and Rastegari, M. Discover- ing neural wirings. In Advances in Neural Information Processing Systems, 2019. Zambaldi, V ., Raposo, D., Santoro, A., Bapst, V ., Li, Y ., Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V ., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. Deep rein- forcement learning with relational inductive biases. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=HkxaFoC9KQ. Zhou, H., Lan, J., Liu, R., and Yosinski, J. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Ad- vances in Neural Information Processing Systems, 2019. Zhu, M. and Gupta, S. To Prune, or Not to Prune: Explor- ing the Efﬁcacy of Pruning for Model Compression. In International Conference on Learning Representations Workshop, 2018.The State of Sparse Training in Deep Reinforcement Learning A. Experimental Details A.1. Training schedule During training all agents are allowed M environment transitions, with policies being evaluated for Kepisodes / steps every N environment frames, where the values vary per suite and shared below. Atari experiments use a frame skip of 4, following (Mnih et al., 2015) thus 1 environment step = 4 environment frames. Environment M N K K = episodes Classic control 100,000 2 ,000 20 MujoCo (SAC) 1,000,000 10 ,000 30 MujoCo (PPO) 1,000,000 2 ,000 20 K = environment steps Atari Suite 40,000,000 1 ,000,000 125 ,000 A.2. FLOPs behaviour of Sparse ERK networks in DRL As reported in Evci et al. (2020), using ERK sparsity distribution often doubles the FLOPs needed for sparse models compared using the uniform sparsity distribution. This is due to the parameter sharing in convolutional layers. The spatial dimensions, kernel size and the stride of a convolutional layer affects how many times each weight is used during the convolution which in turn determines the contribution of each weight towards the total FLOPs count. In modern CNNs, the spatial dimensions of the feature maps often decreases monotonically towards the output of the network, making the contribution of the connections in later layers to the total FLOPs count smaller. Furthermore, the size of the layers typically increases towards the output and thus ERK removes a larger proportion of the connections from these later layers compared to uniform. Consequently a network sparsiﬁed using the ERK distribution will have a larger FLOPs count compared to one sparsiﬁed using a uniform distribution. Due to the lack of parameter sharing fully connected layers used in MuJoCo and classic control experiments, sparse networks with ERK have same amount of FLOPs as the uniform. Networks used in Atari experiments, however, uses convolutional networks and thus ERK doubles the FLOPs required compared to uniform. FLOPs scaling of sparse networks with ERK distributions used for Pong game can be found in Figure 7. Atari games have differently sized action spaces (Pong has 6 actions for example), which affects the number of neurons in the last layer. However since the last layer is very small and fully connected, it should have a very little effect on the results provided here. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 106 107 FLOPs DQN-Atari CNN ERK 2x Uniform Uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 107 108 FLOPs DQN-Atari Resnet ERK 2x Uniform Uniform Figure 7.FLOPs scaling of different sparsity distributions on (left) Nature CNN and (right) Impala ResNet architectures used in MsPacman environment.The State of Sparse Training in Deep Reinforcement Learning A.3. Hyper-parameter sweep We perform a grid search over different hyper parameters used in Dense, Prune, Static, SET and RigL algorithms. Unless otherwise noted, we use hyper-parameters used in regular dense training. When pruning, we start pruning around 20% of training steps and stop when 80% of training is completed following the ﬁndings of Gale et al. (2019). We use same default hyper-parameters for SET and RigL. Fort both algorithms we start updating the mask at initialization and decay the drop fraction over the course of the training using a cosine schedule, similar to pruning stopping the updates when 80% of training is completed. We search over the following parameters: 1. Weight decay ( ): Searched over the grid [0, 1e-6, 1e-4, 1e-3]. 2. Update Interval ( ): refers to how often models are pruned or sparse topology is updated. Searched over the grid [100, 250, 500, 1000, 5000]. 3. Drop Fraction ( ): refers to the maximum percentage of parameters that are dropped and added when network topology is updated. This maximum value is decayed during training according to a cosine decay schedule. Searched over the grid [0.0,0.1,0.2,0.3,0.5]. 4. Sparsity-aware initialization ( ): refers to whether sparse models are initialized with scaled initialization or not. We repeat the hyper-parameter search for each DRL algorithm using the Acrobot (for DQN) and Walker2D (for PPO and SAC) environments. Best hyper-parameters found in these environments are then used when training in other similar environments (i.e. classic control for DQN and MuJoCO for PPO and SAC). See Table 1, Table 2, and Table 3 for the best hyper parameters found in each setting. Table 1.DQN best hyper-parameters for Classic Control from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 1,000 - True Static 1 ·10−6 - - True RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−6 1,000 0.5 True Table 2.SAC best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−4 - - - Pruning 1 ·10−4 1,000 - True Static 1 ·10−4 - - False RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−4 250 0.3 TrueThe State of Sparse Training in Deep Reinforcement Learning Table 3.PPO best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 500 - False Static 0 - - True RigL 1 ·10−4 250 0.3 True SET 1 ·10−6 100 0 True Atari hyper-parameters Due to computational constraints we did not search over hyper-parameters for the Atari environ- ments, except for a small grid-search to tune the dense ResNet. The CNN architecture from Mnih et al. (2015) has been used in many prior works thus was already well tuned. The ResNet hyper-parameter sweep for the original dense model is detailed below: 1. Weight decay:Searched over the grid [0, 1e-6, 1e-5, 1e-4]. 2. Learning rate:Searched over the grid [1e-4, 2.5e-4, 1e-3, 2.5e-3]. The ﬁnal hyper-parameters we used for the Atari environments are shown in in Table 4 for the CNN and Table 5 for the ResNet. Table 4.DQN (CNN) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 0 2.5 ·10−4 - - - Pruning 0 2.5 ·10−4 5,000 - False Static 0 2.5 ·10−4 - - True RigL 0 2.5 ·10−4 5,000 0.3 True SET 0 2.5 ·10−4 5,000 0.3 True Table 5.DQN (ResNet) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−5 1 ·10−4 - - - Pruning 1 ·10−5 1 ·10−4 5,000 - False Static 1 ·10−5 1 ·10−4 - - True RigL 1 ·10−5 1 ·10−4 5,000 0.3 True SET 1 ·10−5 1 ·10−4 5,000 0.3 True Remaining hyper-parameters Next, we include details of the DRL hyper-parameters used in all training settings for DQN (Table 6 and Table 7), SAC (Table 8), and PPO (Table 8).The State of Sparse Training in Deep Reinforcement Learning Table 6.DQN Hyperparameters/ Table format from Haarnoja et al. (2018). Parameter Value Shared optimizer Adam (Kingma & Ba, 2015b) discount (γ) 0.99 nonlinearity ReLU target smoothing coefﬁcient (τ) 1.0 gradient steps per training step 1 exploration policy epsilon greedy epsilon decay period (env steps) 2.5 ·104 Classic Control replay buffer size 105 learning rate 1 ·10−3 initial collect steps 1,000 target update interval 100 reward scale factor 1.0 gradient steps every k env steps, k = 1 ﬁnal epsilon 0.1 eval epsilon 0.1 number of samples per minibatch 128 network type MLP number of hidden dense layers 2 number of hidden units per layer 512 Atari replay buffer size 106 initial collect steps 20,000 target update interval 8000 reward scale factor 1.0 gradient steps every k env steps, k = 4 ﬁnal epsilon 0.01 eval epsilon 0.001 number of samples per minibatch 32 network type CNN or ResNetThe State of Sparse Training in Deep Reinforcement Learning Table 7.DQN Atari: CNN and ResNet Architectures Parameter Value CNN learning rate 2.5 ·10−4 Adam optimizer, epsilon 1 ·10−8 CNN Architecture number of hidden CNN layers 3 number of hidden dense layers 1 number of hidden units per dense layer 512 CNN params per layer (ﬁlters, kernel, stride) layer 1 (ﬁlters, kernel, stride) 32, 8, 4 layer 2 (ﬁlters, kernel, stride) 64, 4, 2 layer 3 (ﬁlters, kernel, stride) 64, 3, 1 ResNet learning rate 1 ·10−4 Adam optimizer, epsilon 3.125 ·10−4 ResNet Architecture number of stacks 3 number of hidden dense layers 1 number of hidden units per dense layer 512 use batch norm False ResNet stack layers num CNN layers 1 num max pooling layers 1 num residual-CNN layers 2 ResNet params per layer (ﬁlters, kernel, stride) stack 1 (ﬁlters, kernel, stride) 32, 3, 1 stack 2 (ﬁlters, kernel, stride) 64, 3, 1 stack 3 (ﬁlters, kernel, stride) 64, 3, 1The State of Sparse Training in Deep Reinforcement Learning Table 8.SAC Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 replay buffer size 106 number of hidden layers (all networks) 2 number of hidden units per layer 256 number of samples per minibatch 256 nonlinearity ReLU target smoothing coefﬁcient (τ) 0.005 target update interval 1 train every k env steps, k = 1 gradient steps per training step = 1 Hopper, Walker, Humanoid initial collect steps 1,000 HalfCheetah, Ant initial collect steps 10,000 Table 9.PPO Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 shared / separate networks separate number of hidden layers (all networks) 2 number of hidden units per layer 64 collect sequence length (batch size) 2048 minibatch size 64 num epochs 10 importance ratio clipping 0.2 use GAE (Schulman et al., 2016) True λ(GAE) 0.95 entropy regularization 0 value loss coeff 0.5 gradient clipping 0.5 A.4. Atari Game Selection Our original three games (MsPacman, Pong, Qbert) were selected to have varying levels of difﬁculty as measured by DQN’s human normalized score in Mnih et al. (2015), Figure 3. To this we added 12 games (Assault, Asterix, BeamRider, Boxing, Breakout, CrazyClimber, DemonAttack, Enduro, FishingDerby, SpaceInvaders, Tutankham, VideoPinball) selected to be roughly evenly distributed amongst the games ranked by DQN’s human normalized score in Mnih et al. (2015) with a lower cut off of approximately 100% of human performance.The State of Sparse Training in Deep Reinforcement Learning B. Sparse Scaling Plots in Other Environments Here we share results on additional environments, Acrobot, CartPole, MountainCar, Hopper, and Ant. Figure 8 compares ﬁnal reward relative to parameter count using DQN. ERK sparsity distribution was used in the top row whilst uniform was used in the bottom row. Figure 9 presents results on the two remaining MuJoCo environment, Hopper and Ant with SAC (top row) and PPO (bottom row). In Figures 10 and 11 we show sparsity scaling plots for 15 Atari games using the standard CNN and ResNet respectively. 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.700.800.900.950.960.97 0.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.700.80 0.900.950.960.970.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.70 0.80 0.900.95 0.96 0.97 0.98 DQN / MountainCar-v0 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.600.700.80 0.90 0.950.96 0.970.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.600.700.800.90 0.95 0.96 0.97 0.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.600.700.800.900.950.960.970.98 DQN / MountainCar-v0 Figure 8.DQN in the Classic Control environments with ERK network sparsity distribution (top) and uniform network sparsity (bottom). 104 105 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.900.950.960.97 0.98 SAC / Hopper-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.70 0.80 0.90 0.950.96 0.97 0.98 SAC / Ant-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000Reward 0.50 0.70 0.80 0.90 0.95 0.96 0.970.98 0.99 PPO / Hopper-v2 Dense Pruning Rigl Set Static 103 104 #Params 1000 1500 2000 2500 3000 3500 4000Reward 0.500.70 0.800.90 0.95 0.96 0.970.98 0.99 PPO / Ant-v2 Dense Pruning Rigl Set Static Figure 9.Additional MuJoCO environments (Hopper and Ant) for SAC and PPO algorithms. Networks are initialized with ERK network sparsity distribution.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 400 600 800 1000 1200Reward 0.500.80 0.900.95 0.98 0.99 Assault / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000Reward 0.500.80 0.90 0.95 0.98 0.99 Asterix / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 750 1000 1250 1500 1750 2000 2250 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 BeamRider / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20 40 60 80Reward 0.50 0.80 0.900.95 0.98 0.99 Boxing / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 10 20 30 40 50 60 70Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000Reward 0.50 0.800.90 0.95 0.98 0.99 CrazyClimber / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 200 400 600 800 1000 1200 1400Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 100 200 300 400 500 600Reward 0.50 0.800.90 0.95 0.98 0.99 Enduro / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 Reward 0.50 0.80 0.90 0.950.98 0.99 FishingDerby / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 300 400 500 600 700 800Reward 0.500.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 40 60 80 100 120Reward 0.500.800.900.95 0.98 0.99 Tutankham / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (CNN) Dense Pruning RigL SET Static Figure 10.CNN: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 500 1000 1500 2000 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 Assault / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.50 0.80 0.900.95 0.98 0.99 Asterix / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.50 0.800.90 0.95 0.98 0.99 BeamRider / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 40 20 0 20 40 60 80 100Reward 0.500.800.900.950.98 0.99 Boxing / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50 100 150 200 250 300Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.500.800.900.950.98 0.99 CrazyClimber / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 10000 20000 30000 40000 50000 60000 70000Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 200 400 600 800 1000 1200 1400Reward 0.500.80 0.900.950.980.99 Enduro / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 0 20 Reward 0.500.800.900.950.98 0.99 FishingDerby / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.80 0.90 0.95 0.98 0.99 MsPacman / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 20 10 0 10 20 Reward 0.500.800.900.950.980.99 Pong / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.500.80 0.900.95 0.98 0.99 Qbert / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000 10000Reward 0.50 0.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 25 50 75 100 125 150 175Reward 0.500.80 0.90 0.950.98 0.99 Tutankham / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (ResNet) Dense Pruning RigL SET Static Figure 11.ResNet: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning C. Additional Results with Uniform Sparsity Distribution In Figure 12 we repeat the experiments presented in Figure 2, however this time using a uniform network sparsity distribution at initialization. These plots provide further evidence as the the beneﬁt of using ERK over uniform to distribute network sparsity. 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.500.800.90 0.95 0.98 0.99 Pong / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 1000 2000 3000 4000 5000 6000 0.50 0.80 0.90 0.950.980.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 2500 0.500.80 0.90 0.95 0.98 0.99 MsPacman / DQN (CNN) 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.600.700.80 0.90 0.950.960.970.98 SAC / Walker2d-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 2000 4000 6000 8000 10000 12000Reward SAC / HalfCheetah-v2 Dense Pruning Rigl Set Static 104 105 #Params 1000 2000 3000 4000 5000 0.50 0.60 0.70 0.800.900.95 0.96 0.97 0.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.980.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.700.80 0.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 250 500 750 1000 1250 1500 1750 2000 2250 0.500.70 0.80 0.90 0.95 0.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 12.Uniform network sparsity initialization. Comparison of ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (row-2) SAC on MuJoCo (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals.The State of Sparse Training in Deep Reinforcement Learning D. Additional Hyper-parameter Sensitivity Plots Here we share the remaining plots for our analysis on the sensitivity of sparse training algorithms to various hyper-parameters. Policies area trained using SAC. We note that different architectures, environments and training algorithms might show different curves, which we omit due to high cost of running such analysis. 0.0 0.1 0.2 0.3 0.4 0.5 Drop Fraction 0 1000 2000 3000 4000 5000 6000Reward Walker2d Rigl Set 102 103 Update Interval 0 1000 2000 3000 4000 5000 6000Reward Walker2d Pruning Rigl Set 0 200 400 600 800 1000 Batch size 0 1000 2000 3000 4000 5000 6000Reward Walker2d Set Dense RigL Static Figure 13.Sensitivity analysis: (left) Drop fraction: Comparing different drop fractions for SET and RigL at 80% sparsity. (center) Comparing the effect of topology update interval on pruning, static, and RigL. (right) The effect of batch size. Drop fraction In Figure 13 (left) we evaluate the effect of drop fraction and observe that drop fractions >0 yield a small performance improvement over drop fraction = 0, which is equivalent to static sparse training. This indicates that changing the network topology during training helps performance. Surprisingly training does not appear particularly sensitive to the drop fraction chosen, with values from 10 - 50% yielding approximately equivalent performance. It is possible that this is because the environment is relatively easy, thus leading to little separation between different settings. The large improvement that RigL and SET give over static (drop fraction = 0) in the Humanoid and Atari environments is one reason to suspect this. Findings: Drop fractions of 10 - 50% worked well for RigL, whilst a drop fraction of 30% worked best for SET. 30% therefore appears to be a reasonable default for dynamic sparse training. However, the uniformity of performance merits investigation on a harder environment in which more separation between drop fractions may be observed. Topology update interval In Figure 13 (center) we evaluate the effect of topology update interval on pruning, RigL and SET and ﬁnd that training is not particularly sensitive to it. Findings: Updating the network every 1000 environment steps appears to be a reasonable default. Batch size In Figure 13 (right) we evaluate the effect of batch size. Batch size is critical for obtaining a good estimate for the gradients during training for all methods, however for RigL it is also used when selecting new connections. We observe performance degradation for all methods when smaller batch sizes are used, with no particular additional effect on RigL. Findings: Sparse networks seems to have similar sensitivity to the batch size as the dense networks. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 400 350 300 250 200 150 100 Reward Acrobot Algorithm rigl static Distribution erk uniform Figure 14.Evaluating non uniform sparsities on HalfCheetah using SAC when all layers pruned (left) and last layer is kept dense (center). On the right we prune all layers of the DQN MLP network on the Acrobot task.The State of Sparse Training in Deep Reinforcement Learning Within network sparsity In Figure 14 we share additional analysis on within network sparsity for SAC and also on DQN in the Acrobot environment. E. Additional Interquartile Mean (IQM) Plots Mujoco SAC In Figure 15 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for SAC at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.99 Normalized Score Figure 15.SAC: IQM plots calculated over ﬁve Mujoco games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), with 10 seeds per game. Except for sparsity = 0.99 which only includes results from HalfCheetah, Hopper, and Walker2d, each with 10 seeds.The State of Sparse Training in Deep Reinforcement Learning Mujoco PPO In Figure 16 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for PPO at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 95% sparsity the IQM is calculated over four environments and for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.99 Normalized Score Figure 16.SAC: IQM plots calculated over multiple Mujoco games with 9 seeds per game. 50 - 90% sparsity include ﬁve games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), 95% sparsity includes Ant, HalfCheetah, Hopper, Walker2d, and 99% sparsity HalfCheetah, Hopper, Walker2d. Atari DQN Figure 17 presents IQM plots calculated over 15 Atari games for the standard CNN network architecture and Figure 18 presents IQM plots calculated over the same set of games for a ResNet architecture with an approximately equivalent number of parameters as the standard CNN (≈4M).The State of Sparse Training in Deep Reinforcement Learning 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.5 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.95 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.99 Human Normalized Score Figure 17.CNN: IQM plots calculated over 15 Atari Games, with 9 seeds per game. 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.5 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.95 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.99 Human Normalized Score Figure 18.ResNet: IQM plots calculated over 15 Atari Games, with 9 seeds per game.",
      "meta_data": {
        "arxiv_id": "2206.10369v1",
        "authors": [
          "Laura Graesser",
          "Utku Evci",
          "Erich Elsen",
          "Pablo Samuel Castro"
        ],
        "published_date": "2022-06-17T14:08:00Z",
        "pdf_url": "https://arxiv.org/pdf/2206.10369v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research systematically investigates the application of existing sparse training techniques to Deep Reinforcement Learning (DRL) agents and environments. The main contributions are: (1) Sparse neural networks consistently outperform dense networks for the same parameter count in DRL. (2) It's possible to train networks with up to 80-90% sparsity with minimal performance degradation compared to standard dense networks. (3) Pruning yields the best results, and dynamic sparse training significantly improves over static sparse training. (4) The distribution of parameters among actor/critic networks and layers greatly impacts training, with the best performance achieved by allocating most parameters to the critic and using Erdos Renyi Kernel (ERK) sparsity distributions. (5) Sparse methods demonstrate better robustness to observational noise compared to dense counterparts in most cases.",
        "methodology": "The study benchmarks four sparse training algorithms: Pruning (dense-to-sparse weight magnitude based), Static (randomly pruned at initialization, fixed structure), Sparse Evolutionary Training (SET - dynamic sparse, replacing lowest magnitude connections randomly), and Rigged Lottery (RigL - dynamic sparse, growing connections based on gradient signal). These methods were applied to value-based DRL (DQN) and actor-critic DRL (SAC, PPO) agents. The methodology also includes detailed analyses of sparsity distribution strategies (uniform vs. ERK), weight decay, layer initialization (sparsity-aware vs. dense), signal-to-noise ratio for gradients, batch size, and topology update strategy and frequency, and actor-critic parameter ratios.",
        "experimental_setup": "The experiments employed DQN for value-based DRL and PPO/SAC for actor-critic DRL. Discrete-control environments included three classic control tasks (CartPole, Acrobot, MountainCar) and 15 games from the ALE Atari suite (using both CNN and ResNet architectures). Continuous-control environments used five MuJoCo suite tasks (HalfCheetah, Hopper, Walker2d, Ant, Humanoid). Policies were trained across sparsity levels from 50% to 99%. Each experiment was repeated with at least 10 different seeds, reporting the average reward over the last 10% of evaluations with 95% confidence intervals. Results were aggregated using the Interquartile Mean (IQM) computed with rliable. Hyper-parameter sweeps were performed for each algorithm, with specific parameters detailed for Classic Control, MuJoCo, and Atari environments. A parameter-equivalent dense baseline (smaller dense networks) was also trained for comparison. The code is built upon TF-Agents, Dopamine, and RigL codebases.",
        "limitations": "Existing research on sparse training predominantly focuses on image classification, which might not directly transfer to DRL. Full hyper-parameter sweeps for Atari DQN experiments were computationally expensive, potentially leading to conservative sparse results compared to well-tuned dense baselines. Gradient-based growing methods (like RigL) show limited performance impact in DRL, possibly due to the low signal-to-noise ratio in DRL gradients. The clarity of observed differences between sparse and dense training is influenced by the stability of the underlying RL algorithm, with PPO results being less distinct due to higher variance. The ERK sparsity distribution, while beneficial, doubles FLOPs for convolutional layers. Sensitivity analyses were not exhaustive across all architectures, environments, and training algorithms due to high computational costs.",
        "future_research_directions": "Promising avenues include improving the overall effectiveness of sparse training methods for DRL, further advancing their widespread use in the field, and specifically investigating why gradient-based growth (RigL) fails to match pruning results in DRL, particularly in the context of low signal-to-noise ratio in gradients. A more thorough understanding of how sparse networks contribute to generalization and robustness against observational noise is also highlighted as an active and important research area. Further inquiry into task- and network-dependent gains of sparse training, and optimizing actor-critic parameter ratios to understand the relative complexity of policy and value functions across environments, are also suggested. Additionally, investigating the impact of different drop fractions on harder environments could provide further insights."
      }
    },
    {
      "title": "The State of Sparse Training in Deep Reinforcement Learning",
      "abstract": "The use of sparse neural networks has seen rapid growth in recent years,\nparticularly in computer vision. Their appeal stems largely from the reduced\nnumber of parameters required to train and store, as well as in an increase in\nlearning efficiency. Somewhat surprisingly, there have been very few efforts\nexploring their use in Deep Reinforcement Learning (DRL). In this work we\nperform a systematic investigation into applying a number of existing sparse\ntraining techniques on a variety of DRL agents and environments. Our results\ncorroborate the findings from sparse training in the computer vision domain -\nsparse networks perform better than dense networks for the same parameter count\n- in the DRL domain. We provide detailed analyses on how the various components\nin DRL are affected by the use of sparse networks and conclude by suggesting\npromising avenues for improving the effectiveness of sparse training methods,\nas well as for advancing their use in DRL.",
      "full_text": "The State of Sparse Training in Deep Reinforcement Learning Laura Graesser* 1 2 Utku Evci* 2 Erich Elsen3 Pablo Samuel Castro2 Abstract The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the re- duced number of parameters required to train and store, as well as in an increase in learning efﬁ- ciency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Re- inforcement Learning (DRL). In this work we perform a systematic investigation into applying a number of existing sparse training techniques on a variety of DRL agents and environments. Our results corroborate the ﬁndings from sparse train- ing in the computer vision domain – sparse net- works perform better than dense networks for the same parameter count – in the DRL domain. We provide detailed analyses on how the various com- ponents in DRL are affected by the use of sparse networks and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL1. 1. Introduction Deep neural networks are typically organized as a stack of layers. Each layer consists of multiple neurons, where each neuron is connected to all neurons in the next layer; this is often referred to as a dense network. Alternatively, each neuron can be wired to a subset of the neurons in the next layer, resulting in a sparse, and smaller, network. Such sparse neural networks have been shown to match the performance of their dense counterparts while requiring only 10%-to-20% of the connections in most cases (Han et al., 2015; Gale et al., 2019; Blalock et al., 2020) providing *Equal contribution 1Robotics at Google 2Google Research, Canada 3Adept AI. Correspondence to: Laura Graesser <laura- graesser@google.com>, Utku Evci <evcu@google.com>, Pablo Samuel Castro <psc@google.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl signiﬁcant memory, storage, and latency gains. Deep networks have become a mainstay of scalable rein- forcement learning (RL), key to recent successes such as playing – at superhuman levels – Atari games (Mnih et al., 2015), Go (Silver et al., 2016), Dota 2 (Berner et al., 2019) and as well as controlling complex dynamical systems such as stratospheric balloons (Bellemare et al., 2020) and plasma in real-time (Degrave et al., 2022). Despite their importance, most deep reinforcement learning (DRL) research focuses on improving the algorithmic aspect of DRL, and less on the architecture aspect. Sparse networks in particular have received very little attention, likely due to the belief that net- work over-parameterization helps with learning. However, recent work suggests that RL agents may suffer from im- plicit under-parameterization when training deep networks with gradient descent (Kumar et al., 2021), suggesting that the network’s expressivity is in fact underused. In addition to this, Nikishin et al. (2022) suggests deep RL agents may have a tendency to overﬁt to early training data. Given this, one might expect there is substantial opportunity to com- press RL agents. Further, sparse networks might beneﬁt DRL by reducing the cost of training or aid running them in latency-constrained settings such as controlling plasma (Degrave et al., 2022). One limitation of current research on training sparse neural networks is that it almost solely focuses on image classiﬁca- tion benchmarks (Blalock et al., 2020; Hoeﬂer et al., 2021) creating the risk of over-ﬁtting to a speciﬁc domain. Do ad- vances observed in computer vision (CV) transfer to DRL? A few recent works (Sokar et al., 2021; Arnob et al., 2021) attempt to address this by applying individual sparse train- ing algorithms to DRL agents. However, it is still unknown if the key observation made in CV , thatsparse models per- form better than dense ones for the same parameter count, transfers to DRL. In this work we focus on answering that question and sys- tematically explore the effectiveness of different sparse learning algorithms in the online DRL setting. In order to achieve this, we benchmark four different sparse training algorithms using value-based (DQN (Mnih et al., 2015)) and actor-critic, (SAC (Haarnoja et al., 2018) and PPO (Schul- man et al., 2017)) agents. Our results also include a broad analysis of various components that play a role in the train- arXiv:2206.10369v1  [cs.LG]  17 Jun 2022The State of Sparse Training in Deep Reinforcement Learning ing of these sparse networks: sparsity distribution strategies, weight decay, layer initialization, signal-to-noise ratio for gradients, as well as batch size, topology update strategy and frequency. We summarize our main ﬁndings below: • In almost all cases, sparse neural networks perform bet- ter than their dense counterparts for a given parameter count, demonstrating their potential for DRL. • It is possible to train up to 80 - 90% sparse networks with minimal loss in performance compared to the standard dense networks. • Pruning often obtains the best results, and dynamic sparse training improves over static sparse training sig- niﬁcantly. However gradient based growth (Evci et al., 2020) seems to have a limited effect on performance. We argue this is due to low signal-to-noise ratio in gradients. • The distribution of parameters among the actor and critic networks, as well as among different layers, im- pact training greatly. We observe that the best perfor- mance is obtained by allocating the majority of parame- ters to the critic network and using Erdos Renyi Kernel (ERK) sparsity distributions. • We observe robust performance over various hyper- parameter variations. Somewhat surprisingly, when adding noise to the observations, sparse methods achieve better robustness in most cases. 2. Background 2.1. Sparse training Removing connections from neural networks was suggested at least as early as Mozer & Smolensky (1989), which coined the name “Skeleton” networks for what we today call sparse networks. Techniques for ﬁnding sparse neural networks can be grouped under two main categories. (1) Dense-to-sparse training approaches (Han et al., 2016; Molchanov et al., 2017; Wortsman et al., 2019; Kusu- pati et al., 2020; Peste et al., 2021) start with a dense neural network and gradually reduce the network size by pruning its weights. This approach often achieves state-of-the-art performance amongst sparse networks, however it requires the same (or more) computation as training a large dense net- work. An alternative to pruning is (2) sparse training (Mocanu et al., 2018). This family of methods sparsiﬁes the network at initialization and maintains this sparsity through- out training, thus reducing the training cost proportional to the sparsity of the network. However, training sparse neural networks from scratch is known to be difﬁcult, leading to sub-optimal solutions (Frankle & Carbin, 2019; Liu et al., 2019; Evci et al., 2019). DRL training is notoriously resource hungry, hence we fo- cus on the second family of methods (i.e. sparse training) in this work. There are various approaches to sparse training. One line of work (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020), attempts to prune a dense networkimmediately on iteration 0. The resulting networks are used as an initial- ization for sparse training and kept ﬁxed throughout. These techniques have been shown to have marginal gains over random pruning (Frankle et al., 2020), especially when used in modern training pipelines. Furthermore they may not generalize well in the RL setting as the non-stationarity of the data make it less clear that any decision made at iteration 0 will remain optimal throughout training. Another line of work starts with randomly initialized sparse neural networks (both weights and masks) and focuses on improving sparse training by changing the sparse connec- tivity among neurons (Mocanu et al., 2018; Bellec et al., 2018) throughout the optimization. Known as Dynamic sparse training (DST), such approaches have been shown to match pruning results, making it possible to train sparse net- works efﬁciently without sacriﬁcing performance (Dettmers & Zettlemoyer, 2019; Evci et al., 2020). In this work we benchmark one dense-to-sparse and three sparse training methods, which we brieﬂy describe below: Pruning (Zhu & Gupta, 2018):uses a simple procedure to slowly make a dense network sparse over the course of one training run using weight magnitudes. We start pruning the network from 20% of the training steps and stop when we reach 80%, keeping the ﬁnal sparse network ﬁxed for the remaining of the training. This simple pruning algorithm is shown to exceed or match more complex pruning algorithms (Gale et al., 2019). Despite the fact it requires the same order of magnitude resources as training a dense network, we included this method since it serves as an upper bound on the sparse training performance. Static: prunes a given dense network randomly at initial- ization and the resulting sparse network is trained with a ﬁxed structure. This is an important baseline to show the effectiveness of DST algorithms explained below. Sparse Evolutionary Training (SET) (Mocanu et al., 2018): Similar to Static, SET starts training with a random sparse network. During training, a portion of the connec- tions are changed every N steps (the update interval) by replacing the lowest magnitude connections with new ran- dom ones. The fraction (drop fraction) of updated weights are decayed over the course of training to help the network converge to a minima. We use cosine decay as proposed by Dettmers & Zettlemoyer (2019). Rigged Lottery (RigL) (Evci et al., 2020):is the same as SET, except the new connections are activated using the gradient signal (highest magnitude) instead of at random.The State of Sparse Training in Deep Reinforcement Learning This criteria has been shown to improve results signiﬁcantly in image classiﬁcation and with enough training iterations matches or exceed accuracies obtained by pruning. 2.2. Reinforcement learning Reinforcement learning (RL) aims to design learning algo- rithms for solving sequential decision-making problems. Typically these are framed as an agent interacting with an environment at discrete time-steps by making action choices from a set of possible agent states; the environ- ment in turn responds to the action selection by (possibly) changing the agent’s state and/or providing a numerical reward (or cost); the agent’s objective is to ﬁnd a pol- icy mapping states to actions so as to maximize (mini- mize) the sum of rewards (costs). This is formalized as a Markov decision process (Puterman, 1994) deﬁned as a tuple ⟨X,A,P,R,γ⟩, where X is the state space, A is the action space, P : X ×A →∆(X) deﬁnes the transition dynamics2, R : X ×A →R is the reward function, and γ ∈[0,1) is a discount factor. A policy π : X →∆(A) formalizes an agent’s behaviour and induces a value func- tion Vπ : X →R deﬁned via the well-known Bellman recurrence: Vπ(x) := Ea∼π(x) [ R(x,a) + γEx′∼P(x,a)Vπ(x′) ] (1) It is convenient to deﬁne state-action value functions Qπ : X×A →R as: Qπ(x,a) := R(x,a)+γEx′∼P(x,a)Vπ(x′). The goal of an RL agent is to ﬁnd a policy π∗:= maxπVπ (which is guaranteed to exist); for notational convenience we denote V∗:= Vπ∗ and Q∗:= Qπ∗ . In online RL the agent achieves this by iteratively improving an initial policy π0: {π0,π1,··· ,πt,···} and using these intermediate policies to collect new experience from the environment in the form of transitions (x,a,r,x ′), where a ∼πt(x), r = R(x,a), and x′∼P(x,a). These transitions constitute the dataset the agent uses to improve its policies. In other words, the learning proocess is a type of closed feedback loop : an agent’s policy directly affects the data gathered from the environment, which in turn directly affects how the agent updates its policy. When X is very large, it is impractical to store Vπ and Qπ in a table, so a function approximator Vθ ≈Vπ (where θare the approximator’s parameters) is employed instead. This function approximator is usually one or more deep networks, and this type of RL is known as deep RL (DRL). DRL algorithms can be broadly categorized into two groups: Value-based: The function Qπ is approximated by a deep network Qθ. The policy is directly induced from the value 2∆(X) denotes the set of probability distributions over a ﬁnite set X. estimate via πt(x) = arg maxa∈A Qθt (x,a)3. The parame- ters θare trained using a temporal difference loss (based on Equation 1) from transitions sampled from D: L(θ) = E(x,a,r,x′)∼D [ Qθ(x,a) −(r+ γmax a′∈A Q¯θ(x′,a′)) ] (2) Here, ¯θs a copy of θthat is infrequently synced with θfor more stable training (Mnih et al., 2015). These methods are typically employed for discrete control environments, where there is a ﬁnite (and relatively small) set of actions (e.g. Atari games (Bellemare et al., 2013)). Policy-gradient: In contrast to value-based methods where the policy is implicitly improved by virtue of improving Qθ, policy-gradient methods maintain and directly improve upon a policy πψ parameterized by ψ. These methods typi- cally still make use of a value estimate Qθ as part of their learning process, and are thus often referred to as actor-critic methods (where πψ is the actor and Qθ the critic). Two po- tential advantages of these methods is that they can be more forgiving of errors in the Qθ estimates, and they can handle continuous action spaces (for instance, by having πψ(x) output mean and variance parameters from which actions may be sampled). These methods are typically employed for continuous control environments, where the action space is continuous (e.g. MuJoCo (Todorov et al., 2012)). 3. Experimental setup DRL algorithms We investigate both value-based and policy-gradient methods. We chose DQN (Mnih et al., 2015) as the value-based algorithm, as it is the algorithm that ﬁrst spurred the ﬁeld of DRL, and has thus been extensively stud- ied and extended. We chose two actor-critic algorithms for our investigations: an on-policy algorithm (PPO (Schulman et al., 2017)) and an off-policy one (SAC (Haarnoja et al., 2018)); both are generally considered to be state-of-the-art for many domains. Environments For discrete-control we focus on three classic control environments (CartPole, Acrobot, and Moun- tainCar) as well as 15 games from the ALE Atari suite (Bellemare et al., 2013) (see subsection A.4 for game selec- tion details). For continuous-control we use ﬁve environ- ments of varying difﬁculty from the MuJoCo suite (Todorov et al., 2012) (HalfCheetah, Hopper, Walker2d, Ant, and Humanoid). Rewards obtained by DRL algorithms have notoriously high variance (Agarwal et al., 2021). Therefore we repeat each experiment with at least 10 different seeds 3Although there are other mechanisms for deﬁning a policy, such as using a softmax, and there are exploration strategies to consider, we present only the argmax setup for simplicity, as the other variants are mostly orthogonal to our analyses.The State of Sparse Training in Deep Reinforcement Learning 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score Figure 1.IQM plots for networks at 90% sparsity for various architecture and algorithm combinations. SAC and PPO are averaged over 5 MuJoCo environments, whereas DQN is averaged over 15 Atari environments. Results at different sparsities can be found at Appendix E. ”Dense: 100%” corresponds to the standard dense model. Atari scores were normalized using human performance per game. MuJoCo scores were normalized using the average returns obtained by the Dense: 100% SAC agent per game. and report the average reward obtained over the last 10% of evaluations. We also provide 95% conﬁdence intervals in all plots. See subsection A.1 for additional details. Training For each sparse training algorithm considered ( Pruning , Static, RigL, and SET) we train poli- cies ranging between 50% to 99% sparsity. To ensure a fair comparison between algorithms, we performed a hy- per parameter sweep for each algorithm separately. The exception is DQN experiments on Atari for which it was too computationally expensive to do a full hyper-parameter sweep and we used values found in previous experiments instead. Sparse results in these environments may therefore be conservative compared to the well tuned dense baseline. In addition to training the standard dense networks used in the literature, we also train smaller dense networks by scaling down layer widths to approximately match the pa- rameter counts of the sparse networks, thereby providing a ”parameter-equivalent” dense baseline. We share details of the hyper parameter sweeps and hyper parameters used for each algorithm in subsection A.3. Code Our code is built upon the TF-Agents (Guadarrama et al., 2018), Dopamine (Castro et al., 2018), and RigL (Evci et al., 2020) codebases. We use rliable (Agarwal et al., 2021) to calculate the interquartile mean (IQM) and plot the results. The IQM is calculated by discarding the bottom and top 25% of normalized scores aggregated from multiple runs and environments, then calculating the mean (reported with 95% conﬁdence intervals) over the remaining 50% runs. Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl 4. The State of Sparse Networks in Deep RL We begin by presenting the outcome of our analyses in Figure 1 and Figure 2 for DQN (Atari), PPO and SAC (Mu- JoCo). Figure 1 presents the IQM at 90% sparsity, whilst in Figure 2 we evaluate ﬁnal performance relative to the number of parameters. We share results for classic control, 2 additional MuJoCo and 12 additional Atari environments in Appendix B. Three main conclusions emerge; (1) In most cases performance obtained by sparse networks signiﬁcantly exceeds that of their dense counterparts with a comparable number of parameters. Critically, in more difﬁcult environ- ments requiring larger networks (e.g. Humanoid, Atari), sparse networks can be obtained with efﬁcient sparse train- ing methods. (2) It is possible to train sparse networks with up to 80-90% fewer parameters and without loss in perfor- mance compared to the standard dense model. (3) Gradient based growing (i.e. RigL) seems to have limited impact on the performance of sparse networks. Next, we discuss each of these points in detail. Sparse networks perform better.Inline with previous ob- servations made in speech (Kalchbrenner et al., 2018), natu-The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.700.800.90 0.950.96 0.97 0.98 SAC / Walker2d-v2 104 105 #Params 0 2000 4000 6000 8000 10000 12000 0.500.700.800.90 0.950.96 0.97 0.98 SAC / HalfCheetah-v2 104 105 #Params 1000 2000 3000 4000 5000 0.500.700.800.90 0.95 0.96 0.970.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.98 0.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.70 0.800.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 500 1000 1500 2000 2500 0.50 0.70 0.80 0.90 0.950.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 2.Comparison of the ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (CNN) (row-2) SAC on MuJoCo, (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% (annotated on the pruning curve) for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals. See Appendix B for results on additional environments. 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 1000 2000 3000 4000 5000Reward Walker2d-v2 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 2000 4000 6000 8000 10000 12000 HalfCheetah-v2 algorithm dense rigl % full dense params 100 20 10 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 1000 2000 3000 4000 5000 Humanoid-v2 Figure 3.Evaluating how varying the actor-critic parameter ratio affects performance for a given parameter budget on different environment with policies trained using SAC. % of parameters allocated to the actor network is reported on the x axis. In SAC the parameter count of both critic networks is summed to give the overall critic parameter count. The vertical line corresponds the the standard parameter split and the horizontal line to the full dense training reward.The State of Sparse Training in Deep Reinforcement Learning ral language modelling (Li et al., 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse net- works found by pruning achieve signiﬁcantly higher rewards than the dense baseline. However training these sparse net- works from scratch ( static) performs poorly. DST algo- rithms (RigL and SET) improve over static signiﬁcantly, however often fall short of matching the pruning perfor- mance. Critically, we observe that for more difﬁcult environment requiring larger networks such as Humanoid, MsPacman, Qbert and Pong, sparse networks found by efﬁcient DST algorithms exceed the performance of the dense baseline. How sparse?Next we asked how much sparsity is possible without loss in performance relative to that of the standard dense model (denoted by Dense:100% in Figure 1 and by the horizontal lines in Figure 2). We ﬁnd that on average DST algorithms maintain performance up to 90% sparsity using SAC (Figure 1 top left) or DQN ( Figure 1 bottom row), after which performance drops. However performance is variable. For example, DST algo- rithms maintain performance especially well in MsPacman and Humanoid. Whereas in Qbert none of the methods are able to match the performance of the standard dense model at any of the examined levels of sparsity. In the Atari environments, training a ResNet (He et al., 2015) following the architecture from Espeholt et al. (2018) instead of the standard CNN alone provided about 3x im- provement in IQM scores. We were also surprised to see that pruning at 90% sparsity exceeds the performance of the standard ResNet model. These observations indicate that while sparse training can bring very signiﬁcant efﬁciency gains in some environments, it is not a guaranteed beneﬁt. Unlike supervised learning, expected gains likely depend on both task and network, and merits further inquiry. RigL and SET:For most sparsities (50% - 95%) we ob- serve little difference between these two sparse training algo- rithms. At very high sparsities, RigL may outperform SET. The difference can be large (e.g. MsPacman), but is more often moderate (e.g. Pong) or negligible (e.g. Humanoid, Qbert) with overlapping conﬁdence intervals. This suggests that the gradient signal used by RigL may be less informa- tive in the DRL setting compared to image classiﬁcation, where it obtains state-of-the-art performance and consis- tently outperforms SET. Understanding this phenomenon could be a promising direction for improving sparse training methods for DRL. Perhaps unsurprisingly, the clarity of the differences be- tween sparse and dense training is affected by the stability of the underlying RL algorithm. Our results using SAC, designed for stability, were the clearest, as were the DQN results. In contrast, our results using PPO which has much higher variance, were less stark. For this reason, we used SAC and DQN when studying the different aspects of sparse agents in the rest of this work. 5. Where should sparsity be distributed? When searching for efﬁcient network architectures for DRL it is natural to ask where sparsity is best allocated. To that end, we consider both how to distribute parameters between network types and as well as within them. Actor or Critic? Although in value-based agents such as DQN there is a single network, in actor-critic methods such as PPO and SAC there are at least two: an actor and a critic network. It is believed that the underlying functions these networks approximate (e.g. a value function vs. a policy) may have signiﬁcantly different levels of complexity, and this complexity likely varies across environments. Actor and critic typically have near-identical network architectures. However, for a given parameter budget it is not clear that this is the best strategy, as the complexity of the functions being approximated may vary signiﬁcantly. We thus seek to understand how performance changes as the parameter ratio between the actor and critic is varied for a given parameter budget. In Figure 3 we assess three parameter budgets: 100%, 20% and 10% of the standard dense parameter count, and two training regimes, dense and sparse. Given the observed similarity in performance between RigL and SET in Figure 2, we selected one method, RigL, for this analysis. We observe that assigning a low proportion of parameters to the critic (10 - 20%) incurs a high performance cost across all regimes. When parameters are more scarce, in 20% and 10% of standard dense settings, performance degradation is highest. This effect is not symmetric. Reducing the actor parameters to just 10% rarely affects performance compared to the default actor-critic split of 34:66 (vertical line). Interestingly the default split appears well tuned, achieving the best performance in most settings. However in the more challenging Humanoid environment we see that for smaller dense networks, reducing the actor parameters to just 10% yields the best performance. Sparse networks follow a simi- lar trend, but we notice that they appear to be more sensitive to the parameter ratio, especially at higher sparsities. Overall this suggests that the value function is the more complex function to approximate in these settings, bene- ﬁting from the lion’s share of parameters. It also suggests that tuning the parameter ratio may improve performance. Furthermore, FLOPs at evaluation time is determined only by the actor network. Since the actor appears to be easier to compress, this suggests large potential FLOPs savings for real-time usage of these agents. Finally, this approachThe State of Sparse Training in Deep Reinforcement Learning 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0 10 6  10 5  10 4  10 3  10 2 Weight Decay 0 1000 2000 3000 4000 5000 6000Reward Walker2d Dense Pruning Rigl Set Static Static Rigl Set Sparsity-aware Initiliazation 0 1000 2000 3000 4000 5000 6000Reward Walker-2d False (ERK) True (ERK) False (Uniform) True (Uniform) Figure 4.Sensitivity analysis on policies trained with SAC: (left) uniform vs ERK sparsity distributions, (center) weight decay, (right) sparsity-aware vs dense initialization. We use 80% (ERK) sparse networks in all plots unless noted otherwise. Plots for the remaining hyper-parameters are shared in Appendix D. could be used to better understand the relative complexity of policies and values functions across different environments. Within network sparsity In Figure 4 (left) we turn our attention to the question of distributing parameters within networks and compare two strategies; uniform and ERK (Evci et al., 2020). Given a target sparsity of, say 90%, uniform achieves this by making each layer 90% sparse; ERK distributes them proportional to the sum of its dimen- sion, which has the effect of making large layers relatively more sparse than the smaller ones. Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.2 for further discussion). On the other hand, ERK has no effect on the FLOPs count of fully connected networks used in MuJoCo environments. Our results show that ERK sig- niﬁcantly improves performance over uniform sparsity and thus we use ERK distribution in all of our experiments and share results with uniform distribution in Appendix C. We hypothesize the advantage of ERK is because it leaves input and output layers relatively more dense, since they typ- ically have few incoming our outgoing connections, and this enables the network to make better use of (a) the observation and (b) the learned representations at the highest layers in the network. It is interesting to observe that maintaining a dense output layer is one of the key design decisions made by Sokar et al. (2021) for their proposed algorithm. 6. Sensitivity analysis In this section we assess the sensitivity of some key hyper- parameters for sparse training. Plots and commentary for the remaining hyper-parameters (drop fraction, topology update interval, and batch size) are shared in Appendix D. Weight decay In Figure 4 (center) we evaluate the effect of weight decay and ﬁnd that a small amount of weight decay is beneﬁcial for pruning, RigL, and SET. This is to be expected since network topology choices are made based on weight magnitude, although we do note that the improvements are quite minor. Surprisingly weight decay seems to help dense even though it is not often used in DRL. Findings: We recommend using small weight decay. Sparsity-aware initialization In Figure 4 (right) we eval- uate the effect of adjusting layer weight initialization based on a layer’s sparsity on static, RigL and SET. A common approach to initialization is to scale a weight’s initializa- tion inversely by the square root of the number of incoming connections. Consequently, when we drop incoming con- nections, the initialization distribution should be scaled pro- portionately to the number of incoming connections (Evci et al., 2022). Figure 4 (right) shows that this sparsity-aware initialization consistently improves performance when us- ing uniform distribution over layer sparsities. However the difference disappears when using ERK for RigL and SET and may even harm performance for static. Findings: Performance is not sensitive to sparsity-aware ini- tialization when using ERK and helps when using uniform layer sparsity. For RigL and SET we recommend always using sparsity-aware weight initialization (since it never ap- pears to harm performance) but for static this may depend on layer sparsity. 7. Signal-to-noise ratio in DRL environments Variance reduction is key to training deep models and of- ten achieved through using momentum based optimizers (Schmidt et al., 2011; Kingma & Ba, 2015a). However when new connections are grown such averages are not available, therefore noise in the gradients can provide mis- leading signals. In Figure 5 we share the signal-to-noise ratio (SNR) for the Classic control and MuJoCo environ- ments over the course of training. SNR is calculated as |µ| σ where µis the mean and σis the standard deviation of gradients over a mini-batch. A low SNR means the signalThe State of Sparse Training in Deep Reinforcement Learning 0 20000 40000 60000 80000 100000 Steps 10 3 10 2 10 1 Average SNR train_eval.env_name MountainCar-v0 CartPole-v0 Acrobot-v1 init_masks.sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 2 10 1 Average SNR Actor Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 1 Average SNR Critic Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 Figure 5.Signal-to-noise (SNR) ratio comparison of different gradient based DRL algorithms. We calculate SNR for every parameter in corresponding network (including the inactive/pruned weights) and report the mean SNR value. (left) DQN networks when training on classic control environments. SAC actor (center) and critic (right) networks during the training on MuJoCo environments. is dominated by the variance and thus the mean (the sig- nal) is uninformative. We calculate SNR for all parameters separately and report the mean. Mini-batch gradients can have average SNR values as low as 0.01 starting early in training. Higher sparsities seem to cause lower SNR values. Similarly, actor networks have lower SNR. Findings: We ﬁnd the average SNR for gradients to decrease with sparsity, potentially explaining the difﬁculty of using gradient based growing criteria in sparse training. 8. Are sparse networks robust to noise? Sparse neural networks can improve results on primary metrics such accuracy and rewards, yet they might have some unexpected behaviours in other aspects (Hooker et al., 2020). In Figure 6 we assess the effect of adding in- creasing amounts of noise to the observations and mea- suring their effect on a trained policy. Noise was sampled ∼N(0,σ),σ ∈[0,1,..., 30], quantized to an integer, and added to each observation’s pixel values (∈[0,255]) before normalization. Noise was sampled independently per pixel. We look at three data regimes; 100%, 50% and 10% of the standard dense model parameter count and compare dense and sparse training (RigL and SET). We made an effort to select policies with comparable performance for all the methods, chosen from the set of all policies trained during this work. We observe that (1) smaller models are generally more ro- bust to high noise than larger models, (2) sparse models are more robust to high noise than dense models on average, and (3) in most cases there are minimal differences when the noise is low. We can see that in the very low data regime (10% full pa- rameter count) policies trained using RigL are more robust to high noise compared with their dense counterparts, a fact observed across every environment. In the moderate data regime (50% full parameter count) the ordering is more mixed. In Qbert the dense model is most robust but the picture is reversed for Pong and McPacman. Finally, SET appears less robust to high noise than RigL, although we note this is not the case for Pong at 50% density. Although a preliminary analysis, it does suggest that sparse training can produce networks that are more robust to obser- vational noise, even when experienced post-training. 9. Related work Sparse training Though research on pruning has a rel- atively long story by deep learning standards (Mozer & Smolensky, 1989; Sietsma & Dow, 1988; Han et al., 2015; Molchanov et al., 2017; Louizos et al., 2017), training sparse networks from scratch has only recently gained popularity. Goyal et al. (2017), Frankle & Carbin (2019), and Evci et al. (2019) showed that training sparse networks from a ran- dom initialization is difﬁcult compared to dense neural net- works. Despite this, various approaches have been recently proposed to improve sparse training, most notably lottery tickets (Frankle et al., 2019; Zhou et al., 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; Liu & Zenke, 2020). Solutions that focus on initialization alone have been shown to be ineffective for contemporary models (Evci et al., 2022; Frankle et al., 2020), possibly due to the catapult mechanism observed early in training (Lewkowycz et al., 2020). For an in-depth survey on the topic, please see Hoeﬂer et al. (2021). Sparse networks in RL Livne & Cohen (2020) used pruning as an intermediary step to guide the width of dense neural networks for DQN and A2C agents. Most of the work investigating the use of sparse sparse training in RL are in the context of the lottery ticket hypothesis; Morcos et al. (2019) studied the existence of lucky sparse initializa- tions using pruning and late-rewinding; Hasani et al. (2020) proposed an interesting approach by repurposing the sparseThe State of Sparse Training in Deep Reinforcement Learning 0 5 10 15 20 25 30 Observation noise StdDev 500 750 1000 1250 1500 1750 2000 2250Average return MsPacman 0 5 10 15 20 25 30 Observation noise StdDev 20 15 10 5 0 5 10 15 20 Average return Pong Density 100 50 10 Method Dense RigL SET 0 5 10 15 20 25 30 Observation noise StdDev 0 1000 2000 3000 4000 5000 6000Average return Qbert Figure 6.Robustness to observation noise. We test the robustness of networks trained using sparse and dense methods (denoted by line style) by adding Gaussian noise to the observations. We examine three parameter regimes, 100% (blue), 50% (pink) and 10% (purple) of the standard dense model parameter count. All policies were trained using DQN. circuitry of the C. elegans soil-worm for RL tasks; Vischer et al. (2021) observed that the success of such initializations dependens heavily on selecting correct features for the in- put data, and not on any general qualities of the different initializations. Lee et al. (2021) proposed the use of block- circulant masks during early steps of training to improve the efﬁciency of pruning on TD3 agents, while Arnob et al. (2021) applied one-shot pruning algorithms in an ofﬂine-RL setting. Perhaps the work closest to ours is the algorithm proposed by Sokar et al. (2021), where authors applied the SET algorithm for end-to-end training of sparse networks in two actor-critic algorithms (TD3 and SAC). By a carefully chosen topology update schedule and dynamic architecture design, the proposed algorithm was able to match the dense network with a sparsity of around 50%. Novel architectures in DRL A number of works have focused on evolving network architectures for RL policies. Nadizar et al. (2021) applied pruning together with evo- lution algorithms. Whiteson & Stone (2006) combined NEAT (Stanley & Miikkulainen, 2002) with Q-learning (Watkins, 1989) to evolve better learners, Gaier & Ha (2019) evolved strong architectural priors, resulting in networks that could solve tasks with a single randomly initialized shared weight, whilst Tang et al. (2020) evolve compact self-attention architectures as a form of indirect network encoding. Zambaldi et al. (2019) similarly explored self- attention enabling agents to perform relational reasoning and achieve state-of-the-art performance on the majority of StarCraft II mini-games. Another line of research seeks to improve the stability (Parisotto et al., 2020) and efﬁciency (Parisotto & Salakhutdinov, 2021) of transformers applied to DRL, whilst Shah & Kumar (2021) explore the utility of using features extracted from a pre-trained Resnet in the standard DRL pipeline. Consistent with our observations in this work, Ha & Schmidhuber (2018) showed it is possible to train very compact controllers (i.e. actors) albeit in a the context of model-based instead of the model-free RL setting considered here. 10. Discussion and Conclusion In this work we sought to understand the state of sparse training for DRL by applying pruning, static, SET and RigL to DQN, PPO, and SAC agents trained on a variety of envi- ronments. We found sparse training methods to be a drop-in alternative for their dense counterparts providing better re- sults for the same parameter count. From a practical stand- point we made recommendations regarding hyper-parameter settings and showed that non-uniform sparse initialization combined with tuning actor:critic parameter ratios improves performance. We hope this work establishes a useful foundation for fu- ture research into sparse DRL algorithms and highlights a number of interesting research questions. In contrast to the computer vision domain, we observe that RigL fails to match pruning results. Low SNR in high sparsity regimes offers a clue but more work is needed to understand this phenomena. Our results in section 8 also suggest that sparse networks may aid in generalization and robustness to obser- vational noise; this is an active area of interest and research in the DRL community, so a more thorough understanding could result in important algorithmic advances. Acknowledgements We thank Fabian Pedregosa, Rishabh Agarwal, and Adrien Ali Ta¨ıga for their helpful feedback on the manuscript, Oscar Ramirez for his help with on the TF-Agents codebase, and Trevor Gale and Sara Hooker for inspiring the title of this work. We also thank Brain Montreal RL team for their useful feedback on an early version of this work. Finally, we thank Bram Grooten for pointing out Degrave et al. (2022) and their contribution to the motivation for this work.The State of Sparse Training in Deep Reinforcement Learning References Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 2021. Arnob, S. Y ., Ohib, R., Plis, S., and Precup, D. Single- shot pruning for ofﬂine reinforcement learning. ArXiv, abs/2112.15579, 2021. Bellec, G., Kappel, D., Maass, W., and Legenstein, R. A. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations, 2018. Bellemare, M., Candido, S., Castro, P., Gong, J., Machado, M., Moitra, S., Ponda, S., and Wang, Z. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588:77–82, 12 2020. doi: 10.1038/ s41586-020-2939-8. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The Arcade Learning Environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, June 2013. Berner, C., Brockman, G., Chan, B., Cheung, V ., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., J ´ozefowicz, R., Gray, S., Olsson, C., Pa- chocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. URL http://arxiv.org/ abs/1912.06680. Blalock, D., Ortiz, J. J. G., Frankle, J., and Guttag, J. What is the state of neural network pruning? ArXiv, 2020. URL https://arxiv.org/abs/2003.03033. Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Belle- mare, M. G. Dopamine: A research framework for deep reinforcement learning. CoRR, abs/1812.06110, 2018. URL http://arxiv.org/abs/1812.06110. Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de las Casas, D., Donner, C., Fritz, L., Galperti, C., Hu- ber, A., Keeling, J., Tsimpoukelli, M., Kay, J., Merle, A., Moret, J.-M., Noury, S., Pesamosca, F., Pfau, D., Sauter, O., Sommariva, C., Coda, S., Duval, B., Fasoli, A., Kohli, P., Kavukcuoglu, K., Hassabis, D., and Riedmiller, M. Magnetic control of tokamak plasmas through deep rein- forcement learning. Nature, 2022. Dettmers, T. and Zettlemoyer, L. Sparse networks from scratch: Faster training without losing performance. ArXiv, 2019. URL http://arxiv.org/abs/1907. 04840. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V ., Ward, T., Doron, Y ., Firoiu, V ., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K. IMPALA: scalable dis- tributed deep-rl with importance weighted actor-learner architectures. CoRR, 2018. Evci, U., Pedregosa, F., Gomez, A. N., and Elsen, E. The difﬁculty of training sparse neural networks. ArXiv, 2019. URL http://arxiv.org/abs/1906.10732. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners. In Pro- ceedings of Machine Learning and Systems 2020, 2020. Evci, U., Ioannou, Y . A., Keskin, C., and Dauphin, Y . Gradi- ent ﬂow in sparse neural networks and how lottery tickets win. In AAAI Conference on Artiﬁcial Intelligence, 2022. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th Interna- tional Conference on Learning Representations (ICLR), 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis. ArXiv, 2019. URL https://arxiv.org/abs/1903.01611. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Pruning neural networks at initialization: Why are we missing the mark? ArXiv, 2020. URL https: //arxiv.org/abs/2009.08576. Gaier, A. and Ha, D. Weight agnostic neural networks. 2019. URL https://weightagnostic.github. io. https://weightagnostic.github.io. Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. ArXiv, 2019. URL http: //arxiv.org/abs/1902.09574. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Guadarrama, S., Korattikara, A., Ramirez, O., Castro, P., Holly, E., Fishman, S., Wang, K., Gonina, E., Wu, N., Kokiopoulou, E., Sbaiz, L., Smith, J., Bart ´ok, G., Berent, J., Harris, C., Vanhoucke, V ., and Brevdo, E. TF-Agents: A library for reinforcement learning in tensorﬂow. https://github.com/tensorflow/ agents, 2018. URL https://github.com/ tensorflow/agents. [Online; accessed 25-June- 2019].The State of Sparse Training in Deep Reinforcement Learning Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings. neurips.cc/paper/2018/file/ 2de5d16682c3c35007e4e92982f1a2ba-Paper. pdf. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor. In International Conference on Machine Learning (ICML), 2018. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efﬁcient neural network. In Advances in neural information processing systems, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. EIE: Efﬁcient Inference Engine on compressed deep neural network. In Proceedings of the 43rd International Symposium on Computer Architecture, 2016. Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu, R. A natural lottery ticket winner: Reinforcement learning with ordinary neural circuits. In Proceedings of the 37th International Conference on Machine Learning. PMLR, 2020. URL https://proceedings.mlr.press/ v119/hasani20a.html. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. CoRR, 2015. Hoeﬂer, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A. Sparsity in deep learning: Pruning and growth for efﬁcient inference and training in neural networks. ArXiv, abs/2102.00554, 2021. Hooker, S., Courville, A. C., Clark, G., Dauphin, Y ., and Frome, A. What do compressed deep neural networks forget. arXiv: Learning, 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural au- dio synthesis. In International Conference on Machine Learning (ICML), 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015a. Kingma, D. P. and Ba, J. Adam: A method for stochas- tic optimization. In Bengio, Y . and LeCun, Y . (eds.), 3rd International Conference on Learning Representa- tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015b. URL http: //arxiv.org/abs/1412.6980. Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Im- plicit under-parameterization inhibits data-efﬁcient deep reinforcement learning. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=O9bnihsFfXU. Kusupati, A., Ramanujan, V ., Somani, R., Wortsman, M., Jain, P., Kakade, S., and Farhadi, A. Soft threshold weight reparameterization for learnable sparsity. In Proceedings of the International Conference on Machine Learning , 2020. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.),Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207–1216, Stan- ford, CA, 2000. Morgan Kaufmann. Lee, J., Kim, S., Kim, S., Jo, W., and Yoo, H.-J. Gst: Group- sparse training for accelerating deep reinforcement learn- ing. ArXiv, abs/2101.09650, 2021. Lee, N., Ajanthan, T., and Torr, P. H. S. SNIP: Single-shot Network Pruning based on Connection Sensitivity. In International Conference on Learning Representations (ICLR), 2019, 2019. Lewkowycz, A., Bahri, Y ., Dyer, E., Sohl-Dickstein, J., and Gur-Ari, G. The large learning rate phase of deep learning: the catapult mechanism. Arxiv, 2020. URL https://arxiv.org/pdf/2003.02218. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train large, then compress: Re- thinking model size for efﬁcient training and inference of transformers. ArXiv, abs/2002.11794, 2020. Liu, S., Yin, L., Mocanu, D. C., and Pechenizkiy, M. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In ICML, 2021. Liu, T. and Zenke, F. Finding trainable sparse networks through neural tangent transfer. In ICML, 2020. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. In International Conference on Learning Representations, 2019. Livne, D. and Cohen, K. Pops: Policy pruning and shrink- ing for deep reinforcement learning. IEEE Journal of Selected Topics in Signal Processing, 14:789–801, 2020. Louizos, C., Ullrich, K., and Welling, M. Bayesian compres- sion for deep learning. InAdvances in Neural Information Processing Systems, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- ness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,The State of Sparse Training in Deep Reinforcement Learning Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier- stra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540): 529–533, 2015. Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of arti- ﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 2018. Molchanov, D., Ashukha, A., and Vetrov, D. P. Variational Dropout Sparsiﬁes Deep Neural Networks. In Proceed- ings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 Au- gust 2017, 2017. Morcos, A., Yu, H., Paganini, M., and Tian, Y . One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Information Processing Systems, 2019. Mostafa, H. and Wang, X. Parameter efﬁcient train- ing of deep convolutional neural networks by dynamic sparse reparameterization. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , 2019. URL http://proceedings.mlr.press/ v97/mostafa19a.html. Mozer, M. C. and Smolensky, P. Skeletonization: A tech- nique for trimming the fat from a network via relevance assessment. In Advances in Neural Information Process- ing Systems 1, 1989. Nadizar, G., Medvet, E., Pellegrino, F. A., Zullich, M., and Nichele, S. On the effects of pruning on evolved neural controllers for soft robots. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, 2021. Nikishin, E., Schwarzer, M., D’Oro, P., Bacon, P.-L., and Courville, A. The primacy bias in deep reinforcement learning. In Proceedings of the Thirty-ninth International Conference on Machine Learning (ICML’22), 2022. Parisotto, E. and Salakhutdinov, R. Efﬁcient transformers in reinforcement learning using actor-learner distillation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=uR9LaO_QxF. Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., Botvinick, M., Heess, N., and Hadsell, R. Stabilizing transformers for reinforcement learning. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7487–7498. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/parisotto20a.html. Peste, A., Ioﬁnova, E., Vladu, A., and Alistarh, D. Ac/dc: Alternating compressed/decompressed training of deep neural networks. ArXiv, abs/2106.12379, 2021. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY , USA, 1st edition, 1994. ISBN 0471619779. Schmidt, M., Roux, N. L., and Bach, F. Convergence rates of inexact proximal-gradient methods for convex optimiza- tion. In Proceedings of the 24th International Conference on Neural Information Processing Systems, 2011. Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. In Bengio, Y . and Le- Cun, Y . (eds.),4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506.02438. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. Shah, R. M. and Kumar, V . Rrl: Resnet as representation for reinforcement learning. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research, pp. 9465–9476. PMLR, 18– 24 Jul 2021. URL https://proceedings.mlr. press/v139/shah21a.html. Sietsma, J. and Dow, R. J. Neural net pruning-why and how. In IEEE International Conference on Neural Networks, 1988. doi: 10.1109/ICNN.1988.23864. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V ., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nature, 529:484–503, 2016. URL http://www.nature.com/nature/journal/ v529/n7587/full/nature16961.html. Sokar, G., Mocanu, E., Mocanu, D. C., Pechenizkiy, M., and Stone, P. Dynamic sparse training for deep reinforcement learning. ArXiv, abs/2106.04217, 2021.The State of Sparse Training in Deep Reinforcement Learning Stanley, K. O. and Miikkulainen, R. Evolving neural networks through augmenting topologies. Evol. Com- put., 10(2):99–127, jun 2002. ISSN 1063-6560. doi: 10.1162/106365602320169811. URL https://doi. org/10.1162/106365602320169811. Tanaka, H., Kunin, D., Yamins, D. L. K., and Ganguli, S. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. ArXiv, 2020. URL https: //arxiv.org/abs/2006.05467. Tang, Y ., Nguyen, D., and Ha, D. Neuroevolution of self- interpretable agents. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ’20, pp. 414–424, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 9781450371285. doi: 10. 1145/3377930.3389847. URL https://doi.org/ 10.1145/3377930.3389847. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012. doi: 10.1109/IROS.2012.6386109. Vischer, M. A., Lange, R., and Sprekeler, H. On lottery tickets and minimal task representations in deep rein- forcement learning. ArXiv, abs/2105.01648, 2021. Wang, C., Zhang, G., and Grosse, R. Picking winning tickets before training by preserving gradient ﬂow. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=SkgsACVKPH. Watkins, C. J. C. H. Learning from Delayed Re- wards. PhD thesis, King’s College, Cambridge, UK, May 1989. URL http://www.cs.rhul.ac.uk/ ˜chrisw/new_thesis.pdf. Whiteson, S. and Stone, P. Evolutionary function approxi- mation for reinforcement learning. Journal of Machine Learning Research, 7(31):877–917, 2006. URL http: //jmlr.org/papers/v7/whiteson06a.html. Wortsman, M., Farhadi, A., and Rastegari, M. Discover- ing neural wirings. In Advances in Neural Information Processing Systems, 2019. Zambaldi, V ., Raposo, D., Santoro, A., Bapst, V ., Li, Y ., Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V ., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. Deep rein- forcement learning with relational inductive biases. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=HkxaFoC9KQ. Zhou, H., Lan, J., Liu, R., and Yosinski, J. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Ad- vances in Neural Information Processing Systems, 2019. Zhu, M. and Gupta, S. To Prune, or Not to Prune: Explor- ing the Efﬁcacy of Pruning for Model Compression. In International Conference on Learning Representations Workshop, 2018.The State of Sparse Training in Deep Reinforcement Learning A. Experimental Details A.1. Training schedule During training all agents are allowed M environment transitions, with policies being evaluated for Kepisodes / steps every N environment frames, where the values vary per suite and shared below. Atari experiments use a frame skip of 4, following (Mnih et al., 2015) thus 1 environment step = 4 environment frames. Environment M N K K = episodes Classic control 100,000 2 ,000 20 MujoCo (SAC) 1,000,000 10 ,000 30 MujoCo (PPO) 1,000,000 2 ,000 20 K = environment steps Atari Suite 40,000,000 1 ,000,000 125 ,000 A.2. FLOPs behaviour of Sparse ERK networks in DRL As reported in Evci et al. (2020), using ERK sparsity distribution often doubles the FLOPs needed for sparse models compared using the uniform sparsity distribution. This is due to the parameter sharing in convolutional layers. The spatial dimensions, kernel size and the stride of a convolutional layer affects how many times each weight is used during the convolution which in turn determines the contribution of each weight towards the total FLOPs count. In modern CNNs, the spatial dimensions of the feature maps often decreases monotonically towards the output of the network, making the contribution of the connections in later layers to the total FLOPs count smaller. Furthermore, the size of the layers typically increases towards the output and thus ERK removes a larger proportion of the connections from these later layers compared to uniform. Consequently a network sparsiﬁed using the ERK distribution will have a larger FLOPs count compared to one sparsiﬁed using a uniform distribution. Due to the lack of parameter sharing fully connected layers used in MuJoCo and classic control experiments, sparse networks with ERK have same amount of FLOPs as the uniform. Networks used in Atari experiments, however, uses convolutional networks and thus ERK doubles the FLOPs required compared to uniform. FLOPs scaling of sparse networks with ERK distributions used for Pong game can be found in Figure 7. Atari games have differently sized action spaces (Pong has 6 actions for example), which affects the number of neurons in the last layer. However since the last layer is very small and fully connected, it should have a very little effect on the results provided here. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 106 107 FLOPs DQN-Atari CNN ERK 2x Uniform Uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 107 108 FLOPs DQN-Atari Resnet ERK 2x Uniform Uniform Figure 7.FLOPs scaling of different sparsity distributions on (left) Nature CNN and (right) Impala ResNet architectures used in MsPacman environment.The State of Sparse Training in Deep Reinforcement Learning A.3. Hyper-parameter sweep We perform a grid search over different hyper parameters used in Dense, Prune, Static, SET and RigL algorithms. Unless otherwise noted, we use hyper-parameters used in regular dense training. When pruning, we start pruning around 20% of training steps and stop when 80% of training is completed following the ﬁndings of Gale et al. (2019). We use same default hyper-parameters for SET and RigL. Fort both algorithms we start updating the mask at initialization and decay the drop fraction over the course of the training using a cosine schedule, similar to pruning stopping the updates when 80% of training is completed. We search over the following parameters: 1. Weight decay ( ): Searched over the grid [0, 1e-6, 1e-4, 1e-3]. 2. Update Interval ( ): refers to how often models are pruned or sparse topology is updated. Searched over the grid [100, 250, 500, 1000, 5000]. 3. Drop Fraction ( ): refers to the maximum percentage of parameters that are dropped and added when network topology is updated. This maximum value is decayed during training according to a cosine decay schedule. Searched over the grid [0.0,0.1,0.2,0.3,0.5]. 4. Sparsity-aware initialization ( ): refers to whether sparse models are initialized with scaled initialization or not. We repeat the hyper-parameter search for each DRL algorithm using the Acrobot (for DQN) and Walker2D (for PPO and SAC) environments. Best hyper-parameters found in these environments are then used when training in other similar environments (i.e. classic control for DQN and MuJoCO for PPO and SAC). See Table 1, Table 2, and Table 3 for the best hyper parameters found in each setting. Table 1.DQN best hyper-parameters for Classic Control from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 1,000 - True Static 1 ·10−6 - - True RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−6 1,000 0.5 True Table 2.SAC best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−4 - - - Pruning 1 ·10−4 1,000 - True Static 1 ·10−4 - - False RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−4 250 0.3 TrueThe State of Sparse Training in Deep Reinforcement Learning Table 3.PPO best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 500 - False Static 0 - - True RigL 1 ·10−4 250 0.3 True SET 1 ·10−6 100 0 True Atari hyper-parameters Due to computational constraints we did not search over hyper-parameters for the Atari environ- ments, except for a small grid-search to tune the dense ResNet. The CNN architecture from Mnih et al. (2015) has been used in many prior works thus was already well tuned. The ResNet hyper-parameter sweep for the original dense model is detailed below: 1. Weight decay:Searched over the grid [0, 1e-6, 1e-5, 1e-4]. 2. Learning rate:Searched over the grid [1e-4, 2.5e-4, 1e-3, 2.5e-3]. The ﬁnal hyper-parameters we used for the Atari environments are shown in in Table 4 for the CNN and Table 5 for the ResNet. Table 4.DQN (CNN) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 0 2.5 ·10−4 - - - Pruning 0 2.5 ·10−4 5,000 - False Static 0 2.5 ·10−4 - - True RigL 0 2.5 ·10−4 5,000 0.3 True SET 0 2.5 ·10−4 5,000 0.3 True Table 5.DQN (ResNet) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−5 1 ·10−4 - - - Pruning 1 ·10−5 1 ·10−4 5,000 - False Static 1 ·10−5 1 ·10−4 - - True RigL 1 ·10−5 1 ·10−4 5,000 0.3 True SET 1 ·10−5 1 ·10−4 5,000 0.3 True Remaining hyper-parameters Next, we include details of the DRL hyper-parameters used in all training settings for DQN (Table 6 and Table 7), SAC (Table 8), and PPO (Table 8).The State of Sparse Training in Deep Reinforcement Learning Table 6.DQN Hyperparameters/ Table format from Haarnoja et al. (2018). Parameter Value Shared optimizer Adam (Kingma & Ba, 2015b) discount (γ) 0.99 nonlinearity ReLU target smoothing coefﬁcient (τ) 1.0 gradient steps per training step 1 exploration policy epsilon greedy epsilon decay period (env steps) 2.5 ·104 Classic Control replay buffer size 105 learning rate 1 ·10−3 initial collect steps 1,000 target update interval 100 reward scale factor 1.0 gradient steps every k env steps, k = 1 ﬁnal epsilon 0.1 eval epsilon 0.1 number of samples per minibatch 128 network type MLP number of hidden dense layers 2 number of hidden units per layer 512 Atari replay buffer size 106 initial collect steps 20,000 target update interval 8000 reward scale factor 1.0 gradient steps every k env steps, k = 4 ﬁnal epsilon 0.01 eval epsilon 0.001 number of samples per minibatch 32 network type CNN or ResNetThe State of Sparse Training in Deep Reinforcement Learning Table 7.DQN Atari: CNN and ResNet Architectures Parameter Value CNN learning rate 2.5 ·10−4 Adam optimizer, epsilon 1 ·10−8 CNN Architecture number of hidden CNN layers 3 number of hidden dense layers 1 number of hidden units per dense layer 512 CNN params per layer (ﬁlters, kernel, stride) layer 1 (ﬁlters, kernel, stride) 32, 8, 4 layer 2 (ﬁlters, kernel, stride) 64, 4, 2 layer 3 (ﬁlters, kernel, stride) 64, 3, 1 ResNet learning rate 1 ·10−4 Adam optimizer, epsilon 3.125 ·10−4 ResNet Architecture number of stacks 3 number of hidden dense layers 1 number of hidden units per dense layer 512 use batch norm False ResNet stack layers num CNN layers 1 num max pooling layers 1 num residual-CNN layers 2 ResNet params per layer (ﬁlters, kernel, stride) stack 1 (ﬁlters, kernel, stride) 32, 3, 1 stack 2 (ﬁlters, kernel, stride) 64, 3, 1 stack 3 (ﬁlters, kernel, stride) 64, 3, 1The State of Sparse Training in Deep Reinforcement Learning Table 8.SAC Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 replay buffer size 106 number of hidden layers (all networks) 2 number of hidden units per layer 256 number of samples per minibatch 256 nonlinearity ReLU target smoothing coefﬁcient (τ) 0.005 target update interval 1 train every k env steps, k = 1 gradient steps per training step = 1 Hopper, Walker, Humanoid initial collect steps 1,000 HalfCheetah, Ant initial collect steps 10,000 Table 9.PPO Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 shared / separate networks separate number of hidden layers (all networks) 2 number of hidden units per layer 64 collect sequence length (batch size) 2048 minibatch size 64 num epochs 10 importance ratio clipping 0.2 use GAE (Schulman et al., 2016) True λ(GAE) 0.95 entropy regularization 0 value loss coeff 0.5 gradient clipping 0.5 A.4. Atari Game Selection Our original three games (MsPacman, Pong, Qbert) were selected to have varying levels of difﬁculty as measured by DQN’s human normalized score in Mnih et al. (2015), Figure 3. To this we added 12 games (Assault, Asterix, BeamRider, Boxing, Breakout, CrazyClimber, DemonAttack, Enduro, FishingDerby, SpaceInvaders, Tutankham, VideoPinball) selected to be roughly evenly distributed amongst the games ranked by DQN’s human normalized score in Mnih et al. (2015) with a lower cut off of approximately 100% of human performance.The State of Sparse Training in Deep Reinforcement Learning B. Sparse Scaling Plots in Other Environments Here we share results on additional environments, Acrobot, CartPole, MountainCar, Hopper, and Ant. Figure 8 compares ﬁnal reward relative to parameter count using DQN. ERK sparsity distribution was used in the top row whilst uniform was used in the bottom row. Figure 9 presents results on the two remaining MuJoCo environment, Hopper and Ant with SAC (top row) and PPO (bottom row). In Figures 10 and 11 we show sparsity scaling plots for 15 Atari games using the standard CNN and ResNet respectively. 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.700.800.900.950.960.97 0.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.700.80 0.900.950.960.970.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.70 0.80 0.900.95 0.96 0.97 0.98 DQN / MountainCar-v0 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.600.700.80 0.90 0.950.96 0.970.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.600.700.800.90 0.95 0.96 0.97 0.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.600.700.800.900.950.960.970.98 DQN / MountainCar-v0 Figure 8.DQN in the Classic Control environments with ERK network sparsity distribution (top) and uniform network sparsity (bottom). 104 105 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.900.950.960.97 0.98 SAC / Hopper-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.70 0.80 0.90 0.950.96 0.97 0.98 SAC / Ant-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000Reward 0.50 0.70 0.80 0.90 0.95 0.96 0.970.98 0.99 PPO / Hopper-v2 Dense Pruning Rigl Set Static 103 104 #Params 1000 1500 2000 2500 3000 3500 4000Reward 0.500.70 0.800.90 0.95 0.96 0.970.98 0.99 PPO / Ant-v2 Dense Pruning Rigl Set Static Figure 9.Additional MuJoCO environments (Hopper and Ant) for SAC and PPO algorithms. Networks are initialized with ERK network sparsity distribution.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 400 600 800 1000 1200Reward 0.500.80 0.900.95 0.98 0.99 Assault / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000Reward 0.500.80 0.90 0.95 0.98 0.99 Asterix / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 750 1000 1250 1500 1750 2000 2250 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 BeamRider / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20 40 60 80Reward 0.50 0.80 0.900.95 0.98 0.99 Boxing / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 10 20 30 40 50 60 70Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000Reward 0.50 0.800.90 0.95 0.98 0.99 CrazyClimber / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 200 400 600 800 1000 1200 1400Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 100 200 300 400 500 600Reward 0.50 0.800.90 0.95 0.98 0.99 Enduro / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 Reward 0.50 0.80 0.90 0.950.98 0.99 FishingDerby / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 300 400 500 600 700 800Reward 0.500.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 40 60 80 100 120Reward 0.500.800.900.95 0.98 0.99 Tutankham / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (CNN) Dense Pruning RigL SET Static Figure 10.CNN: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 500 1000 1500 2000 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 Assault / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.50 0.80 0.900.95 0.98 0.99 Asterix / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.50 0.800.90 0.95 0.98 0.99 BeamRider / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 40 20 0 20 40 60 80 100Reward 0.500.800.900.950.98 0.99 Boxing / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50 100 150 200 250 300Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.500.800.900.950.98 0.99 CrazyClimber / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 10000 20000 30000 40000 50000 60000 70000Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 200 400 600 800 1000 1200 1400Reward 0.500.80 0.900.950.980.99 Enduro / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 0 20 Reward 0.500.800.900.950.98 0.99 FishingDerby / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.80 0.90 0.95 0.98 0.99 MsPacman / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 20 10 0 10 20 Reward 0.500.800.900.950.980.99 Pong / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.500.80 0.900.95 0.98 0.99 Qbert / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000 10000Reward 0.50 0.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 25 50 75 100 125 150 175Reward 0.500.80 0.90 0.950.98 0.99 Tutankham / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (ResNet) Dense Pruning RigL SET Static Figure 11.ResNet: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning C. Additional Results with Uniform Sparsity Distribution In Figure 12 we repeat the experiments presented in Figure 2, however this time using a uniform network sparsity distribution at initialization. These plots provide further evidence as the the beneﬁt of using ERK over uniform to distribute network sparsity. 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.500.800.90 0.95 0.98 0.99 Pong / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 1000 2000 3000 4000 5000 6000 0.50 0.80 0.90 0.950.980.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 2500 0.500.80 0.90 0.95 0.98 0.99 MsPacman / DQN (CNN) 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.600.700.80 0.90 0.950.960.970.98 SAC / Walker2d-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 2000 4000 6000 8000 10000 12000Reward SAC / HalfCheetah-v2 Dense Pruning Rigl Set Static 104 105 #Params 1000 2000 3000 4000 5000 0.50 0.60 0.70 0.800.900.95 0.96 0.97 0.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.980.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.700.80 0.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 250 500 750 1000 1250 1500 1750 2000 2250 0.500.70 0.80 0.90 0.95 0.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 12.Uniform network sparsity initialization. Comparison of ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (row-2) SAC on MuJoCo (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals.The State of Sparse Training in Deep Reinforcement Learning D. Additional Hyper-parameter Sensitivity Plots Here we share the remaining plots for our analysis on the sensitivity of sparse training algorithms to various hyper-parameters. Policies area trained using SAC. We note that different architectures, environments and training algorithms might show different curves, which we omit due to high cost of running such analysis. 0.0 0.1 0.2 0.3 0.4 0.5 Drop Fraction 0 1000 2000 3000 4000 5000 6000Reward Walker2d Rigl Set 102 103 Update Interval 0 1000 2000 3000 4000 5000 6000Reward Walker2d Pruning Rigl Set 0 200 400 600 800 1000 Batch size 0 1000 2000 3000 4000 5000 6000Reward Walker2d Set Dense RigL Static Figure 13.Sensitivity analysis: (left) Drop fraction: Comparing different drop fractions for SET and RigL at 80% sparsity. (center) Comparing the effect of topology update interval on pruning, static, and RigL. (right) The effect of batch size. Drop fraction In Figure 13 (left) we evaluate the effect of drop fraction and observe that drop fractions >0 yield a small performance improvement over drop fraction = 0, which is equivalent to static sparse training. This indicates that changing the network topology during training helps performance. Surprisingly training does not appear particularly sensitive to the drop fraction chosen, with values from 10 - 50% yielding approximately equivalent performance. It is possible that this is because the environment is relatively easy, thus leading to little separation between different settings. The large improvement that RigL and SET give over static (drop fraction = 0) in the Humanoid and Atari environments is one reason to suspect this. Findings: Drop fractions of 10 - 50% worked well for RigL, whilst a drop fraction of 30% worked best for SET. 30% therefore appears to be a reasonable default for dynamic sparse training. However, the uniformity of performance merits investigation on a harder environment in which more separation between drop fractions may be observed. Topology update interval In Figure 13 (center) we evaluate the effect of topology update interval on pruning, RigL and SET and ﬁnd that training is not particularly sensitive to it. Findings: Updating the network every 1000 environment steps appears to be a reasonable default. Batch size In Figure 13 (right) we evaluate the effect of batch size. Batch size is critical for obtaining a good estimate for the gradients during training for all methods, however for RigL it is also used when selecting new connections. We observe performance degradation for all methods when smaller batch sizes are used, with no particular additional effect on RigL. Findings: Sparse networks seems to have similar sensitivity to the batch size as the dense networks. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 400 350 300 250 200 150 100 Reward Acrobot Algorithm rigl static Distribution erk uniform Figure 14.Evaluating non uniform sparsities on HalfCheetah using SAC when all layers pruned (left) and last layer is kept dense (center). On the right we prune all layers of the DQN MLP network on the Acrobot task.The State of Sparse Training in Deep Reinforcement Learning Within network sparsity In Figure 14 we share additional analysis on within network sparsity for SAC and also on DQN in the Acrobot environment. E. Additional Interquartile Mean (IQM) Plots Mujoco SAC In Figure 15 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for SAC at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.99 Normalized Score Figure 15.SAC: IQM plots calculated over ﬁve Mujoco games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), with 10 seeds per game. Except for sparsity = 0.99 which only includes results from HalfCheetah, Hopper, and Walker2d, each with 10 seeds.The State of Sparse Training in Deep Reinforcement Learning Mujoco PPO In Figure 16 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for PPO at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 95% sparsity the IQM is calculated over four environments and for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.99 Normalized Score Figure 16.SAC: IQM plots calculated over multiple Mujoco games with 9 seeds per game. 50 - 90% sparsity include ﬁve games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), 95% sparsity includes Ant, HalfCheetah, Hopper, Walker2d, and 99% sparsity HalfCheetah, Hopper, Walker2d. Atari DQN Figure 17 presents IQM plots calculated over 15 Atari games for the standard CNN network architecture and Figure 18 presents IQM plots calculated over the same set of games for a ResNet architecture with an approximately equivalent number of parameters as the standard CNN (≈4M).The State of Sparse Training in Deep Reinforcement Learning 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.5 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.95 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.99 Human Normalized Score Figure 17.CNN: IQM plots calculated over 15 Atari Games, with 9 seeds per game. 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.5 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.95 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.99 Human Normalized Score Figure 18.ResNet: IQM plots calculated over 15 Atari Games, with 9 seeds per game.",
      "meta_data": {
        "arxiv_id": "2206.10369v1",
        "authors": [
          "Laura Graesser",
          "Utku Evci",
          "Erich Elsen",
          "Pablo Samuel Castro"
        ],
        "published_date": "2022-06-17T14:08:00Z",
        "pdf_url": "https://arxiv.org/pdf/2206.10369v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper provides a systematic investigation into applying existing sparse training techniques to Deep Reinforcement Learning (DRL) agents and environments, addressing the gap where sparse neural networks were largely unexplored in DRL compared to computer vision. The key contributions are: 1. Corroborating the finding that sparse networks perform better than dense networks for the same parameter count in the DRL domain. 2. Demonstrating the possibility of training up to 80-90% sparse networks with minimal performance loss compared to standard dense networks. 3. Showing that pruning generally achieves the best results, and dynamic sparse training (DST) significantly outperforms static sparse training. 4. Highlighting that gradient-based growth (RigL) has limited effectiveness, potentially due to low signal-to-noise ratio in DRL gradients. 5. Identifying that parameter distribution between actor/critic networks and within layers (e.g., Erdos Renyi Kernel or ERK sparsity) greatly influences performance, with the critic benefiting from a majority of parameters and ERK being superior. 6. Observing robust performance across hyperparameter variations, with sparse methods exhibiting better robustness to observational noise.",
        "methodology": "The study systematically benchmarks four sparse training algorithms: Pruning (a dense-to-sparse method used as an upper bound), Static (random pruning at initialization with a fixed structure), Sparse Evolutionary Training (SET), and Rigged Lottery (RigL) (both dynamic sparse training methods that periodically update network topology). These methods are applied to three prominent DRL agents: DQN (value-based, for discrete control), PPO (on-policy actor-critic), and SAC (off-policy actor-critic, both for continuous control). The research also analyzes various components of sparse training, including sparsity distribution strategies (uniform vs. ERK), weight decay, layer initialization (sparsity-aware vs. dense), signal-to-noise ratio for gradients, batch size, and topology update strategy and frequency.",
        "experimental_setup": "The DRL agents were evaluated across a variety of environments: three classic discrete-control environments (CartPole, Acrobot, MountainCar) and 15 games from the ALE Atari suite (e.g., MsPacman, Pong, Qbert). For continuous-control, five MuJoCo suite environments were used (HalfCheetah, Hopper, Walker2d, Ant, Humanoid). To account for high variance in DRL rewards, each experiment was repeated with at least 10 different seeds, and results were reported as the average reward over the last 10% of evaluations, along with 95% confidence intervals. The Interquartile Mean (IQM) was used for aggregate scores. Hyperparameter sweeps were performed for each algorithm, except for DQN on Atari due to computational constraints. Smaller dense networks were also trained to serve as 'parameter-equivalent' baselines. Robustness to observation noise was assessed by adding Gaussian noise with varying standard deviations (σ ∈[0,1,..., 30]) to observations of trained policies.",
        "limitations": "The computational expense limited a full hyper-parameter sweep for DQN experiments on Atari, potentially making sparse results conservative compared to the highly-tuned dense baseline. The observed efficiency gains from sparse training are not a universal benefit and appear to be task and network dependent. The gradient signal used by RigL seems less informative in the DRL setting than in image classification, possibly due to a low signal-to-noise ratio in DRL gradients, which hinders its performance in DRL compared to pruning. The clarity of observed differences between sparse and dense training is affected by the stability of the underlying RL algorithm, with higher variance algorithms like PPO showing less distinct results. For convolutional networks (e.g., in Atari environments), ERK sparsity distribution effectively doubles the FLOPs compared to uniform sparsity due to parameter sharing. The analysis of robustness to observational noise is preliminary, suggesting a more thorough understanding is needed.",
        "future_research_directions": "Future research should focus on understanding why gradient-based growing criteria (like RigL) are less effective in DRL compared to computer vision, particularly investigating the low signal-to-noise ratio in high sparsity regimes. A more thorough investigation into the generalization capabilities and robustness of sparse networks to observational noise is warranted, as this could lead to significant algorithmic advancements in DRL. Further exploration into tuning the actor-critic parameter ratio is suggested, especially given that the critic often requires more parameters and the actor appears easier to compress, which could lead to significant FLOPs savings during real-time agent usage. This parameter ratio approach could also be used to better understand the relative complexity of policies and value functions across different environments. Investigating the sensitivity of dynamic sparse training hyperparameters, such as drop fraction, in harder environments could reveal more distinct performance differences."
      }
    },
    {
      "title": "Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards",
      "abstract": "Reinforcement learning with sparse rewards is challenging because an agent\ncan rarely obtain non-zero rewards and hence, gradient-based optimization of\nparameterized policies can be incremental and slow. Recent work demonstrated\nthat using a memory buffer of previous successful trajectories can result in\nmore effective policies. However, existing methods may overly exploit past\nsuccessful experiences, which can encourage the agent to adopt sub-optimal and\nmyopic behaviors. In this work, instead of focusing on good experiences with\nlimited diversity, we propose to learn a trajectory-conditioned policy to\nfollow and expand diverse past trajectories from a memory buffer. Our method\nallows the agent to reach diverse regions in the state space and improve upon\nthe past trajectories to reach new states. We empirically show that our\napproach significantly outperforms count-based exploration methods (parametric\napproach) and self-imitation learning (parametric approach with non-parametric\nmemory) on various complex tasks with local optima. In particular, without\nusing expert demonstrations or resetting to arbitrary states, we achieve the\nstate-of-the-art scores under five billion number of frames, on challenging\nAtari games such as Montezuma's Revenge and Pitfall.",
      "full_text": "Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards Yijie Guo1 Jongwook Choi1 Marcin Moczulski2 Shengyu Feng1 Samy Bengio2 Mohammad Norouzi2 Honglak Lee2,1 1University of Michigan 2Google Brain {guoyijie,jwook,shengyuf}@umich.edu moczulski@google.com {bengio,mnorouzi,honglak}@google.com Abstract Reinforcement learning with sparse rewards is challenging because an agent can rarely obtain non-zero rewards and hence, gradient-based optimization of param- eterized policies can be incremental and slow. Recent work demonstrated that using a memory buffer of previous successful trajectories can result in more ef- fective policies. However, existing methods may overly exploit past successful experiences, which can encourage the agent to adopt sub-optimal and myopic behaviors. In this work, instead of focusing on good experiences with limited diversity, we propose to learn a trajectory-conditioned policy to follow and expand diverse past trajectories from a memory buffer. Our method allows the agent to reach diverse regions in the state space and improve upon the past trajectories to reach new states. We empirically show that our approach signiﬁcantly outperforms count-based exploration methods (parametric approach) and self-imitation learning (parametric approach with non-parametric memory) on various complex tasks with local optima. In particular, without using expert demonstrations or resetting to arbitrary states, we achieve the state-of-the-art scores under ﬁve billion number of frames, on challenging Atari games such as Montezuma’s Revenge and Pitfall. 1 Introduction Deep reinforcement learning (DRL) algorithms with parameterized policy and value function have achieved remarkable success in various complex domains [32, 49, 48]. However, tasks that require reasoning over long horizons with sparse rewards remain exceedingly challenging for the parametric approaches. In these tasks, a positive reward could only be received after a long sequence of appro- priate actions. The gradient-based updates of parameters are incremental and slow and have a global impact on all parameters, which may cause catastrophic forgetting and performance degradation. Many parametric approaches rely on recent samples and do not explore the state space systematically. They might forget the positive-reward trajectories unless the good trajectories are frequently collected. Recently, non-parametric memory from past experiences is employed in DRL algorithms to improve policy learning and sample efﬁciency. Prioritized experience replay [45] proposes to learn from past experiences by prioritizing them based on temporal-difference error. Episodic reinforcement learning [43, 22, 28], self-imitation learning [36, 19], and memory-augmented policy optimization [27] build a memory to store past good experience and thus can rapidly latch onto past successful policies when encountering with states similar to past experiences. However, the exploitation of good experiences within limited directions might hurt performance in some cases. For example on Montezuma’s Revenge (Fig. 1), if the agent exploits the past good trajectories around the yellow path, it would receive the small positive rewards quickly but it loses the chance to achieve a higher score in the long term. Therefore, in order to ﬁnd the optimal path (red), it is better to consider past experiences in diverse directions, instead of focusing only on the good trajectories which lead to myopic behaviors. Inspired by recent work on memory-augmented generative models [21, 9], we note that generating a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1907.10247v3  [cs.LG]  15 Feb 2021Myopic Optimal : pick a key (+100): open a door (+300): get a diamond (+1000)               Myopic Optimal : pick a key (+100): open a door (+300): get a diamond (+1000)               Figure 1: Left: The map of the ﬁrst level in Montezuma’s Revenge. We simplify the agent’s paths and enlarge some objects to illustrate typical exploration challenges. The agent also needs to tackle control challenges (e.g., jumping between platforms, avoiding collision with moving enemies and electric ﬁelds, etc.), but they are not highlighted here. After getting two keys, the agent can easily expense the keys to open doors in the middle via the yellow path and achieve small incremental rewards, but as each key can only be used once, the agent is unlikely to open doors at the bottom ﬂoor to clear the level. The previous SOTA fails to open the last two doors. Ours visits the left-most room at the bottom ﬂoor, gets many diamonds, and goes to the next level. Right: Comparison to CoEX [13] (previous SOTA) with high-level state embedding. In a challenging setting with random initial delay, without using expert demonstrations or resetting to arbitrary state, ours explores more rooms and achieves a signiﬁcantly higher score. new sequence by editing prototypes in external memory is easier than generating one from scratch. In an RL setting, we aim to generate new trajectories visiting novel states by editing or augmenting the trajectories stored in the memory from past experiences. We propose a novel trajectory-conditioned policy where a full sequence of states is given as the condition. Then a sequence-to-sequence model with an attention mechanism learns to ‘translate’ the demonstration trajectory to a sequence of actions and generate a new trajectory in the environment with stochasticity. The single policy could take diverse trajectories as the condition, imitate the demonstrations to reach diverse regions in the state space, and allow for ﬂexibility in the action choices to discover novel states. Our main contributions are summarized as follows. (1) We propose a novel architecture for a trajectory-conditioned policy that can ﬂexibly imitate diverse demonstration trajectories. (2) We show the importance of exploiting diverse past experiences in the memory to indirectly drive exploration, by comparing with existing approaches on various sparse-reward RL tasks with stochasticity in the environments. (3) We achieve a performance superior to the state-of-the-art under 5 billion number of frames, on hard-exploration Atari games of Montezuma’s Revenge and Pitfall, without using expert demonstrations or resetting to arbitrary states. We also demonstrate the effectiveness of our method on other benchmarks. 2 Method 2.1 Background and Notation for DTSIL In the standard RL setting, at each time step t, an agent observes a state st, selects an action at ∈A, and receives a reward rt when transitioning to a next state st+1 ∈S, where Sand Ais a set of states and actions respectively. The goal is to ﬁnd a policy πθ(a|s) parameterized by θthat maximizes the expected return Eπθ [∑T t=0 γtrt], where γ ∈(0,1] is a discount factor. In our work, instead of directly maximizing expected return, we propose a novel way to ﬁnd best demonstrations g∗with (near-)optimal return and train the policyπθ(·|g) to imitate any trajectory gin the buffer, includingg∗. We assume a state st includes the observation ot (e.g., raw pixel image) and a high-level abstract state embedding et (e.g., the agent’s location in the abstract space). The embeddinget may be available as a part of st (e.g., the physical features in the robotics domain) or may be learnable from o≤t (e.g., [13, 54] could localize the agent in Atari games, as discussed in Sec. 5). A trajectory-conditioned policy πθ(at|e≤t,ot,g) (which can be viewed as a goal-conditioned policy and denoted as πθ(·|g)) takes a sequence of state embeddingsg= {eg 1,eg 2,··· ,eg |g|}as input for a demonstration, where|g|is the length of the trajectoryg. A sequence of the agent’s past state embeddingse≤t = {e1,e2,··· ,et} is provided to determine which part of the demonstration has been followed by the agent. Together with the current observation ot, it helps to determine the correct actionat to imitate the demonstration. Our goal is to ﬁnd a set of optimal state embedding sequence(s)g∗and the policy π∗ θ(·|g) to maximize the return: g∗,θ∗≜ arg maxg,θEπθ(·|g)[∑T t=0 γtrt]. We approximately solve this joint optimization 2(a) Buffer of Diverse Trajectories Ending with Diverse States(b) Sample a Statefor Exploration or Exploitation(c) Imitate the Demonstrationand Generate a New Trajectory: state.          : trajectory ending with       .           : visitation count of       .              : trajectory.    : ending state.  ……… ……… External Memory New Entry of Novel State(d) Update Memory Buffer with Novel States Demonstration Trajectory Agent’s Trajectory Sampled Demonstration Trajectory h g 1 <latexit sha1_base64=\"IV5x0gxtjcX97WeA+Eq95FEUYwA=\">AAACrXicbVFNbxMxEHW2fJTylcKRi0WExCGE3YJEj5W4cCwSaSo1aTT2Tnat+AvbC9qu9j+0V/hl/Bu8aQ40y0iWnt+b8YznMSuFD2n6Z5Ds3bv/4OH+o4PHT54+ez48fHHmTeU4TrmRxp0z8CiFxmkQQeK5dQiKSZyx9edOn/1A54XR30JtcaGg0GIlOIRIzcrLpmiX2XI4SifpJmgfZFswIts4XR4Orue54ZVCHbgE7y+y1IZFAy4ILrE9mFceLfA1FHgRoQaFftFs5m3pm8jkdGVcPDrQDftvRQPK+1qxmKkglH5X68j/ahw0R7nTPayOF43Qtgqo+W3zVSVpMLRbCM2FQx5kHQFwJ+L8lJfggIe4tjuvB7G+anvMu+/BYfzxRugIKZgDVzdK6FyBHXeyb3uyL8GinxRoFAYneD8DnDM//Ziy2K1wptJ5vHCQfExLw1g9ptZ40fkodHRxOMp2HeuDs6NJ9mFy9PXj6OR46+Y+eUVek7ckI5/ICflCTsmUcLImN+QX+Z28T6bJPLm8TU0G25qX5E4kxV/FF9kT</latexit> h g 0 <latexit sha1_base64=\"wTm5nCDfS/xNcfBEzKW+frXUet4=\">AAACrXicbVFNbxMxEHW2fJTylcKRi0WExCGE3YJEj5W4cCwSaSo1aTT2Tnat+AvbC9qu9j+0V/hl/Bu8aQ40y0iWnt+b8YznMSuFD2n6Z5Ds3bv/4OH+o4PHT54+ez48fHHmTeU4TrmRxp0z8CiFxmkQQeK5dQiKSZyx9edOn/1A54XR30JtcaGg0GIlOIRIzcrLpmiX6XI4SifpJmgfZFswIts4XR4Orue54ZVCHbgE7y+y1IZFAy4ILrE9mFceLfA1FHgRoQaFftFs5m3pm8jkdGVcPDrQDftvRQPK+1qxmKkglH5X68j/ahw0R7nTPayOF43Qtgqo+W3zVSVpMLRbCM2FQx5kHQFwJ+L8lJfggIe4tjuvB7G+anvMu+/BYfzxRugIKZgDVzdK6FyBHXeyb3uyL8GinxRoFAYneD8DnDM//Ziy2K1wptJ5vHCQfExLw1g9ptZ40fkodHRxOMp2HeuDs6NJ9mFy9PXj6OR46+Y+eUVek7ckI5/ICflCTsmUcLImN+QX+Z28T6bJPLm8TU0G25qX5E4kxV/C1NkS</latexit> h g | g | <latexit sha1_base64=\"0I7w7KUC75/+crUsojME2zUQP+o=\">AAACsXicbVFNbxMxEHW2fJTy0RaOXCwiJA4h2m2R6LESF45FIm1RGsLYO9m14i9sL2jZ7n/ohSv9Xf03eNMcaJaRLD2/N+MZz2NWCh/S9GaQbN27/+Dh9qOdx0+ePtvd239+6k3lOE64kcadM/AohcZJEEHiuXUIikk8Y8sPnX72A50XRn8OtcWZgkKLheAQIjUtvzZFO28ui8t2vjdMx+kqaB9kazAk6ziZ7w+uLnLDK4U6cAneT7PUhlkDLggusd25qDxa4EsocBqhBoV+1qxmbunryOR0YVw8OtAV+29FA8r7WrGYqSCUflPryP9qHDRHudE9LI5mjdC2Cqj5bfNFJWkwtFsKzYVDHmQdAXAn4vyUl+CAh7i6O68HsfzV9pi334PD+OOV0BFSMAeubpTQuQI76mTf9mRfgkU/LtAoDE7wfgY4Z376EWWxW+FMpfN44SD5iJaGsXpErfGi81LoovMw23SsD04Pxtnh+ODTu+Hx0drNbfKSvCJvSEbek2PykZyQCeHEkN/kD7lODpMvybeE3aYmg3XNC3InkuVfRuXbYQ==</latexit> ... <latexit sha1_base64=\"/ps7m72IRakV+XzlF0MmYPWJgH8=\">AAACqXicbVFNb9QwEPWGr1I+2sKRi8UKCYklSgoSPVbiwrEVbLuiXVWTyWzWWn9hO6AQ5ScgcYLfxr/B2e6BbhjJ0vN7M57xvMJK4UOW/Rklt27fuXtv5/7ug4ePHu/tHzw586Z2SFM00rhZAZ6k0DQNIkiaWUegCknnxep9r59/JeeF0Z9CY2muoNJiIRBCpD6maXq1P87SbB18CPINGLNNnFwdjH5elgZrRTqgBO8v8syGeQsuCJTU7V7WnizgCiq6iFCDIj9v17N2/EVkSr4wLh4d+Jr9t6IF5X2jipipICz9ttaT/9UQNJLc6h4WR/NWaFsH0njdfFFLHgzvl8FL4QiDbCIAdCLOz3EJDjDEld14PYjV927AvP4SHMUfr4WekKJw4JpWCV0qsJNe9t1A9kuw5NOKjKLgBA4zwDnzzU94EbtVztS6jBcEiRO+NEXRTLg1XvQeCl110cN827EhODtM8zfp4enb8fHRxs0d9ow9Zy9Zzt6xY/aBnbApQ1axH+wX+528Sk6TWfL5OjUZbWqeshuR4F8+rdbA</latexit> e g 0 <latexit sha1_base64=\"fxBD3yafJ27jLEz3rWmvyIeBIiU=\">AAACq3icbVFNbxMxEHWWj5by1cKRi0WExCGNdlskeqzEhWMRpK1oQjTrnWys+At7tmhZ7W+AI/w0/g3eNAeaZSRLz+/NeMbzcqdkoDT9M0ju3L13f2f3wd7DR4+fPN0/eHYebOUFToRV1l/mEFBJgxOSpPDSeQSdK7zIV+86/eIafZDWfKLa4UxDaeRCCqBITXCefinn+8N0nK6D90G2AUO2ibP5weDntLCi0mhIKAjhKksdzRrwJIXCdm9aBXQgVlDiVYQGNIZZs5625a8iU/CF9fEY4mv234oGdAi1zmOmBlqGba0j/6sJMALVVndanMwaaVxFaMRN80WlOFnerYMX0qMgVUcAwss4PxdL8CAoLu3W6yRX39sec/iVPMYfr4WOUDL34OtGS1NocKNODm1PDktwGMYlWo3kpehngPf2WxjxPHYrva1MES8ClBjxpc3zesSdDbJzUZqyjR5m2471wfnRODseH314Mzw92bi5y16wl+w1y9hbdsreszM2YYJJ9oP9Yr+Tw+Rj8jmZ3qQmg03Nc3YrEvwLOlzYAw==</latexit> e g | g | <latexit sha1_base64=\"j24CPU+MPvtbyUn1x2bZtma+zOU=\">AAACr3icbVFNbxMxEHWWr1K+WjhysYiQOIRot0Wix0pcOBaJNEVtGma9k40Vf2HPgpbt/gckrvDD+Dd40xxolpEsPb834xnPy52SgdL0zyC5dfvO3Xs793cfPHz0+Mne/tPTYCsvcCKssv4sh4BKGpyQJIVnziPoXOE0X73r9OlX9EFa85FqhzMNpZELKYAi9QnnzVV51V6W871hOk7Xwfsg24Ah28TJfH/w46KwotJoSCgI4TxLHc0a8CSFwnb3ogroQKygxPMIDWgMs2Y9cctfRqbgC+vjMcTX7L8VDegQap3HTA20DNtaR/5XE2AEqq3utDiaNdK4itCI6+aLSnGyvFsJL6RHQaqOAISXcX4uluBBUFzcjddJrr63Peb1F/IYf7wWOkLJ3IOvGy1NocGNOjm0PTkswWEYl2g1kpeinwHe229hxPPYrfS2MkW8CFBixJc2z+sRdzbIzklpyjZ6mG071genB+PscHzw4c3w+Gjj5g57zl6wVyxjb9kxe89O2IQJptlP9ov9TrJkmlwmn69Tk8Gm5hm7EYn8C7uc2lI=</latexit> g ⇠ D <latexit sha1_base64=\"20XvuQowmn/b4wZLNQNpO6yyMXw=\">AAACunicbVFNbxMxEHWWr1K+0nLkYhEhcQjRbotEDxwqwYFjkUhbqYmisXeyseKPxZ4FltX+B34BV/hL/Bu8aQ40y0iWnt+b8YzniVKrQGn6Z5Dcun3n7r29+/sPHj56/GR4cHgeXOUlTqXTzl8KCKiVxSkp0nhZegQjNF6I9btOv/iCPihnP1Fd4txAYdVSSaBILYaHBZ8FZfjMAK0k6OZ9uxiO0km6Cd4H2RaM2DbOFgeDH7PcycqgJakhhKssLWnegCclNbb7sypgCXINBV5FaMFgmDeb4Vv+IjI5XzofjyW+Yf+taMCEUBsRM7sZw67Wkf/VJFiJeqc7LU/mjbJlRWjldfNlpTk53m2H58qjJF1HANKrOD+XK/AgKe7wxuuk1t/bHvPqM3mMP94IHaGV8ODrxiibGyjHnRzanhxWUGKYFOgMkleynwHeu69hzEXsVnhX2TxeomFyzFdOiHrMSxdUZ6qyRedhtutYH5wfTbLjydHH16PTk62be+wZe85esoy9YafsAztjUybZN/aT/WK/k7eJSFSyvk5NBtuap+xGJPQXQeTduw==</latexit> e g 1 <latexit sha1_base64=\"lJw7uE4Ai+bJhyA+7BxJAfB3l8E=\">AAACrXicbVFNbxMxEHWWj5by1ZYjF4sIiUMIuwWJHiv1wrFIpKnUpNHYO9lY8Re2t2i72v9QrvDL+Dd4tznQLCNZen5vxjOex6wUPqTpn0Hy4OGjxzu7T/aePnv+4uX+weG5N6XjOOFGGnfBwKMUGidBBIkX1iEoJnHK1qetPr1G54XR30Jlca6g0GIpOIRITXGRXdVFs9gfpuO0C9oH2QYMySbOFgeD21lueKlQBy7B+8sstWFegwuCS2z2ZqVHC3wNBV5GqEGhn9fdvA19G5mcLo2LRwfasf9W1KC8rxSLmQrCym9rLflfjYPmKLe6h+XxvBbalgE1v2u+LCUNhrYLoblwyIOsIgDuRJyf8hU44CGu7d7rQaxvmh7z/ntwGH/cCS0hBXPgqloJnSuwo1b2TU/2K7DoxwUahcEJ3s8A58wPP6IsdiucKXUeLxwkH9GVYawaUWu8aH0UuvMw23asD86PxtnH8dHXT8OT442bu+Q1eUPekYx8JifkCzkjE8LJmvwkv8jv5EMySWbJ1V1qMtjUvCL3Iin+Ar0C2RA=</latexit> ... <latexit sha1_base64=\"/ps7m72IRakV+XzlF0MmYPWJgH8=\">AAACqXicbVFNb9QwEPWGr1I+2sKRi8UKCYklSgoSPVbiwrEVbLuiXVWTyWzWWn9hO6AQ5ScgcYLfxr/B2e6BbhjJ0vN7M57xvMJK4UOW/Rklt27fuXtv5/7ug4ePHu/tHzw586Z2SFM00rhZAZ6k0DQNIkiaWUegCknnxep9r59/JeeF0Z9CY2muoNJiIRBCpD6maXq1P87SbB18CPINGLNNnFwdjH5elgZrRTqgBO8v8syGeQsuCJTU7V7WnizgCiq6iFCDIj9v17N2/EVkSr4wLh4d+Jr9t6IF5X2jipipICz9ttaT/9UQNJLc6h4WR/NWaFsH0njdfFFLHgzvl8FL4QiDbCIAdCLOz3EJDjDEld14PYjV927AvP4SHMUfr4WekKJw4JpWCV0qsJNe9t1A9kuw5NOKjKLgBA4zwDnzzU94EbtVztS6jBcEiRO+NEXRTLg1XvQeCl110cN827EhODtM8zfp4enb8fHRxs0d9ow9Zy9Zzt6xY/aBnbApQ1axH+wX+528Sk6TWfL5OjUZbWqeshuR4F8+rdbA</latexit> h 0 <latexit sha1_base64=\"avtN/9POZOLHmx9Bng/AEJ4GIPc=\">AAACqXicbVFNb9NAEN2Yr1I+2sKRy4oICYkQ2QWJHitx6bFVSRvRRtF4PLFX2Q+zu6ZyLf8EpJ7gt/Fvuk5zoDEjrfT2vZmd2XlpKYXzcfx3ED14+Ojxk62n28+ev3i5s7v36syZyiJN0Ehjpyk4kkLTxAsvaVpaApVKOk+XXzv9/CdZJ4z+5uuSZgpyLRYCwQfqtJjH891hPI5XwfsgWYMhW8fxfG9wc5kZrBRpjxKcu0ji0s8asF6gpHb7snJUAi4hp4sANShys2Y1a8vfBSbjC2PD0Z6v2H8rGlDO1SoNmQp84Ta1jvyvhqCR5EZ3vziYNUKXlSeNd80XleTe8G4ZPBOW0Ms6AEArwvwcC7CAPqzs3uteLK/bHvPxh7cUfrwSOkKK1IKtGyV0pqAcdbJre7IroCQ3zsko8lZgPwOsNVduxNPQLbem0lm4IEgc8cKkaT3ipXGi81DovA0eJpuO9cHZ/jj5NN4/+Tw8PFi7ucXesLfsPUvYF3bIjtgxmzBkOfvFfrM/0YfoJJpG3+9So8G65jW7FxHeAjXo1y0=</latexit> h 1 <latexit sha1_base64=\"Y276hoMxSPiXPZ1bATcS675oyPE=\">AAACqXicbVFNb9NAEN2Yr1I+2sKRy4oICYkQ2QWJHitx6bFVSRvRRtF4PLFX2Q+zu6ZyLf8EpJ7gt/Fvuk5zoDEjrfT2vZmd2XlpKYXzcfx3ED14+Ojxk62n28+ev3i5s7v36syZyiJN0Ehjpyk4kkLTxAsvaVpaApVKOk+XXzv9/CdZJ4z+5uuSZgpyLRYCwQfqtJgn891hPI5XwfsgWYMhW8fxfG9wc5kZrBRpjxKcu0ji0s8asF6gpHb7snJUAi4hp4sANShys2Y1a8vfBSbjC2PD0Z6v2H8rGlDO1SoNmQp84Ta1jvyvhqCR5EZ3vziYNUKXlSeNd80XleTe8G4ZPBOW0Ms6AEArwvwcC7CAPqzs3uteLK/bHvPxh7cUfrwSOkKK1IKtGyV0pqAcdbJre7IroCQ3zsko8lZgPwOsNVduxNPQLbem0lm4IEgc8cKkaT3ipXGi81DovA0eJpuO9cHZ/jj5NN4/+Tw8PFi7ucXesLfsPUvYF3bIjtgxmzBkOfvFfrM/0YfoJJpG3+9So8G65jW7FxHeAjgr1y4=</latexit> h t <latexit sha1_base64=\"BeuVo79kS0eLzMo85XQ5j31wBg4=\">AAACqXicbVFNb9NAEN2Yr1I+2sKRy4oICYkQ2QWJHitx6bFVSRvRRtF4PbFX2Q+zO6ZyLf8EpJ7gt/Fvuk5zoDEjrfT2vZmd2XlpqaSnOP47iB48fPT4ydbT7WfPX7zc2d17deZt5QROhFXWTVPwqKTBCUlSOC0dgk4VnqfLr51+/hOdl9Z8o7rEmYbcyIUUQIE6LeY03x3G43gVvA+SNRiydRzP9wY3l5kVlUZDQoH3F0lc0qwBR1IobLcvK48liCXkeBGgAY1+1qxmbfm7wGR8YV04hviK/beiAe19rdOQqYEKv6l15H81AUag2uhOi4NZI01ZERpx13xRKU6Wd8vgmXQoSNUBgHAyzM9FAQ4EhZXde53k8rrtMR9/kMPw45XQEUqmDlzdaGkyDeWok33bk30BJfpxjlYjOSn6GeCcvfIjnoZuubOVycJFgBIjXtg0rUe8tF52HkqTt8HDZNOxPjjbHyefxvsnn4eHB2s3t9gb9pa9Zwn7wg7ZETtmEyZYzn6x3+xP9CE6iabR97vUaLCuec3uRSRuAc+013E=</latexit> e 0 <latexit sha1_base64=\"MFpP+xjfV7CWxGvcF7oeNiRozcM=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2h2drKx4i9sL9V2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83IrhQ9p+meQ3Ll77/6DnYe7jx4/ebq3f/DszJvKIU3QSOOmOXiSQtMkiCBpah2ByiWd56uPnX7+nZwXRn8JtaWZglKLhUAIkfpM83S+P0zH6Tp4H2QbMGSbOJkfDH5eFgYrRTqgBO8vstSGWQMuCJTU7l5WnizgCkq6iFCDIj9r1rO2/FVkCr4wLh4d+Jr9t6IB5X2t8pipICz9ttaR/9UQNJLc6h4WR7NGaFsF0njTfFFJHgzvlsEL4QiDrCMAdCLOz3EJDjDEld16PYjVddtj3n4LjuKP10JHSJE7cHWjhC4U2FEn+7Yn+yVY8uOSjKLgBPYzwDlz5Uc8j91KZypdxAuCxBFfmjyvR9waLzoPhS7b6GG27VgfnB2Os3fjw9P3w+OjjZs77AV7yV6zjH1gx+wTO2EThqxkP9gv9jt5k5wm0+TrTWoy2NQ8Z7ciwb8vGdcq</latexit> e 1 <latexit sha1_base64=\"0zcSnWSaIZZ2kh70pAqk1+J9I74=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2h2drKx4i9sL9V2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83IrhQ9p+meQ3Ll77/6DnYe7jx4/ebq3f/DszJvKIU3QSOOmOXiSQtMkiCBpah2ByiWd56uPnX7+nZwXRn8JtaWZglKLhUAIkfpM82y+P0zH6Tp4H2QbMGSbOJkfDH5eFgYrRTqgBO8vstSGWQMuCJTU7l5WnizgCkq6iFCDIj9r1rO2/FVkCr4wLh4d+Jr9t6IB5X2t8pipICz9ttaR/9UQNJLc6h4WR7NGaFsF0njTfFFJHgzvlsEL4QiDrCMAdCLOz3EJDjDEld16PYjVddtj3n4LjuKP10JHSJE7cHWjhC4U2FEn+7Yn+yVY8uOSjKLgBPYzwDlz5Uc8j91KZypdxAuCxBFfmjyvR9waLzoPhS7b6GG27VgfnB2Os3fjw9P3w+OjjZs77AV7yV6zjH1gx+wTO2EThqxkP9gv9jt5k5wm0+TrTWoy2NQ8Z7ciwb8xXNcr</latexit> e t <latexit sha1_base64=\"hCKyM/kT+CpmysNMb36hmbnX4ao=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2jWO9lY8Rf2LNV2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83KnZKA0/TNI7ty9d//BzsPdR4+fPN3bP3h2FmzlBU6EVdZPcwiopMEJSVI4dR5B5wrP89XHTj//jj5Ia75Q7XCmoTRyIQVQpD7jnOb7w3ScroP3QbYBQ7aJk/nB4OdlYUWl0ZBQEMJFljqaNeBJCoXt7mUV0IFYQYkXERrQGGbNetaWv4pMwRfWx2OIr9l/KxrQIdQ6j5kaaBm2tY78rybACFRb3WlxNGukcRWhETfNF5XiZHm3DF5Ij4JUHQEIL+P8XCzBg6C4sluvk1xdtz3m7TfyGH+8FjpCydyDrxstTaHBjTo5tD05LMFhGJdoNZKXop8B3turMOJ57FZ6W5kiXgQoMeJLm+f1iDsbZOehNGUbPcy2HeuDs8Nx9m58ePp+eHy0cXOHvWAv2WuWsQ/smH1iJ2zCBCvZD/aL/U7eJKfJNPl6k5oMNjXP2a1IxF/I5ddu</latexit> Agent’s Trajectory ↵ t <latexit sha1_base64=\"fd7kY3QzsnmcM+2QguBPkZ8g0V8=\">AAACrnicbVFNbxMxEHWWj5by1cKRi0WExCFEuwWpPVbiwrFIJI3UrKKxd7JrxV+1vaBltf8BcYU/xr/Bm+ZAs4xk6fm9Gc94HrNS+JCmf0bJvfsPHh4cPjp6/OTps+fHJy/m3tSO44wbadyCgUcpNM6CCBIX1iEoJvGKbT72+tVXdF4Y/SU0FnMFpRZrwSFEarEEaStYhdXxOJ2m26BDkO3AmOzicnUy+rEsDK8V6sAleH+dpTbkLbgguMTuaFl7tMA3UOJ1hBoU+rzdDtzRN5Ep6Nq4eHSgW/bfihaU941iMVNBqPy+1pP/1ThojnKve1if563Qtg6o+W3zdS1pMLTfCC2EQx5kEwFwJ+L8lFfggIe4tzuvB7H53g2YdzfBYfzxVugJKZgD17RK6EKBnfSy7wayr8Cin5ZoFAYn+DADnDPf/ISy2K10ptZFvHCQfEIrw1gzodZ40RspdNlFD7N9x4ZgfjrN3k9PP38YX5zv3Dwkr8hr8pZk5IxckE/kkswIJ5L8JL/I7yRN5kmerG5Tk9Gu5iW5E0n1Fw6j2Z0=</latexit> AttentionReadoutc t <latexit sha1_base64=\"E8LSHhqg8MLY3BUkCbBYjm87LTA=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2h2drKx4i9sL9V2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83IrhQ9p+meQ3Ll77/6DnYe7jx4/ebq3f/DszJvKIU3QSOOmOXiSQtMkiCBpah2ByiWd56uPnX7+nZwXRn8JtaWZglKLhUAIkfqM8zDfH6bjdB28D7INGLJNnMwPBj8vC4OVIh1QgvcXWWrDrAEXBEpqdy8rTxZwBSVdRKhBkZ8161lb/ioyBV8YF48OfM3+W9GA8r5WecxUEJZ+W+vI/2oIGkludQ+Lo1kjtK0CabxpvqgkD4Z3y+CFcIRB1hEAOhHn57gEBxjiym69HsTquu0xb78FR/HHa6EjpMgduLpRQhcK7KiTfduT/RIs+XFJRlFwAvsZ4Jy58iOex26lM5Uu4gVB4ogvTZ7XI26NF52HQpdt9DDbdqwPzg7H2bvx4en74fHRxs0d9oK9ZK9Zxj6wY/aJnbAJQ1ayH+wX+528SU6TafL1JjUZbGqes1uR4F/EW9ds</latexit> o t <latexit sha1_base64=\"pItX1dLeegvb2dVee+9NQ95+YfY=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2jWO9lY8Rf2LNV2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83KnZKA0/TNI7ty9d//BzsPdR4+fPN3bP3h2FmzlBU6EVdZPcwiopMEJSVI4dR5B5wrP89XHTj//jj5Ia75Q7XCmoTRyIQVQpD7bOc33h+k4XQfvg2wDhmwTJ/ODwc/LwopKoyGhIISLLHU0a8CTFArb3csqoAOxghIvIjSgMcya9awtfxWZgi+sj8cQX7P/VjSgQ6h1HjM10DJsax35X02AEai2utPiaNZI4ypCI26aLyrFyfJuGbyQHgWpOgIQXsb5uViCB0FxZbdeJ7m6bnvM22/kMf54LXSEkrkHXzdamkKDG3VyaHtyWILDMC7RaiQvRT8DvLdXYcTz2K30tjJFvAhQYsSXNs/rEXc2yM5Daco2ephtO9YHZ4fj7N348PT98Pho4+YOe8FestcsYx/YMfvETtiECVayH+wX+528SU6TafL1JjUZbGqes1uRiL/fl9d4</latexit> FC Observation ⇡ ( a t | e  t ,o t ,g ) <latexit sha1_base64=\"UGeE0n2u4w752swz8aaRhd6qrDI=\">AAACx3icbVFNb9NAEN2Yr1I+msKRAysipCKZyClI9FiJC9yKRNpKdWSN1xNnlf0wu+NCsHzgwJ3/wRV+DP+GdZoDjRlppbfvzezszMsrJT0lyZ9BdOPmrdt3du7u3rv/4OHecP/Rqbe1EzgVVll3noNHJQ1OSZLC88oh6FzhWb582+lnl+i8tOYjrSqcaSiNnEsBFKhs+DSt5AFkxFMtC45Zkyrk1MbcZhTz8kU2HCXjZB28DyYbMGKbOMn2Bz/SwopaoyGhwPuLSVLRrAFHUihsd9PaYwViCSVeBGhAo58160la/jwwBZ9bF44hvmb/rWhAe7/SecjUQAu/rXXkfzUBRqDa6k7zo1kjTVUTGnHVfF4rTpZ3q+KFdChIrQIA4WT4PxcLcCAoLPTa6ySXX9se8/ITOQwTr4WOUDJ34FaNlqbQUMWd7Nue7BdQoR+XaDWSk6KfAc7Zzz7meehWOlubIlwEKBHzhc3zVcwr62XnsDRlGzycbDvWB6eH48mr8eGH16Pjo42bO+wJe8YO2IS9YcfsHTthUybYd/aT/WK/o/eRjS6jL1ep0WBT85hdi+jbX6nf4c0=</latexit> V ( e  t ,o t ,g ) <latexit sha1_base64=\"V2f7dB+EUwmFCVXvoaXbYa5NfYA=\">AAACvHicbVHbbtNAEN2YWymXpsAbLysipCKZyC5I9AlV4oXHIpG0UhNZ4/XEWWUvZndc5Fr+Bz6BV/gj/oZ1mgeaMNJKZ8+Z2Zmdk1dKekqSP4Pozt179x/sPdx/9PjJ04Ph4bOpt7UTOBFWWXeRg0clDU5IksKLyiHoXOF5vvrU6+dX6Ly05is1Fc41lEYupAAKVDZ8MT3CrJ0p5NTF3GYU8/JNNhwl42QdfBekGzBimzjLDgc/ZoUVtUZDQoH3l2lS0bwFR1Io7PZntccKxApKvAzQgEY/b9fjd/x1YAq+sC4cQ3zN/lvRgva+0XnI1EBLv6315H81AUag2upOi5N5K01VExpx03xRK06W9/vhhXQoSDUBgHAyzM/FEhwIClu89TrJ1XW3w7z9Rg7Dj9dCTyiZO3BNq6UpNFRxL/tuR/ZLqNCPS7QayUmxmwHO2e8+5nnoVjpbmyJcBCgR86XN8ybmlfWyt1WasgseptuO7YLp8Th9Nz7+8n50erJxc4+9ZK/YEUvZB3bKPrMzNmGCXbOf7Bf7HX2MimgV6ZvUaLCpec5uRXT1F85F3XY=</latexit> Concat Demonstration Trajectory Figure 2: Left: Overview of DTSIL. (a) We maintain a trajectory buffer. (b) For each episode, we sample a state from the buffer, (c) imitate the demonstration leading to the sampled state, obtain a new trajectory, (d) update the memory with the new trajectory and gradually expand the buffer. We repeat this process until training goes to the end. Right: Architecture of the trajectory-conditioned policy (see details in Sec. 2.3). problem via the sampling-based search for g∗ over the space of g realizable by the (trajectory- conditioned) policy πθ and gradient-based local search for θ∗. For robustness, we may want to ﬁnd multiple trajectories with high returns and a trajectory-conditioned policy executing them. We name our method as Diverse Trajectory-conditioned Self-Imitation Learning (DTSIL). 2.2 Overview of DTSIL Organizing Trajectory Buffer As shown in Fig. 2(a), we maintain a trajectory buffer D = {(e(1),τ(1),n(1)),(e(2),τ(2),n(2)),···} of diverse past trajectories. τ(i) is the best trajectory ending with a state with embedding e(i). n(i) is the number of times the cluster represented by the embedding e(i) has been visited during training. To maintain a compact buffer, similar state embeddings within the tolerance threshold δ are clustered together, and an existing entry is replaced if an improved trajectory τ(i) ending with a near-identical state is found. In the buffer, we keep a single representative state embedding for each cluster. If a state embedding et observed in the current episode is close to a representative state embedding e(k), we increase visitation count n(k) of the k-th cluster. If the sub-trajectory τ≤t of the current episode up to step tis better than τ(k), e(k) is replaced by et. Pseudocode for organizing clusters is in the appendix. Sampling States for Exploitation or ExplorationIn RL algorithms, the agent needs to exploit what it already knows to maximize reward and explore new behaviors to ﬁnd a potentially better policy. For exploitation, we aim at reaching the states with the highest total rewards, which probably means a good behavior of receiving high total rewards. For exploration, we would like to look around the rarely visited states, which helps discover novel states with higher total rewards. With probability 1 −p, in exploitation mode, we sample the states in the buffer with the highest cumulative rewards. With probability p, in exploration mode, we sample each state e(i) with the probability proportional to 1/ √ n(i), as inspired by count-based exploration [ 50, 7] and rank-based prioritization [ 45, 16]. To balance between exploration and exploitation, we decrease the hyper-parameterpof taking the exploration mode. The pseudo-code algorithm of sampling the states is in the appendix. Imitating Trajectory to State of InterestIn stochastic environments, in order to reach diverse states e(i) we sampled, the agent would need to learn a goal-conditioned policy [1, 34, 44, 40]. But it is difﬁcult to learn the goal-conditioned policy only with the ﬁnal goal state because the goal state might be far from the agent’s initial states and the agent might have few experiences reaching it. Therefore, we provide the agent with the full trajectory leading to the goal state. So the agent beneﬁts from richer intermediate information and denser rewards. We call this trajectory-conditioned policy πθ(·|g) where g= {eg 1,eg 2,··· ,eg |g|}, and introduce how to train the policy in detail in Sec. 2.3. Updating Buffer with New TrajectoryWith trajectory-conditioned policy, the agent takes actions to imitate the sampled demonstration trajectory. As shown in Fig. 2(c), because there could be stochasticity in the environment and our method does not require the agent to exactly follow the demonstration step by step, the agent’s new trajectory could be different from the demonstration and thus visit novel states. In a new trajectoryE= {(o0,e0,a0,r0),··· ,(oT,eT,aT,rT)}, if et is nearly identical to a state embedding e(k) in the buffer and the partial episode τ≤t is better than (i.e. higher return or shorter trajectory) the stored trajectory τ(k), we replace the existing entry (e(k),τ(k),n(k)) by (et,τ≤t,n(k) + 1). If et is not sufﬁciently similar to any state embedding in the buffer, a new entry (et,τ≤t,1) is pushed into the buffer, as shown in Fig. 2(d). Therefore we gradually increase the diversity of trajectories in the buffer. The detailed algorithm is described in the supplementary material. 32.3 Learning Trajectory-Conditioned Policy Policy Architecture For imitation learning with diverse demonstrations, we design a trajectory- conditioned policy πθ(at|e≤t,ot,g) that should ﬂexibly imitate any given trajectory g. Inspired by neural machine translation methods [51, 6], one can view the demonstration as the source sequence and view the incomplete trajectory of the agent’s state representations as the target sequence. We apply a recurrent neural network (RNN) and an attention mechanism Bahdanau et al. [6] to the sequence data to predict actions that would make the agent follow the demonstration. As illustrated in Fig. 2, RNN computes the hidden features hg i for each state embedding eg i (0 ≤i≤|g|) in the demonstration and derives the hidden features ht for the agent’s state representationet. Then the attention weight αt is computed by comparing the current agent’s hidden featuresht with the demonstration’s hidden features hg i (0 ≤i≤|g|). The attention readout ct is computed as an attention-weighted summation of the demonstration’s hidden features to capture the relevant information in the demonstration and to predict the action at. Training is performed by combining RL and supervised objectives as follows. Reinforcement Learning Objective Given a demonstration trajectory g= {eg 0,eg 1,··· ,eg |g|}, we provide rewards for imitating g and train the policy to maximize rewards. For each episode, we record uto denote the index of state in the given demonstration that is lastly visited by the agent. At the beginning of an episode, the index uof the lastly visited state embedding in the demonstration is initialized as u= −1, which means no state in the demonstration has been visited. At each step t, if the agent’s new state st+1 has an embedding et+1 and it is the similar enough to any of the next ∆tstate embeddings starting from the last visited state embedding eg u in the demonstration (i.e., ∥et+1 −eg u′ ∥<δ where u < u′≤u+ ∆t), then the index of the last visited state embedding in the demonstration is updated as u ←u′and the agent receives environment reward and positive imitation reward rDTSIL t = f(rt) + rim, where f(·) is a monotonically increasing function (e.g., clipping [32]) and rim is the imitation reward with a value of 0.1 in our experiments. Otherwise, the reward rDTSIL t is 0 (see appendix for an illustration example). This encourages the agent to visit states in the demonstration in a soft-order so that it can edit or augment the demonstration when executing a new trajectory. The demonstration plays a role to guide the agent to the region of interest in the state embedding space. After visiting the last (non-terminal) state in the demonstration, the agent performs random exploration (because rDTSIL t = 0) around and beyond the last state until the episode terminates, to push the frontier of exploration. With rDTSIL t , the trajectory-conditioned policy πθ can be trained with a policy gradient algorithm [52]: LRL = Eπθ [−log πθ(at|e≤t,ot,g) ˆAt], (1) where ˆAt = n−1∑ d=0 γdrDTSIL t+d + γnVθ(e≤t+n,ot+n,g) −Vθ(e≤t,ot,g) where Eπθ indicates the empirical average over a ﬁnite batch of on-policy samples and ndenotes the number of rollout steps taken in each iteration. We use Proximal Policy Optimization [48] as an actor-critic policy gradient algorithm for our experiments. Supervised Learning Objective To improve trajectory-conditioned imitation learning and to better leverage the past trajectories, we propose a supervised learning objective. We leverage the actions in demonstrations, similarly to behavior cloning, to help RL for imitation of diverse trajectories. We sample a trajectory τ = {(o0,e0,a0,r0),(o1,e1,a1,r1) ···}∈D , formulate the demonstration g= {e0,e1,··· ,e|g|}and assume the agent’s incomplete trajectory is the partial trajectoryg≤t = e≤t = {e0,e1,··· ,et}for any 1 ≤t≤|g|. Then at is the ‘correct’ action at steptfor the agent to imitate the demonstration. Our supervised learning objective is to maximize the log probability of taking such actions: LSL = −log πθ(at|e≤t,ot,g), where g= {e0,e1,··· ,e|g|}. (2) 2.4 Extensions of DTSIL for Improved Robustness and Generalization DTSIL can be easily extended for more challenging scenarios. Without hand-crafted high-level state embeddings, we can combine DTSIL with state representation learning approaches (Sec. 5.1). In highly stochastic environments, we modify DTSIL to construct and select proper demonstra- tions (Sec. 5.2). In addition, DTSIL can be extended with hierarchical reinforcement learning for generalization over multiple tasks (Sec. 5.3). See individual sections for more details. 43 Related Work Imitation Learning The goal of imitation learning is to train a policy to mimic a given demon- stration. Many previous works achieve good results on hard-exploration Atari games by imitating human demonstrations [23, 41]. Aytar et al. [3] learn embeddings from a variety of demonstration videos and proposes the one-shot imitation learning reward, which inspires the design of rewards in our method. All these successful attempts rely on the availability of human demonstrations. In contrast, our method treats the agent’s past trajectories as demonstrations. Memory Based RL An external memory buffer enables the storage and usage of past experiences to improve RL algorithms. Episodic reinforcement learning methods [43, 22, 28] typically store and update a look-up table to memorize the best episodic experiences and retrieve the episodic memory in the agent’s decision-making process. Oh et al.[36] and Gangwani et al. [19] train a parameterized policy to imitate only the high-reward trajectories with the SIL or GAIL objective. Unlike the previous work focusing on high-reward trajectories, we store the past trajectories ending with diverse states in the buffer, because trajectories with low reward in the short term might lead to high reward in the long term. Badia et al. [5] train a range of directed exploratory policies based on episodic memory. Gangwani et al. [19] propose to learn multiple diverse policies in a SIL framework but their exploration can be limited by the number of policies learned simultaneously and the exploration performance of every single policy, as shown in the supplementary material. Learning Diverse Policies Previous works [20, 17, 42] seek a diversity of policies by maximizing state coverage, the entropy of mixture skill policies, or the entropy of goal state distribution. Zhang et al. [56] learns a variety of policies, each performing novel action sequences, where the novelty is measured by a learned autoencoder. However, these methods focus more on tasks with relatively simple state space and dense rewards while DTSIL shows experimental results performing well on long-horizon, sparse-reward environments with a rich observation space like Atari games. Exploration Many exploration methods [46, 2, 12, 50] in RL tend to award a bonus to encourage an agent to visit novel states. Recently this idea was scaled up to large state spaces [53, 7, 38, 11, 39, 10]. Intrinsic curiosity uses the prediction error or pseudo count as intrinsic reward signals to incentivize visiting novel states. We propose that instead of directly taking a quantiﬁcation of novelty as an intrinsic reward, one can encourage exploration by rewarding the agent when it successfully imitates demonstrations that would lead to novel states. Ecoffet et al.[16] also shows the beneﬁt of exploration by returning to promising states. Our method can be viewed in general as an extension of [16], though we do not need to rely on the assumption that the environment is resettable to arbitrary states. Similar to previous off-policy methods, we use experience replay to enhance exploration. Many off-policy methods [25, 36, 1] tend to discard old experiences with low rewards and hence may prematurely converge to sub-optimal behaviors, but DTSIL using these diverse experiences has a better chance of ﬁnding higher rewards in the long term. Contemporaneous works [5, 4] as off-policy methods also achieved strong results on Atari games. NGU [5] constructs an episodic memory-based intrinsic reward using k-nearest neighbors over the agent’s recent experience to train the directed exploratory policies. Agent57 [4] parameterizes a family of policies ranging from very exploratory to purely exploitative and proposes an adaptive mechanism to choose which policy to prioritize throughout the training process. While these methods require a large number of interactions, ours perform competitively well on the hard-exploration Atari games with less than one-tenth of samples. Model-based reinforcement learning [24, 47, 26] generally improves the efﬁciency of policy learning. However, in the long-horizon, sparse-reward tasks, it is rare to collect precious transitions with non-zero rewards and thus it is difﬁcult to learn a model correctly predicting the dynamics of getting positive rewards. We instead perform efﬁcient policy learning in the hard-exploration tasks because of efﬁcient exploration with the trajectory-conditioned policy. 4 Experiments In the experiments, we aim to answer the following questions: (1) How well does the trajectory- conditioned policy imitate the diverse demonstration trajectories? (2) Does the imitation of the past diverse experience enable the agent to further explore more diverse directions and guide the exploration to ﬁnd the trajectory with a near-optimal total reward? (3) Is our method helpful for avoiding myopic behaviors and converging to near-optimal solutions? We compare our method with the following baselines: (1) PPO [48]; (2) PPO+EXP: PPO with reward f(rt) + λ/ √ N(et), where λ/ √ N(et) is the count-based exploration bonus, N(e) is the number of times the cluster which the state representation ebelongs to was visited during training and λis the hyper-parameter controlling the weight of exploration term; (3) PPO+SIL: PPO with Self-Imitation 5:apple (+1)       :gold (+10)      :rock (-0.05) Myopic Optimal (a) The map. 0M 8M 16M 24M 32M 40M Steps 0 2 4 6 8Average Reward PPO PPO+EXP PPO+SIL DTRA DTSIL (b) Average episode reward. PPO+SILDTSIL0M steps4.8M steps9.6M steps (c) Visualization of the trajectories stored in the buffer for PPO+SIL and DTSIL (ours) as training continues. The agent (gray), apple (red) and treasure (yellow) are shown as squares for simplicity. The rocky region is in dark blue. The reward of getting an apple, collecting the gold and stepping in rock is 1, 10, -0.05 respectively. (d) Visualization of at- tention in two samples. Figure 3: (a) The map of Apple-Gold domain. (b) Average reward of recent 40 episodes. The curves in dark colors are average over 5 curves in light colors. (c) Comparison of trajectories. (d) Attention in the learned trajectory-conditioned policy. The x-axis and y-axis correspond to the state (e.g. agent’s location) in the source sequence (demonstration) and the generated sequence (agent’s new trajectory), respectively. Each cell shows the weight αij of the j-th source state for the i-th target state. Learning [36]; (4) DTRA (“Diverse Trajectory-conditioned Repeat Actions”): we keep a buffer of diverse trajectories and sample the demonstrations as DTSIL, but we simply repeat the action sequence in the demonstration trajectory and then perform random exploration until the episode terminates. More details about the implementation can be found in the appendix. 4.1 Apple-Gold Domain The Apple-Gold domain (Fig. 3a) is a grid-world environment with misleading rewards that can lead the agent to local optima. At the start of each episode, the agent is placed randomly in the left bottom part of the maze. An observation consists of the agent’s location(xt,yt) and binary variables showing whether the agent has gotten the apples or the gold. A state is represented as the agent’s location and the cumulative positive reward indicating the collected objects, i.e. et = (xt,yt,∑ t i=1 max(ri,0)). In Fig. 3b, PPO+EXP achieves the average reward of 4. PPO+EXP agent can explore the environment and occasionally gather the gold to achieve the best episode reward around 8.5. However, it rarely encounters this optimal reward. Thus, this parametric approach might forget the good experience and fails to replicate the best past trajectory to achieve the optimal total reward. Fig. 3b shows that PPO+SIL agent is stuck with the sub-optimal policy of collecting the two apples with a total reward of 2 on average. Fig. 3c visualizes how the trajectories in the memory buffer evolve during the learning process. Obviously, PPO+SIL agent quickly exploits good experiences of collecting the apples and the buffer is ﬁlled with the trajectories in the nearby region. Therefore, the agent only adopts the myopic behavior and fails on this task. In the environment with the random initial location of the agent, repeating the previous action sequences is not sufﬁcient to reach the goal states. The DTRA agent has a difﬁculty in exploring the environment and achieving good performance. Unlike the baseline methods, DTSIL is able to obtain the near-optimal total reward of 8.5. Fig. 3c veriﬁes that DTSIL can generate new trajectories visiting novel states, gradually expand the explored region in the environment, and discover the optimal behavior. A visualization of attention weight in Fig. 3d investigates which states in the demonstration are considered more important when generating the new trajectory. Even though the agent’s random initial location is different from the demonstration, we can see a soft-alignment between the source sequence and the target sequence. The agent tends to pay more attention to states which are several steps away from its current state in the demonstration. Therefore, it is guided by these future states to determine the proper actions to imitate the demonstration. 4.2 Atari Games We evaluate our method on the hard-exploration games in the Arcade Learning Environment [8, 30]. The environment setting is the same as [13]. There is a sticky action [30] resulting in stochasticity in the dynamics. The observation is a frame of raw pixel images, and the state representation et = (roomt,xt,yt,∑ t i=1 max(ri,0)) consists of the agent’s ground truth location (obtained from 6Method DTSIL+EXP PPO+EXP SmartHash NGU* Abstract-HRL IDF A2C+SIL PPO+CoEX RND NGU Agent57 #Frames 3.2B 3.2B 4B 35B 2B 0.1B 0.2B 2B 16B 35B 100B Montezuma 22,616 12,338 6,600 15,000 11,000 2,505 2,500 11,618 10,070 10,400 9,352 Pitfall 12,446 0 - - 10,000 - - - -3 8,400 18,756 Venture 2,011 1,817 - - - 416 0 1,916 1,859 1,700 2,623 Table 1: Comparison with the state-of-the-art results. The top-2 scores for each game are in bold.Abstract-HRL [29] and NGU* (i.e., NGU with hand-crafted controllable states) [5] assume more high-level state information, including the agent’s location, inventory, etc. DTSIL, PPO+EXP [13], and SmartHash [53] only make use of agent’s location information from RAM. IDF [10], A2C+SIL [36], PPO+CoEX [13], RND [11], NGU [5] and Agent57 [4] (a contemporaneous work) do not use RAM information. The score is averaged over multiple runs, gathered from each paper, except PPO+EXP from our implementation. 0M 160M 320M 480M 640M 800M Steps 0 5000 10000 15000 20000 25000 30000Average Reward Montezuma's Revenge PPO+EXP DTRA+Exp DTSIL+EXP 0M 160M 320M 480M 640M 800M Steps 0 50000 100000 150000 200000 250000Best Reward Montezuma's Revenge 0M 160M 320M 480M 640M 800M Steps 0 2500 5000 7500 10000 12500 15000 17500Average Reward Pitfall PPO+Exp DTRA+Exp DTSIL+EXP 0M 160M 320M 480M 640M 800M Steps 0 5000 10000 15000 20000 25000Best Reward Pitfall Figure 4: Learning curves of the average episode reward and the best episode reward found on Montezuma’s Revenge and Pitfall, averaged over 3 runs. More statistics are reported in the appendix. RAM) and the accumulated positive environment reward, which implicitly indicates the objects the agent has collected. It is worth noting that even with the ground-truth location of the agent, on the two infamously difﬁcult games Montezuma’s Revenge and Pitfall, it is highly non-trivial to explore efﬁciently and avoid local optima without relying on expert demonstrations or being able to reset to arbitrary states. Many complicated elements such as moving entities, traps, and the agent’s inventory should be considered in decision-making process. Empirically, as summarized in Tab. 1, the previous SOTA baselines using the agent’s ground truth location information even fail to achieve high scores. Using the state representation et, we introduce a variant ‘DTSIL+EXP’ that adds a count-based exploration bonus r+ t = 1/ √ N(et) to rDTSIL t for faster exploration.1 DTSIL discovers novel states mostly by random exploration after the agent ﬁnishes imitating the demonstration. The pseudo-count bonus brings improvement over random exploration by explicitly encouraging the agent to visit novel states with less count. For a fair comparison, we also include count-based exploration bonus in DTRA. However, with stochasticity in the dynamics, it cannot avoid the dangerous obstacles and fails to reach the goal by just repeating the stored action sequence. Therefore, the performance of DTRA+EXP (Fig. 4) is poor compared to other methods. On Venture (Tab. 1), it is relatively easy to explore and gather positive environment rewards. DTSIL performs only slightly better than the baselines. On Montezuma’s Revenge (Fig. 4), in the early stage, the average episode reward of DTSIL+EXP is worse than PPO+EXP because our policy is trained to imitate diverse demonstrations rather than directly maximize the environment reward. Contrary to PPO+EXP, DTSIL is not eager to follow the myopic path (Fig. 1).2 As training continues, DTSIL+EXP successfully discovers trajectories to pass the ﬁrst level with a total reward more than 20,000. As we sample the best trajectories in the buffer as demonstrations, the average episode reward increases to surpass 20,000 in Fig. 4. On Pitfall, positive rewards are much sparser and most of the actions yield small negative rewards (time step penalty) that would discourage getting a high total reward in the long term. However, DTSIL+EXP stores trajectories with negative rewards, encourages the agent to visit these novel regions, discovers good paths with positive rewards and eventually attains an average episode reward over 0. In Fig. 4, different performances under different random seeds are due to huge positive rewards in some states on Montezuma’s Revenge and Pitfall. Once the agent luckily ﬁnds these states in some runs, DTSIL can exploit them and perform much better than other runs. 1The existing exploration methods listed in Table 1 take advantage of count-based exploration bonus (e.g., SmartHash, Abstract-HRL and PPO+CoEX). Therefore, a combination of DTSIL and the count-based exploration bonus does not introduce unfair advantages over other baselines. 2Demo videos of the learned policies for both PPO+EXP and DTSIL+EXP are available at https://sites. google.com/view/diverse-sil. In comparison to DTSIL+EXP, we can see the PPO+EXP agent does not explore enough to make best use of the tools (e.g. sword, key) collected in the game. 7(a)  (b) 0M 1.6M 3.2M 4.8M 6.4M 8M Steps 0 2 4 6 8Average Reward Nav A3C+D1D2L PPO+EXP DTSIL (c)  (d) 0M 4M 8M 12M 16M 20M Steps 0 20 40 60 80Average Reward DTSIL PPO+EXP (e) 0M 4M 8M 12M 16M 20M Steps 0 200 400 600 800Best Reward (f) Figure 5: (a) Indoor scene for navigation task. (b) A panoramic view from a speciﬁc viewpoint. (c) Learning curves of average reward on navigation task. (d) Bin picking. (e) Learning curves of average reward on manipulation task. (f) Learning curves of best reward on manipulation task. 4.3 Continuous Control Tasks When the initial condition is highly random, previous works imitating expert demonstrations (e.g. [3]) would also struggle. We slightly modify DTSIL to handle the highly random initial states: in each episode, from buffer D, we sample the demonstration trajectory with a start state similar to the current episode. The detailed algorithm is described in the supplementary material. Navigation We focus on a more realistic environment, a distant visual navigation task designed on Gibson dataset [ 55]. To make the task more challenging, the agent is randomly placed in the environment (red rectangle in Fig. 5a), a positive reward 10 is given only when it reaches a ﬁxed target location (green point in Fig. 5a) which is signiﬁcantly further away. The agent receives no information about the target (such as the target location or image) in advance. The observation is a ﬁrst-person view RGB image and the state embedding is the agent’s location and orientation (which is usually available in robotics navigation tasks) and the cumulative reward. This experiment setting is similar to the navigation task with a static goal deﬁned in [31]. Apart from the baseline PPO+EXP, we also compare with Nav A3C+D1D2L [31], which uses the agent’s location and RGB and depth image. This method performs well in navigation tasks on DeepMind Lab where apples with small positive rewards are randomly scattered to encourage exploration, but on our indoor navigation task, it fails to discover the distant goal without the exploration bonus. Fig. 5c shows that Nav A3C+D1D2L can never reach the target. PPO+EXP, as a parametric approach, is sample-inefﬁcient and fails to quickly exploit the previous successful experiences. However, DTSIL agent can successfully explore to ﬁnd the target and gradually imitate the best trajectories of reward 10 to replicate the good behavior. Manipulation Bin picking is one of the hardest tasks in Surreal Robotics Suite [18]. Fig. 5d shows the bin picking task with a single object, where the goal is to pick up the cereal and place it into the left bottom bin. With carefully designed dense rewards (i.e. positive rewards at each step when the robot arm moving near the object, touching it, lifting it, hovering it over the correct bin, or successfully placing it), the PPO agent can pick up, move and drop the object [ 18]. We instead consider a more challenging scenario with sparse rewards. The reward is 0.5 at the single step of picking up the object, -0.5 if the object is dropped in the wrong place, 1 at each step when the object keeps in the correct bin. The observation is the physical state of the robot arm and the object. The state embedding consists of the position of the object and gripper, a variable about whether the gripper is open, and cumulative environment reward. Each episode terminates at 1000 steps. In Fig. 5f, PPO+EXP agent never discovers a successful trajectory with episode reward over 0, because the agent has difﬁculty in lifting the cereal and easily drops it by mistake. In contrast, DTSIL imitates the trajectories lifting the object, explores to move the cereal over the bins, ﬁnds trajectories successfully placing the object, exploits the high-rewarding trajectories, and obtains a higher average reward than the baseline (Fig. 5e). 4.4 Other Domains: Deep Sea and Mujoco Maze In the supplementary material, we present additional details of the experimental results and also the experiments on other interesting domains. On Deep Sea[37], we show that the advantage of DTSIL becomes more obvious when the state space becomes larger and rewards become sparser. OnMujoco Maze [15, 33], we show that DTSIL helps avoid sub-optimal behavior in continuous action space. 5 Discussions: Robustness and Generalization of DTSIL 5.1 Robustness of DTSIL with Learned State Representations Learning a good state representation is an important open question and extremely challenging especially for long-horizon, sparse-reward environments, but it is not the main focus of this work. However, we ﬁnd that DTSIL can be combined with existing approaches of state representation learning if the high-level state embedding is not available. When the quality of the learned state 80M 240M 480M 720M 960M 1200M Steps 0 10000 20000 30000 40000Average Reward Montezuma's Revenge DTSIL+EXP (a)  (b) 0M 8M 16M 24M 32M 40M Steps 0 2 4 6 8Average Reward PPO+EXP DTSIL (c) 0M 8M 16M 24M 32M 40M Steps 0 1 2 3 4Average Reward PPO+EXP DTSIL (d) Figure 6: (a) Experiment with learned state representation. (b) Two samples of the random maze structure in Apple-Gold domain. (c) Learning curves of average episode reward on the training set of mazes in Apple-Gold domain. (d) Average episode reward on the test set of mazes for generalization experiment. representation is not satisfactory (e.g., on Montezuma’s Revenge, [13] fails to differentiate the dark rooms at the last ﬂoor), the trajectory-conditioned policy might be negatively inﬂuenced by the inaccuracy in eg i or ei. Thus, we modify DTSIL to handle this difﬁculty by feeding sequences of observations (instead of sequences of learned state embeddings) into the trajectory-conditioned policy. The learned state embeddings are merely used to cluster states when counting state visitation or determining whether to provide imitation reward rim. Then DTSIL becomes more robust to possible errors in the learned embeddings. With the learned state representation from [13], on Montezuma’s Revenge, DTSIL+EXP reaches the second level of the maze with a reward >20,000 (Fig 6a). 5.2 Robustness of DTSIL in Stochastic Environments In the single-task RL problem, a Markov Decision Process (MDP) is deﬁned by a state set S, an action set A, an initial state distribution p(s0), a state transition dynamics model p(st+1|st,at), a reward function r(st,at) and a discount factor γ. So the environment stochasticity falls into three categories: stochasticity in the initial state distribution, stochasticity in the transition function, and stochasticity in the reward function. For sparse-reward, long-horizon tasks, if the precious reward signals are unstable, the problem would be extremely difﬁcult to solve. Thus, in this paper, we mainly focus on the other two categories of stochasticity. In Sec. 4.2 & 4.3, we show the efﬁciency and robustness of DTSIL in the environment with sticky action (i.e. stochasticity in p(st+1|st,at)) or highly random initial states (i.e. stochasticity in p(s0)). 5.3 Generalization Ability of DTSIL While many previous works about exploration focus on the single-task RL problem with a single MDP [53, 13, 16], we step further to extend DTSIL for the multiple MDPs, where every single task is in a stochastic environment with local optima. For example, in the Apple-Gold domain, we design 12 different structures of the maze as a training set (Fig. 6b). In each episode, the structure of maze is randomly sampled and the location of agent and gold is randomized in a small region. If the structure in the demonstration is different from the current episode, DTSIL agent might fail to recover the state of interest by roughly following the demonstration. Thus, using the buffer of diverse trajectories, we alternatively learn a hierarchical policy, which can behave with higher ﬂexibility in the random mazes to reach the sampled states. We design the rewards so that the high-level policy is encouraged to propose the appropriate sub-goals (i.e., agent’s locations) sequentially to maximize the environment reward and goal-achieving bonus (i.e. positive reward when the low-level policy successfully reaches the long-term goal sampled from the buffer). The low-level policy learns to visit sub-goals given the current observation (i.e. RGB image of the maze). The diverse trajectories in the buffer are also used with a supervised learning objective to improve policy learning. Fig. 6c shows that the hierarchical policy outperforms PPO+EXP during training. When evaluated on 6 unseen mazes in the test set, it can generalize the good behavior to some unseen environments (Fig. 6d). More details of the algorithm and experiments are in the supplementary material. Solving multi-task RL is a challenging open problem [35, 14, 44]. Here we veriﬁed this variant of DTSIL is promising and the high-level idea of DTSIL to leverage and augment diverse past trajectories can help exploration in this scenario. We leave the study of improving DTSIL furthermore as future work. 6 Conclusion This paper proposes to learn diverse policies by imitating diverse trajectory-level demonstrations through count-based exploration over these trajectories. Imitation of diverse past trajectories can guide the agent to rarely visited states and encourages further exploration of novel states. We show that in a variety of stochastic environments with local optima, our method signiﬁcantly improves count-based exploration method and self-imitation learning. It avoids prematurely converging to a myopic solution and learns a near-optimal behavior to achieve a high total reward. 9Broader Impact DTSIL is likely to be useful in real-world RL applications, such as robotics-related tasks. Compared with previous exploration methods, DTSIL shows obvious advantages when the task requires rea- soning over long-horizon and the feedback from environment is sparse. We believe RL researchers and practitioners can beneﬁt from DTSIL to solve RL application problems requiring efﬁcient explo- ration. Especially, DTSIL helps avoid the cost of collecting human demonstration and the manual engineering burden of designing complicated reward functions. Also, as we discussed in Sec. 5, when deployed for more problems in the future, DTSIL has a good potential to perform robustly and avoid local optima in various stochastic environments when combined with other state representation learning approaches. DTSIL in its current form is applied to robotics tasks in the simulated environments. And it likely contributes to real robots in solving hard-exploration tasks in the future. Advanced techniques in robotics make it possible to eliminate repetitive, time-consuming, or dangerous tasks for human workers and might bring positive societal impacts. For example, the advancement in household robots will help reduce the cost for home care and beneﬁt people with disability or older adults who needs personalized care for a long time. However, it might cause negative consequences such as large-scale job disruptions at the same time. Thus, proper public policy is required to reduce the social friction. On the other hand, RL method without much reward shaping runs the risk of taking a step that is harmful for the environments. This generic issue faced by most RL methods is also applicable to DTSIL. To mitigate this issue, given any speciﬁc domain, one simple solution is to apply a constraint on the state space that we are interested to reach during exploration. DTSIL is complementary to the mechanisms to restrict the state space or action space. More principled way to ensure safety during exploration is a future work. In addition to AI safety, another common concern for most RL algorithms is the memory and computational cost. In the supplementary material we discuss how to control the size of the memory for DTSIL and report the cost. Empirically DTSIL provides ideas for solving various hard-exploration tasks with a reasonable computation cost. Acknowledgments and Disclosure of Funding This work was supported in part by NSF grant IIS-1526059 and Korea Foundation for Advanced Studies. Any opinions, ﬁndings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reﬂect the views of the sponsor. References [1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba. Hindsight experience replay. InAdvances in Neural Information Processing Systems, pages 5048–5058, 2017. [2] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397–422, 2002. [3] Y . Aytar, T. Pfaff, D. Budden, T. Paine, Z. Wang, and N. de Freitas. Playing hard exploration games by watching youtube. In Advances in Neural Information Processing Systems, pages 2930–2941, 2018. [4] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and C. Blundell. Agent57: Outperforming the atari human benchmark. arXiv preprint arXiv:2003.13350, 2020. [5] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kapturowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt, et al. Never give up: Learning directed exploration strategies. arXiv preprint arXiv:2002.06038, 2020. [6] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. [7] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. InAdvances in Neural Information Processing Systems, pages 1471–1479, 2016. 10[8] M. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. [9] J. Bornschein, A. Mnih, D. Zoran, and D. J. Rezende. Variational memory addressing in generative models. In Advances in Neural Information Processing Systems, pages 3920–3929, 2017. [10] Y . Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018. [11] Y . Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. [12] N. Chentanez, A. G. Barto, and S. P. Singh. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pages 1281–1288, 2005. [13] J. Choi, Y . Guo, M. Moczulski, J. Oh, N. Wu, M. Norouzi, and H. Lee. Contingency-aware exploration in reinforcement learning. arXiv preprint arXiv:1811.01483, 2018. [14] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2169–2176. IEEE, 2017. [15] Y . Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329– 1338, 2016. [16] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. [17] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. [18] L. Fan, Y . Zhu, J. Zhu, Z. Liu, O. Zeng, A. Gupta, J. Creus-Costa, S. Savarese, and L. Fei-Fei. Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. In Conference on Robot Learning, 2018. [19] T. Gangwani, Q. Liu, and J. Peng. Learning self-imitating diverse policies. arXiv preprint arXiv:1805.10309, 2018. [20] K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016. [21] K. Guu, T. B. Hashimoto, Y . Oren, and P. Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. [22] S. Hansen, A. Pritzel, P. Sprechmann, A. Barreto, and C. Blundell. Fast deep reinforcement learning using online adjustments from the past. In Advances in Neural Information Processing Systems, pages 10567–10577, 2018. [23] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, et al. Deep q-learning from demonstrations. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [24] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019. [25] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018. [26] T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman, and V . Mnih. Unsupervised learning of object keypoints for perception and control. In Advances in neural information processing systems, pages 10724–10734, 2019. [27] C. Liang, M. Norouzi, J. Berant, Q. V . Le, and N. Lao. Memory augmented policy optimization for program synthesis and semantic parsing. In Advances in Neural Information Processing Systems, pages 9994–10006, 2018. [28] Z. Lin, T. Zhao, G. Yang, and L. Zhang. Episodic memory deep q-networks. arXiv preprint arXiv:1805.07603, 2018. 11[29] E. Z. Liu, R. Keramati, S. Seshadri, K. Guu, P. Pasupat, E. Brunskill, and P. Liang. Learning abstract models for long-horizon exploration, 2019. URLhttps://openreview.net/forum? id=ryxLG2RcYX. [30] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2017. [31] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016. [32] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015. [33] O. Nachum, S. S. Gu, H. Lee, and S. Levine. Data-efﬁcient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems, pages 3303–3313, 2018. [34] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine. Combining self-supervised learning and imitation for vision-based rope manipulation. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2146–2153. IEEE, 2017. [35] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2661–2670. JMLR. org, 2017. [36] J. Oh, Y . Guo, S. Singh, and H. Lee. Self-imitation learning.arXiv preprint arXiv:1806.05635, 2018. [37] I. Osband, Y . Doron, M. Hessel, J. Aslanides, E. Sezener, A. Saraiva, K. McKinney, T. Lattimore, C. Szepesvári, S. Singh, B. Van Roy, R. Sutton, D. Silver, and H. van Hasselt. Behaviour suite for reinforcement learning. 2019. [38] G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos. Count-based exploration with neural density models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2721–2730. JMLR. org, 2017. [39] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self- supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16–17, 2017. [40] D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y . Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell. Zero-shot visual imitation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2050–2053, 2018. [41] T. Pohlen, B. Piot, T. Hester, M. G. Azar, D. Horgan, D. Budden, G. Barth-Maron, H. van Hasselt, J. Quan, M. Veˇcerík, et al. Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018. [42] V . H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-ﬁt: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019. [43] A. Pritzel, B. Uria, S. Srinivasan, A. P. Badia, O. Vinyals, D. Hassabis, D. Wierstra, and C. Blundell. Neural episodic control. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2827–2836. JMLR. org, 2017. [44] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In International conference on machine learning, pages 1312–1320, 2015. [45] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. [46] J. Schmidhuber. Adaptive conﬁdence and adaptive curiosity. Technical report, Citeseer, 1991. [47] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock- hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019. [48] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 12[49] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016. [50] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008. [51] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112, 2014. [52] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y . Mansour. Policy gradient methods for rein- forcement learning with function approximation. In Advances in neural information processing systems, pages 1057–1063, 2000. [53] H. Tang, R. Houthooft, D. Foote, A. Stooke, O. X. Chen, Y . Duan, J. Schulman, F. DeTurck, and P. Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural information processing systems, pages 2753–2762, 2017. [54] D. Warde-Farley, T. Van de Wiele, T. Kulkarni, C. Ionescu, S. Hansen, and V . Mnih. Unsuper- vised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018. [55] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9068–9079, 2018. [56] Y . Zhang, W. Yu, and G. Turk. Learning novel policies for tasks. arXiv preprint arXiv:1905.05252, 2019. 13",
      "meta_data": {
        "arxiv_id": "1907.10247v3",
        "authors": [
          "Yijie Guo",
          "Jongwook Choi",
          "Marcin Moczulski",
          "Shengyu Feng",
          "Samy Bengio",
          "Mohammad Norouzi",
          "Honglak Lee"
        ],
        "published_date": "2019-07-24T05:46:27Z",
        "pdf_url": "https://arxiv.org/pdf/1907.10247v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The main problem addressed is the challenge of reinforcement learning with sparse rewards, where agents rarely receive non-zero feedback, leading to slow gradient-based optimization and myopic behaviors. This paper proposes Diverse Trajectory-conditioned Self-Imitation Learning (DTSIL), a novel approach that learns a trajectory-conditioned policy to follow and expand diverse past trajectories from a memory buffer. Key contributions include a new architecture for a trajectory-conditioned policy that can flexibly imitate diverse demonstrations, demonstrating the importance of diverse past experiences for indirect exploration, and achieving state-of-the-art performance on hard-exploration Atari games (Montezuma’s Revenge and Pitfall) without relying on expert demonstrations or arbitrary state resets.",
        "methodology": "DTSIL maintains a memory buffer of diverse past trajectories, including the best trajectory ending in a specific state and its visitation count. During training, states are sampled from this buffer for either exploitation (highest cumulative rewards) or exploration (inversely proportional to the square root of visitation count). The core is a trajectory-conditioned policy, implemented as a sequence-to-sequence model with an attention mechanism (inspired by neural machine translation), which takes a full demonstration trajectory as input to guide action generation. The policy is trained using a combination of a Reinforcement Learning objective, which provides imitation rewards for visiting states in the demonstration in a soft-order and encourages exploration beyond the demonstration's end, and a Supervised Learning objective, which leverages actions from past trajectories (behavior cloning). Updates to the buffer involve replacing existing trajectories with improved ones ending in similar states or adding novel trajectories to gradually increase diversity.",
        "experimental_setup": "The method was evaluated against baselines including PPO, PPO+EXP (PPO with count-based exploration bonus), PPO+SIL (PPO with Self-Imitation Learning), and DTRA (Diverse Trajectory-conditioned Repeat Actions). Experiments were conducted across various domains: (1) **Apple-Gold Domain**: A grid-world environment with misleading rewards to test avoidance of local optima, using agent location and cumulative positive reward as state embedding. (2) **Atari Games**: Hard-exploration games (Montezuma’s Revenge, Pitfall, Venture) from the Arcade Learning Environment, using agent's ground truth location (from RAM) and accumulated positive reward as state representation, evaluated under 5 billion frames (specifically 3.2B for DTSIL+EXP). (3) **Continuous Control Tasks**: A distant visual navigation task on the Gibson dataset (first-person RGB image, agent location/orientation/cumulative reward as state embedding) and a sparse-reward manipulation (bin picking) task from Surreal Robotics Suite (physical state of robot/object, gripper status, cumulative reward as state embedding). Additional experiments were performed on Deep Sea and Mujoco Maze (details in supplementary material). Performance was measured by average episode reward, best episode reward, and visualizations of trajectories and attention weights.",
        "limitations": "The quality of learned state representations can negatively influence the trajectory-conditioned policy, especially in complex environments where high-level state embeddings are not hand-crafted (e.g., Montezuma's Revenge's dark rooms). Although DTSIL can be modified to use observation sequences for robustness, the challenge of learning good state representations for long-horizon, sparse-reward tasks remains an open problem. In early training stages, DTSIL+EXP's average reward might be worse than direct reward maximization methods as it prioritizes imitating diverse paths for long-term optimality. The paper acknowledges general limitations of RL methods, such as the risk of harmful actions if reward shaping is insufficient, and common concerns regarding memory and computational costs, though it suggests DTSIL is empirically reasonable in this regard.",
        "future_research_directions": "Future work includes further improving DTSIL, particularly for multi-task Reinforcement Learning problems. The variant of DTSIL that uses a hierarchical policy for generalization across multiple MDPs (like different maze structures in the Apple-Gold domain) shows promise and warrants further study. Additionally, developing more principled ways to ensure safety during exploration is highlighted as an important future research direction, especially given the generic issue of RL methods potentially taking harmful steps without sufficient reward shaping or state/action space constraints."
      }
    },
    {
      "title": "Replay Memory as An Empirical MDP: Combining Conservative Estimation with Experience Replay"
    },
    {
      "title": "Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online",
      "abstract": "Recent work has shown that sparse representations -- where only a small\npercentage of units are active -- can significantly reduce interference. Those\nworks, however, relied on relatively complex regularization or meta-learning\napproaches, that have only been used offline in a pre-training phase. In this\nwork, we pursue a direction that achieves sparsity by design, rather than by\nlearning. Specifically, we design an activation function that produces sparse\nrepresentations deterministically by construction, and so is more amenable to\nonline training. The idea relies on the simple approach of binning, but\novercomes the two key limitations of binning: zero gradients for the flat\nregions almost everywhere, and lost precision -- reduced discrimination -- due\nto coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that\nprovides non-negligible gradients and produces overlap between bins that\nimproves discrimination. We first show that FTA is robust under covariate shift\nin a synthetic online supervised learning problem, where we can vary the level\nof correlation and drift. Then we move to the deep reinforcement learning\nsetting and investigate both value-based and policy gradient algorithms that\nuse neural networks with FTAs, in classic discrete control and Mujoco\ncontinuous control environments. We show that algorithms equipped with FTAs are\nable to learn a stable policy faster without needing target networks on most\ndomains.",
      "full_text": "Published as a conference paper at ICLR 2021 FUZZY TILING ACTIVATIONS : A S IMPLE APPROACH TO LEARNING SPARSE REPRESENTATIONS ONLINE Yangchen Pan University of Alberta pan6@ualberta.ca Kirby Banman University of Alberta kdbanman@ualberta.ca Martha White University of Alberta whitem@ualberta.ca ABSTRACT Recent work has shown that sparse representations—where only a small percentage of units are active—can signiﬁcantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used ofﬂine in a pre-training phase. In this work, we pursue a direction that achieves sparsity by design, rather than by learning. Speciﬁcally, we design an activation function that produces sparse representations deterministically by construction, and so is more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the ﬂat regions almost everywhere, and lost precision—reduced discrimination—due to coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We ﬁrst show that FTA is robust under covariate shift in a synthetic online supervised learning problem, where we can vary the level of correlation and drift. Then we move to the deep reinforcement learning setting and investigate both value-based and policy gradient algorithms that use neural networks with FTAs, in classic discrete control and Mujoco continuous control environments. We show that algorithms equipped with FTAs are able to learn a stable policy faster without needing target networks on most domains. 1 1 I NTRODUCTION Representation learning in online learning systems can strongly impact learning efﬁciency, both positively due to generalization but also negatively due to interference (Liang et al., 2016; Heravi, 2019; Le et al., 2017; Liu et al., 2019; Chandak et al., 2019; Caselles-Dupré et al., 2018; Madjiheurem & Toni, 2019). Neural networks particularly suffer from interference—where updates for some inputs degrade accuracy for others—when training on temporally correlated data (McCloskey & Cohen, 1989; French, 1999; Kemker et al., 2018). Recent work (Liu et al., 2019; Ghiassian et al., 2020; Javed & White, 2019; Rafati & Noelle, 2019; Hernandez-Garcia & Sutton, 2019), as well as older work (McCloskey & Cohen, 1989; French, 1991), have shown that sparse representation can reduce interference in training parameter updates. A sparse representation is one where only a small number of features are active, for each input (Cheng et al., 2013). Each update only impacts a small number of weights and so is less likely to interfere with many state values. Further, when constrained to learn sparse features, the feature vectors are more likely to be orthogonal (Cover, 1965), which further mitigates interference. The learned features can still be highly expressive, and even more interpretable, as only a small number of attributes are active for a given input. However, learning sparse representations online remains relatively open. Some previous work has relied on representations pre-trained before learning, either with regularizers that encourage sparsity (Tibshirani, 1996; Xiang et al., 2011; Liu et al., 2019) or with meta-learning (Javed & White, 2019). Other work has trained the sparse-representation neural network online, by using sparsity regularizers online with replay buffers (Hernandez-Garcia & Sutton, 2019) or using a winner-take-all strategy where all but the top activations are set to zero (Rafati & Noelle, 2019). Hernandez-Garcia & 1Code is available at https://github.com/yannickycpan/reproduceRL.git 1 arXiv:1911.08068v3  [cs.LG]  16 Mar 2021Published as a conference paper at ICLR 2021 Sutton (2019) found that many of these sparsity regularizers were ineffective for obtaining sparse representations without high levels of dead neurons, though the regularizers did still often improve learning. The Winner-Take-All (WTA) approach is non-differentiable, and there are mixed results on it’s efﬁcacy, some positive (Rafati & Noelle, 2019) and some negative (Liu et al., 2019). Finally, kernel representations can be used online, and when combined with a WTA approach, provide sparse representations. There is some evidence that using only the closest prototypes—and setting kernel values to zero for the further prototypes—may not hurt approximation quality (Schlegel et al., 2017). However, kernel-based methods can be difﬁcult to scale to large problems, due to computation and difﬁculties in ﬁnding a suitable distance metric. Providing a simpler approach to obtain sparse representations, that are easy to train online, would make it easier for researchers from the broad online learning community to adopt sparse representations and further explore their utility. In this work, we pursue a strategy for what we call natural sparsity—an approach where we achieve sparsity by design rather than by encoding sparsity in the loss. We introduce an activation function that facilitates sparse representation learning in an end-to-end manner without the need of additional losses, pre-training or manual truncation. Speciﬁcally, we introduce a Fuzzy Tiling Activation (FTA) function that naturally produce sparse representation with controllable sparsity and can be conveniently used like other activation functions in a neural network. FTA relies on the idea of designing a differentiable approximate binning operation—where inputs are aggregated into intervals. We prove that the FTA guarantees sparsity by construction. We empirically investigate the properties of FTA in an online supervised learning problem, where we can carefully control the level of correlation. We then empirically show FTA’s practical utility in a more challenging online learning setting—the deep Reinforcement Learning (RL) setting. On a variety of discrete and continuous control domains, deep RL algorithms using FTA can learn more quickly and stably compared to both those using ReLU activations and several online sparse representation learning approaches. 2 P ROBLEM FORMULATION FTA is a generic activation that can be applied in a variety of settings. A distinct property of FTA is that it does not need to learn to ensure sparsity; instead, it provides an immediate, deterministic sparsity guarantee. We hypothesize that this property is suitable for handling highly nonstationary data in an online learning setting, where there is highly correlated data stream and a strong need for interference reduction. We therefore explicitly formalize two motivating problems: the online supervised learning problem and the reinforcement learning (RL) problem. Online Supervised Learning problem setting. The agent observes a temporally correlated stream of data, generated by a stochastic process {(Xt,Yt)}t∈N, where the observations Xt depend on the past {Xt−i}i∈N. In our supervised setting, Xt depends only on Xt−1, and the target Yt depends only on Xt according to a stationary underlying mean function f(x) = E[Yt|Xt = x]. On each time step, the agent observes Xt, makes a prediction fθ(Xt) with its parameterized function fθ, receives target Yt and incurs a prediction error. The goal of the agent is to approximate function f—the ideal predictor—by learning from correlated data in an online manner, unlike standard supervised learning where data is independent and identically distributed (iid). RL problem setting. We formalize the interaction using Markov decision processes (MDPs). An MDP consists of (S,A,P,R,γ ), where Sis the state space, Ais the action space, P is the transition probability kernel, R is the reward function, and γ ∈[0,1] is the discount factor. At each time step t = 1,2,... , the agent observes a state st ∈S and takes an action at ∈A. Then the environment transits to the next state according to the transition probability distribution, i.e., st+1 ∼ P(·|st,at), and the agent receives a scalar reward rt+1 ∈ R according to the reward function R : S×A×S → R. A policy is a mapping from a state to an action (distribution) π: S×A→ [0,1]. For a given state-action pair (s,a), the action-value function under policy πis deﬁned as Qπ(s,a) = E[Gt|St = s,At = a; At+1:∞∼π] where Gt def = ∑∞ t=0 γtR(st,at,st+1) is the return of a sequence of transitions s0,a0,s1,a1,... by following the policy π. The goal of an agent is to ﬁnd an optimal policy that obtains maximal expected return from each state. The policy is either directly learned, as in policy gradient methods (Sutton et al., 1999; Sutton & Barto, 2018), or the action-values are learned and the policy inferred by acting greedily with respect to the action-values, as in Q-learning (Watkins & Dayan, 1992). In either setting, we often parameterize the policy/value function by a neural network (NN). For example, Deep QNetworks (DQN) (Mnih 2Published as a conference paper at ICLR 2021 et al., 2015) parameterizes the action-value function Qθ : S×A↦→ R by a NN. The bootstrap target for updating a state-action value is computed by using a separate target NN Qθ− : S×A↦→ R parameterized by θ−: yt = rt+1 + γmaxa′Qθ−(st+1,a′). The target NN parameter θ−is updated by copying from θevery certain number of time steps. Online deep RL control problems can be highly nonstationary, for two primary reasons. First, the environment itself could be highly nonstationary, or alternatively, partially observable. Second, the data distribution is constantly shifting because of both the changing policy and shifting training targets. The latter can be mitigated by using a target NN as described above, which has become critical in successfully training many deep RL algorithms. However, it potentially slows learning as the new information is not immediately used to update action-values; instead, the slower moving and potentially out-dated target NN is used. Several works reported that successful training without a target NN can improve sample efﬁciency of RL algorithms (Liu et al., 2019; van Hasselt et al., 2018; Fan et al., 2020; Kim et al., 2019; Rafati & Noelle, 2019; Fan et al., 2020; Ghiassian et al., 2020). We show that deep RL algorithms using our activation is able to achieve superior performance without using a target NN, indicating the beneﬁt of applying our method to nonstationary, online problems. 3 B INNING WITH NON-NEGLIGIBLE GRADIENTS In this section, we develop the Fuzzy Tiling Activation (FTA), as a new modular component for neural networks that provides sparse representations. We ﬁrst introduce a new way to compute the binning of an input, using indicator functions. This activation provides guaranteed sparsity but has a gradient of zero almost everywhere. Then, we provide a smoothed version, resulting in non-negligible gradients that make it compatible with back-propagation algorithms. We then prove that the fuzzy version is still guaranteed to provide sparse representation and the sparsity can be easily tuned. 3.1 T ILING ACTIVATION The tiling activation inputs a scalar z and outputs a binned vector. This vector is one-hot, with a 1 in the bin corresponding to the value of z, and zeros elsewhere. Note that a standard activation typically maps a scalar to a scalar. However, the tiling activation maps a scalar to a vector, as depicted in Figure 1(a). This resembles tile coding, which inspires the name Tiling Activation; to see this connection, we include a brief review of tile coding in the Appendix A.1. In this section, we show how to write the tiling activation compactly, using element-wise max and indicator functions. z h1 h2 h3 h4 (a) TA, k= 4 z h1 h2 h3 h4 (b) FTA, k= 4,η = 0.1 z h1 h2 h3 h4 (c) FTA, k= 4,η = 0.25 Figure 1: a) The regular TA mapping R →Rk, with each output element hi corresponds to a different bin. b) The FTA with η >0, permitting both overlap in activation, and nonzero gradient between the vertical red and gray lines. c) Larger values for ηextends the sloped lines further from either side of each plateau, increasing the region that has non-negligible gradients. Assume we are given a range [l,u] for constants l,u ∈R, where we expect the input z∈[l,u]. The goal is to convert the input, to a one-hot encoding, with evenly spaced bins of size δ∈R+. Without loss of generality, we assume that u−lis evenly divisible by δ; if it is not, the range [l,u] could be slightly expanded, evenly on each side, to ensure divisibility. Deﬁne the k-dimensional tiling vector c def = (l,l + δ,l + 2δ,...,u −2δ,u −δ). (1) where k= (u−l)/δ. The tiling activationis deﬁned as φ(z) def = 1 −I+(max(c −z,0) + max(z−δ−c,0)) (2) where I+(·) is an indicator function, which returns 1 if the input is positive, and zero otherwise. The indicator function for vectors is applied element-wise. In Proposition 1, we prove that φ returns a 3Published as a conference paper at ICLR 2021 binned encoding: if ci <z <ci+1, then φ(z) returns ei the one-hot (standard basis) vector with a 1 in the i-th entry and zero elsewhere. For values of zthat fall on the boundary, z= ci, the encoding returns a vector with ones in both thei−1th and ith entries. Consider the below example for intuition. Example. Assume [l,u] = [0 ,1] and set the tile width to δ = 0.25. Then the tiling vector c has four tiles (k = 4): c = (0,0.25,0.5,0.75). If we apply the tiling activation to z = 0.3, because 0.25 <0.3 <0.5, the output should be (0,1,0,0). To see φ(z) does in fact return this vector, we compute each max term max(c −z,0) = (0,0,0.2,0.45) and max(z−δ−c,0) = max(0.05 −c,0) = (0.05,0,0,0). The addition of the two is (0.05,0,0.2,0.45) and so 1 −I+(0.05,0,0.2,0.45) = 1 −(1,0,1,1) = (0,1,0,0). The ﬁrst max extracts those components in c that are strictly greater than z, and the second max extracts those strictly less than z. The addition gives the bins that are strictly greater and strictly less than the bin for z, leaving only the entry corresponding to that activated bin as 0, with all others positive. The indicator function sets all nonzero entries to one and then using one minus this indicator function’s output provides us the desired binary encoding. We rigorously characterize the possible output cases for the activation in the Appendix A.2.1. 3.2 F UZZY TILING ACTIVATION (FTA) The Tiling Activation provides a way to obtain sparse, binary encodings for features learned within a NN. Unfortunately, the tiling activation has a zero derivative almost everywhere as visualized in Figure 1(a). In this section, we provide a fuzzy tiling activation, that has non-zero derivatives and so is amenable to use with backpropagation. To design the FTA, we deﬁne the fuzzy indicator function2 Iη,+(x) def = I+(η−x)x+ I+(x−η) (3) where ηis a small constant for controlling the sparsity. The ﬁrst term I+(η−x) is 1 if x<η , and 0 otherwise. The second term I+(x−η) is 1 if x>η , and 0 otherwise. If x<η , then Iη,+(x) = x, and else Iη,+(x) = 1. The original indicator function I+ can be acquired by setting η= 0. When η >0, the derivative is non-zero for x < η, and zero otherwise. Hence the derivative can be propagated backwards through those nonzero entries. Using this fuzzy indicator function, we deﬁne the following Fuzzy Tiling Activation(FTA) φη(z) def = 1 −Iη,+(max(c −z,0) + max(z−δ−c,0)) (4) where again Iη,+ is applied elementwise. We depict FTA with differentηs in Figure 3.1. For the smaller η, the FTA extends the activation to the neighbouring bins. The activation in these neighbouring bins is sloped, resulting in non-zero deriva- tives. For this smaller η, however, there are still regions where the derivative is zero (e.g.,z= 0.3 in Figure 1(b)). The regions where derivatives are non-zero can be expanded by increasing ηas shown in Figure 1(c). Hence we can adjust ηto control the sparsity level as we demonstrate in Section A.5. F F Figure 2: A visualization of an FTA layer Figure 2 shows a neural network with FTA applied to the second hidden layer and its output yis linear in the sparse representation. FTA itself does not introduce any new train- ing parameters, just like other activation func- tions. For input x, after computing ﬁrst layer h1 = xW1, we apply φη(z) to h1W2 ∈Rd to get the layer h2 of size kd. This layer consists of stacking the k-dimensional sparse encodings, for each element in h1W2. 3.3 G UARANTEED SPARSITY FROM THE FTA We now show that the FTA maintains one of the key properties of the tiling activation: sparsity. The distinction with many existing approaches is that our sparsity is guaranteed by design and hence is 2The word fuzzy reﬂects that an input can partially activate a tile, with a lower activation than 1, as an analogy to the concept of partial inclusion and degrees of membership from fuzzy sets. 4Published as a conference paper at ICLR 2021 not a probabilistic guarantee. We ﬁrst characterize the vectors produced by the FTA, in Proposition 2 with proof in Appendix A.2.2. Then, we provide an upper bound on the proportion of nonzero entries in the generated vector in Theorem 1, with in Appendix A.2.3. Assumption 1. δ <u−l, where kδ = u−lfor k∈N. Theorem 1(Sparsity guarantee for FTA.). For any z∈[l,u],η >0, φη(z) outputs a vector whose number of nonzero entries ∥φη(z)∥0 satisﬁes: ∥φη(z)∥0 ≤2 ⌊η δ ⌋ + 3 Corollary 1. Let ρ∈[0,1) be the desired sparsity level: the maximum proportion of nonzero entries of φη(z),∀z∈[l,u]. Assume ρk≥3, i.e., some inputs have three active indices or more (even with η= 0, this minimal active number is 2). Then ηshould be chosen such that ⌊η δ ⌋ ≤kρ−3 2 or equivalently η≤δ 2 (⌊kρ⌋−1) (5) As an example, for k= 100, δ= 0.05 and a desired sparsity of at least 10% (ρ= 0.1), we can use η = 0.05 2 (⌊100 ×0.1⌋−1) = 0.225. Note that the bound in Theorem 1 is loose in practice as the bound is for any input z. In Appendix A.2.3 and A.5, we theoretically and empirically show that the actual sparsity is usually lower than the upper bound, and quite consistent across inputs. 4 E XPERIMENTS IN SUPERVISED LEARNING UNDER COVARIATE SHIFT In this section, we focus on testing the hypothesis that FTA provides representations that are more robust to learning online on correlated data. Speciﬁcally, we hypothesize that convergence speed and stability for ReLU networks suffer under strongly correlated training data, whereas comparable FTA networks are nearly unaffected. We create a synthetic supervised problem with a relatively simply target function, and focus the investigation on the impact of a drifting distribution on inputs, which results both in covariate shift and creates temporal correlation during training. We also report results on two benchmark image classiﬁcation tasks in the Appendix A.6. The Piecewise Random Walk Problem has Gaussian Xt ∼N(St,β2) with ﬁxed variance β2 and a mean St that drifts every T steps. More precisely, the mean St stays ﬁxed for T timesteps, then takes a step according to a ﬁrst order autoregressive random walk: St = (1 −c)St + Zt where c∈(0,1] and Zt ∼N (0,σ2) for ﬁxed variance σ2. If c = 0, then this process is a standard random walk; otherwise, with c< 1, it keeps St in a bounded range with high probability. Forxt ∼Xt, the training label yt is deﬁned as yt = sin(2πx2 t). This process is designed to modulate the level of correlation—which we call correlation difﬁculty— without changing the equilibrium distribution over Xt. As the correlation difﬁculty dvaries from 0 to 1, the training data varies from low to high correlation: d = 0 recovers iid sampling. All d∈[0,1) share the same equilibrium distribution in Xt. Xt is ergodic and has Gaussian equilibrium distribution N(0,ξ2), with variance ξ2 dependent upon β2,σ2 and c. In particular, the visitation distribution Xt for any training run will converge to the equilibrium distribution. This ensures that measuring loss with respect to the stationary distribution is a fair comparison, because the visitation distribution of Xt is identical across all settings. We depict sample trajectories with low dand high din Figure 3(a). For a rigorous construction of Xt and St, and justiﬁcation for this equilibrium distribution and implementation details, see Appendix A.7. We measure the mean squared error over the equilibrium distribution inXt, for neural networks using FTA and ReLU activations across a range of correlation difﬁculty values. In Figure 3, we can see that FTA outperforms ReLU in two ways. First, FTA converges to a lower loss with less variance across all correlation difﬁculties. Second, FTA only marginally suffers under high difﬁculties d >0.9, whereas the performance of ReLU begins to deteriorate for relatively mild d> 0.5. Note that the FTA reaches a lower error, even on iid data (d= 0). We hypothesize this gap arises because the networks are trained online, with one sample from each Xt being used for each weight update. Figure 23 in the Appendix supports this hypothesis, with the gap vanishing in an identical experiment where 50 samples are drawn from each Xt. 5Published as a conference paper at ICLR 2021 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2) (a) Mild Correlation 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (b) High Correlation 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU (c) ReLU and FTA, d∈[0,1) Figure 3: (a) and (b) contain sample trajectories of Xt which are (a) mildly correlated with d= 0.41 and (b) severely correlated with d= 0.98. Both share the same equilibrium distribution (in gray). (c) A plot of the prediction error (under the equilibrium distribution), averaged over the ﬁnal 2.5K iterations, across a range of difﬁculty settings. All networks are trained for 20k online updates. The lines correspond to the mean of 30 runs, with the shaded region corresponding to 99.9% conﬁdence intervals. The iid setting d= 0 is shown as a dotted line for baseline comparison. 5 E XPERIMENTAL RESULTS IN REINFORCEMENT LEARNING In this section, we empirically study the effect of using FTA in RL. First, we show overall performance on several benchmark discrete and continuous control environments. Second, we compare our method with other simple strategies to obtain sparse representations. Third, we provide insight into different hyper-parameter choices of FTA and suggest potential future directions. Appendix A.4 includes details for reproducing experiments and Appendix A.5 has additional RL experiments. 5.1 A LGORITHMS AND NAMING CONVENTIONS All the algorithms use a two-layer neural network, with the primary difference being the activation used on the last hidden layer. See Appendix A.5 for results using FTA in all the hidden layers. DQN is used for the discrete action environments, and Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2016) for continuous action environments. On each step, all algorithms sample a mini-batch size of 64 from an experience replay (Lin, 1992; Mnih et al., 2015) buffer with maximum size 100k. Note that we keep the same FTA setting across all experiments: we set [l,u] = [ −20,20], δ= η= 2.0, and hence c = {−20,−18,−16,..., 18}, k= 40/2 = 20. We ﬁrst compare to standard DQN agents, with the same architectures except the last layer. DQN: DQN with tanh or ReLU on the last layer (best performance reported). DQN-FTA: DQN with FTA on the last layer. DQN-Large: DQN, but with the last layer of the same size as DQN-FTA. If DQN has a last (i.e., the second) hidden layer of size d, then DQN-FTA has a last layer of size dk, since the FTA activation simply expands the number of features due to binning. Hence, we include DQN-Large with the same feature dimension as DQN-FTA in the last hidden layer. Note that DQN-Large has ktimes more parameters in this last hidden layer than DQN or DQN-FTA. 3 We also compare to several simple strategies to obtain local or sparse features. Radial basis functions (RBFs) have traditionally been used to obtain local features in RL, and recent work has used ℓ2 and ℓ1 regularization directly on activations as a simple baseline (Arpit et al., 2016; Liu et al., 2019). All of these strategies have the same sized last layer as the sparse feature dimension of DQN-FTA. DQN-RBF: DQN using radial basis functions (RBFs) on the last layer, with the centers deﬁned by the same c as FTA: φ(z) = [exp (−(z−c1)2 σ ),..., exp (−(z−ck)2 σ )] where σis the bandwidth parameter. DQN-L2/L1: DQN with ℓ2 or ℓ1 regularization on the activation functions of the ﬁnal hidden layer where there is the same number of units as that in DQN-Large. To the best of our knowledge, no suitable sparse representation approaches exist for RL. SR-NNs for RL were only developed for ofﬂine training (Liu et al., 2019). That work also showed that k-sparse NNs (Makhzani & Frey, 2013) and Winner-Take-All NNs (Makhzani & Frey, 2015) performed signiﬁcantly worse than ℓ2 regularization, where ℓ2 actually performed quite well in most of their 3Please refer to Figure 2. FTA does not augment the number of training parameters in the hidden layer where it is applied; it only augments the training parameters in the immediately next layer. 6Published as a conference paper at ICLR 2021 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode DQN-FTA DQN DQN-Large (a) Acrobot (v1) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 2000 1500 1000 500  (b) MountainCar (v0) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 100 200 300 400 500 (c) CartPole (v1) 0 1 2 3 4 5 1e5 400 200 0 200 (d) LunarLander (v2) Figure 4: Evaluation learning curves of DQN-FTA(black), DQN(red), and DQN-Large(blue), showing episodic return versus environment time steps. The dotted line indicates algorithms trained with target networks. The results are averaged over 20 runs and the shading indicates standard error. The learning curve is smoothed over a window of size 10 before averaging across runs. experiments. One other possible option is to use Tile Coding NNs (Ghiassian et al., 2020), which ﬁrst tile code inputs before feeding them into the neural network. This approach focuses on using discretization to break, or overcome, incorrect generalization in the inputs; their goal is not to learn sparse representations. This approach is complementary, in that it could be added to all the agents in this work. Nonetheless, because it is one of the only papers with a lightweight approach to mitigate interference in online RL, we do compare to it in the Appendix A.5.2. 5.2 O VERALL PERFORMANCE In this section, we demonstrate the overall performance of using FTAs on both discrete and continuous control environments. Our goals are to investigate if we can 1) obtain improved performance with FTA, with ﬁxed parameter choices across different domains; 2) improve stability in learning with FTA; and 3) see if we can remove the need to use target networks with FTA, including determining that learning can in fact be faster without target networks and so that it is beneﬁcial to use FTA without them. All experiments are averaged over 20 runs (20 different random seeds), with ofﬂine evaluation performed on the policy every 1000 training/environment time steps. 1 2 3 4 5 Time Steps 1e5 200 1050 Average Return per Episode DDPG DDPG-Large DDPG-FTA (a) InvertedPendu 0.0 0.5 1.0 1.5 2.0 1e6 0 500 1000 1500 2000 (b) Hopper 0.0 0.5 1.0 1.5 2.0 1e6 0 1000 2000 3000 (c) Walker2d 0 1 2 3 4 5 1e5 0 2000 4000 6000 8000 (d) InvertedDouble 0.0 0.5 1.0 1.5 2.0 1e6 0 25 50 75 100 125 150 (e) Swimmer Figure 5: Evaluation learning curves of DDPG-FTA(black), DDPG(red), and DDPG-Large(blue), averaged over 10 runs with shading indicating standard error. The dotted line indicates algorithms trained with target networks. The learning curve is smoothed over a window of size 30 before averaging across runs. Discrete control. We compare performance on four discrete-action environments from Ope- nAI (Brockman et al., 2016): MountainCar, CartPole, Acrobot and LunarLander. We use 64 ×64 ReLU hidden units on all these domains. Since FTA has k = 20 , this yields 64 ×20 = 1280 dimensional sparse features. Hence, DQN-Large, use two layers ReLU with size 64 ×1280. We plot evaluation learning curves, averaged over20 runs, in Figure 4. The results show the following. 1) With or without using a target network, DQN with FTA can signiﬁcantly outperform the version without using FTA. 2) FTA has signiﬁcantly lower variability across runs (smaller standard errors) in most of the ﬁgures. 3) DQN-FTA trained without a target network outperforms DQN-FTA trained with a target network, which indicates a potential gain by removing the target network. 4) Without using FTA, DQN trained without a target network cannot perform well in general, providing further evidence for the utility of sparse feature highlighted in previous works (Liu et al., 2019; Rafati & Noelle, 2019). 5) Simply using a larger neural network does not obtain the same performance improvements, and in some cases signiﬁcantly degrades performance. The exception to these conclusions is LunarLander, where DQN-FTA performed similarly to DQN and actually performed slightly better with target networks. On deeper investigation, we found that this was due to using a tiling bound of [−20,20], which we discuss further in Section 5.4. When using [−1,1], DQN-FTA performs much better and results are consistent with the other domains. 7Published as a conference paper at ICLR 2021 Continuous control.We compare performance on continuous control tasks from Mujoco (Todorov et al., 2012). For these experiments, we use DDPG with exactly the same FTA settings as in the above discrete control domains to show its generality. Corresponding to the discrete control domains, we apply FTA to the critic network and do not use a target network to train it. As seen in Figure 5, on most domains, DDPG-FTA achieves comparable and sometimes signiﬁcantly better performance to DDPG with a target network. However, Figure 5 (e) highlights that FTA is not always sufﬁcient on its own to achieve superior performance. Factors such as the exploration strategy, other hyper-parameters, and neural network architecture may also play an important role on such challenging tasks. 5.3 C OMPARISON WITH OTHER REPRESENTATION LEARNING APPROACHES FTA provides clear beneﬁts, but it is natural to ask if other simple strategies that provide sparse or local features could have provided similar beneﬁts. We compare to DQN-RBF, DQN-L2 and DQN-L1, on the discrete action environments, shown in Figure 6. We ﬁnd the following. 1) FTA performs consistently well across all environments using a ﬁxed parameter setting; none of the other approaches achieve consistent performance, even though we tuned their parameters per environment. 2) Both the ℓ1 and ℓ2 approaches have a high variance across different random seeds. 3) The RBF variant can do better than the ℓ1 and ℓ2 approaches but is worse than our algorithm. It is a known issue that RBF is sensitive to the bandwidth parameter choice and we observe similar phenomenon. It is also known that the exponential activation can be problematic in the back-propagation process. We empirically investigate the overlap and instance sparsities of each algorithm in the Appendix A.5. 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode DQN-L2 DQN-L1 DQN-RBF DQN-FTA (a) Acrobot (v1) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 2000 100  (b) MountainCar (v0) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 100 200 300 400 500 (c) CartPole (v1) 0 1 2 3 4 5 1e5 200 100 0 100 200 (d) LunarLander (v2) Figure 6: Evaluation learning curves of DQN-FTA(black), DQN-RBF(forest green), DQN- L2(red), and DQN-L1(blue), averaging over 20 runs with the shade indicating standard error. All algorithms are trained without using target networks. The learning curve is smoothed over a window of size 10 before averaging across runs. 5.4 C HOOSING THE HYPER -PARAMETERS FOR FTA In this section, we provide some insight into selecting the hyper-parameters of FTA. We argue that η= δcan be used as a rule of thumb. Then we show that given an appropriate tiling bound (i.e., u value), DQN-FTA performs well, with a reasonably large number of bins, i.e., a reasonably small tile width δ. However, we do observe a certain level of sensitivity to the tiling bound, which may partially explain the worse performance of DQN-FTA on LunarLander and some Mujoco domains. Sparsity control parameter. The purpose of the parameter ηis to provide a nonzero gradient for training an NN via backpropagation. Since FTA is an one-to-many mapping, it gives a nonzero gradient as long as any single element in the resulting vector provides a nonzero gradient. Setting η= δ(i.e., the tile width) is the minimum value to guarantee nonzero gradient as we visualize in the Figure 1(c) as long as FTA’s input is within the tiling bound. In all of our experiments, we ﬁxη= δ unless otherwise speciﬁed. In the Appendix A.5.4, we show that DQN-FTA does reasonably well across a broad range of η,δ parameter settings. Number of tiles/tile width. In Figure 7(c), we show that on LunarLander, with u = 1.0, DQN- FTA performs well when the tile width δ = 2 u/k is reasonably small (i.e., number of tiles k is reasonably large). We show the learning curves of DQN-FTA with u = 1 .0, in Figure 7(a), by using number of tiles from k ∈{2,4,6,8,10,12,14,16}, providing sparse feature dimension 64 ×k ∈{128,256,384,512,640,768,896,1024}. We also examine the performance of DQN trained by using NN size 64 ×k, for k ∈{2,4,6,8,10,12,14,16}. In Figure 7(b), one can see 8Published as a conference paper at ICLR 2021 0 1 2 3 4 5 1e5 400 300 200 100 0 100 200 300 2 4 6 8 10 12 14 16 (a) Vary # of tiles k 0 1 2 3 4 5 1e5 400 300 200 100 0 100 200 300 128 256 384 512 640 768 896 1024 (b) Vary # of hidden units 0 1 2 3 4 5 1e5 200 100 0 100 200 300 0.01 0.1 1.0 10.0 20.0 (c) Bound sensitivity 0 1 2 3 4 5 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (d) Sparsity ratio Figure 7: (a) Evaluation learning curves showing episodic return versus environment time steps of DQN-FTA using different number of tiles kas labeled. This is equivalent to varying tile width δas δ= 2u/k. The results are averaged over 5 random seeds. (b) Evaluation learning curves of DQN without using a target NN as we change the number of the second hidden layer units as labeled in the ﬁgure. (c) Evaluation learning curves of DQN-FTA uses [−u,u] as tiling bound where u∈{0.01,0.1,1.0,10.0,20.0}. The results are averaged over 10 runs. The standard error is not shown but is sufﬁcient small to differentiate the two groups (i.e., {0.1,1.0} and {0.01,10.0,20.0}) corresponding to appropriate bound and too large/small bound. To generate this ﬁgure, we ﬁx on using FTA with 20 tiles (i.e., tile width δ = 2u/20). (d) The overlap-instance sparsity ratio v.s. environment time steps. The standard error is very small and is ignored. The results are averaged over 10 runs. that increasing NN size does not provide a clear beneﬁt to DQN; however, DQN-FTA performs signiﬁcantly better as kincreases, in Figure 7(a). Sensitivity to tiling bound. We ﬁnd that the tiling bound [l,u] can be sensitive. Intuitively, if we set l,u extremely small, then the input of FTA may go out of the boundary and FTA provides zero gradient again. When we set the bound too large, many inputs may hit the same bins, both resulting in many dead neurons and increasing interference. In Figure 7(c), we see that very small u= 0.01 and big u= 10,20 perform poorly in LunarLander, and interim values of u= 0.1,1 perform well. We further examine the corresponding representation interference measured by the overlap sparsity divided by instance sparsity, in Figure 7(d). Instance sparsity is the proportion of nonzero entries in the feature vector for each instance. Overlap sparsity (French, 1991) is deﬁned as overlap(φ,φ′) =∑ iI(φi ̸= 0)I(φ′ i ̸= 0)/(kd), given two kd-dimensional sparse vectors φ,φ′. Low overlap sparsity potentially indicates less feature interference between different input samples. Overlap sparsity divided by instance sparsity represents the proportion of overlapped entries among activated ones. We can see in Figure 7(d) that large uincreases this ratio, indicating increased interference, which may explain the worse performance of DQN-FTA withu= 10,20 in Figure 7(c). On the other extreme, when the bound is too small, u = 0.01, the sparsity ratio is low but performance is poor. This is likely because generalization actually becomes too low at this level, reducing sample efﬁciency. We refer to Appendix A.5.6 for additional experiments about gradient interference. 6 D ISCUSSION In this work, we proposed the idea of natural sparsity, aiming at achieving sparsity without learning in a deep learning setting. We design an activation by drawing on the idea of binning which produces a one-hot encoding of an input. We provide a Fuzzy Tiling Activation (FTA) with a sparsity control mechanism that enables backpropagation of gradients, and potentially improves generalization by increasing active feature units. We show that the FTA still has sparsity guarantees, related to the choice of η. The FTA provides sparse representations by construction, and so it is much easier to use when learning online than conventional sparse representation learning methods. We highlight that FTA is robust to high levels of covariate shift, in a synthetic supervised learning problem. We then show across several discrete and continuous control RL environments that using the FTA signiﬁcantly improve learning efﬁciency and stability, and in most cases even removes the need of target networks. Our work suggests several promising future research directions. The ﬁrst is to further investigate the choices for the tiling bound. An adaptive approach for the tiling bound is particularly relevant for ease-of-use, as we found more sensitivity to this choice. Second, FTA is actually complementary to approaches (Liu et al., 2018; Riemer et al., 2019; Javed & White, 2019) designed to explicitly mitigate interference. An interesting direction is to combine FTA with these methods to reduce interference. Last, it is important to study how we should balance generalization and discrimination. We observe that an algorithm’s performance degrades when either the overlap sparsity is too high or too low. Learning sparse representation with an appropriate overlap sparsity may be most desirable. 9Published as a conference paper at ICLR 2021 7 A CKNOWLEDGEMENTS We would like to thank all anonymous reviewers for their helpful feedback. We thank Han Wang for carefully reading our paper and providing helpful suggestions to improve our presentation. We acknowledge the funding from the Canada CIFAR AI Chairs program and Alberta Machine Intelligence Institute. REFERENCES Abadi, M. and et. al. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorﬂow.org, 2015. Arpit, D., Zhou, Y ., Ngo, H., and Govindaraju, V . Why regularized auto-encoders learn sparse representation? In International Conference on Machine Learning, pp. 136–144, 2016. Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences , pp. 15849–15854, 2019. Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv, 2016. Caselles-Dupré, H., Ortiz, M. G., and Filliat, D. Continual state representation learning for reinforce- ment learning using generative replay. arXiv:1810.03880, 2018. Chandak, Y ., Theocharous, G., Kostas, J., Jordan, S., and Thomas, P. Learning action representations for reinforcement learning. In International Conference on Machine Learning, pp. 941–950, 2019. Cheng, H., Liu, Z., Yang, L., and Chen, X. Sparse representation and learning in visual recognition: Theory and applications. Signal Processing, pp. 1408–1425, 2013. Special issue on Machine Learning in Intelligent Image Processing. Cover, T. M. Geometrical and Statistical Properties of Systems of Linear Inequalities with Applica- tions in Pattern Recognition. IEEE Trans. Electronic Computers, 1965. Fan, J., Wang, Z., Xie, Y ., and Yang, Z. A theoretical analysis of deep q-learning. InProceedings of the 2nd Conference on Learning for Dynamics and Control, pp. 486–489, 2020. French, R. M. Using semi-distributed representations to overcome catastrophic forgetting in connec- tionist networks. In Annual Cognitive Science Society Conference, 1991. French, R. M. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, pp. 128–135, 1999. Ghiassian, S., Raﬁee, B., Lo, Y . L., and White, A. Improving performance in reinforcement learning by breaking generalization in neural networks. International Conference on Autonomous Agents and Multi-agent Systems, 2020. Glorot, X. and Bengio, Y . Understanding the difﬁculty of training deep feedforward neural networks. International Conference on Artiﬁcial Intelligence and Statistics, 2010. Grunwald, G., Hyndman, R., and Tedesco, L. A uniﬁed view of linear ar (1) models. 1995. Gu, S., Lillicrap, T. P., Sutskever, I., and Levine, S. Continuous Deep Q-Learning with Model-based Acceleration. In International Conference on Machine Learning, pp. 2829–2838, 2016. Heravi, J. R. Learning Representations in Reinforcement Learning . PhD thesis, University of California, Merced, 2019. Hernandez-Garcia, J. F. and Sutton, R. S. Learning sparse representations incrementally in deep reinforcement learning. Workshop on Continual Learning at Advances in Neural Information Processing Systems, 2019. 10Published as a conference paper at ICLR 2021 Javed, K. and White, M. Meta-learning representations for continual learning. In Advances in Neural Information Processing Systems, pp. 1818–1828. 2019. Kemker, R., McClure, M., Abitino, A., Hayes, T. L., and Kanan, C. Measuring catastrophic forgetting in neural networks. In AAAI conference on Artiﬁcial Intelligence, 2018. Kim, S., Asadi, K., Littman, M., and Konidaris, G. Deepmellow: Removing the need for a target network in deep q-learning. International Joint Conference on Artiﬁcial Intelligence, pp. 2733– 2739, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015. Le, L., Kumaraswamy, R., and White, M. Learning sparse representations in reinforcement learning with sparse coding. In International Joint Conference on Artiﬁcial Intelligence, pp. 2067–2073, 2017. LeCun, Y . and Cortes, C. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/, 2010. Leurent, E. An environment for autonomous driving decision-making, 2018. URL https:// github.com/eleurent/highway-env. Liang, Y ., Machado, M. C., Talvitie, E., and Bowling, M. State of the art control of atari games using shallow reinforcement learning. International Conference on Autonomous Agents & Multiagent Systems, pp. 485–493, 2016. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y ., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016. Lin, L.-J. Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching. Machine Learning, 1992. Liu, V ., Kumaraswamy, R., Le, L., and White, M. The utility of sparse representations for control in reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pp. 4384–4391, 2019. Liu, X., Masana, M., Herranz, L., Van de Weijer, J., López, A. M., and Bagdanov, A. D. Rotate your networks: Better weight consolidation and less catastrophic forgetting. In International Conference on Pattern Recognition, pp. 2262–2268, 2018. Madjiheurem, S. and Toni, L. Representation learning on graphs: A reinforcement learning applica- tion. International Conference on Artiﬁcial Intelligence and Statistics, 2019. Makhzani, A. and Frey, B. k-sparse autoencoders. arXiv:1312.5663, 2013. Makhzani, A. and Frey, B. Winner-take-all autoencoders. In Advances in Neural Information Processing Systems, 2015. McCloskey, M. and Cohen, N. J. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. Psychology of Learning and Motivation, 1989. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., and others. Human-level control through deep reinforcement learning. Nature, 2015. Moore, A. W. and Atkeson, C. G. Prioritized sweeping: Reinforcement learning with less data and less time. Machine learning, pp. 103–130, 1993. Pan, Y ., Zaheer, M., White, A., Patterson, A., and White, M. Organizing experience: a deeper look at replay mechanisms for sample-based planning in continuous state domains. In International Joint Conference on Artiﬁcial Intelligence, pp. 4794–4800, 2018. 11Published as a conference paper at ICLR 2021 Pan, Y ., Yao, H., Farahmand, A.-m., and White, M. Hill climbing on value estimates for search-control in dyna. International Joint Conference on Artiﬁcial Intelligence, 2019. Rafati, J. and Noelle, D. C. Learning sparse representations in reinforcement learning. arXiv:1909.01575, 2019. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., , and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. In International Conference on Learning Representations, 2019. Schlegel, M., Pan, Y ., Chen, J., and White, M. Adapting Kernel Representations Online Using Submodular Maximization. In International Conference on Machine Learning, 2017. Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. SIGART Bulletin, 2(4):160–163, 1991. Sutton, R. S. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems 8 , pp. 1038–1044. MIT Press, 1996. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y . Policy gradient methods for reinforce- ment learning with function approximation. In International Conference on Neural Information Processing Systems, 1999. Sutton, R. S., Szepesvári, C., Geramifard, A., and Bowling, M. Dyna-style planning with linear function approximation and prioritized sweeping. In Conference on Uncertainty in Artiﬁcial Intelligence, pp. 528–536, 2008. Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, pp. 267–288, 1996. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In International Conference on Intelligent Robots and Systems, 2012. van Hasselt, H., Doron, Y ., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement learning and the deadly triad. arXiv:1812.02648, 2018. Watkins, C. J. C. H. and Dayan, P. Q-learning. Machine Learning, 1992. Xiang, Z., Xu, H., and Ramadge, P. J. Learning sparse representations of high dimensional data on large scale dictionaries. In Advances in Neural Information Processing Systems, 2011. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. 2017. 12Published as a conference paper at ICLR 2021 A A PPENDIX This appendix includes the following contents: 1. Section A.1 brieﬂy reviews tile coding which inspires FTA and the naming. 2. Section A.2 shows the proofs for theorems about sparsity guarantee in this paper. 3. Section A.3 discusses an alternative way to handle the case when the inputs of FTA go out of the boundary of the tiling vector c. 4. Section A.4 includes experimental details of Section 5 for reproducible research. 5. Section A.5 presents additional experiments in reinforcement learning setting. 6. Section A.6 reports the results of FTA on two popular image classiﬁcation datasets: Mnist (LeCun & Cortes, 2010) and Mnistfashion (Xiao et al., 2017). 7. Section A.7 includes the details of the synthetic supervised learning experiment from Section 4. A.1 T ILE CODING REVIEW We give a brief review of tile coding here, as tile coding inspires our Fuzzy Tiling Activation (and its naming). Tile coding is a generalization of state aggregation, that uses multiple tilings (aggregations) to improve discrimination. For input z∈[0,1], state-aggregation would map zto a one-hot vector with a 1 in the corresponding bin (bin can be also called tile), with kthe number of bins discretized into intervals of length δ. In tile coding, multiple such tilings of the input are concatenated, where each tiling is offset by a small amount. This is depicted in Figure 8, where we show two tilings, one covering from [−0.05,1] and the other from [0.0,1.05] both with k= 4 and δ = 1.05/4 = 0.2625. The resulting feature vector is a concatenation of these two tilings, to produce 8 features. Two nearby inputs z = 0.3 and z′= 0.55 would be aggregated together in state-aggregation, making it impossible to produce different values for those inputs. Tile coding, on the other hand, would cause them to share one feature in the second tiling, but each have a distinct feature that enables them to have different values. The large bins/tiles allows for some information sharing, for fast generalization; and the overlapping tilings allows for improved discrimination. 1 2 3 4  -0.05 1.05  tiling1 tiling25 6 7 8  z z'  1  Figure 8: Tile coding maps a scalar to an 8 di- mensional binary vector. For example, z activates the second tile on both tilings which gives the fea- ture vector (0,1,0,0,0,1,0,0). Similarly, z′ ↦→ (0,0,1,0,0,1,0,0). Unfortunately, tile coding can scale exponentially in the dimensiondof the inputs. Each dimension is discretized to a granularity of δ(kregions), with the cross-product between all these regions resulting in kd bins. One natural approach to make binning more scalable and more practically usable, is to combine it with neural networks (NN). Our activation function—Fuzzy Tiling Activation—enables a differentiable binning operation. It is known that those simple domains which are able to use tile coding never require a separate slowly moving weight vector (i.e., the counterpart of a target network in linear function approximation setting) (Sutton, 1996; Sutton & Barto, 2018). Based on this observation, Liu et al. (2019) indicates that in a deep learning setting, when using sparse representation, the target network can be removed and the sample efﬁciency can be improved, coinciding with many of our empirical results. It is sensible that the target network possibly slower down learning. Because at each environment time step, the new information is not immediately utilized to estimate the bootstrap target and this could slower down learning. Accurately estimating bootstrap targets may be highly beneﬁcial in Dyna- style model-based reinforcement learning Sutton (1991); Sutton et al. (2008), as the planning stage typically involves efﬁcient improvement of value estimates (Moore & Atkeson, 1993; Pan et al., 2018; Gu et al., 2016; Pan et al., 2019). It may be an interesting future direction to carefully study the effect of removing the target network in Dyna-style planning. 13Published as a conference paper at ICLR 2021 A.2 P ROOFS We now provide the proof for Proposition 1, Proposition 2, Theorem 1 and Corollary 1 below. A.2.1 P ROOF FOR PROPOSITION 1 For convenience, we deﬁne the k-dimensional one-hot vector ei whose ith entry is one and otherwise zero. Proposition 1. Under Assumption 1, for any z∈[l,u], 1. If ci <z <ci+1 for i∈[k−1] or ck <z, then φ(z) = ei 2. If z= ci for i∈{2,...,k }, then φ(z) = ei−1 + ei 3. If z= c1, then φ(z) = e1 Proof. In all thee cases, the ﬁrst max operation in φ is max(c −z,0) = (0,0,..., ci+1 −x,ci+2 −z,..., ck −z). To understand the output of the second max operation, we look at the three cases separately. Case 1:Because ci < z <ci+1, we know that ci−1 < z−δ <ci. This implies the second max operation in φ is: max(z−δ−c,0) = (z−δ−c1,z −δ−c2,...,z −δ−ci−1,0,0,..., 0) Therefore, the sum of both max operations max(c −z,0) + max(z−δ−c,0) has positive elements everywhere except the ith position, which is zero. Hence I+(max(c −z,0) + max(z−δ−c,0)) gives a vector where every entry is 1 except the ith entry which is 0. Then 1 −I+(max(c −z,0) + max(z−δ−c,0)) = ei. Case 2:If z= ci,i ∈{2,...,k }, then ci−1 = z−δ, and max(z−δ−c,0) = (z−δ−c1,z −δ−c2,...,z −δ−ci−2,0,..., 0). It follows that max(c −z,0) + max(z−δ−c,0) has exactly two zero entries, at indices i−1 and i. This gives 1 −I+(max(c −z,0) + max(z−δ−c,0)) = ei−1 + ei, a vector with ones at indices i−1 and i. Case 3:When z= c1, max(z−δ−c,0) is a zero vector and max(c −z,0) is positive everywhere except the ﬁrst entry, which is zero. Again this gives 1 −I+(max(c −z,0) + max(z−δ−c,0)) = e1. A.2.2 P ROOF FOR PROPOSITION 2 Proposition 2. Let Ibe the set of indices where φ(z) is active (from Theorem 1 we know it can only contain one or two indices). Under Assumption 1, for any z∈[l,u], the function φη(z) = φ(z) + ∆, where ∆ is mostly zero, with a few non-zero entries. The vector ∆ has the following properties: 1. ∆ is zero at indices i∈I, i.e., φη(z) equals φ(z) at indices i∈I. 2. ∆ is non-zero at indices {j|j /∈I,j ∈[k],0 <z −δ−cj ≤η,0 <cj −z≤η}. Proof. Part 1.Let the ith entry be active in φ(z). We have one of the three cases hold as stated in Theorem 1. Assume ci <z <ci+1. Note that max(c −z,0) = (0,0,..., ci+1 −z,..., ck −z) max(z−δ−c,0) = (z−δ−c1,...,z −δ−ci−1,0,..., 0), taking the sum of the above two equations gives us a vector as following: (z−δ−c1,...,z −δ−ci−1,0,ci+1 −z,..., ck −z) (6) Then applying Iη,+(·) to vector equation 6 gives us a vector retaining elements ≤ηand all other elements become 1. Hence the ith position is zero after applying Iη,+(·) to vector equation 6. Using 14Published as a conference paper at ICLR 2021 one minus this vector would give us a vector with only the ith is one. But this is exactly φη(z). And since φη(z) = φ(z) + ∆, and the ith entry of φ(z) is also one, the ith entry of ∆ must be zero. Similar reasoning applies to the cases when z= ci,i ∈{2,...,k }or z= c1. Part 2.Note that applyingIη,+(·) to the vectormax(c−z,0)+max(z−δ−c,0) keeps all elements no more than ηand making all other elements one. As a result,1−Iη,+(max(c−z,0)+max(z−δ−c,0)) would give a vector zero everywhere except those entries in max(c −z,0) + max(z−δ−c,0) which are ≤η. The set of indices which are ≤ηin the vector max(c −z,0) + max(z−δ−c,0) can be written as {j|j ∈[k],0 < z−δ −cj ≤η,0 < cj −z ≤η}, which is also the set of indices where φη(z) is nonzero. Since φη(z) = φ(z) + ∆ and φ(z) has nonzero entries in the indices Iand ∆ has zero values at those entries by Part 1, then ∆ must have nonzero entries at {j|j /∈I,j ∈[k],0 <z −δ−cj ≤η,0 <cj −z≤η}. A.2.3 P ROOF FOR THEOREM 1 Theorem 1. Sparsity guarantee for FTA.For any z∈[l,u],η >0, φη(z) outputs a vector whose number of nonzero entries ∥φη(z)∥0 satisﬁes: ∥φη(z)∥0 ≤2 ⌊η δ ⌋ + 3 Proof. Similar to the proof of Theorem 1, we divide to three cases to prove the result. Case 1.Consider the case that ci <z <ci+1,i ∈[k−1]. Note that the number of nonzero entries in φη is equal to the number of entries less than ηin the vector equation 6, hence we can count the number of entries less than ηin (z−δ−c1,...,z −δ−ci−1,0,ci+1 −z,ci+2 −z,..., ck −z). First, we count the number of entries that are less than or equal to ηon the left side of the ith position. Since the ith position is zero, which indicates z−δ−ci <0, hence z−δ−ci−1 −δ <0 and it follows that 0 < z−δ−ci−1 < δ. Then δ < z−δ−ci−1 + δ = z−δ−ci−2 < 2δ. Hence (j−1)δ < z−δ−ci−j < jδ,j∈{1,...,i −1}. Assume there are mentries ≤ηon the the left side of the ith. Then z−δ−ci−m ≤η. It follows (m−1)δ <ηand hence m≤⌊η δ⌋+ 1. Hence the total number of elements ≤ηon the left side of the ith position is at most ⌊η δ⌋+ 1. Second, count the number of entries ≤η on the right side of the ith position. Since ci −z <0, 0 <ci + δ−z= ci+1 −z <δ,0 <ci+2 −z <2δ,.... Hence the possible number of entries ≤η on the right side of ith position is at most ⌊η δ⌋+ 1. As a result, together with the ith position which is 0 ≤η, the possible number of nonzero entries in this case is at most 2⌊η δ⌋+ 3. Case 2.When z= ci,i ∈{2,...,k }, we count the number of entries less than ηin the vector max(c −z,0) + max(z−δ−c,0) = ((i−1)δ,...,2δ,δ,0,0,δ,2δ,...,(k−i)δ) Again, we attempt to count the number of entries less than η in this vector by considering (i− 1)δ,...,2δ,δ and δ,2δ,...,(k−i)δrespectively. We follow the exactly same argument as above and now we have two zero entries ati−1,ith positions. The difference is that, the number of entries less than ηin the vector ((i−1)δ,...,2δ,δ can be at most ⌊η δ⌋. As a result, the number of nonzero entries in this case is still at most 2⌊η δ⌋+ 2. Case 3.When z= c1, max(z−δ−c,0) is a zero vector and max(c −z,0) + max(z−δ−c,0) is positive everywhere except the ﬁrst entry, which is zero. Then we simply count the number of entries ≤ηon the right side of the 0th position, i.e. j ∈{1,...,k }. Similar to the analysis in the above Case 1, the possible number of entries ≤ηon the right side of 0th position is at most ⌊η δ⌋. Hence in this case, the number of nonzero entries is at most ⌊η δ⌋+ 1. In summary, the number of nonzero entries does not exceed 2⌊η δ⌋+ 3. This completes the proof. Remark. As we empirically demonstrated in Table 1 and below Figure 10 and Figure 9, the actual sparsity achieved by FTA is lower than the upper bound. This is because our upper bound is for any possible input z. Consider that in Case 1 in the above proof, we count the number of entries that 15Published as a conference paper at ICLR 2021 are less than or equal to ηon the left side and right side of the ith position. There are ⌊η δ⌋+ 1 such entries on both sides only when zis exactly equal to ci+ci−1 2 , which is unlikely to happen in practice. A.2.4 P ROOF FOR COROLLARY 1 Corollary 1 Let ρ∈[0,1) be the desired sparsity level: the maximum proportion of nonzero entries of φη(z),∀z∈[l,u]. Assume ρk≥3, i.e., some inputs have three active indices or more (even with η= 0, this minimal active number is 2). Then ηshould be chosen such that ⌊η δ ⌋ ≤kρ−3 2 or equivalently η≤δ 2 (⌊kρ⌋−1) (7) Proof. Because ∥φη(z)∥0 ≤⌊kρ⌋, from Theorem 1 it is sufﬁcient to pick ηsuch that 2⌊η δ⌋+ 3 ≤ ⌊kρ⌋≤ kρ. This gives ⌊η δ⌋≤ (⌊kρ⌋−3)/2 ≤(kρ−3)/2. Additionally, we know η δ −1 ≤⌊η δ⌋≤ (⌊kρ⌋−3)/2, giving the second inequality. A.3 M ORE DISCUSSION ABOUT FTA Our development of the FTA assumed that the inputs are bounded in the range [l,u] (recall that c = [l,l + δ,l + 2δ,...,u]). This is not guaranteed for the standard inputs to activations, namely z= x⊤w for some weight vector w. The FTA can still be used if z /∈[l,u], but then the gradient of the FTA will be zero. This means that the weights w cannot be adjusted for inputs where zbecomes too large or too small. This issue is usually called gradient vanish, which is common in many popular, existing activation functions such as ReLU, tanh, sigmoid, etc. In our main paper, we proposed to use different tiling vectors (i.e., cs) with a broad range of bounds to different components in the input vector of FTA to reduce the chance of vanishing gradients. Here we discuss two other possible ways to avoid this issue: 1) use a squashing activation before handingz to the FTA and 2) regularize (penalize) zthat falls outside the range [l,u]. For example, tanh can be applied ﬁrst to produce z= tanh(x⊤w) ∈[−1,1]. Though the simplest strategy, using tanh function can be problematic in deep neural networks due to gradient vanishing problems. An alternative is to use a penalty, that pushes out-of-boundary zback into the chosen range [−u,u] r(z) def = I(|z|>u) ◦|z| (8) This penalty is easily added to the loss, giving gradient∂r(z) ∂z = I(z >u)−I(z <−u). For example, z= max(x⊤w,0), which might produce a value greater than u. If z >u, then the gradient pushes the weights w to decrease z. It should be noted that the number of nonzero entries can only decrease when zgoes out of boundary. However, in our experiments in our main paper, we found such out of boundary loss is unnecessary on all of our tested domains. Furthermore, we further verify that our FTA performs stably across different weights for the above regularization in Section A.5. A.4 R EPRODUCING EXPERIMENTS FROM SECTION 5 Common settings.All discrete action domains are from OpenAI Gym (Brockman et al., 2016) with version 0.14.0. Deep learning implementation is based on tensorﬂow with version 1.13.0 (Abadi & et. al, 2015). We use Adam optimizer (Kingma & Ba, 2015), Xavier initializer (Glorot & Bengio, 2010), mini-batch size b= 64, buffer size 100k, and discount rate γ = 0.99 across all experiments. We evaluate each algorithm every1k training/environment time steps. Algorithmic details.We use64×64 ReLU units neural network for DQN and200×100 ReLU units neural network for DDPG. All activation functions are ReLU except: the output layer of the Qvalue is linear. The weights in output layers were initialized from a uniform distribution [−0.003,0.003]. Note that we keep the same FTA setting across all experiments: we set [l,u] = [−20,20]; we set δ = η = 2.0, c = {−20,−18,−16,..., 18}, and hence k = 40/2 = 20 . This indicates that the DQN-Large and DDPG-Large versions have 64 ×20 = 1280 and 100 ×20 = 2000 ReLU units in the second hidden layer. For RBF coding, we set the bandwidth as σ= 2.0, and uses the same tiling (i.e. c vector) as our FTA. 16Published as a conference paper at ICLR 2021 Meta-parameter details.For DQN, the learning rate is 0.0001 and the target network is updated every 1k steps. For DDPG, the target network moving rate is 0.001 and the actor network learning rate is 0.0001, critic network learning rate is 0.001. For l1,l2 regularization variants, we optimize its regularization weight from {0.1,0.01,0.001,0.0001}on MountainCar, then we ﬁx the chosen optimal weight 0.01 across all domains. Environmental details on discrete control domains.We set the episode length limit as 2000 for MountainCar and keep all other episode limit as default settings. We use warm-up steps 5000 for populating the experience replay buffer before training. Exploration noise is 0.1 without decaying. During policy evaluation, we keep a small noise ϵ= 0.05 when taking action. Environmental details on continuous control domains.On Mujoco domains, we use default settings for maximum episodic length. We use 5,000 warm-up time steps to populate the experience replay buffer. The exploration noise is as suggested in the original paper by Lillicrap et al. (2016). A.5 A DDITIONAL EXPERIMENTS ON REINFORCEMENT LEARNING PROBLEMS This section shows the following additional empirical results which cannot be put in the main body due to space limitations. • A.5.1 empirically investigates the representation sparsity generated by FTA. • A.5.2 shows the comparison with Tile Coding NNs (Ghiassian et al., 2020), which ﬁrst tile code inputs before feeding them into a neural network. • A.5.3 shows the empirical results of DQN-FTA using linear activation function with regular- ization weights ∈{0.0,0.01,1.0}for the out of bound loss in Eq 8. • A.5.4 shows the sensitivity of FTA to the sparsity control parameter η and tile width parameter δ. • A.5.5 shows the results when applying FTA to both hidden layers rather than just the second hidden layer in RL problems. • A.5.6 shows gradient interference analysis result. • A.5.7 shows the result of using FTA on an autonomous driving application. A.5.1 E MPIRICALLY MEASURING SPARSITY We also report two sparsity measures of the learned representations averaged across all steps in Table 1. We compute an estimate of sparsity by sampling a mini-batch of samples from experience replay buffer and taking the average of them. Instance sparsity corresponds to proportion of nonzero entries in the feature vector for each instance. Overlap sparsity French (1991) is deﬁned as overlap(φ,φ′) =∑ iI(φi ̸= 0)I(φ′ i ̸= 0)/(kd), given two kd-dimensional sparse vectors φ,φ′. Low overlap sparsity potentially indicates less feature interference between different input samples. Theorem 1 guarantees that the instance sparsity, with FTA, should be no more than12.5% for our setting of k= 40,η = δ; and should be no more than 25% when k= 20,η = δ. In the table, we can see that FTA has lower (better) instance sparsity than the upper bound provided by the theorem. Further, FTA achieves the lowest instance and overlap sparsity among all baselines in an average sense. Figure 9 and Figure 10 are corresponding to the learning curves as shown in Figure 6 in Section 5.3. We show learning curves of instance/overlap sparsity as a function of training steps for FTA,l1,l2 regularization and RBF. The instance sparsity is computed by randomly sampling a mini-batch of states from replay buffer and count the average number of nonzero entries in that mini-batch divided by feature dimension kd. The overlap sparsity is computed by randomly sampling two mini-batches of states from the replay buffer and count the average number of simultaneously activated (i.e. nonzero) entries in both mini-batches. From the sparsity learning curves, one can see that our FTA has very stable sparsity since the beginning of the learning, and the sparsity is almost constant cross domains. Particularly, the overlap sparsity is very low, which possibly indicates low representation interference. A.5.2 C OMPARISON WITH TC-NN We include the ﬁgure of comparing with TC-NN from the work by Ghiassian et al. (2020), which use a regular tile coding to process the input before feeding into the neural network. It should be noted 17Published as a conference paper at ICLR 2021 Table 1: Sparsity on LunarLander (average across time steps) Sparsity FTA(k=40) FTA(k=20) L1 L2 RBF Instance 7% 14% 16% 44% 99% Overlap 4% 8% 10% 34% 99% 0.5 1.0 1.5 2.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (a) Acrobot-v1 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (b) MountainCar-v0 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 DQN-L2 DQN-L1 DQN-RBF DQN-FTA (c) CartPole-v1 1 2 3 4 5 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (d) LunarLander-v2 Figure 9: Instance sparsity v.s. number of time steps on MountainCar, CartPole, Acrobot, LunarLan- der. The results are averaged over 20 random seeds and the shade indicates standard error. 0.5 1.0 1.5 2.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (a) Acrobot-v1 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (b) MountainCar-v0 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 DQN-L2 DQN-L1 DQN-RBF DQN-FTA (c) CartPole-v1 1 2 3 4 5 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (d) LunarLander-v2 Figure 10: Overlap sparsity (number of simultaneously activated entries in the sparse feature vectors) v.s. number of time steps by averaging over 20 random seeds and the shade indicates standard error. 18Published as a conference paper at ICLR 2021 0 1 2 3 4 1e5 2000 1500 1000 500 DQN-L2 DQN-L1 DQN-RBF DQN-FTA TC-NN Figure 11: DQN-FTA compares with TCNN. Evaluation learning curves are averaged over 20 random seeds and the shade indicates standard error. All variants are trained without target networks. 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode reg-1.0 reg-0.01 reg-0.0 (a) Acrobot-v1 0 1 2 3 4 1e5 2000 1500 1000 500  (b) MountainCar-v0 0 1 2 3 4 1e5 100 200 300 400 500 (c) CartPole-v1 0 1 2 3 4 5 1e5 200 100 0 100 200 (d) LunarLander-v2 Figure 12: DQN-FTA trained with linear activation function with different regularization weight (-reg-1.0 means regularization is set as 1.0). One can see that our FTA isnot sensitive to this choice as those learning curves are almost overlapping with each other. Evaluation learning curves are averaged over 10 random seeds and the shade indicates standard error. All variants are trained without target networks. that this method itself is not a sparse representation learning technique, and requires the original observation space to be scaled within [0,1]. Figure 11 shows that TC-NN does indeed improve performance for DQN, but not compared to FTA. DQN-FTA both learns faster and reaches a better, more stable solution. In our TC-NN implementation, as a correspondent to FTA, we use binning operation to turn each raw input variable to 20 binary variables through binning operation. We use the same learning rate 0.0001 as other baselines. The result is generated by using FTA with a single tiling: [l,u] = [−20,20], δ= η= 2.0, and hence c = {−20,−18,−16,..., 18}, k= 40/2 = 20. A.5.3 I NSENSITIVITY TO OUT OF BOUNDARY LOSS We demonstrate the effect of using an out of boundary loss as discussed in Section A.3. This is supplementary to our experiments in the main body, where we did not use an out of boundary loss, i.e. the regularization weight is 0. To highlight the issue of out of boundary, we intentionally use a tiling vector c with a small range [−1,1] with 20 tiles, i.e. δ = 2/20 = 0.1 and set the sparsity control parameter η= δ= 0.1. In Figure 12, one can see that our algorithm typically does not need such a regularization even if we use a small tiling. A.5.4 FTA’ S STRONG PERFORMANCE WITH DIFFERENT TILE WIDTHS AND SPARSITY CONTROL PARAMETERS The purpose of this section is to show that given an appropriate bound of the tiling vector c, our approach is insensitive to tile width δand sparsity control parameter η. Since we already showed in Section 5 that DQN-FTA works well on LunarLander with tiling vector bound [−1,1], we ﬁx using this setting in this section. For both ηand δ, we sweep over 0.8/2i,i ∈{0,1,2,..., 8}. Note that this range is extreme (δ= 0.8 gives two bins, and η= 0.8 signiﬁcantly increases overlap); we do so to see a clear pattern. 19Published as a conference paper at ICLR 2021 Figure 13 shows the early learning performance of all possible 9 ×9 = 81 combinations of η,δ. We report the average episodic return (rounded to integers) within the ﬁrst 300k time steps, for each combination. The results are averaged over 5 runs. The pattern is as expected. 1) The algorithms performs best with a reasonably small δ and η. 2) For extremely small η and δ, performance is poor, as expected because the gradient is very small and so learning is slow. 3) Given a ﬁxed tile width δ, the performance degrades as ηbecomes smaller than δ, since again the activation has many regions with zero derivative. 4) If ηgets too large, the sparsity level increases and again performance degrades, though performance remained quite good for a broad range between 0.025 and 0.2. 5) If δ gets large (big bins), performance degrades more so that with larger η, which matches the intuition that ηprovides overlap rather than just increasing bin size and so losing precision. tile width eta 8 11 -65 -25 -24 25 -41 -118 -155 29 116 74 46 46 59 -66 -127 -138 68 105 124 95 28 43 -107 -115 -121 37 114 153 149 126 50 -101 -125 -145 31 96 153 173 175 47 -77 -105 -93 -24 114 149 164 138 10 -30 -81 -86 -5 61 108 121 124 -41 -16 48 -148 -117 31 73 67 15 -42 12 -99 -51 -104 24 15 30 62 -72 -85 -99 -123 Figure 13: Sensitivity of η,δ on LunarLander-v2. A.5.5 E MPIRICAL RESULTS WHEN USING FTA TO BOTH HIDDEN LAYERS As an activation function, FTA can be naturally applied in any hidden layer in a NN. As a convention, we typically use the same activation functions in a fully connected NN. In this section, we present such results on the reinforcement learning domains, as a supplement to the results in Section 5. All setting is the same as those in Section 5, except that we apply FTA to both hidden layers in DQN-FTA and DDPG-FTA. It should be noted that, in this case, DQN-FTA and DDPG-FTA have the same number of training parameters as their -Large correspondents respectively. Figure 14 shows the results on the discrete domains, and Figure 15 shows the results on the continuous control domains. It can be seen that, the FTA versions are better than ReLu versions in the case of not using a target network. This further validates the utility of our FTA in dealing with nonstationary problems. A.5.6 G RADIENT INTERFERENCE ANALYSIS We provide gradient interference analysis in this section. The results in this section are pro- duced by using FTA with a single tiling: [l,u] = [ −20,20], δ = η = 2 .0, and hence c = {−20,−18,−16,..., 18}, k= 40/2 = 20. We consider three measures for gradient interference. Let lθ be the DQN loss parameterized by θ. Given two random samples (i.e. experiences) X,X′from ER buffer, we estimate 1. m1: E[∇θlθ(X)⊤∇θlθ(X′)], this is to measure on average, whether the algorithm general- izes positively or interfere. 2. m2: E[∇θlθ(X)⊤∇θlθ(X′)] ONLY for those pairs of gradient vectors who have negative inner product. This is to check for those likely interfered gradient directions, how much they interfere with each other. 3. m3: Within a minibatch of pairs of (X,X′), the proportion of negative gradient inner products (i.e. if 32 out of 64 pairs has negative inner products, then the proportion is 50%). 20Published as a conference paper at ICLR 2021 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode DQN-FTA DQN DQN-Large (a) Acrobot (v1) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 2000 1500 1000 500  (b) MountainCar (v0) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 100 200 300 400 500 (c) CartPole (v1) 0 1 2 3 4 5 1e5 200 100 0 100 200 (d) LunarLander (v2) Figure 14: Evaluation learning curves of of DQN-FTA(black), DQN(red), and DQN-Large(blue), showing episodic return versus environment time steps. The dotted line indicates algorithms trained with target networks. The results are averaged over 20 runs and the shading indicates standard error. The learning curve is smoothed over a window of size 10 before averaging across runs. (a) InvertedPendu 0.0 0.5 1.0 1.5 2.0 1e6 0 500 1000 1500 2000 2500 (b) Hopper 0.0 0.5 1.0 1.5 2.0 1e6 0 1000 2000 3000 4000 (c) Walker2d 0 1 2 3 4 5 1e5 0 2000 4000 6000 8000 (d) InvertedDouble 0.0 0.5 1.0 1.5 2.0 1e6 0 25 50 75 100 125 150 (e) Swimmer Figure 15: Evaluation learning curves of DDPG-FTA(black), DDPG(red), and DDPG-Large(blue) on Mujoco environments, averaged over 20 runs with shading indicating standard error. All algorithms are trained without target networks. The learning curve is smoothed over a window of size 30 before averaging across runs. This is to roughly check how likely it is to get conﬂict gradient directions by randomly drawing two experiences from ER buffer. It should be noted that we normalize the gradient vectors to unit length before computing inner product. For each point in the learning curve, we randomly draw 64 samples from the ER buffer to estimate the interference. Figure 17 shows the algorithms’ performances in terms of number of time steps taken to reach a near-optimal policy: DQN-FTA >DQN-RBF > DQN-L1 >DQN-L2 ≥DQN (i.e. >means better (use fewer time steps) and ≥means slightly better). There are several interesting observations. First, in Figure 16(b)(e), for the weights in the second hidden layer (m2 measurement), DQN-RBF has lower interference strength than DQN-FTA, but DQN-FTA performs better. This discrepancy indicates that it is not necessarily true that the lower interference, the better. Second, DQN tends to over generalize during early learning, which may hurt and result in bad performance. Third, ﬁgure (e) shows that DQN-L1 is similar to DQN-FTA during late learning stage in terms of m2. But DQN-L1 seems to have extremely interfered gradient directions during early learning, which may explain why it learns slower than DQN-FTA. These observations may indicate that DQN-FTA helps generalize appropriately, but not overly generalize or highly interfered. We believe it is worth a separate work to thoroughly study the gradient interference issue to draw some interesting conclusions. A.5.7 T ESTING STABILITY IN A SIMULATED AUTONOMOUS DRIVING DOMAIN Our results have shown improved stability with FTA. In this section, we test FTA in an environment focused on stability, namely an autonomous driving task (Leurent, 2018). In the real world, a stable policy is of vital importance to ensure safety in autonomous driving. In this simulated task, the goal is not only to obtain high return, but also keep the number of car crashes as low as possible. FTA setting is the same as those in Section 5 on discrete control domains. Figure 18(a) shows the domain, where the agent—the green car—has to learn to switch lanes, avoid car crashes, and go as fast as possible. The observations are 25-dimensional, with vehicle dynamics that follow the Kinematic Bicycle Model. The action space is discrete. FTA learns faster, with signiﬁcantly fewer car crashes incurred during the evaluation time steps as shown in Figure 18(c). Target networks are harmful in this environment, potentially because they slow early learning; the agent to accumulate a signiﬁcant number of crashes before improving. 21Published as a conference paper at ICLR 2021 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.2 0.4 0.6 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (a) m1 0.0 0.5 1.0 1.5 2.0 1e5 0.8 0.6 0.4 0.2 0.0 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (b) m2 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.1 0.2 0.3 0.4 0.5 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (c) m3 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.2 0.4 0.6 0.8 1.0 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (d) m1 in 2nd 0.0 0.5 1.0 1.5 2.0 1e5 1.0 0.8 0.6 0.4 0.2 0.0 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (e) m2 in 2nd 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.1 0.2 0.3 0.4 0.5 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (f) m3 in 2nd Figure 16: Gradient inference measurements on mountain car domains, averaging over 10 runs with the shade indicating standard error. (d)(e)(f) are measured for those parameters in the second hidden layer only (which are supposed to directly affect representation). All algorithms are trained without using target networks except DQN. The curve is smoothed over a window of size20 before averaging across runs. 0.0 0.5 1.0 1.5 2.0 1e5 2000 100 DQN-L2 DQN-L1 DQN-RBF DQN-FTA Figure 17: Evaluation learning curves on mountain car. The results are averaged over 20 runs with the shade indicating standard error. All algorithms are trained without using target networks. A.6 R ESULTS ON IMAGE CLASSIFICATION TASKS We now report the empirical results on two popular image classiﬁcation tasks: MNIST (LeCun & Cortes, 2010) and Mnistfashion (Xiao et al., 2017). We found that FTA does not present any clear advantage or disadvantage in such a conventional supervised learning setting. On MNIST, FTA achieves testing error1.22%, and ReLu achieves 1.38%. On Mnistfashion, FTA achieves testing error 10.67%, and ReLu achieves 10.87%. Details. The NN architecture we use is two convolution layer followed by two fully connected 32 ×32 layers. The ﬁrst convolutional layer has 6 ﬁlters with size 5 ×5 and is followed by a max pooling operation. The second convolutional layer has 16 ﬁlters with the same size followed by a max pooling. FTA is applied to the second hidden layer and the setting is exactly the same as we used in the RL experiment A.4. To optimize learning rate, we optimize over the range 22Published as a conference paper at ICLR 2021 (a) Highway 0.0 0.5 1.0 1.5 Time Steps 1e4 18 20 30 32 Average Return per Episode DQN-FTA DQN (b) Evaluation LC 0 1 2 3 4 5 Time Steps 1e3 0 5 25 30 35 Cumulative Number of Car Crashes DQN-FTA DQN (c) Car Crashes LC Figure 18: (a) The Highway environment. (b) The evaluation learning curve. (c) The cumulative number of car crashes as a function of driving time steps. Results are averaged over 30 runs with shading indicating standard error. The learning curve is smoothed over a window of size 10 before averaging across runs. {0.00003,0.00001,0.0003,0.0001,0.003,0.001}. We use 10-fold cross validation to choose the best learning rate and the above error rate is reported on testing set by using the optimal learning rate at the end of learning. The standard deviation of the testing error (randomness comes from NN initialization and random shufﬂing) is sufﬁciently small to get ignored. A.7 C ONSTRUCTION OF PIECEWISE RANDOM WALK PROBLEM In Section 4, recall that we train with data generating process {(Xt,Yt)}t∈N so that interference difﬁculty is controllable, while permitting fair comparison via a ﬁxed equilibrium distribution. The temporal correlation in high difﬁculty {Xt}t∈N is designed to mimic state space trajectories through MDPs with a high degree of local state space connectedness. For example, an agent can only move to adjacent cells in GridWorld, the paddle and ball in Atari Breakout can only move so far in a single frame, and most successor states in Go only differ by a few pieces. Figure 19 depicts sample trajectories across a range of difﬁculties, alongside the ﬁxed equilibrium distribution. 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2) (a) d= 0 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (b) d= 0.21 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (c) d= 0.41 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2) (d) d= 0.62 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (e) d= 0.83 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (f) d= 0.98 Figure 19: Sample trajectories of {Xt}t∈N (blue) and {St}t∈N (black) across a range of difﬁculties. Note in particular the ﬁxed equilibrium distribution (gray) and i.i.d sampling for d= 0 (top left). Here we rigorously construct {Xt}t∈N and {St}t∈N, and show they have the claimed properties. Let random variable Et ∼N (ϵ; 0,σ2) be a source of Gaussian noise for the recursive random variable St 23Published as a conference paper at ICLR 2021 St+1 = (1 −c)St + Et (9) If c∈(0,1], then {St}t∈N is a linear Gaussian ﬁrst order autoregressive process, denoted AR(1).4 Then the equilibrium distribution of {St}t∈N is also Gaussian. If S0 is sampled from the equilibrium distribution, then the process {St}t∈N is stationary with E[St] = 0 and variance ν2 .= E[S2 t] = σ2 2c−c2 (10) It also follows that {St}t∈N is ergodic. See Grunwald et al. (1995) for reference on AR(1) processes, especially the discussion of equation (3.3) for linear Gaussian AR(1) processes, and section 5.3 for their stationarity and ergodicity. Let st be a realization of St. We deﬁne Xt|St = st as a Gaussian r.v. with mean st and variance β2: Xt|St = st ∼N(x; st,β2) (11) The process {Xt}t∈N is also a linear Gaussian AR(1) process, so we can again rely on established AR(1) process theory to conclude that theequilibrium distribution of {Xt}t∈N is Gaussian. Moreover, if X0 is sampled from the equilibrium distribution, then the process {Xt}t∈N is stationary with E[Xt] = 0 and variance ξ2 ξ2 .= E[X2 t] = β2 + ν2 = β2 + σ2 2c−c2 (12) Refer to Theorem 4 in Section A.7.3 for proof that {Xt}t∈N is a linear Gaussian AR(1) process, and that the resulting equilibrium distribution has variance speciﬁed by equation 12. Note there are several variance terms in the above construction. • σ2 is the noise variance in the random walk, i.e. the source of jumps in {St}t∈N • ν2 is the equilibrium distributon variance of {St}t∈N • β2 is the variance in any particular Xt given st at time step t • ξ2 is the equilibrium distribution variance of {Xt}t∈N The processes {St}t∈N and {Xt}t∈N are fully determined by the quantities c,σ2,β2. For our experiments we wish to have a single parameter dsuch that the equilibrium distribution of {Xt}t∈N is ﬁxed for all d∈[0,1), but temporal correlation is variable. Also, d= 0 should correspond to zero temporal correlation. The following two theorems specify this behaviour more rigorously. Theorem 2. Let difﬁculty parameter d∈[0,1) and ξ2 = ( B 2 )2 for some B ∈R+. Then {Xt}t∈N will have ﬁxed equilibrium distribution N(0,ξ2), invariant to d, if parameters c,σ2,β2 are set as follows c= 1 − √ 1 −d σ2 = d2(B 2 )2 β2 = (1 −d)(B 2 )2 ξ2 is deﬁned in terms of B ∈R+ simply because Bis a more intuitive design parameter. In particular, Bis a high probability bound with P(Xt ∈[−B,B]) ≤0.95 for all t, since {Xt}t∈N is ergodic. Theorem 3. d = 0 and S0 = 0 induces i.i.d Xt from N(x; 0,ξ2), the equilibrium distribution of {Xt}t∈N. 4If c= 0, then St is a simple Gaussian random walk. We do not use such a process, because sample paths are unbounded, hitting ± √ tinﬁnitely often as t→∞ 24Published as a conference paper at ICLR 2021 We defer proof of both theorems to Section A.7.3. Correlation difﬁculty dis intuitively characterized as follows. • As dincreases, β2 decreases, so a smaller portion of the overall state space is supported at any given time. i.e. P(Xt = x|St = st) becomes increasingly narrow, inducing higher temporal correlation. • As dincreases, the noise in random walk {St}t∈N increases in amplitude, so that larger jumps in the mean of any particular Xt are more likely. • At d= 0, all correlation difﬁculty from {Xt}t∈N has been removed, in that we recover iid sampling. • As d→1, {Xt}t∈N converges towards pathological correlation, withXt d − →δ(st), where δ(·) is the Dirac delta distribution. i.e. Xt becomes constant everywhere except the jumps in realization st of St. Despite the above relationships between dand the other process parameters, the equilibrium distribu- tion N(x; 0,ξ2) is identical for all difﬁculty settings d∈[0,1), because the increase (or decrease) in parameters σ2,c is tuned speciﬁcally to counteract the decrease (or increase) in β2. Having identical equilibrium distributions means that a hypothetical ideal algorithm, capable of perfectly mitigating interference, would train the identical approximatorfθ for any correlation difﬁculty d∈[0,1). Hence, we use approximation loss on the equilibrium distribution as a measure of robustness to interference. To summarize, correlation difﬁculty d ∈[0,1) controls the likelihood of interference-inducing samples throughout training, but also permits fair comparison between values of dby the (ﬁxed) equilibrium distribution of {Xt}t∈N. A.7.1 P IECEWISE RANDOM WALK ADDITIONAL PLOTS Figure 20 depicts learning curves of loss over training time at different levels of correlation difﬁculty. Figure 21 shows learning rate sensitivity curves for the FTA and ReLU networks for three different difﬁculty settings. The ADAM optimizer Kingma & Ba (2015) was used in all experiments. For the experiment in Figure 3, where one sample from each Xt is used for each weight update, FTA outperforms ReLU even on i.i.d data (d= 0). In order to ﬁnd the conditions under which ReLU and FTA both perform equally well, we repeat the same experiment, but with 50 samples drawn from each Xt. Figure 23 depicts the results, where the performance is indeed equalized for iid sampling (d= 0), but ReLU’s performance still diverges similarly to other experiments as correlation difﬁculty is increased. The size of the FTA and ReLU neural networks used on the Piecewise Random Walk Problem were chosen based on best performance on iid data, and the best performers do not have the same number of parameters. (67K vs 5.2K learnable parameters.) In order to verify that the difference in parameter count was not a signiﬁcant factor in handling high correlation difﬁculty, we additionally include results for a ReLU network with wider hidden layers totalling 81K learnable parameters. See Figure 22. Both ReLU networks perform very similarly—in most cases agreeing within p= 0.05—with loss steeply running away as correlation difﬁculty increases. So we conclude FTA’s robustness to high correlation difﬁculties is not explained simply by parameter count. A.7.2 P IECEWISE RANDOM WALK PROBLEM HYPERPARAMETER SELECTION Experimental Parameters Conﬁguration for Piecewise Random Walk problem across all experiments: 25Published as a conference paper at ICLR 2021 0 5000 10000 15000 20000 Training Iterations 0.0 0.1 0.3 0.4 Loss Over Equilib. Dist. d=0.88 d=0 (a) FTA 0 5000 10000 15000 20000 Training Iterations d=0.88 d=0 (b) ReLU 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU (c) ReLU and FTA, d∈[0,1) Figure 20: Left, Middle: Learning curve of loss over stationary distribution during training on low difﬁculty (dotted) and high difﬁculty (solid) settings for two layer neural nets. The curves are smoothed over a window of 50. Right: The ﬁnal loss over stationary distribution after 20K training iterations across a range of difﬁculty settings, shown as the mean of 30 runs with the shaded region corresponding to p = 0.001. The ﬁnal loss per run is computed as the mean over the ﬁnal 2.5K iterations, with the iid setting d= 0 (dotted) shown for baseline comparison. 10 5  10 4  10 3  10 2 Learning Rate 10 3 10 2 10 1 100 Mean Final Loss d= 0.00 FTA ReLU (a) Lowest Difﬁculty 10 5  10 4  10 3  10 2 Learning Rate 10 3 10 2 10 1 100 Mean Final Loss d= 0.41 FTA ReLU (b) Moderate Difﬁculty 10 5  10 4  10 3  10 2 Learning Rate 10 3 10 2 10 1 100 Mean Final Loss d= 0.98 FTA ReLU (c) High Difﬁculty Figure 21: Learning rate sensitivity of FTA and ReLU for iid, mildly correlated, and severely correlated Xt (left, middle, right, respectively.) Final loss performance is shown as the mean of 30 runs with the shaded region corresponding p= 0.001. These curves corroborate our ﬁndings that, in general, FTA prefers lower learning rates (but converges more quickly nonetheless.) 0 5000 10000 15000 20000 Training Iterations 0.0 0.1 0.3 0.4 Loss Over Equilib. Dist. d=0.88 d=0 (a) FTA 0 5000 10000 15000 20000 Training Iterations d=0.88 d=0 (b) ReLU-large 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU ReLU (large) (c) ReLU-large and FTA, d∈[0,1) Figure 22: Left, Middle: Learning curve for single run using loss over stationary distribution during training on low difﬁculty (dotted) and high difﬁculty (solid) settings for two layer neural nets, both having similar numbers of learnable parameters. The curves are smoothed over a window of 50. Right: The ﬁnal loss over stationary distribution after 20K training iterations across a range of difﬁculty settings, shown as the mean of 10 runs with the shaded region corresponding p= 0.05. The ﬁnal loss per run is computed as the mean over the ﬁnal 2.5K iterations, with the iid setting d= 0 (dotted) shown for baseline comparison. 26Published as a conference paper at ICLR 2021 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU Figure 23: The ﬁnal loss over stationary distribution after 20K training iterations across a range of difﬁculty settings, shown as the mean of 10 runs with the shaded region corresponding p= 0.001. For each weight update, a batch of 50 samples are drawn from each Xt, rather than a single sample as was done in other similar ﬁgures. Iterations per training run 20K Test batch size per iteration 100 Train batch size per iteration 1 Min, max difﬁculty settings d (0.0, 0.98) Number of difﬁculty settings swept 20 (linear range over (min, max), inclusive) Training runs per difﬁculty setting 10 for ReLU Large (Figure 22) 10 for batched run (Figure 23) 30 otherwise Number of piecewise stationary segments 50 Target function Yt = sin(2πX2 t) High probability bound B (−1,1) Conﬁguration for the neural networks used on the Piecewise Random Walk problem: FTA Hidden layers 2 Layer width w 40 Layer binning interval bounds (l,u) ( −1,1) Layer bins k 40 Layer sparsity parameter η 1 40 ReLU Hidden layers 2 Layer width w 50 ReLU Large Hidden layers 2 Layer width w 200 Layer widths w, bin count k, and sparsity parameterηwere chosen by sweeping a range, and choosing the best performing conﬁguration on the iid setting (i.e. difﬁculty d= 0). In the sweep, the same w,k,η are used for each hidden layer of FTA, and the same wis used for each hidden layer of ReLU. See the Parameter Selection Details below. For all three networks, the ADAM optimizer Kingma & Ba (2015) was used with parameters (β1,β2) = (0.9,0.999), and the following learning rates were swept for each difﬁculty setting. Learning rates λ 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001, 0.000005 The results from the best performing learning rate are reported in Figures 3(c), 20(c), 22(c), and 23. That is, the performing learning rate is determined separately for each difﬁculty setting, using the mean ﬁnal loss across 30 runs for the FTA and ReLU networks. 10 runs were used for ReLU Large and the batching run. Results from all learning rates are reported in Figure 21 for three different difﬁculty settings. Parameter Selection Details 27Published as a conference paper at ICLR 2021 Before sweeping any difﬁculty settings, ﬁrst the network architectures were established for the FTA and ReLU networks so that loss was minimized on the iid setting ( d = 0 ). We avoid the overparametrized regime, so that standard bias-variance tradeoff applies here Belkin et al. (2019). (However, the ReLU Large network is overparametrized.) Learning rate was also swept during the architecture sweep, to ensure that the chosen architecture performed best within the learning rate range chosen for the main experiment depicted in Figure 3(c). For the 2 layer FTA network, each hidden layer uses the same width w, and sparsity parameter η. Those parameters, along with learning rate λwere optimized in the following ranges with grid search: • Layer width w∈{10,15,20,30,40} • Sparsity parameter η= 1 w • Learning rate λ∈{0.0005,0.0001,0.00005,0.00001,0.000005} For the 2 layer ReLU network, fully connected hidden layers were used. Width wand learning rate λ were found from the following ranges, also with grid search: • Layer width w∈{5,10,20,30,40,50,60,70,80,100,120} • Learning rate λ∈{0.005,0.001,0.0005,0.0001,0.00005} The learning rates are of different magnitude between FTA and ReLU because they perform best in different ranges, as explained in A.7.1. For the main experiments, where a range of difﬁculties are swept, the range of tested learning rates is shared between FTA and ReLU, and is broad enough to cover the optimal range for both. Also note that we set η = 1 k, rather than separately sweeping it. This is because FTA performs consistently well with ηin this order of magnitude. A.7.3 P ROOFS FOR COVARIATE SHIFT PROPERTIES Theorem 4. Let {Et}t∈N,{St}t∈N,{Xt}t∈N be stochastic processes such that Et ∼N(ϵ; 0,σ2) St+1 = (1 −c)St + Et (13) Xt|St = st ∼N(x; st,β2) (14) where c∈(0,1] and st is a realization of St. Then {Xt}t∈N is a linear Gaussian AR(1) process with equilibrium distribution N(x; 0,β2 + σ2 2c−c2 ). Proof. Begin by rewriting equation 14 to give Xt as a sum of St and a mean zero Gaussian r.v. Bt Xt = St + Bt Bt ∼N(0,β2), iid (15) Now substitute according to equation 13 Xt = (1 −c)St−1 + Et−1 + Bt (16) Let Gaussian r.v. Θ ∼N(0,α2) with some variance α2. Et−1,Bt are independent Gaussian, so Et−1 +Bt can be written as a linear combination ofBt−1,Θt, since all are independent. Speciﬁcally, ﬁx α2 so that the following holds Et−1 + Bt = (1 −c)Bt−1 + Θt (17) and substitute into equation 16 28Published as a conference paper at ICLR 2021 Xt = (1 −c)St−1 + (1 −c)Bt−1 + Θt = (1 −c)(St−1 + Bt−1) + Θt = (1 −c)Xt−1 + Θt By inspection, {Xt}t∈N is a linear Gaussian AR(1) process with coefﬁcient(1−c) and noise variance α2. Elementary AR process theory gives the equilibrium distribution as N ( 0, α2 1 −(1 −c)2 ) (18) For equation 17 to hold, we need the ﬁrst and second moments of LHS and RHS to be equal. All terms are mean zero Gaussian, so it sufﬁces to show when the LHS and RHS have equal variance: Var(Et−1 + Bt) = Var((1−c)Bt−1 + Θt) σ2 + β2 = (1 −c)2β2 + α2 α2 = σ2 + β2 −(1 −c)2β2 α2 = σ2 + (1 −(1 −c)2)β2 Substituting into equation 18 N ( 0, α2 1 −(1 −c)2 ) = N ( 0,σ2 + (1 −(1 −c)2)β2 1 −(1 −c)2 ) = N ( 0, σ2 1 −(1 −c)2 + β2 ) = N ( 0, σ2 2c−c2 + β2 ) So {Xt}t∈N is linear Gaussian AR(1) with the desired equilibrium distribution. Theorem 2 Let difﬁculty parameter d∈[0,1) and ξ2 = (B 2 )2 for some B ∈R+. Then {Xt}t∈N will have ﬁxed equilibrium distribution N(0,ξ2), invariant to d, if parameters c,σ2,β2 are set as follows c= 1 − √ 1 −d σ2 = d2(B 2 )2 β2 = (1 −d)(B 2 )2 Proof. By the above theorem 4, ξ2 = β2 + σ2 2c−c2 = (1 −d)(B 2 )2 + d2(B 2 )2/d = ( B 2 )2. This completes the proof. Theorem 3 d= 0 and S0 = δ(0) induces iid Xt from N(x; 0,ξ2), the equilibrium distribution of {Xt}t∈N. 29Published as a conference paper at ICLR 2021 Proof. St+1 = (1 −c)St + Et (by9) = St + Et (d= 0 =⇒ c= 0) = St (d= 0 =⇒ σ2 = 0 =⇒ Et ∼δ(0)) Hence, d= 0 implies St is constant over all time t, and S0 = δ(0) gives St = δ(0) for all time t. Let r.v. Bt ∼N(0,β2) iid, then Xt = St + Bt = Bt when St is Dirac delta concentrating at zero. β2 = (1 −d)(B 2 )2 = (B 2 )2 by the setting from Theorem 2 and setting d= 0. Hence, d= 0 induces iid sampling from the equilibrium distribution of Xt. This completes the proof. 30",
      "meta_data": {
        "arxiv_id": "1911.08068v3",
        "authors": [
          "Yangchen Pan",
          "Kirby Banman",
          "Martha White"
        ],
        "published_date": "2019-11-19T03:12:06Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08068v3.pdf",
        "github_url": "https://github.com/yannickycpan/reproduceRL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Fuzzy Tiling Activation (FTA), a novel activation function that generates sparse representations deterministically by design, addressing the limitations of complex regularization or meta-learning approaches for achieving sparsity online. FTA overcomes the zero gradient issue and lost precision of simple binning by providing non-negligible gradients and controlled overlap between bins. The key findings are that FTA is robust under covariate shift in online supervised learning, and in deep reinforcement learning settings (value-based and policy gradient), algorithms equipped with FTAs learn stable policies faster and, on most domains, do not require target networks.",
        "methodology": "The core methodology is the design of the Fuzzy Tiling Activation (FTA) function. FTA builds upon a 'Tiling Activation' which bins a scalar input `z` into a one-hot vector, providing guaranteed sparsity but with zero gradients almost everywhere. To address this, FTA introduces a 'fuzzy indicator function' `Iη,+(x)` parameterized by `η` (a small constant). This fuzzy indicator function allows for non-zero derivatives for `x < η`, enabling gradient propagation via backpropagation. FTA thus produces sparse representations by construction, with controllable sparsity (tuned by `η`) and without adding new training parameters. The sparsity is theoretically guaranteed by an upper bound `2⌊η/δ⌋ + 3` on the number of non-zero entries. It is integrated into neural networks as a modular component, typically applied to hidden layers. For online supervised learning, it was applied to a synthetic piecewise random walk problem. For reinforcement learning, it was integrated into Deep Q-Networks (DQN) for discrete control and Deep Deterministic Policy Gradient (DDPG) for continuous control.",
        "experimental_setup": "For **Online Supervised Learning**, experiments used a synthetic 'Piecewise Random Walk Problem' where `Xt ∼ N(St, β^2)` with drifting mean `St` following an AR(1) process, and target `yt = sin(2πx^2t)`. This setup allowed controlling temporal correlation (covariate shift difficulty `d`). Performance was measured by mean squared error over the equilibrium distribution. Comparison was primarily against ReLU networks. For **Deep Reinforcement Learning**, experiments were conducted on OpenAI Gym's discrete control environments (MountainCar, CartPole, Acrobot, LunarLander) using DQN, and Mujoco continuous control environments (InvertedPendu, Hopper, Walker2d, InvertedDouble, Swimmer) using DDPG. A two-layer neural network architecture was used, with FTA typically on the last hidden layer. All algorithms used a mini-batch size of 64 from a 100k-capacity experience replay buffer, Adam optimizer, Xavier initializer, and discount `γ = 0.99`. FTA parameters were fixed as `[l,u] = [-20,20]`, `δ=η=2.0`, resulting in `k=20` bins. Comparisons included standard DQN/DDPG (with ReLU/tanh, with/without target networks), DQN-Large (a larger network with comparable feature dimension to FTA's output), DQN with Radial Basis Functions (RBFs), and DQN with ℓ1/ℓ2 regularization on activations. An autonomous driving task was also used to test stability. Sparsity (instance and overlap) and gradient interference were also empirically analyzed.",
        "limitations": "The performance of FTA can be sensitive to the chosen tiling bound `[l,u]`. If the bound is too small, inputs may fall outside, leading to zero gradients. If the bound is too large, many inputs might activate the same bins, increasing interference and potentially leading to dead neurons. While FTA significantly improves performance and stability, it is not always sufficient on its own to achieve superior performance on all challenging tasks (e.g., Swimmer in Mujoco), suggesting that other factors like exploration strategy, other hyperparameters, or overall neural network architecture may still play an important role. The theoretical sparsity guarantee provides an upper bound, which is looser in practice, meaning actual sparsity is often better than the worst-case bound.",
        "future_research_directions": "Future work could investigate adaptive approaches for selecting the tiling bound `[l,u]` to improve ease-of-use and robustness. Another promising direction is to combine FTA with other existing methods explicitly designed to mitigate interference (e.g., those by Liu et al., Riemer et al., Javed & White), as FTA is complementary. Furthermore, research should focus on how to appropriately balance generalization and discrimination in sparse representations, as observations suggest performance degrades when overlap sparsity is either too high or too low. A deeper study into the effect of removing target networks in Dyna-style planning, particularly with FTA, is also suggested.",
        "experimental_code": "import tensorflow as tf\nimport numpy as np\n\n\n'''' ------------------ usage example: construct a two-layer QNN with FTA on the second layer -------------------- '''\n''' SparseActFunc is an instantce of the class FTA , the FTA configuration is my suggestion '''\n\ndef create_fta_qnn(*args):\n\n    scopename, dtype, n_input, n_output, n_hidden1, n_hidden2, unittype, SparseActFunc = args\n    with tf.variable_scope(scopename):\n        state_input = tf.placeholder(dtype, [None, n_input])\n        hidden1 = tf.layers.dense(state_input, n_hidden1, activation=unittype)\n        # hidden1 = tf.layers.dense(state_input, n_hidden1, activation=SparseActFunc.func)\n        sparse_phi = tf.layers.dense(hidden1, n_hidden2, activation=SparseActFunc.func)\n\n        w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n        q_values = tf.layers.dense(sparse_phi, n_output, activation=None, kernel_initializer=w_init)\n\n        max_qvalue = tf.reduce_max(q_values, axis=1)\n        max_ind = tf.argmax(q_values, axis=1)\n\n        # get variables\n        tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scopename)\n    return state_input, q_values, max_qvalue, max_ind, sparse_phi, tvars\n\ndef create_qnn_inputsparse(*args):\n    scopename, dtype, n_input, n_output, n_hidden1, n_hidden2, unittype, SparseActFunc = args\n    with tf.variable_scope(scopename):\n        state_input = tf.placeholder(dtype, [None, n_input])\n        sparse_state_input = SparseActFunc.func(state_input)\n        print(' sparse input feature is used ============================== ')\n        hidden1 = tf.layers.dense(sparse_state_input, n_hidden1, activation=unittype)\n        hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=unittype)\n\n        w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n        q_values = tf.layers.dense(hidden2, n_output, activation=None, kernel_initializer=w_init)\n\n        max_qvalue = tf.reduce_max(q_values, axis=1)\n        max_ind = tf.argmax(q_values, axis=1)\n\n        # get variables\n        tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scopename)\n    return state_input, q_values, max_qvalue, max_ind, sparse_state_input, tvars\n\n\ndef create_actor_fta_nn(*args):\n    scopename, n_input, n_output, bound, n_hidden1, n_hidden2, usetanh, SparseActFunc = args\n    hidden_type = tf.nn.relu\n    if usetanh == 1:\n        hidden_type = tf.nn.tanh\n    with tf.variable_scope(scopename):\n        actor_input = tf.placeholder(tf.float32, [None, n_input])\n\n        hidden1 = tf.layers.dense(actor_input, n_hidden1, activation=hidden_type)\n        # SparseActFunc.set_extra_act_strength(hidden1, n_hidden2)\n        sparse_phi = tf.layers.dense(hidden1, n_hidden2, activation=SparseActFunc.func)\n\n        w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n        action = bound * tf.layers.dense(sparse_phi, n_output, activation=tf.nn.tanh, kernel_initializer=w_init)\n        tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scopename)\n    return actor_input, action, sparse_phi, tvars\n\n\ndef create_critic_fta_nn_vanilla(*args):\n    scopename, n_input, n_output, actor_output, n_hidden1, n_hidden2, usetanh, SparseActFunc = args\n    hidden_type = tf.nn.relu\n    if usetanh == 1:\n        hidden_type = tf.nn.tanh\n    with tf.variable_scope(scopename):\n        state_input = tf.placeholder(tf.float32, [None, n_input])\n        action_input = actor_output\n\n        hidden1 = tf.layers.dense(state_input, n_hidden1, activation=hidden_type)\n        # SparseActFunc.set_extra_act_strength(hidden1, n_hidden2)\n        sparse_phi = tf.layers.dense(tf.concat([hidden1, action_input], axis=1),\n                                     n_hidden2, activation=SparseActFunc.func)\n\n        w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n        value = tf.layers.dense(sparse_phi, n_output, activation=None, kernel_initializer=w_init)\n        tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scopename)\n    return state_input, action_input, value, sparse_phi, tvars\n\n\nclass FTAConfiguration(object):\n    default_attributes = {'n_tiles': 20, 'n_tilings': 1, 'sparse_dim': None,\n                          'fta_input_max': 20.0, 'fta_input_min': -20.0, 'fta_eta': 2.0,\n                          'outofbound_reg': 0.0, 'extra_strength': False,\n                          'individual_tiling': False,\n                          'actfunctypeFTA': 'linear',  'actfunctypeFTAstrength': 'linear'}\n\n    def __init__(self, configdict):\n        for key in configdict:\n            if key in self.default_attributes:\n                setattr(self, key, configdict[key])\n                \n        if not hasattr(self, 'fta_input_min'):\n            self.fta_input_min = -self.fta_input_max\n        if not hasattr(self, 'fta_eta'):\n            self.fta_eta = (self.fta_input_max - self.fta_input_min)/self.n_tiles\n            \n        for key in self.default_attributes:\n            if not hasattr(self, key):\n                setattr(self, key, self.default_attributes[key])\n        if self.n_tilings > 1:\n            ''' if multi-tiling, use default setting, fta_input_max should be a list '''\n            return\n\n\nclass FTA(object):\n\n    act_func_dict = {'tanh': tf.nn.tanh, 'linear': lambda x: x,\n                     'relu': tf.nn.relu, 'sigmoid': tf.nn.sigmoid, 'clip': None, 'sin': tf.math.sin}\n\n    def __init__(self, params):\n        config = FTAConfiguration(params)\n        self.config = config\n        ''' rewrite the clip activation '''\n        self.act_func_dict['clip'] = lambda x: tf.clip_by_value(x, config.fta_input_min, config.fta_input_max)\n\n        self.extra_strength = config.extra_strength\n        ''' set up sparsity control eta variable '''\n        self.fta_eta = config.fta_eta\n        ''' set up activation function before FTA '''\n        self.actfunctypeFTA = config.actfunctypeFTA\n        ''' set up activation function used for self-strength '''\n        self.actfunctypeFTAstrength = config.actfunctypeFTAstrength\n        ''' set up FTA bound '''\n        self.outofbound_reg = config.outofbound_reg\n        self.fta_loss = 0.0\n        self.extra_act_strength = 1.\n\n        self.n_tiles = config.n_tiles\n        self.n_tilings = config.n_tilings\n        self.individual_tiling = config.individual_tiling\n\n        ''' NOTE: when using individual tiling, number of tilings must be equal to FTA's input dimension '''\n\n        ''' set tilings, tiles '''\n        if self.config.n_tilings > 1:\n            self.c_mat, self.tile_delta_vector \\\n                = self.get_multi_tilings(config.n_tilings, config.n_tiles)\n        else:\n            self.c_vec, self.tile_delta, self.tiling_low_bound, self.tiling_up_bound \\\n                = self.get_tilings(config.n_tilings, config.n_tiles, config.fta_input_min, config.fta_input_max)\n\n        if 'RBF' in params['name']:\n            self.func = self.RBF_func\n        elif self.config.n_tilings > 1 and not self.individual_tiling:\n            self.func = self.FTA_func_multi_tiling\n        elif self.config.n_tilings > 1 and self.individual_tiling:\n            self.func = self.FTA_func_individual_tiling\n        else:\n            self.func = self.FTA_func\n        print(' fta_eta, n_tilings, and n_tiles :: ===================================================== ',\n              self.fta_eta, self.n_tilings, self.n_tiles)\n\n    def Iplus_eta(self, x, eta):\n        if eta == 0:\n            return tf.math.sign(x)\n        return tf.cast(x <= eta, tf.float32) * x + tf.cast(x > eta, tf.float32)\n\n    def _sum_relu(self, c, x, delta):\n        return tf.nn.relu(c - x) + tf.nn.relu(x - delta - c)\n\n    ''' low bound must be nonpositive and up bound must be nonneg '''\n\n    def compute_out_of_bound_loss(self, input):\n        if self.outofbound_reg > 0 and self.actfunctypeFTA not in ['tanh', 'sigmoid', 'clip']:\n            self.fta_loss = tf.reduce_mean(tf.reduce_sum(tf.cast((input > self.tiling_up_bound), tf.float32)\n                                                         * input, axis=1)) \\\n                            - tf.reduce_mean(tf.reduce_sum(tf.cast((input < self.tiling_low_bound), tf.float32)\n                                                           * input, axis=1))\n            self.fta_loss = self.outofbound_reg * self.fta_loss\n\n    def set_extra_act_strength(self, input, n_h):\n        if self.extra_strength:\n            self.extra_act_strength = tf.contrib.layers.fully_connected(input, n_h,\n                                    activation_fn=self.act_func_dict[self.actfunctypeFTAstrength])\n\n    def _get_strength(self, x, d, c):\n        if x is None:\n            return 1.0\n        if self.extra_strength:\n            strength = tf.reshape(self.extra_act_strength, [-1, d, 1])\n        else:\n            strength = 1.0\n        return strength\n\n    ''' for each tiling, operates on all of the input units; if rawinput is None, strenght is one '''\n    def get_sparse_vector(self, input, rawinput, n_tiles, tile_delta, fta_eta, c):\n        d = int(input.shape.as_list()[1])\n        k = int(n_tiles)\n        x = tf.reshape(input, [-1, d, 1])\n        onehot = tf.reshape((1.0 - self.Iplus_eta(self._sum_relu(c, x, tile_delta),\n                                         fta_eta)) * self._get_strength(rawinput, d, c), [-1, d * k])\n        return onehot\n\n    ''' rawinput = previouslayeroutput W + b '''\n    def FTA_func(self, rawinput):\n        \"\"\" this activation function decides if we should preprocess before feeding into FTA function \"\"\"\n        input = self.act_func_dict[self.actfunctypeFTA](rawinput)\n        self.compute_out_of_bound_loss(input)\n        onehot = self.get_sparse_vector(input, rawinput, self.n_tiles, self.tile_delta, self.fta_eta, self.c_vec)\n        print(' after FTA processing the onehot dimension is :: ', onehot.shape)\n        return onehot\n\n    ''' no need for out of boundary loss for RBF '''\n\n    def RBF_func(self, input):\n        input = self.act_func_dict[self.actfunctypeFTA](input)\n        d = int(input.shape.as_list()[1])\n        k = self.n_tiles\n        x = tf.reshape(input, [-1, d, 1])\n        onehot = tf.reshape(tf.exp(-tf.square(self.c_vec - x) / self.fta_eta), [-1, d * k])\n        print(' after RBF processing the sparse dimension is :: ', onehot.shape)\n        return onehot\n\n    def get_tilings(self, n_tilings, n_tile, input_min, input_max):\n        tile_delta = (input_max - input_min) / n_tile\n        if n_tilings == 1:\n            one_c = np.linspace(input_min, input_max, n_tile, endpoint=False).astype(np.float32)\n            c_vec = tf.constant(one_c)\n            tile_delta = one_c[1] - one_c[0]\n            return c_vec, tile_delta, input_min, input_max\n        maxoffset = n_tilings * (input_max - input_min) / n_tile\n        tiling_length = input_max - input_min + maxoffset\n        startc = input_min - np.random.uniform(0, maxoffset, n_tilings)\n        c_list = []\n        for n in range(n_tilings):\n            one_c = np.linspace(startc[n], startc[n] + tiling_length, n_tile, endpoint=False).astype(np.float32)\n            c_list.append(tf.constant(one_c.copy().astype(np.float32)))\n        tiling_low_bound = np.min(startc) - maxoffset\n        tiling_up_bound = np.max(startc) + tiling_length\n        return c_list, tile_delta, tiling_low_bound, tiling_up_bound\n\n\n    ''' \n    get multiple tiling vectors, now fta_input_max is a list of upper bounds;\n    need to study what tilings should be used\n    '''\n    def get_multi_tilings(self, n_tilings, n_tile):\n        input_max_list = np.random.choice(self.config.fta_input_max, n_tilings)\n        c_list = []\n        tile_defta_list = []\n        for n in range(n_tilings):\n            ind = n % len(input_max_list)\n            one_c = np.linspace(-input_max_list[ind], input_max_list[ind], n_tile, endpoint=False).astype(np.float32)\n            c_list.append(tf.constant(one_c.copy().astype(np.float32).reshape((-1, n_tile))))\n            tile_defta_list.append((one_c[1]-one_c[0]))\n        c_mat = tf.concat(c_list, axis=0)\n        tile_defta_vector = tf.reshape(tf.constant(np.array(tile_defta_list).astype(np.float32)), [n_tilings, 1])\n        return c_mat, tile_defta_vector\n\n\n    ''' \n    rawinput has shape (minibatchsize, # of hidden units)\n    for example, rawinput = h W, h is the previous layer's output,\n    W is the weight matrix in the current hidden layer whose activation is FTA.\n    If h is a-by-b, W is b-by-c, then rawinput is a-by-c. If FTA has n_tilings and n_tiles, \n    then the output has shape (a, c * n_tilings * n_tiles). \n    '''\n    def FTA_func_multi_tiling(self, rawinput):\n        input = self.act_func_dict[self.actfunctypeFTA](rawinput)\n        d = int(input.shape.as_list()[1])\n        x = tf.reshape(input, [-1, d, 1, 1])\n        ''' each row in the c_mat is a tiling, for each sample's each hidden unit, apply all those tilings  '''\n        onehots = 1.0 - self.Iplus_eta(self._sum_relu(self.c_mat, x, self.tile_delta_vector),\n                                       self.tile_delta_vector)\n        onehot = tf.reshape(onehots, [-1, int(d * self.n_tiles * self.n_tilings)])\n        print(' after FTA processing the onehot dimension is :: ', onehot.shape)\n        return onehot\n\n\n    '''\n    individualized tiling: each element in a vector uses its own tiling. Hence this is \n    different with FTA_func_multi_tiling, where each element goes through all tilings vectors\n    '''\n    def FTA_func_individual_tiling(self, rawinput):\n        input = self.act_func_dict[self.actfunctypeFTA](rawinput)\n        d = int(input.shape.as_list()[1])\n        x = tf.reshape(input, [-1, d, 1])\n        ''' each row in the c_mat is a tiling, for each sample's each hidden unit, apply one tiling  '''\n        onehots = 1.0 - self.Iplus_eta(self._sum_relu(self.c_mat, x, self.tile_delta_vector),\n                                       self.tile_delta_vector)\n        onehot = tf.reshape(onehots, [-1, int(d * self.n_tiles)])\n        print(' after FTA processing the onehot dimension is :: ', onehot.shape)\n        return onehot\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport sys\nfile_dir = os.path.dirname(__file__)\nsys.path.append(file_dir)\nfrom FunctionApproximator import FunctionApproximator\nfrom network.dqn_network import create_fta_qnn\nfrom network.operations import compute_q_acted, update_target_nn_assign, update_critic_ops, listshape2vec\nfrom Agent import Agent\nfrom network.ftann import FTA\nfrom sparseutils import tfget_sparsities,  get_grad_interferences\n\n\nclass DQN(FunctionApproximator):\n    def __init__(self, params):\n        super(DQN, self).__init__(params)\n\n        self.create_qnn = create_fta_qnn\n\n        self.unittype = tf.nn.tanh if self.usetanh == 1 else tf.nn.relu\n        self.statescale = 255. if self.use_atari_nn else 1.0\n        print('check if use atari nn or not -------------------- ', self.use_atari_nn)\n\n        if 'NoTarget' in self.agent_name:\n            self.update_target_frequency = 1\n            self.useTargetHC = 0\n            self.compute_Qtarget = self.compute_Qtarget_wotar\n        else:\n            self.compute_Qtarget = self.compute_Qtarget_wtar\n\n        print('statedim, actiondim, tiles, use atari, target nn move rate are ---------------------------------- ',\n              self.stateDim, self.actionDim, self.create_qnn, self.update_target_frequency,\n              self.use_atari_nn, self.statescale)\n        # params['sess'] = self.sess\n        with self.g.as_default():\n            self.fta = FTA(params)\n            '''used for batch normalization'''\n            self.action_input = tf.placeholder('int64', [None])\n            self.state_input, self.q_values, self.max_q_value, self.best_act, self.phi, self.tvars = \\\n                self.create_qnn(self.agent_name, self.dtype, self.stateDim, self.actionDim, self.n_h1, self.n_h2,\n                                self.unittype, self.fta)\n\n            self.tar_state_input, self.tar_q_values, self.tar_max_q_value, self.tar_best_act, self.tar_phi, self.tar_tvars \\\n                = self.create_qnn(\"target_\"+self.agent_name, self.dtype, self.stateDim, self.actionDim, self.n_h1, self.n_h2,\n                                  self.unittype, self.fta)\n            print('q value shape is ----------------- ', self.q_values.shape)\n            # define state action value\n            self.sa_value, self.tar_sa_value \\\n                = compute_q_acted(\"sa_values\", self.action_input, self.actionDim, self.q_values, self.tar_q_values)\n            # define loss operation, weight has been incorporated in fta_loss\n            self.qtarget_input, self.params_update = update_critic_ops(self.sa_value, self.learning_rate,\n                                                                           self.fta.fta_loss, 1.0)\n            # define optimization\n            self.target_params_update = update_target_nn_assign(self.tar_tvars, self.tvars)\n            self.gradvars = tf.gradients(self.max_q_value, self.tvars)[0]\n            self.gradvars_list = \\\n                tf.gradients(tf.reduce_mean(tf.square(self.qtarget_input - tf.squeeze(self.sa_value))), self.tvars)\n            self.flattened_gradvec = listshape2vec(self.gradvars_list)\n            self.flattened_hidden2vec = tf.reshape(self.gradvars_list[2], [-1])\n            print('flattened 2nd hidden layer training parameter shape is :: ', self.flattened_hidden2vec.shape)\n            # initialize network\n            self.sess.run(tf.global_variables_initializer())\n            self.sess.run(self.target_params_update)\n            self.saver = tf.train.Saver()\n\n            self.sparse_dim = int(self.phi.shape[1])\n\n    def compute_Qtarget_wtar(self, sp, r, gamma):\n        max_q = self.sess.run(self.tar_max_q_value, feed_dict={self.tar_state_input: sp})\n        qtarget = r + gamma * max_q\n        return qtarget\n\n    def compute_Qtarget_wotar(self, sp, r, gamma):\n        max_q = self.sess.run(self.max_q_value, feed_dict={self.state_input: sp})\n        qtarget = r + gamma * max_q\n        return qtarget\n\n    def take_action(self, state):\n        if len(state.shape) < len(self.state_input.shape):\n            state = state[None, :]\n        act = self.sess.run(self.best_act, feed_dict={self.state_input: state/self.statescale})\n        return act[0]\n\n    def train(self, s, a, sp, r, gamma):\n        qtarget = self.compute_Qtarget(sp/self.statescale, r/self.reward_scale, gamma)\n        self.sess.run(self.params_update,\n                      feed_dict={self.state_input: s/self.statescale,\n                                 self.qtarget_input: qtarget, self.action_input: a})\n        if self.update_count % self.update_target_frequency == 0 and 'NoTarget' not in self.agent_name:\n            self.sess.run(self.target_params_update)\n        #if self.update_count % 500 == 0:\n        #    print(' the mean bound is ------------------------------- ', self.get_state_dependent_bound(s))\n        #    self.test_gradnorm(s, a)\n        self.update_count += 1\n\n    def get_params_grad_interference(self, replaybuffer, batchSize):\n        s, a, sp, r, g = replaybuffer.sample_batch(batchSize)\n        vecs_all = []\n        vecs_2nd = []\n        qtarget = self.compute_Qtarget(sp / self.statescale, r / self.reward_scale, g)\n        for i in range(s.shape[0]):\n            vec_all, vec_2nd = self.sess.run([self.flattened_gradvec, self.flattened_hidden2vec],\n                                feed_dict={self.state_input: s[[i], :] / self.statescale,\n                                           self.qtarget_input: qtarget[[i]],\n                                           self.action_input: a[[i]]})\n            vecs_all.append(vec_all)\n            vecs_2nd.append(vec_2nd)\n        vecs_all = np.vstack(vecs_all)\n        vecs_2nd = np.vstack(vecs_2nd)\n        print('the shape is ========================= ', vecs_2nd.shape, vecs_all.shape)\n        gradallparams_dict = get_grad_interferences(vecs_all)\n        grad2ndparams_dict = get_grad_interferences(vecs_2nd, '2ndlayer')\n        gradallparams_dict.update(grad2ndparams_dict)\n        return gradallparams_dict\n\n\nclass TCDQNAgent(Agent):\n    def __init__(self, params):\n        super(TCDQNAgent, self).__init__(params)\n\n        self.agent_function = DQN(params)\n\n    def take_action(self, state):\n        if np.random.uniform(0.0, 1.0) < self.epsilon or not self.start_learning:\n            action = np.random.randint(self.actionDim)\n        else:\n            action = self.agent_function.take_action(state)\n        return action\n\n    def custermized_log(self, logger):\n        if 'OverlapSparse' not in logger.logger_dict:\n            logger.logger_dict['OverlapSparse'] = []\n        if 'InstanceSparse' not in logger.logger_dict:\n            logger.logger_dict['InstanceSparse'] = []\n        if 'GradDot' not in logger.logger_dict:\n            for key in ['GradDot', 'NegGradDot', 'NegGradDotProp', 'InstanceGradSparse']:\n                logger.logger_dict[key] = []\n                logger.logger_dict[key + '2ndlayer'] = []\n        if self.replaybuffer.getSize() >= self.batchSize:\n            overlapsp, instancesp = tfget_sparsities(self.agent_function.sess, self.agent_function.state_input,\n                                                     self.agent_function.phi, self.replaybuffer,\n                                                     self.agent_function.batchSize, self.agent_function.statescale)\n            logger.logger_dict['OverlapSparse'].append(overlapsp)\n            logger.logger_dict['InstanceSparse'].append(instancesp)\n            gradinfodict \\\n                = self.agent_function.get_params_grad_interference(self.replaybuffer, self.batchSize)\n            for key in gradinfodict:\n                logger.logger_dict[key].append(gradinfodict[key])\n            print(' the overlap and instance are :: ==================== ',\n                  overlapsp, instancesp, gradinfodict)\n\n    def update(self, s, a, sp, r, episodeEnd, info):\n        if self.notTrain:\n            return\n        gamma = self.gamma if not episodeEnd else 0.0\n        ''' epsilon is decayed only in atari games '''\n        self.linear_decay_epsilon()\n        self.replaybuffer.add(s, a, sp, r, gamma)\n        self.n_samples += 1\n\n        if self.replaybuffer.getSize() >= self.warm_up_steps and self.n_samples % self.trainFrequency == 0:\n            self.start_learning = True\n            for pn in range(self.planningSteps):\n                bs, ba, bsp, br, bgamma = self.replaybuffer.sample_batch(self.batchSize)\n                self.agent_function.train(bs, ba, bsp, br, bgamma)\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport sys\nfile_dir = os.path.dirname(__file__)\nsys.path.append(file_dir)\nfrom FunctionApproximator import FunctionApproximator\nfrom network.dqn_network import create_qnn_inputsparse\nfrom network.operations import compute_q_acted, update_target_nn_assign, update_critic_ops\nfrom Agent import Agent\nfrom network.ftann import FTA\n\n\nclass DQN(FunctionApproximator):\n    def __init__(self, params):\n        super(DQN, self).__init__(params)\n\n        self.create_qnn = create_qnn_inputsparse\n        self.unittype = tf.nn.tanh if self.usetanh == 1 else tf.nn.relu\n        self.statescale = 255. if self.use_atari_nn else 1.0\n\n        if 'NoTarget' in self.agent_name:\n            self.update_target_frequency = 1\n            self.compute_Qtarget = self.compute_Qtarget_wotar\n        else:\n            self.compute_Qtarget = self.compute_Qtarget_wtar\n\n        print('statedim, actiondim, tiles, use atari, target nn move rate are ---------------------------------- ',\n              self.stateDim, self.actionDim, self.create_qnn, self.update_target_frequency,\n              self.use_atari_nn, self.statescale)\n\n        with self.g.as_default():\n            self.fta = FTA(params)\n            '''used for batch normalization'''\n            self.action_input = tf.placeholder('int64', [None])\n            self.state_input, self.q_values, self.max_q_value, self.best_act, self.phi, self.tvars = \\\n                self.create_qnn(self.agent_name, self.dtype, self.stateDim, self.actionDim, self.n_h1, self.n_h2,\n                                self.unittype, self.fta)\n            ''' for target network a new FTA needs to be created '''\n            self.tar_state_input, self.tar_q_values, self.tar_max_q_value, self.tar_best_act, self.tar_phi, self.tar_tvars \\\n                = self.create_qnn(\"target_\"+self.agent_name, self.dtype, self.stateDim, self.actionDim, self.n_h1, self.n_h2,\n                                  self.unittype, FTA(params))\n            print('q value shape is ----------------- ', self.q_values.shape)\n            # define state action value\n            self.sa_value, self.tar_sa_value \\\n                = compute_q_acted(\"sa_values\", self.action_input, self.actionDim, self.q_values, self.tar_q_values)\n            # define loss operation, weight has been incorporated in fta_loss\n            self.qtarget_input, self.params_update = update_critic_ops(self.sa_value, self.learning_rate,\n                                                                           self.fta.fta_loss, 1.0)\n            # define optimization\n            self.target_params_update = update_target_nn_assign(self.tar_tvars, self.tvars)\n            self.gradvars = tf.gradients(self.max_q_value, self.tvars)[0]\n            # initialize network\n            self.sess.run(tf.global_variables_initializer())\n            self.sess.run(self.target_params_update)\n            self.saver = tf.train.Saver()\n\n            self.sparse_dim = int(self.phi.shape[1])\n            print(' the sparse dim is ===================================== ', self.sparse_dim)\n\n    def compute_Qtarget_wtar(self, sp, r, gamma):\n        max_q = self.sess.run(self.tar_max_q_value, feed_dict={self.tar_state_input: sp})\n        qtarget = r + gamma * max_q\n        return qtarget\n\n    def compute_Qtarget_wotar(self, sp, r, gamma):\n        max_q = self.sess.run(self.max_q_value, feed_dict={self.state_input: sp})\n        qtarget = r + gamma * max_q\n        return qtarget\n\n    def take_action(self, state):\n        if len(state.shape) < 2:\n            state = state[None, :]\n        act = self.sess.run(self.best_act, feed_dict={self.state_input: state/self.statescale})\n        return act[0]\n\n    def train(self, s, a, sp, r, gamma):\n        qtarget = self.compute_Qtarget(sp/self.statescale, r/self.reward_scale, gamma)\n        self.sess.run(self.params_update,\n                      feed_dict={self.state_input: s/self.statescale,\n                                 self.qtarget_input: qtarget, self.action_input: a})\n        if self.update_count % self.update_target_frequency == 0 and 'NoTarget' not in self.agent_name:\n            self.sess.run(self.target_params_update)\n        self.update_count += 1\n\n\n''' this implementation is a modification of \n    Two Geometric Input Transformation Methods for \n    Fast Online Reinforcement Learning with Neural Nets '''\n\n\nclass TCNNAgent(Agent):\n    def __init__(self, params):\n        super(TCNNAgent, self).__init__(params)\n        self.agent_function = DQN(params)\n\n    def take_action(self, state):\n        if np.random.uniform(0.0, 1.0) < self.epsilon or not self.start_learning:\n            action = np.random.randint(self.actionDim)\n        else:\n            action = self.agent_function.take_action(state)\n        return action\n\n    def update(self, s, a, sp, r, episodeEnd, info):\n        gamma = self.gamma if not episodeEnd else 0.0\n        ''' epsilon is decayed only in atari games '''\n        self.linear_decay_epsilon()\n        self.replaybuffer.add(s, a, sp, r, gamma)\n        self.n_samples += 1\n        if self.replaybuffer.getSize() >= self.warm_up_steps and self.n_samples % self.trainFrequency == 0:\n            self.start_learning = True\n            for pn in range(self.planningSteps):\n                bs, ba, bsp, br, bgamma = self.replaybuffer.sample_batch(self.batchSize)\n                self.agent_function.train(bs, ba, bsp, br, bgamma)\n\nimport os\nimport sys\nfile_dir = os.path.dirname(__file__)\nsys.path.append(file_dir)\nimport numpy as np\nimport tensorflow as tf\nfrom FunctionApproximator import FunctionApproximator\nfrom network.actor_critic_network import create_actor_nn, create_actor_fta_nn, create_critic_fta_nn_vanilla\nfrom network.operations import update_target_nn_assign, update_target_nn_move\nfrom Agent import Agent\nfrom sparseutils import tfget_sparsities_state_action_pair\nfrom network.ftann import FTA\n\n\nclass DDPG(FunctionApproximator):\n    def __init__(self, params):\n        # NOTE the first assignment\n        super(DDPG, self).__init__(params)\n        # use the same setting as used in the paper\n        self.actor_lc = params['alpha']\n        self.critic_lc = params['critic_factor'] * params['alpha']\n\n        self.sparseactor = params['sparseactor']\n\n        self.sample_count = 0\n\n        self.create_critic = create_critic_fta_nn_vanilla\n        self.create_actor = create_actor_fta_nn\n\n        if 'NoTarget' in self.agent_name:\n            self.computeQtargets = self.computeQtargets_wotar\n        elif 'ActorTarget' in self.agent_name:\n            self.computeQtargets = self.computeQtargets_wactortar\n        else:\n            self.computeQtargets = self.computeQtargets_wtar\n\n        with self.g.as_default():\n            self.actor_fta = FTA(params)\n            self.critic_fta = FTA(params)\n            ''' create actor network '''\n            if self.sparseactor == 1:\n                self.actor_input, self.actor_output, self.actorhiddenphi, self.actor_vars \\\n                    = self.create_actor('actor',  self.stateDim, self.actionDim, self.actionBound,\n                                        self.n_h1, self.n_h2, self.usetanh, self.actor_fta)\n                self.target_actor_input, self.target_actor_output, _, self.target_actor_vars \\\n                    = self.create_actor('target_actor',  self.stateDim, self.actionDim, self.actionBound,\n                                        self.n_h1, self.n_h2, self.usetanh, self.actor_fta)\n                print(' sparse actor also used ======================================================== ')\n            else:\n                self.actor_input, self.actor_output, self.actor_vars \\\n                    = create_actor_nn('actor', self.stateDim, self.actionDim, self.actionBound, self.n_h1, self.n_h2,\n                                      self.usetanh)\n                self.target_actor_input, self.target_actor_output, self.target_actor_vars \\\n                    = create_actor_nn('target_actor', self.stateDim, self.actionDim, self.actionBound, self.n_h1,\n                                      self.n_h2, self.usetanh)\n            ''' create critic network '''\n            self.critic_input_s, self.critic_input_a, self.critic_output, self.phi, self.critic_vars \\\n                = self.create_critic(self.agent_name, self.stateDim, 1, self.actor_output,\n                                     self.n_h1, self.n_h2, self.usetanh, self.critic_fta)\n            self.target_critic_input_s, self.target_critic_input_a, self.target_critic_output,\\\n                self.tar_phi, self.target_critic_vars \\\n                = self.create_critic('target_'+self.agent_name, self.stateDim, 1, self.target_actor_output,\n                                     self.n_h1, self.n_h2, self.usetanh, self.critic_fta)\n            # create ops to update critic and actor\n            self.critic_value_holders, self.critic_update = self.update_critic_ops(self.critic_lc)\n            self.actor_update = self.update_actor_ops(self.actor_lc)\n            # init target NN the same variable values\n            self.tar_tvars = self.target_actor_vars + self.target_critic_vars\n            self.tvars = self.actor_vars + self.critic_vars\n\n            self.gradvars = tf.gradients(self.critic_output, self.critic_vars)[0]\n            self.target_params_init = update_target_nn_assign(self.tar_tvars, self.tvars)\n            self.target_params_update = update_target_nn_move(self.tar_tvars, self.tvars, self.tau)\n            self.target_actor_params_update = update_target_nn_move(self.target_actor_vars, self.actor_vars, self.tau)\n\n            # init session\n            self.sess = tf.Session()\n            self.sess.run(tf.global_variables_initializer())\n            self.sess.run(self.target_params_init)\n            ''' init saver '''\n            self.saver = tf.train.Saver()\n\n            ''' sparse dim '''\n            self.sparse_dim = int(self.phi.shape[1])\n            self.state_visit_count = np.zeros(self.sparse_dim)\n\n    def update_actor_ops(self, lr):\n        with self.g.as_default():\n            critic_output_mean = -tf.reduce_mean(self.critic_output) \\\n                                 + self.actor_fta.fta_loss\n            actor_grads = tf.gradients(critic_output_mean, self.actor_vars)\n            actor_grads_feed = zip(actor_grads, self.actor_vars)\n            actor_update = tf.train.AdamOptimizer(lr, name='actor_opt').apply_gradients(actor_grads_feed)\n        return actor_update\n\n    def update_critic_ops(self, lr):\n        with self.g.as_default():\n            critic_value_holders = tf.placeholder(tf.float32, [None], name='q_holders')\n            critic_loss = tf.reduce_mean(tf.square(critic_value_holders - tf.squeeze(self.critic_output))) \\\n                          + self.critic_fta.fta_loss\n            critic_update_grads = tf.gradients(critic_loss, self.critic_vars)\n            critic_grads_feed = zip(critic_update_grads, self.critic_vars)\n            critic_update = tf.train.AdamOptimizer(lr, name='critic_opt').apply_gradients(critic_grads_feed)\n        return critic_value_holders, critic_update\n\n    '''return an action to take for each state, NOTE this action is in [0, 1]'''\n    def take_action(self, state):\n        action = self.sess.run(self.actor_output,\n                               {self.actor_input: np.expand_dims(state, 0)})\n        return action[0]\n\n    def computeQtargets_wtar(self, state_tp, reward, gamma):\n        qvalues = self.sess.run(self.target_critic_output,\n                                {self.target_critic_input_s: state_tp, self.target_actor_input: state_tp})\n        qtargets = reward + gamma * np.squeeze(qvalues)\n        return qtargets\n\n    def computeQtargets_wactortar(self, state_tp, reward, gamma):\n        actions = self.sess.run(self.target_actor_output, {self.target_actor_input: state_tp})\n        qvalues = self.sess.run(self.critic_output,\n                                {self.critic_input_s: state_tp, self.critic_input_a: actions})\n        qtargets = reward + gamma * np.squeeze(qvalues)\n        return qtargets\n\n    def computeQtargets_wotar(self, state_tp, reward, gamma):\n        qvalues = self.sess.run(self.critic_output,\n                                {self.critic_input_s: state_tp, self.actor_input: state_tp})\n        qtargets = reward + gamma * np.squeeze(qvalues)\n        return qtargets\n\n    def update_target_nn(self):\n        if 'ActorTarget' in self.agent_name:\n            self.sess.run(self.target_actor_params_update)\n        elif 'NoTarget' not in self.agent_name:\n            self.sess.run(self.target_params_update)\n        self.update_count += 1\n\n    def train(self, state, action, state_tp, reward, gamma):\n        qtargets = self.computeQtargets(state_tp, reward/self.reward_scale, gamma)\n        self.sess.run(self.critic_update, feed_dict={self.critic_input_s: state, self.critic_input_a: action,\n                                                     self.critic_value_holders: qtargets})\n        self.sess.run(self.actor_update,\n                      feed_dict={self.actor_input: state, self.critic_input_s: state})\n        self.update_target_nn()\n\n\nclass TCDDPGAgent(Agent):\n    def __init__(self, params):\n        super(TCDDPGAgent, self).__init__(params)\n        self.agent_function = DDPG(params)\n\n        self.max_reward = 0.0\n\n        self.n_episode = 0.\n        self.noise_t = np.zeros(self.actionDim)\n\n        self.notTrain = False\n\n\n    def take_action(self, state):\n        if not self.start_learning:\n            return np.random.uniform(-self.actionBound, self.actionBound, self.actionDim)\n        action = self.agent_function.take_action(state)\n        self.noise_t += np.random.normal(0, 0.2, self.actionDim) - self.noise_t * 0.15\n        action = action + self.noise_t * self.conti_noisescale\n        return np.clip(action, -self.actionBound, self.actionBound)\n\n    def custermized_log(self, logger):\n        if 'OverlapSparse' not in logger.logger_dict:\n            logger.logger_dict['OverlapSparse'] = []\n        if 'InstanceSparse' not in logger.logger_dict:\n            logger.logger_dict['InstanceSparse'] = []\n        if self.replaybuffer.getSize() >= self.batchSize:\n            overlapsp, instancesp = tfget_sparsities_state_action_pair(self.agent_function.sess,\n                                                                       self.agent_function.critic_input_s,\n                                                                       self.agent_function.critic_input_a,\n                                                     self.agent_function.phi, self.replaybuffer,\n                                                     self.agent_function.batchSize, 1.0)\n            logger.logger_dict['OverlapSparse'].append(overlapsp)\n            logger.logger_dict['InstanceSparse'].append(instancesp)\n\n            print(' the overlap, instance, critic, actor bounds are :: ========================================== ',\n                  overlapsp, instancesp)\n\n\n    '''here we use and store option, not primal actions, so primal action is not directly used for training'''\n    '''they passed a in this function is the primal action, not option'''\n    def update(self, s, a, sp, r, episodeEnd, info):\n        gamma = self.gamma if not episodeEnd else 0.0\n        if episodeEnd:\n            self.n_episode += 1.\n        self.replaybuffer.add(s, a, sp, r, gamma)\n        self.n_samples += 1\n        #self.agent_function.test_sparse(self.replaybuffer, self.n_samples)\n        if self.replaybuffer.getSize() >= self.warm_up_steps:\n            self.start_learning = True\n            for _ in range(self.planningSteps):\n                bs, ba, bsp, br, bgamma = self.replaybuffer.sample_batch(self.batchSize)\n                self.agent_function.train(bs, ba, bsp, br, bgamma)",
        "experimental_info": "The Fuzzy Tiling Activation (FTA) method's core implementation resides in the `FTAConfiguration` and `FTA` classes. These classes manage the creation of tiling centers, the fuzzy indicator function (`Iplus_eta`), and the logic for generating sparse representations. FTA can be applied as a standard activation function within a neural network layer.\n\n### Configuration Parameters (from `default_params` in `agents/Agent.py`):\n-   `n_tiles`: 20 - Number of tiles used in a single tiling.\n-   `n_tilings`: 1 - Number of distinct tilings. If `n_tilings` > 1, the `fta_input_max` can be a list.\n-   `fta_input_max`: 1.0 - The maximum input value for the tiling. `fta_input_min` is derived as `-fta_input_max` by default.\n-   `fta_eta`: (derived) - The parameter controlling the 'fuzziness' of the indicator function. It is calculated as `(fta_input_max - fta_input_min) / n_tiles` if not explicitly set.\n-   `actfunctypeFTA`: 'linear' - The activation function applied to the input *before* the FTA transformation (e.g., 'tanh', 'relu', 'linear', 'clip', 'sin').\n-   `outofbound_reg`: 0.0 - A regularization weight for inputs that fall outside the defined `fta_input_min`/`max` bounds. Applied if `actfunctypeFTA` is not 'tanh', 'sigmoid', or 'clip'.\n-   `extra_strength`: False - If true, an additional fully-connected layer computes a 'strength' factor for the sparse activation, parameterized by `actfunctypeFTAstrength`.\n-   `actfunctypeFTAstrength`: 'linear' - The activation function used for the `extra_strength` layer if `extra_strength` is enabled.\n-   `sparse_dim`: None - The sparse dimension is automatically calculated based on network architecture and FTA parameters.\n-   `sparseactor`: 0 - A specific parameter for DDPG to control whether the actor network utilizes FTA (1 for sparse, 0 for regular).\n\n### Integration into Neural Networks:\nFTA is integrated into neural networks by replacing a standard activation function with `SparseActFunc.func` (where `SparseActFunc` is an instance of the `FTA` class). This is demonstrated in:\n\n-   **DQN-based Agents (`agents/TCDQN.py`, `agents/TCNN.py`):**\n    -   `create_fta_qnn` (used by `TCDQN`): Applies FTA to a hidden layer output. For example, `sparse_phi = tf.layers.dense(hidden1, n_hidden2, activation=SparseActFunc.func)`. The `fta_loss` (out-of-bound regularization) is added to the critic's loss function.\n    -   `create_qnn_inputsparse` (used by `TCNN`): Applies FTA directly to the state input, e.g., `sparse_state_input = SparseActFunc.func(state_input)`.\n\n-   **DDPG-based Agents (`agents/TCDDPG.py`):**\n    -   `create_actor_fta_nn`: Applies FTA to a hidden layer in the actor network. The `actor_fta.fta_loss` is added to the actor's objective function.\n    -   `create_critic_fta_nn_vanilla`: Applies FTA to a hidden layer in the critic network, combining state and action features. The `critic_fta.fta_loss` is added to the critic's loss function.\n\n### Sparsity Monitoring:\n-   `TCDQNAgent` and `TCDDPGAgent` include `custermized_log` methods that calculate and log sparsity metrics like `OverlapSparse` (mean overlap between active units for different states) and `InstanceSparse` (mean number of active units per instance), as well as gradient interference metrics."
      }
    },
    {
      "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
      "abstract": "We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.",
      "full_text": "Online Learned Continual Compression with Adaptive Quantization Modules Lucas Caccia 1 2 3 Eugene Belilovsky 2 4 Massimo Caccia 2 4 5 Joelle Pineau 1 2 3 Abstract We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a rep- resentative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting en- counters a major challenge: representations de- rived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the com- pression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and cur- rent progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to sig- niﬁcant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learn- ing agents. 1. Introduction Interest in machine learning in recent years has been fueled by the plethora of data being generated on a regular basis. Effectively storing and using this data is critical for many applications, especially those involving continual learning. In general, compression techniques can greatly improve data storage capacity, and, if done well, reduce the memory and compute usage in downstream machine learning tasks (Gueguen et al., 2018; Oyallon et al., 2018). Thus, learned compression has become a topic of great interest (Theis 1McGill 2Mila 3Facebook AI Research 4University of Montreal 5ElementAI. Correspondence to: Lucas Caccia <lucas.page-caccia@mail.mcgill.ca>, Eugene Belilovsky <eu- gene.belilovsky@umontreal.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). et al., 2017; Ball´e et al., 2016; Johnston et al., 2018). Yet its application in reducing the size of datasets bound for machine learning applications has been limited. This work focuses on the following familiar setting: new training data arrives continuously for a learning algorithm to exploit, however this data might not be iid, and further- more there is insufﬁcient storage capacity to preserve all the data uncompressed. We may want to train classiﬁers, reinforcement learning policies, or other models continu- ously from this data as it’s being collected, or use samples randomly drawn from it at a later point for a downstream task. For example, an autonomous vehicle (with bounded memory) collects large amounts of high-dimensional train- ing data (video, 3D lidar) in a non-stationary environment (e.g. changing weather conditions), and overtime applies an ML algorithm to improve its behavior using this data. This data might be transferred at a later point for use in down- stream learning. Current learned compression algorithms, e.g. (Torfason et al., 2018), are not well designed to deal with this case, as their convergence speed is too slow to be usable in an online setting. In the ﬁeld of continual/lifelong learning (Thrun & Mitchell, 1995), which has for now largely focused on classiﬁcation, approaches based on storing memories for later use have emerged as some of the most effective in online settings (Lopez-Paz et al., 2017; Aljundi et al., 2018; Chaudhry et al.; 2019; Aljundi et al., 2019). These memories can be stored as is, or via a generative model (Shin et al., 2017). Then, they can either be used for rehearsal (Chaudhry et al., 2019; Aljundi et al., 2019) or for constrained optimization (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2018). Indeed many continual learning applications would be greatly improved with replay approaches if one could afford to store all samples. These approaches are however inherently limited by the amount of data that can be stored Learning a generative model to compress the previous data stream thus seems like an appealing idea. However, learn- ing generative models, particularly in the online and non- stationary setting, continues to be challenging, and can greatly increase the complexity of the continual learning task. Furthermore, such models are susceptible to catas- trophic forgetting (Aljundi et al., 2019). An alternate ap- proach is to learn a compressed representation of the data, arXiv:1911.08019v3  [cs.LG]  20 Aug 2020Online Learned Continual Compression with Adaptive Quantization Modules Online Continual Compression z_1 z_1 z_2 z_3z_1 z_2 X ~ non-iid t=1 t=2 t=3 query  samples query  samples Learner Learner Encoder Encoder Encoder Decoder     t=3 Decoder     t=1 Decoder     t=2 storage storage storage  Decoder     t=3 z_1 Decoder     t=1 Representation Drift z_1 X_3 X_2 X_1 Figure 1.Illustration of the challenges in the Online Continual Compression problem. A model must be able to decode representations encoded by previous versions of the autoencoder, permitting anytime access to data for the learner. This must be accomplished while dealing with a time-varying data distribution and ﬁxed memory constraints which can be more stable than learning generative models. Learned compression in the online and non-stationary set- ting itself introduces several challenges illustrated in Fig 1. Firstly the learned compression module must be able to de- code representations encoded by earlier versions of itself, introducing a problem we refer to as representation drift. Secondly, the learned compressor is itself susceptible to catastrophic forgetting. Finally, the learned compression needs to be adaptive to maintain a prescribed level of re- construction quality even if it has not fully adapted to the current distribution. In this work we demonstrate that the VQ-V AE framework (van den Oord et al., 2017; Razavi et al., 2019), originally introduced in the context of generative modeling and density estimation, can be used online to effectively address repre- sentation drift while achieving high compression. Further- more, when augmented with an internal replay mechanism it can overcome forgetting. Finally we use propose to use multiple gradient-isolated compression levels to allow the compressor to adaptively store samples at different compres- sion scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. The main contributions of this work are: (a) we intro- duce and highlight the online learned continual compres- sion (OCC) problem and its challenges. (b) We show how representation drift, one of the key challenges, can be tackled by effective use of codebooks in the VQ-V AE framework. (c) We propose an architecture using multi- ple VQ-V AE’s, adaptive compression scheme, stream sam- pling scheme, and self-replay mechanism that work together to effectively tackle the OCC problem. (d) We demon- strate this can yield state-of-the-art performance in stan- dard online continual image classiﬁcation benchmarks and demonstrate the applications of our OCC solution in a va- riety of other contexts. The code to reproduce our results is available at https://github.com/pclucas14/ adaptive-quantization-modules 2. Related Work Learned compression has been recently studied for the case of image compression. Works by Theis et al. (2017); Ball´e et al. (2016); Johnston et al. (2018) have shown learned compressions can outperform standard algorithms like JPEG. These methods however are difﬁcult to adapt for online settings as they do not directly address the challenges of the OCC problem (e.g. representation drift). Continual Learning research currently focuses on over- coming catastrophic forgetting (CF) in the supervised learn- ing setting, with some limited work in the generative mod- eling and reinforcement learning settings. Most continual learning methods can be grouped into three major families. Some algorithms dynamically change the model’s architec- ture to incorporate learning from each task separately (Rusu et al., 2016; Li & Hoiem, 2018; Fernando et al., 2017). Al- though these methods can perform well in practice, their introduction of task-speciﬁc weights requires growing com- pute and memory costs which are problematic for the online setting. Another set of techniques employ regularization to constrain weights updates in the hope of maintaining knowl- edge from previous tasks. Notable methods in this class include (Kirkpatrick et al., 2017; Husz´ar, 2017; Zenke et al., 2017; Nguyen et al., 2017; Chaudhry et al., 2018). However, this set of approaches has been shown to be inefﬁcient in the online setting (Chaudhry et al., 2019). The last family of methods encapsulates all that have a mech- anism to store information about the previous data distribu- tions. This memory then serves as a tool for the continual learner to rehearse previous tasks. The simplest instantia- tion of this method is to keep and sample from a buffer of old data to retrain the model after every update (Chaudhry et al., 2019). This approach is widely used in RL where it is known as Experience Replay (ER) (Lin, 1993; Mnih et al., 2013; Andrychowicz et al., 2016). Another method, known as Generative Replay (GR) (Shin et al., 2017), uses generative modeling to store past task distributions. The con- tinual learner then trains on generated samples to alleviateOnline Learned Continual Compression with Adaptive Quantization Modules CF. Other notable examples are Gradient Episodic Memory (GEM) (Lopez-Paz et al., 2017), iCarl (Rebufﬁ et al., 2017), and Maximally Interfered Retrieval (MIR) (Aljundi et al., 2019), as well as (Aljundi et al., 2018; Hu et al., 2018). Most closely related to our work, Riemer et al. (2017) consider compressing memories for use in the continual classiﬁcation task. They also employ a discrete latent variable model but with the Gumbel approximation, which shows to be less effective than our approach. Furthermore a separate ofﬂine iid pre-training step for the learned compression is required in order to surpass the ER baseline, distinctly different from the online continual compression we consider. Lidar compression is considered in (Tu et al., 2019) and (Caccia et al., 2018). Both approaches use a similar projec- tion from 3D (x,y,z ) coordinates to 2D cylindrical coordi- nates, and leverage deep generative models to compress the data. However, neither accounts for potential distribution shift, nor for online learning. In this work we show that us- ing this 2D projection in conjunction with our model allows us to mitigate the two issues above for lidar data. 3. Methodology In this section we outline our approach to the online con- tinual compression problem. First we will review the VQ- V AE and highlight the properties making it effective for representational drift. Then we will describe our adaptive architecture, storage, and sampling scheme. 3.1. Problem Setting: Online Continual Compression We consider the problem setting where a stream of samples x ∼Dt arrives from different distributions Dt changing over time t= 1...T . We have a ﬁxed storage capacity ofC bytes where we would like to store the most representative information from all data distributions D1,...DT . There is notably a trade-off in quality of information versus the amount of samples stored. We propose to use a learned compression model, and most crucially, this model must also be stored within the Cbytes, to encode and decode the data samples. Another critical requirement is that at anytime tthe content of the storage (data and/or compression model) be usable for downstream applications. An important chal- lenge, illustrated in Figure 1, is that the learned compression module will change over time, while we still need to be able to decode the memories in storage. 3.2. Vector Quantized V AE for Online Compression The VQ-V AE is a discrete auto-encoder which relies on a vector quantization step to obtain discrete latent repre- sentations. An embedding table, E ∈RK×D consisting of K vectors of size D, is used to quantize encoder out- puts. Given an input (e.g. an RGB image), the encoder ﬁrst encodes it as a Hh ×Wh ×D tensor, where Hh Encoder     t=1      Encoder     t=3      same after VQ Vector Quantization different codebook Figure 2.Illustration of reduced representation drift from Vector Quantization and Wh denote the height and width of the latent repre- sentation. Then, every Ddimensional vector is quantized using a nearest-neighbor lookup on the embedding table. Speciﬁcally, zij = arg mine∈E ||enc(x)ij −e||2, where i,j refers to a spatial location. The output of the quanti- zation step is then fed through the decoder. The gradient of this non-differentiable step is approximated using the straight-through estimator. An important property to notice is that to reconstruct the input, only the Hh ×Wh indices are required, thus yielding high compression (van den Oord et al., 2017). Critically, the embedding tables are updated independently from the encoder and decoder, namely by minimizing mine ||sg[enc(x)ij] −e||, where sg is the stop gradient operator. Observe in the case of online compression, if the embedding table is ﬁxed, then a change in the encoder parameters and therefore a change in the encoder output for a given input will not change the ﬁnal quantized representation z, unless it is sufﬁciently large, thus we can observe that if the em- beddings change slowly or are ﬁxed we can greatly improve our control of the representational drift. This effect is illus- trated in Figure 2. On the other hand we do need to adapt the embedding table, since randomly selected embeddings would not cover well the space of encoder outputs. 3.3. Adaptive Quantization Modules To address issues of how to optimize storage and sampling in the context of Online Continual Compression we intro- duce Adaptive Quantization Modules (AQM). We use AQM to collectively describe the architecture, adaptive multi-level storage mechanism, and data sampling method used. To- gether they allow effectively constraining individual sample quality and memory usage while keeping in storage a faith- ful representation of the overall distribution.Online Learned Continual Compression with Adaptive Quantization Modules Algorithm 1: AQM L EARNING WITH SELF -REPLAY Input: Learning rate α, EXTERNAL LEARNER 1 Initialize: AQM Memory M; AQM Parameters θaqm 2 for t∈1..T do 3 % Fetch data from current task 4 for Binc ∼Dt do 5 for n∈1..N do 6 B←Binc 7 if t> 1 then 8 % Fetch data from buffer 9 Bre ∼SAMPLE (M,θaqm) 10 B ←(Binc,Bre) 11 end 12 % Update AQM 13 θaqm ←ADAM(θaqm,B,α ) 14 % Send data to external learner 15 EXTERNAL LEARNER (B) 16 if t> 1 then UPDATE BUFFER REP(M,θaqm) 17 %Save current indices 18 ADDTOMEMORY (M,Binc,θaqm) 19 end 20 end 21 end AQM uses an architecture consisting of a sequence of VQ- V AEs, each with a buffer. The AQM approach to online continual compression is overall summarized in Algorithm 1 (note ADDTOMEMORY is described in Appendix A). In- coming data is added to storage using an adaptive compres- sion scheme described in Algorithm 2. Incoming data is also used along with randomly sampled data from storage (self- replay) to update the current AQM model. The randomly sampled data also updates its representation in storage as per Algorithm 3. As illustrated by the optional lines in blue, Algorithm 1 can run concurrently with a downstream learn- ing task (e.g. online continual classiﬁcation) which would use the same batch order. It can also be run independently as part of a data collection. In the sequel we give further details on all these elements 3.3.1. A RCHITECTURE AND TRAINING Each AQM module contains a VQ-V AE and a corresponding buffer of adaptive capacity. A diagram of the architecture is given in Figure 3. We will denote the output after quan- tization of each module i as zi q and the set of codebook indexes used to obtain zi q as ai. Note that ai are the discrete representations we actually store. Each subsequent module produces and stores an ai requiring fewer bits to represent. For RGB images, the compression rate at a given level is given by H×W×3×log2(256) Nc×Hhi×Whi×⌈log2 (Ki)⌉. Here Ki is the number of embeddings in the codebooks, ( Hhi, Whi) the spatial dimension of the latent representation and Nci the number of codebooks. Algorithm 2: ADAPTIVE COMPRESS Input: datapoint x, AQM with Lmodules, threshold dth 1 % Forward all modules, store encodings 2 {zi q,ai}i=1..L= ENCODE (x) 3 for i∈L...1 do 4 % Decode from level i to output space 5 ˆx= DECODE (zi q) 6 % Check reconstruction error 7 if MSE(ˆx,x) <dth then return ai, i 8 end 9 % Otherwise, return original input 10 return x,0 Algorithm 3: UpdateBufferRep Input: Memory M, AQM with Llevels, data D, distortion threshold dth 1 for x∈Ddo 2 hidx, blockid = ADAPTIVE COMPRESS (x, AQM, dth) 3 % Delete Old Repr. 4 DELETE (M[x]) 5 % Add new one 6 ADD(M, hidx) 7 end VQV AE-2 (Razavi et al., 2019) also uses a multi-scale hier- archical organization, where unlike our AQM, the top level models global information such as shape, while the bottom level, conditioned on the top one, models local information. While this architecture is tailored for generative modeling, it is less attractive for compression, as both the bottom and top quantized representations must be stored for high qual- ity reconstructions. Furthermore in AQM each module is learned in a greedy manner using the current estimate of z(i−1) q without passing gradients between modules similar to (Belilovsky et al., 2019; Nøkland & Eidnes, 2019). A subsequent module is not required to build representations which account for all levels of compression, thus minimiz- ing interference across resolutions. This allows the modules to each converge as quickly as possible with minimal drift at their respective resolution, particularly important in the online continual learning case. 3.3.2. M ULTI -LEVEL STORAGE Our aim is to store the maximum number of samples in an allotted Cbytes of storage, while assuring their quality, and our ability to reconstruct them. Samples are thus stored at different levels based on the compressors’ current ability. The process is summarized in Algorithm 2. Such an approach is particularly helpful in the online non- stationary setting, allowing knowledge retention before the compressor network has learned well the current distribution. Note in Alg. 2 samples can be completely uncompressed until the ﬁrst module is able to effectively encode them. This can be crucial in some cases, if the compressor has not yet converged, to avoid storing poorly compressed rep-Online Learned Continual Compression with Adaptive Quantization Modules Figure 3.Architecture of Adaptive Quantization Modules. Each level uses its own loss and maintains its own replay buffer. Yello dotted lines indicate gradient isolation between modules resentations. Further taking into account that compression difﬁculty is not the same for all datapoints, this allows use of more capacity for harder data, and fewer for easier. We also note, since we maintain stored samples at each mod- ule and the modules are decoupled, that such an approach allows to easily distribute training in an asynchronous man- ner as per Belilovsky et al. (2019). 3.3.3. S ELF -REPLAY AND STREAM SAMPLING As shown in Alg. 1 our AQM is equipped with an internal experience replay mechanism (Mnih et al., 2013), which reconstructs a random sample from storage and uses it to perform an update to the AQM modules, while simultane- ously freeing up overall memory if the sample can now be compressed at a later AQM module. This has the effect of both reducing forgetting and freeing memory. In practice we replay at the same rate as incoming samples arrive and thus replay will not increase asymptotic complexity of the online learning. Finally, for efﬁciency the replay can be coupled to an external online learner querying for random samples from the overall memory. Since we would like the AQM to work in cases of a ﬁxed memory capacity it must also be equipped with a mecha- nism for selecting which samples from the stream to store and which to delete from memory. Reservoir Sampling (RS) is a simple yet powerful approach to this problem, used successful in continual learning (Chaudhry et al., 2019). It adds a sample from the stream with prob. p= buffer capacity points seen so far while remove a random sample. However, RS is not directly compatible with AQM primarily because the amount of sam- ples that can be stored varies over time. This is because samples at different levels have different memory usage and memory can be freed by replay. We thus propose an alternative scheme, which maximally ﬁlls available memory and selects non-uniformly samples for deletion. Speciﬁ- cally when a larger amount of samples are added at one point in the stream, they become more likely to be removed. The details of this stream sampling method are provided in Appendix A. 3.3.4. DRIFT CONTROL VIA CODEBOOK STABILIZATION As mentioned previously, a good online compressor must control its representational drift, which occurs when up- dates in the auto-encoder parameters creates a mismatch with the static representations in the buffer. Throughout the paper we measure representational drift by compar- ing the following time varying quantity: DRIFT t(z) = RECON ERR ( Decode(θt; z),x ) where θt the model pa- rameters at timetand (z,x) is a stored compressed represen- tation and its original uncompressed datapoint respectively. For all experiments on images, RECON ERR is simply the mean squared error. As illustrated in Sec 3.2 a slow changing codebook can allow to control drifting representations. This can be in part accomplished by updating the codebook with an exponential moving average as described in (van den Oord et al., 2017, Appendix A), where it was used to reduce the variance of codebook updates. This alone is insufﬁcient to fully control drift, thus once a given module yields satisfactory compressions on the data stream, we freeze the module’s embedding matrix but leave encoder and decoder parameters free to change and adapt to new data. Moreover, we note that ﬁxing the codebook for a given module does not affect the reconstruction performance of subsequent modules, as they only need access to the current module’s decoder which can still freely change. 4. Experiments We evaluate the efﬁcacy of the proposed methods on a suite of canonical and new experiments. In Section 4.1 we present results on standard supervised continual learning benchmarks on CIFAR-10. In Section 4.2 we evaluate other downstream tasks such as standard iid training applied on the storage at the end of online continual compression. For this evaluation we consider larger images from Imagenet, as well as on lidar data. Finally we apply AQM on observations of an agent in an RL environment.Online Learned Continual Compression with Adaptive Quantization Modules Accuracy (↑) M= 20 M= 50 iid online 60.8 ±1.0 60 .8 ±1.0 iid ofﬂine 79.2 ±0.4 79 .2 ±0.4 GEM (Lopez-Paz et al., 2017) 16.8 ±1.1 17 .1 ±1.0 iCarl (5 iter) (Rebufﬁ et al., 2017) 28.6 ±1.2 33 .7 ±1.6 ﬁne-tuning 18.4 ±0.3 18 .4 ±0.3 ER 27.5 ±1.2 33 .1 ±1.7 ER-MIR (Aljundi et al., 2019) 29.8 ±1.1 40 .0 ±1.1 ER-JPEG 33.9 ±1.0 43 .1 ±0.6 Gumbel AE (Riemer et al., 2018) 25.5 ±2.0 28 .8 ±2.9 AQM (ours) 43.5 ±0.7 47 .0 ±0.8 Forgetting (↓) M= 20 M= 50 N/A N/A N/A N/A 73.5 ±1.7 70 .7 ±4.5 49 ±2.4 40 .6 ±1.1 85.4 ±0.7 85 .4 ±0.7 50.5 ±2.4 35 .4 ±2.0 50.2 ±2.0 30 .2 ±2.3 54.8 ±1.2 44 .3 ±0.9 71.5 ±2.8 67 .2 ±3.9 23.0 ±1.0 19 .0 ±1.4 Table 1.Shared head results on disjoint CIFAR-10. Total memory per class M measured in sample memory size. We report (a) Accuracy, (b) Forgetting (lower is better). 4.1. Online Continual Classiﬁcation Although CL has been studied in generative modeling (Ramapuram et al., 2017; Lesort et al., 2018; Zhai et al., 2019; Lesort et al., 2019) and reinforcement learning (Kirk- patrick et al., 2017; Fernando et al., 2017; Riemer et al., 2018), supervised learning is still the standard for evalua- tion of new methods. Thus, we focus on the online continual classiﬁcation of images for which our approach can provide a complement to experience replay. In this setting, a new task consists of new image classes that the classiﬁer must learn, while not forgetting the previous ones. The model is only allowed one pass through the data (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2019; Chaudhry et al., 2019). The online compression here takes the role of replay buffer in replay based methods such as (Chaudhry et al., 2019; Aljundi et al., 2019). We thus run Algorithm 1, with an additional online classiﬁer being updated performed at line 15. Here we consider the more challenging continual classiﬁca- tion setting often referred to as using ashared-head (Aljundi et al., 2019; Farquhar & Gal, 2018; Aljundi et al., 2018). Here the model is not informed of the task (and thereby the subset of classes within it) at test time. This is in contrast to other (less realistic) CL classiﬁcation scenarios where the task, and therefore subset of classes, is provided explicitly to the learner (Farquhar & Gal, 2018; Aljundi et al., 2019). For this set of experiments, we report accuracy, i.e. 1 T ∑T i=1 RT,i, and forgetting, i.e. 1 T−1 ∑T−1 i=1 max(R:,i) − RT,i with R ∈ RT×T representing the accuracy matrix where Ri,j is the test classiﬁcation accuracy on task jwhen task iis completed. Baselines A basic baseline for continual supervised learn- ing is Experience Replay ( ER). It consists of storing old data in a buffer to replay old memories. Although relatively simple recent research has shown it is a critical baseline to consider, and in some settings is actually state-of-the- art (Chaudhry et al., 2019; Aljundi et al., 2019; Rolnick et al., 2018). AQM can be used as an add-on to ER that in- corporates online continual compression. We also compare against ER with standard JPEG compression. In addition we consider the following baselines. iid online (upper-bound) trains the model with a single-pass through the data on the same set of samples, but sampled iid. iid ofﬂine (upper- bound) evaluates the model using multiple passes through the data, sampled iid. We use 5 epochs in all the experiments for this baseline. ﬁne-tuning trains continuously upon ar- rival of new tasks without any forgetting avoidance strategy. iCarl (Rebufﬁ et al., 2017) incrementally classiﬁes using a nearest neighbor algorithm, and prevents catastrophic for- getting by using stored samples. GEM (Lopez-Paz et al., 2017) uses stored samples to avoid increasing the loss on previous task through constrained optimization. It has been shown to be a strong baseline in the online setting. It gives similar results to the recent A-GEM (Chaudhry et al.). ER- MIR (Aljundi et al., 2019) controls the sampling of the replays to bias sampling towards samples that will be for- gotten. We note that the ER-MIR critera is orthogonal to AQM, and both can be applied jointly.Gumbel AE (Riemer et al., 2018) learns an autoencoder for ER using the Gumbel softmax to obtain discrete representations. We evaluate with the standard CIFAR-10 split (Aljundi et al., 2018), where 5 tasks are presented sequentially, each adding two new classes. Evaluations are shown in Table 1. Due to our improved storage of previous data, we observe sig- niﬁcant improvement over other baselines at various mem- ory sizes. We can contrast AQM’s performance with ER’s to understand the net impact of our compression scheme. Speciﬁcally, AQM improves over ER by 16.0% and 13.9% in the M=20 and M=50 case, highlighting the effectiveness of online compression. Our approach is also superior in forgetting by a signiﬁcant margin in both memory settings. To compare directly to reporting in (Riemer et al., 2018) we also benchmarked our implementation on the Incremental CIFAR-100 multi-head experiment (Lopez-Paz et al., 2017) with the same settings as in (Riemer et al., 2018). By using AQM we were able to get 65.3 vs the reported 43.7 using a buffer of size 200. To speciﬁcally isolate the advantageOnline Learned Continual Compression with Adaptive Quantization Modules Accuracy RS 5.2 ±0.2 2 Module AQM (ours) 23.2 ± 1.1 Ablate 2nd Module 20.5 ±1.3 Ablate Fixing Codebook 19.2 ±0.6 Ablate Decoupled Training 16.5 ±0.7 Ablate Adaptive Compression 13.1 ±3.2 Table 2.Imagenet ofﬂine training evaluation from online continual compression. We see a clear gain over a standard Reservoir sam- pling approach. We then ablate each component of our proposal showing each component is important. Note storage used in each experiment is identical (including accounting for model sizes). of gumbel softmax versus the vector quantization for drift, we replaced the vector quantization approach with gumbel softmax in an AQM. We observed signﬁcantly less drift in the case where vector quantization is used. Full details of this experiment are described in the supplementary materials along with visualizations. The CIFAR-10 dataset has a low resolution (3 ×32 ×32) and uses a lot of data per task (10K samples). These two characteristics might leave the online compression problem easier than in a real-life scenario. Speciﬁcally, if the ﬁrst tasks are long enough and the compression rate is not too large, the model can quickly converge and thus not incur too much representation drift. Indeed, we found that us- ing a single module is already sufﬁcient for this task. For these reasons, we now study the AQM in more challenging settings presented in the next section. 4.2. Ofﬂine Evaluation on Larger Images Besides the standard continual classiﬁcation setup, we pro- pose several other evaluations to determine the effectiveness of the stored data and compression module after learning online compression. We also perform a detailed ablation to study the efﬁcacy of each component in AQM. Ofﬂine training on Imagenet We compare the effective- ness of the stored memories of AQM after a certain amount of online continual compression. We do this by training in a standard iid way an ofﬂine classiﬁcation model using only reconstructions obtained from the storage sampled after on- line continual compression has progressed for a period of time. In each case we would have the same sized storage available. We note that simply having more stored memo- ries does not amount to better performance as their quality may be severely degraded and affected by drift. Using this evaluation we ﬁrst compare a standard reservoir sampling approach on uncompressed data to a 2 module AQM using the same size storage. We observe that perfor- mance is drastically increased using the compressed sam- ples. We then use this to perform a series of ablations to Figure 4.Impact of codebook freezing. Vertical black line indi- cates freezing point. We see that AQM is still able to adapt and reduce its reconstruction loss, while having stable compressed representations. Results averaged over 5 runs demonstrate each component of our proposal is important. Speciﬁcally (a) we restrict AQM to have only one module, (b) instead of decoupled training we train modules end-to- end, (c) we remove adaptive compression, thus all samples are stored in the most compressed block, regardless of qual- ity, and (d) we do not stabilize the codebook, the embedding matrices of every block are never ﬁxed. We observe that all these elements contribute to successfully storing a represen- tative set of data for the distribution online. Drift Ablation We have seen the importance of codebook freezing when dealing with high dimensional datasets. How- ever, judging solely from the ﬁnal downstream task perfor- mance it’s difﬁcult to see if the model continues adapting after freezing. As alluded in Sec 3.2 there is a tradeoff be- tween keeping recoverable representations and a model’s ability to continue to adapt. To shed some light on this, we run the following experiment: we run a vanilla VQ-V AE on the same 20 task mini-imagenet stream, without storing any samples. When it reaches a pre-speciﬁed performance threshold, we ﬁx the codebook, and store compressed held- out data from the ﬁrst task. We then continue to update the VQ-V AE parameters, and the memory is kept ﬁxed for the rest of the stream. We apply self-replay but no other AQM mechanisms (e.g. no sampling from the input stream and no adaptive compression). We monitor how well the VQ-V AE can adapt by looking at the streaming reconstruction cost, measured on the in- coming data before an update. We also monitor the drift of samples stored in the buffer. Results are presented in Figure 4. They demonstrate that drift is controlled by stabilizing the codebook, while the model can still improve at nearly the same rate. Further analysis, along with an additional ex- periment showcasing the robustness of vector quantization to small perturbations is included in the Appendix.Online Learned Continual Compression with Adaptive Quantization Modules Figure 5.Top: Sample decoded from the buffer at the end of train- ing from scratch (32x compression rate). Bottom: Original lidar LiDAR Range data enables autonomous vehicles to scan the topography of their surrounding, giving precise mea- surements of an obstacle’s relative location. In its raw form, range data can be very large, making it costly to transmit in real time, or for long term storage. Equipping self-driving cars with a good lidar compressor can enable fast vehicle- to-vehicle (V2V) communication, leading to safer driving (Eckelmann et al., 2017). Moreover, since data collected by autonomous vehicles can be highly non-stationary (new objects on the road, changing weather or trafﬁc conditions), having a compressor which can quickly adapt to this distri- bution change will reduce the required memory for storage (or bandwidth for real time transmission). We proceed to train AQM on the Kitti Dataset (Geiger et al., 2013), which contains 61 LiDAR scan recordings, each belonging to either the “residential”, “road”, “city” envi- ronments. The data is processed as in (Caccia et al., 2018), where points from the same elevation angle are sorted in increasing order of azimuth angle along the same row. This yield a 2D grid, making it compatible with the same architec- ture used in the previous experiments. As in (Caccia et al., 2018; Tu et al., 2019), we report the reconstruction cost in Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE) which allows to compare two point clouds. Note AQM can also be adapted to use task relevant criteria besides MSE. We consider two settings. In the ﬁrst, we train AQM from scratch on a data stream consisting of recordings from all three environments. We present (once) all the recordings of an environment before moving on to another, in order to maximise the distribution shift. We show qualitative results in Figure 5 and in the supplementary materials. Ob- serve that we are able to effectively reconstruct the LiDAR samples and can easily tradeoff quality with compression. Overall we obtain 18.8 cm SNNRMSE with 32×compres- Size in Mb Raw 1326.8 Gzip 823.0 AQM 35.5 ±.06 AQM + ﬁnetune 33.0 ±.07 AQM + ﬁnetune + PNG 27.9 ±.01 Table 3.Compression results for the data transmission of the city lidar recordings. We require that each compressed scan has an SNNRMSE under 15 cm. sion, which lies in a range that has been shown in (Tu et al., 2019) to be sufﬁcient to enable SLAM localization with very minimal error. In the second setting, we wish to simulate a scenario where some data is available a priori for the model to leverage. However, this data is limited and does not cover all the possible modalities to which an autonomous vehicle could be exposed. To this end, we pretrain AQM in a fully ofﬂine iid manner on the road and residential recordings. We then simulate the deployment of the compressor on a vehicle, where it must compress and transmit in real time the lidar data feed from a new distribution. We therefore stream the held-out city recordings and show that AQM can be ﬁne-tuned on the ﬂy to reduce the required bandwidth for data transmission. Quantitative results are presented in table 3. We ensure that the reconstructed lidar scans have a SNNRMSE smaller than 15.0 cm. Moreover, since the stored representations in AQM are 2D and discrete, we can apply lossless compression schemes such as Portable Network Graphics (PNG). 4.3. Atari RL Environments Another application of online continual compression is for preserving the states of an reinforcement learning agent op- erating online. These agents may often learn new tasks or enter new rooms thus the observations will often be highly non-iid. Furthermore many existing reinforcement learning algorithms already rely on potentially large replay buffers which can be prohibitive (Mnih et al., 2014; Rolnick et al., 2018) to run and may greatly beneﬁt from an approach such as the AQM to run concurrently with reinforcement learning algorithms. We thus perform a proof of concept for the AQM for storing the state sequence encountered by an RL learner in the atari environment(Bellemare et al., 2013). We use the dataset and tasks introduced in (Anand et al., 2019), which runs a random or learned policy in the atari environments and provides a set of classiﬁcation tasks to evaluate whether key information about the state is preserved. Results are shown Table 4. We run the online learning with the AQM on the data stream observed by the random agent. We use the same observations and optimiza- tion as in (Anand et al., 2019) and report the F1 resultsOnline Learned Continual Compression with Adaptive Quantization Modules Figure 6.Top: original. Bottom: reconstructed from AQM Game Cls Input F1 Pong Orig. State 86.7 AQM Recon 86.8 Ms Pacman Orig. State 89.4 AQM Recon 88.3 Pitfall Orig. State 68.2 AQM Recon 66.7 Table 4.Results on RL probing tasks from (Anand et al., 2019) with linear probe applied to original observation and to reconstruc- tions from AQM after online compression. Acc is averaged for each game over game speciﬁc prediction. of a linear probe directly on states for our reconstructions after online compression and the originals. Results for 3 environments are shown in Table 4 and examples in in Fig 6 and the Appendix. We ﬁnd that AQM can well preserve the critical information while compressing the state by 16x. The reference accuracies achieved by our classiﬁer are sim- ilar to those in (Anand et al., 2019). However, we do not control for the representation size unlike those evaluations of various unsupervised models. 5. Conclusion We have introduced online continual compression. We demonstrated vector quantization can be used to control drift and how to create mechanisms that allow maintain- ing quality and maximizing memory usage. These allowed learning compression while compressing. We have shown effectiveness of this online compression approach on stan- dard continual classiﬁcation benchmarks, as well as for compressing larger images, lidar, and atari data. We believe future work can consider dealing with temporal correlations for video and reinforcement learning tasks, as well as im- proved prioritization of samples for storage. References Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Online continual learning with no task boundaries. In arXiv, 2018. Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Charlin, L., and Tuytelaars, T. Online continual learning with maximally interfered retrieval. In Advances in Neural Information Processing (NeurIPS), 2019. Anand, A., Racah, E., Ozair, S., Bengio, Y ., C ˆot´e, M.- A., and Hjelm, R. D. Unsupervised state representation learning in atari. arXiv preprint arXiv:1906.08226, 2019. Andrychowicz, M., Denil, M., G´omez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N. Learning to learn by gradient descent by gradient descent. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Sys- tems 29, pp. 3981–3989. Curran Associates, Inc., 2016. Ball´e, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2016. Belilovsky, E., Eickenberg, M., and Oyallon, E. De- coupled greedy learning of cnns. arXiv preprint arXiv:1901.08164, 2019. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Caccia, L., van Hoof, H., Courville, A., and Pineau, J. Deep generative modeling of lidar data. arXiv preprint arXiv:1812.01180, 2018. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. In ICLR 2019. Chaudhry, A., Dokania, P. K., Ajanthan, T., and Torr, P. H. Riemannian walk for incremental learning: Un- derstanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H., and Ranzato, M. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. Eckelmann, S., Trautmann, T., Ußler, H., Reichelt, B., and Michler, O. V2v-communication, lidar system and posi- tioning sensors for future fusion algorithms in connected vehicles. Transportation research procedia, 27:69–76, 2017. Farquhar, S. and Gal, Y . Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733 , 2018.Online Learned Continual Compression with Adaptive Quantization Modules Fernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra, D. Pathnet: Evolu- tion channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, pp. 0278364913491297, 2013. Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J. Faster neural networks straight from jpeg. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31 , pp. 3933–3944. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7649-faster-neural-networks-straight-from-jpeg. pdf. Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., Zhao, D., and Yan, R. Overcoming catastrophic forgetting for continual learning via model adaptation. 2018. Husz´ar, F. On quadratic penalties in elastic weight consoli- dation. arXiv preprint arXiv:1712.03847, 2017. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Jin Hwang, S., Shor, J., and Toderici, G. Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4385–4393, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. Lesort, T., Caselles-Dupr´e, H., Garcia-Ortiz, M., Stoian, A., and Filliat, D. Generative models from the perspective of continual learning. arXiv preprint arXiv:1812.09111, 2018. Lesort, T., Gepperth, A., Stoian, A., and Filliat, D. Marginal replay vs conditional replay for continual learning. In International Conference on Artiﬁcial Neural Networks, pp. 466–480. Springer, 2019. Li, Z. and Hoiem, D. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 40(12):2935–2947, 2018. Lin, L.-J. Reinforcement learning for robots using neu- ral networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. Lopez-Paz, D. et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 2014. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. arXiv preprint arXiv:1901.06656, 2019. Oyallon, E., Belilovsky, E., Zagoruyko, S., and Valko, M. Compressing the input for cnns with the ﬁrst-order scatter- ing transform. In The European Conference on Computer Vision (ECCV), September 2018. Ramapuram, J., Gregorova, M., and Kalousis, A. Lifelong generative modeling. arXiv preprint arXiv:1705.09847, 2017. Razavi, A., Oord, A. v. d., and Vinyals, O. Generating diverse high-ﬁdelity images with vq-vae-2.arXiv preprint arXiv:1906.00446, 2019. Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. Riemer, M., Franceschini, M., and Klinger, T. Genera- tion and consolidation of recollections for efﬁcient deep lifelong learning. CoRR, abs/1711.06761, 2017. URL http://arxiv.org/abs/1711.06761. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and Wayne, G. Experience replay for continual learning, 2018. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had- sell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.Online Learned Continual Compression with Adaptive Quantization Modules Shin, H., Lee, J. K., Kim, J., and Kim, J. Continual learn- ing with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017. Theis, L., Shi, W., Cunningham, A., and Husz ´ar, F. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017. Thrun, S. and Mitchell, T. M. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25–46, 1995. Torfason, R., Mentzer, F., ´Ag´ustsson, E., Tschannen, M., Timofte, R., and Gool, L. V . Towards image under- standing from deep compression without decoding. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkXWCMbRW. Tu, C., Takeuchi, E., Carballo, A., and Takeda, K. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 Interna- tional Conference on Robotics and Automation (ICRA), pp. 3274–3280. IEEE, 2019. van den Oord, A., Vinyals, O., et al. Neural discrete repre- sentation learning. In Advances in Neural Information Processing Systems, pp. 6306–6315, 2017. Zenke, F., Poole, B., and Ganguli, S. Continual learn- ing through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017. Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G. Lifelong gan: Continual learning for conditional im- age generation. ArXiv, abs/1907.10107, 2019.Online Learned Continual Compression with Adaptive Quantization Modules A. Buffer Memory Management We consider two settings to manage the samples inside the memory. In the ﬁrst setting, we do not perform any codebook freezing. Therefore, as the model trains, the amount of compressed representations AQM can store increases smoothly. In this setting, the current amount of samples stored by AQM is a good approximation of the model’s capacity. Therefore, we simply use this estimate instead of the total buffer size in the regular reservoir sampling scheme. The algorithm is presented in Alg 4. Algorithm 4: AddToMemory Input: Memory Mwith capacity C(bytes), sample x 1 Nreg = C BY TES(x) 2 capacity = max( Nreg, NUM SAMPLES (M) ) 3 %Probability of adding x 4 add ∼B( capacity SAMPLE AMT SEEN SO FAR ) %Bernoulli 5 if add then 6 hidx, blockid = ADAPTIVE COMPRESS (x, AE, dth) 7 while BITS(hidx) - FREE SPACE(M) >0 do 8 DELETE RANDOM (M) 9 end 10 end This is not the case when we perform codebook freezing. In the latter setting, consider the moment when the ﬁrst codebook is ﬁxed; suddenly, the amount of samples the model can store has increased by a factor equal to the compression rate of the ﬁrst block. Therefore, at this moment, the amount of samples currently stored by AQM is not a good approximation for the model’s capacity. Moreover, when performing codebook freezing, since the capacity suddenly spikes, we must decide between a) having an imbalance in the buffer, where certain temporal regions of the streams are not equally represented, or not utilising all available memory and storing less incoming samples so they are in similar quantities as previous samples. We opt for the former approach, and propose a procedure that allows the buffer to rebalance itself as new training data becomes available. We illustrate the procedure with an example. Consider an AQM where the distribution of samples in the buffer is the one plotted in Fig 10. Speciﬁcally, we show the number of samples stored for each minibatch processed by the model. In this example, very few (<20) samples are stored from the earliest part of the stream, while a much larger number comes from the more recent part of the stream. Assuming that the model is over its memory capacity, we need to remove samples until the memory requirement is met. Ideally, we would like to remove more samples from parts of the stream where samples are abundant. In order to do so, we use Kernel Density Estimation on the histogram in Fig 10. Doing so gives us the line labelled iter0 in Fig 10. We then sample points according to the distribution given by iter0, remove them, and ﬁt a new KDE with the remaining points (labelled iter1). In this example we repeat this procedure 10 times, until iter9. As we can see, the distribution of stored samples becomes closer to the uniform distribution, i.e. the setting where all parts of the streeam are equally represented. Therefore, in the setting where codebook freezing is performed, we ﬁrst add all incoming points to the buffer. Then, points are removed according to the procedure described above. This allows for maximal memory usage while ensuring that the buffer self balances over time. Note that we need to store the timestamp alongside each sample, which has a negligible cost when dealing with high-dimensional inputs. B. Further Details of Experiments We include here further details regarding the models used in our experiments. For all reported results, almost all hyperpa- rameters are kept the same. we set D the size of the embedding table equal to 100, we use a decay value of 0.6 for the Embedding EMA update, and the same architectural blocks. Across problems, we mainly vary the reconstruction threshold, as well how the blocks are stacked and their compression rates (by e.g. changing the number of codebooks per block).Online Learned Continual Compression with Adaptive Quantization Modules Figure 7.(left) histogram of samples in AQM where no buffer balancing was performed. (right) iterative buffer balancing procedure B.1. Cifar For CIFAR-10, we use a 1 block AQM, latent size (16 x 16 x 100) is quantized with (16 x 16 x 1) indices where the last index represents the number of codebooks. The codebook here contains 128 embeddings, giving a compression factor of 13.7×. Due to the simplistic nature of the task and low resolution of the images, AQM already yields good compression before the end of the ﬁrst task, hence adaptive compression and codebook freezing are not required. For the (Riemer et al., 2018) baseline, we ran a hyperparameter search to vary the compression size. Speciﬁcally, we ran a grid search for the number of categories per latent variable, as well as for the number of latent variables. We found the gumbel softmax much less stable during training and harder to cross-validate than vector quantization. Below we show an example of the image quality of our approach compared to (Riemer et al., 2018). We ran both AQM and (Riemer et al., 2018) on the split CIFAR-10 task, then extracted images which happened to be in the buffer of both methods. Figure 8.Bottom row: random buffer reconstructions using (Riemer et al., 2018). Middle row: random buffer reconstructions using SQM. Top row: corresponding original image. Columns are ordered w.r.t their entry time in the buffer, from oldest to newest. All samples above were obtained from the disjoint CIFAR-10 task, and are 12×smaller than their original image. B.2. Imagenet For the Ofﬂine 128 x 128 Imagenet experiment, we use the following three blocks to build AQM, with the following latent sizes : 1. (64 x 64 x 100) quantized using (64 x 64 x 1) indices, with a codebook of 16 vectors, giving a 24 ×compression. 2. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 256 vectors, giving a 48 ×compression. 3. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 32 vectors, giving a 76.8 ×compression. For the 2 block AQM, we searched over using blocks (1-2) (2-3) and (1-3). For 1 block AQM we simply tried all three blocks independently. When stacking two blocks with the same latent sizes (e.g. block 2 and 3) the encoder and decoder functions for the second block are the identity. In other words, the second block simply learns another embedding matrix.Online Learned Continual Compression with Adaptive Quantization Modules C. Drift Ablation Here we provide additional results for the drift ablation studied in 4. We repeat the same experiment for different values of the reconstruction threshold parameter, which controls when the codebook freezing occurs. The table below shows that the same conclusion holds across multiple values for this parameter: codebook freezing yields little to no drift, while only negligibly hindering the model’s ability to adapt to a distribution shift. It is worth noting that in cases of severe drift (e.g. with recon th = 7.5) the model diverges because it is rehearsing on samples of poor quality. In this setting, codebook freezing performs better on both the streaming MSE and the drift MSE. recon th Freezing Streaming MSE Drift MSE Streaming + Drift MSE 1 No 0.59 ±0.03 1.01 ±0.14 1.60 ±0.17 1 Yes 0.60 ±0.03 0.49 ±0.02 1.09±0.05 2.5 No 0.63 ±0.06 13.24 ±5.36 13.87 ±5.42 2.5 Yes 0.81 ±0.02 1.07 ±0.05 1.88±1.12 5 No 0.65 ±0.04 55.31 ±36.82 55.96 ±36.86 5 Yes 0.92 ±0.03 1.69 ±0.18 2.61±0.21 7.5 nan nan nan nan 7.5 Yes 0.98 ±0.11 2.10 ±0.28 3.08±0.39 Table 5.Here we provide additional results for the drift ablation discussion in section shown in Fig 4. For clarity all results are multiplied by 100. (e.g. recon th of 2.5 corresponds to 0.025). Results averaged over 5 runs. Figure 9.Visualization of results reported in Table C. We kept the scale of the y axis consistent across the four graphs for easy comparison.Online Learned Continual Compression with Adaptive Quantization Modules D. Atari Below are sample reconstructions used in the Atari experiments. For each game, reconstructions are in the ﬁrst row and the original uncompressed samples in the second row. Reconstructions are 16×smaller than the original RGB images. Figure 10.Pong Figure 11.Pitfall Figure 12.Ms PacmanOnline Learned Continual Compression with Adaptive Quantization Modules E. Lidar Samples Here we show reveral lidar compressions (left) and their original counterpart (right). The compression rate is 32 ×. We note that unlike RGB images, raw lidar scans are stored using ﬂoating points. We calculate the compression rate from the (smaller) polar projection instead of the 3 channel cartesian representation. Figure 13.Lidar reconstruction (left) vs original (right)Online Learned Continual Compression with Adaptive Quantization Modules Figure 14.Lidar reconstruction (left) vs original (right)",
      "meta_data": {
        "arxiv_id": "1911.08019v3",
        "authors": [
          "Lucas Caccia",
          "Eugene Belilovsky",
          "Massimo Caccia",
          "Joelle Pineau"
        ],
        "published_date": "2019-11-19T00:43:16Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08019v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces and studies the problem of Online Continual Compression (OCC), where a model learns to compress and store a representative dataset from a non-i.i.d data stream, observing each sample only once, while addressing challenges like representation drift and catastrophic forgetting. The key contribution is the proposal of Adaptive Quantization Modules (AQM), which leverage Vector Quantized Variational Autoencoders (VQ-VAEs) with codebooks to tackle representation drift effectively. AQM incorporates an architecture with multiple VQ-VAEs, an adaptive compression scheme, a stream sampling mechanism, and a self-replay mechanism. The authors demonstrate that AQM achieves state-of-the-art performance in standard online continual image classification benchmarks and is applicable to larger images, LiDAR data, and reinforcement learning agents.",
        "methodology": "The core methodology revolves around Adaptive Quantization Modules (AQM) built upon the VQ-VAE framework. VQ-VAE uses a vector quantization step with an embedding table to obtain discrete latent representations, where the embedding tables are updated independently from the encoder and decoder to control representation drift. AQM's architecture consists of a sequence of VQ-VAEs, each with a buffer, where subsequent modules produce more compressed representations. Training is greedy and gradient-isolated between modules to minimize interference and accelerate convergence. An adaptive multi-level storage mechanism (Algorithm 2) stores samples at different compression levels based on reconstruction quality and available memory, even allowing uncompressed storage if compression is ineffective. A self-replay mechanism (Algorithm 1) uses randomly sampled data from storage to update the AQM modules, reducing forgetting and potentially freeing memory by allowing samples to be compressed at later modules. A custom stream sampling method (Algorithm 4 and Appendix A) manages fixed memory capacity by non-uniformly selecting samples for deletion and rebalancing the buffer. Drift control is achieved through codebook stabilization using an exponential moving average and freezing the embedding matrix of a module once satisfactory compression is reached, while allowing encoder and decoder parameters to continue adapting.",
        "experimental_setup": "The effectiveness of AQM is evaluated across several tasks and datasets. For online continual classification, experiments are conducted on CIFAR-10 (disjoint, 5 tasks with 2 new classes each) and Incremental CIFAR-100 (multi-head setup), using accuracy and forgetting as metrics. Baselines include iid online/offline, fine-tuning, ER (Experience Replay), ER-MIR, ER-JPEG, GEM, iCarl, and Gumbel AE. For offline evaluation on larger images, Imagenet (128x128) is used, where an offline classification model is trained on reconstructions from AQM storage, with a detailed ablation study comparing components against a standard Reservoir Sampling approach. For LiDAR data, the Kitti Dataset (61 recordings from 'residential', 'road', 'city' environments) is processed into a 2D grid, and performance is measured using Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE). Finally, for Atari RL environments (Pong, Ms Pacman, Pitfall), AQM is used to store agent observations from the dataset in Anand et al. (2019), and a linear probe's F1 score on original vs. reconstructed states is used to evaluate information preservation.",
        "limitations": "The current work does not explicitly address temporal correlations, which would be beneficial for video and reinforcement learning tasks. While the paper suggests future improvements for prioritization of samples for storage, this also indicates a present limitation. The authors note that for datasets like CIFAR-10, with low resolution and abundant data per task, the problem might be less challenging, and a single AQM module can be sufficient, implying the full complexity of AQM is more critical for harder, high-dimensional datasets. Additionally, when comparing against baselines, the Gumbel softmax approach was found to be less stable and harder to cross-validate than vector quantization, and for Atari RL, representation size was not controlled for, which might slightly affect direct comparisons with other unsupervised models.",
        "future_research_directions": "Future research directions include dealing with temporal correlations for video and reinforcement learning tasks. Another suggested area for improvement is the prioritization of samples for storage to optimize memory usage and retention further."
      }
    },
    {
      "title": "Vector Quantization Prompting for Continual Learning",
      "abstract": "Continual learning requires to overcome catastrophic forgetting when training\na single model on a sequence of tasks. Recent top-performing approaches are\nprompt-based methods that utilize a set of learnable parameters (i.e., prompts)\nto encode task knowledge, from which appropriate ones are selected to guide the\nfixed pre-trained model in generating features tailored to a certain task.\nHowever, existing methods rely on predicting prompt identities for prompt\nselection, where the identity prediction process cannot be optimized with task\nloss. This limitation leads to sub-optimal prompt selection and inadequate\nadaptation of pre-trained features for a specific task. Previous efforts have\ntried to address this by directly generating prompts from input queries instead\nof selecting from a set of candidates. However, these prompts are continuous,\nwhich lack sufficient abstraction for task knowledge representation, making\nthem less effective for continual learning. To address these challenges, we\npropose VQ-Prompt, a prompt-based continual learning method that incorporates\nVector Quantization (VQ) into end-to-end training of a set of discrete prompts.\nIn this way, VQ-Prompt can optimize the prompt selection process with task loss\nand meanwhile achieve effective abstraction of task knowledge for continual\nlearning. Extensive experiments show that VQ-Prompt outperforms\nstate-of-the-art continual learning methods across a variety of benchmarks\nunder the challenging class-incremental setting. The code is available at\n\\href{https://github.com/jiaolifengmi/VQ-Prompt}{this https URL}.",
      "full_text": "Vector Quantization Prompting for Continual Learning Li Jiao1, Qiuxia Lai 1∗, Yu Li 2, Qiang Xu 3 1 Communication University of China 2 Harbin Institute of Technology, Shenzhen 3 The Chinese University of Hong Kong {jl0930,qxlai}@cuc.edu.cn;li.yu@hit.edu.cn;qxu@cse.cuhk.edu.hk Abstract Continual learning requires to overcome catastrophic forgetting when training a single model on a sequence of tasks. Recent top-performing approaches are prompt- based methods that utilize a set of learnable parameters (i.e., prompts) to encode task knowledge, from which appropriate ones are selected to guide the fixed pre- trained model in generating features tailored to a certain task. However, existing methods rely on predicting prompt identities for prompt selection, where the identity prediction process cannot be optimized with task loss. This limitation leads to sub-optimal prompt selection and inadequate adaptation of pre-trained features for a specific task. Previous efforts have tried to address this by directly generating prompts from input queries instead of selecting from a set of candidates. However, these prompts are continuous, which lack sufficient abstraction for task knowledge representation, making them less effective for continual learning. To address these challenges, we propose VQ-Prompt, a prompt-based continual learning method that incorporates Vector Quantization (VQ) into end-to-end training of a set of discrete prompts. In this way, VQ-Prompt can optimize the prompt selection process with task loss and meanwhile achieve effective abstraction of task knowledge for continual learning. Extensive experiments show that VQ- Prompt outperforms state-of-the-art continual learning methods across a variety of benchmarks under the challenging class-incremental setting. The code is available at https://github.com/jiaolifengmi/VQ-Prompt. 1 Introduction Humans have the remarkable capability to continually acquire and integrate knowledge of new concepts or categories without forgetting old ones, whereas deep learning models struggle with catastrophic forgetting[40] when tasked with learning a sequence of classes [42, 10, 39]. Continual learning aims at addressing catastrophic forgetting in deep neural networks (DNNs) by striking a balance between plasticity for learning new incoming data effectively and stability to retain prior knowledge. Approaches in this field vary: some methods dynamically expand network architec- tures [64, 29, 58] or reconfigure their internal structures [48, 17, 24] for new tasks. Others penalize the update of crucial parameters from previous tasks [20, 27, 65, 1, 49] or alter parameter update rules to prevent interference across tasks [34, 7, 47, 23]. Additionally, certain methods interleave stored past data with current ones for training [18, 8, 44, 45, 4, 6, 36]. Despite recent advances, continual learning remains an open challenge for DNNs. Recently, prompt-based continual learning has emerged as a promising solution to mitigate catas- trophic forgetting in sequential task learning. This approach enhances a pre-trained Vision Trans- ∗Corresponding author 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2410.20444v2  [cs.LG]  20 Jul 2025Diffrentiable?Discrete Prompt?  Diffrentiable?Discrete Prompt?  Diffrentiable?Discrete Prompt? (a) (b) (c) Query Prompt  Query Module Input-dependent Prompt Query Module Vector Quantization  Input-dependent Prompt  Gradient  Gradient  Select  Gradient Estimation  Module Id  Gradient  ... Prompt Pool ... Prompt Pool ������ Figure 1: Concept comparison. (a) Prior prompt-based continual learning methods predict prompt identities for prompt selection, which cannot be optimized end-to-end with task loss. (b) Some methods enable end-to-end training by directly generating prompts from the queries using learnable parameters. However, these prompts are continuous, lacking the necessary abstraction to effectively represent the task knowledge essential for generating features tailored to a certain task. (c) Our method incorporates Vector Quantization (VQ) into the prompt generation pipeline to enable end-to- end training of discrete prompts with task loss. See §1 for details. former (ViT) [11] with a small set of learnable parameters, known as “prompts”. These prompts encapsulate task-specific knowledge, shifting the learning focus from the entire model to the prompts themselves, which guide the pre-trained model in generating task-relevant outputs. During inference, the most suitable prompt containing necessary task knowledge is selected from the prompt pool based on the input image to direct the behavior of the frozen pre-trained model. Current prompt-based methods either involve a key-query matching mechanism to select prompts based on the similarity between the image features and the key parameters paired with prompts [62, 61], or explicitly predict the prompt indices and perform the selection accordingly [60, 59]. However, the non-differentiable nature of indexing impedes the prompt selection from being optimized end-to- end with task loss. This limitation can lead to diminished performance, as inaccurate prompt selection may fail to tailor the pre-trained features for the specific task. Efforts to address this issue include implementing a differentiable prompt selection, such as generating prompts as a weighted sum from the prompt pool [50], or deriving prompts from the intermediate features of the input image [51, 26]. However, the resulting prompts are continuous, which lack the necessary abstraction to effectively represent the task knowledge essential for guiding the pre-trained model to generate features tailored to a certain task. A concept comparison is shown in Fig. 1. The assumption that discrete prompts better represent task knowledge than continuous prompts for continual learning can be supported by both theoretical insights from cognitive science and empirical evidence. Discrete prompts mimic the organizational structure of memory and knowledge in the human brain, which is typically understood to consist of discrete units such as concepts and facts [19]. This clear separation of information helps prevent interference among different knowledge domains, and enables models to provide distinct guidance for feature extraction specific to each task. Such knowledge abstraction aligns with categorical perception in human cognition, where sensory inputs are perceived as distinct categories (e.g., colors, phonemes) rather than continuous spectrum [41]. Furthermore, empirical comparisons in §5.2 demonstrate the effectiveness of discrete prompts when optimized end-to-end with task loss (e.g., VQ-Prompt V .S. CODA-P or EvoPrompt). In summary, discrete prompts hold significant promise for improving the continual learning capabilities of models, bringing them more in line with human learning. Optimizing prompts with task loss while preserving their discrete properties as representations of concepts poses a non-trivial challenge. In this paper, we introduce Vector Quantization Prompting (VQ-Prompt) for continual learning, which can optimize prompts using task loss while preserving their discrete characteristics as concept representations. This method involves initially generating a continuous prompt and then replacing it with its nearest match from a predefined prompt pool. To address the non-differentiability inherent in prompt quantization, we apply gradient estimation to propagate task loss to the continuous prompt, while additional vector quantization regularization terms further refine the learning of the prompt pool. To further stabilize task knowledge learning, we use representation statistics to mitigate the classification bias towards previous tasks, thereby enhancing continual learning performance. Our contributions are three-folds: (1) We propose VQ-Prompt, an end-to-end learnable discrete prompting mechanism for continual learning, addressing a critical yet overlooked aspect in the current literature. (2) We leverage gradient estimation to pass the task loss to prompt-related parameters while regularizing the learning of the prompts with vector quantization terms, which facilitates the 2end-to-end training of the discrete prompt pool. (3) We incorporate representation statistics during training to further stabilize task knowledge learning and improve the overall continual learning performance. Extensive experiments show that VQ-Prompt consistently outperforms state-of-the-art continual learning methods on a variety of benchmarks. 2 Related work Continual Learning refers to the process where multiple tasks are learned sequentially without forgetting [42, 10, 39]. Generally, continual learning has three scenarios [ 54]. Task-incremental learning (TIL) learns different classes for each task and assumes having task identities available at test time. Domain-incremental learning (DIL) maintains the same set of classes for different tasks while changing the data distributions across tasks, and task identities are not provided for inference. For Class-incremental learning (CIL), each task involves new classes and all the learned classes are to be classified without task identities available during inference. In this paper, we focus on the more representative and challenging CIL scenario. Numerous efforts have been devoted to alleviating catastrophic forgetting. Architecture-based methodsaddress this by either dynamically expanding network architectures [64, 29, 58] or modifying internal network structures [48, 17, 24] for new tasks. Regularization-based methodsfocus on limiting updates to vital parameters from earlier tasks [ 20, 27, 65, 1, 49], or modifying the rules for parameter updates to reduce task interference [34, 7, 47, 23]. Rehearsal-based methodsincorporate previous data with current data during training to mitigate forgetting [ 18, 8, 44, 45, 4, 6, 36]. Despite recent advances, continual learning remains a challenging and evolving field. Prompt-based Continual Learning Methods.Recently, there has been a surge in methods leveraging prompting techniques from natural language processing (NLP) [ 28, 30] for continual learning. These methods instruct a frozen pre-trained transformer using learnable prompts that encode task knowledge. During training, prompt selection is either through key-query similarity matching [62] or indicated by task identity [61, 12, 60, 59]. In inference, the appropriate prompt is chosen through similarity matching with key or feature centroids. However, both kinds of prompt selection are non-differentiable, making it challenging to optimize them end-to-end with the task loss, particularly when the gap between the pre-training task and unknown future tasks is large. To address this, CODA-Prompt [50] adopts a soft prompt selection, i.e., generating prompts as a weighted sum from the prompt pool. APG [ 51] and EvoPrompt [26] learn to derive prompts from intermediate image features. Nevertheless, all three methods generate prompts that are continuous, which lack the necessary abstraction to effectively represent the task knowledge essential for instructing the pre-trained model to produce features tailored to a certain task. In this paper, we present a new prompting framework for continual learning capable of optimizing prompts with task loss while preserving their discrete properties as the representation of task knowledge. Vector Quantization in Representation Learning. Vector Quantization (VQ) is a technique used in signal processing and data compression to represent a set of vectors (data points) with a smaller set of “coding vectors” (CVs). Unsupervised VQ algorithms such as Self-organizing Maps (SOMs) [22] and Neural Gas (NG) networks [38] attempt to obtain a set of CVs that optimally represent the data. Supervised VQ algorithms such as Learning Vector Quantization (LVQ) [21] focus on reducing the misclassification rates by refining decision boundaries between classes. In generative modeling, VQ has been used to learn structured discrete latent spaces in VQ-V AE [55] and VQ-GAN [13] to achieve higher fidelity images. Recently studies have explored combining VQ with continual learning to constrain the feature space, aiming to enhance class separation and retrain prior knowledge across increments [52, 53, 9, 37]. In this paper, instead of utilizing VQ to confine the feature space, we employ VQ to enable end-to-end learning a set of discrete prompts that effectively encode task knowledge in a learning system that evolves over time. 3 Preliminary Problem Formulation. In class-incremental learning (CIL), a model is required to sequentially learn a series of tasks with disjoint class sets, and to accurately classify all seen classes during evaluation. Formally, let Dt = {(xt i, yt i)}Nt i=1 denote the training set of the t-th task, where xt i ∈Xt is an input image, yt i ∈Yt is the target label, and Nt is the number of samples. The label spaces of all the tasks are mutually exclusive, i.e., ∩T t=1Yt = ∅, where T is the total number of tasks. Consider a deep 3learning model M = ϕ ◦ f with a backbone f(·) and a classifier ϕ(·). During training on task t, the model only has access to Dt, which raises a risk of forgetting old tasks. After learning task t, the model is expected to perform well on all classes in Y1:t = ∪t k=1Yk, and further on Y1:T after completing training on all T tasks. Prompt-based Learning is an emerging approach in NLP [ 32] that involves incorporating extra instructions into pre-trained models to guide their performance on specific tasks. Rather than relying on extensive retraining or task-specific fine-tuning, this technique leverages prompts to shape the behavior of pre-trained models, providing adaptable instructions that helps them handle a wide range of downstream tasks more effectively. In vision-related continual learning, prompting is typically employed with Vision Transformer (ViT) [11]. ViT consists of a sequence of multi-head self-attention (MSA) blocks [56]. For clarity, we take one MSA block as an example to illustrate the prompting. We denote the input query, key, and value of the MSA block as hQ, hK and hV , respectively. Here, h∗ ∈RL×D, L is the sequence length, and D is the embedding dimension. The output of the MSA is computed as: MSA(hQ, hK, hV ) =Concat(h1, . . . ,hM )WO, hm = Attention(hQWQ m, hKWK m , hV WV m ), (1) where WO, WQ m, WK m and WV m are projection matrices, m=1, ··· , Mis the head index, M is the number of heads, and hQ =hK =hV =h for SA. Previous prompt-based continual learning methods mainly implement Prompt Tuning (Pro-T) [28] and Prefix Tuning (Pre-T) [30]. Pro-T prepends the same prompt p∈RLp×D to hQ, hK, and hV . The prompting function of Pro-T is defined as: fPro-T(p, h) =MSA([p; hQ], [p; hK], [p; hV ]), (2) where [·; ·] means concatenating along the sequence length dimension. The output dimension is (Lp+L)×D. Pre-T splits p along the sequence length dimension into pK, pV ∈RLp/2×D, which are prepended to hK and hV , respectively: fPre-T(p, h) =MSA(hQ, [pK; hK], [pV ; hV ]). (3) The output dimension is the same as that of h. In continual learning, the pre-trained ViT backbone is kept frozen as a general feature extractor, and the prompt parameters p are trained to capture task knowledge. Proper prompts corresponding to the input samples are selected to guide the feature extraction during inference. Following [61, 50, 59], we adopt Pre-T strategy in our method. 4 Method As shown in Fig. 2, our VQ-Prompt approach begins by constructing a continuous prompt through a soft selection from the prompt pool (§4.1). The continuous prompt is then quantized to an element in the prompt pool, which is inserted into an MSA block of a frozen pre-trained transformer. This process is made end-to-end trainable through gradient estimation and vector quantization regularization (§4.2), such that the prompting parameters, namely the keys and the prompt pool could all be optimized using the task loss. In this way, VQ-Prompt can yield a discrete prompt for each input while maintaining end-to-end optimization. To better stabilize task knowledge learning, representation statistics of previously learned classes are employed to mitigate the classification bias (§4.3). 4.1 Prompt Formation Most previous prompting-based continual learning approaches construct their prompts by selecting from the prompt pool based on key-query similarity [62, 61] or other task identity prediction mecha- nisms [60, 59], making the prompt selection process non-differentiable. In our prompt formation, we first generate a continuous prompt by aggregating all the elements in the prompt pool based on the similarity scores between the query and the keys. Specifically, given a query q from the input image, the similarity score is calculated as: α = Softmax(Kq), (4) 4Input  Image Query Function Cosine Similarity Weighted Sum  Gradient Estimation   NN Look-up  ... Prompt Keys K ... Prompt Pool P Pre-trained Transformer Classifier ...  Insert Prompts  Statistics Figure 2: VQ-Prompt framework. An input image is passed through a query function (e.g., a fixed pre-trained ViT) to generate a query q, which is then used to compute similarity scores with prompt keys K. These scores α serve as weights to aggregate elements from the prompt pool P to form a continuous prompt p′. This prompt is subsequently quantized to an element within the prompt pool p, and then fed into a specific MSA block of a frozen pre-trained transformer. To ensure differentiability, the prompt quantization process employs gradient estimation and prompt pool regularization. The representation statistics of features from learned classes are used to stabilize task knowledge learning. More details are shown in §4. where K ∈RN×D is the prompt key matrix, q∈RD is the query, N is the number of keys, and D is the embedding dimension. Then, the continuous prompt is obtained by: p′ = X i αiPi, i = 1, ··· , N, (5) where Pi ∈RLp×D is the i-th element in the prompt pool, and Lp is the length of the prompt. Such a prompt formation process is differentiable and can be viewed as a simplified version of CODA-P [50]. Here, we do not learn an extra attention parameter for weighting the query, nor do we increase the number of elements in the prompt pool or the number of keys during sequential task learning. 4.2 Vector Quantization Prompting (VQ-Prompt) Nearest-neighbour Look-up. The continuous prompt p′ obtained in Eq. (5) is conditioned on a specific instance, i.e., it varies with the input images, making it insufficiently abstract to capture task knowledge effectively. We further perform prompt quantization by performing the nearest neighbour (NN) look-up in the prompt pool P using p′. The quantized prompt to be fed to the MSA block is obtained by: p = Pk, k= arg min j ∥p′ − Pj∥2, p∈RLp×D. (6) Such a prompt selection pipeline can be viewed as a specific non-linearity that maps the continuous prompt to 1-of-N elements in the prompt pool. Gradient Estimation. Because the arg minoperation in Eq. (6) is non-differentiable, we use the straight-through estimator [3] to approximate the gradient of p′ using the gradient of p. Despite its simplicity, this estimator has demonstrated its effectiveness in our experiments. Specifically, in the forward process, the quantized prompt p is passed to the MSA block in the pre-trained transformer. During the backward computation, the gradient ofp is transferred unaltered top′, and the optimization of prompt pool P and keys K guided by the similarity scores (c.f., Eq. (4)). This gradient estimation is justified, as p and p′ share the same Lp ×D-dimensional space, and the gradient of p provides valuable information on how prompt parameter learning could instruct the transformer features to minimize the cross-entropy (CE) loss during task learning. In this way, each prompt and key element is adjusted according to its relevance to the current learning context, rather than undergoing wholesale changes. This allows for more updates of task-relevant elements without disrupting less relevant ones, thereby maintaining previously acquired knowledge while adapting to new tasks. Vector Quantization (VQ) Regularization. Though the prompt pool P could receive gradients from the task loss through straight-through gradient estimation of mapping from p to p′, to enhance the learning of the prompt embedding space, we add an extra VQ objective. This VQ objective uses 5the L2 error to move the selected element p of prompt pool towards the continuous prompt p′: LVQ = ∥sg[p′] − p∥2 2. (7) Here, sg[·] stands for the stop-gradient operation [ 55], which constrains its operand to be a non- updated constant during training. To ensure that the learning processes of the prompt keys K and the continuous prompt p′ align closely with the characteristics of the element p from the prompt pool P, we further introduce a commitment regularization term. This term is defined mathematically as follows: LCommit = ∥p′ − sg[p]∥2 2. (8) The incorporation of this commitment loss ensures that the prompt formation process described in §4.1 is optimized to yield prompts to commit to the elements in the prompt poolP, thereby promoting consistency and stability in the prompt learning process. 4.3 Stabilizing Task Knowledge Learning with Representation Statistics Though prompts effectively capture task knowledge to guide the backbonef(·) in producing instructed representations, the classifier ϕ(·) may develop a bias towards new classes in continual learning scenarios [16, 2]. This bias can adversely affect the learning of the prompts for subsequent tasks. To mitigate this issue and stabilize task knowledge learning, we employ a strategy similar to [59], which leverages the representation statistics of previously learned classes to correct classifier bias and stabilize prompt learning. Specifically, after completing task t, we calculate the mean µc and variance σc for each class c ∈ Y1:t with the learned prompt parameters and the pre-trained backbone. By modeling each class as a Gaussian distribution, we generate pseudo features through sampling from these distributions. These pseudo features are then used to fine-tune the classifier, thereby mitigating its bias towards recent classes. The balanced classifier could help stabilize task knowledge learning in the prompts, alleviate catastrophic forgetting, and enhance overall continual learning performance. 4.4 Overall Optimization Objective The overall loss function is defined in Eq. (9), which extends the task lossLCE with two terms, namely a quantization objective LVQ weighted by λq, and a commitment term LCommit weighted by λc. L = LCE + λqLVQ + λcLCommit. (9) Here, LCE is the cross-entropy loss that supervises the learning of the image classification task, LVQ is the VQ regularization defined in Eq. (7) that updates the prompt pool elements to move towards the continuous prompt p′, and LCommit is the commitment term defined in Eq. (8) that forces the prompt formation process to commit to the prompt pool elements. During training, the pre-trained backbone f(·) is frozen, while the classifier ϕ(·) and prompting parameters K and P are optimized across all the tasks. During task knowledge stabilization, only the classifier ϕ(·) is actively trained. 5 Experiment 5.1 Experimental Setups Datasets. We consider three representative benchmarks for evaluating CIL.ImageNet-R [14] includes 200-class images that are either hard samples for ImageNet or newly collected data with different styles, thus can serve as a challenging benchmark for continual learning with pre-trained models. In the experiments, we divide it into 5, 10, and 20 disjoint tasks and report the corresponding performance. Split CIFAR-100randomly splits the original CIFAR-100 [25] into 10 disjoint tasks, each containing 10 classes. Split CUB-200is built on CUB-200-2011 [57], a fine-grained classification dataset, by randomly splitting the 200 classes into 10 tasks, where each task contains 20 classes. Baselines. We evaluate our approach against a comprehensive set of baselines to contextualize its performance. Following [ 50], we include “Joint Training” as the upper-bound performance, setting a benchmark for optimal results. To establish lower bounds, we employ two sequential learning baselines, denoted as “FT” and “FT++”, with the latter refraining from updating the logits of previously learned classes during the training of new tasks. We also consider 5 prompt-based approaches: L2P [62], DualPrompt [61], HiDe-Prompt [59], CODA- Prompt [50] and EvoPrompt [26], where the last two yield continuous prompts. For L2P, we include 6Table 1: Comparison on ImageNet-R. Results on “5-task”, “10-task”, and “20-task” settings are included. Backbones are pre-trained on ImageNet-1K. ↑ denotes larger values are better. See §5.2. Method Pub. 5-task 10-task 20-task FAA (↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) Joint-Train. 82.06 82.06 82.06 FT 18.74 ±0.44 48.39±0.58 10.12±0.51 35.23±0.92 4.75±0.40 22.8±0.37 FT++ 60.42 ±0.87 71.59±0.50 48.93±1.15 66.79±0.92 35.98±1.38 59.68±0.95 L2P++ [62] CVPR22 70.83 ±0.58 78.34±0.47 69.29±0.73 78.30±0.69 65.89±1.30 77.15±0.65 Deep L2P++ [62] CVPR22 73.93±0.37 80.14±0.54 71.66±0.64 79.63±0.90 68.42±1.20 78.68±1.03 DualPrompt [61] ECCV22 73.05±0.50 79.47±0.40 71.32±0.62 78.94±0.72 67.87±1.39 77.42±0.80 CODA-P [50] CVPR23 76.51 ±0.38 82.04±0.54 75.45±0.56 81.59±0.82 72.37±1.19 79.88±1.06 HiDe-Prompt*[59] NeurIPS23 76.29±0.10 78.77±0.11 76.74±0.18 78.76±0.11 76.46±0.06 78.76±0.11 EvoPrompt [26] AAAI24 77.16±0.18 82.22±0.54 76.83±0.08 82.09±0.68 74.41±0.23 80.96±1.42 VQ-Prompt — 79.23±0.29 82.96±0.50 78.71±0.22 83.24±0.68 78.10±0.22 82.70±1.16 * denotes results obtained by running the official code with ImageNet-1K pre-trained weights. Table 2: Comparison on Split CIFAR-100. Backbones are pre-trained on ImageNet-1K. See §5.2 for details. Method Pub. 10-task FAA (↑) CAA ( ↑) Joint-Train. 91.38 FT 29.21 ±0.18 37.37±0.89 FT++ 49.91 ±0.42 74.76±0.93 LwF [31] TPAMI17 64.83 ±1.03 - L2P++ [62] CVPR22 82.50 ±1.10 88.96±0.82 Deep L2P++ [62] CVPR22 84.30±1.03 90.50±0.69 DualPrompt [61] ECCV22 66.00±0.57 77.92±0.50 CODA-P [50] CVPR23 70.03±0.47 74.26±0.24 EvoPrompt [26] AAAI24 87.97±0.30 92.26±0.86 VQ-Prompt — 88.73±0.2792.84±0.73 Table 3: Comparison on Split CUB-200. Back- bones are pre-trained on ImageNet-21K. ∗ de- notes backbone is not frozen. See §5.2. Method Pub. 10-task FAA (↑) CAA ( ↑) Joint-Train. 88.00 FT 11.04 ±0.78 31.96±0.74 FT++ 37.81 ±2.86 63.55±1.62 LwF [31] TPAMI17 69.75 ±1.37 80.45±2.08 BiC [63] CVPR19 81.91 ±2.59 89.92±1.57 DualPrompt [61] ECCV22 66.00±0.57 77.92±0.50 CODA-P [50] CVPR23 70.03±0.47 74.26±0.24 ∗SLCA [67] ICCV23 84.71 ±0.4090.94±0.68 HiDe-Prompt [59] NeurIPS23 86.61±0.18 87.01±0.03 VQ-Prompt — 86.72±0.9490.33±1.03 its two variations from [50]: “L2P++” and “Deep L2P++”. L2P++ uses Pre-T instead of Pro-T and inserts the prompts to the first MSA block, which achieves better performance than the original L2P [33]. Deep L2P++ extends L2P++ by incorporating prompts into the first five MSA blocks. In addition to prompt-based methods, we include a classical regularization-based method LwF [31], and a rehearsal-based method BiC [63], providing a more comprehensive overview for evaluation. Evaluation Metrics. We present Final Average Accuracy (FAA)and Cumulative Average Accuracy (CAA) for comparison. FAA refers to the last average accuracy after learning all the tasks, which is equivalent to “Last-Acc” in [67]. CAA is the average of historical FAA values after learning each task, which is equivalent to “Inc-Acc” in [67]. The formal definitions of the metrics are in §A.1. Implementation Details. We follow prior works [62, 61, 50, 59, 67, 26] and use ViT-Base [11] pre-trained with supervised learning on ImageNet-1K [46] or ImageNet-21K [43] as the backbone. The number of keys and prompt elements N is 10. The prompt length Lp is 8. The embedding dimension D=768 which is the same as the feature dimension of ViT-Base. Our method is trained using an AdamW optimizer [35] with an initial learning rate of 0.0025 and a cosine decay schedule. The batch size is 128 for Split CIFAR-100 and Split CUB-200, and 64 for ImageNet-R. The number of epochs is set to be 20 for training on all three datasets. The classifier bias mitigation process described in §4.3 requires ten epochs of training. Each experiment is run on a single NVIDIA GeForce RTX 4090 GPU. More details are presented in §A.2. 5.2 Comparison Results In this section, we present a comprehensive comparison with established baselines across various datasets and pre-training regimes. The performances of different methods are reported in separate 7Table 4: Results on 10-task ImageNet-R with different self-supervised pre-training paradigms. Method Pub. iBOT-1K [68] DINO-1K [5] FAA (↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) DualPrompt [61] ECCV22 61.51 ±1.05 67.11±0.08 58.57±0.45 64.89±0.15 CODA-Prompt [50] CVPR23 66.56±0.68 73.14±0.57 63.15±0.39 69.73±0.25 HiDe-Prompt [59] NeurIPS23 71.33±0.21 73.62±0.13 68.11±0.18 71.70±0.01 VQ-Prompt — 71.68±0.72 76.66±0.40 68.42±0.28 74.43±0.58 tables due to the varying experimental settings such as the number of tasks and the pre-training dataset, to ensure a fair and accurate comparison. Results on ImageNet-R. Table 1 shows the results across five runs on ImageNet-R with 5-task, 10-task, and 20-task splits using the ViT-Base backbone pre-trained with supervised learning on ImageNet-1K. Our VQ-Prompt consistently outperforms other methods across key metrics such as FAA and CAA for all task splits, including the latest prompt-based method EvoPrompt. Results on Split CIFAR-100. Table 2 presents the results across five runs on CIFAR-100 split into 10 tasks, with the ViT-Base backbone also pre-trained on ImageNet-1K with supervised learning. Our VQ-Prompt achieves superior results compared to other methods using the same pre-training weights. Results on Split CUB-200. Table 3 displays the results on Split CUB-200. Following [67], we use the ViT-Base backbone pre-trained on ImageNet-21K for this dataset. VQ-Prompt achieves superior or comparable performance compared with all other methods, including SLCA [67], which trains the entire network without freezing the pre-trained feature extraction backbone. This highlights its potential and efficacy in continual learning for fine-grained classification tasks. Other Pre-training Regimes. Table 4 summarizes the experimental results across three runs on the 10-task ImageNet-R dataset, utilizing different self-supervised pre-training paradigms, namely iBOT-1K [68] and DINO-1K [5]. These results demonstrate that our method consistently outperforms state-of-the-art prompt-based continual learning methods, underscoring its robustness and efficiency in leveraging self-supervised pre-training for continual learning tasks. Specifically, VQ-Prompt shows a greater advantage in CAA than FAA, indicating its superior ability to leverage past knowledge for aiding current tasks, despite a slightly higher degree of forgetting relative to some baselines. This trade-off between adaptation to new tasks and forgetting of previous ones is a common challenge in continual learning. With self-supervised pre-training, our method tends to prioritize adaptability to new tasks to ensure that the model remains relevant and effective in dynamic environments. 5.3 Ablation Study and Additional Analysis In this section, we assess the effectiveness of different components illustrated in §4. The experiments are performed on 10-task ImageNet-R with the ViT-Base backbone pre-trained on ImageNet-1K. Effectiveness of VQ Design. Our VQ design (c.f., §4.2) enables end-to-end training of the discrete prompt selection in continual learning. Here, we compare it with an alternative intuition design choice, i.e., rewriting Eq. (4) as α = Softmax(Kq/τ), and reducing the temperature τ of the softmax operation. A lower temperature leads to a “sharper” distribution of α, allowing the prompt formation in Eq. (5) to more closely approximate discrete prompt selection during end-to-end training with task loss. This baseline is denoted as “Soft-Prompt”. Fig. 3 (a) presents the FAA values of Soft-Prompt with different τ values. Surprisingly, Soft-Prompt achieves its best performance of 77.15 at τ = 1.0 instead of at lower values. While its performance is comparable to other prompt- based methods, Soft-Prompt falls short of “VQ-Prompt-S”, which achieves an FAA value of78.05 on 10-task ImageNet-R. Here, VQ-Prompt-S is a simplified version of VQ-Prompt that does not use representation statistics. Our standard version VQ-Prompt further achieves an FAA value of 78.83. This observation underscores the effectiveness of our VQ design compared to the intuitive low-temperature soft prompt selection. The rationale behind this is that reducing the temperature makes the softmax operation more sensitive to differences in logits. While this heightened sensitivity is acceptable when the model is confident in its predictions, it can lead to more aggressive prompt choices at the beginning of the training when the model is less fully trained. In contrast, our VQ-Prompt utilizes a standard softmax for prompt formation, substitutes the resulting prompt with the nearest one in the prompt pool, and enables 8Softmax Temperature  FAAFAA FAA   (a) (b) (c) VQ-Prompt-S   78.05 VQ-Prompt       78.83 Figure 3: Ablation study. (a) VQ Design. We show the performance of an alternative of VQ Design, “Soft-Prompt”, that generates the continuous prompt with low-temperature softmax operation only without using VQ. Here, “VQ-Prompt-S” is a simplified version of VQ-Prompt without using representation statistics. (b) Prompt Hyperparameters. The results of varying the size of the prompt pool N and the length of a single prompt L p are displayed. (c) Loss Weights. The results of different combinations of λq and λc values are presented. See §5.3 for details. Table 5: Effectiveness of classifier bias mitigation. Results for “5-task”, “10-task”, and “20-task” settings on ImageNet-R are included. “C.B.M.” denotes “Classifier Bias Mitigation”. Backbones are pre-trained on ImageNet-1K. ↑ denotes larger values are better. See §5.3 for details. Method C.B.M. 5-task 10-task 20-task FAA (↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) L2P++ [62] No 70.83 ±0.58 78.34±0.47 69.29±0.73 78.30±0.69 65.89±1.30 77.15±0.65 L2P++ V2 [62] Yes 74.11 ±0.08 78.44±0.63 72.93±0.27 78.63±0.80 70.99±0.26 77.65±0.79 EvoPrompt [26] No 77.16 ±0.18 82.22±0.54 76.83±0.08 82.09±0.68 74.41±0.23 80.96±1.42 VQ-Prompt-SNo 78.52±0.34 82.64±0.68 78.00±0.39 82.83±0.69 76.19±0.26 81.68±1.02 VQ-Prompt Yes 79.23±0.29 82.96±0.50 78.71±0.22 83.24±0.68 78.10±0.22 82.70±1.16 end-to-end learning through gradient estimation and VQ regularization, which proves to be more robust for task knowledge learning compared with Soft-Prompt. Hyperparameters for Prompting. There are two key hyperparameters: i) the size of the prompt pool N that represents the total capacity of the learnable prompts, and ii) the length of a single prompt L p which determines the capacity of a single prompt to encode certain aspects of task knowledge. The total size of the prompts to be prepended to the input of one MSA block is given by N ×L p. Fig. 3 (b) illustrates the impact of varying L p and N on FAA performance. Across different parameter configurations, our method consistently outperforms existing approaches, demonstrating its robustness. Specifically, an excessively small L p value consistently yields sub-optimal results, as indicated by the lower FAA scores across different N values. Increasing N can partially compensate for small L p values, leading to improved performance. In contrast, increasing L p generally enhances performance up to a certain threshold, beyond which an overly large L p may cause knowledge overfitting, as reflected by the stable or slightly declining FAA scores for larger L p values. We selected L p =8 and N =10 as our default configuration. This configuration achieves superior results with fewer parameters compared to other prompt-based methods (c.f., § A.3). Our competitive performance is primarily attributed to the use of VQ, which offers several key benefits for prompt-based continual learning. First, VQ enables the encoding of task knowledge into discrete prompts, which provide a more compact representation than continuous prompts. This discrete nature helps in capturing essential task-specific features with the necessary level of abstraction. Second, integrating VQ within the prompt-based framework facilitates end-to-end optimization with task loss, ensuring that the selected prompts are highly relevant to the task at hand, thereby enhancing the task 9knowledge learning of the prompts. This enables the use of shorter prompts while maintaining strong performance, making our approach more parameter-efficient and effective. Impact of λq and λc. To further enhance the learning of prompt-related parameters, we introduce two regularization terms LVQ and LCommit to guide the learning (c.f., §4.2). We investigate the impact of various loss weights, as shown in Eq. (9). The outcomes are detailed in Fig. 3 (c). As can be observed, these two terms can contribute to good performance when assigned with a relatively broad range of values. According to Fig. 3 (c), we set λq =0.4 and λc =0.1 in all of our experiments. Effectiveness of Classifier Bias Mitigation. The classifier bias mitigation utilizes representation statistics to stabilize task knowledge learning (c.f., §4.3), which can also be applied to other methods. To evaluate its potential advantage, we integrated this component into L2P++. As shown in Table 5, L2P++ with representation statistics (“L2P++ V2”) achieves improved performance over the original L2P++ across all three ImageNet-R settings, but remains inferior to our method. Additionally, we include the results of “VQ-Prompt-S”, a simplified version of VQ-Prompt that omits classifier bias mitigation. Notably, VQ-Prompt-S still outperforms other methods such as EvoPrompt, demonstrating the effectiveness of our approach. This indicates that the classifier bias mitigation process can contribute to performance improvements, but is not the sole determinant of the final performance. 6 Discussion and Conclusion This study focuses on one critical deficiency inherent in current prompt-based continual learning methodologies, specifically the end-to-end optimization of the prompt selection process with task loss while keeping its discrete nature as the representation of task knowledge. Our proposed Vector Quantization Prompting (VQ-Prompt) framework mitigates the challenge by substituting continuous prompts with their nearest counterparts from the prompt pool, thereby enhancing task accuracy through a more aligned and abstract representation of conceptual task knowledge. To overcome the non-differentiability inherent in this process, we employed gradient estimation along with vector quantization regularization terms, which allows for optimizing prompt retrieval with task loss. Repre- sentation statistics are utilized to further stabilize task knowledge learning. Extensive experiments in class-incremental scenarios consistently demonstrate VQ-Prompt’s superiority over SOTA methods. Limitations and Future Work. One limitation of VQ-Prompt is its dependence on pre-trained models. While these models offer rich initial knowledge, enabling a more mature learning process akin to that of an adult, they also inherit the limitations of the pre-trained data distribution and the high computational costs associated with their use. This challenge is not unique to VQ-Prompt but applies broadly to other continual learning methods that rely on pre-trained models. Another limitation is the absence of constraints in calculating similarity scores and prompt keys, which can result in suboptimal prompt utilization, i.e., some prompts are more frequently selected for samples from different tasks while others are less frequently used. To alleviate this, one possible strategy is to introduce constraints on prompt selection, such as limiting the reuse of prompts that have already been heavily utilized by previous tasks to enhance the diversity and utility of the prompts. We leave a more thorough exploration of such prompt selection constraints for future research. Broader Impacts This paper presents a prompt-based continual learning method to tackle the challenges of knowledge preservation in sequential task learning. Our approach facilitates continual adaptation and learning, thereby contributing to the advancement of intelligent, adaptive, and efficient technologies applicable to various domains, including autonomous vehicles and personalized AI agents. While VQ-Prompt advances class-incremental continual learning, its robustness could be compromised if poisonous samples are introduced after a concept has been learned. One possible mitigation strategy is to implement robust anomaly detection methods to identify and filter out suspicious input samples before they are introduced into the training process. Acknowledgments and Disclosure of Funding This work is supported by the National Natural Science Foundation of China (No. 62306292) and the Fundamental Research Funds for the Central Universities (No. CUC24QT06). 10References [1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, pages 139–154, 2018. [2] E. Belouadah and A. Popescu. Il2m: Class incremental learning with dual memory. In ICCV, pages 583–592, 2019. [3] Y . Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [4] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for general continual learning: a strong, simple baseline. NeurIPS, 33:15920–15930, 2020. [5] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In CVPR, pages 9650–9660, 2021. [6] S. Cha, S. Cho, D. Hwang, S. Hong, M. Lee, and T. Moon. Rebalancing batch normalization for exemplar- based class-incremental learning. In CVPR, pages 20127–20136, 2023. [7] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efficient lifelong learning with a-gem. In ICLR, 2019. [8] A. a. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019, 2019. [9] K. Chen and C.-G. Lee. Incremental few-shot learning via vector quantization in deep embedded space. In ICLR, 2021. [10] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE TPAMI, 44(7):3366–3385, 2021. [11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min- derer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [12] A. Douillard, A. Ramé, G. Couairon, and M. Cord. Dytox: Transformers for continual learning with dynamic token expansion. In CVPR, pages 9285–9295, 2022. [13] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 12873–12883, 2021. [14] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, pages 8340–8349, 2021. [15] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In CVPR, pages 15262–15271, 2021. [16] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin. Learning a unified classifier incrementally via rebalancing. In CVPR, pages 831–839, 2019. [17] C.-Y . Hung, C.-H. Tu, C.-E. Wu, C.-H. Chen, Y .-M. Chan, and C.-S. Chen. Compacting, picking and growing for unforgetting continual learning. NeurIPS, 32, 2019. [18] D. Isele and A. Cosgun. Selective experience replay for lifelong learning. In AAAI, volume 32, 2018. [19] M. Kiefer and F. Pulvermüller. Conceptual representations in mind and brain: Theoretical developments, current evidence and future directions. Cortex, 48(7):805–825, 2012. [20] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017. [21] T. Kohonen. Improved versions of learning vector quantization. In IJCNN, pages 545–550, 1990. [22] T. Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9):1464–1480, 1990. 11[23] Y . Kong, L. Liu, Z. Wang, and D. Tao. Balancing stability and plasticity through advanced null space in continual learning. In ECCV, pages 219–236, 2022. [24] T. Konishi, M. Kurokawa, C. Ono, Z. Ke, G. Kim, and B. Liu. Parameter-level soft-masking for continual learning. In ICML, 2023. [25] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. [26] M. R. Kurniawan, X. Song, Z. Ma, Y . He, Y . Gong, Y . Qi, and X. Wei. Evolving parameterized prompt memory for continual learning. In AAAI, volume 38, pages 13301–13309, 2024. [27] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang. Overcoming catastrophic forgetting by incremental moment matching. NeurIPS, 30, 2017. [28] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. [29] X. Li, Y . Zhou, T. Wu, R. Socher, and C. Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In ICML, pages 3925–3934, 2019. [30] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL-IJCNLP, pages 4582–4597, 2021. [31] Z. Li and D. Hoiem. Learning without forgetting. IEEE TPAMI, 40(12):2935–2947, 2017. [32] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1–35, 2023. [33] Y . Liu and T. Tuytelaars. Residual tuning: Toward novel category discovery without labels.IEEE TNNLS, 2022. [34] D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. NeurIPS, 30, 2017. [35] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2018. [36] Z. Luo, Y . Liu, B. Schiele, and Q. Sun. Class-incremental exemplar compression for class-incremental learning. In CVPR, pages 11371–11380, 2023. [37] T. Malepathirana, D. Senanayake, and S. Halgamuge. Napa-vq: Neighborhood-aware prototype augmenta- tion with vector quantization for continual learning. In CVPR, pages 11674–11684, 2023. [38] T. Martinetz, K. Schulten, et al. A “neural-gas” network learns topologies. ANN, pages 397–402, 1991. [39] M. Masana, X. Liu, B. Twardowski, M. Menta, A. D. Bagdanov, and J. van de Weijer. Class-incremental learning: Survey and performance evaluation on image classification. IEEE TPAMI, 2022. [40] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. 1989. [41] G. L. Murphy and D. L. Medin. The role of theories in conceptual coherence. Psychological Review, 92 (3):289, 1985. [42] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019. [43] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor. Imagenet-21k pretraining for the masses. In NeurIPS, 2021. [44] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y . Tu, and G. Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR, 2019. [45] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne. Experience replay for continual learning. NeurIPS, 32, 2019. [46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211–252, 2015. [47] G. Saha, I. Garg, and K. Roy. Gradient projection memory for continual learning. In ICLR, 2021. [48] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, pages 4548–4557, 2018. 12[49] Y . Shi, K. Zhou, J. Liang, Z. Jiang, J. Feng, P. H. Torr, S. Bai, and V . Y . Tan. Mimicking the oracle: an initial phase decorrelation approach for class incremental learning. In CVPR, pages 16722–16731, 2022. [50] J. S. Smith, L. Karlinsky, V . Gutta, P. Cascante-Bonilla, D. Kim, A. Arbelle, R. Panda, R. Feris, and Z. Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In CVPR, pages 11909–11919, 2023. [51] Y .-M. Tang, Y .-X. Peng, and W.-S. Zheng. When prompt-based incremental learning does not meet strong pretraining. In ICCV, pages 1706–1716, 2023. [52] X. Tao, X. Chang, X. Hong, X. Wei, and Y . Gong. Topology-preserving class-incremental learning. In ECCV, pages 254–270, 2020. [53] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y . Gong. Few-shot class-incremental learning. InCVPR, pages 12183–12192, 2020. [54] G. M. Van de Ven and A. S. Tolias. Three scenarios for continual learning.arXiv preprint arXiv:1904.07734, 2019. [55] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. [56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. NeurIPS, 30, 2017. [57] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. [58] F.-Y . Wang, D.-W. Zhou, L. Liu, H.-J. Ye, Y . Bian, D.-C. Zhan, and P. Zhao. Beef: Bi-compatible class-incremental learning via energy-based expansion and fusion. In ICLR, 2023. [59] L. Wang, J. Xie, X. Zhang, M. Huang, H. Su, and J. Zhu. Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality. In NeurIPS, 2023. [60] Y . Wang, Z. Huang, and X. Hong. S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning. NeurIPS, 2022. [61] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y . Lee, X. Ren, G. Su, V . Perot, J. Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In ECCV, pages 631–648, 2022. [62] Z. Wang, Z. Zhang, C.-Y . Lee, H. Zhang, R. Sun, X. Ren, G. Su, V . Perot, J. Dy, and T. Pfister. Learning to prompt for continual learning. In CVPR, pages 139–149, 2022. [63] Y . Wu, Y . Chen, L. Wang, Y . Ye, Z. Liu, Y . Guo, and Y . Fu. Large scale incremental learning. InCVPR, pages 374–382, 2019. [64] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable networks. ICLR, 2018. [65] G. Zeng, Y . Chen, B. Cui, and S. Yu. Continual learning of context-dependent processing in neural networks. Nature Machine Intelligence, 1(8):364–372, 2019. [66] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. [67] G. Zhang, L. Wang, G. Kang, L. Chen, and Y . Wei. Slca: Slow learner with classifier alignment for continual learning on a pre-trained model. In ICCV, 2023. [68] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. Image bert pre-training with online tokenizer. In ICLR, 2022. 13A Appendix / supplemental material In this section, we provide detailed supplementary information. §A.1 outlines the evaluation metrics used to assess performance, providing detailed descriptions and formulas for clarity. §A.3 elaborates on the configurations of prompt-based methods compared in our experiments. §A.2 offers additional implementation details, including model training procedures, detailed parameter settings, and method- ology for obtaining results of other methods compared in the experiments. Finally, §A.4 provides more results on two challenging datasets. A.1 Evaluation Metrics To assess the performance of continual learning, we record the average classification accuracy of all seen classes at the end of each task training, and denote the average accuracy on the i-th task after learning the j-th task as Aij. The formal definitions of FAA and CAA are introduced as follows. i) Final Average Accuracy (FAA)refers to the last average accuracy after learning all the tasks: FAA = 1 T TX i=1 AiT , (10) where AiT is the average accuracy of taski after learning task T, and T is the number of tasks. Larger FAA indicates greater learning capacity and less forgetting. FAA is also denoted as “Last-Acc”. ii) Cumulative Average Accuracy (CAA)is the average of historical FAA values after learning each task, which is calculated as: CAA = 1 T TX j=1 1 j jX i=1 Aij. (11) CAA reflects the overall performance after learning each incremental task, which can also be denoted as “Inc-Acc”. A.2 More Implementation Details We use AdamW [35] with β1 = 0.9 and β2 = 0.999. Our batch size is 64 for ImageNet-R, and 128 for Split CIFAR-100 and Split CUB-200. We resize the input images to 224×224 and perform data transform following [50], including random horizontal flip and normalization. Following DualPrompt [61] and CODA-Prompt [50], we use 20% of the training data as validation data, and perform hyperparameters tuning on it. After hyperparameter searching, we use a learning rate of 0.0025 for our method. For all other prompt-based methods, we use the hyperparameters following [50] We search the values of prompt length LP from 4 to 24 with a step of 4. We search the number of prompt elements N in {10, 30, 50, 100}. We found that a prompt length of 8 and 10 prompt elements already work fine. We insert prompts at the same locations as all other implemented prompt-based methods in this paper, namely, the first 5 MSA blocks. A detailed comparison of the prompt configurations can be found in §A.3. Finally, we run FT, FT++, L2P++, Deep L2P++, DualPrompt, and CODA-Prompt by using the official implementation provided by CODA-Prompt [50]. We set the predicted logits for past task classes to be 0 to prevent gradients from flowing to the linear heads of these classes. This is recommended by CODA-Prompt to improve the performance of these methods during code reproduction, as it could alleviate the bias towards new classes in CIL for rehearsal-free methods. For HiDe-Prompt [59] and EvoPrompt [26], we reproduce the results using their respective official implementations. A.3 Configurations of Prompt-based Methods Table 6 presents the configurations of all the prompt-based continual learning methods compared in our experiment. Here, “Pro-T” denotes Prompt Tuning [28], and “Pre-T” denotes Prefix Tuning strategy [30], “Locations” indicates the MSA blocks to insert the prompts, N is the number of prompts/components in the prompt pool, and Lp is the length of a single prompt/component. L2P++ 14Table 6: Prompt configurations for prompt-based approaches in our experiments. See §A.3. Approaches Strategy Locations Datasets Hyperparameters L2P++ [62] Pre-T [0] All N=30, Lp=20 Deep L2P++ [62] Pre-T [0 1 2 3 4] All N=30, Lp=20 Dual-Prompt [61] Pre-T [0 1] All G: N=1, Lp=6 Pre-T [2 3 4] All E: N=10, Lp=20 CODA-P [50] Pre-T [0 1 2 3 4] All N=100, Lp=8 HiDe-Prompt [59] Pre-T [0 1 2 3 4] ImageNet-R N=10, Lp=40 Split CIFAR-100N=10, Lp=10 Split CUB-200 N=10, Lp=40 EvoPrompt [26] Pro-T [0 1 2 3 4 5 6 7 8 9 10 11] All Input-cond., Lp=5 VQ-Prompt (Ours) Pre-T [0 1 2 3 4] All N=10, Lp=8 and Deep L2P++ are two variants of L2P for fair comparison. Specifically, L2P++ uses Pre-T instead of Pro-T prompting, and inserts the prompts to the first MSA block. Deep L2P++ is an extension of L2P++ with prompts incorporated into the same 5 MSA blocks as DualPrompt. For DualPrompt, “G” denotes the general prompt shared by all the tasks, and “E” denotes the expert prompt pool where only one of the elements is selected for a certain query. As can be observed, our method requires fewer prompting parameters while consistently achieving superior or comparable performance across the benchmarks in continual learning. A.4 More Experiment Results This section presents results on two challenging datasets, namely ImageNet-A [15] and VTAB [66], for evaluating continual learning methods based on pre-trained models. ImageNet-A contains adversarial images that fool current ImageNet pre-trained classifiers, while VTAB includes 19 datasets with diverse classes that do not overlap with ImageNet-1K. For ImageNet-A, we split the 200 classes into 20 tasks. For VTAB, we sample five 10-class datasets from it to construct the cross-domain CIL setting. We used a batch size of 64 for ImageNet-A and 8 for VTAB, with other training hyperparameters consistent with those used on other datasets. As shown in Table 7, our VQ-Prompt outperforms other SOTA methods such as HiDe-Prompt when evaluated using FAA. Table 7: Results evaluated using the FAA metric on the ImageNet-A and VTAB datasets. Backbones are pre-trained on ImageNet-1K. Larger values are better. Method ImageNet-A [15] VTAB [66] HiDe-Prompt [59] 51.67 86.38 VQ-Prompt 52.96 90.46 15",
      "meta_data": {
        "arxiv_id": "2410.20444v2",
        "authors": [
          "Li Jiao",
          "Qiuxia Lai",
          "Yu Li",
          "Qiang Xu"
        ],
        "published_date": "2024-10-27T13:43:53Z",
        "pdf_url": "https://arxiv.org/pdf/2410.20444v2.pdf",
        "github_url": "https://github.com/jiaolifengmi/VQ-Prompt"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the catastrophic forgetting problem in continual learning, specifically the sub-optimal prompt selection and inadequate abstraction of pre-trained features in existing prompt-based methods. It proposes VQ-Prompt, an end-to-end learnable discrete prompting mechanism that incorporates Vector Quantization (VQ) for effective abstraction of task knowledge. Key contributions include: 1) Proposing VQ-Prompt, an end-to-end learnable discrete prompting mechanism for continual learning, which is a critical and overlooked aspect in current literature. 2) Leveraging gradient estimation to propagate task loss to prompt-related parameters and regularizing prompt learning with vector quantization terms, facilitating end-to-end training of the discrete prompt pool. 3) Incorporating representation statistics during training to stabilize task knowledge learning and improve overall continual learning performance. Extensive experiments demonstrate that VQ-Prompt consistently outperforms state-of-the-art continual learning methods in challenging class-incremental settings.",
        "methodology": "The VQ-Prompt method begins by constructing a continuous prompt through a soft selection from a predefined prompt pool, based on cosine similarity scores between an input query and learnable prompt keys. This continuous prompt is then quantized to its nearest match within the prompt pool, which is subsequently inserted into a Multi-Head Self-Attention (MSA) block of a frozen pre-trained transformer (Vision Transformer using Prefix Tuning). To overcome the non-differentiability of the nearest-neighbor look-up (quantization), the method employs a straight-through estimator for gradient estimation, propagating the task loss back to the continuous prompt and subsequently to the prompt pool and keys. Additionally, two Vector Quantization (VQ) regularization terms are introduced: an L2 error term to move the selected prompt pool element towards the continuous prompt, and a commitment loss to ensure the continuous prompt commits to the selected pool element. Furthermore, representation statistics (mean and variance) of previously learned classes are utilized to mitigate classifier bias towards new classes, by generating pseudo features to fine-tune the classifier. The overall objective function combines the cross-entropy task loss with the VQ regularization and commitment terms.",
        "experimental_setup": "The method was evaluated on three representative benchmarks for Class-Incremental Learning (CIL): ImageNet-R (divided into 5, 10, and 20 disjoint tasks), Split CIFAR-100 (10 tasks, 10 classes each), and Split CUB-200 (10 tasks, 20 classes each). Additional experiments were conducted on ImageNet-A (200 classes split into 20 tasks) and VTAB (five 10-class datasets for cross-domain CIL). The backbone model used was ViT-Base, pre-trained with supervised learning on ImageNet-1K or ImageNet-21K, and also with self-supervised paradigms like iBOT-1K and DINO-1K. Baselines included 'Joint Training' (upper-bound), 'FT' and 'FT++' (lower-bounds), as well as several prompt-based methods (L2P++, Deep L2P++, DualPrompt, HiDe-Prompt, CODA-Prompt, EvoPrompt) and other continual learning methods (LwF, BiC, SLCA). Performance was measured using Final Average Accuracy (FAA) and Cumulative Average Accuracy (CAA). Implementation details involved an AdamW optimizer, cosine decay learning rate schedule (0.0025 initial LR), batch sizes of 128 (CIFAR, CUB), 64 (ImageNet-R, ImageNet-A), and 8 (VTAB), training for 20 epochs, and classifier bias mitigation for 10 epochs. Prompt hyperparameters were N=10 (number of keys/prompt elements) and Lp=8 (prompt length), with prompts inserted into the first 5 MSA blocks.",
        "limitations": "One limitation is the dependence on pre-trained models, which, while providing rich initial knowledge, inherit the limitations of the pre-trained data distribution and incur high computational costs. Another limitation is the absence of constraints in calculating similarity scores and prompt keys, potentially leading to suboptimal prompt utilization where some prompts are over-selected and others are under-utilized across different tasks. Additionally, the robustness of VQ-Prompt could be compromised if poisonous samples are introduced after a concept has been learned.",
        "future_research_directions": "Future research directions include introducing constraints on prompt selection, such as limiting the reuse of prompts that have already been heavily utilized by previous tasks, to enhance prompt diversity and utility. Another suggested direction is to implement robust anomaly detection methods to identify and filter out suspicious input samples before they are introduced into the training process, thereby mitigating the vulnerability to poisonous samples.",
        "experimental_code": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# From models/zoo.py\nclass VQPrompt(nn.Module):\n    def __init__(self, emb_d, n_tasks, prompt_param, key_dim=768):\n        super().__init__()\n        self.task_count = 0\n        self.emb_d = emb_d\n        self.key_d = key_dim\n        self.n_tasks = n_tasks\n        self.soft_t = prompt_param[-1] # Temperature for soft selection\n        self._init_smart(emb_d, prompt_param)\n\n        # e prompt init\n        for e in self.e_layers:\n            e_l = self.e_p_length\n            p = tensor_prompt(self.e_pool_size, e_l, emb_d) # Prompt pool elements\n            k = tensor_prompt(self.e_pool_size, self.key_d) # Learnable prompt keys\n\n            setattr(self, f'e_p_{e}',p)\n            setattr(self, f'e_k_{e}',k)\n\n    def _init_smart(self, emb_d, prompt_param):\n        # Prompt basic parameters\n        self.e_pool_size = int(prompt_param[0]) # Prompt pool size\n        self.e_p_length = int(prompt_param[1]) # Prompt length\n        self.e_layers = [0,1,2,3,4] # Layers where prompts are inserted\n\n        # VQ loss weights\n        self.vq_coef = 0.4\n        self.comit_coef = 0.1\n        \n    def forward(self, x_querry, l, x_block, train=False, task_id=None):\n\n        e_valid = False\n        if l in self.e_layers:\n            e_valid = True\n            B, C = x_querry.shape\n\n            K = getattr(self,f'e_k_{l}') # Learnable keys for current layer\n            p = getattr(self,f'e_p_{l}') # Prompt pool for current layer\n            \n            # Soft selection from prompt pool based on cosine similarity\n            n_K = nn.functional.normalize(K, dim=1)\n            cos_sim = torch.einsum('bd,kd->bk', x_querry, n_K)\n            alpha = torch.softmax(cos_sim/self.soft_t, dim=1)\n            p_a = torch.einsum('bk,kld->bld', alpha, p) # Continuous prompt\n            \n            # Quantization: nearest-neighbor look-up\n            p_a_expended = p_a.unsqueeze(1)\n            dist = torch.pow(p_a_expended - p, 2)\n            _, idxmin = dist.sum(-1).sum(-1).min(1)\n            quantized  = p.index_select(0, idxmin.view(-1))\n\n            # VQ regularization terms with Straight-Through Estimator (STE)\n            e_latent_loss = F.mse_loss(p_a, quantized.detach()) # L2 error\n            q_latent_loss = F.mse_loss(quantized, p_a.detach()) # Commitment loss\n            P_ = p_a + (quantized - p_a).detach() # STE for gradient flow\n\n            # Split into key and value prefixes for Prefix Tuning\n            i = int(self.e_p_length/2)\n            Ek = P_[:,:i,:]\n            Ev = P_[:,i:,:]\n\n            # Combine VQ regularization losses\n            loss = self.vq_coef*e_latent_loss + self.comit_coef*q_latent_loss \n\n        else:\n            loss = 0\n\n        # Prepare prompts for injection into transformer block\n        if e_valid:\n            p_return = [Ek, Ev]\n        else:\n            p_return = None\n\n        return p_return, loss, x_block\n\ndef tensor_prompt(a, b, c=None, ortho=False):\n    if c is None:\n        p = torch.nn.Parameter(torch.FloatTensor(a,b), requires_grad=True)\n    else:\n        p = torch.nn.Parameter(torch.FloatTensor(a,b,c), requires_grad=True)\n    if ortho:\n        nn.init.orthogonal_(p)\n    else:\n        nn.init.uniform_(p)\n    return p\n\n# From models/vit.py\nclass Attention(nn.Module):\n    # ... (other __init__ and methods)\n    def forward(self, x, register_hook=False, prompt=None):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]   \n\n        if prompt is not None:\n            pk, pv = prompt # Prompt keys and values from the VQPrompt module\n            pk = pk.reshape(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n            pv = pv.reshape(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n            k = torch.cat((pk,k), dim=2) # Inject prompts as prefixes to keys\n            v = torch.cat((pv,v), dim=2) # Inject prompts as prefixes to values\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        # ... (rest of attention calculation)\n        return x\n\n# From learners/prompt.py\nclass Prompt(nn.Module): # Inherits from NormalNN, simplified here for context\n    # ... (init and other methods)\n    def update_model(self, inputs, targets):\n        # Forward pass through the model, which includes the VQPrompt module\n        logits, prompt_loss = self.model(inputs, train=True, cls_mean=self.cls_mean)\n        logits = logits[:,:self.valid_out_dim]\n\n        # Heuristic to mask out previous class logits during training of current task\n        logits[:,:self.last_valid_out_dim] = -float('inf')\n        dw_cls = self.dw_k[-1 * torch.ones(targets.size()).long()]\n        total_loss = self.criterion(logits, targets.long(), dw_cls)\n\n        # Add prompt regularization loss to the total objective\n        total_loss = total_loss + prompt_loss.sum()\n\n        # Standard backpropagation and optimizer step\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        self.optimizer.step()\n\n        return total_loss.detach(), logits\n\n# From trainer.py\n@torch.no_grad()\ndef _compute_mean(self, model: torch.nn.Module, class_mask=None):\n    model.eval()\n\n    for cls_id in class_mask:\n        # Load data for a specific class\n        self.train_dataset.load_class(cls_id)\n        data_loader_cls = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.workers) \n        features_per_cls = []\n\n        # Extract pre-logits features for the class\n        for i, (inputs, targets, task) in enumerate(data_loader_cls):\n            if self.learner.gpu:\n                inputs = inputs.cuda()\n                targets = targets.cuda()\n            features = model(inputs, return_pre_logits=True)\n            features_per_cls.append(features)\n        features_per_cls = torch.cat(features_per_cls, dim=0)\n\n        # Apply KMeans clustering to get multiple centroids/statistics per class\n        n_clusters = self.n_centroids  # Number of clustering centers (default 1)\n        features_per_cls = features_per_cls.cpu().numpy()\n        kmeans = KMeans(n_clusters=n_clusters, n_init='auto')\n        kmeans.fit(features_per_cls)\n        cluster_labels = kmeans.labels_\n\n        cluster_means = []\n        cluster_vars = []\n        # Store mean and variance for each cluster\n        for i in range(n_clusters):\n            cluster_data = features_per_cls[cluster_labels == i]\n            cluster_mean = torch.tensor(np.mean(cluster_data, axis=0), dtype=torch.float64).to(inputs.device)\n            cluster_var = torch.tensor(np.var(cluster_data, axis=0), dtype=torch.float64).to(inputs.device)\n            cluster_means.append(cluster_mean)\n            cluster_vars.append(cluster_var)\n        \n        self.learner.cls_mean[cls_id] = cluster_means\n        self.learner.cls_cov[cls_id] = cluster_vars\n\ndef train_task_adaptive_prediction(self, model: torch.nn.Module, class_mask=None, task_id=-1):\n    model.train()\n    run_epochs = self.crct_epochs # Number of epochs for statistics replay (default 10)\n    crct_num = 0\n    valid_out_dim = self.learner.valid_out_dim\n    ca_lr = self.ca_lr # Learning rate for statistics replay (default 0.0001)\n    weight_decay = self.ca_weight_decay # Weight decay for statistics replay (default 5e-4)\n    batch_size = self.batch_size\n    # Only optimize parameters not related to prompts\n    param_list = [p for n, p in model.named_parameters() if p.requires_grad and 'prompt' not in n]\n    network_params = [{'params': param_list, 'lr': ca_lr, 'weight_decay': weight_decay}]\n\n    optimizer = torch.optim.AdamW(network_params, lr=ca_lr / 10, weight_decay=weight_decay)\n\n    criterion = torch.nn.CrossEntropyLoss()\n    if self.learner.gpu:\n        criterion = criterion.cuda()\n\n    # Calculate number of previous classes for scheduler's iter_step\n    for i in range(task_id): \n        crct_num += len(class_mask[i]) \n\n    # Scheduler configuration for the fine-tuning process\n    scheduler_cfg = {\n            'base_value': [ca_lr / 10], \n            'final_value': [1e-6], \n            'optimizer': optimizer, \n            'iter_step': crct_num, # Iterations per epoch for LR schedule, based on previous classes\n            'n_epochs': run_epochs, \n            'last_epoch': -1, \n            'warmup_epochs': 0, \n            'start_warmup_value': 0, \n            'freeze_iters': 0\n        }\n    scheduler = CosineSchedulerIter(**scheduler_cfg)\n\n    # Fine-tuning loop\n    for epoch in range(run_epochs):\n        sampled_data = []\n        sampled_label = []\n        num_sampled_pcls = int(batch_size * self.ca_batch_size_ratio) # Batch size ratio for generated features\n\n        # Generate pseudo features for all classes seen so far (current and previous tasks)\n        for i in range(task_id + 1):\n            for c_id in class_mask[i]:\n                mapped_c_id = self.train_dataset.class_mapping[c_id]\n                for cluster in range(len(self.learner.cls_mean[c_id])):\n                    mean = self.learner.cls_mean[c_id][cluster]\n                    var = self.learner.cls_cov[c_id][cluster]\n                    if var.mean() == 0: # Skip if variance is zero\n                        continue\n                    # Generate samples using Multivariate Normal distribution\n                    m = MultivariateNormal(mean.float(), (torch.diag(var) + 1e-4 * torch.eye(mean.shape[0]).to(mean.device)).float())\n                    sampled_data_single = m.sample(sample_shape=(num_sampled_pcls,))\n                    sampled_data.append(sampled_data_single)\n                    sampled_label.extend([mapped_c_id] * num_sampled_pcls)\n\n        if not sampled_data: # Handle case with no sampled data\n            continue\n\n        sampled_data = torch.cat(sampled_data, dim=0).float().cuda()\n        sampled_label = torch.tensor(sampled_label).long().to(sampled_data.device)\n\n        # Shuffle generated data\n        sf_indexes = torch.randperm(sampled_data.size(0))\n        inputs = sampled_data[sf_indexes]\n        targets = sampled_label[sf_indexes]\n\n        # Iterate through batches of generated data\n        num_batches_per_epoch = (len(inputs) + num_sampled_pcls - 1) // num_sampled_pcls\n        for batch_idx in range(num_batches_per_epoch):\n            start_idx = batch_idx * num_sampled_pcls\n            end_idx = min((batch_idx + 1) * num_sampled_pcls, len(inputs))\n            inp = inputs[start_idx:end_idx]\n            tgt = targets[start_idx:end_idx]\n\n            if inp.size(0) == 0: continue\n\n            # Forward pass through the classifier head\n            try:\n                logits = model.module.forward_fc(inp)\n            except:\n                logits = model.forward_fc(inp)\n\n            logits = logits[:,:valid_out_dim]\n            loss = criterion(logits, tgt)  # Cross-entropy loss\n\n            # Backpropagate and update parameters\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step() # Update learning rate according to schedule\n",
        "experimental_info": "Method Name: VQ-Prompt\nPrompting Strategy:\n- A continuous prompt is constructed by performing a soft selection from a predefined prompt pool. The selection uses cosine similarity scores between an input query (CLS token from the backbone) and learnable prompt keys, followed by a softmax function with a temperature parameter (`soft_t`).\n- This continuous prompt is then quantized to its nearest match within the prompt pool, determined by L2 distance.\n- The quantized prompt is subsequently inserted into the Multi-Head Self-Attention (MSA) blocks of a frozen pre-trained Vision Transformer (ViT) via Prefix Tuning, where it's concatenated to the key and value matrices.\nGradient Estimation:\n- A Straight-Through Estimator (STE) is employed to handle the non-differentiable nearest-neighbor quantization step, allowing task loss gradients to propagate back to the continuous prompt, prompt pool, and keys.\nVQ Regularization Terms:\n- L2 Error: `vq_coef` (default 0.4) * `F.mse_loss(continuous_prompt, quantized_prompt.detach())`. This term encourages the selected prompt pool element to move towards the continuous prompt.\n- Commitment Loss: `comit_coef` (default 0.1) * `F.mse_loss(quantized_prompt, continuous_prompt.detach())`. This term ensures the continuous prompt commits to the selected pool element.\nBackbone Model:\n- A frozen pre-trained Vision Transformer (ViT) is used as the feature extractor.\nPre-training Weights:\n- The ViT can be initialized with various pre-trained weights, including `sup1k`, `sup21k`, `ibot1k`, and `dino1k`.\nPrompt Parameters:\n- `e_pool_size`: The number of elements in the prompt pool (configurable via `prompt_param[0]`).\n- `e_p_length`: The length (dimension) of each prompt element (configurable via `prompt_param[1]`).\n- `soft_t`: Temperature parameter for the softmax in the soft selection process (derived from `prompt_param[-1]`).\n- `e_layers`: A list of transformer layers where prompts are inserted (default `[0, 1, 2, 3, 4]`).\nClassifier Bias Mitigation (Task-Adaptive Prediction):\n- Representation Statistics: Mean and variance of features for previously learned classes are computed and stored. Features are obtained by passing inputs through the model with `return_pre_logits=True`.\n- Clustering: KMeans clustering with `n_centroids` (default 1) is applied to the features of each class to obtain multiple means and variances, providing richer statistics.\n- Pseudo Feature Generation: Multivariate Normal distributions (`MultivariateNormal`) are utilized to generate pseudo features based on the stored class-wise (and cluster-wise) means and variances.\n- Classifier Fine-tuning: The final classification layer (`self.last` or `forward_fc`) is fine-tuned using these generated pseudo features from all tasks seen so far.\n- Fine-tuning Parameters:\n    - `crct_epochs`: Number of epochs for this statistics replay-based fine-tuning (default 10).\n    - `ca_lr`: Learning rate for the fine-tuning optimizer (default 0.0001, but the actual LR for AdamW is `ca_lr / 10`).\n    - `ca_weight_decay`: Weight decay for the fine-tuning optimizer (default 5e-4).\n    - `ca_batch_size_ratio`: A multiplier determining the batch size of generated features for fine-tuning (`ca_batch_size = ca_batch_size_ratio * batch_size`, default 4).\nOverall Objective Function:\n- Combines the Cross-Entropy task loss with the two Vector Quantization (VQ) regularization terms (L2 error and commitment loss)."
      }
    },
    {
      "title": "Mitigating Forgetting in Online Continual Learning with  Neuron Calibration"
    },
    {
      "title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks",
      "abstract": "Continual learning aims to learn new tasks without forgetting previously\nlearned ones. This is especially challenging when one cannot access data from\nprevious tasks and when the model has a fixed capacity. Current\nregularization-based continual learning algorithms need an external\nrepresentation and extra computation to measure the parameters'\n\\textit{importance}. In contrast, we propose Uncertainty-guided Continual\nBayesian Neural Networks (UCB), where the learning rate adapts according to the\nuncertainty defined in the probability distribution of the weights in networks.\nUncertainty is a natural way to identify \\textit{what to remember} and\n\\textit{what to change} as we continually learn, and thus mitigate catastrophic\nforgetting. We also show a variant of our model, which uses uncertainty for\nweight pruning and retains task performance after pruning by saving binary\nmasks per tasks. We evaluate our UCB approach extensively on diverse object\nclassification datasets with short and long sequences of tasks and report\nsuperior or on-par performance compared to existing approaches. Additionally,\nwe show that our model does not necessarily need task information at test time,\ni.e. it does not presume knowledge of which task a sample belongs to.",
      "full_text": "Published as a conference paper at ICLR 2020 UNCERTAINTY -GUIDED CONTINUAL LEARNING WITH BAYESIAN NEURAL NETWORKS Sayna Ebrahimi∗ UC Berkeley Mohamed Elhoseiny† KAUST, Stanford University Trevor Darrell UC Berkeley Marcus Rohrbach Facebook AI Research ABSTRACT Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a ﬁxed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters’ importance. In contrast, we propose Uncertainty- guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty deﬁned in the probability distribution of the weights in networks. Uncertainty is a natural way to identify what to rememberand what to change as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classiﬁcation datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to. 1 I NTRODUCTION Humans can easily accumulate and maintain knowledge gained from previously observed tasks, and continuously learn to solve new problems or tasks. Artiﬁcial learning systems typically forget prior tasks when they cannot access all training data at once but are presented with task data in sequence. Overcoming these challenges is the focus ofcontinual learning, sometimes also referred to as lifelong learning or sequential learning. Catastrophic forgetting(McCloskey & Cohen, 1989; McClelland et al., 1995) refers to the signiﬁcant drop in the performance of a learner when switching from a trained task to a new one. This phenomenon occurs because trained parameters on the initial task change in favor of learning new objectives. Given a network of limited capacity, one way to address this problem is to identify the importance of each parameter and penalize further changes to those parameters that were deemed to be important for the previous tasks (Kirkpatrick et al., 2017; Aljundi et al., 2018; Zenke et al., 2017). An alternative is to freeze the most important parameters and allow future tasks to only adapt the remaining parameters to new tasks (Mallya & Lazebnik, 2018). Such models rely on the explicit parametrization of importance. We propose here implicit uncertainty-guided importance representation. Bayesian approaches to neural networks (MacKay, 1992b) can potentially avoid some of the pitfalls of explicit parameterization of importance in regular neural networks. Bayesian techniques, naturally account for uncertainty in parameters estimates. These networks represent each parameter with a distribution deﬁned by a mean and variance over possible values drawn from a shared latent probability distribution (Blundell et al., 2015). Variational inference can approximate posterior distributions using Monte Carlo sampling for gradient estimation. These networks act like ensemble methods in that they reduce the prediction variance but only use twice the number of parameters present in a regular neural network. We propose to use the predicted mean and variance of the latent distributions to characterize the importance of each parameter. We perform continual learning with ∗Corresponding author: sayna@berkeley.edu †Work done while at Facebook AI Research 1 arXiv:1906.02425v2  [cs.LG]  20 Feb 2020Published as a conference paper at ICLR 2020 (a) (b) (c)  Illustration of evolution of weight distributions through learning two tasks. (a) circles represent  weight parameters, initialized by distributions with mean and variance values randomly sampled  from Ɲ(0,0.1).  As an example we show five color-coded and plot their distributions. (b) Shows  posterior distribution after learning Task 1. While W1 and W2 exhibit lower uncertainties (more  contributions in learning Task 1), W3, W4, and W5 appear to have larger uncertainties, with the  highest STD in W5, making them available to learn more tasks. (c) Task 2 is learned using higher  learning rates for previously uncertain parameters (W3 and W4, W5) while learning rates for W1  and W2 are moderated according to their predicted low uncertainty after finishing task 1.  p(𝜃) 𝜃 𝜃 𝜃 Training Task 1 Training Task 2 1 3 2 4 p(𝜃) p(𝜃) 5 p(y| x,[𝜃1,𝜃2,𝜃3,𝜃4,𝜃5]) ⩰ p(y) p(y| x,[𝜃1,𝜃2,𝜃3,𝜃4,𝜃5]) ⩰  p(y|x,[𝜃1,𝜃2]) p(y| x, [𝜃1,𝜃2,𝜃3,𝜃4,𝜃5]) ⩰  p(y|x,[𝜃1,𝜃2,𝜃3,𝜃4]) Figure 1: Illustration of the evolution of weight distributions – uncertain weights adapt more quickly – when learning two tasks using UCB. (a) weight parameter initialized by distributions initialized with mean and variance values randomly sampled from N(0,0.1). (b) posterior distribution after learning task one; while θ1 and θ2 exhibit lower uncertainties after learning the ﬁrst task, θ3, θ4, and θ5 have larger uncertainties, making them available to learn more tasks. (c) a second task is learned using higher learning rates for previously uncertain parameters (θ1, θ2, θ3, and θ4) while learning rates for θ1 and θ2 are reduced. Size of the arrows indicate the magnitude of the change of the distribution mean upon gradient update. Bayesian neural networks by controlling the learning rate of each parameter as a function of its uncertainty. Figure 1 illustrates how posterior distributions evolve for certain and uncertain weight distributions while learning two consecutive tasks. Intuitively, the more uncertain a parameter is, the more learnable it can be and therefore, larger gradient steps can be taken for it to learn the current task. As a hard version of this regularization technique, we also show that pruning, i.e., preventing the most important model parameters from any change and learning new tasks with the remaining parameters, can be also integrated into UCB. We refer to this method as UCB-P. Contributions: We propose to perform continual learning with Bayesian neural networks and develop a new method which exploits the inherent measure of uncertainty therein to adapt the learning rate of individual parameters (Sec. 4). Second, we introduce a hard-threshold variant of our method that decides which parameters to freeze (Sec. 4.2). Third, in Sec. 5, we extensively validate our approach experimentally, comparing it to prior art both on single datasets split into different tasks, as well as for the more difﬁcult scenario of learning a sequence of different datasets. Forth, in contrast to most prior work, our approach does not rely on knowledge about task boundaries at inference time, which humans do not need and might not be always available. We show in Sec. 6 that our approach naturally supports this scenario and does not require task information at test time, sometimes also referred to as a “single head” scenario for all tasks. We refer to evaluation metric of a “single head” model without task information at test time as “generalized accuracy”. Our code is available at https://github.com/SaynaEbrahimi/UCB. 2 R ELATED WORK Conceptually, approaches to continual learning can be divided into the following categories: dynamic architectural methods, memory-based methods, and regularization methods. Dynamic architectural methods: In this setting, the architecture grows while keeping past knowl- edge ﬁxed and storing new knowledge in different forms such as additional layers, nodes, or modules. In this approach, the objective function remains ﬁxed whereas the model capacity grows –often exponentially– with the number of tasks. Progressive networks (Rusu et al., 2016; Schwarz et al., 2018) was one of the earliest works in this direction and was successfully applied to reinforcement learning problems; the base architecture was duplicated and lateral connections added in response to new tasks. Dynamically Expandable Network (DEN) (Yoon et al., 2018) also expands its network by selecting drifting units and retraining them on new tasks. In contrast to our method, these approaches require the architecture grow with each new task. Memory-based methods:In this regime, previous information is partially stored to be used later as a form of rehearsal (Robins, 1995). Gradient episodic memory (GEM) (Lopez-Paz et al., 2017) uses this idea to store the data at the end of each episode to be used later to prevent gradient updates from deviating from their previous values. GEM also allows for positive backward knowledge transfer, i.e, 2Published as a conference paper at ICLR 2020 an improvement on previously learned tasks, and it was the ﬁrst method capable of learning using a single training example. Recent approaches in this category have mitigated forgetting by using external data combined with distillation loss and/or conﬁdence-based sampling strategies to select the most representative samples. (Castro et al., 2018; Wu et al., 2019; Lee et al., 2019) Regularization methods: In these approaches, signiﬁcant changes to the representation learned for previous tasks are prevented. This can be performed through regularizing the objective function or directly enforced on weight parameters. Typically, thisimportance measure is engineered to represent the importance of each parameter. Inspired by Bayesian learning, in elastic weight consolidation (EWC) method (Kirkpatrick et al., 2017) important parameters are those to have the highest in terms of the Fisher information matrix. In Synaptic Intelligence (SI) (Zenke et al., 2017) this parameter importance notion is engineered to correlate with the loss function: parameters that contribute more to the loss are more important. Similar to SI, Memory-aware Synapses (MAS) (Aljundi et al., 2018) proposed an online way of computing importance adaptive to the test set using the change in the model outputs w.r.t the inputs. While all the above algorithms are task-dependent, in parallel development to this work, (Aljundi et al., 2019) has recently investigated task-free continual learning by building upon MAS and using a protocol to update the weights instead of waiting until the tasks are ﬁnished. PackNet (Mallya & Lazebnik, 2018) used iterative pruning to fully restrict gradient updates on important weights via binary masks. This method requires knowing which task is being tested to use the appropriate mask. PackNet also ranks the weight importance by their magnitude which is not guaranteed to be a proper importance indicative. HAT (Serra et al., 2018) identiﬁes important neurons by learning an attention vector to the task embedding to control the gradient propagation. It maintains the information learned on previous tasks using an almost-binary mask per previous tasks. Bayesian approaches: Using Bayesian approach in learning neural networks has been studied for few decades (MacKay, 1992b;a). Several approaches have been proposed for Bayesian neural networks, based on, e.g., the Laplace approximation (MacKay, 1992a), Hamiltonian Monte Carlo (Neal, 2012), variational inference (Hinton & Van Camp, 1993; Graves, 2011), and probabilistic backpropagation (Hern´andez-Lobato & Adams, 2015). Variational continual learning (Nguyen et al., 2018) uses Bayesian inference to perform continual learning where new posterior distribution is simply obtained by multiplying the previous posterior by the likelihood of the dataset belonging to the new task. They also showed that by using a core-set, a small representative set of data from previous tasks, VCL can experience less forgetting. In contrast, we rely on Bayesian neural networks to use their predictive uncertainty to perform continual learning. Moreover, we do not use episodic memory or any other way to access or store previous data in our approach. Natural gradient descent methods:A fast natural gradient descent method for variational inference was introduced in (Khan & Nielsen, 2018) in which, the Fisher Information matrix is approximated using the generalized Gauss-Newton method. In contrast, in our work, we use classic gradient descent. Although second order optimization algorithms are proven to be more accurate than the ﬁrst order methods, they add considerable computational cost. Tseran et al. (2018); Chen et al. (2019) both investigate the effect of natural gradient descent methods as an alternative to classic gradient descent used in VCL and EWC methods. GNG (Chen et al., 2019) uses Gaussian natural gradients in the Adam optimizer (Kingma & Ba, 2014) in the framework of VCL because as opposed to conventional gradient methods which perform in Euclidian space, natural gradients cause a small difference in terms of distributions following the changes in parameters in the Riemannian space. Similar to VCL, they obtained their best performance by adding a coreset of previous examples. Tseran et al. (2018) introduce two modiﬁcations to VCL called Natural-VCL (N-VCL) and VCL-Vadam. N-VCL (Tseran et al., 2018) uses a Gauss-Newton approximation introduced by (Schraudolph, 2002; Graves, 2011) to estimate the VCL objective function and used natural gradient method proposed in (Khan et al., 2018) to exploit the Riemannian geometry of the variational posterior by scaling the gradient with an adaptive learning rate equal to σ−2 obtained by approximating the Fisher Information matrix in an online fashion. VCL-Vadam (Tseran et al., 2018) is a simpler version of N-VCL to trade-off accuracy for simplicity which uses Vadam (Khan et al., 2018) to update the gradients by perturbing the weights with a Gaussian noise using a reparameterization trick and scaling by σ−1 instead of its squared. N-VCL/VCL-Vadam both use variational inference to adapt the learning rate within Adam optimizer at every time step, whereas in our method below, gradient decent is used with constant learning rate during each task where learning rate scales with uncertainty only after ﬁnishing a task. We show extensive comparison with state-of-the-art results on short and relatively long sequence of vision datasets with Bayesian convolutional neural networks, whereas VCL-Vadam only rely on 3Published as a conference paper at ICLR 2020 multi-layer perceptron networks. We also like to highlight that this is the ﬁrst work which evaluates and shows the working of convolutional Bayesian Neural Networks rather than only fully connected MLP models for continual learning. 3 B ACKGROUND : VARIATIONAL BAYES -BY-BACKPROP In this section, we review the Bayes-by-Backprop (BBB) framework which was introduced by (Blundell et al., 2015); to learn a probability distribution over network parameters. (Blundell et al., 2015) showed a back-propagation-compatible algorithm which acts as a regularizer and yields comparable performance to dropout on the MNIST dataset. In Bayesian models, latent variables are drawn from a prior density p(w) which are related to the observations through the likelihood p(x|w). During inference, the posterior distribution p(w|x) is computed conditioned on the given input data. However, in practice, this probability distribution is intractable and is often estimated through approximate inference. Markov Chain Monte Carlo (MCMC) sampling (Hastings, 1970) has been widely used and explored for this purpose, see (Robert & Casella, 2013) for different methods under this category. However, MCMC algorithms, despite providing guarantees for ﬁnding asymptotically exact samples from the target distribution, are not suitable for large datasets and/or large models as they are bounded by speed and scalability issues. Alternatively, variational inference provides a faster solution to the same problem in which the posterior is approximated using optimization rather than being sampled from a chain (Hinton & Van Camp, 1993). Variational inference methods always take advantage of fast optimization techniques such as stochastic methods or distributed methods, which allow them to explore data models quickly. See (Blei et al., 2017) for a complete review of the theory and (Shridhar et al., 2018) for more discussion on how to use Bayes by Backprop (BBB) in convolutioal neural networks. 3.1 B AYES BY BACKPROP (BBB) Let x ∈I Rn be a set of observed variables and w be a set of latent variables. A neural network, as a probabilistic model P(y|x,w), given a set of training examples D= (x,y) can output y which belongs to a set of classes by using the set of weight parameters w. Variational inference aims to calculate this conditional probability distribution over the latent variables by ﬁnding the closest proxy to the exact posterior by solving an optimization problem. We ﬁrst assume a family of probability densities over the latent variables w parametrized by θ, i.e., q(w|θ). We then ﬁnd the closest member of this family to the true conditional probability of interest P(w|D) by minimizing the Kullback-Leibler (KL) divergence between qand P which is equivalent to minimizing variational free energy or maximizing the expected lower bound: θ∗= arg minθKL ( q(w|θ)∥P(w|D) ) (1) The objective function can be written as: LBBB(θ,D) = KL [ q(w|θ)∥P(w) ] −Eq(w|θ) [ log(P(D|w)) ] (2) Eq. 2 can be approximated using N Monte Carlo samples wi from the variational posterior (Blundell et al., 2015): LBBB(θ,D) ≈ N∑ i=1 log q(wi|θ) −log P(wi) −log(P(D|wi)) (3) We assume q(w|θ) to have a Gaussian pdf with diagonal covariance and parametrized by θ= (µ,ρ). A sample weight of the variational posterior can be obtained by sampling from a unit Gaussian and reparametrized by w = µ+ σ◦ϵ where ϵ is the noise drawn from unit Gaussian, and ◦is a pointwise multipliation. Standard deviation is parametrized as σ = log(1 + exp( ρ)) and thus is always positive. For the prior, as suggested by Blundell et al. (2015), a scale mixture of two Gaussian pdfs are chosen which are zero-centered while having different variances ofσ2 1 and σ2 2. The uncertainty obtained for every parameter has been successfully used in model compression (Han et al., 2015) and uncertainty-based exploration in reinforcement learning (Blundell et al., 2015). In this work we propose to use this framework to learn sequential tasks without forgetting using per-weight uncertainties. 4Published as a conference paper at ICLR 2020 4 U NCERTAINTY -GUIDED CONTINUAL LEARNING IN BAYESIAN NEURAL NETWORKS In this section, we introduce Uncertainty-guided Continual learning approach with Bayesian neural networks (UCB), which exploits the estimated uncertainty of the parameters’ posterior distribution to regulate the change in “important” parameters both in a soft way (Section 4.1) or setting a hard threshold (Section 4.2). 4.1 UCB WITH LEARNING RATE REGULARIZATION A common strategy to perform continual learning is to reduce forgetting by regularizing further changes in the model representation based on parameters’importance. In UCB the regularization is performed with the learning rate such that the learning rate of each parameter and hence its gradient update becomes a function of its importance. As shown in the following equations, in particular, we scale the learning rate of µand ρfor each parameter distribution inversely proportional to its importance Ω to reduce changes in important parameters while allowing less important parameters to alter more in favor of learning new tasks. αµ ←αµ/Ωµ (4) αρ ←αρ/Ωρ (5) The core idea of this work is to base the deﬁnition of importance on the well-deﬁned uncertainty in parameters distribution of Bayesian neural networks, i.e., setting the importance to be inversely proportional to the standard deviation σwhich represents the parameter uncertainty in the Baysian neural network: Ω ∝1/σ (6) We explore different options to set Ω in our ablation study presented in Section A.2 of the appendix, Table 1. We empirically found that Ωµ = 1/σand not adapting the learning rate for ρ(i.e. Ωρ = 1) yields the highest accuracy and the least forgetting. The key beneﬁt of UCB with learning rate as the regularizer is that it neither requires additional memory, as opposed to pruning technique nor tracking the change in parameters with respect to the previously learned task, as needed in common weight regularization methods. More importantly, this method does not need to be aware of task switching as it only needs to adjust the learning rates of the means in the posterior distribution based on their current uncertainty. The complete algorithm for UCB is shown in Algorithm 1 with parameter update function given in Algorithm 2. 4.2 UCB USING WEIGHT PRUNING (UCB-P) In this section, we introduce a variant of our method, UCB-P, which is related to recent efforts in weight pruning in the context of reducing inference computation and network compression (Liu et al., 2017; Molchanov et al., 2016). More speciﬁcally, weight pruning has been recently used in continual learning (Mallya & Lazebnik, 2018), where the goal is to continue learning multiple tasks using a single network’s capacity. (Mallya & Lazebnik, 2018) accomplished this by freeing up parameters deemed to be unimportant to the current task according to their magnitude. Forgetting is prevented in pruning by saving a task-speciﬁc binary mask of important vs. unimportant parameters. Here, we adapt pruning to Bayesian neural networks. Speciﬁcally, we propose a different criterion for measuring importance: the statistically-grounded uncertainty deﬁned in Bayesian neural networks. Unlike regular deep neural networks, in a BBB model weight parameters are represented by proba- bility distributions parametrized by their mean and standard deviation. Similar to (Blundell et al., 2015), in order to take into account both mean and standard deviation, we use the signal-to-noise ratio (SNR) for each parameter deﬁned as Ω = SNR = |µ|/σ (7) 5Published as a conference paper at ICLR 2020 Algorithm 1Uncertainty-guided Continual Learning with Bayesian Neural Networks UCB 1: Require Training data for all tasks D= (x,y), µ(mean of posterior), ρ, σ1 and σ2 (std for the scaled mixture Gaussian pdf of prior), π(weighting factor for prior), N (number of samples in a mini-batch), M (Number of minibatches per epoch), initial learning rate (α0) 2: αµ = αρ = α0 3: for every task do 4: repeat 5: ϵ∼N(0,I) 6: σ= log(1 + exp(ρ)) ⊿ Ensures σis always positive 7: w = µ+ σ◦ϵ ⊿w = {w1,..., wi,..., wN}posterior samples of weights 8: l1 = ∑N i=1 log N(wi|µ,σ2) ⊿ l1 := Log-posterior 9: l2 = ∑N i=1 log ( πN(wi |0,σ2 1) + (1−π)N(wi |0,σ2 2) ) ⊿ l2 := Log-prior 10: l3 = ∑N i=1 log(p(D|wi)) ⊿ l3 := Log-likelihood of data 11: LBBB = 1 M(l1 −l2 −l3 ) 12: µ←µ−αµ∇LBBBµ 13: ρ←ρ−αρ∇LBBBρ 14: until loss plateaus 15: αµ,αρ ←LearningRateUpdate(αµ,αρ,σ,µ) ⊿ See Algorithm 2 for UCB and 3 for UCB-P 16: end for Algorithm 2LearningRateUpdate in UCB 1: function LearningRateUpdate(αµ,αρ,σ) 2: for each parameter do 3: Ωµ ←1/σ 4: Ωρ ←1 5: αµ ←αµ/Ωµ 6: αρ ←αρ/Ωρ 7: end for 8: end function Algorithm 3LearningRateUpdate in UCB-P 1: function LearningRateUpdate(αµ,αρ,σ,µ) 2: for each parameter jin each layer ldo 3: Ω ←|µ|/σ ⊿ Signal to noise ratio 4: if Ω[j] ∈top p% of Ωs in lthen 5: αµ = αρ = 0 6: end if 7: end for 8: end function SNR is a commonly used measure in signal processing to distinguish between “useful” information from unwanted noise contained in a signal. In the context of neural models, the SNR can be thought as an indicative of parameter importance; the higher the SNR, the more effective or important the parameter is to the model predictions for a given task. UCB-P, as shown in Algorithms 1 and 3, is performed as follows: for every layer, convolutional or fully-connected, the parameters are ordered by their SNR value and those with the lowest importance are pruned (set to zero). The pruned parameters are marked using a binary mask so that they can be used later in learning new tasks whereas the important parameters remain ﬁxed throughout training on future tasks. Once a task is learned, an associated binary mask is saved which will be used during inference to recover key parameters and hence the exact performance to the desired task. The overhead memory per parameter in encoding the mask as well as saving it on the disk is as follows. Assuming we have ntasks to learn using a single network, the total number of required bits to encode an accumulated mask for a parameter is at max log2 nbits assuming a parameter deemed to be important from task 1 and kept being encoded in the mask. 5 R ESULTS 5.1 E XPERIMENTAL SETUP Datasets: We evaluate our approach in two common scenarios for continual learning: 1) class- incremental learning of a single or two randomly alternating datasets, where each task covers only a subset of the classes in a dataset, and 2) continual learning of multiple datasets, where each task is a dataset. We use Split MNIST with 5 tasks (5-Split MNIST) similar to (Nguyen et al., 2018; Chen et al., 2019; Tseran et al., 2018) and permuted MNIST (Srivastava et al., 2013) for class incremental learning with similar experimental settings as used in (Serra et al., 2018; Tseran et al., 2018). Furthermore, to have a better understanding of our method, we evaluate our approach on continually learning a sequence of 8 datasets with different distributions using the identical sequence 6Published as a conference paper at ICLR 2020 as in (Serra et al., 2018), which includes FaceScrub (Ng & Winkler, 2014), MNIST, CIFAR100, NotMNIST (Bulatov, 2011), SVHN (Netzer et al., 2011), CIFAR10, TrafﬁcSigns (Stallkamp et al., 2011), and FashionMNIST (Xiao et al., 2017). Details of each are summarized in Table 4 in appendix. No data augmentation of any kind has been used in our analysis. Baselines: Within the Bayesian framework, we compare to three models which do not incorporate the importance of parameters, namely ﬁne-tuning, feature extraction, and joint training. In ﬁne-tuning (BBB-FT), training continues upon arrival of new tasks without any forgetting avoidance strategy. Feature extraction, denoted as (BBB-FE), refers to freezing all layers in the network after training the ﬁrst task and training only the last layer for the remaining tasks. In joint training (BBB-JT) we learn all the tasks jointly in a multitask learning fashion which serves as the upper bound for average accuracy on all tasks, as it does not adhere to the continual learning scenario. We also perform the counterparts for FT, FE, and JT using ordinary neural networks and denote them as ORD-FT, ORD- FE, and ORD-JT. From the prior work, we compare with state-of-the-art approaches including Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), Incremental Moment Matching (IMM) (Lee et al., 2017), Learning Without Forgetting (LWF) (Li & Hoiem, 2016), Less-Forgetting Learning (LFL) (Jung et al., 2016), PathNet (Fernando et al., 2017), Progressive neural networks (PNNs) (Rusu et al., 2016), and Hard Attention Mask (HAT) (Serra et al., 2018) using implementations provided by (Serra et al., 2018). On Permuted MNIST results for SI (Zenke et al., 2017) are reported from (Serra et al., 2018). On Split and Permuted MNIST, results for VCL (Nguyen et al., 2018) are obtained using their original provided code whereas for VCL-GNG (Chen et al., 2019) and VCL-Vadam (Tseran et al., 2018) results are reported from the original work without re-implementation. Because our method lies into the regularization-based regime, we only compare against baselines which do not beneﬁt from episodic or coreset memory. Hyperparameter tuning: Unlike commonly used tuning techniques which use a validation set composed of all classes in the dataset, we only rely on the ﬁrst two task and their validations set, similar to the setup in (Chaudhry et al., 2019). In all our experiments we consider a 0.15 split for the validation set on the ﬁrst two tasks. After tuning, training starts from the beginning of the sequence. Our scheme is different from (Chaudhry et al., 2019), where the models are trained on the ﬁrst (e.g. three) tasks for validation and then training is restarted for the remaining ones and the reported performance is only on the remaining tasks. Training details:It is important to note that in all our experiments, no pre-trained model is used. We used stochastic gradient descent with a batch size of 64 and a learning rate of 0.01, decaying it by a factor of 0.3 once the loss plateaued. Dataset splits and batch shufﬂe are identically in all UCB experiments and all baselines. Pruning procedure and mask size: Once a task is learned, we compute the performance drop for a set of arbitrary pruning percentages from the maximum training accuracy achieved when no pruning is applied. The pruning portion is then chosen using a threshold beyond which the performance drop is not accepted. Mask size is chosen without having the knowledge of how many tasks to learn in the future. Upon learning each task we used a uniform distribution of pruning ratios (50-100%) and picked the ratio resulted in at most 1%, 2%, and 3% forgetting for MNIST, CIFAR, and 8tasks experiments, respectively. We did not tune this parameter because in our hyperparameter tuning, we only assume we have validation sets of the ﬁrst two tasks. Parameter regularization and importance measurement:Table 1 ablates different ways to com- pute the importance Ω of an parameter in Eq. 4 and 5. As shown in Table 1 the conﬁguration that yields the highest accuracy and the least forgetting (maximum BWT) occurs when the learning rate regularization is performed only on µof the posteriors using Ωµ = 1/σas the importance and Ωρ = 1. Performance measurement:Let nbe the total number of tasks. Once all are learned, we evaluate our model on all n tasks. ACC is the average test classiﬁcation accuracy across all tasks. To measure forgetting we report backward transfer, BWT, which indicates how much learning new tasks has inﬂuenced the performance on previous tasks. While BWT <0 directly reports catastrophic forgetting, BWT >0 indicates that learning new tasks has helped with the preceding tasks. Formally, BWT and ACC are as follows: BWT = 1 n n∑ i=1 Ri,n −Ri,i, ACC = 1 n n∑ i=1 Ri,n (8) 7Published as a conference paper at ICLR 2020 Table 1: Variants of learning rate regularization and importance measurement on 2-Split MNIST Method µ ρ Importance Ω BWT (%) ACC (%) UCB x - 1/σ 0.00 99 .2 UCB - x 1/σ −0.04 98 .7 UCB x x 1/σ −0.02 98 .0 UCB x - |µ|/σ −0.03 98 .4 UCB - x |µ|/σ −0.52 98 .7 UCB x x |µ|/σ −0.32 98 .8 UCB-P x x |µ|/σ −0.01 99 .0 UCB-P x x 1/σ −0.01 98 .9 Table 2: Continually learning on different datasets. BWT and ACC in %. (*) denotes that methods do not adhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORD networks, respectively. ‡denotes results reported by (Serra et al., 2018). †denotes the result reported from original work. BWT was not reported in ‡and †. All others results are (re)produced by us and are averaged over 3 runs with standard deviations given in Section A.3 of the appendix. (a) 5-Split MNIST, 5 tasks. Method BWT ACC VCL-Vadam† - 99.17 VCL-GNG† - 96.50 VCL - 0.56 98 .20 IMM - 11.20 88 .54 EWC - 4.20 95 .78 HAT 0.00 99 .59 ORD-FT - 9.18 90 .60 ORD-FE 0.00 98 .54 BBB-FT - 6.45 93 .42 BBB-FE 0.00 98 .76 UCB-P (Ours) - 0.72 99 .32 UCB (Ours) 0.00 99.63 ORD-JT∗ 0.00 99 .78 BBB-JT∗ 0.00 99 .87 (b) Permuted MNIST, 10 permutations. Method #Params BWT ACC SI ‡ 0.1M - 86.0 EWC ‡ 0.1M - 88.2 HAT ‡ 0.1M - 91.6 VCL-Vadam†0.1M - 86.34 VCL-GNG† 0.1M - 90.50 VCL 0.1M - 7.90 88.80 UCB (Ours) 0.1M -0.38 91.44 LWF 1.9M - 31.17 65.65 IMM 1.9M - 7.14 90.51 HAT 1.9M 0.03 97.34 BBB-FT 1.9M - 0.58 90.01 BBB-FE 1.9M 0.02 93.54 UCB-P (Ours) 1.9M - 0.95 97.24 UCB (Ours) 1.9M 0.03 97.42 BBB-JT∗ 1.9M 0.00 98.12 (c) Alternating CIFAR10/100 Method BWT ACC PathNet 0.00 28 .94 LWF - 37.9 42 .93 LFL - 24.22 47 .67 IMM - 12.23 69 .37 PNN 0.00 70 .73 EWC - 1.53 72 .46 HAT - 0.04 78 .32 BBB-FE - 0.04 51 .04 BBB-FT - 7.43 68 .89 UCB-P (Ours) - 1.89 77 .32 UCB (Ours) -0.72 79.44 BBB-JT∗ 1.52 83 .93 (d) Sequence of 8 tasks Method BWT ACC LFL - 10.0 8 .61 PathNet 0.00 20 .22 LWF - 54.3 28 .22 IMM - 38.5 43 .93 EWC - 18.04 50 .68 PNN 0.00 76 .78 HAT - 0.14 81 .59 BBB-FT - 23.1 43 .09 BBB-FE - 0.01 58 .07 UCB-P (Ours) - 2.54 80 .38 UCB (Ours) -0.84 84.04 BBB-JT∗ -1.2 84 .1 where Ri,n is the test classiﬁcation accuracy on task iafter sequentially ﬁnishing learning the nth task. Note that in UCB-P, Ri,i refers the test accuracy on taskibefore pruning and Ri,n after pruning which is equivalent to the end of sequence performance. In Section 6, we show that our UCB model can be used when tasks labels are not available at inference time by training it with a “single head” architecture with a sum of number of classes for all tasks. We refer to the ACC measured for this scenario as “Generalized Accuracy”. 5.2 5-S PLIT MNIST We ﬁrst present our results for class incremental learning of MNIST (5-Split MNIST) in which we learn the digits 0 −9 in ﬁve tasks with 2 classes at a time in 5 pairs of 0/1, 2/3, 4/5, 6/7, and 8/9. Table 2a shows the results for reference baselines in Bayesian and non-Bayesian neural networks including ﬁne-tuning ( BBB-FT, ORD-FT), feature extraction ( BBB-FE, ORD-FE) and, joint training (BBB-JT, ORD-JT) averaged over 3 runs and standard deviations are given in Table 9 in the appendix. Although the MNIST dataset is an “easy” dataset, we observe throughout all experiments that Bayesian ﬁne-tuning and joint training perform signiﬁcantly better than their counterparts, ORD-FT and ORD-JT. For Bayesian methods, we compare against VCL and its variations named as VCL with Variational Adam (VCL-Vadam), VCL with Adam and Gaussian natural gradients (VCL-GNG). For non-Bayesian methods, we compare against HAT, IMM, and EWC (EWC can be regarded as Bayesian-inspired). VCL-Vadam (ACC= 99.17%) appears to be outperforming VCL (ACC=98.20%) and VCL-GNG (ACC=96.50%) in average accuracy. However, full comparison is not possible because forgetting was not reported for Vadam and GNG. Nevertheless, UCB (ACC=99.63%) is able to surpass all the baselines including VCL-Vadam in average accuracy while in zero forgetting it is on par with HAT (ACC=99.59%). We also report results on incrementally learning MNIST in two tasks (2-Split MNIST) in Table 8 in the appendix, where we compare it 8Published as a conference paper at ICLR 2020 against PackNet, HAT, and LWF where PackNet, HAT,UCB-P, and UCB have zero forgetting while UCB has marginally higher accuracy than all others. 5.3 P ERMUTED MNIST Permuted MNIST is a popular variant of the MNIST dataset to evaluate continual learning approaches in which each task is considered as a random permutation of the original MNIST pixels. Following the literature, we learn a sequence of 10 random permutations and report average accuracy at the end. Table 2b shows ACC and BWT of UCB and UCB-P in comparison to state-of-the-art models using a small and a large network with 0.1M and 1.9M parameters, respectively (architecture details are given in Section A.2 of the appendix). The accuracy achieved by UCB (ACC=91.44 ±0.04%) using the small network outperforms the ACC reported by Serra et al. (2018) for SI (ACC=86.0%), EWC (ACC=88.2%), while HAT attains a slightly better performance (ACC=91.6%). Comparing the average accuracy reported in VCL-Vadam (ACC=86.34%) and VCL-GNG (ACC=90.50%) as well as obtained results for VCL (ACC=88.80%) shows UCB with BWT=(0.03% ±0.00%) is able to outperform other Bayesian approaches in accuracy while forgetting signiﬁcantly less compared to VCL with BWT=−7.9%. While we do not experiment with memory in this work, not surprisingly adding memory to most approaches will improve their performance signiﬁcantly as it allows looking into past tasks. E.g. Chen et al. (2019) report ACC=94.37% for VCL-GNC when adding a memory of size 200. Next, we compare the results for the larger network (1.9M). While HAT andUCB have zero forgetting, UCB, reaching ACC=97.42±0.01%, performs better than all baselines including HAT which obtains ACC=97.34 ±0.05% using 1.9M parameters. We also observe again that BBB-FT, despite being not speciﬁcally penalized to prevent forgetting, exhibits reasonable negative BWT values, performing better than IMM and LWF baselines. It is close to joint training, BBB-JT, with ACC=98.1%, which can be seen as an upper bound. 5.4 A LTERNATING CIFAR10 AND CIFAR100 In this experiment, we randomly alternate between class incremental learning of CIFAR10 and CIFAR100. Both datasets are divided into 5 tasks each with 2 and 20 classes per task, respectively. Table 2c presents ACC and BWT obtained with UCB-P, UCB, and three BBB reference methods compared against various continual learning baselines. Among the baselines presented in Table 2c, PNN and PathNet are the only zero-forgetting-guaranteed approaches. It is interesting to note that in this setup, some baselines (PathNet, LWF, and LFL) do not perform better than the naive accuracy achieved by feature extraction. PathNet suffers from bad pre-assignment of the network’s capacity per task which causes poor performance on the initial task from which it never recovers. IMM performs almost similar to ﬁne-tuning in ACC, yet forgets more. PNN, EWC, and HAT are the only baselines that perform better than BBB-FE and BBB-FT. EWC and HAT are both allowed to forget by construction, however, HAT shows zero forgetting behavior. While EWC is outperformed by both of our UCB variants, HAT exhibits 1% better ACC over UCB-P. Despite having a slightly higher forgetting, the overall accuracy of UCB is higher, reaching 79.4%. BBB-JT in this experiment achieves a positive BWT which shows that learning the entire sequence improves the performance on earlier tasks. 5.5 M ULTIPLE DATASETS LEARNING Finally, we present our results for continual learning of 8 tasks using UCB-P and UCB in Table 2d. Similar to the previous experiments we look at both ACC and BWT obtained forUCB-P, UCB, BBB references (FT, FE, JT) as well as various baselines. Considering the ACC achieved by BBB-FE or BBB-FT (58.1%) as a lower bound we observe again that some baselines are not able to do better than BBB-FT including LFL, PathNet, LWF, IMM, and EWC while PNN and HAT remain the only strong baselines for our UCB-P and UCB approaches. UCB-P again outperforms PNN by 3.6% in ACC. HAT exhibits only−0.1% BWT, but our UCB achieves2.4% higher ACC. 6 S INGLE HEAD AND GENERALIZED ACCURACY OF UCB UCB can be used even if the task information is not given at test time. For this purpose, at training time, instead of using a separate fully connected classiﬁcation head for each task, we use a single 9Published as a conference paper at ICLR 2020 Table 3: Single Head vs. Multi-Head architecture and Generalized vs. Standard Accuracy. Generalized accuracy means that task information is not available at test time. SM, PM, CF, and 8T denote the 5-Split MNIST, Permuted MNIST, Alternating CIFAR10/100, and sequence of8 tasks, respectively. Generalized ACC ACC Single Head Single Head Multi Head Exp UCB BBB-FT UCB BBB-FT UCB BBB-FT SM 98.7 98 .1 98.9 98 .7 99.2 98 .4 PM 92.5 86 .1 95.1 88 .3 97.7 90 .0 CF 71.2 65 .2 74.3 67 .8 79.4 68 .9 8T 76.8 47 .6 79.9 53 .2 84.0 43 .1 head with the total number of outputs for all tasks. For example in the 8-dataset experiment we only use one head with 293 number of output classes, rather than using 8 separate heads, during training and inference time. Table 3 presents our results for UCB and BBB-FT trained with a single head against having a multi-head architecture, in columns 4-7. Interestingly, we see only a small performance degrade for UCB from training with multi-head to a single head. The ACC reduction is 0.3%, 2.6%, 5.1%, and 4.1% for 2-Split MNIST, Permuted MNIST, Alternating CIFAR10/100, and sequence of8 tasks experiments, respectively. We evaluatedUCB and BBB-FT with a more challenging metric where the prediction space covers the classes across all the tasks. Hence, confusion of similar class labels across tasks can be measured. Performance for this condition is reported as Generalized ACC in Table 3 in columns2-3. We observe a small performance reduction in going from ACC to Generalized ACC, suggesting non-signiﬁcant confusion caused by the presence of more number of classes at test time. The performance degradation from ACC to Generalized ACC is0.2%, 2.6%, 3.1%, and 3.1% for 2-Split MNIST, Permuted MNIST, Alternating CIFAR10/100, and sequence of 8 tasks, respectively. This shows that UCB can perform competitively in more realistic conditions such as unavailability of task information at test time. We believe the main insight of our approach is that instead of computing additional measurements of importance, which are often task, input or output dependent, we directly use predicted weight uncertainty to ﬁnd important parameters. We can freeze them using a binary mask, as in UCB-P, or regularize changes conditioned on current uncertainty, as in UCB. 7 C ONCLUSION In this work, we propose a continual learning formulation with Bayesian neural networks, called UCB, that uses uncertainty predictions to perform continual learning: important parameters can be either fully preserved through a saved binary mask (UCB-P) or allowed to change conditioned on their uncertainty for learning new tasks (UCB). We demonstrated how the probabilistic uncertainty distributions per weight are helpful to continually learning short and long sequences of benchmark datasets compared against baselines and prior work. We show that UCB performs superior or on par with state-of-the-art models such as HAT (Serra et al., 2018) across all the experiments. Choosing between the two UCB variants depends on the application scenario: While UCB-P enforces no forgetting after the initial pruning stage by saving a small binary mask per task, UCB does not require additional memory and allows for more learning ﬂexibility in the network by allowing small forgetting to occur. UCB can also be used in a single head setting where the right subset of classes belonging to the task is not known during inference leading to a competitive model that can be deployed where it is not possible to distinguish tasks in a continuous stream of the data at test time. UCB can also be deployed in a single head scenario and where tasks information is not available at test time. REFERENCES Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. InProceedings of the European Conference on Computer Vision (ECCV), pp. 139–154, 2018. 10Published as a conference paper at ICLR 2020 Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 11254–11263, 2019. David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859–877, 2017. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1613–1622. PMLR, 2015. Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available: http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2011. Francisco M Castro, Manuel J Mar´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 233–248, 2018. Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with A-GEM. In International Conference on Learning Representations, 2019. Yu Chen, Tom Diethe, and Neil Lawrence. Facilitating bayesian continual learning by natural gradients and stein gradients. arXiv preprint arXiv:1904.10644, 2019. Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Alex Graves. Practical variational inference for neural networks. In Advances in neural information processing systems, pp. 2348–2356, 2011. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. Biometrika, 1970. Jos´e Miguel Hern´andez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International Conference on Machine Learning, pp. 1861–1869, 2015. Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pp. 5–13. ACM, 1993. Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim. Less-forgetting learning in deep neural networks. arXiv preprint arXiv:1607.00122, 2016. Mohammad Emtiyaz Khan and Didrik Nielsen. Fast yet simple natural-gradient descent for variational inference in complex models. In 2018 International Symposium on Information Theory and Its Applications (ISITA), pp. 31–35. IEEE, 2018. Mohammad Emtiyaz Khan, Didrik Nielsen, V oot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivas- tava. Fast and scalable bayesian deep learning by weight-perturbation in adam. arXiv preprint arXiv:1806.04854, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. 11Published as a conference paper at ICLR 2020 Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with unlabeled data in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp. 312–321, 2019. Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, pp. 4652–4662, 2017. Zhizhong Li and Derek Hoiem. Learning without forgetting. In European Conference on Computer Vision, pp. 614–629. Springer, 2016. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn- ing efﬁcient convolutional networks through network slimming. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2736–2744, 2017. David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computa- tion, 4(3):448–472, 1992a. David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992b. Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. James L McClelland, Bruce L McNaughton, and Randall C O’reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995. Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165. Elsevier, 1989. Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efﬁcient inference. In International Conference on Learning Repre- sentations (ICLR), 2016. Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011. Hong-Wei Ng and Stefan Winkler. A data-driven approach to cleaning large face datasets. In Image Processing (ICIP), 2014 IEEE International Conference on, pp. 343–347. IEEE, 2014. Cuong V . Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018. Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science & Business Media, 2013. Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2): 123–146, 1995. 12Published as a conference paper at ICLR 2020 Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural computation, 14(7), 2002. Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame- work for continual learning. arXiv preprint arXiv:1805.06370, 2018. Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4548–4557. PMLR, 2018. Kumar Shridhar, Felix Laumann, and Marcus Liwicki. Uncertainty estimations by softplus nor- malization in bayesian convolutional neural networks with variational inference. arXiv preprint arXiv:1806.05978, 2018. Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and J¨urgen Schmidhu- ber. Compete to compute. In Advances in neural information processing systems, pp. 2310–2318, 2013. Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german trafﬁc sign recognition benchmark: a multi-class classiﬁcation competition. In Neural Networks (IJCNN), The 2011 International Joint Conference on, pp. 1453–1460. IEEE, 2011. Hanna Tseran, Mohammad Emtiyaz Khan, Tatsuya Harada, and Thang D Bui. Natural variational continual learning. In Continual Learning Workshop@ NeurIPS, volume 2, 2018. Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 374–382, 2019. Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for bench- marking machine learning algorithms, The MIT License (MIT) Copyright c⃝2017 Zalando SE. https://tech.zalando.com, arXiv preprint arXiv:1708.07747, 2017. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. In International Conference on Learning Representations, 2018. Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Doina Precup and Yee Whye Teh (eds.),Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3987–3995. PMLR, 2017. 13Published as a conference paper at ICLR 2020 A A PPENDIX A.1 D ATASETS Table 4 shows a summary of the datasets utilized in our work along with their size and number of classes. In all the experiments we resized images to 32 ×32 ×3 if necessary. For datasets with monochromatic images, we replicate the image across all RGB channels. Table 4: Utilized datasets summary Names #Classes Train Test FaceScrub (Ng & Winkler, 2014) 100 20,600 2,289 MNIST (LeCun et al., 1998) 10 60,000 10,000 CIFAR100 (Krizhevsky & Hinton, 2009) 100 50,000 10,000 NotMNIST (Bulatov, 2011) 10 16,853 1,873 SVHN (Netzer et al., 2011) 10 73,257 26,032 CIFAR10 (Krizhevsky & Hinton, 2009) 10 39,209 12,630 TrafﬁcSigns (Stallkamp et al., 2011) 43 39,209 12,630 FashionMNIST (Xiao et al., 2017) 10 60,000 10,000 A.2 I MPLEMENTATION DETAILS In this section we take a closer look at elements of our UCB model on MNIST and evaluate variants of parameter regularization, importance measurement, as well as the effect of the number of samples drawn from the posited posterior. Bayes-by-backprop (BBB) Hyperparamters:Table 5 shows the search space for hyperparamters in the BBB algorithm Blundell et al. (2015) which we used for tuning on the validation set of the ﬁrst two tasks. Table 5: Search space for hyperparamters in BBB given by Blundell et al. (2015) BBB hyperparamters −log σ1 −log σ2 π Search space {0,1,2} {6,7,8} {0.25,0.5,0.75} Network architecture:For Split MNIST and Permuted MNIST experiments, we have used a two- layer perceptron which has 1200 units. Because there is more number of parameters in our Bayesian neural network compared to its equivalent regular neural net, we ensured fair comparison by matching the total number of parameters between the two to be 1.9M unless otherwise is stated. For the multiple datasets learning scenario, as well as alternating incremental CIFAR10/100 datasets, we have used a ResNet18 Bayesian neural network with 7.1-11.3M parameters depending on the experiment. However, the majority of the baselines provided in this work are originally developed using some variants of AlexNet structure and altering that, e.g. to ResNet18, resulted in degrading in their reported and experimented performance as shown in Table 6. Therefore, we kept the architecture for baselines as AlexNet and ours as ResNet18 and only matched their number of parameters to ensure having equal capacity across different approaches. Table 6: Continually learning on CIFAR10/100 using AlexNet and ResNet18 for UCB (our method) and HAT (Serra et al., 2018). BWT and ACC in %. All results are (re)produced by us. Method BWT ACC HAT (AlexNet) 0.0 78 .3 HAT (ResNet18) −9.0 56 .8 UCB (AlexNet) −0.7 79 .44 UCB (ResNet18) −0.7 79 .70 14Published as a conference paper at ICLR 2020 Number of Monte Carlo samples:UCB is ensured to be robust to random noise using multiple samples drawn from posteriors. Here we explore different number of samples and the effect on ﬁnal performance for ACC and BWT. We have usedΩµ = 1/σas importance and regularization has been performed on mean values only. Following the result in Table 7 we chose the number of samples to be 10 for all experiments. Table 7: Number of Monte Carlo samples (N) in 2-Split MNIST Method N BWT (%) ACC (%) UCB 1 0 .00 98 .0 UCB 2 0 .00 98 .3 UCB 5 −0.15 99 .0 UCB 10 0 .00 99 .2 UCB 15 −0.01 98 .3 A.3 A DDITIONAL RESULTS Here we include some additional results such as Table 8 for 2-split MNIST and some complementary results for tables in the main text as follows: 9, 10, and 11 include standard deviation for results shown in Table 2a, 2b, 2c, respectively. Table 8: Continually learning on 2-Split MNIST. BWT and ACC in %. (*) denotes that methods do not adhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORD networks, respectively. All results are (re)produced by us. Method BWT ACC PackNet (Mallya & Lazebnik, 2018) 0.04 ±0.01 98 .91 ±0.03 LWF (Li & Hoiem, 2016) −0.22 ±0.04 99 .12 ±0.03 HAT (Serra et al., 2018) 0.01 ±0.00 99 .02 ±0.00 ORD-FT −6.81 ±0.03 92 .42 ±0.02 ORD-FE 0.04 ±0.04 97 .90 ±0.04 BBB-FT −0.61 ±0.03 98 .44 ±0.03 BBB-FE 0.02 ±0.05 98 .03 ±0.05 UCB-P (Ours) 0.03 ±0.04 99 .02 ±0.01 UCB (Ours) 0.01 ±0.00 99.18 ±0.01 ORD-JT∗ 0.02 ±0.03 99 .13 ±0.03 BBB-JT∗ 0.03 ±0.02 99 .51 ±0.02 Table 9: Continually learning on 5-Split MNIST. BWT and ACC in %. (*) denotes that methods do not adhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORD networks, respectively. All results are (re)produced by us. Method BWT ACC VCL-Vadam (Tseran et al., 2018) - 99.17 ±0.05 VCL-GNG (Chen et al., 2019) - 96.50 ±0.07 VCL (Nguyen et al., 2018) - 0.56 ±0.03 98 .20 ±0.03 IMM (Lee et al., 2017) - 11.20 ±1.57 88 .54 ±1.56 EWC (Kirkpatrick et al., 2017) - 4.20 ±1.08 95 .78 ±1.08 HAT (Serra et al., 2018) 0.00 ±0.02 99 .59 ±0.02 ORD-FT∗ -9.18 ±1.12 90 .60 ±1.12 ORD-FE∗ 0.00 ±1.56 98 .54 ±1.57 BBB-FT∗ -6.45 ±1.99 93 .42 ±1.98 BBB-FE∗ 0.00 ±2.23 98 .76 ±2.23 UCB-P (Ours) - 0.72 ±0.04 99 .32 ±0.04 UCB (Ours) 0.00 ±0.04 99.63 ±0.03 ORD-JT∗ 0.00 ±0.02 99 .78 ±0.02 BBB-JT∗ 0.00 ±0.01 99 .87 ±0.01 15Published as a conference paper at ICLR 2020 Table 10: Continually learning on Permuted MNIST. BWT and ACC in %. (*) denotes that method does not adhere to the continual learning setup: BBB-JT serves as the upper bound for ACC for BBB network. ‡denotes results reported by (Serra et al., 2018). †denotes the result reported from original work. BWT was not reported in ‡and †. All others results are (re)produced by us. Method #Params BWT ACC SI (Zenke et al., 2017)‡ 0.1M - 86.0 EWC (Kirkpatrick et al., 2017)‡ 0.1M - 88.2 HAT (Serra et al., 2018)‡ 0.1M - 91.6 VCL-Vadam† 0.1M - 93.34 VCL-GNG† 0.1M - 94.62 VCL 0.1M −7.90 ±0.23 88 .80 ±0.23 UCB (Ours) 0.1M −0.38 ±0.02 91 .44 ±0.04 LWF (Li & Hoiem, 2016) 1.9M −31.17 ±0.05 65 .65 ±0.05 IMM (Lee et al., 2017) 1.9M −7.14 ±0.07 90 .51 ±0.08 HAT (Serra et al., 2018) 1.9M 0 .03 ±0.05 97 .34 ±0.05 BBB-FT 1.9M −0.58 ±0.05 90 .01 ±0.05 BBB-FE 1.9M 0 .02 ±0.03 93 .54 ±0.04 UCB-P (Ours) 1.9M −0.95 ±0.06 97 .24 ±0.06 UCB (Ours) 1.9M 0 .03 ±0.00 97.42 ±0.01 BBB-JT∗ 1.9M 0 .00 ±0.00 98 .12 ±0.01 Table 11: Continually learning on CIFAR10/100. BWT and ACC in %. (*) denotes that method does not adhere to the continual learning setup: BBB-JT serves as the upper bound for ACC for BBB network. All results are (re)produced by us. Method BWT ACC PathNet (Fernando et al., 2017) 0.00 ±0.00 28 .94 ±0.03 LWF (Li & Hoiem, 2016) −37.9 ±0.32 42 .93 ±0.30 LFL (Jung et al., 2016) −24.22 ±0.21 47 .67 ±0.22 IMM (Lee et al., 2017) −12.23 ±0.06 69 .37 ±0.06 PNN (Rusu et al., 2016) 0.00 ±0.00 70 .73 ±0.08 EWC (Kirkpatrick et al., 2017) −1.53 ±0.07 72 .46 ±0.06 HAT (Serra et al., 2018) 0.04 ±0.06 78 .32 ±0.06 BBB-FE 0.04 ±0.02 51 .04 ±0.03 BBB-FT −7.43 ±0.07 68 .89 ±0.07 UCB-P (Ours) −1.89 ±0.03 77 .32 ±0.03 UCB (Ours) −0.72 ±0.02 79.44 ±0.02 BBB-JT∗ 1.52 ±0.04 83 .93 ±0.04 16",
      "meta_data": {
        "arxiv_id": "1906.02425v2",
        "authors": [
          "Sayna Ebrahimi",
          "Mohamed Elhoseiny",
          "Trevor Darrell",
          "Marcus Rohrbach"
        ],
        "published_date": "2019-06-06T05:40:25Z",
        "pdf_url": "https://arxiv.org/pdf/1906.02425v2.pdf",
        "github_url": "https://github.com/SaynaEbrahimi/UCB"
      },
      "llm_extracted_info": {
        "main_contributions": "The research addresses catastrophic forgetting in continual learning for models with fixed capacity and no access to past task data. It proposes Uncertainty-guided Continual Bayesian Neural Networks (UCB) which adapt the learning rate based on the uncertainty in the probability distribution of network weights to mitigate forgetting. A variant, UCB-P, is introduced, utilizing uncertainty for weight pruning and saving binary masks per task. The study extensively validates UCB's superior or on-par performance compared to existing approaches on diverse object classification datasets with varying task sequences. A key finding is that UCB does not necessarily require task information at test time, supporting a \"single head\" inference scenario. Additionally, it is highlighted as the first work to evaluate convolutional Bayesian Neural Networks for continual learning.",
        "methodology": "The core methodology relies on Bayesian Neural Networks (BNNs), which represent each weight parameter as a probability distribution (mean and variance), naturally accounting for uncertainty. The proposed UCB method uses this inherent uncertainty to regulate per-parameter learning rates: the learning rate for a parameter's mean is scaled inversely proportional to its standard deviation (uncertainty) after each task. The UCB-P variant employs weight pruning by defining parameter importance using the Signal-to-Noise Ratio (SNR = |mean|/standard deviation). Parameters with low SNR are pruned (set to zero), and task-specific binary masks are saved to fix important weights for subsequent tasks. Both methods build upon the Bayes-by-Backprop (BBB) framework, using variational inference with Gaussian posterior approximations and a reparameterization trick.",
        "experimental_setup": "The approach was evaluated in two scenarios: class-incremental learning (single or alternating datasets) and continual learning of multiple distinct datasets. Datasets included 5-Split MNIST, Permuted MNIST (10 permutations), and a sequence of 8 diverse datasets (FaceScrub, MNIST, CIFAR100, NotMNIST, SVHN, CIFAR10, TrafﬁcSigns, FashionMNIST), along with alternating CIFAR10/100. No data augmentation was used. Baselines included Bayesian (BBB-FT, BBB-FE, BBB-JT) and ordinary neural network (ORD-FT, ORD-FE, ORD-JT) counterparts, as well as state-of-the-art continual learning methods like EWC, IMM, LWF, LFL, PathNet, PNNs, HAT, SI, VCL, VCL-Vadam, and VCL-GNG, focusing on regularization-based approaches without episodic or coreset memory. Hyperparameter tuning was conducted using validation sets from the first two tasks. Training used SGD with a batch size of 64 and an initial learning rate of 0.01. Architectures included a two-layer perceptron for MNIST experiments (0.1M and 1.9M parameters) and ResNet18 for multi-dataset/CIFAR experiments (7.1-11.3M parameters), with parameter counts matched for fair comparison. Pruning percentages for UCB-P were chosen to limit forgetting to 1-3% without prior knowledge of future tasks. Performance was measured by average accuracy (ACC) and Backward Transfer (BWT), with an additional \"Generalized Accuracy\" metric for scenarios without task information at test time.",
        "limitations": "UCB-P, while enforcing no forgetting for pruned parameters, requires additional memory to save a small binary mask per task (log2 n bits per parameter for n tasks). The UCB variant, which uses soft learning rate regularization, allows for small amounts of forgetting to occur. The approach does not utilize episodic memory or coresets, which some competing methods leverage for improved performance, suggesting a limitation compared to memory-augmented strategies. Furthermore, the method adjusts learning rates only after finishing a task, unlike some natural gradient methods which adapt learning rates at every time step, which might limit its real-time adaptation capabilities.",
        "future_research_directions": "Not explicitly mentioned, but implied directions include: further optimizing the trade-off between forgetting prevention and memory consumption, potentially by exploring adaptive strategies that combine the strengths of UCB and UCB-P; investigating more dynamic and continuous learning rate adjustment mechanisms within the Bayesian framework, possibly integrating efficient natural gradient descent methods; and extending the application of convolutional Bayesian Neural Networks to a broader range of complex continual learning tasks and scenarios, building on this work's initial exploration. Improving performance robustness in the \"single-head\" inference scenario where task labels are unavailable at test time could also be a valuable direction.",
        "experimental_code": "import os,sys,time\nimport numpy as np\nimport copy\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom .utils import BayesianSGD\n\n\nclass Appr(object):\n\n    def __init__(self,model,args,lr_min=1e-6,lr_factor=3,lr_patience=5,clipgrad=1000):\n        self.model=model\n        self.device = args.device\n        self.lr_min=lr_min\n        self.lr_factor=lr_factor\n        self.lr_patience=lr_patience\n        self.clipgrad=clipgrad\n\n        self.init_lr=args.lr\n        self.sbatch=args.sbatch\n        self.nepochs=args.nepochs\n\n        self.arch=args.arch\n        self.samples=args.samples\n        self.lambda_=1.\n\n        self.output=args.output\n        self.checkpoint = args.checkpoint\n        self.experiment=args.experiment\n        self.num_tasks=args.num_tasks\n\n        self.modules_names_with_cls = self.find_modules_names(with_classifier=True)\n        self.modules_names_without_cls = self.find_modules_names(with_classifier=False)\n\n\n\n    def train(self,t,xtrain,ytrain,xvalid,yvalid):\n\n        # Update the next learning rate for each parameter based on their uncertainty\n        params_dict = self.update_lr(t)\n        self.optimizer = BayesianSGD(params=params_dict)\n\n        best_loss=np.inf\n\n        # best_model=copy.deepcopy(self.model)\n        best_model = copy.deepcopy(self.model.state_dict())\n        lr = self.init_lr\n        patience = self.lr_patience\n\n\n        # Loop epochs\n        try:\n            for e in range(self.nepochs):\n                # Train\n                clock0=time.time()\n                self.train_epoch(t,xtrain,ytrain)\n                clock1=time.time()\n                train_loss,train_acc=self.eval(t,xtrain,ytrain)\n                clock2=time.time()\n\n                print('| Epoch {:3d}, time={:5.1f}ms/{:5.1f}ms | Train: loss={:.3f}, acc={:5.1f}% |'.format(e+1,\n                    1000*self.sbatch*(clock1-clock0)/xtrain.size(0),1000*self.sbatch*(clock2-clock1)/xtrain.size(0),\n                    train_loss,100*train_acc),end='')\n                # Valid\n                valid_loss,valid_acc=self.eval(t,xvalid,yvalid)\n                print(' Valid: loss={:.3f}, acc={:5.1f}% |'.format(valid_loss, 100 * valid_acc), end='')\n\n                if math.isnan(valid_loss) or math.isnan(train_loss):\n                    print(\"saved best model and quit because loss became nan\")\n                    break\n\n                # Adapt lr\n                if valid_loss<best_loss:\n                    best_loss=valid_loss\n                    best_model=copy.deepcopy(self.model.state_dict())\n                    patience=self.lr_patience\n                    print(' *',end='')\n                else:\n                    patience-=1\n                    if patience<=0:\n                        lr/=self.lr_factor\n                        print(' lr={:.1e}'.format(lr),end='')\n                        if lr<self.lr_min:\n                            print()\n                            break\n                        patience=self.lr_patience\n\n                        params_dict = self.update_lr(t, adaptive_lr=True, lr=lr)\n                        self.optimizer=BayesianSGD(params=params_dict)\n\n                print()\n        except KeyboardInterrupt:\n            print()\n\n        # Restore best\n        self.model.load_state_dict(copy.deepcopy(best_model))\n        self.save_model(t)\n\n\n\n\n    def update_lr(self,t, lr=None, adaptive_lr=False):\n        params_dict = []\n        if t==0:\n            params_dict.append({'params': self.model.parameters(), 'lr': self.init_lr})\n        else:\n            for name in self.modules_names_without_cls:\n                n = name.split('.')\n                if len(n) == 1:\n                    m = self.model._modules[n[0]]\n                elif len(n) == 3:\n                    m = self.model._modules[n[0]]._modules[n[1]]._modules[n[2]]\n                elif len(n) == 4:\n                    m = self.model._modules[n[0]]._modules[n[1]]._modules[n[2]]._modules[n[3]]\n                else:\n                    print (name)\n\n                if adaptive_lr is True:\n                    params_dict.append({'params': m.weight_rho, 'lr': lr})\n                    params_dict.append({'params': m.bias_rho, 'lr': lr})\n\n                else:\n                    w_unc = torch.log1p(torch.exp(m.weight_rho.data))\n                    b_unc = torch.log1p(torch.exp(m.bias_rho.data))\n\n                    params_dict.append({'params': m.weight_mu, 'lr': torch.mul(w_unc,self.init_lr)})\n                    params_dict.append({'params': m.bias_mu, 'lr': torch.mul(b_unc,self.init_lr)})\n                    params_dict.append({'params': m.weight_rho, 'lr':self.init_lr})\n                    params_dict.append({'params': m.bias_rho, 'lr':self.init_lr})\n\n        return params_dict\n\n\n    def find_modules_names(self, with_classifier=False):\n        modules_names = []\n        for name, p in self.model.named_parameters():\n            if with_classifier is False:\n                if not name.startswith('classifier'):\n                    n = name.split('.')[:-1]\n                    modules_names.append('.'.join(n))\n            else:\n                n = name.split('.')[:-1]\n                modules_names.append('.'.join(n))\n\n        modules_names = set(modules_names)\n\n        return modules_names\n\n    def logs(self,t):\n\n        lp, lvp = 0.0, 0.0\n        for name in self.modules_names_without_cls:\n            n = name.split('.')\n            if len(n) == 1:\n                m = self.model._modules[n[0]]\n            elif len(n) == 3:\n                m = self.model._modules[n[0]]._modules[n[1]]._modules[n[2]]\n            elif len(n) == 4:\n                m = self.model._modules[n[0]]._modules[n[1]]._modules[n[2]]._modules[n[3]]\n\n            lp += m.log_prior\n            lvp += m.log_variational_posterior\n\n        lp += self.model.classifier[t].log_prior\n        lvp += self.model.classifier[t].log_variational_posterior\n\n        return lp, lvp\n\n\n    def train_epoch(self,t,x,y):\n\n        self.model.train()\n\n        r=np.arange(x.size(0))\n        np.random.shuffle(r)\n        r=torch.LongTensor(r).to(self.device)\n\n        num_batches = len(x)//self.sbatch\n        j=0\n        # Loop batches\n        for i in range(0,len(r),self.sbatch):\n\n            if i+self.sbatch<=len(r): b=r[i:i+self.sbatch]\n            else: b=r[i:]\n            images, targets = x[b].to(self.device), y[b].to(self.device)\n\n            # Forward\n            loss=self.elbo_loss(images,targets,t,num_batches,sample=True).to(self.device)\n\n            # Backward\n            self.model.cuda()\n            self.optimizer.zero_grad()\n            loss.backward(retain_graph=True)\n            self.model.cuda()\n\n            # Update parameters\n            self.optimizer.step()\n        return\n\n\n    def eval(self,t,x,y,debug=False):\n        total_loss=0\n        total_acc=0\n        total_num=0\n        self.model.eval()\n\n        r=np.arange(x.size(0))\n        r=torch.as_tensor(r, device=self.device, dtype=torch.int64)\n\n        with torch.no_grad():\n            num_batches = len(x)//self.sbatch\n            # Loop batches\n            for i in range(0,len(r),self.sbatch):\n                if i+self.sbatch<=len(r): b=r[i:i+self.sbatch]\n                else: b=r[i:]\n                images, targets = x[b].to(self.device), y[b].to(self.device)\n\n                # Forward\n                outputs=self.model(images,sample=False)\n                output=outputs[t]\n                loss = self.elbo_loss(images, targets, t, num_batches,sample=False,debug=debug)\n\n                _,pred=output.max(1, keepdim=True)\n\n                total_loss += loss.detach()*len(b)\n                total_acc += pred.eq(targets.view_as(pred)).sum().item() \n                total_num += len(b)           \n\n        return total_loss/total_num, total_acc/total_num\n\n\n    def set_model_(model, state_dict):\n        model.model.load_state_dict(copy.deepcopy(state_dict))\n\n\n    def elbo_loss(self, input, target, t, num_batches, sample,debug=False):\n        if sample:\n            lps, lvps, predictions = [], [], []\n            for i in range(self.samples):\n                predictions.append(self.model(input,sample=sample)[t])\n                lp, lv = self.logs(t)\n                lps.append(lp)\n                lvps.append(lv)\n\n            # hack\n            w1 = 1.e-3\n            w2 = 1.e-3\n            w3 = 5.e-2\n\n            outputs = torch.stack(predictions,dim=0).to(self.device)\n            log_var = w1*torch.as_tensor(lvps, device=self.device).mean()\n            log_p = w2*torch.as_tensor(lps, device=self.device).mean()\n            nll = w3*torch.nn.functional.nll_loss(outputs.mean(0), target, reduction='sum').to(device=self.device)\n\n            return (log_var - log_p)/num_batches + nll\n\n        else:\n            predictions = []\n            for i in range(self.samples):\n                pred = self.model(input,sample=False)[t]\n                predictions.append(pred)\n\n\n            # hack\n            # w1 = 1.e-3\n            # w2 = 1.e-3\n            w3 = 5.e-6\n\n            outputs = torch.stack(predictions,dim=0).to(self.device)\n            nll = w3*torch.nn.functional.nll_loss(outputs.mean(0), target, reduction='sum').to(device=self.device)\n\n            return nll\n\n\n        # w1, w2, w3 = self.get_coefs(nll,log_var,log_p,num_batches)\n        # print (\"New coefficients for task {} are w1={}, w2={}, w3={}\".format(t,w1,w2,w3))\n        # if math.isnan(log_var) or math.isnan(log_p) or math.isnan(nll):\n        #     nll = torch.nn.functional.nll_loss(outputs.mean(0), target, reduction='sum')\n        # # if log_var > 1e3 or log_p > 1e3 or nll>1e3:\n        #     print (\"BEFORE: \", (log_var/num_batches).item(), (log_p / num_batches).item(), nll.item())\n        #     # while math.isnan(nll):\n        #         # nll = 1e-5*torch.nn.functional.nll_loss(outputs.mean(0), target, reduction='sum')\n\n\n    def save_model(self,t):\n        torch.save({'model_state_dict': self.model.state_dict(),\n        }, os.path.join(self.checkpoint, 'model_{}.pth.tar'.format(t)))\n\n\n\n    # def get_coefs(self,nll,log_var,log_p,num_batches):\n    #     def take_n(num):\n    #         return torch.log10(num).item()\n    #\n    #     exponents = np.array([take_n(num) for num in [nll, log_p, log_var]])\n    #     min_exp = exponents.min()\n    #     min_exp_idx = np.argmin(exponents)\n    #     if min_exp_idx == 0:\n    #         w1 = (10**(3-(take_n(log_var)+min_exp)))*num_batches\n    #         w2 = (10**-(3-(take_n(log_p)+min_exp)))*num_batches\n    #         w3 = 10.**(3-min_exp_idx)\n    #     if min_exp_idx == 1:\n    #         w1 = (10**(3-(take_n(log_var)+min_exp)))*num_batches\n    #         w3 = 10**(3-(take_n(nll)+min_exp))\n    #         w2 = (10.**-(3-min_exp_idx))*num_batches\n    #     if min_exp_idx == 2:\n    #         w3 = 10**(3-(take_n(nll)+min_exp))\n    #         w2 = (10**-(3-(take_n(log_p)+min_exp)))*num_batches\n    #         w1 = (10.**(3-min_exp_idx))*num_batches\n    #\n    #     return w1, w2, w3\n",
        "experimental_info": "The UCB (Uncertainty-based Continual Learning) method relies on Bayesian Neural Networks (BNNs) for continual learning. It adapts per-parameter learning rates based on the uncertainty (standard deviation) of the weight distributions.\n\n**Core Mechanism (`Appr` class in `src/approaches/ucb.py`):**\n-   **Learning Rate Adaptation:** The `update_lr` method is called at the beginning of each task and also when the learning rate adapts due to plateauing validation loss. For task t > 0, it dynamically sets learning rates for the `weight_mu` and `bias_mu` parameters (means of the posterior distributions) of Bayesian layers. The learning rate for a mean parameter is calculated as `sigma * self.init_lr`, where `sigma = torch.log1p(torch.exp(rho))` (standard deviation of the variational posterior) and `self.init_lr` is the base learning rate. The `weight_rho` and `bias_rho` parameters (which define the standard deviation) use `self.init_lr` directly. This implies that learning rates for mean parameters are *directly* proportional to their standard deviation, which contradicts the 'inversely proportional' statement in the Method description.\n-   **ELBO Loss (Evidence Lower Bound):** The `elbo_loss` method calculates the total loss using Monte Carlo sampling. For `sample=True` (during training), it performs `self.samples` forward passes, computes the mean `log_prior`, `log_variational_posterior`, and `nll` (negative log-likelihood) from these samples. The loss is `(mean_log_variational_posterior - mean_log_prior) / num_batches + nll`. Fixed weights `w1=1e-3`, `w2=1e-3`, `w3=5e-2` are applied to `log_var`, `log_p`, and `nll` respectively.\n-   **Optimizer:** A custom `BayesianSGD` optimizer is used, which is capable of handling per-parameter learning rates passed via the `params_dict` generated by `update_lr`.\n\n**Bayesian Network Components (`src/networks/*.py` and `src/networks/distributions.py`):**\n-   **Variational Posterior:** The `VariationalPosterior` class represents the posterior distribution of weights as a Gaussian parameterized by `mu` (mean) and `rho` (a parameter from which `sigma = log(1 + exp(rho))` is derived). It uses the reparameterization trick to sample weights `(mu + sigma * epsilon)` and computes `log_prob` for KL divergence calculation.\n-   **Prior:** The `Prior` class implements a scaled Gaussian mixture model for the prior distribution, with two Gaussian components (standard deviations `sig1`, `sig2`) and a mixing coefficient `pi`.\n-   **Bayesian Layers:** Bayesian versions of common neural network layers are defined:\n    -   `BayesianLinear` (fully connected layer in `src/networks/FC.py`)\n    -   `BayesianConv2D` (convolutional layer in `src/networks/BayesianConvs.py`)\n    -   `BayesianBatchNorm2d` (batch normalization layer in `src/networks/BatchNorm.py`)\n    These layers use `VariationalPosterior` for their weights and biases, sample from them during training/inference (`sample=True`), and compute `log_prior` and `log_variational_posterior` for the ELBO loss.\n-   **Network Architectures:** `BayesianMLP` (Multi-Layer Perceptron in `src/networks/mlp_ucb.py`) and `BayesianResNet` (ResNet in `src/networks/resnet_ucb.py`) are implemented using these Bayesian layers.\n\n**Pruning (UCB-P variant):**\n-   While the Bayesian layers (`BayesianLinear`, `BayesianConv2D`, `BayesianBatchNorm2d`) and network architectures (`BayesianMLP`, `BayesianResNet`) contain `prune_module` methods and `mask_flag`s to enable pruning by applying a mask to `weight_mu` (and `weight_rho` for `BayesianLinear`), the provided `src/approaches/ucb.py` *does not implement the actual logic for calculating SNR, generating these pruning masks, or activating the pruning process*. This suggests that the provided code implements the base UCB method for learning rate adaptation but not the UCB-P pruning variant, or the pruning mechanism is external to this specific `Appr` class.\n\n**Experimental Settings (`src/run.py`):**\n-   **Datasets:** Experiments are configured for `mnist2`, `mnist5`, `pmnist` (Permuted MNIST), `cifar` (binary split CIFAR10/100), and `mixture` (mixed datasets). These datasets are split into multiple tasks for continual learning.\n-   **Hyperparameters (defaults):**\n    -   `samples`: 10 (number of Monte Carlo samples for ELBO calculation).\n    -   `rho`: -3.0 (initial value for `rho` parameter, affecting initial standard deviation).\n    -   `sig1`: 0.0 (STD for the first prior Gaussian).\n    -   `sig2`: 6.0 (STD for the second prior Gaussian).\n    -   `pi`: 0.25 (weighting factor for the mixture prior).\n    -   `nepochs`: 200 (training epochs per task).\n    -   `sbatch`: 64 (batch size).\n    -   `lr`: 0.01 (initial base learning rate).\n    -   `nlayers`: 1 (for MLP, number of hidden layers).\n    -   `nhid`: 1200 (number of hidden units for MLP).\n    -   `arch`: 'mlp' or 'resnet'.\n-   **Training Loop:** The `run.py` script orchestrates the continual learning process, iterating through tasks, calling `appr.train` for each task, and evaluating performance on all previously learned tasks."
      }
    },
    {
      "title": "Lifelong Domain Adaptation via Consolidated Internal Distribution"
    }
  ],
  "reference_research_study_list": [
    {
      "title": "Online continual learning with maximal interfered retrieval",
      "abstract": "Continual learning, the setting where a learning agent is faced with a never\nending stream of data, continues to be a great challenge for modern machine\nlearning systems. In particular the online or \"single-pass through the data\"\nsetting has gained attention recently as a natural setting that is difficult to\ntackle. Methods based on replay, either generative or from a stored memory,\nhave been shown to be effective approaches for continual learning, matching or\nexceeding the state of the art in a number of standard benchmarks. These\napproaches typically rely on randomly selecting samples from the replay memory\nor from a generative model, which is suboptimal. In this work, we consider a\ncontrolled sampling of memories for replay. We retrieve the samples which are\nmost interfered, i.e. whose prediction will be most negatively impacted by the\nforeseen parameters update. We show a formulation for this sampling criterion\nin both the generative replay and the experience replay setting, producing\nconsistent gains in performance and greatly reduced forgetting. We release an\nimplementation of our method at\nhttps://github.com/optimass/Maximally_Interfered_Retrieval.",
      "full_text": "Online Continual Learning with Maximally Interfered Retrieval Rahaf Aljundi∗ KU Leuven rahaf.aljundi@gmail.com Lucas Caccia∗ Mila lucas.page-caccia@mail.mcgill.ca Eugene Belilovsky∗ Mila eugene.belilovsky@umontreal.ca Massimo Caccia∗ Mila massimo.p.caccia@gmail.com Min Lin Mila mavenlin@gmail.com Laurent Charlin Mila lcharlin@gmail.com Tinne Tuytelaars KU Leuven tinne.tuytelaars@esat.kuleuven.be Abstract Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or \"single-pass through the data\" setting has gained attention recently as a natural setting that is difﬁcult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally_Interfered_Retrieval. 1 Introduction Artiﬁcial neural networks have exceeded human-level performance in accomplishing individual narrow tasks [19]. However, such success remains limited compared to human intelligence that can continually learn and perform an unlimited number of tasks. Humans’ ability of learning and accumulating knowledge over their lifetime has been challenging for modern machine learning algorithms and particularly neural networks. In that perspective, continual learning aims for a higher level of machine intelligence by providing the artiﬁcial agents with the ability to learn online from a non-stationary and never-ending stream of data. A key component for such never-ending learning ∗Authors contributed equally 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1908.04742v3  [cs.LG]  29 Oct 2019process is to overcome the catastrophic forgetting of previously seen data, a problem that neural networks are well known to suffer from [13]. The solutions developed so far often relax the problem of continual learning to the easier task-incremental setting, where the data stream can be divided into tasks with clear boundaries and each task is learned ofﬂine. One task here can be recognizing hand written digits while another different types of vehicles (see [25] for example). Existing approaches can be categorized into three major families based on how the information regarding previous task data is stored and used to mitigate forgetting and potentially support the learning of new tasks. These include replay-based [8, 31] methods which store prior samples, dynamic architectures [36, 40] which add and remove components and prior-focused [18, 42, 9, 7] methods that rely on regularization. In this work, we consider an online continual setting where a stream of samples is seen only once and is not-iid. This is a much harder and more realistic setting than the milder incremental task assumption[4] and can be encountered in practice e.g. social media applications. We focus on the replay-based approach [27, 37] which has been shown to be successful in the online continual learning setting compared to other approaches [27]. In this family of methods, previous knowledge is stored either directly in a replay buffer, or compressed in a generative model. When learning from new data, old examples are reproduced from a replay buffer or a generative model. In this work, assuming a replay buffer or a generative model, we direct our attention towards answering the question of what samples should be replayed from the previous history when new samples are received. We opt for retrieving samples that suffer from an increase in loss given the estimated parameters update of the model. This approach also takes some motivation from neuroscience where replay of previous memories is hypothesized to be present in the mammalian brain [ 28, 35], but likely not random. For example it is hypothesized in [ 15, 26] similar mechanisms might occur to accommodate recent events while preserving old memories. We denote our approach Maximally Interfered Retrieval (MIR) and propose variants using stored memories and generative models. The rest of the text is divided as follows: we discuss closely related work in Sec. 2. We then present our approach based on a replay buffer or a generative model in Sec. 3 and show the effectiveness of our approach compared to random sampling and strong baselines in Sec. 4. 2 Related work The major challenge of continual learning is the catastrophic forgetting of previous knowledge once new knowledge is acquired [12, 33] which is closely related to the stability/plasticity dilemma [14] that is present in both biological and artiﬁcial neural networks. While these problems have been studied in early research works [10, 11, 20, 21, 38], they are receiving increased attention since the revival of neural networks. Several families of methods have been developed to prevent or mitigate the catastrophic forgetting phenomenon. Under the ﬁxed architecture setting, one can identify two main streams of works: i) methods that rely on replaying samples or virtual (generated) samples from the previous history while learning new ones and ii) methods that encode the knowledge of the previous tasks in a prior that is used to regularize the training of the new task [17, 41, 1, 29]. While the prior-focused family might be effective in the task incremental setting with a small number of disjoint tasks, this family often shows poor performance when tasks are similar and training models are faced with long sequences as shown in Farquhar and Gal [9]. Replayed samples from previous history can be either used to constrain the parameters update based on the new sample, to stay in the feasible region of the previous ones [27, 6, 5] or for rehearsal [31, 34]. Here, we consider a rehearsal approach on samples played from previous history as it is a cheaper and effective alternative to the constraint optimization approach [8, 5]. Rehearsal methods usually play random samples from a buffer, or pseudo samples from a generative model trained on the previous data [37, 23, 24]. These works showed promising results in the ofﬂine incremental tasks setting and recently been extended to the online setting [ 8, 5], where a sequence of tasks forming a non i.i.d. stream of training data is considered with one or few samples at a time. However, in the online setting and given a limited computational budget, one can’t replay all buffer samples each time and it 2Incoming Batch Find Likely  Interfered  Samples Estimate  Update Update on Augmented Batch Randomly Select  Memories Stored  Memories Generative  ModelOR Update on Augmented Batch Stream of Non-iid Samples Cat v Dog Orange v AppleWolf vs Car Lion vs Zebra Dog vs. Horse Naive Approach Maximally Interfered Figure 1: High-level illustration of a standard rehearsal method (left) such as generative replay or experience replay which selects samples randomly. This is contrasted with selecting samples based on interferences with the estimated update (right). becomes crucial to select the best candidates to be replayed. Here, we propose a better strategy than random sampling in improving the learning behaviour and reducing the interference. Continual learning has also been studied recently for the case of learning generative models [30, 22]. Riemer et al. [32] used an autoencoder to store compressed representation instead of raw samples. In this work we will leverage this line of research and will consider for the ﬁrst time generative modeling in the online continual learning setting. 3 Methods We consider a (potentially inﬁnite) stream of data where at each time step, t, the system receives a new set of samples Xt,Yt drawn non i.i.d from a current distribution Dt that could itself experience sudden changes corresponding to task switching from Dt to Dt+1. We aim to learn a classiﬁerf parameterized by θthat minimizes a predeﬁned loss Lon new sample(s) from the data stream without interfering, or increasing the loss, on previously observed samples. One way to encourage this is by performing updates on old samples from a stored history, or from a generative model trained on the previous data. The principle idea of our proposal is that instead of using randomly selected or generated samples from the previous history [6, 37], we ﬁnd samples that would be (maximally) interfered by the new incoming sample(s), had they been learned in isolation (Figure 1). This is motivated by the observation that the loss of some previous samples may be unaffected or even improved, thus retraining on them is wasteful. We formulate this ﬁrst in the context of a small storage of past samples and subsequently using a latent variable generative model. 3.1 Maximally Interfered Sampling from a Replay Memory We ﬁrst instantiate our method in the context of experience replay (ER), a recent and successful rehearsal method [8], which stores a small subset of previous samples and uses them to augment the incoming data. In this approach the learner is allocated a memory Mof ﬁnite size, which is updated by the use of reservoir sampling [3, 8] as the stream of samples arrives. Typically samples are drawn randomly from memory and concatenated with the incoming batch. Given a standard objective min θ L(fθ(Xt),Yt), when receiving sample(s) Xt we estimate the would- be parameters update from the incoming batch as θv = θ−α∇L(fθ(Xt),Yt), with learning rate α. We can now search for the top-kvalues x∈M using the criterion sMI-1(x) =l(fθv(x),y) − l(fθ(x),y), where lis the sample loss. We may also augment the memory to additionally store the best l(fθ(x),y) observed so far for that sample, denoted l(fθ∗(x),y). Thus instead we can evaluate sMI-2(x) =l(fθv(x),y) −min ( l(fθ(x),y),l(fθ∗(x),y) ) . We will consider both versions of this criterion in the sequel. 3We denote the budget of samples to retrieve, B. To encourage diversity we apply a simple strategy of performing an initial random sampling of the memory, selecting C samples where C >Bbefore applying the search criterion. This also reduces the compute cost of the search. The ER algorithm with MIR is shown in Algorithm 1. We note that for the case of sMI-2 the loss of the C selected samples at line 7 is tracked and stored as well. 3.2 Maximally Interfered Sampling from a Generative Model We now consider the case of replay from a generative model. Assume a function f parameterized by θ(e.g. a classiﬁer) and an encoder qφ and decoder gγ model parameterized by φand γ, respectively. We can compute the would-be parameter update θv as in the previous section. We want to ﬁnd in the given feature space data points that maximize the difference between their loss before and after the estimated parameters update: max Z L ( fθv(gγ(Z)),Y ∗) −L ( fθ′(gγ(Z)),Y ∗) s.t. ||zi −zj||2 2 >ϵ ∀zi,zj ∈Z with zi ̸= zj (1) with Z ∈RB×K, Kthe feature space dimension, and ϵa threshold to encourage the diversity of the retrieved points. Here θ ′ can correspond to the current model parameters or a historical model as in Shin et al. [37]. Furthermore, y∗denotes the true label i.e. the one given to the generated sample by the real data distribution. We will explain how to approximate this value shortly. We convert the constraint into a regularizer and optimize the Equation 1 with stochastic gradient descent denoting the strength of the diversity term as λ. From these points we reconstruct the full corresponding input samples X ′ = gγ(Z) and use them to estimate the new parameters update min θ L(fθ(Xt ∪X ′ )). Using the encoder encourages a better representation of the input samples where similar samples lie close. Our intuition is that the most interfered samples share features with new one(s) but have different labels. For example, in handwritten digit recognition, the digit 9 might be written similarly to some examples from digits {4,7}, hence learning 9 alone may result in confusing similar 4(s) and 7(s) with 9 (Fig. 2). The retrieval is initialized with Z ∼qφ(Xt) and limited to a few gradient updates, limiting its footprint. Figure 2: Most interfered retrieval from V AE on MNIST. Top row shows in- coming data from a ﬁnal task ( 8 v 9). The next rows show the samples caus- ing most interference for the classiﬁer (Eq. 1) To estimate the loss in Eq. 1 we also need an estimate of y∗i.e. the label when using a generator. A straightforward approach for is based on the generative replay ideas [37] of storing the predictions of a prior model. We thus suggest to use the predicted labels given by fθ′ as pseudo labels to estimate y∗. Denoting ypre = fθ′(gγ(z)) and ˆy = fθv(gγ(z)) we compute the KL divergence, DKL(ypre ∥ ˆy), as a proxy for the interference. Generative models such as V AEs [16] are known to gen- erate blurry images and images with mix of categories. To avoid such a source of noise in the optimization, we mini- mize an entropy penalty to encourage generating points for which the previous model is conﬁdent. The ﬁnal objective of the generator based retrieval is max Z ∑ z∈Z [DKL(ypre ∥ˆy) −αH(ypre)] s.t. ||zi −zj||2 2 >ϵ ∀zi,zj ∈Z with zi ̸= zj, (2) with the entropy H and a hyperparameter αto weight the contribution of each term. So far we have assumed having a perfect encoder/decoder that we use to retrieve the interfered samples from the previous history for the function being learned. Since we assume an online continual learning setting, we need to address learning the encoder/decoder continually as well. We could use a variational autoencoder (V AE) withpγ(X |z) = N(X |gγ(z),σ2I) with mean gγ(z) and covariance σ2I. As for the classiﬁer we can also update the V AE based on incoming samples and the replayed samples. In Eq. 1 we only retrieve samples that are going to be interfered given the classiﬁer update, assuming 4a good feature representation. We can also use the same strategy to mitigate catastrophic forgetting in the generator by retrieving the most interfered samples given an estimated update of both parameters (φ,γ). In this case, the intereference is with respect to the V AE’s loss, the evidence lower bound (ELBO). Let us denote γv,φv the virtual updates for the encoder and decoder given the incoming batch. We consider the following criterion for retrieving samples for the generator: max Zgen E z∼qφv [−log(pγv(gγv(Zgen)|z))] − E z∼qφ′ [−log(pγ′(gγ′(Zgen)|z))] + DKL(qφv(z|gγv(Zgen))||p(z)) −DKL(qφ′(z|gγ′(Zgen))||p(z)) (3) s.t. ||zi −zj||2 2 >ϵ ∀zi,zj ∈Zgen s.t. zi ̸= zj Here (φ′,γ′) can be the current V AE or stored from the end of the previous task. Similar toZ, Zgen is initialized with Zgen ∼qφ(Xt) and limited to few gradient updates. A complete view of the MIR based generative replay is shown in Algorithm 2 3.3 A Hybrid Approach Training generative models in the continual learning setting on more challenging datasets like CIFAR- 10 remains an open research problem [ 23]. Storing samples for replay is also problematic as it is constrained by storage costs and very-large memories can become difﬁcult to search. To leverage the beneﬁts of both worlds while avoiding training the complication of noisy generation, Similar to Riemer et al. [32] we use a hybrid approach where an autoencoder is ﬁrst trained ofﬂine to store and compress incoming memories. Differently, in our approach, we perform MIR search in the latent space of the autoencoder using Eq. 1. We then select nearest neighbors from stored compressed memories to ensure realistic samples. Our strategy has several beneﬁts: by storing lightweight representations, the buffer can store more data for the same ﬁxed amount of memory. Moreover, the feature space in which encoded samples lie is fully differentiable. This enables the use of gradient methods to search for most interfered samples. We note that this is not the case for the discrete autoencoder proposed in [32]. Finally, the autoencoder with its simpler objective is easier to train in the online setting than a variational autoencoder. The method is summarized in Algorithm 3 in the Appendix. Algorithm 1: Experience MIR (ER-MIR) Input: Learning rate α, Subset size C; Budget B 1 Initialize: Memory M; θ 2 for t∈1..T do 3 for Bn ∼Dt do 4 %%Virtual Update 5 θv ←SGD(Bn,α) 6 %Select C samples 7 BC∼M 8 %Select based on score 9 S ←sort(sMI (BC)) 10 BMC ←{Si}B i=1 11 θ←SGD(Bn ∪BMC,α) 12 %Add samples to memory 13 M← UpdateMemory(Bn); 14 end 15 end Algorithm 2: Generative-MIR (GEN-MIR) Input: Learning rate α 1 Initialize: Memory M; θ, φ,γ 2 for t∈1..T do 3 θ ′ ,φ ′ ,γ ′ ←θ,φ,γ 4 for Bn ∼Dt do 5 %Virtual Update 6 θv ←SGD(Bn,α) 7 BC ←Retrieve samples as per Eq (2) 8 BG ←Retrieve samples as per Eq (3) 9 %Update Classifier 10 θ←SGD(Bn ∪BC,α) 11 %Update Generative Model 12 φ,γ ←SGD(Bn ∪BG,α) 13 end 14 end 4 Experiments We now evaluate the proposed method under the generative and experience replay settings. We will use three standard datasets and the shared classiﬁer setting described below. • MNIST Split splits MNIST data to create 5 different tasks with non-overlapping classes. We consider the setting with 1000 samples per task as in [2, 27]. • Permuted MNIST permutes MNIST to create 10 different tasks. We consider the setting with 1000 samples per task as in [2, 27]. 5• CIFAR-10 Split splits CIFAR-10 dataset into 5 disjoint tasks as in Aljundi et al. [3]. However, we use a more challenging setting, with all 9,750 samples per task and 250 retained for validation. • MiniImagenet Split splits MiniImagenet [39] dataset into 20 disjoint tasks as in Chaudhry et al. [8] with 5 classes each. In our evaluations we will focus the comparisons of MIR to random sampling in the experience replay (ER) [3, 8] and generative replay [37, 22] approaches which our method directly modiﬁes. We also consider the following reference baselines: • ﬁne-tuning trains continuously upon arrival of new tasks without any forgetting avoidance strategy. • iid online (upper-bound) considers training the model with a single-pass through the data on the same set of samples, but sampled iid. • iid ofﬂine (upper-bound) evaluates the model using multiple passes through the data, sam- pled iid. We use 5 epochs in all the experiments for this baseline. • GEM [27] is another method that relies on storing samples and has been shown to be a strong baseline in the online setting. It gives similar results to the recent A-GEM [6]. We do not consider prior-based baselines such as Kirkpatrick et al. [18] as they have been shown to work poorly in the online setting as compared to GEM and ER [8, 27]. For evaluation we primarily use the accuracy as well as forgetting [8]. Shared Classiﬁer A common setting for continual learning applies a separate classiﬁer for each task. This does not cover some of the potentially more interesting continual learning scenarios where task metadata is not available at inference time and the model must decide which classes correspond to the input from all possible outputs. As in Aljundi et al. [3] we adopt a shared-classiﬁer setup for our experiments where the model can potentially predict all classes from all tasks. This sort of setup is more challenging, yet can apply to many realistic scenarios. Multiple Updates for Incoming Samples In the one-pass through the data continual learning setup, previous work has been largely restricted to performing only a single gradient update on incoming samples. However, as in [3] we argue this is not a necessary constraint as the prescribed scenario should permit maximally using the current sample. In particular for replay methods, performing additional gradient updates with additional replay samples can improve performance. In the sequel we will refer to this as performing more iterations. Comparisons to Reported Results Note comparing reported results in Continual Learning re- quires great diligence because of the plethora of experimental settings. We remind the reviewer that our setting, i.e. shared-classiﬁer, online and (in some cases) lower amount of training data, is more challenging than many of the other reported continual learning settings. 4.1 Experience Replay Here we evaluate experience replay with MIR comparing it to vanilla experience replay [8, 3] on a number of shared classiﬁer settings. In all cases we use a single update for each incoming batch, multiple iterations/updates are evaluated in a ﬁnal ablation study. We restrict ourselves to the use of reservoir sampling for deciding which samples to store. We ﬁrst evaluate using the MNIST Split and Permuted MNIST (Table 1). We use the same learning rate, 0.05, used in Aljundi et al. [3]. The number of samples from the replay buffer is always ﬁxed to the same amount as the incoming samples, 10, as in [8]. For MIR we select by validation C = 50and the sMI-2 criterion for both MNIST datasets. ER-MIR performs well and improves over (standard) ER in both accuracy and forgetting. We also show the accuracy on seen tasks after each task sequence is completed in Figure 7. We now consider the more complex setting of CIFAR-10 and use a larger number of samples than in prior work [3]. We study the performance for different memory sizes (Table 2). For MIR we select by validation at M = 50, C = 50and the sMI-1 criterion. We observe that the performance gap increases when more memories are used. We ﬁnd that the GEM method does not perform well in this 6Accuracy↑ Forgetting↓ iid online 86.8 ±1.1 N/A iid ofﬂine 92.3 ±0.5 N/A ﬁne-tuning 19.0 ±0.2 97 .8 ±0.2 GEN 79.3 ±0.6 19 .5 ±0.8 GEN-MIR 82.1 ±0.3 17 .0 ±0.4 GEM [27] 86.3 ±1.4 11 .2 ±1.2 ER 82.1 ±1.5 15 .0 ±2.1 ER-MIR 87.6 ±0.7 7 .0 ±0.9 Accuracy↑ Forgetting↓ iid online 73.8 ±1.2 N/A iid ofﬂine 86.6 ±0.5 N/A ﬁne-tuning 64.6 ±1.7 15 .2 ±1.9 GEN 79.7 ±0.1 5 .8 ±0.2 GEN-MIR 80.4 ±0.2 4 .8 ±0.2 GEM [27] 78.8 ±0.4 3.1 ±0.5 ER 78.9 ±0.6 3 .8 ±0.6 ER-MIR 80.1 ±0.4 3.9 ±0.3 Table 1: Results for MNIST SPLIT (left) and Permuted MNIST (right). We report the Average Accuracy (higher is better) and Average Forgetting (lower is better) after the ﬁnal task. We split results into privileged baselines, methods that don’t use a memory storage, and those that store memories. For the ER methods, 50 memories per class are allowed. Each approach is run 20 times. Accuracy↑ M= 20 M= 50 M= 100 iid online 60.8±1.0 60 .8±1.0 60 .8±1.0 iid ofﬂine 79.2±0.4 79 .2±0.4 79 .2±0.4 GEM [27] 16.8±1.1 17 .1±1.0 17 .5±1.6 iCarl (5 iter) [31]28.6±1.2 33 .7±1.6 32 .4±2.1 ﬁne-tuning 18.4±0.3 18 .4±0.3 18 .4±0.3 ER 27.5±1.2 33 .1±1.7 41 .3±1.9 ER-MIR 29.8±1.1 40.0±1.1 47 .6±1.1 Forgetting↓ M= 20 M= 50 M= 100 N/A N/A N/A N/A N/A N/A 73.5±1.7 70 .7±4.5 71 .7±1.3 49±2.4 40.6±1.1 40 ±1.8 85.4±0.7 85 .4±0.7 85 .4±0.7 50.5±2.4 35 .4±2.0 23 .3±2.9 50.2±2.0 30.2±2.3 17 .4±2.1 Table 2: CIFAR-10 results. Memories per class M, we report (a) Accuracy, (b) Forgetting (lower is better). For larger sizes of memory ER-MIR has better accuracy and improved forgetting metric. Each approach is run 15 times. setting. We also consider another baseline iCarl [31]. Here we boost the iCarl method permitting it to perform 5 iterations for each incoming sample to maximize its performance. Even in this setting it is only able to match the experience replay baseline and is outperformed by ER-MIR for larger buffers. Number of iterations 1 5 iid online 60.8 ±1.0 62 .0 ±0.9 ER 41.3 ±1.9 42 .4 ±1.1 ER-MIR 47.6 ±1.1 49 .3 ±0.1 Table 3: CIFAR-10 accuracy ( ↑) results for in- creased iterations and 100 memories per class. Each approach is run 15 times. Accuracy ↑ Forgetting ↓ ER 24.7 ±0.7 23 .5 ±1.0 ER-MIR 25.2 ±0.6 18.0 ±0.8 Table 4: MinImagenet results. 100 memories per class and using 3 updates per incoming batch, accuracy is slightly better and forget- ting is greatly improved. Each approach is run 15 times Increased iterations We evaluate the use of additional iterations on incoming batches by comparing the 1 iteration results above to running 5 iterations. Results are shown in Table 3 We use ER an and at each iteration we either re-sample randomly or using the MIR criterion. We observe that increasing the number of updates for an incoming sample can improve results on both methods. Longer Tasks Sequence we want to test how our strategy performs on longer sequences of tasks. For this we consider the 20 tasks sequence of MiniImagenet Split. Note that this dataset is very challenging in our setting given the shared classiﬁer and the online training. A naive experience replay with 100 memories per class obtains only 17% accuracy at the end of the task sequence. To overcome this difﬁculty, we allow more iterations per incoming batch. Table 4 compares ER and ER-MIR accuracy and forgetting at the end of the sequence. It can be seen how our strategy continues to outperform, in particular we achieve over 5% decrease in forgetting. 7(a) Generation with the best V AE baseline. Complications arising from both properties leave the V AE generating blurry and/or fad- ing digits. (b) Most interfered samples while learning the last task (8 vs 9). Top row is the incoming batch. Rows 2 and 3 show the most interfered samples for the classiﬁer, Row 4 and 5 for the V AE. We observe retrieved samples look similar but belong to different category. Figure 3: Online and low data regime MNIST Split generation. Qualitatively speaking, most interfered samples are superior to baseline’s. 4.2 Generative Replay We now study the effect of our proposed retrieval mechanism in the generative replay setting (Alg. 2). Recall that online continual generative modeling is particularly challenging and to the best of our knowledge has never been attempted. This is further exacerbated by the low data regime we consider. Results for the MNIST datasets are presented in Table 1. To maximally use the incoming samples, we (hyper)-parameter searched the amount of additional iterations for both GEN and GEN-MIR. In that way, both methodologies are allowed their optimal performance. More hyperarameter details are provided in Appendix B.2. On MNIST Split, MIR outperforms the baseline by 2.8% and 2.5% on accuracy and forgetting respectively. Methods using stored memory show improved performance, but with greater storage overhead. We provide further insight into theses results with a generation comparison (Figure 3). Complications arising from online generative modeling combined with the low data regime cause blurry and/or fading digits (Figure 3a) in the V AE baseline (GEN). In line with the reported results, the most interfered retrievals seem qualitatively superior (see Figure 3b where the GEN-MIR generation retrievals is demonstrated). We note that the quality of the samples causing most interference on the V AE seems higher than those on the classiﬁer. For the Permuted MNIST dataset, GEN-MIR not only outperforms the its baselines, but it achieves the best performance over all models. This result is quite interesting, as generative replay methods can’t store past data and require much more tuning. The results discussed thus far concern classiﬁcation. Nevertheless, GEN-MIR alleviates catastrophic forgetting in the generator as well. Table 5 shows results for the online continual generative modeling. The loss of the generator is signiﬁcantly lower on both datasets when it rehearses on maximally interfered samples versus on random samples. This result suggest that our method is not only viable in supervised learning, but in generative modeling as well. MNIST Split Permuted MNIST GEN 107.2 ±0.2 196 .7 ±0.7 GEN-MIR 102.5 ±0.2 193 .7 ±1.0 Table 5: Generator’s loss (↓), i.e. negative ELBO, on the MNIST datasets. Our methodology outperforms the baseline in online continual generative modeling as well. Our last generative replay experiment is an ablation study. The results are presented in Table 6. All facets of our proposed method- ology seem to help in achieving the best pos- sible results. It seems however that the min- imization of the label entropy, i.e. H(ypre), which ensures that the previous classiﬁer is conﬁdent about the retrieved sample’s class, is most important and is essential to outper- form the baseline. As noted in [23], training generative models in the continual learning setting on more challenging datasets remains an open research problem. [23] found that generative replay is not yet a viable strategy for CIFAR-10 given the current state of the generative modeling. We too arrived at the same conclusion, which led us to design the hybrid approach presented next. 8Accuracy GEN-MIR 83.0 ablate MIR on generator 82.7 ablate MIR on classiﬁer 81.7 ablate DKL(ypre ∥ˆy) 80.7 ablate H(ypre) 78.3 ablate diversity constraint 80.7 GEN 80.0 Table 6: Ablation study of GEN-MIR on the MNIST Split dataset. The H(ypre) term in the MIR loss function seems to play an important role in the success of our method. 2 4 6 8 10 Task 74 76 78 80 82Test Accuracy ER ER-MIR Table 7: Permuted MNIST test accuracy on tasks seen so far for rehearsal methods. 4.3 Hybrid Approach 100 / 1k 500 / 5k 1k / 10k 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Accuracy ( ) 100 / 1k 500 / 5k 1k / 10k 0.0 0.1 0.2 0.3 0.4 0.5 Forgetting ( ) AE-Random AE-MIR Real Memory Slots / Compressed Memory Slots Figure 4: Results for the Hybrid Approach In this section, we evaluate the hybrid approach pro- posed in Sec 3.3 on the CIFAR-10 dataset. We use an autoencoder to compress the data stream and simplify MIR search. We ﬁrst identify an important failure mode arising from the use of reconstructions which may also apply to generative replay. During training, the classiﬁer sees real images, from the current task, from the data stream, along with reconstructions from the buffer, which belong to old tasks. In the shared classiﬁer setting, this discrepancy can be leveraged by the clas- siﬁer as a discriminative feature. The classiﬁer will tend to classify all real samples as belonging to the classes of the last task, yielding low test accuracy. To address this problem, we ﬁrst autoencode the incom- ing data with the generator before passing it to the classiﬁer. This way, the classiﬁer cannot leverage the distribution shift. We found that this simple correction led to a signiﬁcant performance increase. We perform an ablation experiment to validate this claim, which can be found in Appendix C, along with further details about the training procedure. In practice, we store a latent representation of size 4 ×4 ×20 = 320, giving us a compression factor of 32×32×3 320 = 9.6 (putting aside the size of the autoencoder, which is less than 2% of total parameters for large buffer size). We therefore look at buffer size which are 10 times as big i.e. which can contain 1k, 5k, 10k compressed images, while holding memory equivalent to storing 100 / 5000 / 1k real images. Results are shown in Figure 4. We ﬁrst note that as the number of compressed samples increases we continue to see performance improvement, suggesting the increased storage capacity gained from the autoencoder can be leveraged. We next observe that even though AE-MIR obtains almost the same average accuracies as AE-Random, it achieved a big decrease in the forgetting metric, indicating a better trade-offs in the performance of the learned tasks. Finally we note a gap still exists between the performance of reconstructions from incrementally learned AE or V AE models and real images, further work is needed to close it. 5 Conclusion We have proposed and studied a criterion for retrieving relevant memories in an online continual learning setting. We have shown in a number of settings that retrieving interfered samples reduces forgetting and signiﬁcantly improves on random sampling and standard baselines. Our results and analysis also shed light on the feasibility and challenges of using generative modeling in the online continual learning setting. We have also shown a ﬁrst result in leveraging encoded memories for more compact memory and more efﬁcient retrieval. 9Acknowledgements We would like to thank Kyle Kastner and Puneet Dokania for helpful discussion. Eugene Belilvosky is funded by IV ADO and Rahaf Aljundi is funded by FWO. References [1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte- laars. Memory aware synapses: Learning what (not) to forget. In ECCV 2018, . [2] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In CVPR 2019, . [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Online continual learning with no task boundaries. In arXiv, . [4] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [5] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Online continual learning with no task boundaries. arXiv preprint arXiv:1903.08671, 2019. [6] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. In ICLR 2019. [7] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018. [8] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. [9] Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733, 2018. [10] Robert M French. Semi-distributed representations and catastrophic forgetting in connectionist networks. Connection Science, 4(3-4):365–377, 1992. [11] Robert M French. Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference. network, 1111:00001, 1994. [12] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128–135, 1999. [13] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. [14] Stephen Grossberg. Studies of mind and brain : neural principles of learning, perception, development, cognition, and motor control. Boston studies in the philosophy of science 70. Reidel, Dordrecht, 1982. ISBN 9027713596. [15] Christopher J Honey, Ehren L Newman, and Anna C Schapiro. Switching between internal and external modes: a multiscale learning principle. Network Neuroscience, 1(4):339–356, 2017. [16] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [17] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. arXiv preprint arXiv:1612.00796, 2016. 10[18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, page 201611835, 2017. [19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [20] John K Kruschke. Alcove: an exemplar-based connectionist model of category learning. Psychological review, 99(1):22, 1992. [21] John K Kruschke. Human category learning: Implications for backpropagation models. Con- nection Science, 5(1):3–36, 1993. [22] Frantzeska Lavda, Jason Ramapuram, Magda Gregorova, and Alexandros Kalousis. Continual classiﬁcation learning using generative models, 2018. [23] Timothée Lesort, Hugo Caselles-Dupré, Michael Garcia-Ortiz, Andrei Stoian, and David Filliat. Generative models from the perspective of continual learning. arXiv preprint arXiv:1812.09111, 2018. [24] Timothée Lesort, Alexander Gepperth, Andrei Stoian, and David Filliat. Marginal replay vs conditional replay for continual learning. In International Conference on Artiﬁcial Neural Networks, pages 466–480. Springer, 2019. [25] Zhizhong Li and Derek Hoiem. Learning without forgetting. In European Conference on Computer Vision, pages 614–629. Springer, 2016. [26] Kuhl BA Long NM. Decoding the tradeoff between encoding and retrieval to predict memory for overlapping events. In SSRN 3265727. 2018 Oct 13. [27] David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pages 6467–6476, 2017. [28] JAMES L McCLELLAND. Complementary learning systems in the brain: A connectionist approach to explicit and implicit cognition and memory. Annals of the New York Academy of Sciences, 843(1):153–169, 1998. [29] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. [30] Jason Ramapuram, Magda Gregorova, and Alexandros Kalousis. Lifelong generative modeling. arXiv preprint arXiv:1705.09847, 2017. [31] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. [32] Matthew Riemer, Tim Klinger, Djallel Bouneffouf, and Michele Franceschini. Scalable recol- lections for continual lifelong learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 1352–1359, 2019. [33] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7 (2):123–146, 1995. [34] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Greg Wayne. Experience replay for continual learning. CoRR, abs/1811.11682, 2018. URL http: //arxiv.org/abs/1811.11682. [35] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Greg Wayne. Experi- ence replay for continual learning, 2018. [36] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. 11[37] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pages 2990–2999, 2017. [38] Steven A Sloman and David E Rumelhart. Reducing interference in distributed memories through episodic gating. Essays in honor of WK Estes, 1:227–248, 1992. [39] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pages 3630–3638, 2016. [40] Ju Xu and Zhanxing Zhu. Reinforced continual learning. arXiv preprint arXiv:1805.12369, 2018. [41] Friedemann Zenke, Ben Poole, and Surya Ganguli. Improved multitask learning through synaptic intelligence. In Proceedings of the International Conference on Machine Learning (ICML), 2017. [42] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017. 12M=20 M=50 M=100 iid online 86.8 ±1.1 86.8 ±1.1 86.8 ±1.1 iid ofﬂine 86.8 ±1.1 86.8 ±1.1 86.8 ±1.1 ﬁne-tuning 19 ±0.2 19 ±0.2 19 ±0.2 ER 78.9 ±1.5 82.6 ±1.0 81.3 ±1.9 ER-MIR 81.5 ±1.9 87.4 ±0.8 87.4 ±1.2 M=20 M=50 M=100 N/A N/A N/A N/A N/A N/A 97.8 ±0.2 97.8 ±0.2 97.8 ±0.2 19.1 ±2.0 14.0 ±1.1 15.8 ±2.7 15.9 ±2.5 7.2 ±0.9 6.7 ±1.4 Table 8: MNIST results. Memories per class M, we report the (a) Accuracy (b) Forgetting (lower is better). For larger sizes of memory ER-MIR has better accuracy and improved forgetting metric. Each approach is run 20 times M=20 M=50 M=100 iid online 73.8 ±1.2 73.8 ±1.2 73.8 ±1.2 iid ofﬂine 86.6 ±0.5 86.6 ±0.5 86.6 ±0.5 ﬁne-tuning 64.6 ±1.7 64.6 ±1.7 64.6 ±1.7 ER 76.3 ±0.6 78.4 ±0.6 79.9 ±0.3 ER-MIR 76.3 ±0.5 80.1 ±0.4 82.3 ±0.2 M=20 M=50 M=100 N/A N/A N/A N/A N/A N/A 15.2 ±1.9 15.2 ±1.9 15.2 ±1.9 5.6 ±0.6 3.7 ±0.5 2.48±0.5 6.5 ±0.5 3.4 ±0.3 1.89±0.3 Table 9: Permuted MNIST results. Memories per class M, we report the (a) Accuracy (b) Forgetting (lower is better). For larger sizes of memory ER-MIR has better accuracy and improved forgetting metric. Each approach is run 10 times A Full Results on ER-MIR In this section we show full results on the ER-MIR for different settings of the buffer size for Permuted MNIST and MNIST Split. We also include results for CIFAR-10 with 1000 samples per task as studied in [3]. We note that the margins of gain for ER-MIR is lower here than in the full CIFAR-10 setting (using 9750 samples per task) suggesting ER-MIR is more effective in the more challenging settings. B Details of Hyperparameters The code to reproduce all results can be found at https://github.com/optimass/Maximally_ Interfered_Retrieval. B.1 Experience Replay Experiments For ER and ER-MIR we use the same base settings as in [3, 8]. Speciﬁcally the batch size is 10 for the incoming samples and 10 for the buffered samples. As in that work we use a learning rate of 0.05 for our MNIST experiments. For CIFAR-10 we select by validation 0.1. ER-reservoir-MIR we also add the hyperparameter of the initial sampling size, C, which is chosen from 30,50,100,150 to be 50. For MNIST we use a 2 layer MLP with 400 hidden nodes. For CIFAR-10 experiments we use a standard Resnet-18 used in [27, 6]. B.2 Generative Modeling Experiments For the Generative Replay experiments, we kept the same classiﬁer as in the Experience Replay experiments to facilitate comparison (2-layer MLP with 400 hidden nodes). Again, the batch size for the incoming data is is kept at 10. The V AE is also a MLP. We searched for the following hyperparameters: learning rate (5e-1, 1e-1, 1e-2), number of updates per incoming datapoints (2,5,10, 15, 20), generator dropout weight (0.0, 0.1, 0.2, 0.3, 0.4), the weight of the KL(q(z|x)||p(z)) in the generator loss (0.2, 0.5, 1.0, 1.0), KL cost annealing schedule in terms of number of samples before reaching ﬁnal value (1, 250, 500, 1000), number of replayed memories (1,2,4,10), coefﬁcient of the replay loss (1, 2, 3, 5), size of latent space of the generator (50, 100), depth of generator (2,3,4), number of hidden dimension for the generator (218, 256). Next, for GEN-MIR, we searched the coefﬁcient of the losses (0.0, 0.1, 1.0, 2.0) and the number of iterations (2,3,5,10). The baseline and our method was allowed an equal number or runs and the hyperpameters were chosen in such a 13way that more computation (number of memories and number of iterations) doesn’t lead to better performance (remember that more training equals more forgetting). C Further Description of Hybrid Approach We give the algorithm block fully describing the method of Sec. 3.3. Algorithm 3: AE-MIR Input: Learning rate α, Subset size C; Budget B, Gen. Epochs Ngen 1 Initialize: Memory M; θ,θae 2 for t∈1..T do 3 %%Offline Generator Training 4 for epoch∈1...Ngen do 5 for Bn ∼Dt do 6 h←Encode(θae; Bn) 7 ˜Bn ←Decode(θae; h) 8 lossae ←MSE( ˜Bn,Bn) 9 Adam (lossae, θae) 10 end 11 end 12 for Bn ∼Dt do 13 %%Virtual Update 14 θv ←SGD( Bn,α) 15 %%Autoencode batch 16 h←Encode(θae; Bn) 17 ˜Bn ←Decode(θae; h) 18 %Select C samples 19 BC∼M 20 BG ←Retrieve samples acc. to Eq 1 21 %%Store compressed rep. 22 M← UpdateMemory(h,Ln) 23 %% Train the Classifier 24 θ←SGD(˜Bn ∪BMC,α) 25 end 26 end For all experiments, we train the generator ofﬂine for 5 epochs, but still in the incremental setting. As in the replay experiments, the batch size is 10. All results are averaged over 5 runs. Ablation Study Here we provide results for the AE hybrid approach. We ﬁrst change the test set evaluation, by feeding the real images, instead of autoencoded ones. We denote this model as “- test AE\". We also look at additionally feeding real images from the current data stream, instead of reconstructed ones (i.e. replacing line 24 θ←SGD(˜Bn ∪BMC,α) as θ←SGD(Bn ∪BMC,α)). We call this model “- train & test AE\". From these results we see that never training the classiﬁer on real images is essential to obtain good results, as “- train & test AE\" performs badly. Moreover, we notice that also autoencoding the data at test time is also responsible for some performance gain. This is denoted by the small but noticeable performance increase from “- test AE\" to “AE-Random\" 14100 / 1k 500 / 5k 1k / 10k 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Accuracy ( ) - test AE - train & test AE 100 / 1k 500 / 5k 1k / 10k 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Forgetting ( ) Real Memory Slots / Compressed Memory Slots Figure 5: Ablation results 100 / 1k 500 / 5k 1k / 10k 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Accuracy ( ) 100 / 1k 500 / 5k 1k / 10k 0.0 0.1 0.2 0.3 0.4 0.5 Forgetting ( ) AE-Random AE-MIR Real Memory Slots / Compressed Memory Slots Figure 6: Reference Performance 15",
      "meta_data": {
        "arxiv_id": "1908.04742v3",
        "authors": [
          "Rahaf Aljundi",
          "Lucas Caccia",
          "Eugene Belilovsky",
          "Massimo Caccia",
          "Min Lin",
          "Laurent Charlin",
          "Tinne Tuytelaars"
        ],
        "published_date": "2019-08-11T21:16:44Z",
        "venue": "NeurIPS 2019",
        "pdf_url": "https://arxiv.org/pdf/1908.04742v3.pdf"
      }
    },
    {
      "title": "Task-free continual learning",
      "abstract": "Methods proposed in the literature towards continual deep learning typically\noperate in a task-based sequential learning setup. A sequence of tasks is\nlearned, one at a time, with all data of current task available but not of\nprevious or future tasks. Task boundaries and identities are known at all\ntimes. This setup, however, is rarely encountered in practical applications.\nTherefore we investigate how to transform continual learning to an online\nsetup. We develop a system that keeps on learning over time in a streaming\nfashion, with data distributions gradually changing and without the notion of\nseparate tasks. To this end, we build on the work on Memory Aware Synapses, and\nshow how this method can be made online by providing a protocol to decide i)\nwhen to update the importance weights, ii) which data to use to update them,\nand iii) how to accumulate the importance weights at each update step.\nExperimental results show the validity of the approach in the context of two\napplications: (self-)supervised learning of a face recognition model by\nwatching soap series and learning a robot to avoid collisions.",
      "full_text": "Task-Free Continual Learning Rahaf Aljundi∗ Klaas Kelchtermans∗ Tinne Tuytelaars KU Leuven, ESAT-PSI, Belgium firstname.lastname@esat.kuleuven.be Abstract Methods proposed in the literature towards continual deep learning typically operate in a task-based sequential learning setup. A sequence of tasks is learned, one at a time, with all data of current task available but not of previous or future tasks. Task boundaries and identities are known at all times. This setup, however, is rarely encountered in practi- cal applications. Therefore we investigate how to transform continual learning to an online setup. We develop a sys- tem that keeps on learning over time in a streaming fash- ion, with data distributions gradually changing and with- out the notion of separate tasks. To this end, we build on the work on Memory Aware Synapses, and show how this method can be made online by providing a protocol to de- cide i) when to update the importance weights, ii) which data to use to update them, and iii) how to accumulate the importance weights at each update step. Experimental re- sults show the validity of the approach in the context of two applications: (self-)supervised learning of a face recogni- tion model by watching soap series and learning a robot to avoid collisions. 1. Introduction In machine learning, one of the most basic paradigms is to clearly distinguish between a training and testing phase. Once a model is trained and validated, it switches to a test mode: the model gets frozen and deployed for inference on previously unseen data, without ever making changes to the model parameters again. This setup assumes a static world, with no distribution shifts over time. Further, it as- sumes a static task speciﬁcation, so no new requirements in terms of output (e.g. novel category labels) or new tasks added over time. Such strong division between training and testing makes it easier to develop novel machine learning algorithms, yet is also very restrictive. ∗Rahaf Aljundi and Klaas Kelchtermans contributed equally to this work and are listed in alphabetical order. Inspired by biological systems, the ﬁeld of incremental learning, also referred to as continual learning or lifelong learning [24, 34, 37], aims at breaking this strong barrier between the training and testing phase. The goal is to de- velop algorithms that do not stop learning, but rather keep updating the model parameters over time. This holds the promise of a system that gradually accumulates knowledge, reaching increasingly better accuracy and better coverage as time passes. However, it is practically not possible to store all previous data - be it due to storage constraints or for pri- vacy reasons. Yet updating parameters based only on recent data introduces a bias towards that data and a phenomenon known as catastrophic interference, in other words degrad- ing performance on older data [8, 30]. To make progress in this direction, several works have opted for a speciﬁc experimental setup, consisting of a se- quence of distinct tasks, learned one after the other. Each time, only the data for the ‘current’ task is available for training. We refer to this as task-based sequential learning. Training a shared model one task at a time has led to signif- icant progress and new insights towards continual learning, such as different strategies for preserving the knowledge of previous tasks [19, 13, 1, 17]. However, the methods de- veloped in this speciﬁc setup all too often depend on know- ing the task boundaries. These boundaries indicate good moments to consolidate knowledge, namely after learning a task. Moreover, data can be shufﬂed within a task so as to guarantee i.i.d. data. In an online setting, on the other hand, data needs to be processed in a streaming fashion and data distributions might change gradually. In this work, we aim at overcoming this requirement of hard task boundaries. In particular, we investigate how methods proposed for task-based sequential learning can be generalized to an online setting. This requires a protocol to determine when to consolidate knowledge. Moreover, we investigate the effect of keeping a small buffer with difﬁcult samples. For the latter, we take inspiration from the ﬁeld of reinforcement learning, namely experience replay [22], although using much smaller replay buffers, unlike very re- cent work of Rolnick et al. [31]. 1 arXiv:1812.03596v3  [cs.CV]  19 Aug 2019Task-based sequential learning has mostly been studied for image classiﬁcation [19, 2, 17, 39, 28]. Whenever the learner arrives at a new task, that is when learning on the previous task has converged, a standard procedure is to ex- tend the output layer of the network with additional ‘heads’ for each of the new task’s categories. Instead, the output of our network is ﬁxed. In our ﬁrst application, learning to recognize faces, we cope with a varying number of cate- gories by using an embedding rather than class predictions. In our second application, learning a lightweight robot to navigate without collisions, it is not the output labels that change over time but rather the environment. For both ap- plications, data is processed in a streaming fashion. This is challenging, since the data is not i.i.d. causing samples within one batch to be unbalanced. The contributions of this paper are as follows: i) We are the ﬁrst to extend task-based sequential learning to free and unknown task boundaries in an online continual learn- ing scenario; ii) We develop protocols to integrate an im- portance weight regularizer, MAS, in this online continual learning setting; iii) Our experiments on face recognition from TV series and on monocular collision avoidance prove the effectiveness of our method in handling the distribu- tion changes in the streaming data and stabilizing the on- line learning behaviour, resulting in knowledge accumula- tion rather than catastrophic interference and improved per- formance in all the test cases. In the following we discuss related work (section 2). We then describe our online continual learning approach in sec- tion 3. We validate our system in the experimental section 4 and end with discussion and conclusion in section 5. 2. Related Work Online Learning: Whereas in traditional ofﬂine learning, the entire training data has to be made available prior to learning the task, on the contrary online learning studies learning algorithms that learn to optimize predictive mod- els over a stream of data instances sequentially. We refer to [5, 33] for surveys and overviews on the topic. A ﬁrst set of online learning algorithms consists of dif- ferent techniques designed to learn a linear model [9, 6, 36, 12]. Online learning with kernels [14] extends this line of work to non-linear models, but the models remain shallow and their performance lags behind the modern deep neu- ral networks. Unfortunately, attempts towards online learn- ing of neural networks suffer from issues like convergence, catastrophic interference and more. Some recent works in- clude [32, 27], who both start from a small network and then adapt the capacity by adding more neurons as new samples arrive, while for online deep metric learning, [18] proposed a method based on stacking multiple metric layers. In terms of applications, the work of Pernici et al. [26, 25] is similar to our ﬁrst application scenario. They learn face identities in a self-supervised fashion via temporal con- sistency. They start from the VGG face detector and de- scriptor, and use a memory of detected faces. In contrast, we start from a much weaker pretrained model (not face- speciﬁc), and update the model parameters over time while they do not. A joint problem in continual and online learning is catastrophic interference [21, 8] which is the severe for- getting of previous samples when learning new ones. This phenomenon manifests itself at different scales: in online learning it happens while learning samples with different patterns than previous ones; in the traditional setting of continual learning it happens over a sequence of tasks. Continual Learning: In [11], Hsu et al. classify the studied scenarios for continual learning into incremental task learning, incremental domain learning and incremen- tal class learning. They argue that more attention should go to the last two - i.e. to methods that do not require to know the task identity, since that is the case encountered in most practical scenarios. Yet as indicated before, most methods to date follow the task-based sequential learning setup. This includes vari- ous regularization-based methods, such as Elastic Weight Consolidation [13], Synaptic Intelligence [39] and Memory Aware Synapses [1]. These methods estimate importance weights for each model parameter and penalize changes to parameters deemed important for previous tasks. We will discuss how to extend one of them to the online setting later. Note that, while Synaptic Intelligence computes the impor- tance weights in an online fashion, it still waits until the end of a task before updating the losses, so like the other methods it depends on knowing the task boundaries. Incre- mental Moment Matching [17] builds on similar ideas, yet stores different models for different tasks and merges them only at the very end. As such, it is unclear how this could be extended to an online, task-free setting. Also related is the work on Dynamically Expandable Networks [38]. They exploit the relatedness between the new task and previously learned tasks to determine which neurons can be reused and add new neurons to account for the new knowledge. Next there are several data-driven methods such as Learning without Forgetting [19] or Encoder-based Life- long Learning [28]. With a separate knowledge distilla- tion loss term for previous tasks, it’s again unclear how they could be applied without knowing the task identity. Other methods use an episodic memory, such as iCARL (incremental Classiﬁer and Representation Learning) [29] and Memory Based Parameter Adaptation [35]. A special mention here goes to Gradient Episodic Memory for Con- tinual Learning [20], as it moves a step forward towards the online setting: it assumes that the learner receives exam- ples one by one but simpliﬁes the scenario to locally i.i.d. drawn samples from a task distribution. Moreover, it still 2assumes that a task identiﬁer is given. Like the buffer we use, they use an episodic memory for each task consist- ing of recently seen examples. A buffer from which recent data can be reused for training is similar to the concept of a replay buffer often used in Deep Reinforcement Learning (DRL). However a crucial difference is that in both old and recent DRL works the replay buffer typically contains up to 1M samples corresponding to over 100 days of experience [22, 10]. Here, we want to keep the algorithm more online by using a buffer of up to 100 samples only. A common DRL technique, known as “prioritized sweeping”, is to sample experiences with a large error more often than others [23]. In a similar fashion we propose “pri- oritized keeping” where a hard buffer drops the easy sam- ples ﬁrst rather than the oldest. 3. Method Our goal is to design a training method for task-free online continual learning. Task-based sequential learning methods assume that data comes in tasks, with tasks bound- aries identiﬁed, so the training procedure can be divided in consecutive phases. In between the training phases, when training has stabilized, the continual learning method up- dates its meta-knowledge on how to avoid forgetting previ- ous tasks. However, in the case of online learning where data is streaming and the distribution is shifting gradually, it is unclear whether these methods can be applied and how. After studying a couple of methods mentioned above, we identiﬁed Memory Aware Synapses (MAS) [1] as the most promising method in this respect. It enjoys the following favorable characteristics. 1) Static storage requirement: it only stores an importance weight for each parameter in the network avoiding an increase of memory consumption over time; 2) Task agnostic: it can be applied to any task and is not limited to classiﬁcation. In particular, we can use it with an embedding as output, avoiding the need to add extra ‘heads’ for new outputs over time; 3)Fast: it only needs one backward pass to update the importance weights. During training, the gradients of the imposed penalty are simply the change that occurs on each parameter weighted by its im- portance. Therefore, the penalty gradients can be added lo- cally and do not need a backpropagation step; 4)top perfor- mance: MAS shows superior performance to other impor- tance weight regularizers [1, 11]. In order to deploy MAS in an online continual learning scenario, we need to deter- mine i) when to update the importance weights, ii) which data to use to update the importance weights, and iii) how to accumulate the importance weights at each update step. We ﬁrst introduce the considered online continual learn- ing setup, then explain MAS and our training procedure un- der this setup. Setup: We assume an inﬁnite stream of data and a super- visory or self-supervisory signal that is generated based on Figure 1: By detecting plateaus and peaks in the loss surface our method decides when to update the importance weights, corresponding to the Big Bang Theory experiment, see section 4.2; x-axis represents update steps few consecutive samples. At each time step s, the system receives a few consecutive samples along with their gener- ated labels {xk,yk}drawn non i.i.d from a current distri- bution Dt. Moreover, the distribution Dt could itself expe- rience sudden or gradual changes from Dt to Dt+1 at any moment. The system is unaware of when these distribu- tion changes are happening. The goal is to continually learn and update a function F that minimizes the prediction er- rors on previously seen and future samples. In other words, it aims at continuously updating and accumulating knowl- edge. Given an input model with parameters θ, the system at each time step reduces the empirical risk based on the recently received samples and a small buffer B composed of updated hard samples XB. The learning objective of the online system is: min θ L(F(X; θ),Y ) + L(F(XB; θ),YB) (1) Due to the strong non-i.i.d conditions and the very low num- ber of samples used for the gradient step, the system is vul- nerable to catastrophic interference between recent samples and previous samples and faces difﬁculty in accumulating the knowledge over time. Memory Aware Synapses (MAS) [1]:In a traditional task- based sequential learning setting, MAS works as follows. After each training phase (task), the method estimates an importance weight for each network parameter indicating the importance of this parameter to the previously learned task. To estimate the importance, MAS computes the sensi- tivity of the learned function to the parameters changes. F(xk; θ+ δ) −F(xk; θ) ≈ ∑ i gi(xk)δi (2) Ωi = 1 N N∑ k=1 ||gi(xk) || (3) where {xk}are the N samples from the previous task, δi is a small change to model parameterθiand gi(xk) = ∂F(xk) ∂θi . Ωiis the importance weight of parameterθi. When learning a new task, changes to important parameters are penalized: L(θ) = Ln(θ) + λ 2 ∑ i Ωi(θi −θ∗ i)2 (4) 3Algorithm 1Online Continual Learning 1: Input: δµ,δσ,N 2: Initialize: B= {},W= {}, 3: Ω = ⃗0,µold L = 0,σold L = 0, P= 0 4: repeat 5: Receive K recent samples X,Y 6: for nin Ndo 7: LT = Lθ(X,Y ) + Lθ(XB,YB) + λ 2 ∑ iΩi(θi−θ∗ i)2 8: θ←SGD(θ,LT) 9: if n= 1 then 10: W← update(W,L(X,Y ),L(XB,YB)) 11: end if 12: end for 13: if ¬P∧ µ(W) <δµ ∧σ(W) <δσ then 14: Ω ←update(Ω,θ, (XB,YB)) 15: θ∗←θ 16: µold L = µ(W),σold L = σ(W) 17: W= {}, P= 1 18: end if 19: if µ(W) >µold L + σold L then 20: P= 0 21: end if 22: (XB,YB) ←update((XB,YB),(X,Y ),Lθ(X,Y )) with θ∗ the parameters values at the time of importance weight estimation, i.e. the optimal parameters for the pre- vious task in the traditional sequential setup. Ln(θ) is the loss for the new task. After each task the newly estimated Ωi are accumulated to the previous estimates. When to update importance weights:In case of a task- based sequential learning setting where tasks have prede- ﬁned boundaries, importance weights are updated after each task, when learning has converged. In the online case, the data is streaming without knowledge of a task’s start or end (i.e. when distribution shifts occur). So we need a mecha- nism to determine when to update the importance weights. For this, we look at the surface of the loss function. By observing the loss, we can derive some information about the data presented to the system. When the loss decreases, this indicates that the model has learned some meaningful new knowledge from those seen samples. Yet the loss does not systematically decrease all the time. When new samples are received that are harder or contain differ- ent objects or input patterns than what was presented to the learner before, the loss may increase again. In those cases, the model has to update its knowledge, while minimally in- terfering with what has been learned previously. We can conclude that plateaus in the loss function indi- cate stable learning regimes, where the model is conﬁdently predicting the current labels, see Figure 1. Whenever the model is in such a stable area, it’s a good time to consolidate the knowledge by updating the importance weights. This way, we identify the parameters that are important for the currently acquired knowledge. When learning new, “differ- ent” samples the model will then be encouraged to preserve this knowledge. This should allow the model to accumulate knowledge over time rather than replace previously learned bits of knowledge. Detecting plateaus in the loss surface:To detect these plateaus in the loss surface, we use a sliding window over consecutive losses during training. We monitor the mean and the variance of the losses in this window and trigger an importance weight update whenever they are both lower than a given threshold. We do not keep re-estimating im- portance weights: we only re-check for plateaus in the loss surface after observing a peak. Peaks are detected when the window loss mean becomes higher than 85% of a nor- mal distribution estimated on the loss window of the previ- ous plateau - that is when µ(Lwin) > µold L + σold L where µold L and σold L are the statistics of the previously detected plateau. This accounts for the continuous ﬂuctuations in the loss function in the online learning and detects when signif- icantly harder samples are observed. A small buffer with hard samples:In a task-based se- quential learning setup, importance weights are estimated on all the training data of the previous task. This is not an option for online learning, as storing all the previous data violates the condition of our setup. On the other hand, us- ing only the most recent sequence of samples would lead to misleading estimates as these few consecutive samples might not be representative and hence do not capture the acquired knowledge correctly. To stabilize the online learn- ing, we use a small buffer of hard samples that is updated at each learning step by keeping the samples with highest loss among the new samples and the current buffer. This is important as previous samples cannot be revisited and hence gives the system the advantage to re-process those hard samples and adjust its parameters towards better pre- dictions in addition to getting a better estimate of the gra- dient step by averaging over the recent and hard samples. Moreover, the hard buffer represents a better estimate of the acquired knowledge than the few recent samples, hence al- lows for a better identiﬁcation of importance weights. Accumulating importance weights:As we frequently up- date the importance weights, simply adding the new esti- mated importance values to the previous ones as suggested in MAS [1] would lead to very high values and exploding gradients. Instead, we maintain a cumulative moving aver- age of the estimated importance weights. Note, one could deploy a decaying factor that allows replacing old knowl- edge in the long term. However, in our experiments a cu- mulative moving average showed more stable results. After updating the importance weights, the model con- tinues the learning process while penalizing changes to pa- rameters that have been identiﬁed as important so far. As such our ﬁnal learning objective is: min θ L(F(X; θ),Y )+L(F(XB; θ),YB)+λ 2 ∑ i Ωi(θi−θ∗ i)2 (5) 4Figure 2: Synthetic experiment: Predictions on ﬁrst quadrant after train- ing second quadrant. Test accuracy on ﬁrst quadrant (total test accuracy on both quadrants) overlaid. where θ∗ are the parameters values at the last importance weight update step. Algorithm 1 summarizes the different steps of the proposed continual learning system. 4. Experiments As a proof-of-concept, we validate our proposed method on a simple synthetic experiment. Later, we evaluate the method on two applications with either weak or self- supervision. First, we learn actor identities from watching soap series. The second application is robot navigation. In both cases, data is streaming and online continual learning is a key factor. 4.1. Synthetic Experiment We constructed a binary classiﬁcation problem with points in 4D in/out of the unit sphere. On a sequence of two tasks where each task corresponds to a quadrant, we test the performance of i) online without hard buffer, ii) on- line and iii) our full online continual method. Fig.2 depicts the predictions near the decision boundary in the ﬁrst quad- rant at the end of training on data in the second quadrant. The hard buffer results in better learning (higher total test accuracy), while the full method also avoids forgetting. 4.2. Continual Learning by watching Soap Series Here, we assume that an intelligent agent is watching episodes from a tv series and learns to differentiate between the faces of the different actors. The agent is equipped with a face detector module that is detecting faces online and a multi-object face tracker. In the case of weak supervision, we assume there is an annotator telling the agent whether two consecutive tracks are of the same identity or not. For the self-supervised case, we use the fact that if two faces are detected in the same image then their tracks must belong to two different actors. Setup: We start from an AlexNet [15] architecture with the convolutional layers pre-trained on ImageNet [16] and the fully connected layers initialized randomly. The output layer is of size 100. Since the input consists of two tracks of two different identities, we use the triplet margin loss [4] which has been shown to work well in face recognition ap- plications. This has the additional advantage that we don’t need to know all the identities beforehand and new actors can be added as more episodes are watched. Dataset: We use the actor labelling dataset from [3], speciﬁcally 6 episodes of The Big Bang Theory (BBT), 4 episodes of Breaking Bad (BB), and one episode of Mad Men (MM)1. Note that for BB and MM, the episodes were further split into a total of 22 and 5 chunks, respectively. For each episode we use the frames, detected faces and tracks along with track labels from [3]. Tracks are processed in chronological order, imitating the setting where tracks are extracted in an online fashion while watching the tv series. As a result, the data is clearly non-i.i.d.. For the supervised setup, every tenth/ﬁfth track is held out as test data in BBT/BB respectively as the latter has more tracks, 339 tracks BBT compared to 3941 BB. All the other tracks are used for training. As we only have one episode for MM, we decided not to use it for the supervised setup. For the self-supervised setup, BB turned out to be un- suitable, given that it is an actor centric series with a large majority of the scenes focusing on one actor. To still have results on two series, we do report also on MM in this case, in spite of it being only one episode. Further, the original tracks provided by [3] were quite short (an average of 8/22 faces per track in BBT/MM). Since this is problematic for the self-supervised setting, we use a simple heuristic based on the distance between the faces embedding (based on AlexNet pretrained on ImageNet) to merge adjacent tracks belonging to the same actor. Training: Whenever two tracks are encountered belonging to different actors, a training step is performed using the detected faces (one face every 5 frames). If the two tracks contain more than 100 faces, a random sampling step is per- formed. We use a hard buffer size of 100 triplets and a ﬁxed loss window size of 5. A few gradient steps are performed at each time step (2-3 for the supervised setting, 10-15 for the self-supervised one). We use SGD optimizer with a learning rate of 10−4. Hyperparameters were set based on the ﬁrst BBT episode, please refer to the supplementary material for more details. Test: To test the accuracy of the trained model on recog- nizing the actors in the tv series, we use 5 templates of each actor selected from different episodes. We then compute the Euclidean distance of each test face to the templates, based on the learned representation, and assign the input face to the identity of the template that is closest. Baselines: To estimate the beneﬁt of our system, Online Continual, we compare it against the following baselines: 1. Initial : the pretrained model, i.e. before training on any of the episodes. 2. Online Baseline : a model trained in the explained on- line setting but without the MAS importance weight regularizer. 1Unfortunately, there was an issue with the labels for the other episodes of Mad Men, which prevented us from using these. 5ep1 ep2 ep3 ep4 ep5 ep6 45 50 55 60 65 70 75 80Accuracy % Online Continual (Ours) Online Online Joint  Offline Joint  Initial (a) ch1 ch5 ch9 ch13 ch17 ch21 20 30 40 50Accuracy % Online Continual (Ours) Online Online Joint  Offline Joint  Initial (b) ep1 ep2 ep3 ep4 ep5 ep6 45 50 55 60 65 70 75Accuracy % Online Continual (Ours) Online Online Continual  Decaying Online Continual No Hard Buffer Online No Hard Buffer Initial (c) Figure 3: Accuracy on test data at the end of each episode for Big Bang Theory (a) and after chunks 1,5,9,13,17 and 21 of Breaking Bad (b). (c) a study on the importance of the hard buffer and the cumulative Ω average versus a decaying Ω, ﬁgure shows the test accuracy after each episode of BBT. 3. Online Joint Training : a model trained online, again without MAS regularization, but with shufﬂed tracks across episodes to obtain i.i.d. drawn data. 4. Ofﬂine Joint Training : a model that differs from On- line Joint Training by going multiple epochs over the whole data. This stands as an upper bound. 4.2.1 Weak Supervision Results Figure 3 (a) shows the actor recognition accuracy evalu- ated on all the test data of BBT, at the end of each episode. Initially, the Online Baseline (orange) obtains an increase of 20% in accuracy compared to the initial model. Yet it fails to continue accumulating knowledge and improving the accuracy as training continues. After the third episode, the overall accuracy starts to decay, probably because the knowledge learned from these new episodes interferes with what was learned previously. In contrast, our Online Con- tinual Learning system (blue) continues to improve its per- formance and achieves at the end of the 6 episodes an ac- curacy that matches the accuracy of the model trained with shufﬂed data under the i.i.d. condition (Online Joint Train- ing, pink). Ofﬂine Joint Training (purple) with multiple revisits to the shufﬂed data achieves the top performance. Note that this is only 8% higher than our continual learning system trained under the online and changing distribution condition. Figure 3 (b) shows the accuracy on all the test data of BB, after each 4 chunks while learning the 4 episodes. Clearly this tv series is much harder than BBT. Most of the shots are outdoor and under varied lighting conditions, as also noted in [3]. This corresponds to large distribution changes within and between episodes. Here, the Online Baseline (orange) fails to increase the performance after the ﬁrst episode. Its accuracy notably ﬂuctuates, probably depending on how (un)related the recently seen data is to the rest of the series. Again, our Online Continual Learning system (blue) suc- ceeds in improving and accumulating knowledge – up to a 100% improvement over the Online Baseline. Like the On- line Baseline, its performance drops at times, yet the drops are dampened signiﬁcantly, allowing the model to keep on learning over time. Surprisingly, it even outperforms the Online Joint Training baseline (pink) and comes close to the Ofﬂine Joint training upper bound (purple) that only reaches this accuracy after ten revisits to the training data. 4.2.2 Self Supervision Results Next we move to the case with self-supervision. This sce- nario reﬂects the ideal case where continual learning be- comes most interesting. Remember that, as a clue for self- supervision, we use the fact that multiple tracks appearing in the same image should have different identities. We use the six episodes of BBT, although only the ﬁrst and the sixth episodes actually have a good number of tracks with two persons appearing in one image. Figure 4 (a) shows the ac- curacy on all the episodes after learning each episode. Note how the Online Learning Baseline (orange) continues to im- prove slightly as more episodes are watched. It’s only when we get to the last episode, with a larger number of useful tracks, that our Continual Online Learning (blue) starts to outperform the Online Learning Baseline. Figure 4 (b) shows the recognition accuracy on the ﬁrst episode of Mad Men after each chunk. Similar to the previ- ous experiments our Online Continual learning (blue) suc- ceeds in improving the performance and accumulating the knowledge. We conclude that the ability of continual learn- ing of stabilizing the online learning is clearly shown, both for weak and self-supervised scenarios. 4.2.3 Ablation Study Next we perform an ablation study to evaluate the impact of two components of our system. The ﬁrst factor is the hard buffer used for stabilizing the online training and for updating the importance weights. The second factor is the mechanism for accumulating importance weights across up- dates. In our system we use a cumulative moving average, which gives all the estimated importance weights the same weight. An alternative is to deploy a decaying average. This reduces the impact of old importance weights in favor of the newest ones. To this end, one can set Ωt = (Ωt−1 + Ω∗)/2 where Ω∗ are the currently estimated importance weights. Figure 3(c) shows the accuracy on all the test data of BBT after each episode achieved by the different variants. The hard buffer clearly improves the performance of both the Online Baseline and Online Continual learning. The buffer 6ep1 ep2 ep3 ep4 ep5 ep6 44 46 48 50 52 54 56 58Accuracy % Online Continual Selfsupervised (Ours) Online Selfsupervised Initial (a) ch1 ch2 ch3 ch4 ch5 28 30 32 34 36 38Accuracy % Online Continual (Ours) Online Initial (b) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Chunks 0 20 40 60 80 100Test accuracy Skyler-Ours Walter-Ours Skyler-Online Walter-Online (c) Figure 4: Self-supervised setting: accuracy on all faces of Big Bang Theory after each episode (a) and of Mad Men after each of the 5 chunks (b). (c) a study on the actors recognition during the course of training, ﬁgure shows two main actors test accuracy after each chunk in BB. Figure 5: Example views in the corridor sequence corresponding to en- vironments A, B, C and D, depicted from left to right. with hard samples, even if small, gives the learner a chance to re-pass over hard samples and to adjust its gradients for a better estimate of the parameters update step. Additionally, it allows a better estimate of the importance weights used in our Online Continual Learning. The decaying average for the importance weights update, leads to more ﬂuctua- tions due to the higher impact of more recent importance estimates. This allows more forgetting and more bias to- wards the recent estimate that could be unrepresentative to the overall test data. Relationship between samples and recognition perfor- mance during training:To show how the predictions on the seen actors change over the online training time, we plotted the accuracies per actor after each chunk (for two most frequent characters of BB, to avoid overloading the ﬁgure), see Fig.4(c). Marker size indicates the actor’s fre- quency in a chunk; no marker indicates zero appearance. Low frequency in a chunk typically causes the accuracy of the online baseline to drop while our method is more stable. 4.3. Monocular Collision Avoidance Collision avoidance is the task in which a robot navi- gates through the environment aimlessly while avoiding ob- stacles. We train a neural network to perform this task, at test time, based on single RGB images. Training is done with self-supervision where a simple heuristic based on ex- tra sensors, serves as a supervising expert. The deep neural network learns to imitate the expert’s control, so cloning its behavior. The task of collision avoidance is best demon- strated in a variety of environments. However, hardware or legal constraints might prevent storing all training data, urging the need for an online learning setup. As the net- work tends to forget what it has learned over time, the setup is excellent for testing online continual learning. Architecture: Our model takes a 128x128 RGB frame as input and outputs three discrete steering directions. The ar- chitecture consists of 2 convolutional and 2 fully-connected layers with ReLU-activations. The training starts with ran- dom initialization of the weights and continues with gradi- ent descent on a cross-entropy loss. Simulation: The experiment is done in a Gazebo simulated environment with the Hector Quadrotor model. The ex- pert is a heuristic reading scans from a Laser Range Finder mounted on the drone and turning towards the direction with the furthest depth. The demonstration of the expert follows a sequence of four different corridors, referred to as A,B,C and D. The environments differ in texture, obstacles and turns, as visible in ﬁgure 5. Training: Every 10 steps a backward pass occurs, minimiz- ing the cross-entropy loss, shown in the lower right of ﬁgure 6. For each model, three networks are trained with different seeds resulting in the error bars plotted. Test: The models are evaluated on the entire data sequence as reported in ﬁgure 6. The grey bars on the x-axis indicate crossings to new environments. Besides the general online with no continual learning baseline, the performance of fol- lowing models are given: a scratch initialized model, an online jointly trained model as well as ofﬂine. The online joint model has seen all the data once but in an i.i.d. manner. The accuracy of both the online with and without contin- ual learning increases in environments where it is currently learning. Online training without continual training, how- ever, tends to forget the early environments like A, B and C while training in new environments. Especially in environ- ment B and D, the effect is outspoken. The cross-entropy loss in environment D rises for all models, indicating a sig- niﬁcant change in the data. 4.4. Proof Of Concept in the Real World In a ﬁnal experiment, we apply online continual learning on a turtlebot in a small arena in our lab, see ﬁgure 7. The model is on-policy pretrained in a similar simulated environment without continual learning. On-policy refers to the model being in control during training instead of the expert. In the previous experiments, continual learning has proven to be advantageous when big differences occur in the data. In this setup we show that continual learning also provides stabilization during on-policy training within one environment. Again, an expert based on the Laser Range Finder is providing a self-supervisory signal. On-policy learning tends to be more difﬁcult as the data contains a lot of “dummy” samples when the model visits irrelevant states. This data inefﬁciency causes the model to learn slower and possibly forget along the way. For example, if 7Figure 6: The training accuracies on the different types of corridors as well as the total accuracy during training on the corridor sequence (A,B,C,D) as depicted in ﬁgure 5. Grey lines indicate the transition to a new environment. The lower right ﬁgure shows the cross-entropy loss on the recent buffer. The accuracies of the baselines are added as horizontal lines for the initial model, the jointly trained model both online and ofﬂine. Figure 7: Left: Real-world online and on-policy setup. Right: Number of collisions per training step. Using online continual learning speeds up the training. the model collides on the left side, the recent data teaches the model to turn right more often. However, after crossing the arena and bumping on the right side, you still want the model to remember its mistakes made earlier. As such, preserving acquired knowledge over time is crucial for on-policy online learning. In ﬁgure 7, we show the number of collisions per step over time with error bars taken over three different models. Clearly continual learning helps the model to learn faster, with the number of collisions dropping faster than without it. 5. Discussion and Conclusion The importance weight regularization appears most ef- fective in online training scenarios when large changes in the learned distribution occur. The closer the online data stream is to i.i.d. samples, the smaller the positive continual learning effect. In some cases however, continual learning tends to slow down the adaptation to newly seen data. Especially when the new data is much more informative or representative than the old, continual learning initially has a negative ef- fect on the training. In other words, pure online learning is faster to adapt to new changes but therefore also inher- ently less stable. Ultimately, whether the stabilizing effect of continual learning is advantageous or not, depends on the time scale of the changes in the data. While in this work we focus on a setting where the net- work architecture remains ﬁxed, and no new outputs or tasks are added over time, we believe it could also be ap- plied in other settings. For instance in a class-incremental setting, an extra head could be added to the network each time a new category label appears. Alternatively, a projec- tion into an embedding space could be used, as in [7], avoid- ing the need for a growing network architecture. These are directions for future work. Due to the limited time, we used data from published datasets in the face recognition experiment allowing quan- titative evaluation. However, as future work, we plan to test self-supervised online continual learning on large scale tv- series, thus learning for a longer time. In conclusion, we pushed the limits of current task-based sequential learning towards online task-free continual learn- ing. We assume an inﬁnite stream of input data, containing changes in the input distribution both gradual and sudden. Our protocol deploys a state of the art importance weight regularization method for online continual learning by de- tecting when, how and on what data to perform importance weight updates. Its effectiveness is validated successfully for both supervised and self-supervised learning. More speciﬁcally, by using our continual learning method, we demonstrate an improvement of stability and performance over the baseline in applications like learning face identities from watching tv-series and robotic collision avoidance. Acknowledgments: Rahaf Aljundi’s PhD is funded by an FWO scholarship. This work was further supported by the CAMETRON research project (GOA) of the KU Leuven and the FWO SBO project Omnidrone. 8References [1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. arXiv preprint arXiv:1711.09601, 2017. [2] R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert gate: Lifelong learning with a network of experts. InIEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) , 2016. [3] R. Aljundi, P. Chakravarty, and T. Tuytelaars. Whos that ac- tor? automatic labelling of actors in tv series starting from imdb images. In Asian Conference on Computer Vision , pages 467–483. Springer, 2016. [4] V . Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learn- ing local feature descriptors with triplets and shallow con- volutional neural networks. BMVC, pages 119.1–119.11, 01 2016. [5] L. Bottou. Online learning and stochastic approximations. On-line learning in neural networks, 17(9):142, 1998. [6] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradi- ent methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011. [7] M. Elhoseiny, F. babiloni, R. Aljundi, M. Rohrbach, and T. Tuytelaars. Exploring the challenges towards lifelong fact learning. In Asian Conference on Computer Vision, 2018. [8] R. M. French. Catastrophic forgetting in connectionist net- works. Trends in cognitive sciences, 3(4):128–135, 1999. [9] E. Hazan, A. Rakhlin, and P. L. Bartlett. Adaptive online gra- dient descent. In Advances in Neural Information Processing Systems, pages 65–72, 2008. [10] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostro- vski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017. [11] Y .-C. Hsu, Y .-C. Liu, and Z. Kira. Re-evaluating contin- ual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. [12] T. Hu. Online regression with varying gaussians and non-identical distributions. Analysis and Applications , 9(04):395–408, 2011. [13] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ra- malho, A. Grabska-Barwinska, et al. Overcoming catas- trophic forgetting in neural networks. arXiv preprint arXiv:1612.00796, 2016. [14] J. Kivinen, A. J. Smola, and R. C. Williamson. Online learn- ing with kernels. IEEE transactions on signal processing , 52(8):2165–2176, 2004. [15] A. Krizhevsky. One weird trick for parallelizing convo- lutional neural networks. arXiv preprint arXiv:1404.5997 , 2014. [16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems , pages 1097–1105, 2012. [17] S.-W. Lee, J.-H. Kim, J.-W. Ha, and B.-T. Zhang. Overcom- ing catastrophic forgetting by incremental moment match- ing. arXiv preprint arXiv:1703.08475, 2017. [18] W. Li, J. Huo, Y . Shi, Y . Gao, L. Wang, and J. Luo. On- line deep metric learning. arXiv preprint arXiv:1805.05510, 2018. [19] Z. Li and D. Hoiem. Learning without forgetting. In Eu- ropean Conference on Computer Vision , pages 614–629. Springer, 2016. [20] D. Lopez-Paz et al. Gradient episodic memory for contin- ual learning. In Advances in Neural Information Processing Systems, pages 6470–6479, 2017. [21] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989. [22] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve- ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep rein- forcement learning. Nature, 2014. [23] A. W. Moore and C. G. Atkeson. Prioritized sweeping: Re- inforcement learning with less data and less time. Machine Learning, 13(1):103–130, Oct 1993. [24] A. Pentina and C. H. Lampert. Lifelong learning with non- i.i.d. tasks. NIPS, 2015. [25] F. Pernici, F. Bartoli, M. Bruni, and A. D. Bimbo. Mem- ory based online learning of deep representations from video streams. CoRR, abs/1711.07368, 2017. [26] F. Pernici and A. Del Bimbo. Unsupervised incre- mental learning of deep descriptors from video streams. ICMEW.2017.8026276., pages 477–482, 2017. [27] S. Ramasamy, K. Rajaraman, P. Krishnaswamy, and V . Chandrasekhar. Online deep learning: growing rbm on the ﬂy. arXiv preprint arXiv:1803.02043, 2018. [28] A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars. Encoder based lifelong learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1320–1328, 2017. [29] S.-A. Rebufﬁ, A. Kolesnikov, and C. H. Lampert. icarl: Incremental classiﬁer and representation learning. arXiv preprint arXiv:1611.07725, 2016. [30] A. Robins. Catastrophic forgetting, rehearsal and pseudore- hearsal. Connection Science, 7(2):123–146, 1995. [31] D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, and G. Wayne. Unsupervisedexperience replay for continual learning. arxiv:1811.11682., 2018. [32] D. Sahoo, Q. Pham, J. Lu, and S. C. Hoi. Online deep learn- ing: Learning deep neural networks on the ﬂy.arXiv preprint arXiv:1711.03705, 2017. [33] S. Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and TrendsR⃝ in Machine Learn- ing, 4(2):107–194, 2012. [34] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algorithms. In AAAI Spring Sym- posium: Lifelong Machine Learning, pages 49–55. Citeseer, 2013. 9[35] P. Sprechmann, S. M. Jayakumar, J. W. Rae, A. Pritzel, A. P. Badia, B. Uria, O. Vinyals, D. Hassabis, R. Pascanu, and C. Blundell. Memory-based parameter adaptation. arXiv preprint arXiv:1802.10542, 2018. [36] A. L. Strehl and M. L. Littman. Online linear regression and its application to model-based reinforcement learning. In Advances in Neural Information Processing Systems , pages 1417–1424, 2008. [37] S. Thrun and T. M. Mitchell. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25–46, 1995. [38] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learn- ing with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2018. [39] F. Zenke, B. Poole, and S. Ganguli. Improved multitask learning through synaptic intelligence. In Proceedings of the International Conference on Machine Learning (ICML), 2017. 10Supplementary Materials These supplementary materials contain the following extra in- formation: • Hyperparameters and architectural details for the experi- ments from section 4. • Example images of the soap series. • Results on collision avoidance in an extra lengthy simulated corridor. • Details and extra results on the real-world collision avoid- ance with the Turtlebot. • Closing discussion and guidelines on the application of con- tinual learning in an online setting. 6. Hyper-parameters and architectural details As to be able to reproduce the results, we provide the reader with the used hyperparameters and networks, see table 1. Regu- larization weight corresponds to λ, the continual learning weight in Equation 5. The Tiny v2 network for the collision avoidance task is a net- work build especially small in order to allow faster training. The details of the network can be found in ﬁgure 8. 7. Examples of the soap series data (Sec. 4.1) In ﬁgure 9, 4 example frames are shown for each of the dif- ferent soap series: Big Bang Theory, Breaking Bad and Mad Men. ∗Rahaf Aljundi and Klaas Kelchtermans contributed equally to this work and are listed in alphabetical order. Exp 1 Exp 2 Exp 3 Architecture Alexnet Tiny v2 Tiny v2 Initialization imagenet random random Learning rate 0.0001 0.01 0.01 Optimizer SGD SGD SGD Hard Buffer Size 100 40 30 Regularization Weight 100 0.5 0.5 Threshold Mean Loss 0.3 0.5 0.5 Threshold Variance Loss 0.1 0.1 0.02 Length Window Loss 5 5 5 Table 1: Hyperparameters for different experiments: exp 1 ∼Soap Series (4.1), exp 2 ∼Simulated Corridor (4.2) and exp 3 ∼Real Turtlebot (4.3). These examples demonstrate the scene diversity and large variance in imaging conditions. As mentioned in the paper, Breaking Bad is more actor-centric with a majority of the frames showing only the main character, making it less suited for the self-supervised setup. 8. Larger experiment on collision avoidance in simulation (Sec. 4.2) To demonstrate both the strengths and weaknesses of our con- tinual learning method, we expanded the corridor experiment of Sec. 4.2 to a sequence of 10 corridors, equal to about 20 minutes ﬂying time or around 10.000 frames. The sequence of different corridors is depicted in ﬁgure 12, exhibiting a large variety in tex- tures and obstacles. The length of the sequence allows us to see the longer trend of continual learning. While training the models online, we evaluate the accuracy on different corridors separately. Due to an imbalance over actions within one corridor, we perform an evaluation based on the total accuracy averaged over the different actions, referred as ’Weighted Accuracy’. When a model becomes degenerated, thus only pre- dicting the most common action in a corridor, an unnormalized accuracy would remain high. We observe that this data imbalance also affects the online learning as often multiple gradient steps are taken in favor of only some actions. To bypass this impediment, we experiment here with an additional normalization constraint on the hard buffer forc- ing an equal distribution over all actions. Results In ﬁg. 10, we show the improvement obtained by our proposed online continual learning method over the online baseline, both with and without the normalization constraint on the buffer with hard examples. The bars express the ﬁnal accuracy of each corri- dor as a mean over three models trained with different seeds. The normalization constraint has a positive effect on both continual and normal online learning. Our online continual learning process clearly outperforms the online baseline for most of the corridors. Without the action normalization, the models fail to learn certain corridors, like 1 and 4, resulting in no knowledge that can be pre- served by our continual learning method. However if the model grasps information while going through a corridor, it succeeds at preserving it with continual learning, outperforming the online baseline with 15 to 20% accuracy. Moreover with the action nor- malization, continual learning succeeds at acquiring knowledge in each corridor, outperforming the baseline in all but last corridors. Figure 13 provides a more in-depth analysis. Here, we show the evolution over time of the cross-entropy loss and the total ac- 11Figure 8: Architecture of the Tiny v2 network used in the monocular collision avoidance experiments (4.2 and 4.3). Figure 9: Four example images for each soap series, from left to right: Big Bang Theory, Breaking Bad and Mad Men. Figure 10: Accuracy’s for all 10 corridors at the end of training on the corridor sequence without (top) and with (bottom) the normalization constraint on the hard buffer. curacy over all corridors. We also report the evolution over time of the weighted accuracy, for each corridor separately. From these plots, one can conclude that the buffer normalization clearly has beneﬁcial effects for online learning, especially in the green areas (corresponding to learning taking place on imbalanced corridors). However, the constraint leaves less room in the hard buffer for recent samples causing a slower adaptation of the model during training, as can be observed in the red areas, allowing the models without normalization to improve faster. In multiple examples, highlighted with blue, the continual learning allows a preservation of knowledge seen before, demon- strating the success of our method. The trend is most clear for the early corridors as the forgetting tends to be worse over time. This phenomenon is also responsible for the total accuracy reach- ing 80% for continual learning instead of only 70 % for normal online learning. This positive trend can be expected to increase when learning over even longer sequences. In some cases, highlighted in orange, the baseline performance of an old corridor improves while training in a new corridor, reach- ing a similar accuracy as our continual learning method. In other words, the impact of forgetting seems less as the baseline is able to learn the same knowledge again. This lengthy experiment demonstrates the strengths of contin- ual learning, including the expected positive trend when applying it to longer sequences of data. 9. Collision avoidance on real Turtlebot (Sec. 4.3) In this proof-of-concept, an neural network steers a turtlebot around one big yellow object (see ﬁgure 7 in the paper). Each frame is kept in a buffer containing the 40 most recent frames combined with expert labels. Every 10 frames a gradient step is taken. When a collision is detected by the Lazer Range Finder, the training is paused and the Turtlebot turns automatically such that the closest obstacle is at its back. The hyperparameters can be found in table 1. Each model is trained three times and takes about 20 minutes, or 300 gradient steps, till convergence. Extra results and baselines are shown in ﬁgure 11, plotting the total number of collisions divided by the total number of gradient 12Figure 11: Performance expressed as the average number of collisions - i.e. the total number of collisions divided by the total number of gradient steps. steps. Driving straight leads to an average of 0.6 collisions per gradient step. Adding action normalization in the hard buffer and applying continual learning both have a clear positive inﬂuence. The action normalization allows an even larger improvement of our continual learning method over the baseline. This real-world experiment differs in two signiﬁcant ways from the previous experiments. First, the agent stays in one domain that does not vary over time. Second, the agent acts within the envi- ronment to create new data making the setup on-policy and online. Although there are no domain or task changes over time, our con- tinual learning method has a clear positive effect. This result fully supports our claim of ”Task-Free” continual learning, namely that it is not required to have signiﬁcant changes in your data in order to do better than a normal online learner. The continual learning method inherently stabilizes the online learning in an on-policy setup. However, a major challenge in online/on-policy learning is dealing with uninformative states. These states lead to less infor- mation in a batch and thus slower training. Samples that do contain relevant information, are better preserved in the online-continual setting, resulting in faster learning. Unfortunately, the exact mo- ment of large information gain varies over different runs resulting in a higher variance. This explains the larger variance in ﬁgure 11 and 7. 10. Closing discussion / General guidelines When considering applying continual learning to a speciﬁc problem, it is best to keep two guidelines in mind: The mean and variance thresholdof the loss window should be carefully chosen. If both thresholds are too low, the model will not use the MAS regularization; conversely, if too high, the model will slow down the learning by preserving irrelevant information as the importance weights are updated too frequently. The latter case in combination with global averaging usually deteriorates the ﬁnal performance. Therefore, it is recommended to place the thresh- old low enough while still allowing importance weight updates. As relaxing the threshold, results in more updates, a decaying up- date rule allows the model to forget irrelevant previous knowledge. In practice, we discovered that the mean threshold could remain quite high, as long as the variance threshold is low. Moreover, meta-learning techniques, such as learning-to-learn, could auto- mate these settings. A trainable task is a necessary condition: In order to have the MAS regularization exceed in performance over the baseline, the task must be actually trainable. Although this seems obvious, it is far from trivial in an online learning setting, due to the non- i.i.d. nature. Predicting whether continual learning will perform better than typical online learning depends on stable training. For instance, in the collision avoidance task, including an action nor- malization constraint in the hard buffer, clearly improves the sta- bility of online learning. In conclusion, we successfully extended continual learning to a task-free online learning algorithm and demonstrated its advan- tage in following applications: face recognition in soap series, and monocular collision avoidance both on a drone in simulation and on a Turtlebot in the real-world. 13Figure 12: Example views in the longer corridor sequence, corresponding to 10 environments depicted in lexicographic order. Figure 13: Cross-Entropy loss and accuracy’s on total and separate corridors while training online on the sequence of 10 corridors. Blue squares indicate continual learning outperforming baseline models. Green squares indicate positive normalization effects for both continual and baseline models. Red squares indicate slower learning due to normalization constraint. Orange squares indicate learning forgotten knowledge by the baseline model. 14",
      "meta_data": {
        "arxiv_id": "1812.03596v3",
        "authors": [
          "Rahaf Aljundi",
          "Klaas Kelchtermans",
          "Tinne Tuytelaars"
        ],
        "published_date": "2018-12-10T02:07:57Z",
        "pdf_url": "https://arxiv.org/pdf/1812.03596v3.pdf"
      }
    },
    {
      "title": "Gradient based sample selection for online continual learning",
      "abstract": "A continual learning agent learns online with a non-stationary and\nnever-ending stream of data. The key to such learning process is to overcome\nthe catastrophic forgetting of previously seen data, which is a well known\nproblem of neural networks. To prevent forgetting, a replay buffer is usually\nemployed to store the previous data for the purpose of rehearsal. Previous\nworks often depend on task boundary and i.i.d. assumptions to properly select\nsamples for the replay buffer. In this work, we formulate sample selection as a\nconstraint reduction problem based on the constrained optimization view of\ncontinual learning. The goal is to select a fixed subset of constraints that\nbest approximate the feasible region defined by the original constraints. We\nshow that it is equivalent to maximizing the diversity of samples in the replay\nbuffer with parameters gradient as the feature. We further develop a greedy\nalternative that is cheap and efficient. The advantage of the proposed method\nis demonstrated by comparing to other alternatives under the continual learning\nsetting. Further comparisons are made against state of the art methods that\nrely on task boundaries which show comparable or even better results for our\nmethod.",
      "full_text": "Gradient based sample selection for online continual learning Rahaf Aljundi∗ KU Leuven rahaf.aljundi@gmail.com Min Lin Mila mavenlin@gmail.com Baptiste Goujaud Mila baptiste.goujaud@gmail.com Yoshua Bengio Mila yoshua.bengio@mila.quebec Abstract A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a ﬁxed subset of constraints that best approximate the feasible region deﬁned by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efﬁcient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method. 1 Introduction The central problem of continual learning is to overcome the catastrophic forgetting problem of neural networks. Current continual learning methods can be categorized into three major families based on how the information of previous data are stored and used. We describe each of them below. The prior-focused Prior-focused methods use a penalty term to regularize the parameters rather than a hard constraint. The parameter gradually drifts away from the feasible regions of previous tasks, especially when there is a long chain of tasks and when the tasks resemble each other [6]. It is often necessary to hybridize the prior-focused approach with the replay-based methods for better results [13, 6]. Another major family is the parameter isolation methods which dedicates different parameters for different tasks to prevent interference. Dynamic architectures that freeze/grow the network belong to this family. However, it is also possible to isolate parameters without changing the architecture [7, 11]. ∗Work mostly done while ﬁrst author was a visiting researcher at Mila. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1903.08671v5  [cs.LG]  31 Oct 2019Both of the above strategies explicitly associate neurons with different tasks, with the consequence that task boundaries are mandatory during both training and testing. Due to the dependency on task boundaries during test, this family of methods tilts more towards multi-task learning than continual learning. The replay-based approach stores the information in the example space either directly in a replay buffer or in a generative model. When learning new data, old examples are reproduced from the replay buffer or generative model, which is used for rehearsal/retraining or used as constraints for the current learning, yet the old examples could also be used to provide constraints [10]. Most of the works in the above three families use a relaxed task incremental assumption: the data are streamed one task at a time, with different distributions for each task, while keeping the independent and identically distributed (i.i.d.) assumption and performing ofﬂine training within each task. Consequently, they are not directly applicable to the more general setting where data are streamed online with neither i.i.d. assumption nor task boundary information. Both prior-focused methods and replay-based methods have the potential to be adapted to the general setting. However, we are mostly interested in the replay-based methods in this work, since it is shown that prior-focused methods lead to no improvement or marginal improvement when applied on top of a replay-based method. Speciﬁcally, we develop strategies to populate the replay buffer under the most general condition where no assumptions are made about the online data stream. Our contributions are as follows: 1) We formulate replay buffer population as a constraint selection problem and formalize it as a solid angle minimization problem. 2) We propose a surrogate objective for it and empirically verify that the surrogate objective aligns with the goal of solid angle minimiza- tion 3) As a cheap alternative for large sample selection, we propose a greedy algorithm that is as efﬁcient as reservoir sampling yet immune to imbalanced data stream. 4) We compare our method to different selection strategies and show the ability of our solutions to always select a subset of samples that best represents the previous history 5) We perform experiments on continual learning benchmarks and show that our method is on par with, or better than, the previous methods. Yet, requiring no i.i.d. assumptions or task boundaries. 2 Related Work Our continual learning approach belongs to the replay based family. Methods in this family alleviate forgetting by replaying stored samples from previous history when learning new ones. Although storage of the original examples in memory for rehearsal dates back to 1990s [16], to date it is still a rule of thumb to overcome catastrophic forgetting in practical problems. For example, experience replay is widely used in reinforcement learning where the data distributions are usually non-stationary and prone to catastrophic forgetting [9, 12]. Recent works that use replay buffer for continual learning include iCaRL [14] and GEM [10], both of which allocate memory to store a core-set of examples from each task. These methods still require task boundaries in order to divide the storage resource evenly to each task. There are also a few previous works that deals with the situation where task boundary and i.i.d. assumption is not available. For example, reservoir sampling has been employed in [5, 8] so that the data distribution in the replay buffer follows the data distribution that has already been seen. The problem of reservoir sampling is that the minor modes in the distribution with small probability mass may fail to be represented in the replay buffer. As a remedy to this problem, coverage maximization is also proposed in [8]. It intends to keep diverse samples in the replay buffer using Euclidean distance as a difference measure.While the Euclidean distance may be enough for low dimensional data, it could be uninformative when the data lies in a structured manifold embedded in a high dimensional space. In contrast to previous works, we start from the constrained optimization formulation of continual learning, and show that the data selection for replay buffer is effectively a constraint reduction problem. 3 Continual Learning as Constrained Optimization We consider the supervised learning problem with an online stream of data where one or a few pairs of examples (x,y) are received at a time. The data stream is non-stationary with no assumption on the distribution such as the i.i.d. hypothesis. Our goal is to optimize the loss on the current example(s) without increasing the losses on the previously learned examples. 23.1 Problem Formulation We formulate our goal as the following constrained optimization problem. Without loss of generality, we assume the examples are observed one at a time. θt = argmin θ ℓ(f(xt; θ),yt) (1) s.t. ℓ(f(xi; θ),yi) ≤ℓ(f(xi; θt−1),yi); ∀i∈[0 ..t −1] f(.; θ) is a model parameterized by θand ℓis the loss function. tis the index of the current example and iindexes the previous examples. As suggested by [ 10], the original constraints can be rephrased to the constraints in the gradient space: ⟨g,gi⟩= ⟨∂ℓ(f(xt; θ),yt) ∂θ ,∂ℓ(f(xi; θ),yi) ∂θ ⟩ ≥0; (2) However, the number of constraints in the above optimization problem increases linearly with the number of previous examples. The required computation and storage resource for an exact solution of the above problem will increase indeﬁnitely with time. It is thus more desirable to solve the above problem approximately with a ﬁxed computation and storage budget. In practice, a replay buffer M limited to M memory slots is often used to keep the previous examples. The constraints are thus only active for (xi,yi) ∈M. How to populate the replay buffer then becomes a crucial research problem. Gradient episodic memory (GEM) assumes access to task boundaries and an i.i.d. distribution within each task episode. It divides the memory budget evenly among the tasks. i.e. m = M/T slots is allocated for each task, where T is the number of tasks. The last mexamples from each task are kept in the memory. This has clear limitations when the task boundaries are not available or when the i.i.d. assumption is not satisﬁed. In this work, we consider the problem of how to populate the replay buffer in a more general setting where the above assumptions are not available. 3.2 Sample Selection as Constraint Reduction Motivated by Eq.1, we set our goal to selecting M examples so that the feasible region formed by the corresponding reduced constraints is close to the feasible region of the original problem. We ﬁrst convert the original constraints in 2 to the corresponding feasible region: C = ⋂ i∈[0..t−1] {g|⟨g,gi⟩≥ 0} (3) We assume here that C is generally not empty. It is highly unlikely to happen if we consider a number of parameters much larger than the number of gradients gi, except if we encounter an outlier that has different label yi with the same input xi. In this work we don’t consider the existence of outliers. Geometrically, Cis the intersection of the half spaces described by ⟨g,gi⟩≥ 0, which forms a polyhedral convex cone. The relaxed feasible region corresponding to the replay buffer is: ˜C = ⋂ gi∈M {g|⟨g,gi⟩≥ 0} (4) For best approximation of the original feasible region, we require ˜Cto be as close to Cas possible. It is easy to see that C ⊂ ˜Cbecause M⊂ [g0 ..g t−1]. We illustrate the relation between Cand ˜Cin Figure 1. On the left, Cis represented while the blue hyperplane on the right corresponds to a constraint that has been removed. Therefore, ˜C (on the right) is larger than C for the inclusion partial order. As we want ˜C to be as close to C as possible, we actually want the \"smallest\" ˜C, where \"small\" here remains to be deﬁned, as the inclusion order is not a complete order. A potential measure of the size of a convex cone is its solid angle deﬁned as the intersection between the cone and the unit sphere. minimizeMλd−1  Sd−1 ∩ ⋂ gi∈M {g|⟨g,gi⟩≥ 0}   (5) 3C C~ Figure 1: Feasible region (poly- hedral cone) before and after con- straint selection. The selected constraints (excluding the blue one) are chosen to best approxi- mate the original feasible region. 8.5 9.0 9.5 10.0 10.5 log surrogate 20.0 17.5 15.0 12.5 10.0 7.5 5.0 2.5 0.0 log angle Generated Samples Linear Approximation Figure 2: Correlation between solid angle and our proposed sur- rogate in 200 dimension space in log scale. Note that we only need monotocity for our objective to hold. Figure 3: Relation be- tween angle formed by two vectors (α) and the associ- ated feasible set (grey re- gion) where ddenotes the dimension of the space,Sd−1 the unit sphere in this space, andλd−1 the Lebesgue measure in dimension d−1. Therefore, solving 5 would achieve our goal. Note that, in practice, the number of constraints and thus the number of gradients is usually smaller than the dimension of the gradient, which means that the feasible space can be seen as the Cartesian product between its own intersection with span(M) and the orthogonal subspace of span(M). That being said, we can actually reduce our interest to the size of the solid angle in the M-dimensional space span(M), as in 6. minimizeMλM−1  Sspan(M) M−1 ∩ ⋂ gi∈M {g|⟨g,gi⟩≥ 0}   (6) where Sspan(M) M−1 denotes the unit sphere in span(M). Note that even if the sub-spaces span(M) are different from each other, they all have the same dimension as M, which is ﬁxed, hence comparing their λM-measure makes sense. However, this objective is hard to minimize since the formula of the solid angle is complex, as shown in [15] and [2]. Therefore, we propose, in the next section, a surrogate to this objective that is easier to deal with. 3.3 An Empirical Surrogate to Feasible Region Minimization Intuitively, to decrease the feasible set, one must increase the angles between each pair of gradients. Indeed, this is directly visible in 2D with Figure 3. Based on this observation, we propose the surrogate in Eq.9. minimizeM ∑ i,j∈M ⟨gi,gj⟩ ∥gi∥∥gj∥ (7) s.t.M⊂ [0 ..t −1]; |M|= M We empirically studied the relationship between the solid angle and the surrogate function in higher dimensional space using randomly sampled vectors as the gradient. Given a set of sampled vectors, the surrogate value is computed analytically and the solid angle is estimated using Monte Carlo approximation of Eq.6. The results are presented in Figure 2, which shows a monotonic relation between the solid angle and our surrogate. It is worth noting that minimization of Eq.9 is equivalent to maximization of the variance of the gradient direction as is shown in Eq.8. VarM [ g ∥g∥ ] = 1 M ∑ k∈M ⏐⏐⏐⏐ ⏐⏐⏐⏐ g ∥g∥ ⏐⏐⏐⏐ ⏐⏐⏐⏐ 2 − ⏐⏐⏐⏐⏐ ⏐⏐⏐⏐⏐ 1 M ∑ k∈M g ∥g∥ ⏐⏐⏐⏐⏐ ⏐⏐⏐⏐⏐ 2 (8) =1 − 1 M2 ∑ i,j∈M ⟨gi,gj⟩ ∥gi∥∥gj∥ 4This brings up a new interpretation of the surrogate, which is maximizing the diversity of samples in the replay buffer using the parameter gradient as the feature. Intuitively, keeping diverse samples in the replay buffer could be an efﬁcient way to use the memory budget. It is also possible to maximize the variance directly on the samples or on the hidden representations, we argue that the parameter gradient could be a better option given its root in Eq. 1. This is also veriﬁed with experiments. 3.4 Online Sample Selection 3.4.1 Online sample selection with Integer Quadratic Programming. We assume an inﬁnite input stream of data where at each time a new sample(s) is received. From this stream we keep a ﬁxed buffer of size M to be used as a representative of the previous samples. To reduce computation burden, we use a “recent” buffer in which we store the incoming examples and once is full we perform selection on the union of the replay buffer and the “recent” buffer and replace the samples in the replay buffer with the selection. To perform the selection of M samples, we solve Eq. 9 as an integer quadratic programming problem as shown in Appendix A.2. The exact procedures are described in algorithm 1. While this is reasonable in the cases of small buffer size, we observed a big overhead when utilizing larger buffers which is likely the case in practical scenario. The overhead, comes from both the need to get the gradient of each sample in the buffer and the recent buffer and from solving the quadratic problem that is polynomial w.r.t. the size of the buffer. Since this might limit the scalability of our approach, we suggest an alternative greedy method. 3.4.2 An in-exact greedy alternative. We propose an alternative greedy method based on heuristic, which could achieve the same goal of keeping diverse examples in the replay buffer, but is much cheaper than performing integer quadratic programming. The key idea is to maintain a score for each sample in the replay buffer. The score is computed by the maximal cosine similarity of the current sample with a ﬁxed number of other random samples in the buffer. When there are two samples similar to each other in the buffer, their scores are more likely to be larger than the others. In the beginning when the buffer is not full, we add incoming samples along with their score to the replay buffer. Once the buffer is full, we randomly select samples from the replay buffer as the candidate to be replaced. We use the normalized score as the probability of this selection. The score of the candidate is then compared to the score of the new sample to determine whether the replacement should happen or not. More formally, denote the score as Ci for sample iin the buffer. Sample iis selected as a candidate to be replaced with probability P(i) =Ci/∑ jCj. The replacement is a bernoulli event that happens with probability Ci/(c+ Ci) where Ci is the score of the candidate and cis the score of the new data. We can apply the same procedure for each example when a batch of new data is received. Algorithm 2 describes the main steps of our gradient based greedy sample selection procedure. It can be seen that the major cost of this selection procedure corresponds only to the estimation of the gradients of the selected candidates which is a big computational advantage over the other selection strategies. 3.5 Constraint vs Regularization Projecting the gradient of the new sample(s) exactly into the feasible region is computationally very expensive especially when using a large buffer. A usual work around for constrained optimization is to convert the constraints to a soft regularization loss. In our case, this is equivalent to performing rehearsal on the buffer. Note that [4] suggests to constrain only with one random gradient direction from the buffer as a cheap alternative that works equally well to constraining with the gradients of the previous tasks, it was later shown by the same authors [5] that rehearsal on the buffer has a competitive performance. In our method, we do rehearsal while in Appendix B.2 we evaluate both rehearsal and constrained optimization on a small subset of disjoint MNIST and show comparable results. 53.6 Summary of the Proposed Approach To recap, we start from the constrained optimization view of continual learning, which needs to be relaxed by constraint selection. Instead of random selection, we perform constraint selection by minimizing the solid angle formed by the constraints. We propose a surrogate for the solid angle objective, and show their relation numerically. We further propose a greedy alternative that is computationally more efﬁcient. Finally, we test the effectiveness of the proposed approach on continual learning benchmarks in the following section. Algorithm 1 IQP Sample Selection 1: Input: Mr, Mb 2: function SELECT SAMPLES (M, M) 3: ˆM← argmin ˆM ∑ i,j∈ˆM ⟨gi,gj ⟩ ∥gi∥∥gj ∥ 4: s.t. ˆM⊂M ; |ˆM|= M 5: return ˆM 6: end function 7: Initialize: Mr, Mb 8: Receive: (x,y) ⊿one or few consecutive examples 9: Update( x, y, Mb) 10: Mr ←Mr ∪{(x,y)} 11: if len(Mr) >Mr then 12: Mb ←Mb ∪Mr 13: Mr ←{} 14: if len(Mb) >Mb then 15: Mb ←SelectSamples(Mb,Mb) 16: end if 17: end if Algorithm 2 Greedy Sample Selection 1: Input: n, M 2: Initialize: M, C 3: Receive: (x,y) 4: Update( x, y, M) 5: X,Y ←RandomSubset(M, n) 6: g←∇ℓθ(x,y); G←∇θℓ(X,Y ) 7: c= maxi( ⟨g,Gi⟩ ∥g∥∥Gi∥) + 1⊿make the score positive 8: if len(M) >= M then 9: if c< 1 then ⊿cosine similarity < 0 10: i∼P(i) =Ci/∑ jCj 11: r∼uniform(0,1) 12: if r< Ci/(Ci + c) then 13: Mi ←(x,y); Ci ←c 14: end if 15: end if 16: else 17: M←M∪{ (x,y)}; C∪{c} 18: end if 4 Experiments This section serves to validate our approach and show its effectiveness at dealing with continual learning problems where task boundaries are not available. Benchmarks We consider 3 different benchmarks, detailed below. Disjoint MNIST: MNIST dataset divided into 5 tasks based on the labels with two labels in each task. We use 1k examples per task for training and report results on all test examples. Permuted MNIST: We perform 10 unique permutations on the pixels of the MNIST images. The permutations result in 10 different tasks with same distributions of labels but different distributions of the input images. Following [10], each of the task in permuted MNIST contains only 1k training examples. The test set for this dataset is the union of the MNIST test set with all different permutations. Disjoint CIFAR-10: Similar to disjoint MNIST, the dataset is split into 5 tasks according to the labels, with two labels in each task. As this is harder than mnist, we use a total of 10k training examples with 2k examples per task. In all experiments, we use a ﬁxed batch size of 10 samples and perform few iterations over a batch (1-5), note that this is different from multiple epochs over the whole data. In disjoint MNIST, we report results using different buffer sizes in table 1. For permuted MNIST results are reported using buffer size 300 while for disjoint CIFAR-10 we couldn’t get sensible performance for the studied methods with buffer size smaller than 1k. All results are averaged over 3 different random seeds. Models Following [10], for disjoint and permuted MNIST we use a two-layer neural network with 100 neurons each while for CIFAR-10 we use ResNet18. Note that we employ a shared head in the incremental classiﬁcation experiments, which is much more challenging than the multi-head used in [10]. In all experiments, we use SGD optimizer with a learning rate of 0.05 for disjoint MNIST and permuted MNIST and 0.01 for disjoint Cifar-10. 6Table 1: Average test accuracy of sample selection methods on disjoint MNIST with different buffer sizes. Method Buffer Size 300 400 500 Rand 37.5 ±1.3 45.9 ±4.8 57.9 ±4.1 GSS-IQP(ours) 75.9 ±2.5 82.1 ±0.6 84.1 ±2.4 GSS-Clust 75.7 ±2.2 81.4 ±4.4 83.9±1.6 FSS-Clust 75.8 ±1.7 80.6 ±2.7 83.4 ±2.6 GSS-Greedy(ours) 82.6±2.9 84.6 ±0.9 84.8 ±1.8 Table 2: Comparison of different selection strategies on permuted MNIST benchmark. Method T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 Avg Rand 67.01±2.7 62.18±4.6 69.63±3.2 62.05±2.4 68.41±1.0 72.81±3.0 77.67±2.3 77.28±1.8 83.92±0.6 84.52±0.3 72.54±0.4 GSS-IQP 74.1±2.2 69.73±0.6 70.77±4.9 70.5±2.5 73.34±4.8 78.6±2.8 81.8±0.6 81.8±0.7 86.4±0.8 85.45±0.4 77.3±0.5(ours)GSS-Clust75.3±1.3 75.22±1.9 76.66±0.9 75.09±1.6 78.76±0.9 81.14±1.1 81.32±2.0 83.87±0.7 84.52±1.2 85.52±0.6 79.74±0.2 FSS-Clust82.2±0.9 71.34±2.3 76.9±1.3 70.5±4.1 70.56±1.4 74.9±1.5 77.68±3.3 79.56±2.6 82.7±1.5 85.3±0.6 77.8±0.3 GSS-Greedy83.35±1.1 70.84±1.3 72.48±1.7 70.5±3.4 72.8±1.7 73.75±3.8 79.86±1.8 80.45±2.9 82.56±1.1 84.8±1.6 77.3±0.5(ours) Table 3: Comparison of different selection strategies on disjoint CIFAR10 benchmark. Method T1 T2 T3 T4 T5 Avg Rand 0 ±0.0 0.49±0.4 5.68±4.4 52.18±0.8 84.96±4.4 28.6±1.2 GSS-Clust 0.35±0.5 15.27±8.3 7.96±6.3 9.97±2.1 77.83±0.7 22.5±0.4 FSS-Clust 0.2±0.2 0.8±0.5 5.4±0.7 38.12±5.2 87.90±3.1 26.7±1.5 GSS-Greedy(ours)42.36±12.1 14.61±2.7 13.60±4.5 19.30±2.7 77.83±4. 2 33.56±1.7 4.1 Comparison with Sample Selection Methods We want to study the buffer population in the context of the online continual learning setting when no task information are present and no assumption on the data generating distribution is made. Since most existing works assume knowledge of task boundaries, we decide to deploy 3 baselines along with our two proposed methods2 . Given a ﬁxed buffer size M we compare the following: Random (Rand): Whenever a new batch is received, it joins the buffer. When the buffer is full, we randomly select samples to keep of size M from the new batch and samples already in buffer. Online Clustering: A possible way to keep diverse samples in the buffer is online clustering with the goal of selecting a set of M centroids. This can be done either in the feature space (FSS-Clust), where we use as a metric the distance between the samples features, here the last layer before classiﬁcation, or in the gradient space (GSS-Clust), where as a metric we consider the Euclidean distance between the normalized gradients. We adapted the doubling algorithm for incremental clustering described in [3]. IQP Gradients (GSS-IQP): Our surrogate to select samples that minimize the feasible region de- scribed in Eq.9 and solved as an integer quadratic programming problem. Due to the cost of computation we report our GSS-IQP on permuted MNIST and disjoint MNIST only. Gradient greedy selection (GSS-Greedy): Our greedy selection variant detailed in Algo. 2. Note that differently from previous selection strategies, it doesn’t require re-processing all the recent and buffer samples to perform the selection which is a huge gain in the online streaming setting. 4.2 Performance of Sample Selection Methods Tables 1, 2, 3 report the test accuracy on each task at the end of the data stream of disjoint MNIST, permuted MNIST, disjoint CIFAR-10 sequentially. First of all, the accuracies reported in the tables might appear lower than state of the art numbers. This is due to the strict online setting, we use shared head and more importantly we use no information of the task boundary. In contrast, all previous works assume availability of task boundary either at training or both training and testing. The performance of the random baseline Rand clearly indicates the difﬁculty of this setting. It can 2The code is available at https://github.com/rahafaljundi/Gradient-based-Sample-Selection 7Table 4: Comparison with reservoir sampling on different imbalanced data sequences from disjoint MNIST. Method Seq1 Seq2 Seq3 Seq4 Seq5 Avg Reservoir 63.7±0.8 69.4 ±0.7 66.8±4.8 69.1±2.4 76.6±1.6 69.12±4.3 GSS-IQP(ours) 75.9 ±3.2 76.2±4.1 79.06±0.7 76.6±2.0 74.7±1.8 76.49±1.4 GSS-Greedy(ours) 71.2±3.6 78.5±2.7 81.5±2.3 79.5±0.6 79.1±0.7 77.96±3.5 be seen that both of our selection methods stably outperform the different buffer sizes on different benchmarks. Notably, the gradient based clustering GSS-Clust performs comparably and even favorably on permuted MNIST to the feature clustering FSS-Clust suggesting the effectiveness of a gradient based metric in the continual learning setting. Surprisingly, GSS-Greedy performs on par and even better than the other selection strategies especially on disjoint CIFAR-10 indicating not only a cheap but a strong sample selection strategy. It is worth noting that Rand achieves high accuracy on T4 and T5 of the Cifar-10 sequence. In fact, this is an artifact of the random selection strategy where at the end of the sequence, the buffer sampled by Rand is composed of very few samples from the ﬁrst tasks, 24 samples from T1, T2 and T3, but more from the recent T4 (127) & T5 (849). As such, it forgets less the more recent task at the cost of older ones. 4.3 Comparison with Reservoir Sampling Reservoir sampling [17] is a simple replacement strategy to ﬁll the memory buffer when the task boundaries are unknown based on the underlying assumption that the overall data stream is i.i.d distributed. It would work well when each of the tasks has a similar number of examples. However, it could lose the information on the under-represented tasks if some of the tasks have signiﬁcantly fewer examples than the others. In this paper we study and propose algorithms to sample from an imbalanced stream of data. Our strategy has no assumption on the data stream distribution, hence it could be less affected by imbalanced data, which is often encountered in practice. We test this scenario on disjoint MNIST. We modify the data stream to settings where one of the tasks has an order of magnitude more samples than the rest, generating 5 different sequences where the the ﬁrst sequence has 2000 samples of the ﬁrst task and 200 from each other task, the second sequence has 2000 samples from the second task and 200 from others, and same strategy applies to the rest of the sequences. Table 4 reports the average accuracy at the end of each sequence over 3 runs with 300 samples as buffer size. It can be clearly seen that our selection strategies outperform reservoir sampling especially when ﬁrst tasks are under-represented with many learning steps afterwords leading to forgetting. Our improvement reaches 15%, while we don’t show the individual tasks accuracy here due to space limit, it worth noting that reservoir sampling suffers severely on under-represented tasks resulting in very low accuracy. Having shown the robustness of our selection strategies both GSS-IQP and GSS-Greedy, we move now to compare with state of the art replay-based methods that allocate separate buffer per task and only play samples from previous tasks during the learning of others. 4.4 Comparison with State-of-the-art Task Aware Methods Our method ignores any tasks information which places us at a disadvantage because the methods that we compare to utilize the task boundaries as an extra information. In spite of this disadvantage, we show that our method performs similarly on these datasets. Compared Methods Single: is a model trained online on the stream of data without any mechanism to prevent forgetting. i.i.d.online: is the Single baseline trained on an i.i.d. stream of the data. i.i.d.offline: is a model trained ofﬂine for multiple epochs with i.i.d. sampled batches. As such i.i.d.online trains on the i.i.d. stream for just one pass, while i.i.d.offline takes multiple epochs. GEM [10]: stores a ﬁxed amount of random examples per task and uses them to provide constraints when learning new examples. 8Figure 4: Comparison with state-of-the-art task aware replay methods GEM. Figures show test accuracy. (a) Disjoint MNIST 0 1000 2000 3000 4000 5000 Number of learned examples 0.0 0.2 0.4 0.6 0.8 1.0Accuracy T1 T2 T3 T4 T5 (b) Permuted MNIST 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Number of learned examples 0.0 0.2 0.4 0.6 0.8 1.0Accuracy T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 (c) Disjoint CIFAR-10 0 2000 4000 6000 8000 10000 Number of learned examples 0.0 0.1 0.2 0.3 0.4 0.5 0.6Accuracy T1 T2 T3 T4  T5 GSS-Greedy GEM Single i.i.d.online i.i.d.offline iCaRL GSS-IQP iCaRL [14]: follows an incremental classiﬁcation setting. It also stores a ﬁxed number of examples per class but uses them to rehearse the network when learning new information. For ours, we report both GSS-IQP and GSS-Greedy on permuted and disjoint MNIST and only GSS-Greedy on disjoint CIFAR-10 due to the computational burden. Since we perform multiple iterations over a given batch still in the online setting, we treat the number of iterations as a hyper parameter for GEM and iCaRL. We found that GEM performance constantly deteriorates with multiple iterations while iCaRL improves. Figure 4a shows the test accuracy on disjoint MNIST which is evaluated during the training procedure at an interval of 100 training examples with 300 buffer size. For the i.i.d. baselines, we only show the achieved performance at the end of the training. For iCaRL, we only show the accuracy at the end of each task because iCaRL uses the selected exemplars for prediction that only happens at the end of each task. We observe that both variants of our method have a very similar learning curve to GEM except the few last iterations where GSS-IQP performance slightly drops. Figure 4b compares our methods with the baselines and GEM on the permuted MNIST dataset. Note that iCaRL is not included, as it is designed only for incremental classiﬁcation. From the performance of the Single baseline it is apparent that permuted MNIST has less interference between the different tasks. Ours perform better than GEM and get close to i.i.d.online performance. Figure 4c shows the accuracy on disjoint CIFAR-10 evaluated during the training procedure at an interval of 100 training examples. GSS-Greedy shows better performance than GEM and iCaRL, and it even achieves a better average test performance at the end of the sequence than i.i.d.online. We found that GEM suffers more forgetting on previous tasks while iCaRL shows lower performance on the last task. Note that our setting is much harder than ofﬂine tasks training used in iCaRL or the multi-heard setting used in . 5 Conclusion In this paper, we prove that in the online continual learning setting we can smartly select a ﬁnite number of data to be representative of all previously seen data without knowing task boundaries. We aim for samples diversity in the gradient space and introduce a greedy selection approach that is efﬁcient and constantly outperforming other selection strategies. We still perform as well as algorithms that use the knowledge of task boundaries to select the representative examples. Moreover, our selection strategy gives us advantage under the settings where the task boundaries are blurry or data are imbalanced. Acknowledgements Rahaf Aljundi is funded by FWO. References [1] Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selﬂess sequential learning. In ICLR 2019. [2] Matthias Beck, Sinai Robins, and Steven V Sam. Positivity theorems for solid-angle polynomials. arXiv preprint arXiv:0906.4031, 2009. 9[3] Moses Charikar, Chandra Chekuri, Tomás Feder, and Rajeev Motwani. Incremental clustering and dynamic information retrieval. SIAM Journal on Computing, 33(6):1417–1440, 2004. [4] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018. [5] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. [6] Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733, 2018. [7] Robert M French. Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference. network, 1111:00001, 1994. [8] David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In Thirty- Second AAAI Conference on Artiﬁcial Intelligence, 2018. [9] Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. [10] David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pages 6467–6476, 2017. [11] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7765–7773, 2018. [12] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [13] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. [14] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. [15] Jason M Ribando. Measuring solid angles beyond dimension three. Discrete & Computational Geometry, 36(3):479–487, 2006. [16] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123–146, 1995. [17] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 11(1):37–57, 1985. 10A Clariﬁcations of points in the main paper A.1 Estimation of the solid angle We faced the problem of no tractable formula for the solid angle, so we estimated it with sampling method. The estimated angle is an average of Bernoulli random variables with variance then bounded by 1 4N where N is the number of these Bernoulli variables. By taking N = 109, we reach an asymptotic conﬁdence interval of length around 10−4. A.2 IQP formulation to our surrogate Our surrogate formulation for selecting a ﬁxed set of samples that minimize the solid angle is: minimizeM ∑ i,j∈M ⟨gi,gj⟩ ∥gi∥∥gj∥ (9) s.t.M⊂ [0 ..t −1]; |M|= M We solve the surrogate minimization as an integer quadratic programming problem. We ﬁrst normalize the gradients: G= ⟨gi,gj ⟩ ∥gi∥∥gj ∥and ﬁnd a selection vector X that minimizes the following: minimize X 1 2XTGX s.t. 1T.X = M xi ∈{0,1} ∀xi ∈X where 1 is a vector of ones with the same length as X. Selected samples will correspond to values of 1 in X. B Additional Experiments B.1 Performance under blurry task boundary An interesting setting is the scenario where there are no clear task boundaries in the data stream, as we mentioned in the introduction, such situation can happen in practice. We start by blurring the task boundaries in disjoint Cifar10 benchmark. For each task in a dataset, we keep the majority of the examples while we randomly swap a small percentage of the examples with other tasks. A larger swap percentage corresponds to more blurry boundaries. A similar setting has been used in [1]. We keep 90% of the data for each task, and introduce 10% of data from the other tasks. We make comparisons to the other studied selection methods. Since tasks are not disjoint, forgetting is not as sever as complete disjoint tasks. Hence, we use a buffer of 500 samples and train on 1k samples per task which allows us to run our GSS-IQP more smoothly. Table 5 reports the accuracy of each task at the end of the sequence. Our both methods perform better than other selection strategies. Method T1 T2 T3 T4 T5 Avg Rand 0 3.45 9.85 54.67 78.76 29.0 GSS-IQP(ours) 9.38 11.33 17.05 30.84 79.53 29.6 GSS-Clust 2.43 16.75 9.09 20.71 77.98 25.0 FSS-Clust 2.95 05.09 6.06 38.16 78.14 26.0 GSS-Greedy(ours) 34.2 11.14 14.96 20.25 67.5 29.6 Table 5: Comparison of different selection strategies on disjoint Cifar10 with blurry task boundary. B.2 Constrained Optimization Compared to Rehearsal By the end of the section 3.5, we have elaborated on the computational complexity of the constrained optimization with large buffers which renders infeasible. That’s mainly because at each learning step, 11Method T1 T2 T3 T4 T5 Avg GSS-IQP(Constrained) 90.0 70.0 45.13 88.77 86.08 76.26 GSS-IQP(Rehearsal) 81.5 69.47 46.96 69.80 88.0 71.3 Table 6: Comparison between our GSS-IQP constrained and GSS-IQP rehearsal, buffer size 100. Method T1 T2 T3 T4 T5 Avg GSS-IQP(Constrained) 95.0 83.0 68.7 87.6 82.4 83.4 GSS-IQP(Rehearsal) 94.6 83.89 50.6 77.0 88.67 78.9 Table 7: Comparison between our GSS-IQP constrained and GSS-IQP rehearsal, buffer size 200. Figure 5: Greedy Sample Selection Ablation Study. Figures show test accuracy. (a) Average test accuracy at the end of disjoint MNIST for different values of n. n=1 n=5 n=10 n=20 n=30 n=40 n=50 Comparing to n gradient vectors 60 65 70 75 80 85 90Avg. Accuracy GSS-Greedy (b) Average test accuracy at the end of disjoint MNIST for different batch sizes. B=5 B=10 B=20 B=50 B=100 Batch size effect 60 65 70 75 80 85 90Avg. accracy GSS-Greedy gradients of each sample in the buffer needs to be estimated and then the new sample gradient need to be projected onto the feasible region determined by all the samples gradients. As an alternative, we per- form rehearsal on the buffer. Here, we want to compare the performance of the two update strategies, the constrained optimization GSS-IQP(Constrained) and the rehearsal GSS-IQP(Rehearsal). We consider disjoint MNIST benchmark and use 200 training samples per task. Table 6 reports the test accuracy on each task achieved by each strategy at the end of the training when using a buffer of size 100 while table 7 reports the accuracy with 200 buffer size. GSS-IQP(Constrained) improves over GSS-IQP(Rehearsal) with a margin of 3 −5% but requires a long time to train as it scales polynomialy with the number of samples in the buffer apart from the need to compute the gradient of each buffer sample at each training step. GSS-IQP(Rehearsal) with larger buffer is less computational and yields similar results, comparing GSS-IQP(Rehearsal) with a buffer of size 200 (78.9%) and GSS-IQP(Constrained) with a buffer of size 100 (76.26%). B.3 Effect of nin Greedy Sample Selection In our Greedy Sample Selection (GSS-Greedy), for each newly received sample we compute a score indicating its similarity to samples already in the buffer (lines (5-7) in Algorithm 2). This is done by computing the cosine similarity of the new sample(s) gradient to ngradient vectors of samples drawn from the buffer. We study the effect ofnon the performance of GSS-Greedy given disjoint MNIST benchmark. Figure 5a shows the average test accuracy at the end of disjoint MNIST sequence for different values of n. Very small value of ntends to give a very noisy estimate of the new sample(s) similarity score and hence new samples are added more often to the buffer resulting in a more forgetting of previous tasks and a less average test accuracy, Avg.Acc. = 67.3 for n = 1. Increasing ntends to give a better estimation and as a result better average test accuarcies. However, large values of nlead to a high rejection rates as it only adds new samples that are very different from all samples in buffer. As such, ﬁrst tasks will have more representative samples in the buffer than later tasks and average test accuracy slightly decreases. In all the experiments, we used n= 10as a good trade-off between good score approximation and computational cost. 12B.4 Batch size effect. In this paper, we consider a never-ending stream of data that we aim at learning efﬁciently. In our experiments, we wanted to be as close as possible to the full online setting. Hence, we used a batch size of 10, as in GEM [10], which seems a good approximation. To test the effect of the batch size on the our continual learning performance, we run GSS-Greedy on disjoint MNIST benchmark with buffer size M = 300considering different batch sizes. Figure 5b shows the average test accuracy at the end of the sequence for different batch sizes. In the online learning, only one pass over the training data of a given “task” is performed. Large batch sizes lead less parameter updates. As a result, the learning methods fails to achieve a good performance on the learned samples compared to the conﬁguration with smaller batch size. Additionally, very small batch size means noisier parameter update estimation. 13",
      "meta_data": {
        "arxiv_id": "1903.08671v5",
        "authors": [
          "Rahaf Aljundi",
          "Min Lin",
          "Baptiste Goujaud",
          "Yoshua Bengio"
        ],
        "published_date": "2019-03-20T18:01:55Z",
        "pdf_url": "https://arxiv.org/pdf/1903.08671v5.pdf"
      }
    },
    {
      "title": "Efﬁcient lifelong learning with a-gem",
      "abstract": "In lifelong learning, the learner is presented with a sequence of tasks,\nincrementally building a data-driven prior which may be leveraged to speed up\nlearning of a new task. In this work, we investigate the efficiency of current\nlifelong approaches, in terms of sample complexity, computational and memory\ncost. Towards this end, we first introduce a new and a more realistic\nevaluation protocol, whereby learners observe each example only once and\nhyper-parameter selection is done on a small and disjoint set of tasks, which\nis not used for the actual learning experience and evaluation. Second, we\nintroduce a new metric measuring how quickly a learner acquires a new skill.\nThird, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017),\ndubbed Averaged GEM (A-GEM), which enjoys the same or even better performance\nas GEM, while being almost as computationally and memory efficient as EWC\n(Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we\nshow that all algorithms including A-GEM can learn even more quickly if they\nare provided with task descriptors specifying the classification tasks under\nconsideration. Our experiments on several standard lifelong learning benchmarks\ndemonstrate that A-GEM has the best trade-off between accuracy and efficiency.",
      "full_text": "Published as a conference paper at ICLR 2019 EFFICIENT LIFELONG LEARNING WITH A-GEM Arslan Chaudhry1, Marc’Aurelio Ranzato2, Marcus Rohrbach2, Mohamed Elhoseiny2 1University of Oxford, 2Facebook AI Research arslan.chaudhry@eng.ox.ac.uk, {ranzato,mrf,elhoseiny}@fb.com ABSTRACT In lifelong learning, the learner is presented with a sequence of tasks, incremen- tally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate theefﬁciency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we ﬁrst introduce a new and a more realistic evaluation protocol, whereby learn- ers observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experi- ence and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM ), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efﬁcient as EWC (Kirkpatrick et al., 2016) and other regularization- based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the clas- siﬁcation tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between ac- curacy and efﬁciency.1 1 I NTRODUCTION Intelligent systems, whether they are natural or artiﬁcial, must be able to quickly adapt to changes in the environment and to quickly learn new skills by leveraging past experiences. While current learning algorithms can achieve excellent performance on a variety of tasks, they strongly rely on copious amounts of supervision in the form of labeled data. The lifelong learning (LLL) setting attempts at addressing this shortcoming, bringing machine learn- ing closer to a more realistic human learning by acquiring new skills quickly with a small amount of training data, given the experience accumulated in the past. In this setting, the learner is presented with a stream of tasks whose relatedness is not known a priori. The learner has then the potential to learn more quickly a new task, if it can remember how to combine and re-use knowledge acquired while learning related tasks of the past. Of course, for this learning setting to be useful, the model needs to be constrained in terms of amount of compute and memory required. Usually this means that the learner should not be allowed to merely store all examples seen in the past (in which case this reduces the lifelong learning problem to a multitask problem) nor should the learner be engaged in computations that would not be feasible in real-time, as the goal is to quickly learn from a stream of data. Unfortunately, the established training and evaluation protocol as well as current algorithms for lifelong learning do not satisfy all the above desiderata, namely learning from a stream of data using limited number of samples, limited memory and limited compute. In the most popular training paradigm, the learner does several passes over the data (Kirkpatrick et al., 2016; Aljundi et al., 2018; Rusu et al., 2016; Schwarz et al., 2018), while ideally the model should need only a handful of samples and these should be provided one-by-one in a single pass (Lopez-Paz & Ranzato, 2017). Moreover, when the learner has several hyper-parameters to tune, the current practice is to go over the sequence of tasks several times, each time with a different hyper-parameter value, again ignoring the requirement of learning from a stream of data and, strictly speaking, violating the assumption of 1The code is available at https://github.com/facebookresearch/agem. 1 arXiv:1812.00420v2  [cs.LG]  9 Jan 2019Published as a conference paper at ICLR 2019 the LLL scenario. While some algorithms may work well in a single-pass setting, they unfortunately require a lot of computation (Lopez-Paz & Ranzato, 2017) or their memory scales with the number of tasks (Rusu et al., 2016), which greatly impedes their actual deployment in practical applications. In this work, we propose an evaluation methodology and an algorithm that better match our desider- ata, namely learning efﬁciently – in terms of training samples, time and memory – from a stream of tasks. First, we propose a new learning paradigm, whereby the learner performs cross validation on a set of tasks which is disjoint from the set of tasks actually used for evaluation (Sec. 2). In this setting, the learner will have to learn and will be tested on an entirely new sequence of tasks and it will perform just a single pass over this data stream. Second, we build upon GEM (Lopez-Paz & Ranzato, 2017), an algorithm which leverages a small episodic memory to perform well in a single pass setting, and propose a small change to the loss function which makes GEM orders of magnitude faster at training time while maintaining similar performance; we dub this variant of GEM, A-GEM (Sec. 4). Third, we explore the use of compositional task descriptors in order to improve the few- shot learning performance within LLL showing that with this additional information the learner can pick up new skills more quickly (Sec. 5). Fourth, we introduce a new metric to measure the speed of learning, which is useful to quantify the ability of a learning algorithm to learn a new task (Sec. 3). And ﬁnally, using our new learning paradigm and metric, we demonstrate A-GEM on a variety of benchmarks and against several representative baselines (Sec. 6). Our experiments show that A- GEM has a better trade-off between average accuracy and computational/memory cost. Moreover, all algorithms improve their ability to quickly learn a new task when provided with compositional task descriptors, and they do so better and better as they progress through the learning experience. 2 L EARNING PROTOCOL Currently, most works on lifelong learning (Kirkpatrick et al., 2016; Rusu et al., 2016; Shin et al., 2017; Nguyen et al., 2018) adopt a learning protocol which is directly borrowed from supervised learning. There are T tasks, and each task consists of a training, validation and test sets. During training the learner does as many passes over the data of each task as desired. Moreover, hyper- parameters are tuned on the validation sets by sweeping over the whole sequence of tasks as many times as required by the cross-validation grid search. Finally, metrics of interest are reported on the test set of each task using the model selected by the previous cross-validation procedure. Since the current protocol violates our stricter deﬁnition of LLL for which the learner can only make a single pass over the data, as we want to emphasize the importance of learning quickly from data, we now introduce a new learning protocol. We consider two streams of tasks, described by the following ordered sequences of datasetsDCV = {D1,··· ,DTCV }and DEV = {DTCV +1,··· ,DT}, where Dk = {(xk i,tk i,yk i)nk i=1}is the dataset of the k-th task, TCV <T (in all our experiments TCV = 3while T = 20), and we assume that all datasets are drawn from the same distribution over tasks. To avoid cluttering of the notation, we let the context specify whether Dk refers to the training or test set of the k-th dataset. DCV is the stream of datasets which will be used during cross-validation; DCV allows the learner to replay all samples multiple times for the purposes of model hyper-parameter selection. Instead, DEV is the actual dataset used for ﬁnal training and evaluation on the test set; the learner will observe training examples from DEV once and only once, and all metrics will be reported on the test sets of DEV. Since the regularization-based approaches for lifelong learning (Kirkpatrick et al., 2016; Zenke et al., 2017) are rather sensitive to the choice of the regularization hyper-parameter, we introduced the set DCV , as it seems reasonable in practical applications to have similar tasks that can be used for tuning the system. However, the actual training and testing are then performed on DEV using a single pass over the data. See Algorithm 1 for a summary of the training and evaluation protocol. Each example in any of these dataset consists of a triplet deﬁned by an input ( xk ∈X ), task de- scriptor (tk ∈T , see Sec. 5 for examples) and a target vector ( yk ∈yk), where yk is the set of labels speciﬁc to task k and yk ⊂Y . While observing the data, the goal is to learn a predictor fθ : X×T →Y , parameterized by θ ∈RP (a neural network in our case), that can map any test pair (x,t) to a target y. 2Published as a conference paper at ICLR 2019 Algorithm 1Learning and Evaluation Protocols 1: for hin hyper-parameter list do ⊿Cross-validation loop, executing multiple passes over DCV 2: for k= 1to TCV do ⊿Learn over data stream DCV using h 3: for i= 1to nk do ⊿Single pass over Dk 4: Update fθ using (xk i,tk i,yk i) and hyper-parameter h 5: Update metrics on test set of DCV 6: end for 7: end for 8: end for 9: Select best hyper-parameter setting, h∗, based on average accuracy of test set of DCV , see Eq. 1. 10: Reset fθ. 11: Reset all metrics. 12: for k= TCV + 1to T do ⊿Actual learning over datastream DEV 13: for i= 1to nk do ⊿Single pass over Dk 14: Update fθ using (xk i,tk i,yk i) and hyper-parameter h∗ 15: Update metrics on test set of DEV 16: end for 17: end for 18: Report metrics on test set of DEV. 3 M ETRICS Below we describe the metrics used to evaluate the LLL methods studied in this work. In addition to Average Accuracy ( A) and Forgetting Measure ( F) (Chaudhry et al., 2018), we deﬁne a new measure, the Learning Curve Area (LCA), that captures how quickly a model learns. The training dataset of each task, Dk, consists of a total Bk mini-batches. After each presentation of a mini-batch of task k, we evaluate the performance of the learner on all the tasks using the corresponding test sets. Let ak,i,j ∈[0,1] be the accuracy evaluated on the test set of task j, after the model has been trained with the i-th mini-batch of task k. Assuming the ﬁrst learning task in the continuum is indexed by 1 (it will be TCV + 1for DEV) and the last one by T (it will be TCV for DCV ), we deﬁne the following metrics: Average Accuracy (A ∈[0,1]) Average accuracy after the model has been trained continually with all the mini-batches up till task kis deﬁned as: Ak = 1 k k∑ j=1 ak,Bk,j (1) In particular, AT is the average accuracy on all the tasks after the last task has been learned; this is the most commonly used metric used in LLL. Forgetting Measure (F ∈[−1,1]) (Chaudhry et al., 2018) Average forgetting after the model has been trained continually with all the mini-batches up till task kis deﬁned as: Fk = 1 k−1 k−1∑ j=1 fk j (2) where fk j is the forgetting on task ‘j’ after the model is trained with all the mini-batches up till task kand computed as: fk j = max l∈{1,···,k−1} al,Bl,j −ak,Bk,j (3) Measuring forgetting after all tasks have been learned is important for a two-fold reason. It quantiﬁes the accuracy drop on past tasks, and it gives an indirect notion of how quickly a model may learn a new task, since a forgetful model will have little knowledge left to transfer, particularly so if the new task relates more closely to one of the very ﬁrst tasks encountered during the learning experience. 3Published as a conference paper at ICLR 2019 Learning Curve Area (LCA∈[0,1]) Let us ﬁrst deﬁne an average b-shot performance (where b is the mini-batch number) after the model has been trained for all the T tasks as: Zb = 1 T T∑ k=1 ak,b,k (4) LCA at βis the area of the convergence curve Zb as a function of b∈[0,β]: LCAβ = 1 β+ 1 ∫ β 0 Zbdb= 1 β+ 1 β∑ b=0 Zb (5) LCA has an intuitive interpretation. LCA 0 is the average 0-shot performance, the same as forward transfer in Lopez-Paz & Ranzato (2017). LCA β is the area under the Zb curve, which is high if the 0-shot performance is good and if the learner learns quickly. In particular, there could be two models with the same Zβ or AT, but very different LCA β because one learns much faster than the other while they both eventually obtain the same ﬁnal accuracy. This metric aims at discriminating between these two cases, and it makes sense for relatively small values of βsince we are interested in models that learn from few examples. 4 A VERAGED GRADIENT EPISODIC MEMORY (A-GEM ) So far we discussed a better training and evaluation protocol for LLL and a new metric to measure the speed of learning. Next, we review GEM (Lopez-Paz & Ranzato, 2017), which is an algorithm that has been shown to work well in the single epoch setting. Unfortunately, GEM is very intensive in terms of computational and memory cost, which motivates our efﬁcient variant, dubbed A-GEM . In Sec. 5, we will describe how compositional task descriptors can be leveraged to further speed up learning in the few shot regime. GEM avoids catastrophic forgetting by storing an episodic memory Mk for each task k. While minimizing the loss on the current task t, GEM treats the losses on the episodic memories of tasks k < t, given by ℓ(fθ,Mk) = 1 |Mk| ∑ (xi,k,yi)∈Mk ℓ(fθ(xi,k),yi), as inequality constraints, avoiding their increase but allowing their decrease. This effectively permits GEM to do positive backward transfer which other LLL methods do not support. Formally, at task t, GEM solves for the following objective: minimizeθ ℓ(fθ,Dt) s.t. ℓ(fθ,Mk) ≤ℓ(ft−1 θ ,Mk) ∀k<t (6) Where ft−1 θ is the network trained till task t−1. To inspect the increase in loss, GEM computes the angle between the loss gradient vectors of previous tasksgk, and the proposed gradient update on the current task g. Whenever the angle is greater than 90° with any of the gk’s, it projects the proposed gradient to the closest in L2 norm gradient ˜gthat keeps the angle within the bounds. Formally, the optimization problem GEM solves is given by: minimize˜g 1 2||g−˜g||2 2 s.t. ⟨˜g,gk⟩≥ 0 ∀k<t (7) Eq.7 is a quadratic program (QP) in P-variables (the number of parameters in the network), which for neural networks could be in millions. In order to solve this efﬁciently, GEM works in the dual space which results in a much smaller QP with only t−1 variables: minimizev 1 2v⊤GG⊤v+ g⊤G⊤v s.t. v≥0 (8) where G = −(g1,··· ,gt−1) ∈R(t−1)×P is computed at each gradient step of training. Once the solution v∗to Eq. 8 is found, the projected gradient update can be computed as ˜g= G⊤v∗+ g. While GEM has proven very effective in a single epoch setting (Lopez-Paz & Ranzato, 2017), the performance gains come at a big computational burden at training time. At each training step, GEM computes the matrix Gusing all samples from the episodic memory, and it also needs to solve the QP of Eq. 8. Unfortunately, this inner loop optimization becomes prohibitive when the size of M and the number of tasks is large, see Tab. 7 in Appendix for an empirical analysis. To alleviate 4Published as a conference paper at ICLR 2019 the computational burden of GEM , next we propose a much more efﬁcient version of GEM , called Averaged GEM (A-GEM ). Whereas GEM ensures that at every training step the loss of each individual previous tasks, approx- imated by the samples in episodic memory, does not increase, A-GEM tries to ensure that at every training step the average episodic memory loss over the previous tasks does not increase. Formally, while learning task t, the objective of A-GEM is: minimizeθ ℓ(fθ,Dt) s.t. ℓ(fθ,M) ≤ℓ(ft−1 θ ,M) where M= ∪k<tMk (9) The corresponding optimization problem reduces to: minimize˜g 1 2||g−˜g||2 2 s.t. ˜g⊤gref ≥0 (10) where gref is a gradient computed using a batch randomly sampled from the episodic memory, (xref,yref) ∼M, of all the past tasks. In other words, A-GEM replaces the t−1 constraints of GEM with a single constraint, where gref is the average of the gradients from the previous tasks computed from a random subset of the episodic memory. The constrained optimization problem of Eq. 10 can now be solved very quickly; when the gradient gviolates the constraint, it is projected via: ˜g= g− g⊤gref g⊤ refgref gref (11) The formal proof of the update rule of A-GEM (Eq. 11) is given in Appendix C. This makes A-GEM not only memory efﬁcient, as it does not need to store the matrix G, but also orders of magnitude faster than GEM because 1) it is not required to compute the matrix G but just the gradient of a random subset of memory examples, 2) it does not need to solve any QP but just an inner product, and 3) it will incur in less violations particularly when the number of tasks is large (see Tab. 7 and Fig. 6 in Appendix for empirical evidence). All together these factors make A-GEM faster while not hampering its good performance in the single pass setting. Intuitively, the difference betweenGEM and A-GEM loss functions is that GEM has better guarantess in terms of worst-case forgetting of each individual task since (at least on the memory examples) it prohibits an increase of any task-speciﬁc loss, while A-GEM has better guaratees in terms of average accuracy since GEM may prevent a gradient step because of a task constraint violation although the overall average loss may actually decrease, see Appendix Sec. D.1 and D.2 for further analysis and empirical evidence. The pseudo-code of A-GEM is given in Appendix Alg. 2. 5 J OINT EMBEDDING MODEL USING COMPOSITIONAL TASK DESCRIPTORS In this section, we discuss how we can improve forward transfer for all the LLL methods includ- ing A-GEM . In order to speed up learning of a new task, we consider the use of compositional task descriptors where components are shared across tasks and thus allow transfer. Examples of compositional task descriptors are, for instance, a natural language description of the task under consideration or a matrix specifying the attribute values of the objects to be recognized in the task. In our experiments, we use the latter since it is provided with popular benchmark datasets (Wah et al., 2011; Lampert et al., 2009). For instance, if the model has already learned and remembers about two independent properties (e.g., color of feathers and shape of beak), it can quickly recog- nize a new class provided a descriptor specifying the values of its attributes (yellow feathers and red beak), although this is an entirely unseen combination. Borrowing ideas from literature in few-shot learning (Lampert et al., 2014; Zhang et al., 2018; Elhoseiny et al., 2017; Xian et al., 2018), we learn a joint embedding space between image features and the attribute embeddings. Formally, let xk ∈X be the input (e.g., an image), tk be the task descriptor in the form of a matrix of size Ck ×A, where Ck is the number of classes in the k- th task and A is the total number of attributes for each class in the dataset. The joint embedding model consists of a feature extraction module, φθ : xk →φθ(xk), where φθ(xk) ∈RD, and a task embedding module, ψω : tk →ψω(tk), where ψω(tk) ∈RCk×D. In this work,φθ(.) is implemented as a standard multi-layer feed-forward network (see Sec. 6 for the exact parameterization), whereas 5Published as a conference paper at ICLR 2019 ψω(.) is implemented as a parameter matrix of dimensionsA×D. This matrix can be interpreted as an attribute look-up table as each attribute is associated with a Ddimensional vector, from which a class embedding vector is constructed via a linear combination of the attributes present in the class; the task descriptor embedding is then the concatenation of the embedding vectors of the classes present in the task (see Appendix Fig. 9 for the pictorial description of the joint embedding model). During training, the parameters θand ωare learned by minimizing the cross-entropy loss: ℓk(θ,ω) = 1 N N∑ i=1 −log(p(yk i|xk i,tk; θ,ω)) (12) where (xk i,tk,yk i) is the i-th example of task k. If yk i = c, then the distribution p(.) is given by: p(c|xk i,tk; θ,ω) = exp([φθ(xk i)ψω(tk)⊤]c)∑ jexp([φθ(xk i)ψω(tk)⊤]j) (13) where [a]i denotes the i-th element of the vector a. Note that the architecture and loss functions are general, and apply not only to A-GEM but also to any other LLL model (e.g., regularization based approaches). See Sec. 6 for the actual choice of parameterization of these functions. 6 E XPERIMENTS We consider four dataset streams, see Tab.1 in Appendix Sec. A for a summary of the statistics. Permuted MNIST(Kirkpatrick et al., 2016) is a variant of MNIST (LeCun, 1998) dataset of hand- written digits where each task has a certain random permutation of the input pixels which is applied to all the images of that task. Split CIFAR(Zenke et al., 2017) consists of splitting the original CIFAR-100 dataset (Krizhevsky & Hinton, 2009) into20 disjoint subsets, where each subset is con- structed by randomly sampling 5 classes without replacement from a total of 100 classes. Similarly to Split CIFAR,Split CUBis an incremental version of the ﬁne-grained image classiﬁcation dataset CUB (Wah et al., 2011) of 200 bird categories split into 20 disjoint subsets of classes. Split A WA, on the other hand, is the incremental version of the AW A dataset (Lampert et al., 2009) of50 animal categories, where each task is constructed by sampling 5 classes with replacement from the total 50 classes, constructing 20 tasks. In this setting, classes may overlap among multiple tasks, but within each task they compete against different set of classes. Note that to make sure each training example is only seen once, the training data of a each class is split into disjoint sets depending on the frequency of its occurrence in different tasks. For Split AW A, the classiﬁer weights of each class are randomly initialized within each head without any transfer from the previous occurrence of the class in past tasks. Finally, while on Permuted MNIST and Split CIFAR we provide integer task descriptors, on Split CUB and Split AW A we stack together the attributes of the classes (specifying for instance the type of beak, the color of feathers, etc.) belonging to the current task to form a descriptor. In terms of architectures, we use a fully-connected network with two hidden layers of 256 ReLU units each for Permuted MNIST, a reduced ResNet18 for Split CIFAR like in Lopez-Paz & Ranzato (2017), and a standard ResNet18 (He et al., 2016) for Split CUB and Split AW A. For a given dataset stream, all models use the same architecture, and all models are optimized via stochastic gradient descent with mini-batch size equal to 10. We refer to the joint-embedding model version of these models by appending the sufﬁx ‘-JE’ to the method name. As described in Sec. 2 and outlined in Alg. 1, in order to cross validate we use the ﬁrst 3 tasks, and then report metrics on the remaining 17 tasks after doing a single training pass over each task in sequence. Lastly, we compared A-GEM against several baselines and state-of-the-art LLL approaches which we describe next. VAN is a single supervised learning model, trained continually without any reg- ularization, with the parameters of a new task initialized from the parameters of the previous task. ICARL (Rebufﬁ et al., 2017) is a class-incremental learner that uses nearest-exemplar-based clas- siﬁer and avoids catastrophic forgetting by regularizing over the feature representation of previous tasks using a knowledge distillation loss. EWC (Kirkpatrick et al., 2016), PI (Zenke et al., 2017), RWALK (Chaudhry et al., 2018) and MAS (Aljundi et al., 2018) are regularization-based approaches aiming at avoiding catastrophic forgetting by limiting learning of parameters critical to the perfor- mance of past tasks. Progressive Networks(PROG -NN) (Rusu et al., 2016) is a modular approach 6Published as a conference paper at ICLR 2019 AT (↑) 0.50 0.60 0.70 0.80 0.90 FT (↓) 0.00 0.10 0.20 0.30 0.40 0.50 LCA10(↑) 0.20 0.22 0.24 0.26 0.28 Time(↓) Mem(↓) 0.0 0.2 0.4 0.6 0.8 1.0 AT (↑) 0.40 0.45 0.50 0.55 0.60 0.65 FT (↓) 0.00 0.05 0.10 0.15 0.20 0.25 LCA10(↑) 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 Time(↓) Mem(↓) 0.0 0.2 0.4 0.6 0.8 1.0 0 1 0.0 0.2 0.4 0.6 0.8 1.0 VAN EWC PROG-NN GEM A-GEM (a) Permuted MNIST (b) Split CIFAR Figure 1: Performance of LLL models across different measures on Permuted MNIST and Split CIFAR. For Accuracy (AT) and Learning Curve Measure (LCA10) the higher the number (indicated by ↑) the better is the model. For Forgetting (FT), Time and Memory the lower the number (indicated by ↓) the better is the model. For Time and Memory, the method with the highest complexity is taken as a reference (value of 1) and the other methods are reported relative to that method. AT, FT and LCA10 values and conﬁdence intervals are computed over 5 runs. A-GEM provides the best trade-off across different measures and dimensions. Other baselines are given in Tab. 4 and 7 in the Appendix, which are used to generate the plots. AT (↑) 0.50 0.55 0.60 0.65 0.70 0.75 FT (↓) 0.04 0.06 0.08 0.10 0.12 0.14 0.16 LCA10(↑) 0.30 0.35 0.40 0.45 0.50 0.55 Time(↓)Mem(↓) 0.0 0.2 0.4 0.6 0.8 1.0 AT (↑) 0.30 0.35 0.40 0.45 0.50 FT (↓) 0.02 0.04 0.06 0.08 0.10 LCA10(↑) 0.20 0.23 0.25 0.28 0.30 0.33 0.35 0.38 0.40 Time(↓) Mem(↓) 0.00 0.05 0.10 0.15 0.20 0.25 0.30 VAN VAN-JE EWC EWC-JE A-GEM A-GEM-JE (a) Split CUB (b) Split AW A Figure 2: Performance of LLL models across different measures on Split CUB and Split AWA. On both the datasets, PROG -NN runs out of memory. The memory and time complexities of joint em- bedding models are the same as those of the corresponding standard models and are hence omitted. AT, FT and LCA10 values and conﬁdence intervals are computed over 10 runs. Other baselines are given in Tab. 5, 6 and 7 in the Appendix, which are used to generate the plots. whereby a new “column” with lateral connections to previous hidden layers is added once a new task arrives. GEM (Lopez-Paz & Ranzato, 2017) described in Sec. 4 is another natural baseline of comparison since A-GEM builds upon it. The amount of episodic memory per task used in ICARL , GEM and A-GEM is set to 250, 65, 50, and 100, and the batch size for the computation ofgref (when the episodic memory is sufﬁciently ﬁlled) in A-GEM is set to 256, 1300, 128 and 128 for MNIST, CIFAR, CUB and AW A, respectively. While populating episodic memory, the samples are cho- sen uniformly at random for each task. Whereas the network weights are randomly initialized for MNIST, CIFAR and AW A, on the other hand, for CUB, due to the small dataset size, a pre-trained ImageNet model is used. Finally, we consider a multi-task baseline, MULTI -TASK, trained on a single pass over shufﬂed data from all tasks, and thus violating the LLL assumption. It can be seen as an upper bound performance for average accuracy. 6.1 R ESULTS Fig. 1 and 2 show the overall results on all the datasets we considered (for brevity we show only representative methods, see detailed results in Appendix Tab. 4, 5, 6 and 7). First, we observe that A-GEM achieves the best average accuracy on all datasets, except Permuted MNIST, where PROG - NN works better. The reason is because on this dataset each task has a large number of training 7Published as a conference paper at ICLR 2019 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.5 0.6 0.7 0.8 0.9 Avg Accuracy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.4 0.5 0.6 0.7 Avg Accuracy MAS VAN PROG-NN EWC A-GEM RWALK ICARL PI GEM MULTI-TASK 0 1 2 3 4 5 6 7 8 9 10 Batches 0.10 0.15 0.20 0.25 0.30 LCA (a) Permuted MNIST 0 1 2 3 4 5 6 7 8 9 10 Batches 0.20 0.25 0.30 0.35 LCA  (b) Split CIFAR Figure 3: Top Row:Evolution of average accuracy ( Ak) as new tasks are learned. Bottom Row: Evolution of LCA during the ﬁrst ten mini-batches. 0 1 2 3 4 5 6 7 8 9 10 Batches 0.1 0.2 0.3 0.4 0.5 LCA (a) Split CUB 0 1 2 3 4 5 6 7 8 9 10 Batches 0.200 0.225 0.250 0.275 0.300 0.325 0.350 0.375 0.400 LCA VAN EWC RWALK A-GEM VAN-JE EWC-JE RWALK-JE A-GEM-JE (b) Split AW A Figure 4: Evolution of LCA during the ﬁrst ten mini-batches. examples, which enables PROG -NN to learn its task speciﬁc parameters and to leverage its lateral connections. However, notice how PROG -NN has the worst memory cost by the end of training - as its number of parameters grows super-linearly with the number of tasks. In particular, in large scale setups (Split CUB and AW A),PROG -NN runs out of memory during training due to its large size. Also, PROG -NN does not learn well on datasets where tasks have fewer training examples. Second, A-GEM and GEM perform comparably in terms of average accuracy, but A-GEM has much lower time (about 100 times faster) and memory cost (about10 times lower), comparable to regularization- based approaches like EWC. Third, EWC and similar methods perform only slightly better thanVAN on this single pass LLL setting. The analysis in Appendix Sec. F demonstrates that EWC requires several epochs and over-parameterized architectures in order to work well. Fourth, PROG -NN has no forgetting by construction and A-GEM and GEM have the lowest forgetting among methods that use a ﬁxed capacity architecture. Next, all methods perform similarly in terms of LCA, with PROG - NN being the worst because of its ever growing number of parameters and A-GEM slightly better than all the other approaches. And ﬁnally, the use of task descriptors improves average accuracy across the board as shown in Fig.2, with A-GEM a bit better than all the other methods we tried. All joint-embedding models using task descriptors have better LCA performance, although this is the same across all methods including A-GEM . Overall, we conclude that A-GEM offers the best trade-off between average accuracy performance and efﬁciency in terms of sample, memory and computational cost. Fig. 3 shows a more ﬁne-grained analysis and comparison with more methods on Permuted MNIST and Split CIFAR. The average accuracy plots show how A-GEM and GEM greatly outperform other approaches, with the exception of PROG -NN on MNIST as discussed above. On different datasets, 8Published as a conference paper at ICLR 2019 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.1 0.2 0.3 0.4 0.5 0.6 Zero-shot Acc (a) Split CUB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.20 0.25 0.30 0.35 0.40 0.45 0.50 Zero-shot Acc VAN EWC RWALK A-GEM VAN-JE EWC-JE RWALK-JE A-GEM-JE (b) Split AW A Figure 5: Evolution of zero-shot performance as the learner sees new tasks on Split CUB and Split AWA datasets. different methods are best in terms of LCA, althoughA-GEM is always top-performing. Fig. 4 shows in more detail the gain brought by task descriptors which greatly speed up learning in the few-shot regime. On these datasets, A-GEM performs the best or on par to the best. Finally, in Fig. 5, we report the 0-shot performance of LLL methods on Split CUB and Split AW A datasets over time, showing a clear advantage of using compositional task descriptors with joint embedding models, which is more signiﬁcant for A-GEM . Interestingly, the zero-shot learning per- formance of joint embedding models improves over time, indicating that these models get better at forward transfer or, in other words, become more efﬁcient over time. 7 R ELATED WORK Continual (Ring, 1997) or Lifelong Learning (LLL) (Thrun, 1998) have been the subject of extensive study over the past two decades. One approach to LLL uses modular compositional models (Fer- nando et al., 2017; Aljundi et al., 2017; Rosenbaum et al., 2018; Chang et al., 2018; Xu & Zhu, 2018; Ferran Alet, 2018), which limit interference among tasks by using different subset of modules for each task. Unfortunately, these methods require searching over the space of architectures which is not sample efﬁcient with current methods. Another approach is to regularize parameters important to solve past tasks (Kirkpatrick et al., 2016; Zenke et al., 2017; Chaudhry et al., 2018), which has been proven effective for over-parameterized models in the multiple epoch setting (see Appendix Sec. F), while we focus on learning from few examples using memory efﬁcient models. Methods based on episodic memory (Rebufﬁ et al., 2017; Lopez-Paz & Ranzato, 2017) require a little bit more mem- ory at training time but can work much better in the single pass setting we considered (Lopez-Paz & Ranzato, 2017). The use of task descriptors for LLL has already been advocated by Isele et al. (2016) but using a sparse coding framework which is not obviously applicable to deep nets in a computationally efﬁ- cient way, and also by Lopez-Paz & Ranzato (2017) although they did not explore the use of com- positional descriptors. More generally, tasks descriptors have been used in Reinforcement Learning with similar motivations by several others (Sutton et al., 2011; Schaul et al., 2015; Baroni et al., 2017), and it is also a key ingredient in all the zero/few-shot learning algorithms (Lampert et al., 2014; Xian et al., 2018; Elhoseiny et al., 2017; Wah et al., 2011; Lampert et al., 2009). 8 C ONCLUSION We studied the problem of efﬁcient Lifelong Learning (LLL) in the case where the learner can only do a single pass over the input data stream. We found that our approach, A-GEM , has the best trade- off between average accuracy by the end of the learning experience and computational/memory cost. Compared to the original GEM algorithm, A-GEM is about 100 times faster and has 10 times less memory requirements; compared to regularization based approaches, it achieves signiﬁcantly 9Published as a conference paper at ICLR 2019 higher average accuracy. We also demonstrated that by using compositional task descriptors all methods can improve their few-shot performance, with A-GEM often being the best. Our detailed experiments reported in Appendix E also show that there is still a substantial perfor- mance gap between LLL methods, including A-GEM , trained in a sequential learning setting and the same network trained in a non-sequential multi-task setting, despite seeing the same data samples. Moreover, while task descriptors do help in the few-shot learning regime, the LCA performance gap between different methods is very small; suggesting a poor ability of current methods to trans- fer knowledge even when forgetting has been eliminated. Addressing these two fundamental issues will be the focus of our future research. 10Published as a conference paper at ICLR 2019 REFERENCES Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, pp. 7120–7129, 2017. Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. Marco Baroni, Armand Joulin, Allan Jabri, Germ `an Kruszewski, Angeliki Lazaridou, Klemen Si- monic, and Tomas Mikolov. Commai: Evaluating the ﬁrst steps towards a useful general ai.arXiv preprint arXiv:1701.08954, 2017. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Grifﬁths. Automatically com- posing representation transformations as a means for generalization. In ICML workshop Neural Abstract Machines and Program Induction v2, 2018. Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018. Mohamed Elhoseiny, Ahmed Elgammal, and Babak Saleh. Write a classiﬁer: Predicting visual classiﬁers from unstructured text. IEEE TPAMI, 39(12):2539–2553, 2017. Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Leslie P. Kaelbling Ferran Alet, Tomas Lozano-Perez. Modular meta-learning. arXiv preprint arXiv:1806.10166v1, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In CVPR, pp. 770–778, 2016. David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge transfer in lifelong learning. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence, IJCAI’16, pp. 1620–1626. AAAI Press, 2016. ISBN 978-1-57735-770- 4. James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An- drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic for- getting in neural networks. Proceedings of the National Academy of Sciences of the United States of America (PNAS), 2016. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/cifar.html, 2009. Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 951–958. IEEE, 2009. Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classiﬁcation for zero-shot visual object categorization. IEEE Transactions on Pattern Analysis and Machine In- telligence, 36(3):453–465, 2014. Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continuum learning. In NIPS, 2017. Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. ICLR, 2018. S-V . Rebufﬁ, A. Kolesnikov, and C. H. Lampert. iCaRL: Incremental classiﬁer and representation learning. In CVPR, 2017. 11Published as a conference paper at ICLR 2019 Mark B Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104, 1997. Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In International Conference on Learning Represen- tations, 2018. Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. ICML, 2015. Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress and compress: A scalable framework for continual learning. In International Conference in Machine Learning, 2018. Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NIPS, 2017. R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interac- tion. The 10th International Conference on Autonomous Agents and Multiagent Systems, 2011. Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998. C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning-a com- prehensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence, 2018. Ju Xu and Zhanxing Zhu. Reinforced continual learning. In arXiv preprint arXiv:1805.12369v1, 2018. F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. Ji Zhang, Yannis Kalantidis, Marcus Rohrbach, Manohar Paluri, Ahmed Elgammal, and Mohamed Elhoseiny. Large-scale visual relationship understanding.arXiv preprint arXiv:1804.10660, 2018. 12Published as a conference paper at ICLR 2019 APPENDIX In Sec. A we report the summary of datasets used for the experiments. Sec. B details our A-GEM algorithm and Sec. C provides the proof of update rule ofA-GEM discussed in Sec. 4 of the main pa- per. In Sec. D, we analyze the differences between A-GEM and GEM , and describe another variation of GEM , dubbed Stochastic GEM (S-GEM ). The detailed results of the experiments which were used to generate Fig 1 and 2 in the main paper are given in Sec. E. In Sec. F, we provide empirical ev- idence to the conjecture that regularization-based approaches like EWC require over-parameterized architectures and multiple passes over data in order to perform well as discussed in the Sec. 6.1 of the main paper. In Sec. G, we provide the grid used for the cross-validation of different hyper- parameters and report the optimal values for different models. Finally, in Sec. H, we pictorially describe the joint embedding model discussed in Sec. 5. A D ATASET STATISTICS Table 1: Dataset statistics. Perm. MNIST Split CIFAR Split CUB Split A WA num. of tasks 20 20 20 20 input size 1 ×28×28 3 ×32×32 3 ×224×224 3 ×224×224 num. of classes per task 10 5 10 5 num. of training images per task 60000 2500 300 - num. of test images per task 10000 500 290 560 B A-GEM ALGORITHM Algorithm 2Training and evaluation of A-GEM on sequential data D= {D1,··· ,DT} 1: procedure TRAIN( fθ,Dtrain,Dtest) 2: M←{} 3: A←0 ∈RT×T 4: for t= {1,··· ,T}do 5: for (x,y) ∈Dtrain t do 6: (xref,yref) ∼M 7: gref ←∇θℓ(fθ(xref,t),yref) 8: g←∇θℓ(fθ(x,t),y) 9: if g⊤gref ≥0 then 10: ˜g←g 11: else 12: ˜g←g− g⊤gref g⊤ ref gref gref 13: end if 14: θ←θ−α˜g 15: end for 16: M← UPDATE EPSMEM(M,Dtrain t ,T) 17: At,: ←EVAL(fθ,Dtest) 18: end for 19: return fθ,A 20: end procedure 1: procedure EVAL(fθ,Dtest) 2: a←0 ∈RT 3: for t= {1,··· ,T}do 4: at ←0 5: for (x,y) ∈Dtest t do 6: at ←at + ACCURACY (fθ(x,t),y) 7: end for 8: at ← at len(Dtest t ) 9: end for 10: return a 11: end procedure 1: procedure UPDATE EPSMEM(M,Dt,T) 2: s←|M| T 3: for i= {1,··· ,s}do 4: (x,y) ∼Dt 5: M← (x,y) 6: end for 7: return M 8: end procedure C A-GEM UPDATE RULE Here we provide the proof of the update rule of A-GEM (Eq. 11), ˜g = g− g⊤gref g⊤ ref gref gref, stated in Sec. 4 of the main paper. 13Published as a conference paper at ICLR 2019 Proof. The optimization objective of A-GEM as described in the Eq. 10 of the main paper, is: minimize˜g 1 2||g−˜g||2 2 s.t. ˜g⊤gref ≥0 (14) Replacing ˜gwith zand rewriting Eq. 14 yields: minimizez 1 2z⊤z−g⊤z s.t. −z⊤gref ≤0 (15) Note that we discard the termg⊤gfrom the objective and change the sign of the inequality constraint. The Lagrangian of the constrained optimization problem deﬁned above can be written as: L(z,α) =1 2z⊤z−g⊤z−αz⊤gref (16) Now, we pose the dual of Eq. 16 as: θD(α) = min z L(z,α) (17) Lets ﬁnd the value z∗that minimizes the L(z,α) by setting the derivatives of L(z,α) w.r.t. to z to zero: ∇zL(z,α) = 0 z∗= g+ αgref (18) The simpliﬁed dual after putting the value of z∗in Eq. 17 can be written as: θD(α) =1 2(g⊤g+ 2αg⊤gref + α2g⊤ refgref) −g⊤g−2αg⊤gref −α2g⊤ refgref = −1 2g⊤g−αg⊤gref −1 2α2g⊤ refgref The solution α∗= maxα;α>0 θD(α) to the dual is given by: ∇αθD(α) = 0 α∗= −g⊤gref g⊤ refgref By putting α∗in Eq. 18, we recover the A-GEM update rule: z∗= g− g⊤gref g⊤ refgref gref = ˜g D A NALYSIS OF GEM AND A -GEM In this section, we empirically analyze the differences between A-GEM and GEM , and report experi- ments with another computationally efﬁcient but worse performing version of GEM . D.1 F REQUENCY OF CONSTRAINT VIOLATIONS Fig. 6 shows the frequency of constraint violations (see Eq. 8 and 10) on Permuted MNIST and Split CIFAR datasets. Note that, the number of gradient updates (training steps) per task on MNIST and CIFAR are 5500 and 250, respectively. As the number of tasks increase, GEM violates the optimization constraints at almost each training step, whereas A-GEM plateaus to a much lower value. Therefore, the computational efﬁciency of A-GEM not only stems from the fact that it avoids solving a QP at each training step (which is much more expensive than a simple inner product) but also from the fewer number of constraint violations. From the ﬁgure, we can also infer that as the number of tasks grows the gap between GEM and A-GEM would grow further. Thus, the computational and memory overhead ofGEM over A-GEM , see also Tab. 7, gets worse as the number of tasks increases. 14Published as a conference paper at ICLR 2019 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0 1000 2000 3000 4000 5000 Constraint Violations GEM A-GEM (a) MNIST 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0 50 100 150 200 Constraint Violations GEM A-GEM (b) CIFAR Figure 6: Number of constraint violations in GEM and A-GEM on Permuted MNIST and Split CIFAR as new tasks are learned. D.2 A VERAGE ACCURACY AND WORST -CASE FORGETTING In Tab. 2, we empirically demonstrate the different properties induced by the objective functions of GEM and A-GEM . GEM enjoys lower worst-case task forgetting while A-GEM enjoys better overall average accuracy. This is particularly true on the training examples stored in memory, as on the test set the result is confounded by the generalization error. Table 2: Comparison of average accuracy ( AT) and worst-case forgetting ( Fwst) on the Episodic Memory (M) and Test Set (DEV). Methods MNIST CIFAR M D EV M D EV AT Fwst AT Fwst AT Fwst AT Fwst GEM 99.5 0 89.5 0.10 97.1 0.05 61.2 0.14 A-GEM 99.3 0.008 89.1 0.13 72.1 0.15 62.3 0.15 D.3 S TOCHASTIC GEM (S-GEM ) In this section we report experiments with another variant of GEM , dubbed Stochastic GEM (S- GEM ). The main idea in S-GEM is to randomly sample one constraint, at each training step, from the possible t−1 constraints of GEM . If that constraint is violated, the gradient is projected only taking into account that constraint. Formally, the optimization objective of S-GEM is given by: minimize˜g 1 2||g−˜g||2 2 s.t. ⟨˜g,gk⟩≥ 0 where k∼{1,··· ,t −1} (19) In other words, at each training step, S-GEM avoids the increase in loss of one of the previous tasks sampled randomly. In Tab. 3 we report the comparison of GEM , S-GEM and A-GEM on Permuted MNIST and Split CIFAR. Although, S-GEM is closer in spirit to GEM , as it requires randomly sampling one of the GEM con- straints to satisfy, compared to A-GEM , which deﬁnes the constraint as the average gradient of the previous tasks, it perform slightly worse than GEM , as can be seen from Tab. 3. 15Published as a conference paper at ICLR 2019 Table 3: Comparison of different variations of GEM on MNIST Permutations and Split CIFAR. Methods Permuted MNIST Split CIFAR AT(%) FT AT(%) FT GEM 89.5 0.06 61.2 0.06 S-GEM 88.2 0.08 56.2 0.12 A-GEM 89.1 0.06 62.3 0.07 E R ESULT TABLES In Tab. 4, 5, 6 and 7 we report the detailed results which were used to generate Fig.1 and 2. Table 4: Comparison with different baselines on Permuted MNIST and Split CIFAR. The value of ∞is assigned to a metric when the model fails to train with the cross-validated values of hyper- parameters found on the subset of the tasks as discussed in Sec. 2 of the main paper. The numbers are averaged across 5 runs using a different seed each time. The results from this table are used to generate Fig 1 in Sec. 6.1 of the main paper. Methods Permuted MNIST Split CIFAR AT(%) FT LCA10 AT(%) FT LCA10 VAN 47.9 (± 1.32) 0.51 (± 0.01) 0.26 (± 0.006) 42.9 (± 2.07) 0.25 (± 0.03) 0.30 (± 0.008) ICARL - - - 50.1 0.11 - EWC 68.3 (± 0.69) 0.29 (± 0.01) 0.27 (± 0.003) 42.4 (± 3.02 ) 0.26 (± 0.02) 0.33 (± 0.01) PI ∞ ∞ ∞ 47.1 (± 4.41) 0.17 (± 0.04) 0.31 (± 0.008) MAS 69.6 (± 0.93) 0.27 (± 0.01) 0.29 (± 0.003) 44.2 (± 2.39) 0.25 (± 0.02) 0.33 (± 0.009) RWALK 85.7 (± 0.56) 0.08 (± 0.01) 0.31(± 0.005) 40.9 (± 3.97) 0.29 (± 0.04) 0.32 (± 0.005) PROG-NN 93.5(± 0.07) 0 0.19 (± 0.006) 59.2 (± 0.85) 0 0.21 (± 0.001) GEM 89.5 (± 0.48) 0.06 (± 0.004) 0.23 (± 0.005) 61.2 (± 0.78) 0.06 (± 0.007) 0.36(± 0.007) A-GEM (Ours) 89.1 (± 0.14) 0.06 (± 0.001) 0.29 (± 0.004) 62.3(± 1.24) 0.07 (± 0.01) 0.35 (± 0.01) MULTI-TASK 95.3 - - 68.3 - - Table 5: Average accuracy and forgetting of standard models (left) and joint embedding models (right) on Split CUB. The value of ‘OoM’ is assigned to a metric when the model fails to ﬁt in the memory. The numbers are averaged across 10 runs using a different seed each time. The results from this table are used to generate Fig 2 in Sec. 6.1 of the main paper. Methods Split CUB AT(%) FT LCA10 VAN 54.3 (± 2.03) / 67.1 (± 4.77) 0.13 (± 0.02)/ 0.10 (± 0.04) 0.29 (± 0.009) / 0.52 (± 0.01) EWC 54 (± 1.08) / 68.4 (± 4.08) 0.13 (± 0.02) / 0.09 (± 0.03) 0.29 (± 0.007) / 0.52 (± 0.01) PI 55.3 (± 2.28) / 66.6 (± 5.18) 0.12 (± 0.02)/ 0.10 (± 0.04) 0.29 (± 0.008) / 0.52 (± 0.01) RWALK 54.4 (± 1.82) / 67.4 (± 3.50) 0.13 (± 0.01) / 0.10 (± 0.03) 0.29 (± 0.008) / 0.52 (± 0.01) PROG-NN OoM / OoM OoM / OoM OoM / OoM A-GEM (Ours) 62 (± 3.5) / 71 (± 2.83) 0.07 (± 0.02) /0.07 (± 0.01) 0.30 (± 0.008) /0.54 (± 0.02) MULTI-TASK 65.6 / 73.8 - / - - / - 16Published as a conference paper at ICLR 2019 Table 6: Average accuracy and forgetting of standard models (left) and joint embedding models (right) on Split AWA. The value of ‘OoM’ is assigned to a metric when the model fails to ﬁt in the memory. The numbers are averaged across 10 runs using a different seed each time. The results from this table are used to generate Fig 2 in Sec. 6.1 of the main paper. Methods Split A WA AT(%) FT LCA10 VAN 30.3 (± 2.84) / 42.8 (± 2.86) 0.04 (± 0.01) / 0.07 (± 0.02) 0.21 (± 0.008) / 0.37 (± 0.02) EWC 33.9 (± 2.87) / 43.3 (± 3.71) 0.08 (± 0.02) / 0.07 (± 0.03) 0.26 (± 0.01) / 0.37 (± 0.02) PI 33.9 (± 3.25) / 43.4 (± 3.49) 0.08 (± 0.02) / 0.06 (± 0.02) 0.26 (± 0.01) / 0.37 (± 0.02) RWALK 33.9 (± 2.91) / 42.9 (± 3.10) 0.08 (± 0.02) / 0.07 (± 0.02) 0.26 (± 0.01) / 0.37 (± 0.02) PROG-NN OoM / OoM OoM / OoM OoM / OoM A-GEM (Ours) 44 (± 4.10) /50 (± 3.25) 0.05 (± 0.02) / 0.03 (± 0.02) 0.29 (± 0.01) /0.39 (± 0.02) MULTI-TASK 64.8 / 66.8 - / - - / - Table 7: Computational cost and memory complexity of different LLL approaches. The timing refers to training time on a GPU device. Memory cost is provided in terms of the total number of parame- ters P , the size of the minibatch B, the total size of the network hidden state H (assuming all methods use the same architecture), the size of the episodic memory M per task. The results from this table are used to generate Fig. 1 and 2 in Sec. 6.1 of the main paper. Methods Training Time [s] Memory MNIST CIFAR CUB AW A Training Testing VAN 186 105 54 4123 P + B*H P + B*H EWC 403 250 72 4136 4*P + B*H P + B*H PROGRESSIVE NETS 510 409 ∞ ∞ 2*P*T + B*H*T 2*P*T + B*H*T GEM 3442 5238 - - P*T + (B+M)*H P + B*H A-GEM (Ours) 477 449 420 5221 2*P + (B+M)*H P + B*H F A NALYSIS OF EWC In this section we provide empirical evidence to the conjecture that regularization-based approaches like EWC need over-parameterized architectures and multiple passes over the samples of each task in order to perform well. The intuition as to why models need to be over-parameterized is because it is easier to avoid cross-task interference when the model has additional capacity. In the single-pass set- ting and when each task does not have very many training samples, regularization-based appraches also suffer because regularization parameters cannot be estimated well from a model that has not fully converged. Moreover, for tasks that do not have much data, rgularization-based approaches do not enable any kind of positive backward transfer (Lopez-Paz & Ranzato, 2017) which further hurts performance as the predictor cannot leverage knowledge acquired later to improve its prediction on past tasks. Finally, regularization-based approaches perform much better in the multi-epoch setting simply because in this setting the baseline un-regularized model performs much worse, as it overﬁts much more to the data of the current task, every time unlearning what it learned before. We consider Permuted MNIST and Split CIFAR datasets as described in Sec. 6 of the main paper. For MNIST, the two architecture variants that we experiment with are;1) two-layer fully-connected network with 256 units in each layer (denoted by −S sufﬁx), and 2) two-layer fully-connected network with 2000 units in each layer (denoted by −Bsufﬁx). For CIFAR, the two architecture variants are;1) ResNet-18 with 3 times less feature maps in all the layers (denoted by −Ssufﬁx), and 2) Standard ResNet-18 (denoted by −Btoken). We run the experiments on VAN and EWC with increasing the number of epochs from 1 to 10 for Permuted MNIST and from 1 to 30 for CIFAR. For instance, when epoch is set to 10, it means that the training samples of task tare presented 10 times before showing examples from task t+ 1. In Fig. 7 and 8 we plot the Average Accuracy (Eq. 1) and Forgetting (Eq. 2) on Permuted MNIST and Split CIFAR, respectively. 17Published as a conference paper at ICLR 2019 We observe that the average accuracy signiﬁcantly improves with the number of epochs only when EWC is applied to the big network. In particular, in the single epoch setting, EWC peforms similarly to the baseline VAN on Split CIFAR which has fewer number of training examples per task. 1 3 10 Epochs 0.5 0.6 0.7 0.8 Avg Accuracy (a) Accuracy 1 3 10 Epochs 0.2 0.3 0.4 0.5 Forgetting VAN-S EWC-S VAN-B EWC-B (b) Forgetting Figure 7: Permuted MNIST: Change in average accuracy and forgetting as the number of epochs are increased. Tokens ’-S’ and ’-B’ denote smaller and bigger networks, respectively. 1 3 10 30 Epochs 0.3 0.4 0.5 0.6 Avg Accuracy (a) Accuracy 1 3 10 30 Epochs 0.1 0.2 0.3 0.4 Forgetting VAN-S EWC-S VAN-B EWC-B (b) Forgetting Figure 8: Split CIFAR: Change in average accuracy and forgetting as the number of epochs are increased. Tokens ’-S’ and ’-B’ denote smaller and bigger networks, respectively. G H YPER -PARAMETER SELECTION Below we report the hyper-parameters grid considered for different experiments. Note, as described in the Sec. 6 of the main paper, to satisfy the requirement that a learner does not see the data of a task more than once, ﬁrst TCV tasks are used to cross-validate the hyper-parameters. In all the datasets, the value of TCV is set to ‘3’. The best setting for each experiment is reported in the parenthesis. • MULTI -TASK – learning rate: [ 0.3, 0.1, 0.03 (MNIST perm, Split CIFAR, Split CUB, Split AW A), 0.01, 0.003, 0.001, 0.0003, 0.0001] • MULTI -TASK -JE – learning rate: [ 0.3, 0.1, 0.03 (Split CUB, Split AW A), 0.01, 0.003, 0.001, 0.0003, 0.0001] • VAN – learning rate: [ 0.3, 0.1, 0.03 (MNIST perm, Split CUB), 0.01 (Split CIFAR), 0.003, 0.001 (Split AW A),0.0003, 0.0001] • VAN-JE – learning rate: [ 0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] • PROG -NN – learning rate: [ 0.3, 0.1 (MNIST perm, ), 0.03 (Split CIFAR, Split AW A),0.01 (Split CUB), 0.003, 0.001, 0.0003, 0.0001] • EWC – learning rate: [ 0.3, 0.1, 0.03 (MNIST perm, Split CIFAR, Split CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] 18Published as a conference paper at ICLR 2019 – regularization: [ 1 (Split CUB), 10 (MNIST perm, Split CIFAR), 100 (Split AW A), 1000, 10000] • EWC -JE – learning rate: [ 0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] – regularization: [1, 10 (Split CUB), 100 (Split AW A),1000, 10000] • PI – learning rate: [ 0.3, 0.1 (MNIST perm), 0.03 (Split CUB), 0.01 (Split CIFAR), 0.003 (Split AW A),0.001, 0.0003, 0.0001] – regularization: [ 0.001, 0.01, 0.1 (MNIST perm, Split CIFAR, Split CUB), 1 (Split AW A),10] • PI-JE – learning rate: [ 0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] – regularization: [0.001, 0.01, 0.1 (Split CUB), 1, 10 (Split AW A)] • MAS – learning rate: [ 0.3, 0.1 (MNIST perm), 0.03 (Split CIFAR, Split CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] – regularization: [ 0.01, 0.1 (MNIST perm, Split CIFAR, Split CUB), 1 (Split AW A), 10] • MAS -JE – learning rate: [ 0.3, 0.1, 0.03 (Split CUB), 0.01, 0.003, 0.001 (Split AW A),0.0003, 0.0001] – regularization: [0.01, 0.1 (Split CUB, Split AW A),1, 10] • RWALK – learning rate: [ 0.3, 0.1 (MNIST perm), 0.03 (Split CIFAR, Split CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] – regularization: [0.1, 1 (MNIST perm, Split CIFAR, Split CUB),10 (Split AW A),100, 1000] • RWALK -JE – learning rate: [ 0.3, 0.1, 0.03 (SPLIT CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] – regularization: [0.1, 1 (Split CUB), 10 (Split AW A),100, 1000] • A-GEM – learning rate: [ 0.3, 0.1 (MNIST perm), 0.03 (Split CIFAR, Split CUB), 0.01 (Split AW A),0.003, 0.001, 0.0003, 0.0001] • A-GEM -JE – learning rate: [ 0.3, 0.1, 0.03 (SPLIT CUB), 0.01, 0.003 (Split AW A),0.001, 0.0003, 0.0001] H P ICTORIAL DESCRIPTION OF JOINT EMBEDDING MODEL In Fig. 9 we provide a pictorial description of the joint embedding model discussed in the Sec. 5 of the main paper. 19Published as a conference paper at ICLR 2019 (xk, tk, yk) xktk Ck×A ϕθ( . )Pψω( . )A×D yk ℝDℝCk×D ℝCk Image EmbeddingAttribute Embedding Softmax Classiﬁer Input = Figure 9: Pictorial description of the joint embedding model discussed in the Sec. 5 of the main paper. Modules; φθ(.) and ψω(.) are implemented as feed-forward neural networks with P and A×Dparameters, respectively. The descriptor of task k(tk ) is a matrix of dimensions Ck ×A, shared among all the examples of the task, constructed by concatenating the A-dimensional class attribute vectors of Ck classes in the task. 20",
      "meta_data": {
        "arxiv_id": "1812.00420v2",
        "authors": [
          "Arslan Chaudhry",
          "Marc'Aurelio Ranzato",
          "Marcus Rohrbach",
          "Mohamed Elhoseiny"
        ],
        "published_date": "2018-12-02T16:39:19Z",
        "pdf_url": "https://arxiv.org/pdf/1812.00420v2.pdf"
      }
    },
    {
      "title": "Continual learning with tiny episodic memories",
      "abstract": "In continual learning (CL), an agent learns from a stream of tasks leveraging\nprior experience to transfer knowledge to future tasks. It is an ideal\nframework to decrease the amount of supervision in the existing learning\nalgorithms. But for a successful knowledge transfer, the learner needs to\nremember how to perform previous tasks. One way to endow the learner the\nability to perform tasks seen in the past is to store a small memory, dubbed\nepisodic memory, that stores few examples from previous tasks and then to\nreplay these examples when training for future tasks. In this work, we\nempirically analyze the effectiveness of a very small episodic memory in a CL\nsetup where each training example is only seen once. Surprisingly, across four\nrather different supervised learning benchmarks adapted to CL, a very simple\nbaseline, that jointly trains on both examples from the current task as well as\nexamples stored in the episodic memory, significantly outperforms specifically\ndesigned CL approaches with and without episodic memory. Interestingly, we find\nthat repetitive training on even tiny memories of past tasks does not harm\ngeneralization, on the contrary, it improves it, with gains between 7\\% and\n17\\% when the memory is populated with a single example per class.",
      "full_text": "On Tiny Episodic Memories in Continual Learning Arslan Chaudhry arslan.chaudhry@eng.ox.ac.uk University of Oxford Marcus Rohrbach Facebook AI Research Mohamed Elhoseiny King Abdullah University of Science and Technology Thalaiyasingam Ajanthan Australian National University Puneet K. Dokania University of Oxford Philip H. S. Torr University of Oxford Marc’Aurelio Ranzato Facebook AI Research Abstract In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, signiﬁcantly outperforms speciﬁcally designed CL approaches with and without episodic memory. Interestingly, we ﬁnd that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7% and 17% when the memory is populated with a single example per class.1 1 Introduction The objective of continual learning (CL) is to rapidly learn new skills from a sequence of tasks leveraging the knowledge accumulated in the past. Catastrophic forgetting [McCloskey and Cohen, 1989], i.e. the inability of a model to recall how to perform tasks seen in the past, makes such efﬁcient adaptation extremely difﬁcult. This decades old problem of CL [Ring, 1997; Thrun, 1998] is now seeing a surge of interest in the research community with several methods proposed to tackle catastrophic forgetting [Rebufﬁ et al., 2017; Kirkpatrick et al., 2016; Zenke et al., 2017; Lee et al., 2017a; Aljundi et al., 2018; Lopez-Paz and Ranzato, 2017; Lee et al., 2017b; Chaudhry et al., 2019]. In this work, we quantitatively study some of these methods (that assume a ﬁxed network architecture) on four benchmark datasets under the following assumptions: i) each task is fully supervised, ii) each example from a task can only be seen once using the learning protocol proposed by Chaudhry et al.[2019] (see §3), and iii) the model has access to a small memory storing examples of past tasks. Restricting the size of such episodic 1Code: https://github.com/facebookresearch/agem Preprint. Under review. arXiv:1902.10486v4  [cs.LG]  4 Jun 2019memory is important because it makes the continual learning problem more realistic and distinct from multi-task learning where complete datasets of all the tasks are available at each step. We empirically observe that a very simple baseline, dubbed Experience Replay (ER)2, that jointly trains on both the examples from the current task and examples stored in the very small episodic memory not only gives superior performance over the existing state-of-the-art approaches speciﬁcally designed for CL (with and without episodic memory), but it also is computationally very efﬁcient. We verify this ﬁnding on four rather different supervised learning benchmarks adapted for CL; Permuted MNIST, Split CIFAR, Split miniImageNet and Split CUB. Importantly, repetitive training on the same examples of a tiny episodic memory does not harm generalization on past tasks. In §5.5, we analyze this phenomenon and provide insights as to why directly training on the episodic memory does not have a detrimental effect in terms of generalization. Brieﬂy, we observe that the training on the datasets of subsequent tasks acts like a data-dependent regularizer on past tasks allowing the repetitive training on tiny memory to generalize beyond the episodic memory. We further observe that methods, that do not train directly on the memory, such as GEM [Lopez-Paz and Ranzato, 2017] and A-GEM [Chaudhry et al., 2019], underﬁt the training data and end up not fully utilizing the beneﬁcial effects of this implicit and data depdendent regularization. Overall, ER with tiny episodic memories offers very strong performance at a very small additional computational cost over the ﬁne-tuning baseline. We believe that this approach will serve as a stronger baseline for the development of future CL approaches. 2 Related Work Regularization-based CL approaches These works attempt to reduce forgetting by regularizing the objective such that it either penalizes the feature drift on already learned tasks [Li and Hoiem, 2016; Rebufﬁ et al., 2017] or discourages change in parameters that were important to solve past tasks [Kirkpatrick et al., 2016; Zenke et al., 2017; Chaudhry et al., 2018; Aljundi et al., 2018]. The former approach relies on the storage of network activations and subsequent deployment of knowledge distillation [Hinton et al., 2014], whereas the latter approach stores a measure of parameter importance whose best case memory complexity is the same as the total number of network parameters. Memory-Based CL approaches These approaches [Lopez-Paz and Ranzato, 2017; Riemer et al., 2019; Chaudhry et al., 2019] use episodic memory that stores a subset of data from past tasks to tackle forgetting. One approach to leverage such episodic memory is to use it to constrain the optimization such that the loss on past tasks can never increase [Lopez-Paz and Ranzato, 2017]. Experience Replay (ER) The use of ER is well established in reinforcement learning (RL) tasks [Mnih et al., 2013, 2015; Foerster et al., 2017; Rolnick et al., 2018]. Isele and Cosgun [2018], for instance, explore different ways to populate a relatively large episodic memory for a continual RL setting where the learner does multiple passes over the data. In this work instead, we study supervised learning tasks with a single pass through data and a very small episodic memory. More recently, [Hayes et al., 2018; Riemer et al., 2019] used ER for supervised CL learning tasks. Hayes et al.[2018], independently, study different replay strategies in ER and show improvements over the ﬁnetune baseline. Our contribution is to show the improvements brought by ER, perhaps surprisingly, over the speciﬁcally designed CL approaches. We differ from [Riemer et al., 2019] in considering episodic memories of much smaller sizes. Finally, and most importantly, we extend these previous studies by analyzing why repetitive training on tiny memories does not lead to overﬁtting (§5.5). 3 Learning Framework 3.1 Protocol for Single-Pass Through the Data We use the learning protocol proposed by Chaudhry et al.[2019]. There are two streams of tasks, described by the following ordered sequences of datasets, one stream for Cross-Validation DCV = {D−TCV ,··· ,D−1}consisting of TCV tasks, and one for EValuation DEV = {D1,··· ,DT} 2For consistency to prior work in the literature, we will refer to this approach which trains on the episodic memory as ER, although its usage for supervised learning tasks is far less established. 2consisting of T tasks, where Dk = {(xk i,tk i,yk i)nk i=1}is the dataset of the k-th task. The sequence DCV contains only a handful of tasks and it is only used for cross-validation purposes. Tasks from this sequence can be replayed as many times as needed and have various degree of similarity to tasks in the training and evaluation dataset,DEV. The latter stream, DEV, instead can be played only once; the learner will observe examples in sequence and will be tested throughout the learning experience. The ﬁnal performance will be reported on the held-out test set drawn from DEV. The k-th task in any of these streams consists of Dk = {(xk i,tk i,yk i)nk i=1}, where each triplet consti- tutes an example deﬁned by an input (xk ∈X), a task descriptor (tk ∈T ) which is an integer id in this work, and a target vector (yk ∈yk), where yk is the set of labels speciﬁc to task kand yk ⊂Y. 3.2 Metrics We measure performance onDEV using two metrics, as standard practice in the literature [Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2018]: Average Accuracy (A∈[0,1]) Let ai,j be the performance of the model on the held-out test set of task ‘j’ after the model is trained on task ‘i’. The average accuracy at taskT is then deﬁned as: AT = 1 T T∑ j=1 aT,j (1) Forgetting (F ∈[−1,1]) Let fi j be the forgetting on task ‘j’ after the model is trained on task ‘i’ which is computed as: fi j = max l∈{1,···,i−1} al,j −ai,j (2) The average forgetting measure at task T is then deﬁned as: FT = 1 T −1 T−1∑ j=1 fT j (3) 4 Experience Replay Recent works [Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019] have shown that methods relying on episodic memory have superior performance than regularization based approaches (e.g., [Kirkpatrick et al., 2016; Zenke et al., 2017]) when using a “single-pass through the data” protocol (§3.1). While GEM [Lopez-Paz and Ranzato, 2017] and its more efﬁcient version A-GEM [Chaudhry et al., 2019] used the episodic memory as a mean to project gradients, here we drastically simplify the optimization problem and, similar to Riemer et al.[2019] and Hayes et al.[2018], directly train on the the examples stored in a very small memory, resulting in better performance and more efﬁcient learning. The overall training procedure is given in Alg. 1. Compared to the simplest baseline model that merely ﬁne-tunes the parameters on the new task starting from the previous task parameter vector, ER makes two modiﬁcations. First, it has an episodic memory which is updated at every time step, line 8. Second, it doubles the size of the minibatch used to compute the gradient descent parameter update by stacking the actual minibatch of examples from the current task with a minibatch of examples taken at random from the memory, line 7. As we shall see in our empirical validation, these two simple modiﬁcations yield much better generalization and substantially limit forgetting, while incurring in a negligible additional computational cost on modern GPU devices. Next, we explain the difference between the direct (ER) and indirect (A-GEM) training on episodic memory from the optimization perspective. A-GEM vs ER: Let us assume that Bn is a mini-batch of size Kfrom the current task tand BMis the same size mini-batch from a very small episodic memoryM. Furthermore, following the notation from Chaudhry et al.[2019], let gbe the gradient computed with mini-batch Bn and gref be the gradient computed with BM. In A-GEM, if gTgref ≥0, then the current task gradient gis directly used for optimization whereas if gTgref <0, gis projected such that gTgref = 0. Refer to Eq. 11 3Algorithm 1 Experience Replay for Continual Learning. 1: procedure ER(D,mem_sz,batch_sz,lr) 2: M←{}∗ mem_sz ⊿Allocate memory buffer of size mem_sz 3: n←0 ⊿Number of training examples seen in the continuum 4: for t∈{1,··· ,T}do 5: for Bn K ∼Dt do ⊿Sample without replacement a mini-batch of size Kfrom task t 6: BM K ∼M ⊿Sample a mini-batch from M 7: θ←SGD(Bn ∪BM,θ, lr) ⊿Single gradient step to update the parameters by stacking current minibatch with minibatch from memory 8: M← UpdateMemory(mem_sz,t,n,B n) ⊿Memory update, see §4 9: n←n+ batch_sz ⊿Counter update 10: return θ,M in Chaudhry et al.[2019] for the exact form of projection. In ER instead, since both mini-batches are used in the optimization step, the average of gand gref is used. It may seem a bit counter-intuitive that, even though ER repetitively trains on M, it is still able to generalize to previous tasks beyond the episodic memory. We investigate this question in §5.5. Since we study the usage of tiny episodic memories, the sample that the learner selects to populate the memory becomes crucial, see line 8 of the algorithm. For this, we describe various strategies to write into the memory. All these strategies assume access to a continuous stream of data and a small episodic memory, which rules out approaches relying on the temporary storage of all the examples seen so far. This restriction is consistent with our deﬁnition of CL: a learning experience through a stream of data under the constraint of a ﬁxed and small sized memory and limited compute budget. Reservoir Sampling: Similarly to Riemer et al.[2019], Reservoir sampling [Vitter, 1985] takes as input a stream of data of unknown length and returns a random subset of items from that stream. If ‘n’ is the number of points observed so far and ‘mem_sz’ is the size of the reservoir (sampling buffer), this selection strategy samples each data point with a probability mem_sz n . The routine to update the memory is given in Appendix Alg. 2. Ring Buffer: Similarly to Lopez-Paz and Ranzato [2017], for each task, the ring buffer strategy allocates as many equally sized FIFO buffers as there are classes. If Cis the total number of classes across all tasks, and mem_sz is the total size of episodic memory, each stack has a buffer of sizemem_sz C . As shown in Appendix Alg. 3, the memory stores the last few observations from each class. Unlike reservoir sampling, samples from older tasks do not change throughout training, leading to potentially stronger overﬁtting. Also, at early stages of training the memory is not fully utilized since each stack has a constant size throughout training. However, this simple sampling strategy guarantees equal representation of all classes in the memory, which is particularly important when the memory is tiny. k-Means: For each class, we use online k-Means to estimate the k centroids in feature space, using the representation before the last classiﬁcation layer. We then store in the memory the input examples whose feature representation is the closest to such centroids, see Appendix Alg. 4. This memory writing strategy has similar beneﬁts and drawbacks of ring buffer, except that it has potentially better coverage of the feature space in L2 sense. Mean of Features (MoF):Similarly to Rebufﬁ et al.[2017], for each class we compute a running estimate of the average feature vector just before the classiﬁcation layer and store examples whose feature representation is closest to the average feature vector (see details in Appendix Alg. 5). This writing strategy has the same balancing guarantees of ring buffer and k-means, but it populates the memory differently. Instead of populating the memory at random or using k-Means, it puts examples that are closest to the mode in feature space. 45 Experiments In this section, we review the benchmark datasets used in our evaluation, as well as the architectures and the baselines we compared against. We then report the results we obtained using episodic memory and experience replay (ER). Finally, we conclude with a brief analysis investigating generalization when using ER on tiny memories. 5.1 Datasets We consider four commonly used benchmarks in CL literature. Permuted MNIST[Kirkpatrick et al., 2016] is a variant of MNIST [LeCun, 1998] dataset of handwritten digits where each task has a certain random permutation of the input pixels which is applied to all the images of that task. Our Permuted MNIST benchmark consists of a total of 23 tasks. Split CIFAR[Zenke et al., 2017] consists of splitting the original CIFAR-100 dataset [Krizhevsky and Hinton, 2009] into 20 disjoint subsets, each of which is considered as a separate task. Each task has 5 classes that are randomly sampled without replacement from the total of 100 classes. Similarly to Split CIFAR, Split miniImageNetis constructed by splitting miniImageNet [Vinyals et al., 2016], a subset of ImageNet with a total of 100 classes and 600 images per class, to 20 disjoint subsets. Finally, Split CUB [Chaudhry et al., 2019] is an incremental version of the ﬁne-grained image classiﬁcation dataset CUB [Wah et al., 2011] of 200 bird categories split into 20 disjoint subsets of classes. In all cases, DCV consists of 3 tasks while DEV contains the remaining tasks. As described in § 3.2, we report metrics on DEV after doing a single training pass over each task in the sequence. The hyper-parameters selected via cross-validation on DCV are reported in Appenddix Tab. 8. 5.2 Architectures For MNIST, we use a fully-connected network with two hidden layers of 256 ReLU units each. For CIFAR and miniImageNet, a reduced ResNet18, similar to Lopez-Paz and Ranzato [2017], is used and a standard ResNet18 with ImageNet pretraining is used for CUB. The input integer task id is used to select a task speciﬁc classiﬁer head, and the network is trained via cross-entropy loss. For a given dataset stream, all baselines use the same architecture, and all baselines are optimized via stochastic gradient descent with a mini-batch size equal to 10. The size of the mini-batch sampled from the episodic memory is also set to 10 irrespective of the size of the episodic buffer. 5.3 Baselines We compare against the following baselines: • FINETUNE , a model trained continually without any regularization and episodic memory, with parameters of a new task initialized from the parameters of the previous task. • EWC [Kirkpatrick et al., 2016], a regularization-based approach that avoids catastrophic forgetting by limiting the learning of parameters critical to the performance of past tasks, as measured by the Fisher information matrix (FIM). In particular, we compute the FIM as a moving average similar to EWC ++ in Chaudhry et al.[2018] and online EWC in Schwarz et al. [2018]. • A-GEM [Chaudhry et al., 2019], a model that uses episodic memory as an optimization constraint to avoid catastrophic forgetting. Since GEM [Lopez-Paz and Ranzato, 2017] and A-GEM have similar performance, we only consider the latter in our experiments due to its computational efﬁciency. • MER [Riemer et al., 2019], a model that also leverages an episodic memory and uses a loss that approximates the dot products of the gradients of current and previous tasks to avoid forgetting. To make the experimental setting more comparable (in terms of SGD updates) to the other methods, we set the number of inner gradient steps to 1 for each outer Reptile [Nichol and Schulman, 2018] meta-update with the mini-batch size of 10. 55.4 Results 1 10 Memory per Class 0.6 0.7 0.8Avg. Accuracy ER gain over FT=+16.7% ER gain over EWC=+7.1% (a) MNIST 1 10 Memory per Class 0.4 0.5 0.6 0.7Avg. Accuracy ER gain over FT=+15.6% ER gain over EWC=+15% FINETUNE EWC A-GEM MER ER-Reservoir ER-Ringbuffer ER-K-Means ER-MoF (b) CIFAR 1 10 Memory per Class 0.55 0.60 0.65 0.70 0.75Avg. Accuracy ER gain over FT=+9.3% ER gain over EWC=+10 (c) CUB 1 10 Memory per Class 0.35 0.40 0.45 0.50 0.55 0.60Avg. Accuracy ER gain over FT=+14.3% ER gain over EWC=+11.3 FINETUNE EWC A-GEM MER ER-Reservoir ER-Ringbuffer ER-K-Means ER-MoF (d) miniImageNet Figure 1: Average accuracy as a function of episodic memory size. The box shows the gain in average accuracy of ER-RINGBUFFER over FINETUNE and EWC baselines when only1 sample per class is used. The performance is averaged over5 runs. Uncertainty estimates are provided in Appendix Tabs 3,4,5,6. In the ﬁrst experiment, we measured average accuracy at the end of the learning experience on DEV as a function of the size of the memory (detailed numerical results are provided in Appendix Tabs 3,4,5,6). From the results in Fig. 1, we can make several observations. First, methods using ER greatly outperformnot only the baseline approaches that do not have episodic memory (FINETUNE and EWC ) but also state-of-the-art approaches relying on episodic memory of the same size (A-GEM and MER ). Moreover, the ER variants outperform even when the episodic memory is very small. For instance, on CIFAR the gain over A-GEM brought by ER is 1.7% when the memory only stores 1 example per class, and more than 5% when the memory stores 13 examples per class. This ﬁnding might seem quite surprising as repetitive training on a very small episodic memory may potentially lead to overﬁtting on the examples stored in the memory. We will investigate this ﬁnding in more depth in §5.5. In the same setting, the gain compared to methods that do not use memory (FINETUNE and EWC ) is 15% and about 28% when using a single example per class and 13 examples per class, respectively. Second and not surprisingly, average accuracy increases with the memory size, and does not saturate at 13 examples per class which is our self-imposed limit. Third, experience replay based on reservoir sampling works the best across the board except when the memory size is very small (less than 3 examples per class). Empirically we observed that as more and more tasks arrive and the size of the memory per class shrinks, reservoir sampling often ends up evicting some of the earlier classes from the memory, thereby inducing higher forgetting. Fourth, when the memory is tiny, sampling methods that by construction guarantee a balanced number of samples per class, work the best (even better than reservoir sampling). All methods that have this property, ring buffer, k-Means and Mean of Features, have a rather similar performance which is substantially better than the reservoir sampling. For instance, on CIFAR, with one example per class in the memory, ER with reservoir sampling is 3.5% worse than ER K-Means, but ER K-Means, ER Ring Buffer and ER MoF are all within 0.5% from each other (see Appendix Tab. 4 for numerical values). These ﬁndings are further conﬁrmed by looking at the evolution of the average accuracy (Appendix Fig. 5 left) as new tasks arrive when the memory can store at most one example per class. The better performance of strategies like ring buffer for tiny episodic memories, and reservoir sampling for bigger episodic memories, suggests a hybrid approach, whereby the writing strategy relies on reservoir sampling till some classes have too few samples stored in the memory. At that point, the writing strategy switches to the ring buffer scheme which guarantees a minimum number of examples for each class. For instance, in the experiment of Fig. 2 the memory budget consists of only 85 memory slots, an average of 1 sample per class by the end of the learning experience (as there are 61 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.50 0.55 0.60 0.65 Avg Accuracy ER-Reservoir ER-Ringbuffer ER-Ringbuffer-Full ER-Hybrid Figure 2: Evolution of average accuracy (Ak) as new tasks are learned in Split CIFAR. The memory has only 85 slots (in average 1 slot per class). The vertical bar marks where the hybrid approach switches from reservoir to ring buffer strategy. The hybrid approach works better than both reservoir (once more tasks arrive) and ring buffer (initially, when the memory is otherwise not well utilized). The orange curve is a variant of ring buffer that utilizes the full memory at all times, by reducing the ring buffer size of observed classes as new classes arrive. Overall, the proposed hybrid approach works at least as good as the other approaches throughout the whole learning experience. (Averaged over3 runs). Methods Forgetting MNIST CIFAR CUB miniImageNet FINETUNE 0.29 0.27 0.13 0.26 EWC 0.18 0.27 0.14 0.21 A-GEM 0.21 0.14 0.09 0.13 MER 0.14 0.19 0.10 0.15 ER-RINGBUFFER (ours) 0.12 0.13 0.03 0.12 Table 1: Forgetting when using a tiny episodic memory of single example per class. Methods Training Time [s] CIFAR CUB FINETUNE 87 194 EWC 159 235 A-GEM 230 510 MER 755 277 ER-RINGBUFFER (ours) 116 255 Table 2: Learning Time onDEV [s] 17 tasks and 5 classes per task). The learner switches from reservoir sampling to ring buffer once it observes that any of the classes seen in the past has only one sample left in the memory. When the switch happens (marked by a red vertical line in the ﬁgure), the learner only keeps randomly picked min(n,|M| K ) examples per class in the memory, where nis the number of examples of class cin the memory and K are the total number of classes observed so far. The overwriting happens opportunistically, removing examples from over-represented classes as new classes are observed. Fig. 2 shows that when the number of tasks is small, the hybrid version enjoys the high accuracy of reservoir sampling. As more tasks arrive and the memory per task shrinks, the hybrid scheme achieves superior performance than reservoir (and at least similar to ring buffer). Finally, experience replay methods are not only outperforming all other approaches in terms of accuracy (and lower forgetting as reported in Tab. 1), but also in terms of compute time. Tab. 2 reports training time on both Split CIFAR and Split CUB, using ring buffer as a use case since all other ER methods have the same computational complexity. We observe that ER adds only a slight overhead compared to the ﬁnetuning baseline, but it is much cheaper than stronger baseline methods like A-GEM and MER . 5.5 Analysis The strong performance of experience replay methods which directly learn using the examples stored in the small episodic memory may be surprising. In fact, Lopez-Paz and Ranzato [2017] discounted this repetitive training on the memory option by saying: “Obviously, minimizing the loss at the current example together with [the loss on the episodic memory] results in overﬁtting to the examples stored in [the memory]”. How can the repeated training over the same very small handful of examples possibly generalize? To investigate this matter we conducted an additional experiment. For simplicity, we consider only two tasks, T1 and T2, and study the generalization performance on T1 as we train on T2. We denote by D2 the training set of T2 and by M1 the memory storing examples from T1’s training set. Our hypothesis is that although direct training on the examples in M1 (in addition to those coming from D2) does indeed lead to strong memorization of M1 (as measured by nearly zero cross-entropy loss on M1), such training is still overall beneﬁcial in terms of generalization on the original task T1 70 25 50 75 100 125 150 Iterations on T2 (k) 0.4 0.5 0.6 0.7 0.8 0.9 Accuracy T1 (a) 20° rotation 0 25 50 75 100 125 150 Iterations on T2 (k) 0.4 0.5 0.6 0.7 0.8 Accuracy T1  (b) 40° rotation 0 25 50 75 100 125 150 Iterations on T2 (k) 0.2 0.4 0.6 0.8 Accuracy T1  1 D2 D2 1  (c) 60° rotation Figure 3: Analysis on MNIST Rotation: Test accuracy on Task 1 as a function of the training iterations over Task 2. The blue curves are the accuracy when the model is trained using onlyM1. The red curves are the accuracy when the model is trained using onlyD2, the training set of Task 2. The green curves are the accuracy when in addition toD2, the model uses the memory from Task 1,M1 (experience replay). (Averaged over3 runs). because the joint learning with the examples of the current task T2 acts as a strong, albeit implicit and data-dependent, regularizer for T1. To validate this hypothesis, we consider the MNIST Rotations dataset [Lopez-Paz and Ranzato, 2017], where each task has digits rotated by a certain degree, a setting that enables ﬁne control over the relatedness between the tasks. The architecture is the same as for Permuted MNIST, with only 10 memory slots, one for each class of T1. First, we veriﬁed that the loss on M1 quickly drops to nearly 0 as the model is trained using both M1 and D2. As expected, the model achieves a perfect performance on the examples in the memory, which is not true for methods like A-GEM which make less direct use of the memory (see Appendix Tab. 7). We then veriﬁed that only training on M1 without D2, yields strong overﬁtting to the examples in the memory and poor generalization performance, with a mere average accuracy of 40% on T1 from the initial 85% which was obtained just after training on T1. If we only train on D2 without using M1 (same as FINETUNE baseline), we also observed overﬁtting to D2 as long as T2 and T1 are sufﬁciently unrelated, Fig. 3(b) and 3(c). When the two tasks are closely related instead (difference of rotation angles less than 20 degrees), we observe that even without the memory, generalization on T1 improves as we train on T2 because of positive transfer from the related task, see red curve in Fig. 3(a). However, when we train on both D2 and M1, generalization on T1 is better than FINETUNE baseline, i.e., training with D2 only, regardless of the degree of relatedness between the two tasks, as shown by the green curves in Fig. 3. These ﬁndings suggest that while the model essentially memorizes the examples in the memory, this does not necessarily have a detrimental effect in terms of generalization as long as such learning is performed in conjunction with the examples of T2. Moreover, there are two major axes controlling this regularizer: the number of examples in T2 and the relatedness between the tasks. The former sets the strength of the regularizer. The latter, as measured by the accuracy on T1 when training only on D2, controls its effectiveness. When T1 and T2 are closely related, Fig. 3(a), training on D2 prevents overﬁtting to M1 by providing a data-dependent regularization that, even by itself, produces positive transfer. When T1 and T2 are somewhat related, Fig. 3(b), training on D2 still improves generalization on T1 albeit to a much lesser extent. However, when the tasks are almost adversarial to each other as an upside down 2 may look like a 5, the resulting regularization becomes even harmful, Fig. 3(c). In this case, accuracy drops from 40% (training only on M1) to 30% (training on both M1 and D2). One remaining question related to generalization is how ER relates to A-GEM [Chaudhry et al., 2019] and whether A-GEM overﬁts even less? The answer is positive. As shown in Appendix Tab. 7, A-GEM’s accuracy on the memory examples does not reach 100% even after having processed 1000 samples. Interestingly, accuracy on the training set is lower than ER suggesting that the more constrained weight updates of A-GEM make it actually underﬁt. This underﬁtting prevents A-GEM from reaping the full regularization beneﬁts brought by training on the data of subsequent tasks. Conclusions In this work we studied ER methods for supervised CL tasks. Our empirical analysis on several benchmark streams of data shows that ER methods even with a tiny episodic memory offer a very large performance boost at a very marginal increase of computational cost compared to the ﬁnetuning baseline. We also studied various ways to populate the memory and proposed a hybrid approach that 8strikes a good trade-off between randomizing the examples in the memory while keeping enough representatives for each class. Our study also sheds light into a very interesting phenomenon: memorization (zero cross-entropy loss) of tiny memories is useful for generalization because training on subsequent tasks acts like a data dependent regularizer. Overall, we hope the CL community will adopt experience replay methods as a baseline, given their strong empirical performance, efﬁciency and simplicity of implementation. There are several avenues of future work. For instance, we would like to investigate what are the optimal inputs that best mitigate expected forgetting and optimal strategies to remove samples from the memory when it is entirely ﬁlled up. 9References Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018. Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. In ICLR, 2019. Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1146–1155. JMLR. org, 2017. Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efﬁcient experience replay for streaming learning. arXiv preprint arXiv:1809.05922, 2018. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS, 2014. David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. arXiv preprint arXiv:1802.10269, 2018. James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An- drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America (PNAS), 2016. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/cifar.html, 2009. Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998. Jeongtae Lee, Jaehong Yun, Sungju Hwang, and Eunho Yang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017. Sang-Woo Lee, Jin-Hwa Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In NIPS, 2017. Zhizhong Li and Derek Hoiem. Learning without forgetting. In European Conference on Computer Vision, pages 614–629, 2016. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continuum learning. In NIPS, 2017. Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018, 2018. S-V . Rebufﬁ, A. Kolesnikov, and C. H. Lampert. iCaRL: Incremental classiﬁer and representation learning. In CVPR, 2017. 10Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR, 2019. Mark B Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104, 1997. David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Greg Wayne. Experience replay for continual learning. CoRR, abs/1811.11682, 2018. Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress and compress: A scalable framework for continual learning. In International Conference in Machine Learning, 2018. Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pages 181–209. Springer, 1998. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pages 3630–3638, 2016. Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 11(1):37–57, 1985. C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 11Appendix In §A, we provide algorithms for different memory update strategies described in §4 of the main paper. The detailed results of the experiments which were used to generate Fig. 1 and Tab. 1 in the main paper are provided in §B. The analysis conducted in §5.5 of the main paper is further described in §C. Finally, in §D, we list the hyper-parameters used for each of the baselines across all the datasets. A Memory Update Algorithms Here we provide the algorithms to write into memory as discussed in §4 of the main paper. Algorithm 2 Reservoir sampling update. mem_sz is the number of examples the memory can store, tis the task id, nis the number of examples observed so far in the data stream, and Bis the input mini-batch. 1: procedure UPDATE MEMORY (mem_sz,t,n,B ) 2: j ←0 3: for (x,y) in Bdo 4: M ←|M| ⊿Number of samples currently stored in the memory 5: if M <mem_sz then 6: M.append(x,y,t ) 7: else 8: i= randint(0,n + j) 9: if i< mem_sz then 10: M[i] ←(x,y,t ) ⊿Overwrite memory slot. 11: j ←j+ 1 12: return M Algorithm 3 Ring buffer. 1: procedure UPDATE MEMORY (mem_sz,t,n,B ) 2: for (x,y) in Bdo 3: # Assume FIFO stacks M[t][y] of ﬁxed size are already initialized 4: M[t][y].append(x) 5: return M Algorithm 4 K-Means. Memory is populated using samples closest (in feature space) to sequential K-Means centroids. 1: procedure UPDATE MEMORY (mem_sz,t,n,B ) 2: # Assume array M[t][y] of ﬁxed size is already initialized 3: # Assume K centroids cj are already initialized 4: # Assume cluster counters nj are already initialized to 0 5: for (x,y) in Bdo 6: j ←argminj∈{1,···,K}||φθ(x) −cj|| 7: nj ←nj + 1 8: cj ←cj + 1 nj ∗(φθ(x) −cj) 9: d= ||φθ(x) −cj|| 10: if d< M[t][y][j].get_dst() then ⊿Store the current example if it is closer to the centroid 11: M[t][y][j] ←(x,d) 12: return M 12Algorithm 5 Mean of Features. Store examples that are closest to the running average feature vector. 1: procedure UPDATE MEMORY (mem_sz,t,n,B ) 2: # Assume heaps M[t][y] of ﬁxed size are already initialized 3: # Assume average features f[t][y] are already initialized 4: # Assume moving average decay hyper-parameter (α) is given 5: for (x,y) in Bdo 6: f[t][y] ←α∗f[t][y] + (1−α) ∗φθ(x) 7: d= ||φθ(x) −f[t][y]|| 8: if M[t][y].ﬁnd_max() >d then ⊿Store the current example if it is closer to the center 9: M[t][y].delete_max() 10: M[t][y].insert(x; d) 11: return M B Detailed Results Here we describe the detailed results used to generate the Fig. 1 in the main paper. In addition we also report the forgetting metric (3). Note that the MULTI -TASK baseline does not follow the deﬁnition of continual learning as it keeps the dataset of all the tasks around at every step. Table 3: Permuted MNIST: Performance (average accuracy (left column) and forgetting (right column)) for different number of samples per class. The average accuracy numbers from the this table are used to generate Fig. 1 in §5.4 of the main paper. Methods Episodic Memory (Samples Per Class) Average Accuracy [AT(%)] Forgetting [ FT] 1 3 5 15 1 3 5 15 A-GEM 62.1 (± 1.39) 63.2 (± 1.47) 64.1 (± 0.74) 66.0 (± 1.78) 0.21 (± 0.01) 0.20 (± 0.01) 0.19 (± 0.01) 0.17 (± 0.02) MER 69.9 (± 0.40) 74.9 (± 0.49) 78.3 (± 0.19) 81.2 (± 0.28) 0.14 (± 0.01) 0.09 (± 0.01) 0.06 (± 0.01) 0.03 (± 0.01) ER-RINGBUFFER70.2 (± 0.56) 73.5 (± 0.43) 75.8 (± 0.24) 79.4 (± 0.43) 0.12 (± 0.01) 0.09 (± 0.01) 0.07 (± 0.01) 0.04 (± 0.01) ER-MOF 69.9 (± 0.68) 73.9 (± 0.64) 75.9 (± 0.21) 79.7 (± 0.19) 0.13 (± 0.01) 0.09 (± 0.01) 0.07 (± 0.01) 0.04 (± 0.01) ER-K-MEANS 70.5 (± 0.42) 74.7 (± 0.62) 76.7 (± 0.51) 79.1 (± 0.32) 0.12 (± 0.01) 0.08 (± 0.01) 0.06 (± 0.01) 0.04 (± 0.01) ER-RESERVOIR 68.9 (± 0.89) 75.2 (± 0.33) 76.2 (± 0.38) 79.8 (± 0.26) 0.15 (± 0.01) 0.08 (± 0.01) 0.07 (± 0.01) 0.04 (± 0.01) FINETUNE 53.5 (± 1.46) - - - 0.29 (± 0.01) - - EWC 63.1 (± 1.40) - - - 0.18 (± 0.01) - - MULTI-TASK 83 - Table 4: Split CIFAR: Performance (average accuracy (left column) and forgetting (right column)) for different number of samples per class. The average accuracy numbers from the this table are used to generate Fig. 1 in §5.4 of the main paper. Methods Episodic Memory (Samples Per Class) Average Accuracy [AT(%)] Forgetting [ FT] 1 3 5 13 1 3 5 13 A-GEM 54.9 (± 2.92) 56.9 (± 3.45) 59.9 (± 2.64) 63.1 (± 1.24) 0.14 (± 0.03) 0.13 (± 0.03) 0.10 (± 0.02) 0.07 (± 0.01) MER 49.7 (± 2.97) 57.7 (± 2.59) 60.6 (± 2.09) 62.6 (± 1.48) 0.19 (± 0.03) 0.11 (± 0.01) 0.09 (± 0.02) 0.07 (± 0.01) ER-RINGBUFFER56.2 (± 1.93) 60.9 (± 1.44) 62.6 (± 1.77) 64.3 (± 1.84) 0.13 (± 0.01) 0.09 (± 0.01) 0.08 (± 0.02) 0.06 (± 0.01) ER-MOF 56.6 (± 2.09) 59.9 (± 1.25) 61.1 (± 1.62) 62.7 (± 0.63) 0.12 (± 0.01 ) 0.10 (± 0.01) 0.08 (± 0.01) 0.07 (± 0.01) ER-K-MEANS 56.6 (± 1.40) 60.1 (± 1.41) 62.2 (± 1.20) 65.2 (± 1.81) 0.13 (± 0.01) 0.09 (± 0.01) 0.07 (± 0.01) 0.04 (± 0.01) ER-RESERVOIR 53.1 (± 2.66) 59.7 (± 3.87) 65.5 (± 1.99) 68.5 (± 0.65) 0.19 (± 0.02) 0.12 (± 0.03) 0.09 (± 0.02) 0.05 (± 0.01) FINETUNE 40.6 (± 3.83) - - - 0.27 (± 0.04) - - EWC 41.2 (± 2.67) - - - 0.27 (± 0.02) - - MULTI-TASK 68.3 - 13Table 5: miniImageNet: Performance (average accuracy (left column) and forgetting (right column)) for different number of samples per class. The average accuracy numbers from the this table are used to generate Fig. 1 in §5.4 of the main paper. Methods Episodic Memory (Samples Per Class) Average Accuracy [AT(%)] Forgetting [ FT] 1 3 5 13 1 3 5 13 A-GEM 48.2 (± 2.49) 51.6 (± 2.69) 54.3 (± 1.56) 54 (± 3.63) 0.13 (± 0.02) 0.10 (± 0.02) 0.08 (± 0.01) 0.09 (± 0.03) MER 45.5 (± 1.49) 49.4 (± 3.43) 54.8 (± 1.79) 55.1 (± 2.91) 0.15 (± 0.01) 0.12 (± 0.02) 0.07 (± 0.01) 0.07 (± 0.01) ER-RINGBUFFER49.0 (± 2.61) 53.5 (± 1.42) 54.2 (± 3.23) 55.9 (± 4.05) 0.12 (± 0.02) 0.07 (± 0.02) 0.08 (± 0.02) 0.06 (± 0.03) ER-MOF 48.5 (± 1.72) 53.3 (± 2.80) 53.3 (± 3.11) 56.5 (± 1.92) 0.12 (± 0.01) 0.08 (± 0.01) 0.08 (± 0.02) 0.05 (± 0.02) ER-K-MEANS 48.5 (± 0.35) 52.3 (± 3.12) 56.6 (± 2.48) 55.1 (± 1.86) 0.12 (± 0.02) 0.09 (± 0.02) 0.06 (± 0.01) 0.06 (± 0.01) ER-RESERVOIR 44.4 (± 3.22) 50.7 (± 3.36) 56.2 (± 4.12) 61.3 (± 6.72) 0.17 (± 0.02) 0.12 (± 0.03) 0.07 (± 0.04) 0.04 (± 0.06) FINETUNE 34.7 (± 2.69) - - - 0.26 (± 0.03) - - EWC 37.7 (± 3.29) - - - 0.21 (± 0.03) - - MULTI-TASK 62.4 - Table 6: CUB: Performance (average accuracy (left column) and forgetting (right column)) for different number of samples per class. The average accuracy numbers from the this table are used to generate Fig. 1 in §5.4 of the main paper. Methods Episodic Memory (Samples Per Class) Average Accuracy [AT(%)] Forgetting [ FT] 1 3 5 10 1 3 5 10 A-GEM 62.1 (± 1.28) 62.1 (± 1.87) 63.4 (± 2.33) 62.5 (± 2.34) 0.09 (± 0.01) 0.08 (± 0.02) 0.07 (± 0.01) 0.08 (± 0.02) MER 55.4 (± 1.03) 65.3 (± 1.68) 68.1 (± 1.61) 71.1 (± 0.93) 0.10 (± 0.01) 0.04 (± 0.01) 0.03 (± 0.01) 0.03 (± 0.01) ER-RINGBUFFER65.0 (± 0.96) 71.4 (± 1.53) 73.6 (± 1.57) 75.5 (± 1.84) 0.03 (± 0.01) 0.01 (± 0.01) 0.01 (± 0.01) 0.02 (± 0.01) ER-K-MEANS 67.9 (± 0.87) 71.6 (± 1.56) 73.9 (± 2.01) 76.1 (± 1.74) 0.02 (± 0.01) 0.02 (± 0.01) 0.02 (± 0.01) 0.01 (± 0.01) ER-RESERVOIR 61.7 (± 0.62) 71.4 (± 2.57) 75.5 (± 1.92) 76.5 (± 1.56) 0.09 (± 0.01) 0.04 (± 0.01) 0.02 (± 0.01) 0.03 (± 0.02) FINETUNE 55.7 (± 2.22) - - - 0.13 (± 0.03) - - EWC 55.0 (± 2.34) - - - 0.14 (± 0.02) - - MULTI-TASK 65.6 - 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Tasks 0.6 0.7 0.8 Avg Accuracy (a) 1 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Tasks 0.6 0.7 0.8 Avg Accuracy ER-K-Means EWC FINETUNE MER A-GEM ER-MoF ER-Ringbuffer ER-Reservoir (b) 15 Samples Figure 4: MNIST: Evolution of average accuracy (Ak) as new tasks are learned when ‘1’ and ‘15’ samples per class are used. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.40 0.45 0.50 0.55 0.60 0.65 Avg Accuracy (a) 1 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.4 0.5 0.6 0.7 Avg Accuracy ER-K-Means EWC FINETUNE MER A-GEM ER-MoF ER-Ringbuffer ER-Reservoir (b) 13 Samples Figure 5: CIFAR: Evolution of average accuracy (Ak) as new tasks are learned when using ‘1’ and ‘13’ samples per class. The performance is averaged over5 runs. Uncertainty estimates are provided in Tabs 3, 4, 5, 6. 141 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.35 0.40 0.45 0.50 Avg Accuracy (a) 1 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.35 0.40 0.45 0.50 0.55 0.60 Avg Accuracy ER-K-Means EWC FINETUNE MER A-GEM ER-MoF ER-Ringbuffer ER-Reservoir (b) 13 Samples Figure 6: miniImageNet: Evolution of average accuracy (Ak) as new tasks are learned when ‘1’ and ‘13’ samples per class are used. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.55 0.60 0.65 0.70 Avg Accuracy (a) 1 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Tasks 0.5 0.6 0.7 Avg Accuracy ER-K-Means EWC FINETUNE MER A-GEM ER-Ringbuffer ER-Reservoir (b) 10 Samples Figure 7: CUB: Evolution of average accuracy (Ak) as new tasks are learned when ‘1’ and ‘10’ samples per class are used. C Further Analysis Table 7: MNIST RotationPerformance of task 1 after training on task 2. Task 2 Samples Rotation Angle 10° 90° ER-RINGBUFFER A -GEM ER -RINGBUFFER A -GEM Train Mem Test Train Mem Test Train Mem Test Train Mem Test 1000 85.6 1 86.2 81.5 86.6 82.5 68.7 1 69.4 51.7 73.3 52.1 20000 91.4 1 91.6 91.4 1 91.5 32.7 1 33.4 31.6 1 33.0 In Tab. 7, we provide train, memory and test set performance on both theER-RINGBUFFER and A-GEM with two different conﬁgurations of tasks; similar tasks (10° rotation), dissimilar tasks (90° rotation). It can be seen from the table, and as argued in the §5.5 of the main paper that ER-RINGBUFFER always achieves the perfect performance on the memory. To achieve the same effect with A-GEM one has to train for more iterations. D Hyper-parameter Selection Table 8: Hyper-parameters selection on the four benchmark datasets. ‘lr’ is the learning rate, ‘λ’ is the synaptic strength forEWC , ‘γ’ is the with in batch meta-learning rate forMER , ‘s’ is current example learning rate multiplier forMER . Methods MNIST CIFAR CUB miniImageNet FINETUNE lr (0.1) lr (0.03) lr (0.03) lr (0.03) EWC lr (0.1), λ(10) lr (0.03), λ(10) lr (0.03), λ(10) lr (0.03), λ(10) A-GEM lr (0.1) lr (0.03) lr (0.03) lr (0.03) MER lr (0.03), γ(0.1), s (10) lr (0.03), γ(0.1), s (5) lr (0.1), γ(0.1), s (5) lr (0.03), γ(0.1), s (5) ER-RESERVOIR lr (0.1) lr (0.1) lr (0.03) lr (0.1) ER-[OTHERS ] lr (0.1) lr (0.03) lr (0.03) lr (0.03) 15",
      "meta_data": {
        "arxiv_id": "1902.10486v4",
        "authors": [
          "Arslan Chaudhry",
          "Marcus Rohrbach",
          "Mohamed Elhoseiny",
          "Thalaiyasingam Ajanthan",
          "Puneet K. Dokania",
          "Philip H. S. Torr",
          "Marc'Aurelio Ranzato"
        ],
        "published_date": "2019-02-27T12:34:19Z",
        "pdf_url": "https://arxiv.org/pdf/1902.10486v4.pdf"
      }
    },
    {
      "title": "Stochastic gradient hamiltonian monte carlo",
      "abstract": "Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for\ndefining distant proposals with high acceptance probabilities in a\nMetropolis-Hastings framework, enabling more efficient exploration of the state\nspace than standard random-walk proposals. The popularity of such methods has\ngrown significantly in recent years. However, a limitation of HMC methods is\nthe required gradient computation for simulation of the Hamiltonian dynamical\nsystem-such computation is infeasible in problems involving a large sample size\nor streaming data. Instead, we must rely on a noisy gradient estimate computed\nfrom a subset of the data. In this paper, we explore the properties of such a\nstochastic gradient HMC approach. Surprisingly, the natural implementation of\nthe stochastic approximation can be arbitrarily bad. To address this problem we\nintroduce a variant that uses second-order Langevin dynamics with a friction\nterm that counteracts the effects of the noisy gradient, maintaining the\ndesired target distribution as the invariant distribution. Results on simulated\ndata validate our theory. We also provide an application of our methods to a\nclassification task using neural networks and to online Bayesian matrix\nfactorization.",
      "full_text": "Stochastic Gradient Hamiltonian Monte Carlo Tianqi Chen TQCHEN @CS.WASHINGTON .EDU Emily B. Fox EBFOX @STAT.WASHINGTON .EDU Carlos Guestrin GUESTRIN @CS.WASHINGTON .EDU MODE Lab, University of Washington, Seattle, W A. Abstract Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for deﬁning dis- tant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efﬁcient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown signiﬁcantly in recent years. However, a limitation of HMC methods is the required gradient computation for simula- tion of the Hamiltonian dynamical system—such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we ex- plore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural imple- mentation of the stochastic approximation can be arbitrarily bad. To address this problem we intro- duce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the de- sired target distribution as the invariant distribu- tion. Results on simulated data validate our the- ory. We also provide an application of our meth- ods to a classiﬁcation task using neural networks and to online Bayesian matrix factorization. 1. Introduction Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) sampling methods provide a powerful Markov chain Monte Carlo (MCMC) sampling algorithm. The methods deﬁne a Hamiltonian function in terms of the tar- get distribution from which we desire samples—the po- tential energy —and a kinetic energy term parameterized by a set of “momentum” auxiliary variables. Based on Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy- right 2014 by the author(s). simple updates to the momentum variables, one simu- lates from a Hamiltonian dynamical system that enables proposals of distant states. The target distribution is in- variant under these dynamics; in practice, a discretiza- tion of the continuous-time system is needed necessitating a Metropolis-Hastings (MH) correction, though still with high acceptance probability. Based on the attractive proper- ties of HMC in terms of rapid exploration of the state space, HMC methods have grown in popularity recently (Neal, 2010; Hoffman & Gelman, 2011; Wang et al., 2013). A limitation of HMC, however, is the necessity to com- pute the gradient of the potential energy function in or- der to simulate the Hamiltonian dynamical system. We are increasingly faced with datasets having millions to bil- lions of observations or where data come in as a stream and we need to make inferences online, such as in online advertising or recommender systems. In these ever-more- common scenarios of massive batch or streaming data, such gradient computations are infeasible since they utilize the entire dataset, and thus are not applicable to “big data” problems. Recently, in a variety of machine learning al- gorithms, we have witnessed the many successes of utiliz- ing a noisy estimate of the gradient based on a minibatch of data to scale the algorithms (Robbins & Monro, 1951; Hoffman et al., 2013; Welling & Teh, 2011). A major- ity of these developments have been in optimization-based algorithms (Robbins & Monro, 1951; Nemirovski et al., 2009), and a question is whether similar efﬁciencies can be garnered by sampling-based algorithms that maintain many desirable theoretical properties for Bayesian infer- ence. One attempt at applying such methods in a sam- pling context is the recently proposed stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011; Ahn et al., 2012; Patterson & Teh, 2013). This method builds on ﬁrst-order Langevin dynamics that do not include the crucial momentum term of HMC. In this paper, we explore the possibility of marrying the efﬁciencies in state space exploration of HMC with the big-data computational efﬁciencies of stochastic gradients. Such an algorithm would enable a large-scale and online arXiv:1402.4102v2  [stat.ME]  12 May 2014Stochastic Gradient Hamiltonian Monte Carlo Bayesian sampling algorithm with the potential to rapidly explore the posterior. As a ﬁrst cut, we consider simply applying a stochastic gradient modiﬁcation to HMC and assess the impact of the noisy gradient. We prove that the noise injected in the system by the stochastic gradient no longer leads to Hamiltonian dynamics with the desired tar- get distribution as the stationary distribution. As such, even before discretizing the dynamical system, we need to cor- rect for this effect. One can correct for the injected gradi- ent noise through an MH step, though this itself requires costly computations on the entire dataset. In practice, one might propose long simulation runs before an MH correc- tion, but this leads to low acceptance rates due to large de- viations in the Hamiltonian from the injected noise. The efﬁciency of this MH step could potentially be improved using the recent results of (Korattikara et al., 2014; Bar- denet et al., 2014). In this paper, we instead introduce a stochastic gradient HMC method with friction added to the momentum update. We assume the injected noise is Gaus- sian, appealing to the central limit theorem, and analyze the corresponding dynamics. We show that using suchsecond- order Langevin dynamicsenables us to maintain the desired target distribution as the stationary distribution. That is, the friction counteracts the effects of the injected noise. For discretized systems, we consider letting the step size tend to zero so that an MH step is not needed, giving us a sig- niﬁcant computational advantage. Empirically, we demon- strate that we have good performance even for ϵ set to a small, ﬁxed value. The theoretical computation versus ac- curacy tradeoff of this small- ϵapproach is provided in the Supplementary Material. A number of simulated experiments validate our theoretical results and demonstrate the differences between (i) exact HMC, (ii) the na¨ıve implementation of stochastic gradient HMC (simply replacing the gradient with a stochastic gra- dient), and (iii) our proposed method incorporating friction. We also compare to the ﬁrst-order Langevin dynamics of SGLD. Finally, we apply our proposed methods to a classi- ﬁcation task using Bayesian neural networks and to online Bayesian matrix factorization of a standard movie dataset. Our experimental results demonstrate the effectiveness of the proposed algorithm. 2. Hamiltonian Monte Carlo Suppose we want to sample from the posterior distribution of θgiven a set of independent observations x∈D: p(θ|D) ∝exp(−U(θ)), (1) where the potential energy function U is given by U = − ∑ x∈D log p(x|θ) −log p(θ). (2) Hamiltonian (Hybrid) Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) provides a method for proposing sam- ples of θ in a Metropolis-Hastings (MH) framework that efﬁciently explores the state space as compared to stan- dard random-walk proposals. These proposals are gener- ated from a Hamiltonian system based on introducing a set of auxiliary momentum variables, r. That is, to sample from p(θ|D), HMC considers generating samples from a joint distribution of (θ,r) deﬁned by π(θ,r) ∝exp ( −U(θ) −1 2rTM−1r ) . (3) If we simply discard the resulting r samples, the θ sam- ples have marginal distribution p(θ|D). Here, M is a mass matrix, and together with r, deﬁnes a kinetic energy term. M is often set to the identity matrix, I, but can be used to precondition the sampler when we have more information about the target distribution. The Hamiltonian function is deﬁned by H(θ,r) = U(θ) + 1 2 rTM−1r. Intuitively, H measures the total energy of a physical system with posi- tion variables θand momentum variables r. To propose samples, HMC simulates the Hamiltonian dy- namics { dθ= M−1rdt dr= −∇U(θ) dt. (4) To make Eq. (4) concrete, a common analogy in 2D is as follows (Neal, 2010). Imagine a hockey puck sliding over a frictionless ice surface of varying height. The potential energy term is based on the height of the surface at the cur- rent puck position, θ, while the kinetic energy is based on the momentum of the puck, r, and its mass, M. If the sur- face is ﬂat (∇U(θ) = 0,∀θ), the puck moves at a constant velocity. For positive slopes (∇U(θ) > 0), the kinetic en- ergy decreases as the potential energy increases until the kinetic energy is 0 ( r = 0 ). The puck then slides back down the hill increasing its kinetic energy and decreasing potential energy. Recall that in HMC, the position vari- ables are those of direct interest whereas the momentum variables are artiﬁcial constructs (auxiliary variables). Over any interval s, the Hamiltonian dynamics of Eq. (4) deﬁnes a mapping from the state at time t to the state at time t+ s. Importantly, this mapping is reversible, which is important in showing that the dynamics leave π invari- ant. Likewise, the dynamics preserve the total energy, H, so proposals are always accepted. In practice, however, we usually cannot simulate exactly from the continuous system of Eq. (4) and instead consider a discretized system. One common approach is the “leapfrog” method, which is out- lined in Alg. 1. Because of inaccuracies introduced through the discretization, an MH step must be implemented (i.e., the acceptance rate is no longer 1). However, acceptance rates still tend to be high even for proposals that can be quite far from their last state.Stochastic Gradient Hamiltonian Monte Carlo Algorithm 1: Hamiltonian Monte Carlo Input: Starting position θ(1) and step size ϵ for t= 1,2 ··· do Resample momentum r r(t) ∼N(0,M) (θ0,r0) = (θ(t),r(t)) Simulate discretization of Hamiltonian dynamics in Eq. (4): r0 ←r0 −ϵ 2 ∇U(θ0) for i= 1 to mdo θi ←θi−1 + ϵM−1ri−1 ri ←ri−1 −ϵ∇U(θi) end rm ←rm −ϵ 2 ∇U(θm) (ˆθ,ˆr) = (θm,rm) Metropolis-Hastings correction: u∼Uniform[0,1] ρ= eH(ˆθ,ˆr)−H(θ(t),r(t)) if u< min(1,ρ), then θ(t+1) = ˆθ end There have been many recent developments of HMC to make the algorithm more ﬂexible and applicable in a va- riety of settings. The “No U-Turn” sampler (Hoffman & Gelman, 2011) and the methods proposed by Wang et al. (2013) allow automatic tuning of the step size, ϵ, and num- ber of simulation steps,m. Riemann manifold HMC (Giro- lami & Calderhead, 2011) makes use of the Riemann ge- ometry to adapt the mass M, enabling the algorithm to make use of curvature information to perform more efﬁ- cient sampling. We attempt to improve HMC in an orthog- onal direction focused on computational complexity, but these adaptive HMC techniques could potentially be com- bined with our proposed methods to see further beneﬁts. 3. Stochastic Gradient HMC In this section, we study the implications of implement- ing HMC using a stochastic gradient and propose variants on the Hamiltonian dynamics that are more robust to the noise introduced by the stochastic gradient estimates. In all scenarios, instead of directly computing the costly gradient ∇U(θ) using Eq. (2), which requires examination of the entire dataset D, we consider a noisy estimate based on a minibatch ˜Dsampled uniformly at random from D: ∇˜U(θ) = −|D| |˜D| ∑ x∈˜D ∇log p(x|θ) −∇log p(θ), ˜D⊂D . (5) We assume that our observations x are independent and, appealing to the central limit theorem, approximate this noisy gradient as ∇˜U(θ) ≈∇U(θ) + N(0,V (θ)). (6) Here, V is the covariance of the stochastic gradient noise, which can depend on the current model parameters and sample size. Note that we use an abuse of notation in Eq. (6) where the addition of N(µ,Σ) denotes the intro- duction of a random variable that is distributed according to this multivariate Gaussian. As the size of ˜Dincreases, this Gaussian approximation becomes more accurate. Clearly, we want minibatches to be small to have our sought-after computational gains. Empirically, in a wide range of set- tings, simply considering a minibatch size on the order of hundreds of data points is sufﬁcient for the central limit theorem approximation to be accurate (Ahn et al., 2012). In our applications of interest, minibatches of this size still represent a signiﬁcant reduction in the computational cost of the gradient. 3.1. Na¨ıve Stochastic Gradient HMC The most straightforward approach to stochastic gradient HMC is simply to replace ∇U(θ) in Alg. 1 by ∇˜U(θ). Re- ferring to Eq. (6), this introduces noise in the momentum update, which becomes ∆r = −ϵ∇˜U(θ) = −ϵ∇U(θ) + N(0,ϵ2V). The resulting discrete time system can be viewed as an ϵ-discretization of the following continuous stochastic differential equation: { dθ= M−1rdt dr= −∇U(θ) dt+ N(0,2B(θ)dt). (7) Here, B(θ) = 1 2 ϵV(θ) is the diffusion matrix contributed by gradient noise. As with the original HMC formulation, it is useful to return to a continuous time system in order to derive properties of the approach. To gain some intuition about this setting, consider the same hockey puck analogy of Sec. 2. Here, we can imagine the puck on the same ice surface, but with some random wind blowing as well. This wind may blow the puck further away than expected. Formally, as given by Corollary 3.1 of Theorem 3.1, when Bis nonzero, π(θ,r) of Eq. (3) is no longer invariant under the dynamics described by Eq. (7). Theorem 3.1. Let pt(θ,r) be the distribution of (θ,r) at time t with dynamics governed by Eq. (7). Deﬁne the entropy of pt as h(pt) = − ∫ θ,rf(pt(θ,r))dθdr, where f(x) = xln x. Assume pt is a distribution with density and gradient vanishing at inﬁnity. Furthermore, assume the gradient vanishes faster than 1 ln pt . Then, the entropy of pt increases over time with rate ∂th(pt(θ,r)) =∫ θ,r f ′′ (pt)(∇rpt(θ,r))TB(θ)∇rpt(θ,r)dθdr. (8)Stochastic Gradient Hamiltonian Monte Carlo Eq. (8) implies that ∂th(pt(θ,r)) ≥0 since B(θ) is a pos- itive semi-deﬁnite matrix. Intuitively, Theorem 3.1 is true because the noise-free Hamiltonian dynamics preserve entropy, while the addi- tional noise term strictly increases entropy if we assume (i) B(θ) is positive deﬁnite (a reasonable assumption due to the normal full rank property of Fisher information) and (ii) ∇rpt(θ,r) ̸= 0 for all t. Then, jointly, the entropy strictly increases over time. This hints at the fact that the distribution pt tends toward a uniform distribution, which can be very far from the target distribution π. Corollary 3.1. The distribution π(θ,r) ∝exp (−H(θ,r)) is no longer invariant under the dynamics in Eq. (7). The proofs of Theorem 3.1 and Corollary 3.1 are in the Supplementary Material. Because π is no longer invariant under the dynamics of Eq. (7), we must introduce a correction step even before considering errors introduced by the discretization of the dynamical system. For the correctness of an MH step (based on the entire dataset), we appeal to the same argu- ments made for the HMC data-splitting technique of Neal (2010). This approach likewise considers minibatches of data and simulating the (continuous) Hamiltonian dynam- ics on each batch sequentially. Importantly, Neal (2010) alludes to the fact that the resulting H from the split-data scenario may be far from that of the full-data scenario af- ter simulation, which leads to lower acceptance rates and thereby reduces the apparent computational gains in simu- lation. Empirically, as we demonstrate in Fig. 2, we see that even ﬁnite-length simulations from the noisy system can diverge quite substantially from those of the noise-free sys- tem. Although the minibatch-based HMC technique con- sidered herein is slightly different from that of Neal (2010), the theory we have developed in Theorem 3.1 surrounding the high-entropy properties of the resulting invariant distri- bution of Eq. (7) provides some intuition for the observed deviations in H both in our experiments and those of Neal (2010). The poorly behaved properties of the trajectory ofHbased on simulations using noisy gradients results in a complex computation versus efﬁciency tradeoff. On one hand, it is extremely computationally intensive in large datasets to insert an MH step after just short simulation runs (where deviations in H are less pronounced and acceptance rates should be reasonable). Each of these MH steps requires a costly computation using all of the data, thus defeating the computational gains of considering noisy gradients. On the other hand, long simulation runs between MH steps can lead to very low acceptance rates. Each rejection corre- sponds to a wasted (noisy) gradient computation and simu- lation using the proposed variant of Alg. 1. One possible di- rection of future research is to consider using the recent re- sults of Korattikara et al. (2014) and Bardenet et al. (2014) that show that it is possible to do MH using a subset of data. However, we instead consider in Sec. 3.2 a straightforward modiﬁcation to the Hamiltonian dynamics that alleviates the issues of the noise introduced by stochastic gradients. In particular, our modiﬁcation allows us to again achieve the desired πas the invariant distribution of the continuous Hamiltonian dynamical system. 3.2. Stochastic Gradient HMC with Friction In Sec. 3.1, we showed that HMC with stochastic gradients requires a frequent costly MH correction step, or alterna- tively, long simulation runs with low acceptance probabili- ties. Ideally, instead, we would like to minimize the effect of the injected noise on the dynamics themselves to allevi- ate these problems. To this end, we consider a modiﬁcation to Eq. (7) that adds a “friction” term to the momentum up- date: { dθ= M−1rdt dr= −∇U(θ) dt−BM−1rdt+ N(0,2Bdt). (9) Here and throughout the remainder of the paper, we omit the dependence of B on θ for simplicity of notation. Let us again make a hockey analogy. Imagine we are now playing street hockey instead of ice hockey, which intro- duces friction from the asphalt. There is still a random wind blowing, however the friction of the surface prevents the puck from running far away. That is, the friction term BM−1rhelps decrease the energy H(θ,r), thus reducing the inﬂuence of the noise. This type of dynamical system is commonly referred to as second-order Langevin dynam- ics in physics (Wang & Uhlenbeck, 1945). Importantly, we note that the Langevin dynamics used in SGLD (Welling & Teh, 2011) are ﬁrst-order, which can be viewed as a lim- iting case of our second-order dynamics when the friction term is large. Further details on this comparison follow at the end of this section. Theorem 3.2. π(θ,r) ∝exp(−H(θ,r)) is the unique sta- tionary distribution of the dynamics described by Eq. (9). Proof. Let G = [ 0 −I I 0 ] , D = [ 0 0 0 B ] , where G is an anti-symmetric matrix, and D is the symmetric (dif- fusion) matrix. Eq. (9) can be written in the following de- composed form (Yin & Ao, 2006; Shi et al., 2012) d [ θ r ] = − [ 0 −I I B ][ ∇U(θ) M−1r ] dt+ N(0,2Ddt) = −[D+ G] ∇H(θ,r)dt+ N(0,2Ddt). The distribution evolution under this dynamical system isStochastic Gradient Hamiltonian Monte Carlo governed by a Fokker-Planck equation ∂tpt(θ,r)=∇T{[D+G] [pt(θ,r)∇H(θ,r) + ∇pt(θ,r)]}. (10) See the Supplementary Material for details. We can ver- ify that π(θ,r) is invariant under Eq. (10) by calculating[ e−H(θ,r)∇H(θ,r) + ∇e−H(θ,r)] = 0. Furthermore, due to the existence of diffusion noise, πis the unique station- ary distribution of Eq. (10). In summary, we have shown that the dynamics given by Eq. (9) have a similar invariance property to that of the original Hamiltonian dynamics of Eq. (4), even with noise present. The key was to introduce a friction term using second-order Langevin dynamics. Our revised momentum update can also be viewed as akin to partial momentum refreshment (Horowitz, 1991; Neal, 1993), which also cor- responds to second-order Langevin dynamics. Such partial momentum refreshment was shown to not greatly improve HMC in the case of noise-free gradients (Neal, 2010). However, as we have demonstrated, the idea is crucial in our stochastic gradient scenario in order to counterbalance the effect of the noisy gradients. We refer to the resulting method as stochastic gradient HMC (SGHMC). CONNECTION TO FIRST -ORDER LANGEVIN DYNAMICS As we previously discussed, the dynamics introduced in Eq. (9) relate to the ﬁrst-order Langevin dynamics used in SGLD (Welling & Teh, 2011). In particular, the dynamics of SGLD can be viewed as second-order Langevin dynam- ics with a large friction term. To intuitively demonstrate this connection, let BM−1 = 1 dt in Eq. (9). Because the friction and momentum noise terms are very large, the mo- mentum variable rchanges much faster than θ. Thus, rel- ative to the rapidly changing momentum, θcan be consid- ered as ﬁxed. We can study this case as simply: dr= −∇U(θ)dt−BM−1rdt+ N(0,2Bdt) (11) The fast evolution of r leads to a rapid convergence to the stationary distribution of Eq. (11), which is given by N(MB−1∇U(θ),M). Let us now consider a change in θ, with r ∼N(MB−1∇U(θ),M). Recalling BM−1 = 1 dt, we have dθ= −M−1∇U(θ)dt2 + N(0,2M−1dt2), (12) which exactly aligns with the dynamics of SGLD where M−1 serves as the preconditioning matrix (Welling & Teh, 2011). Intuitively, this means that when the friction is large, the dynamics do not depend on the decaying series of past gradients represented by dr, reducing to ﬁrst-order Langevin dynamics. Algorithm 2: Stochastic Gradient HMC for t= 1,2 ··· do optionally, resample momentum ras r(t) ∼N(0,M) (θ0,r0) = (θ(t),r(t)) simulate dynamics in Eq.(13): for i= 1 to mdo θi ←θi−1 + ϵtM−1ri−1 ri ←ri−1 −ϵt∇˜U(θi) −ϵtCM−1ri−1 +N(0,2(C−ˆB)ϵt) end (θ(t+1),r(t+1)) = (θm,rm), no M-H step end 3.3. Stochastic Gradient HMC in Practice In everything we have considered so far, we have assumed that we know the noise model B. Clearly, in practice this is not the case. Imagine instead that we simply have an es- timate ˆB. As will become clear, it is beneﬁcial to instead introduce a user speciﬁed friction term C ⪰ ˆB and con- sider the following dynamics    dθ= M−1rdt dr= −∇U(θ) dt−CM−1rdt +N(0,2(C−ˆB)dt) + N(0,2Bdt) (13) The resulting SGHMC algorithm is shown in Alg. 2. Note that the algorithm is purely in terms of user-speciﬁed or computable quantities. To understand our choice of dy- namics, we begin with the unrealistic scenario of perfect estimation of B. Proposition 3.1. If ˆB = B, then the dynamics of Eq. (13) yield the stationary distribution π(θ,r) ∝e−H(θ,r). Proof. The momentum update simpliﬁes to r = −∇U(θ) dt−CM−1rdt+N(0,2Cdt), with friction term CM−1 and noise term N(0,2Cdt). Noting that the proof of Theorem 3.2 only relied on a matching of noise and fric- tion, the result follows directly by using Cin place of Bin Theorem 3.2. Now consider the beneﬁt of introducing the C terms and revised dynamics in the more realistic scenario of inaccu- rate estimation of B. For example, the simplest choice is ˆB = 0 . Though the true stochastic gradient noise B is clearly non-zero, as the step sizeϵ→0, B = 1 2 ϵV goes to 0 and Cdominates. That is, the dynamics are again governed by the controllable injected noise N(0,2Cdt) and friction CM−1. It is also possible to set ˆB = 1 2 ϵˆV, where ˆV is esti- mated using empirical Fisher information as in (Ahn et al., 2012) for SGLD.Stochastic Gradient Hamiltonian Monte Carlo COMPUTATIONAL COMPLEXITY The complexity of Alg. 2 depends on the choice of M, C and ˆB, and the complexity for estimating∇˜U(θ)—denoted as g(|D|,d)—where d is the dimension of the parameter space. Assume we allow ˆB to be an arbitrary d×dpos- itive deﬁnite matrix. Using empirical Fisher information estimation of ˆB, the per-iteration complexity of this esti- mation step is O(d2|˜D|). Then, the time complexity for the (θ,r) update is O(d3), because the update is dom- inated by generating Gaussian noise with a full covari- ance matrix. In total, the per-iteration time complexity is O(d2|˜D|+ d3 + g(|˜D|,d)). In practice, we restrict all of the matrices to be diagonal when d is large, resulting in time complexity O(d|˜D|+ d+ g(|˜D|,d)). Importantly, we note that our SGHMC time complexity is the same as that of SGLD (Welling & Teh, 2011; Ahn et al., 2012) in both parameter settings. In practice, we must assume inaccurate estimation of B. For a decaying series of step sizes ϵt, an MH step is not required (Welling & Teh, 2011; Ahn et al., 2012) 1. How- ever, as the step size decreases, the efﬁciency of the sampler likewise decreases since proposals are increasingly close to their initial value. In practice, we may want to tolerate some errors in the sampling accuracy to gain efﬁciency. As in (Welling & Teh, 2011; Ahn et al., 2012) for SGLD, we consider using a small, non-zeroϵleading to some bias. We explore an analysis of the errors introduced by such ﬁnite-ϵ approximations in the Supplementary Material. CONNECTION TO SGD WITH MOMENTUM Adding a momentum term to stochastic gradient descent (SGD) is common practice. In concept, there is a clear rela- tionship between SGD with momentum and SGHMC, and here we formalize this connection. Letting v = ϵM−1r, we ﬁrst rewrite the update rule in Alg. 2 as    ∆θ= v ∆v= −ϵ2M−1∇˜U(θ) −ϵM−1Cv +N(0,2ϵ3M−1(C−ˆB)M−1). (14) Deﬁne η = ϵ2M−1, α = ϵM−1C, ˆβ = ϵM−1 ˆB. The update rule becomes { ∆θ= v ∆v= −η∇˜U(x) −αv+ N(0,2(α−ˆβ)η). (15) Comparing to an SGD with momentum method, it is clear from Eq. (15) that η corresponds to the learning rate and 1−αthe momentum term. When the noise is removed (via C = ˆB = 0 ), SGHMC naturally reduces to a stochastic 1We note that, just as in SGLD, an MH correction is not even possible because we cannot compute the probability of the reverse dynamics. −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 θ     True Distribution Standard HMC(with MH) Standard HMC(no MH) Naive stochastic gradient HMC(with MH) Naive stochastic gradient HMC(no MH) SGHMC Figure 1.Empirical distributions associated with various sam- pling algorithms relative to the true target distribution with U(θ) = −2θ2 + θ4. We compare the HMC method of Alg. 1 with and without the MH step to: (i) a naive variant that replaces the gradient with a stochastic gradient, again with and without an MH correction; (ii) the proposed SGHMC method, which does not use an MH correction. We use ∇˜U(θ) = ∇U(θ) + N(0,4) in the stochastic gradient based samplers and ϵ= 0.1 in all cases. Momentum is resampled every 50 steps in all variants of HMC. gradient method with momentum. We can use the equiv- alent update rule of Eq. (15) to run SGHMC, and borrow experience from parameter settings of SGD with momen- tum to guide our choices of SGHMC settings. For example, we can set αto a ﬁxed small number (e.g., 0.01 or 0.1), se- lect the learning rate η, and then ﬁx ˆβ = ηˆV/2. A more sophisticated strategy involves using momentum schedul- ing (Sutskever et al., 2013). We elaborate upon how to se- lect these parameters in the Supplementary Material. 4. Experiments 4.1. Simulated Scenarios To empirically explore the behavior of HMC using exact gradients relative to stochastic gradients, we conduct ex- periments on a simulated setup. As a baseline, we consider the standard HMC implementation of Alg. 1, both with and without the MH correction. We then compare to HMC with stochastic gradients, replacing ∇U in Alg. 1 with ∇˜U, and consider this proposal with and without an MH correction. Finally, we compare to our proposed SGHMC, which does not use an MH correction. Fig. 1 shows the empirical distri- butions generated by the different sampling algorithms. We see that even without an MH correction, both the HMC and SGHMC algorithms provide results close to the true dis- tribution, implying that any errors from considering non- zero ϵ are negligible. On the other hand, the results of na¨ıve stochastic gradient HMC diverge signiﬁcantly from the truth unless an MH correction is added. These ﬁnd- ings validate our theoretical results; that is, both standard HMC and SGHMC maintain πas the invariant distribution as ϵ→0 whereas na¨ıve stochastic gradient HMC does not, though this can be corrected for using a (costly) MH step.Stochastic Gradient Hamiltonian Monte Carlo −8 −6 −4 −2 0 2 4 6 8 −8 −6 −4 −2 0 2 4 6 8 θ r     Noisy Hamiltonian dynamics Noisy Hamiltonian dynamics(resample r each 50 steps) Noisy Hamiltonian dynamics with friction Hamiltonian dynamics Figure 2.Points ( θ,r) simulated from discretizations of various Hamiltonian dynamics over 15000 steps using U(θ) = 1 2 θ2 and ϵ = 0 .1. For the noisy scenarios, we replace the gradient by ∇˜U(θ) = θ+ N(0,4). We see that noisy Hamiltonian dynam- ics lead to diverging trajectories when friction is not introduced. Resampling rhelps control divergence, but the associated HMC stationary distribution is not correct, as illustrated in Fig. 1. 0 50 100 150 200 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Autocorrelation Time Average Absolute Error of Sample Covariance    SGLD SGHMC x y     −2 −1 0 1 2 3 −2 −1 0 1 2 3 SGLD SGHMC Figure 3.Contrasting sampling of a bivariate Gaussian with cor- relation using SGHMC versus SGLD. Here, U(θ) = 1 2 θTΣ−1θ, ∇˜U(θ) = Σ−1θ+ N(0,I) with Σ11 = Σ22 = 1 and correlation ρ = Σ 12 = 0 .9. Left: Mean absolute error of the covariance estimation using ten million samples versus autocorrelation time of the samples as a function of 5 step size settings. Right: First 50 samples of SGHMC and SGLD. We also consider simply simulating from the discretized Hamiltonian dynamical systems associated with the vari- ous samplers compared. In Fig. 2, we compare the result- ing trajectories and see that the path of(θ,r) from the noisy system without friction diverges signiﬁcantly. The modiﬁ- cation of the dynamical system by adding friction (corre- sponding to SGHMC) corrects this behavior. We can also correct for this divergence through periodic resampling of the momentum, though as we saw in Fig. 1, the correspond- ing MCMC algorithm (“Naive stochastic gradient HMC (no MH)”) does not yield the correct target distribution. These results conﬁrm the importance of the friction term in maintaining a well-behaved Hamiltonian and leading to the correct stationary distribution. It is known that a beneﬁt of HMC over many other MCMC algorithms is the efﬁciency in sampling from correlated distributions (Neal, 2010)—this is where the introduction of the momentum variable shines. SGHMC inherits this property. Fig. 3 compares SGHMC and SGLD (Welling & Teh, 2011) when sampling from a bivariate Gaussian with positive correlation. For each method, we examine ﬁve different settings of the initial step size on a linearly de- creasing scale and generate ten million samples. For each of these sets of samples (one set per step-size setting), we calculate the autocorrelation time 2 of the samples and the average absolute error of the resulting sample covariance. Fig. 3(a) shows the autocorrelation versus estimation error for the ﬁve settings. As we decrease the stepsize, SGLD has reasonably low estimation error but high autocorrelation time indicating an inefﬁcient sampler. In contrast, SGHMC achieves even lower estimation error at very low autocorre- lation times, from which we conclude that the sampler is in- deed efﬁciently exploring the distribution. Fig. 3(b) shows the ﬁrst 50 samples generated by the two samplers. We see that SGLD’s random-walk behavior makes it challenging to explore the tails of the distribution. The momentum vari- able associated with SGHMC instead drives the sampler to move along the distribution contours. 4.2. Bayesian Neural Networks for Classiﬁcation We also test our method on a handwritten digits classiﬁca- tion task using the MNIST dataset 3. The dataset consists of 60,000 training instances and 10,000 test instances. We randomly split a validation set containing 10,000 instances from the training data in order to select training parame- ters, and use the remaining 50,000 instances for training. For classiﬁcation, we consider a two layer Bayesian neu- ral network with 100 hidden variables using a sigmoid unit and an output layer using softmax. We tested four meth- ods: SGD, SGD with momentum, SGLD and SGHMC. For the optimization-based methods, we use the validation set to select the optimal regularizer λof network weights4. For the sampling-based methods, we take a fully Bayesian approach and place a weakly informative gamma prior on each layer’s weight regularizerλ. The sampling procedure is carried out by running SGHMC and SGLD using mini- batches of 500 training instances, then resampling hyperpa- rameters after an entire pass over the training set. We run the samplers for 800 iterations (each over the entire training dataset) and discard the initial 50 samples as burn-in. The test error as a function of MCMC or optimization iter- ation (after burn-in) is reported for each of these methods in Fig. 4. From the results, we see that SGD with mo- mentum converges faster than SGD. SGHMC also has an advantage over SGLD, converging to a low test error much more rapidly. In terms of runtime, in this case the gra- 2Autocorrelation time is deﬁned as 1 +∑∞ s=1 ρs, where ρs is the autocorrelation at lag s. 3http://yann.lecun.com/exdb/mnist/ 4We also tried MAP inference for selecting λ in the optimization-based method, but found similar performance.Stochastic Gradient Hamiltonian Monte Carlo 0 200 400 600 800 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 iteration test error     SGD SGD with momentum SGLD SGHMC Figure 4.Convergence of test error on the MNIST dataset using SGD, SGD with momentum, SGLD, and SGHMC to infer model parameters of a Bayesian neural net. dient computation used in backpropagation dominates so both have the same computational cost. The ﬁnal results of the sampling based methods are better than optimization- based methods, showing an advantage to Bayesian infer- ence in this setting, thus validating the need for scalable and efﬁcient Bayesian inference algorithms such as SGHMC. 4.3. Online Bayesian Probabilistic Matrix Factorization for Movie Recommendations Collaborative ﬁltering is an important problem in web applications. The task is to predict a user’s prefer- ence over a set of items (e.g., movies, music) and pro- duce recommendations. Probabilistic matrix factorization (PMF) (Salakhutdinov & Mnih, 2008b) has proven effec- tive for this task. Due to the sparsity in the ratings matrix (users versus items) in recommender systems, over-ﬁtting is a severe issue with Bayesian approaches providing a nat- ural solution (Salakhutdinov & Mnih, 2008a). We conduct an experiment in online Bayesian PMF on the Movielens dataset ml-1M 5. The dataset contains about 1 million ratings of 3,952 movies by 6,040 users. The num- ber of latent dimensions is set to 20. In comparing our stochastic-gradient-based approaches, we use minibatches of 4,000 ratings to update the user and item latent matri- ces. We choose a signiﬁcantly larger minibatch size in this application than that of the neural net because of the dra- matically larger parameter space associated with the PMF model. For the optimization-based approaches, the hyper- parameters are set using cross validation (again, we did not see a performance difference from considering MAP esti- mation). For the sampling-based approaches, the hyperpa- rameters are updated using a Gibbs step after every 2,000 steps of sampling model parameters. We run the sampler to generate 2,000,000 samples, with the ﬁrst 100,000 samples discarded as burn-in. We use ﬁve-fold cross validation to 5http://grouplens.org/datasets/movielens/ Table 1.Predictive RMSE estimated using 5-fold cross validation on the Movielens dataset for various approaches of inferring pa- rameters of a Bayesian probabilistic matrix factorization model. METHOD RMSE SGD 0.8538 ±0.0009 SGD WITH MOMENTUM 0.8539 ±0.0009 SGLD 0.8412 ±0.0009 SGHMC 0.8411 ±0.0011 evaluate the performance of the different methods. The results are shown in Table 1. Both SGHMC and SGLD give better prediction results than optimization-based meth- ods. In this experiment, the results for SGLD and SGHMC are very similar. We also observed that the per-iteration running time of both methods are comparable. As such, the experiment suggests that SGHMC is an effective candidate for online Bayesian PMF. 5. Conclusion Moving between modes of a distribution is one of the key challenges for MCMC-based inference algorithms. To address this problem in the large-scale or online setting, we proposed SGHMC, an efﬁcient method for generat- ing high-quality, “distant” steps in such sampling meth- ods. Our approach builds on the fundamental framework of HMC, but using stochastic estimates of the gradient to avoid the costly full gradient computation. Surprisingly, we discovered that the natural way to incorporate stochas- tic gradient estimates into HMC can lead to divergence and poor behavior both in theory and in practice. To address this challenge, we introduced second-order Langevin dy- namics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distri- bution as the invariant distribution of the continuous sys- tem. Our empirical results, both in a simulated experiment and on real data, validate our theory and demonstrate the practical value of introducing this simple modiﬁcation. A natural next step is to explore combining adaptive HMC techniques with SGHMC. More broadly, we believe that the uniﬁcation of efﬁcient optimization and sampling tech- niques, such as those described herein, will enable a signif- icant scaling of Bayesian methods. Acknowledgements This work was supported in part by the TerraSwarm Research Center sponsored by MARCO and DARPA, ONR Grant N00014- 10-1-0746, DARPA Grant FA9550-12-1-0406 negotiated by AFOSR, NSF IIS-1258741 and Intel ISTC Big Data. We also appreciate the discussions with Mark Girolami, Nick Foti, Ping Ao and Hong Qian.Stochastic Gradient Hamiltonian Monte Carlo References Ahn, S., Korattikara, A., and Welling, M. Bayesian pos- terior sampling via stochastic gradient Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning (ICML’12), pp. 1591–1598, July 2012. Bardenet, R., Doucet, A., and Holmes, C. Towards scal- ing up Markov chain Monte Carlo: An adaptive sub- sampling approach. In Proceedings of the 30th Inter- national Conference on Machine Learning (ICML’14) , volume 32, pp. 405–413, February 2014. Duane, S., Kennedy, A.D., Pendleton, B.J., and Roweth, D. Hybrid Monte Carlo. Physics Letters B, 195(2):216 – 222, 1987. Girolami, M. and Calderhead, B. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Jour- nal of the Royal Statistical Society Series B , 73(2):123– 214, 03 2011. Hoffman, M.D. and Gelman, A. The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. arXiv, 1111.4246, 2011. Hoffman, M.D., Blei, D. M., Wang, C., and Paisley, J. Stochastic variational inference. Journal of Maching Learning Research, 14(1):1303–1347, May 2013. Horowitz, A.M. A generalized guided Monte Carlo algo- rithm. Physics Letters B, 268(2):247 – 252, 1991. Korattikara, A., Chen, Y ., and Welling, M. Austerity in MCMC land: Cutting the Metropolis-Hastings budget. In Proceedings of the 30th International Conference on Machine Learning (ICML’14), volume 32, pp. 181–189, February 2014. Levin, D.A., Peres, Y ., and Wilmer, E.L. Markov Chains and Mixing Times . American Mathematical Society, 2008. Neal, R.M. Bayesian learning via stochastic dynamics. In Advances in Neural Information Processing Systems 5 (NIPS’93), pp. 475–482, 1993. Neal, R.M. MCMC using Hamiltonian dynamics. Hand- book of Markov Chain Monte Carlo, 54:113–162, 2010. Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization , 19(4): 1574–1609, January 2009. Patterson, S. and Teh, Y .W. Stochastic gradient Rieman- nian Langevin dynamics on the probability simplex. In Advances in Neural Information Processing Systems 26 (NIPS’13), pp. 3102–3110. 2013. Robbins, H. and Monro, S. A stochastic approximation method. The Annals of Mathematical Statistics , 22(3): 400–407, 09 1951. Salakhutdinov, R. and Mnih, A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning (ICML’08), pp. 880–887, 2008a. Salakhutdinov, R. and Mnih, A. Probabilistic matrix factor- ization. In Advances in Neural Information Processing Systems 20 (NIPS’08), pp. 1257–1264, 2008b. Shi, J., Chen, T., Yuan, R., Yuan, B., and Ao, P. Relation of a new interpretation of stochastic differential equations to Ito process. Journal of Statistical Physics , 148(3): 579–590, 2012. Sutskever, I., Martens, J., Dahl, G. E., and Hinton, G. E. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Con- ference on Machine Learning (ICML’13) , volume 28, pp. 1139–1147, May 2013. Wang, M.C. and Uhlenbeck, G.E. On the Theory of the Brownian Motion II. Reviews of Modern Physics, 17(2- 3):323, 1945. Wang, Z., Mohamed, S., and Nando, D. Adaptive Hamil- tonian and Riemann manifold Monte Carlo. In Proceed- ings of the 30th International Conference on Machine Learning (ICML’13), volume 28, pp. 1462–1470, May 2013. Welling, M. and Teh, Y .W. Bayesian learning via stochas- tic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML’11), pp. 681–688, June 2011. Yin, L. and Ao, P. Existence and construction of dynamical potential in nonequilibrium processes without detailed balance. Journal of Physics A: Mathematical and Gen- eral, 39(27):8593, 2006.Stochastic Gradient Hamiltonian Monte Carlo Supplementary Material A. Background on Fokker-Planck Equation The Fokker-Planck equation (FPE) associated with a given stochastic differential equation (SDE) describes the time evolution of the distribution on the random variables under the speciﬁed stochastic dynamics. For example, consider the SDE: dz= g(z)dt+ N(0,2D(z)dt), (16) where z∈Rn,g(z) ∈Rn,D(z) ∈Rn×n. The distribution of zgoverned by Eq. (16) (denoted bypt(z)), evolves under the following equation ∂tpt(z) = − n∑ i=1 ∂zi[gi(z)pt(z)]+ n∑ i=1 n∑ j=1 ∂zi∂zj[Dij(z)pt(z)]. Here gi(z) is the i-th entry of vector g(z) and Dij(z) is the (i,j) entry of the matrix D. In the dynamics considered in this paper, z= (θ,r) and D= [ 0 0 0 B(θ) ] . (17) That is, the random variables are momentumrand position θ, with noise only added to r (though dependent upon θ). The FPE can be written in the following compact form: ∂tpt(z) = −∇T[g(z)pt(z)] + ∇T[D(z)∇pt(z)], (18) where ∇T[g(z)pt(z)] = ∑n i=1 ∂zi[gi(z)pt(z)] , and ∇T[D∇pt(θ,r)] = ∑ ij ∂zi[Dij(z)∂zjpt(z)] = ∑ ij ∂zi[Dij(z)∂zjpt(z)] + ∑ ij ∂zi[(∂zjDij(z))pt(z)] = ∑ ij ∂zi∂zj[Dij(z)pt(z)]. Note that ∂zjDij(z) = 0 for all i,j, since ∂rjBij(θ) = 0 (the noise is only added torand only depends on parameter θ). B. Proof of Theorem 3.1 Let G = [ 0 −I I 0 ] and D = [ 0 0 0 B(θ) ] . The noisy Hamiltonian dynamics of Eq. (7) can be written as d [ θ r ] = − [ 0 −I I 0 ][ ∇U(θ) M−1r ] dt+ N(0,2Ddt) = −G∇H(θ,r)dt+ N(0,2Ddt). Applying Eq. (18), deﬁning g(z) = −G∇H), the corre- sponding FPE is given by ∂tpt(θ,r)=∇T[G∇H(θ,r)pt(θ,r)] + ∇T[D∇pt(θ,r)]. (19) We use z = (θ,r) to denote the joint variable of position and momentum. The entropy is deﬁned by h(pt(θ,r)) = − ∫ θ,rf(pt(θ,r))dθdr. Here f(x) = xln x is a strictly convex function deﬁned on (0,+∞). The evolution of the entropy is governed by ∂th(pt(z)) =∂t ∫ z f(pt(z))dz = − ∫ z f′(pt(z))∂tpt(z)dz = − ∫ z f′(pt(z))∇T[G∇H(z)pt(z)]dz − ∫ z f′(p)∇T[D(z)∇pt(z)]dz. The entropy evolution can be described as the sum of two parts: the noise-free Hamiltonian dynamics and the stochastic gradient noise term. The Hamiltonian dynamics part does not change the entropy, since − ∫ z f′(pt(z))∇T[G∇H(z)pt]dz = − ∫ z f′(pt(z))∇T[G∇H(z)]ptdz − ∫ z f′(pt(z))(∇pt(z))T[G∇H(z)]dz = − ∫ z (∇f(pt(z)))T[G∇H(z)]dz = ∫ z f(pt(z))∇T[G∇H(z)]dz= 0. In the second equality, we use the fact that∇T[G∇H(z)] = −∂θ∂rH+ ∂r∂θH = 0. The last equality is given by inte- gration by parts, using the assumption that the probability density vanishes at inﬁnity and f(x) →0 as x →0 such that f(pt(z))[G∇H(z)] →0 as z→∞. The contribution due to the stochastic gradient noise can be calculated as − ∫ z f′(pt(z))∇T[D(z)∇pt(z)]dz = ∫ z (f ′′ (pt(z))∇pt(z))TD(z)∇pt(z)dz = ∫ θ,r f ′′ (pt(z))(∇rpt(θ,r))TB(θ)∇rpt(θ,r)dθdr. The ﬁrst equality is again given by integration by parts, assuming that the gradient of pt vanishes at inﬁnity faster than 1 ln pt(z) . That is, f′(pt(z))∇pt(z) = (1 + ln pt(z))∇pt(z) →0 such that f′(pt(z))[D(z)∇pt(z)] → 0 as z →∞. The statement of Theorem 3.1 immediately follows. C. Proof of Corollary 3.1 Assume π(θ,r) = exp ( −H(θ,r)) /Z is invariant un- der Eq. (7) and is a well-behaved distribution such thatStochastic Gradient Hamiltonian Monte Carlo H(θ,r) →∞ as ∥θ∥,∥r∥ → ∞. Then it is straight- forward to verify that π(θ,r) and ln π(θ,r)∇π(θ,r) = 1 Z exp (−H(θ,r)) ∇H2(θ,r) vanish at inﬁnity, such that π satisﬁes the conditions of Theorem 3.1. We also have ∇rπ(θ,r) = 1 Z exp (−H(θ,r)) M−1r. Using the assump- tion that the Fisher information matrix B(θ) has full rank, and noting that f′′(p) > 0 for p > 0, from Eq. (8) of Theorem 3.1 we conclude that entropy increases over time: ∂th(pt(θ,r))|pt=π > 0. This contradicts that π is the in- variant distribution. D. FPE for Second-Order Langevin Dynamics Second-order Langevin dynamics can be described by the following equation d [ θ r ] = − [ 0 −I I B ][ ∇U(θ) M−1r ] dt+ N(0,2τDdt) = −[D+ G] ∇H(θ,r)dt+ N(0,2τDdt), (20) where τ is a temperature (usually set to 1). In this paper, we use the following compact form of the FPE to calculate the distribution evolution under Eq (20): ∂tpt(θ,r)=∇T{[D+G] [pt(θ,r)∇H(θ,r) + τ∇pt(θ,r)]}. (21) To derive this FPE, we apply Eq. (18) to Eq (20), deﬁning g(z) = −(D+ G)∇H, which yields ∂tpt(θ,r)=∇T{[D+G] [∇H(θ,r)pt(θ,r)]}+∇T [τD∇pt(θ,r)] . Using the fact that ∇T [G∇pt(θ,r)] = −∂θ∂rpt(θ,r) + ∂r∂θpt(θ,r) = 0 , we get Eq. (21). This form of the FPE allows easy veriﬁcation that the stationary distribution is given by π(θ,r) ∝e−1 τH(θ,r). In particular, if we substi- tute the target distribution into Eq. (21), we note that [ e−1 τH(θ,r)∇H(θ,r) + τ∇e−1 τH(θ,r) ] = 0 such that ∂tπ(θ,r) = 0, implying that πis indeed the sta- tionary distribution. The compact form of Eq. (21) can also be used to construct other stochastic processes with the desired invariant distri- bution. A generalization of the FPE in Eq. (21) is given by Yin & Ao (2006). The system we have discussed in this paper considers cases where G = [ 0 −I I 0 ] and D only depends on θ. In practice, however, it might be help- ful to make Gdepend on θas well. For example, to make use of the Riemann geometry of the problem, as in Giro- lami & Calderhead (2011) and Patterson & Teh (2013), by adapting Gaccording to the local curvature. For us to con- sider these more general cases, a correction term needs to be added during simulation (Shi et al., 2012). With that correction term, we still maintain the desired target distri- bution as the stationary distribution. E. Reversibility of SGHMC Dynamics The dynamics of SGHMC are not reversible in the conven- tional deﬁnition of reversibility. However, the dynamics satisfy the following property: Theorem E.1. Assume P(θt,rt|θ0,r0) is the distribution governed by dynamics in Eq. (20), i.e. P(θt,rt|θ0,r0) fol- lows Eq. (21), then for π(θ,r) ∝exp(−H(θ,r)), π(θ0,r0)P(θt,rt|θ0,r0) = π(θt,−rt)P(θ0,−r0|θt,−rt). (22) Proof. Assuming π is the stationary distribution and P∗ the reverse-time Markov process associated with P: π(θ0,r0)P(θt,rt|θ0,r0) = π(θt,rt)P∗(θ0,r0|θt,rt). Let L(p) = ∇T{[D+ G] [p∇H(θ,r) + τ∇p]}be the genera- tor of Markov process described by Eq. (21). The generator of the reverse process is given by L∗, which is the adjoint operator of Lin the inner-product space l2(π), with inner- product deﬁned by ⟨p,q⟩π = Ex∼π(x)[p(x)q(x)]. We can verify that L∗(p) = ∇T{[D −G] [p∇H(θ,r) + τ∇p]}. The corresponding SDE of the reverse process is given by d [ θ r ] = [D−G] ∇H(θ,r) + N(0,2τDdt), which is equivalent to d [ θ −r ] = [D+ G] ∇H(θ,−r) + N(0,2τDdt). This means P∗(θ0,r0|θt,rt) = P(θ0,−r0|θt,−rt). Re- calling that we assume Gaussian momentum, r, centered about 0, we also have π(θ,r) = π(θ,−r). Together, we then have π(θ0,r0)P(θt,rt|θ0,r0) = π(θt,rt)P∗(θ0,r0|θt,rt) = π(θt,−rt)P(θ0,−r0|θt,−rt). Theorem E.1 is not strictly detailed balance by the conven- tional deﬁnition since L∗ ̸= Land P∗ ̸= P. However, it can be viewed as a kind of time reversibility. When we re- verse time, the sign of speed needs to be reversed to allow backward travel. This property is shared by the noise-free HMC dynamics of (Neal, 2010). Detailed balance can be enforced by the symmetry ofrduring the re-sampling step. However, we note that we do not rely on detailed balance to have πbe the stationary distribution of our noisy Hamil- tonian with friction (see Eq. (9)). F. Convergence Analysis In the paper, we have discussed that the efﬁciency of SGHMC decreases as the step size ϵ decreases. In prac- tice, we usually want to trade a small amount of error forStochastic Gradient Hamiltonian Monte Carlo efﬁciency. In the case of SGHMC, we are interested in a small, nonzero ϵ and fast approximation of B given by ˆB. In this case, even under the continuous dynamics, the sampling procedure contains error that relates to ϵ due to inaccurate estimation of B with ˆB. In this section, we in- vestigate how the choice of ϵcan be related to the error in the ﬁnal stationary distribution. The sampling procedure with inaccurate estimation of B can be described with the following dynamics { dθ=M−1rdt dr=−∇U(θ) dt−CM−1rdt+ N(0,2(C+ δS)dt). Here, δS = B−ˆBis the error term that is not considered by the sampling algorithm. Assume the setting where ˆB = 0, then we can let δ= ϵand S = 1 2 V. Let ˜πbe the stationary distribution of the dynamics. In the special case when V = C, we can calculate ˜πexactly by ˜π(θ,r) ∝exp ( − 1 1 + δH(θ,r) ) . (23) This indicates that for small ϵ, our stationary distribution is indeed close to the true stationary distribution. In general case, we consider the FPE of the distribution of this SDE, given by ∂t˜pt(θ,r)= [L+ δS]˜pt(θ,r). (24) Here, L(p) = ∇T{[D+G] [p∇H(θ,r) + ∇p]}is the oper- ator corresponds to correct sampling process. Let the oper- ator S(p) = ∇r[S∇rp] correspond to the error term intro- duced by inaccurate ˆB. Let us consider the χ2-divergence deﬁned by χ2(p,π) = Ex∼π [(p(x) −π(x))2 π2(x) ] = Ex∼π [p2(x) π2(x) ] −1, which provides a measure of distance between the distribu- tion pand the true distribution π. Theorem F.1 shows that the χ2-divergence decreases as δbecomes smaller. Theorem F.1. Assume ptevolves according to∂tpt = Lpt, and satisﬁes the following mixing rate λwith respect to χ2 divergence at ˜π: ∂tχ2(pt,π)|pt=˜π ≤ −λχ2(˜π,π). Fur- ther assume the process governed by S(∂tqt = Sqt) has bounded divergence change |∂tχ2(qt,π)|<c. Then ˜πsat- isﬁes χ2(˜π,π) < δc λ. (25) Proof. Consider the divergence change of ˜p governed by Eq.(24). It can be decomposed into two components, the change of divergence due to L, and the change of diver- gence due to δS ∂tχ2(˜pt,π) =Ex∼π [ ˜p(x) π2(x)[L+ δS]˜pt(x) ] =Ex∼π [˜pt(x) π2(x)L˜pt(x) ] + δEx∼π [ ˜p(x) π2(x)S˜pt(x) ] =∂tχ2(pt,π)|pt=˜pt + δ∂tχ2(qt,π)|qt=˜pt. We then evaluate the above equation at the station- ary distribution of the inaccurate dynamics ˜π. Since ∂tχ2(˜pt,π)|˜p=˜π = 0, we have λχ2(˜π,π) = δ ⏐⏐(∂tχ2(qt,π)|qt=˜π) ⏐⏐<δc. This theorem can also be used to measure the error in SGLD, and justiﬁes the use of small ﬁnite step sizes in SGLD. We should note that the mixing rate bound λat ˜π exists for SGLD and can be obtained using spectral anal- ysis (Levin et al., 2008), but the corresponding bounds for SGHMC are unclear due to the irreversibility of the pro- cess. We leave this for future work. Our proof relies on a contraction bound relating the error in the transition distribution to the error in the ﬁnal sta- tionary distribution. Although our argument is based on a continuous-time Markov process, we should note that a similar guarantee can also be proven in terms of a discrete- time Markov transition kernel. We refer the reader to (Ko- rattikara et al., 2014) and (Bardenet et al., 2014) for further details. G. Setting SGHMC Parameters As we discussed in Sec. 3.3, we can connect SGHMC with SGD with momentum by rewriting the dynamics as (see Eq.(15)) { ∆θ= v ∆v= −η∇˜U(x) −αv+ N(0,2(α−ˆβ)η). In analogy to SGD with momentum, we call ηthe learning rate and 1 −αthe momentum term. This equivalent update rule is cleaner and we recommend parameterizing SGHMC in this form. The ˆβ term corresponds to the estimation of noise that comes from the gradient. One simple choice is to ignore the gradient noise by setting ˆβ = 0 and relying on small ϵ. We can also set ˆβ = ηˆV/2, where ˆV is estimated using empirical Fisher information as in (Ahn et al., 2012). There are then three parameters: the learning rate η, mo- mentum decay α, and minibatch size |˜D|. Deﬁne β = ϵM−1B = 1 2 ηV(θ) to be the exact term induced by in- troduction of the stochastic gradient. Then, we have β = O ( η|D| |˜D| I ) , (26) where Iis ﬁsher information matrix of the gradient, |D|is size of training data, |˜D|is size of minibatch, and ηis our learning rate. We want to keep βsmall so that the resulting dynamics are governed by the user-controlled term and theStochastic Gradient Hamiltonian Monte Carlo sampling algorithm has a stationary distribution close to the target distribution. From Eq. (26), we see that there is no free lunch here: as the training size gets bigger, we can either set a small learning rate η = O( 1 |D|) or use a bigger minibatch size |˜D|. In practice, choosing η= O( 1 |D|) gives better numerical stability, since we also need to multiply η by ∇˜U, the mean of the stochastic gradient. Large η can cause divergence, especially when we are not close to the mode of distribution. We note that the same discussion holds for SGLD (Welling & Teh, 2011). In practice, we ﬁnd that using a minibatch size of hundreds (e.g |˜D|= 500) and ﬁxing αto a small number (e.g. 0.01 or 0.1) works well. The learning rate can be set as η = γ/|D|, where γis the “per-batch learning rate”, usually set to 0.1 or 0.01. This method of setting parameters is also commonly used for SGD with momentum (Sutskever et al., 2013). H. Experimental Setup H.1. Bayesian Neural Network The Bayesian neural network model used in Sec. 4.2 can be described by the following equation: P(y= i|x) ∝exp ( AT i σ(BTx+ b) + ai ) . (27) Here, y∈{1,2,··· ,10}is the output label of a digit. A∈ R10×100 contains the weight for output layers and we use Ai to indicate i-th column of A. B ∈Rd×100 contains the weight for the ﬁrst layer. We also introduce a ∈R10 and b∈R100 as bias terms in the model. In the MNIST dataset, the input dimension d = 784. We place a Gaussian prior on the model parameters P(A) ∝exp(−λA∥A∥2),P(B) ∝exp(−λB∥B∥2) P(a) ∝exp(−λa∥a∥2),P(b) ∝exp(−λb∥a∥2). We further place gamma priors on each of the precision terms λ: λA,λB,λa,λb i.i.d. ∼ Γ(α,β). We simply set α and β to 1 since the results are usually insensitive to these parameters. We generate samples from the posterior distribution P(Θ|D) ∝ ∏ y,x∈D P(y|x,Θ)P(Θ), (28) where parameter set Θ = {A,B,a,b,λ A,λB,λa,λb}. The sampling procedure is carried out by alternating the following steps: • Sample weights fromP(A,B,a,b |λA,λB,λa,λb,D) using SGHMC or SGLD with minibatch of 500 in- stances. Sample for 100 steps before updating hyper- parameters. • Sample λ from P(λA,λB,λa,λb|A,B,a,b ) using a Gibbs step. Note that the posterior for λis a gamma distribution by conditional conjugacy. We used the validation set to select parameters for the various methods we compare. Speciﬁcally, for SGD and SGLD, we tried step-sizes ϵ∈{0.1,0.2,0.4,0.8}×10−4, and the best settings were found to be ϵ = 0 .1 ×10−4 for SGD and ϵ = 0 .2 ×10−4 for SGLD. We then fur- ther tested ϵ = 0 .16 ×10−4 and ϵ = 0 .06 ×10−4 for SGD, and found ϵ = 0 .16 ×10−4 gave the best result, thus we used this setting for SGD. For SGD with mo- mentum and SGHMC, we ﬁxed α = 0 .01 and ˆβ = 0 , and tried η ∈ {0.1,0.2,0.4,0.8}× 10−5. The best set- tings were η = 0.4 ×10−5 for SGD with momentum, and η = 0.2 ×10−5 for SGHMC. For the optimization-based methods, we use tried regularizer λ ∈{0,0.1,1,10,100}, and λ= 1 was found to give the best performance. H.2. Online Bayesian Probabilistic Matrix Factorization The Bayesian probabilistic matrix factorization (BPMF) model used in Sec. 4.3 can be described as: λU,λV,λa,λb i.i.d. ∼ Gamma(1,1) Uki ∼N(0,λ−1 U ),Vkj ∼N(0,λ−1 V ), ai ∼N(0,λ−1 a ),bi ∼N(0,λ−1 b ) Yij|U,V ∼N(UT i Vj + ai + bj,τ−1). (29) The Ui ∈Rd and Vj ∈Rd are latent vectors for user i and movie j, while ai and bj are bias terms. We use a slightly simpliﬁed model than the BPMF model considered in (Salakhutdinov & Mnih, 2008a), where we only place priors on precision variables λ= {λU,λV,λa,λb}. How- ever, the model still beneﬁts from Bayesian inference by integrating over the uncertainty in the crucial regulariza- tion parameter λ. We generate samples from the posterior distribution P(Θ|Y) ∝P(Y|Θ)P(Θ), (30) with the parameter set Θ = {U,V,a,b,λ U,λV,λa,λb}. The sampling procedure is carried out by alternating the followings • Sample weights from P(U,V,a,b |λU,λV,λa,λb,Y ) using SGHMC or SGLD with a minibatch size of 4,000 ratings. Sample for 2,000 steps before updat- ing the hyper-parameters. • Sample λ from P(λU,λV,λa,λb|U,V,a,b ) using a Gibbs step. The training parameters for this experiment were directly selected using cross-validation. Speciﬁcally, for SGD andStochastic Gradient Hamiltonian Monte Carlo SGLD, we tried step-sizes ϵ ∈ {0.1,0.2,0.4,0.8,1.6}× 10−5, and the best settings were found to beϵ= 0.4×10−5 for SGD and ϵ = 0 .8 ×10−5 for SGLD. For SGD with momentum and SGHMC, we ﬁxed α = 0.05 and ˆβ = 0, and tried η∈{0.1,0.2,0.4,0.8}×10−6. The best settings were η = 0.4 ×10−6 for SGD with momentum, and η = 0.4 ×10−6 for SGHMC.",
      "meta_data": {
        "arxiv_id": "1402.4102v2",
        "authors": [
          "Tianqi Chen",
          "Emily B. Fox",
          "Carlos Guestrin"
        ],
        "published_date": "2014-02-17T19:57:59Z",
        "pdf_url": "https://arxiv.org/pdf/1402.4102v2.pdf"
      }
    },
    {
      "title": "Svgd as a kernelized wasserstein gradient ﬂow of the chi-squared divergence",
      "abstract": "Stein Variational Gradient Descent (SVGD), a popular sampling algorithm, is\noften described as the kernelized gradient flow for the Kullback-Leibler\ndivergence in the geometry of optimal transport. We introduce a new perspective\non SVGD that instead views SVGD as the (kernelized) gradient flow of the\nchi-squared divergence which, we show, exhibits a strong form of uniform\nexponential ergodicity under conditions as weak as a Poincar\\'e inequality.\nThis perspective leads us to propose an alternative to SVGD, called Laplacian\nAdjusted Wasserstein Gradient Descent (LAWGD), that can be implemented from the\nspectral decomposition of the Laplacian operator associated with the target\ndensity. We show that LAWGD exhibits strong convergence guarantees and good\npractical performance.",
      "full_text": "SVGD as a kernelized Wasserstein gradient ﬂow of the chi-squared divergence Sinho Chewi schewi@mit.edu Thibaut Le Gouic tlegouic@mit.edu Chen Lu chenl819@mit.edu Tyler Maunu maunut@mit.edu Philippe Rigollet rigollet@mit.edu Abstract. Stein Variational Gradient Descent (SVGD), a popular sam- pling algorithm, is often described as the kernelized gradient ﬂow for the Kullback-Leibler divergence in the geometry of optimal transport. We introduce a new perspective on SVGD that instead views SVGD as the (kernelized) gradient ﬂow of the chi-squared divergence which, we show, exhibits a strong form of uniform exponential ergodicity under conditions as weak as a Poincar´ e inequality. This perspective leads us to propose an alternative to SVGD, called Laplacian Adjusted Wasserstein Gradient Descent (LAWGD), that can be implemented from the spec- tral decomposition of the Laplacian operator associated with the target density. We show that LAWGD exhibits strong convergence guarantees and good practical performance. 1. INTRODUCTION The seminal paper of Jordan, Kinderlehrer, and Otto [JKO98] has profoundly reshaped our understanding of sampling algorithms. What is now commonly known as theJKO scheme interprets the evolution of marginal distributions of a Langevin diﬀusion as a gradient ﬂow of a Kullback- Leibler (KL) divergence over the Wasserstein space of probability measures. This optimization perspective on Markov Chain Monte Carlo (MCMC) has not only renewed our understanding of algorithms based on Langevin diﬀusions [Dal17a,Ber18,CB18,Wib18,DMM19,VW19], but has also fueled the discovery of new MCMC algorithms inspired by the diverse and powerful optimization toolbox [MWBG12, SBCR16, CCBJ18, Ber18, HKRC18, Wib18, MCC+19, Wib19, CLL+20, DRD20, ZPFP20]. The Unadjusted Langevin Algorithm (ULA) [Dal17b,DM17] is the most common discretization of the Wasserstein gradient ﬂow for the KL divergence, but it is unclear whether it is the most eﬀective one. In fact, ULA is asymptotically biased, which results in slow convergence and often requires ad-hoc adjustments [DCWY19]. To overcome this limitation, various methods that track the Wasserstein gradient ﬂow more closely have been recently developed [Ber18,Wib18,SKL20]. Let F denote a functional over the Wasserstein space of distributions. The Wasserstein gradient 1 arXiv:2006.02509v1  [math.ST]  3 Jun 20202 CHEWI ET AL. ﬂow of F may be described as the deterministic and time-inhomogeneous Markov process ( Xt)t≥0 started at a random variable X0 ∼µ0 and evolving according to ˙Xt = −[∇W2F(µt)](Xt), where µt denotes the distribution of Xt. Here [ ∇W2F(µ)](·) : Rd →Rd is the Wasserstein gradient of F at µ. If F(µ) = DKL(µ∥π), where π ∝e−V is a given target distribution on Rd, it is known [AGS08, Vil09, San17] that ∇W2F(µ) = ∇ln(dµ/dπ). Therefore, a natural discretization of the Wasserstein gradient ﬂow with step size h> 0, albeit one that cannot be implemented since it depends on the distribution µt of Xt, is: Xt+1 = Xt −h∇ln (dµt dπ(Xt) ) , t = 0,1,2,... . While µt can, in principle, be estimated by evolving a large number of particles X[1] t ,...,X [N] t , estimation of µt is hindered by the curse of dimensionality and this approach still faces signiﬁcant computational challenges despite attempts to improve the original JKO scheme [SKL20,WL20]. A major advance in this direction was achieved by allowing for approximate Wasserstein gradi- ents. More speciﬁcally, Stein Variational Gradient Descent (SVGD), recently proposed by [LW16] (see Section 2 for more details), consists in replacing ∇W2F(µ) by its image Kµ∇W2F(µ) under the integral operator Kµ : L2(µ) →L2(µ) associated to a chosen kernel K : Rd ×Rd →R and deﬁned by Kµf(x) := ∫ K(x,y)f(y) dµ(y) for f ∈L2(µ). This leads to the following process: ˙Xt = −[Kµt∇W2F(µt)](Xt) , (SVGDp) where we apply the integral operatorKµt individually to each coordinate of the Wasserstein gradient. In turn, this kernelization trick overcomes most of the above computational bottleneck. Building on this perspective, [DNS19] introduced a new geometry, diﬀerent from the Wasserstein geometry and which they call the Stein geometry, in which ( SVGDp) becomes the gradient ﬂow of the KL divergence. However, despite this recent advance, the theoretical properties of SVGD as a sampling algorithm as well as guidelines for the choice of the kernel K are still largely unexplored. In this work, we revisit the above view of SVGD as a kernelized gradient ﬂow of the KL divergence over Wasserstein space that was put forward in [Liu17]. Our contributions. We introduce, in Section 2.3, a new perspective on SVGD by viewing it as kernelized gradient ﬂow of the chi-squared divergence rather than the KL divergence. This per- spective is fruitful in two ways. First, it uses a single integral operator Kπ—as opposed to (SVGDp), which requires a family of integral operators Kµ, µ ≪π—providing a conceptually clear guide- line for choosing K, namely: K should be chosen to make Kπ approximately equal to the identity operator. Second, under the idealized choice Kπ = id, we show that this gradient ﬂow converges exponentially fast in KL divergence as soon as the target distribution π satisﬁes a Poincar´ e in- equality. In fact, our results are stronger than exponential convergence and they highlight strong uniform ergodicity : the gradient ﬂow forgets the initial distribution after a ﬁnite time that is at most half of the Poincar´ e constant. To establish this exponential convergence under a relatively weak condition (Poincar´ e inequality), we employ the following technique. While the gradient ﬂow aims at minimizing the chi-squared divergence by following the curve in Wasserstein space with steepest descent, we do not track its progress with the objective function itself, the chi-squared divergence, but instead we track it with the KL divergence. This is in a sense dual to argument employed in [CLL+20], where the chi-squared divergence is used to track the progress of a gradientSVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 3 ﬂow on the KL divergence. A more standard analysis relying on / Lojasiewicz inequalities also yields rates of convergence on the chi-squared divergence under stronger assumptions such as a log-Sobolev inequality, and log-concavity. These results establish the ﬁrst ﬁnite-time theoretical guarantees for SVGD in an idealized setting. 4  2  0 2 4 4 2 0 2 4 LAWGD Init. Fig 1 . Sampling from a mixture of two 2D Gaussians withLAWGD. See Appendix C. Beyond providing a better understanding of SVGD, our novel perspective is instrumental in the development of a new sampling algorithm, which we call Laplacian Adjusted Wasserstein Gradient Descent ( LAWGD) and present in Section 4. Although LAWGD is challenging to implement in high dimensions, we show that it possesses a striking theoretical property: assuming that the tar- get distribution π satisﬁes a Poincar´ e inequality, LAWGD con- verges exponentially fast, with no dependence on the Poincar´ e constant. This scale invariance has been recently demonstrated for the Newton-Langevin diﬀusion [CLL+20], but under the additional assumption that π is log-concave. A successful implementation of LAWGD hinges on the spectral decomposition of a certain diﬀer- ential operator which is within reach of modern PDE solvers. As a proof of concept, we show that LAWGD performs well in one or two dimensions using a na¨ ıve ﬁnite diﬀerences method and leave the question of applying more sophisticated numerical solvers open for future research. Related work. Since its introduction in [LW16], a number of variants of SVGD have been considered. They include a stochastic version [LLL +19], a version that approximates the Newton direction in Wasserstein space [DCM +18], a version that uses matrix kernels [WTBL19], an ac- celerated version [LZC +19], and a hybrid with Langevin [ZZCC18]. Several works have studied theoretical properties of SVGD, including its interpretation as a gradient ﬂow under a modiﬁed geometry [Liu17,DNS19], and its asymptotic convergence [LLN19]. Notation. In this paper, all probability measures are assumed to have densities w.r.t. Lebesgue measure; therefore, we frequently abuse notation by identifying a probability measure with its Lebesgue density. For a diﬀerentiable kernel K : Rd ×Rd →R, we denote by ∇1K : Rd ×Rd →Rd (resp. ∇2K) the gradient of the kernel w.r.t. the ﬁrst (resp. second) argument. When describing particle algorithms, we use a subscript to denote the time index and brackets to denote the particle index, i.e., X[i] t refers to the ith particle at time (or iteration number) t. 2. SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 2.1 Wasserstein gradient ﬂows In this section, we review the theory of gradient ﬂows on the space P2,ac(Rd) of probability measures absolutely continuous w.r.t. Lebesgue measure and possessing a ﬁnite second moment, equipped with the 2-Wasserstein metric W2. We refer readers to [Vil03,San15,San17] for introduc- tory treatments of optimal transport, and to [AGS08,Vil09] for detailed treatments of Wasserstein gradient ﬂows. Let F : P2,ac(Rd) →R ∪{∞} be a functional deﬁned on Wasserstein space. We say that a curve4 CHEWI ET AL. (µt)t≥0 of probability measures is a Wasserstein gradient ﬂow for the functional F if it satisﬁes ∂tµt = div ( µt∇W2F(µt) ) (2.1) in a weak sense. Here, ∇W2F(µ) := ∇δF(µ) is the Wasserstein gradient of the functional F at µ, where δF(µ) : Rd →R is the ﬁrst variation of F at µ, deﬁned by lim ε→0 F(µ+ εξ) −F(µ) ε = ∫ δF(µ) dξ, for all ξ with ∫ dξ= 0, and ∇ denotes the usual (Euclidean) gradient. Hence, the Wasserstein gradient, at each µ ∈ P2,ac(Rd), is a map from Rd to Rd. Using the continuity equation, we can give an Eulerian interpretation to the evolution equa- tion (2.1) (see [San15, §4] and [AGS08, §8]). Given a family of vector ﬁelds ( vt)t≥0, let ( Xt)t≥0 be a curve in Rd with random initial point X0 ∼µ0, and such that ( Xt)t≥0 is an integral curve of the vector ﬁelds ( vt)t≥0, that is, ˙Xt = vt(Xt). If we let µt denote the law of Xt, then ( µt)t≥0 evolves according to the continuity equation ∂tµt = −div(µtvt). (2.2) Comparing (2.1) and (2.2), we see that (2.1) describes the evolution of the marginal law ( µt)t≥0 of the curve (Xt)t≥0 with X0 ∼µ0 and ˙Xt = −[∇W2F(µt)](Xt). Wasserstein calculus provides the following (formal) calculation rule: the Wasserstein gradient ﬂow (µt)t≥0 for the functional F dissipates F at the rate ∂tF(µt) = −Eµt[∥∇W2F(µt)∥2]. More generally, for a curve (µt)t≥0 evolving according to the continuity equation (2.2), the time-derivative of F is given by ∂tF(µt) = Eµt⟨∇W2F(µt),vt⟩. In this paper, we are primarily concerned with two functionals: the Kullback-Leibler (KL) di- vergence DKL(·∥ π), and the chi-squared divergence χ2(·∥ π) (see, e.g., [Tsy09]). It is a standard exercise [AGS08,San15] to check that Wasserstein gradients of these functionals are, respectively, ( ∇W2DKL(·∥π) ) (µ) = ∇ln dµ dπ, ( ∇W2χ2(·∥π) ) (µ) = 2∇dµ dπ. (2.3) 2.2 SVGD as a kernelized gradient ﬂow of the KL divergence SVGD1 is achieved by replacing the Wasserstein gradient ∇ln(dµt/dπ) of the KL divergence with Kµt∇ln(dµt/dπ), leading to the particle evolution equation ( SVGDp). Recalling that π∝e−V, we get Kµt∇ln dµt dπ(x) := ∫ K(x,·)∇ln dµt dπ dµt = ∫ K(x,·)∇V dµt − ∫ ∇2K(x,·) dµt, (2.4) where, in the second identity, we used integration by parts. This expression shows that rather than having to estimate the distribution µt, it is suﬃcient to estimate the expectation ∫ ∇2K(x,·) dµt. 1Throughout this paper, we call SVGD the generalization of the original method of [LW16,Liu17] that was intro- duced in [WTBL19].SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 5 This is the key to the computational tractability of SVGD. Indeed, the kernelized gradient ﬂow can implemented by drawing N particles X[1] 0 ,...,X [N] 0 i.i.d.∼ µ0 and following the coupled dynamics ˙X[i] t = −Kµt∇ln dµt dπ(X[i] t ) = − ∫ K(X[i] t ,·)∇V dµt + ∫ ∇2K(X[i] t ,·) dµt, i ∈[N]. With this, we can simply estimate the expectation with respect to µt with an average over all particles. Discretizing the resulting process in time, we obtain the SVGD algorithm: X[i] t+1 = X[i] t −h N N∑ j=1 K(X[i] t ,X[j] t )∇V(X[j] t ) + h N N∑ j=1 ∇2K(X[i] t ,X[j] t ), i ∈[N]. (2.5) 2.3 SVGD as a kernelized gradient ﬂow of the chi-squared divergence Recall from Section 2.1 that by the continuity equation, the particle evolution equation (SVGDp) translates into the following PDE that describes the evolution of the distribution µt of Xt: ∂tµt = div ( µtKµt∇ln dµt dπ ) . (SVGDd) We make the simple observation that Kµt∇ln dµt dπ(x) = ∫ K(x,y)∇ln dµt dπ(y) dµt(y) = ∫ K(x,y)∇dµt dπ(y) dπ(y) = Kπ∇dµt dπ(x). Thus, the continuous-dynamics of SVGD, as given in ( SVGDd), can equivalently be expressed as ∂tµt = div ( µtKπ∇dµt dπ ) . (SVGD) To interpret this equation, we recall that the Wasserstein gradient of the chi-squared divergence χ2(·∥π) at µ is 2∇(dµ/dπ) (by (2.3)), so the gradient ﬂow for the chi-squared divergence is ∂tµt = 2 div ( µt∇dµt dπ ) . (CSF) Comparing (SVGD) and ( CSF), we see that (up to a factor of 2), SVGD can be understood as the ﬂow obtained by replacing the gradient of the chi-squared divergence, ∇(dµ/dπ), by Kπ∇(dµ/dπ). Although (SVGDd) and ( SVGD) are equivalent ways of expressing the same dynamics, the for- mulation of ( SVGD) presents a signiﬁcant advantage: it involves a kernel integral operator Kπ that does not change with time and depends only on the target distribution π. 3. CHI-SQUARED GRADIENT FLOW In this section, study the idealized case where Kπ taken to be the identity operator. In this case, (SVGD) reduces to the gradient ﬂow CSF. The existence, uniqueness, and regularity of this ﬂow are studied in [OT11,OT13] and [AGS08, Theorem 11.2.1]. The rate of convergence of the gradient ﬂow of the KL divergence is closely related to two functional inequalities: the Poincar´ e inequality controls the rate of exponential convergence in chi- squared divergence ([Pav14, Theorem 4.4], [CLL +20]) while a log-Sobolev inequality characterizes6 CHEWI ET AL. the rate of exponential convergence of the KL divergence [BGL14, Theorem 5.2.1]. In this section, we show that these inequalities also guarantee exponential rates of convergence of CSF. Recall that π satisﬁes a Poincar´ e inequalitywith constant CP if varπf ≤CP Eπ[∥∇f∥2], for all locally Lipschitz f ∈L2(π), (P) while π satisﬁes a log-Sobolev inequality with constant CLSI entπ(f2) := Eπ[f2 ln(f2)] −Eπ[f2] lnEπ[f2] ≤2CLSI Eπ[∥∇f∥2] ( LSI) for all locally Lipschitz f for which entπ(f2) <∞. We brieﬂy review some facts regarding the strength of these assumptions. It is standard that the log-Sobolev inequality is stronger than the Poincar´ e inequality: (LSI) implies (P) with constant CP ≤CLSI. In turn, if πis α-strongly log-concave, i.e. ∇2V ⪰αId, then it implies the validity of (LSI) with CLSI ≤1/α, and thus a Poincar´ e inequality holds too. However, a Poincar´ e inequality is in general much weaker than strong log-concavity. For instance, if λ2 π denotes the largest eigenvalue of the covariance matrix of π, then it is currently known that π satisﬁes a Poincar´ e inequality as soon as it is log-concave, with CP ≤C(d)λ2 π, where C(d) is a dimensional constant [Bob99,AGB15,LV17], and the well-known Kannan-Lov´ asz-Simonovitz(KLS) conjecture [KLS95] asserts that C(d) does not actually depend on the dimension. Our ﬁrst result shows that a Poincar´ e inequality suﬃces to establish exponential decay of the KL divergence along CSF. In fact, we establish a remarkable property, which we call strong uniform ergodicity: under a Poincar´ e inequality,CSF forgets its initial distribution after a time of no more than CP/2. Uniform ergodicity is central in the theory of Markov processes [MT09, Ch. 16] but is often limited to compact state spaces. Moreover, this theory largely focuses on total variation, so the distance from the initial distribution to the target distribution is trivially bounded by 1. Theorem 1. Assume that π satisﬁes a Poincar´ e inequality(P) with constant CP > 0 and let (µt)t≥0 denote the law of CSF. Assume that χ2(µ0 ∥π) <∞. Then, DKL(µt ∥π) ≤DKL(µ0 ∥π) e −2t CP , ∀t≥0. (3.1) In fact, a stronger convergence result holds: DKL(µt ∥π) ≤ ( DKL(µ0 ∥π) ∧2 ) e −2t CP , ∀t≥CP 2 . (3.2) Proof. Given the Wasserstein gradients (2.3) in Section 2.1, we get that ( µt)t≥0 satisﬁes ∂tDKL(µt ∥π) = −2 Eµt ⟨ ∇ln dµt dπ,∇dµt dπ ⟩ = −2 Eπ [∇dµt dπ 2] . Applying the Poincar´ e inequality (P) with f = dµt/dπ−1, we get ∂tDKL(µt ∥π) ≤− 2 CP χ2(µt ∥π) ≤− 2 CP DKL(µt ∥π) , where, in the last inequality, we use the fact that DKL(·∥ π) ≤χ2(·∥ π) (see [Tsy09, §2.4]). The bound (3.1) follows by applying Gr¨ onwall’s inequality.SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 7 To prove (3.2), we use the stronger inequality DKL(·∥π) ≤ln[1 + χ2(·∥π)] (see [Tsy09, §2.4]). Our diﬀerential inequality now reads: ∂tDKL(µt ∥π) ≤− 2 CP ( eDKL(µt∥π) −1 ) ⇐⇒∂tψ ( DKL(µt ∥π) ) ≤− 2 CP ψ ( DKL(µt ∥π) ) , where ψ(x) = 1 −e−x ≤1. Gr¨ onwall’s inequality now yields ψ ( DKL(µt ∥π) ) ≤e −2t CP ψ ( DKL(µ0 ∥π) ) ≤e −2t CP . Note that x≤2ψ(x) whenever ψ(x) ≤1/e. Thus, if t≥CP/2, we get ψ ( DKL(µt ∥π) ) ≤e−1 so DKL(µt ∥π) ≤2ψ ( DKL(µt ∥π) ) ≤e −2t CP , which, together with (3.1), completes the proof of (3.2). Remark 1. In [CLL+20], it was observed that the chi-squared divergence decays exponentially fast along the gradient ﬂow ( µt)t≥0 for the KL divergence, provided that π satisﬁes a Poincar´ e inequality. This observation is made precise and more general in [MMS09] where it is noted that the gradient ﬂow of a functional U dissipates a diﬀerent functional V at the same rate that the gradient ﬂow of V dissipates the functional U. A similar method is used to study the thin ﬁlm equation in [CT02] and [Car11, §5]. Since we are studying the gradient ﬂow of the chi-squared divergence, it is natural to ask whether CSF converges to πin chi-squared divergence as well. In the next results, we show quantitative decay of the chi-squared divergence along the gradient ﬂow under a Poincar´ e inequality (P), but we obtain only a polynomial rate of decay. However, if we additionally assume either that π is log-concave or that it satisﬁes a log-Sobolev inequality ( LSI), then we obtain exponential decay of the chi-squared divergence along CSF. Theorem 2. Suppose that πsatisﬁes a Poincar´ e inequality(P). Then, provided χ2(µ0∥π) <∞, the law (µt)t≥0 of CSF satisﬁes χ2(µt ∥π) ≤χ2(µ0 ∥π) ∧ (9CP 8t )2. If we further assume that π is log-concave, then χ2(µt ∥π) ≤χ2(µ0 ∥π) e − t 2CP . Proof. The proof is deferred to Appendix B. Under the stronger assumption ( LSI), we can show strong uniform ergodicity as in Theorem 1. Theorem 3. Assume that π satisﬁes a log-Sobolev inequality (LSI). Let (µt)t≥0 denote the law of CSF, and assume that χ2(µ0 ∥π) <∞. Then, for all t≥7CLSI, χ2(µt ∥π) ≤ ( χ2(µ0 ∥π) ∧2 ) e − t 9CLSI .8 CHEWI ET AL. Proof. The proof is deferred to Appendix B. Convergence in chi-squared divergence was studied in recent works such as [CLL19, VW19, CLL+20]. From standard comparisons between information divergences (see [Tsy09, §2.4]), it im- plies convergence in total variation distance, Hellinger distance, and KL divergence. Moreover, recent works have shown that the Poincar´ e inequality ( P) yields transportation-cost inequalities which bound the 2-Wasserstein distance by powers of the chi-squared divergence [Din15, Led18, CLL+20, Liu20], so we obtain convergence in the 2-Wasserstein distance as well. In particular, we mention that [CLL +20] uses the chi-squared gradient ﬂow ( CSF) to prove a transportation-cost inequality. 4. LAPLACIAN ADJUSTED WASSERSTEIN GRADIENT DESCENT (LA WGD) While the previous section leads to a better understanding of the convergence properties of SVGD in the case that Kπ is the identity operator, it is still unclear how to choose the kernel K to approach this idealized setup. ForSVGD with a general kernel K, the calculation rules of Section 2.1 together with the method of the previous section yield the formula ∂tDKL(µt ∥π) = −Eπ ⟨ ∇dµt dπ,Kπ∇dµt dπ ⟩ , for the dissipation of the KL divergence along SVGD. From this, a natural way to proceed is to seek an inequality of the form Eπ⟨f,Kπf⟩≳ Eπ[f2], for all locally Lipschitz f ∈L2(π). (4.1) Applying this inequality to each coordinate of∇(dµt/dπ) separately and using a Poincar´ e inequality would then allow us to conclude as in the proof of Theorem 1. The inequality (4.1) can be interpreted as a positive lower bound on the smallest eigenvalue of the operator Kπ. However, this approach is doomed to fail; under mild conditions on the kernel K, it is a standard fact that the eigenvalues of Kπ form a sequence converging to 0, so no such spectral gap can hold. 2 This suggests that any approach which seeks to prove ﬁnite-time convergence results for SVGD in the spirit of Theorem 1 must exploit ﬁner properties of the eigenspaces of the operator Kπ. Motivated by this observation, we develop a new algorithm called Laplacian Adjusted Wasserstein Gradient Descent ( LAWGD) in which the kernel K is chosen carefully so that Kπ = L−1 is the inverse of the generator of the Langevin diﬀusion that has π as invariant measure. More precisely, the starting point for our approach is the following integration-by-parts formula, which is a crucial component of the theory of Markov semigroups [BGL14]: Eπ⟨∇f,∇g⟩= Eπ[fLg], for all locally Lipschitz f,g ∈L2(π), (4.2) where L := −∆ +⟨∇V,∇·⟩. The operator L is the (negative) generator of the standard Langevin diﬀusion with stationary distributionπ[Pav14, §4.5]. We refer readers to Appendix A for background on the spectral theory of L. 2It is enough that K is a symmetric kernel with K ∈L2(π⊗π), and that π is not discrete (so that L2(π) is inﬁnite-dimensional); see [BGL14, Appendix A.6].SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 9 In order to use (4.2), we replace −Kπ∇(dµt/dπ) by the vector ﬁeld −∇Kπ(dµt/dπ). The new dynamics follow the evolution equation ∂tµt = div ( µt∇Kπ dµt dπ ) . (LAWGD) The vector ﬁeld in the above continuity equation may also be written −∇Kπ dµt dπ(x) = − ∫ ∇1K(x,·) dµt dπ dπ= − ∫ ∇1K(x,·) dµt. Replacing µt by an empirical average over particles and discretizing the process in time, we again obtain an implementable algorithm, which we give as Algorithm 1. A careful inspection of Algorithm 1 reveals that the update equation for the particles in Algo- rithm 1 does not involve the potential V directly, unlike the SVGD algorithm (2.5); thus, the kernel for LAWGD must contain all the information about V. Our choice for the kernel K is guided by the following observation (based on (4.2)): ∂tDKL(µt ∥π) = −Eπ ⟨ ∇dµt dπ,∇Kπ dµt dπ ⟩ = −Eπ [dµt dπLKπ dµt dπ ] . As a result, we choose K to ensure that Kπ = L−1. This choice yields ∂tDKL(µt ∥π) = −Eπ [(dµt dπ −1 )2] = −χ2(µt ∥π). (4.3) Algorithm 1LAWGD 1: procedure LAWGD(KL,µ0) 2: draw N particles X[1] 0 ,...,X [N] 0 i.i.d. ∼ µ0 3: for t= 1,...,T −1 do 4: for i= 1,...,N do 5: X[i] t+1 ←X[i] t −h N ∑N j=1 ∇1KL(X[i] t ,X[j] t ) 6: end for 7: end for 8: return X[1] T ,...,X [N] T 9: end procedure It remains to see which kernel K imple- ments Kπ = L−1. To that end, assume that L has a discrete spectrum and let ( λi,φi), i= 0,1,2,... be its eigenvalue-eigenfunction pairs where λjs are arranged in nondecreasing order. Assume further that λ1 > 0 (which amounts to a Poincar´ e inequality; see Appendix A) and deﬁne the following spectral kernel: KL (x,y) = ∞∑ i=1 φi(x)φi(y) λi (4.4) We now show that this choice of kernel endows LAWGD with a remarkable property: it converges to the target distribution exponentially fast, with a rate which has no dependence on the Poincar´ e constant. Moreover, akin to CSF—see (3.2)—it also also exhibit strong uniform ergodicity. Theorem 4. Assume that L has a discrete spectrum and that π satisﬁes a Poincar´ e inequal- ity (P) with some ﬁnite constant. Let (µt)t≥0 be the law of LAWGD with the kernel described above. Then, for all t≥1, DKL(µt ∥π) ≤ ( DKL(µ0 ∥π) ∧2 ) e−t. Proof. In light of (4.3), the proof is identical to that of Theorem 1.10 CHEWI ET AL. The convergence rate in Theorem 4 has no dependence on the target measure. Thisscale-invariant convergence also appears in [CLL +20], where it is shown for the Newton-Langevin diﬀusion with a strictly log-concave target measure π. In Theorem 4, we obtain similar guarantees under the much weaker assumption of a Poincar´ e inequality; indeed, there are many examples of non-log-concave distributions which satisfy a Poincar´ e inequality [VW19]. 5. EXPERIMENTS 5.0  2.5  0.0 2.5 5.0 0.0 0.1 0.2 0.3 0.4  LAWGD Init. True density Fig 2 . Samples from the standard Gaussian distribution generated by LAWGD, with kernel approximated by Hermite polynomials. For details, see Appendix C. To implement Algorithm 1, we numerically approximate the kernel K = KL given in (4.4). When π is the standard Gaus- sian distribution on R, the eigendecomposition of the operator L in (4.2) is known explicitly in terms of the Hermite poly- nomials [BGL14, §2.7.1], and we approximate the kernel via a truncated sum: ˆK(x,y) = ∑k i=1 λ−1 i φi(x)φi(y) (Figure 2) in- volving the smallest eigenvalues of L. In the general case, we implement a basic ﬁnite diﬀerence (FD) method to approximate the eigenvalues and eigenfunctions of L. We obtain better numerical results by ﬁrst transforming the operator L into the Schr¨ odinger operatorLS := −∆ + VS, where VS := 1 4 ∥∇V∥2 −1 2 ∆V. If φS is an eigenfunction of LS with eigenvalue λ (normalized such that ∫ φ2 S = 1), then φ := eV/2φS is an eigenfunction of L also with eigenvalue λ (and normalized such that ∫ φ2 dπ= 1); see [BGL14, §1.15.7]. 10  5  0 5 10 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 LAWGD SVGD Init. True Density Fig 3. LAWGD and SVGD run with constant step size for a mixture of three Gaussians. Both kernel density estimators use the same bandwidth. On a grid of points (with spacing ε), if we replace the Laplacian with the FD operator ∆ εf(x) := {f(x−ε) +f(x+ ε) −2f(x)}/ε2 (in 1D), then the FD Schr¨ odinger operatorLS,ε := −∆ε + VS can be represented as a sparse matrix, and its eigenvalues and (unit) eigenvectors are found with standard linear algebra solvers. When the potential V is known only up to an additive constant, then the approximate eigenfunctions produced by this method are not normalized correctly; instead, they satisfy ∥φ∥L2(π) = C for some constant C (which is the same for each eigenfunction). In turn, this causes the kernel K in LAWGDto be oﬀ by a multiplica- tive constant. For implementation purposes, however, this constant is absorbed in the step size of Algorithm 1. We also note that the eigenfunctions are diﬀerentiated using a FD approximation. To demonstrate, we sample from a mixture of three Gaussians: 2 5 N(−3,1)+ 1 5 N(0,1)+ 2 5 N(4,2). We compare LAWGDwith SVGD using the RBF kernel and median-based bandwidth as in [LW16]. We approximate the eigenfunctions and eigenvalues using a ﬁnite diﬀerence scheme, on 256 grid points evenly spaced between −14 and 14. Constant step sizes for LAWGD and SVGD are tuned and the algorithms are run for 5000 iterations, and the samples are initialized to be uniform on [1 ,4]. The results are displayed in Figure 3. All 256 discrete eigenfunctions and eigenvalues are used.SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 11 6. OPEN QUESTIONS We conclude this paper with some interesting open questions. The introduction of the chi-squared divergence as an objective function allows us to obtain both theoretical insights about SVGD and a new algorithm, LAWGD. This perspective opens the possibility of identifying other functionals deﬁned over Wasserstein space and that yield gradient ﬂows which are amenable to mathematical analysis and eﬃcient computation. Towards this goal, an intriguing direction is to develop alternative methods, besides kernelization, which provide eﬀective implementations of Wasserstein gradient ﬂows. Finally, we note that LAWGD provides a hitherto unexplored connection between sampling and computing the spectral decomposition of the Schr¨ odinger operator, the latter of which has been intensively studied in numerical PDEs. We hope our work further stimulates research at the intersection of these communities. Acknowledgments. Philippe Rigollet was supported by NSF awards IIS-1838071, DMS-1712596, DMS-TRIPODS- 1740751, and ONR grant N00014-17- 1-2147. Sinho Chewi and Austin J. Stromme were supported by the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. Thibaut Le Gouic was supported by ONR grant N00014-17-1-2147 and NSF IIS-1838071. APPENDIX A: REVIEW OF SPECTRAL THEORY In this paper, we consider elliptic diﬀerential operators of the form L = −∆ + ⟨∇V,∇·⟩, where V is a continuously diﬀerentiable potential. In this section, we provide a brief review of the spectral theory of these operators, and we refer to [Eva10, §6.5] for a standard treatment. The operator L (when suitably interpreted) is a linear operator deﬁned on a domain D ⊂L2(π). For any locally Lipschitz function f ∈L2(π), integration by parts shows that Eπ[fLf] = Eπ[∥∇f∥2]. Therefore, L has a non-negative spectrum. Also, we have L1 = 0, so that 0 is always an eigenvalue of L. We say that L has a discrete spectrum if it has a countable sequence of eigenvalues 0 = λ0 ≤λ1 ≤λ2 ≤λ3 ≤··· and corresponding eigenfunctions ( φi)∞ i=1 which form a basis of D. The eigenfunctions can be chosen to be orthogonal and normalized such that ∥φi∥L2(π) = 1; we always assume this is the case. Then, L can be expressed as L = ∞∑ i=1 λi⟨φi,·⟩L2(π) φi. The operator L has a discrete spectrum under the following condition ([Fri34], [RS78, Theorem XIII.67], [BGL14, Corollary 4.10.9]): VS ∈L1 loc(Rd), inf VS >−∞, and lim ∥x∥→∞ VS(x) = +∞, where VS := −∆V + 1 2 ∥∇V∥2. Moreover, under this condition we also have λi →∞ as i →+∞. For example, this condition is satisﬁed for V(x) = ∥x∥α for α >1, but not for α = 1. In fact, for α= 1, the spectrum of L is not discrete [BGL14, §4.1.1].12 CHEWI ET AL. The Poincar´ e inequality(P) is interpreted as a spectral gap inequality , since it asserts that λ1 = 1/CP > 0. Thus, under a Poincar´ e inequality,L : D ∩{f ∈L2(π) |Eπf = 0}→ L2(π) is bijective. Moreover, if it has a discrete spectrum, its inverse satisﬁes L−1 = ∞∑ i=1 λ−1 i ⟨φi,·⟩L2(π) φi. APPENDIX B: PROOFS OF THE CONVERGENCE GUARANTEES FOR THE CHI-SQUARED FLOW In this section, we give proofs of the convergence results we stated in Section 3. Proof of Theorem 2 (non-log-concave case). According to [CLL+20, Proposition 1], the Poincar´ e inequality implies the following inequality for the chi-squared divergence: χ2(µ∥π) 3/2 ≤9CP 4 Eµ [∇dµ dπ 2] , ∀µ≪π. (B.1) Since the Wasserstein gradient of χ2(·∥π) at µ is given by 2∇(dµ/dπ) (see Section 2.1), it yields ∂tχ2(µt ∥π) = −4 Eµt [∇dµt dπ 2] ≤− 16 9CP χ2(µt ∥π) 3/2 Solving the above diﬀerential inequality yields χ2(µt ∥π) ≤ χ2(µ0 ∥π) {1 + 8t √ χ2(µ0 ∥π)/(9CP)} 2 , which implies the desired result. We now prepare for the proof of exponentially fast convergence in chi-squared divergence for log-concave measures. The key to proving such results lies in diﬀerential inequalities of the form χ2(µ∥π) ≤CPL Eµ [∇dµ dπ 2] , ∀µ≪π, (B.2) which may be interpreted as a Polyak-6 Lojasiewicz(PL) inequality [KNS16] for χ2(·∥ π). PL in- equalities are well-known in the optimization literature, and can be even used when the objective is not convex [CMRS20]. In contrast, the preceding proof uses the weaker inequality (B.1), which may be interpreted as a 6 Lojasiewicz inequality[Loj63]. To see that a PL inequality readily yields exponential convergence, observe that ∂tχ2(µt ∥π) = −4 Eµt [∇dµt dπ 2] ≤− 4 CPL χ2(µt ∥π) . Together with Gr¨ onwall’s inequality, the diﬀerential inequality yieldsχ2(µt∥π) ≤χ2(µ0 ∥π) e − 4t CPL . In order to prove a PL inequality of the type (B.2), we require two ingredients. The ﬁrst one is a transportation-cost inequality for the chi-squared divergence proven in [Liu20], building onSVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 13 the works [Din15, Led18]. It asserts that if π satisﬁes a Poincar´ e inequality (P), then the following inequality holds: W2 2 (µ,π) ≤2CPχ2(µ∥π), ∀µ≪π. (B.3) For the second ingredient, we use an argument of [OV00] to show that if π satisﬁes a chi-squared transportation-cost inequality such as (B.3), and in addition is log-concave, then it satisﬁes an inequality of the type (B.2). We remark that the converse statement, that is, if π satisﬁes a PL in- equality (B.2) then it satisﬁes an appropriate chi-squared transportation-cost inequality, was proven in [CLL+20] without the additional assumption of log-concavity. It implies that for log-concave dis- tributions, the PL inequality (B.2) and the chi-squared transportation-cost inequality (B.3) are, in fact, equivalent. Theorem 5. Let π be log-concave, and assume that for some q∈(1,∞) and a constant C >0, W2 2 (µ,π) ≤Cχ2(µ∥π) 2/q , ∀µ≪π. Then, χ2(µ∥π) 2/p ≤4C Eµ [∇dµ dπ 2] , ∀µ≪π, (B.4) where p satisﬁes 1/p+ 1/q= 1. Proof. Following [OV00], let T be the optimal transport map from µ to π. Since χ2(·∥ π) is displacement convex [OT11,OT13] and has Wasserstein gradient 2∇(dµ/dπ) at µ(c.f. Section 2.1), the “above-tangent” formulation of displacement convexity ([Vil03, Proposition 5.29]) yields 0 = χ2(π∥π) ≥χ2(µ∥π) + 2Eµ ⟨ ∇dµ dπ,T −id ⟩ ≥χ2(µ∥π) −2W2(µ,π) √ Eµ [∇dµ dπ 2] , where we used the Cauchy-Schwarz inequality for the last inequality. Rearranging the above display and using the transportation-cost inequality assumed in the statement of theorem, we get χ2(µ∥π) ≤2W2(µ,π) √ Eµ [∇dµ dπ 2] ≤2 √ C Eµ [∇dµ dπ 2] χ2(µ∥π) 1/q . The result follows by rearranging the terms. Proof of Theorem 2 (log-concave case). From the transportation-cost inequality (B.3) and Theorem 5 with p= q= 2, we obtain χ2(µ∥π) ≤8CP Eµ [∇dµ dπ 2]. This PL inequality together with Gr¨ onwall’s inequality readily yields the result. We conclude this section with the proof of Theorem 3, which shows exponential convergence of CSF in chi-squared divergence under the assumption of a log-Sobolev inequality ( LSI) (but without the assumption of log-concavity).14 CHEWI ET AL. Proof of Theorem 3. We ﬁrst claim that ∂tχ2(µt ∥π) ≤− 4 9CLSI [χ2(µt ∥π) + 1] 3/2 ln[χ2(µt ∥π) + 1]. (B.5) Indeed, applying (LSI), we obtain ∂tχ2(µt ∥π) = −4 ∫ ∇dµt dπ 2 dµt = −16 9 ∫ ∇ ⏐⏐dµt dπ ⏐⏐3/22 dπ≤− 8 9CLSI entπ (⏐⏐dµt dπ ⏐⏐3) . Next, the variational formula for the entropy gives entπf = sup{Eπ(fg) : g satisﬁes Eπexp g= 1}, see [vH16, Lemma 3.15] or [BLM13, Theorem 4.13]. Choosing g= ln(dµt/dπ) yields entπ (⏐⏐dµt dπ ⏐⏐3) ≥Eπ [⏐⏐dµt dπ ⏐⏐3 ln dµt dπ ] = 1 3 Eπ [⏐⏐dµt dπ ⏐⏐3 ln (⏐⏐dµt dπ ⏐⏐3)] ≥1 3 Eπ [⏐⏐dµt dπ ⏐⏐3] ln Eπ [⏐⏐dµt dπ ⏐⏐3] ≥1 2 Eπ [⏐⏐dµt dπ ⏐⏐2]3/2 ln Eπ [⏐⏐dµt dπ ⏐⏐2] = 1 2[χ2(µt ∥π) + 1] 3/2 ln[χ2(µt ∥π) + 1], where in the second inequality, we used that x↦→xln x is convex on R+ and in the third, we used that it increasing when x≥1 together with Eπ [⏐⏐dµt dπ ⏐⏐2] = 1 + χ2(µt ∥π) ≥1. This proves (B.5). To simplify the inequality (B.5), we use the crude bounds ln[χ2(µt ∥π) + 1] ≥ { 1, if χ2(µt ∥π) ≥e−1 χ2(µt ∥π)/2, otherwise. It yields respectively ∂tχ2(µt ∥π) ≤− 2 9CLSI { 2χ2(µt ∥π)3/2, if χ2(µt ∥π) ≥e−1, χ2(µt ∥π), otherwise. (B.6) Solving the diﬀerential inequality in the ﬁrst case yields e−1 ≤χ2(µt ∥π) ≤ [ 9CLSI √ χ2(µ0 ∥π) 9CLSI + 2t √ χ2(µ0 ∥π) ]2 ≤ [9CLSI 2t ]2 , so that in this ﬁrst case, it must holds that t≤ 9CLSI 2√e−1 <3.5CLSI =: t0.SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 15 Therefore, if t≥t0, we are in the second case. In particular, χ2(µt0 ∥π) ≤e−1 ≤2 and integrating the diﬀerential inequality between t0 and t we get χ2(µt ∥π) ≤χ2(µt0 ∥π) e −2(t−t0) 9CLSI ≤ ( χ2(µ0 ∥π) ∧2 ) e −2(t−t0) 9CLSI , where in the last inequality, we used the fact that t↦→χ2(µt ∥π) is decreasing so that it also holds χ2(µt0 ∥π) ≤χ2(µ0 ∥π). In particular, taking t≥2t0 = 7CLSI yields the desired result. APPENDIX C: DETAILS FOR THE EXPERIMENTS We give additional details for the experiments presented in this paper. All methods were im- plemented in Python. Since the Schr¨ odinger operator requires the Laplacian and gradient of the potential V, we employ automatic diﬀerentiation to avoid laborious calculations of these derivatives. The probabilists’ Hermite polynomials are well-known to be eigenfunctions of the 1D Ornstein- Uhlenbeck operator L given by Lf(x) := −f′′(x) + xf′(x), and they satisfy the recursive re- lationship Hn+1(x) = xHn(x) −nHn−1(x), with H0(x) = 1 and H1(x) = x. It also holds that H′ n(x) = nHn−1(x). With these equations, it is easy to check that the eigenvalue corresponding to Hn is λn = n. These are used as the eigenfunctions and eigenvalues in the standard normal example given in Figure 2. In the simulation, we use the ﬁrst 150 Hermite polynomials. We run LAWGD for 2000 iterations with a constant step size, with initial points drawn uniformly from the interval [2.5,4.5]. In Figure 1, we display an example of sampling 50 particles from a mixture of two 2-dimensional Gaussian distributions given by π = 1 2 N((−1,−1)⊤,I2) + 1 2 N((1,1)⊤,I2). To run this experiment, we use a 2-dimensional FD method, which approximates the Laplacian as ∆εf(x,y) := f(x−ε,y) + f(x+ ε,y) + f(x,y −ε) + f(x,y + ε) −4f(x) ε2 . We again use the Schr¨ odinger operator for stability and use FD again to compute the gradients of the eigenfunctions. We use a 128 ×128 grid of evenly spaced xand y values between −6 and 6. We calculate only the bottom 100 eigenvalues and eigenfunctions, since the other eigenfunctions incur additional computational cost without noticeably changing the result. Any negative eigenvalues (which arise from numerical errors) are discarded. 4  2  0 2 4 4 2 0 2 4 LAWGD Init. 4  2  0 2 4 4 2 0 2 4 SVGD Init. 4  2  0 2 4 4 2 0 2 4 True samples Fig 4. Left: 50 particles and trajectories generated from1 2 N((−1,−1)⊤,I2) + 1 2 N((1,1)⊤,I2) with LAWGD. Middle: 50 particles and trajectories generated by SVGD. Right: true samples from the distribution.16 CHEWI ET AL. 4  2  0 2 4 4 2 0 2 4 LAWGD Init. 4  2  0 2 4 4 2 0 2 4 LAWGD Init. 4  2  0 2 4 4 2 0 2 4 LAWGD Init. 4  2  0 2 4 4 2 0 2 4 LAWGD Init. 4  2  0 2 4 4 2 0 2 4 SVGD Init. 4  2  0 2 4 4 2 0 2 4 SVGD Init. 4  2  0 2 4 4 2 0 2 4 SVGD Init. 4  2  0 2 4 4 2 0 2 4 SVGD Init. Fig 5 . Top: LAWGD after 100, 200, 1000, and 2000 iterations. Bottom: SVGD after 100, 200, 1000, and 2000 iterations. Additionally, we display the results from running SVGD with the RBF kernel and median-based bandwith on this example with a less favorable initialization. True samples from π are displayed for comparison. Both LAWGD and SVGD are run for 20000 iterations with a constant step size. The samples from LAWGDtend to move very fast from their initial positions and then tend to settle into their ﬁnal positions as seen in Figure 4. On the other hand, with constant step size, the samples of SVGD do not seem to converge, and one must use a decreasing step size scheme in order for the particles to stabilize. We also note that many of the samples generated by SVGD tend to blow up with a constant step size. In Figure 5, we plot the particles of LAWGD and SVGD at iterations 100, 200, 1000, and 2000 to compare the speed of convergence. REFERENCES [AGB15] D. Alonso-Guti ´errez and J. Bastero , Approaching the Kannan-Lov´ asz- Simonovits and variance conjectures , Lecture Notes in Mathematics 2131, Springer, Cham, 2015. [AGS08] L. Ambrosio, N. Gigli, and G. Savar´e, Gradient ﬂows: in metric spaces and in the space of probability measures, Springer Science & Business Media, 2008. [BGL14] D. Bakry, I. Gentil, and M. Ledoux, Analysis and geometry of Markov diﬀusion operators, Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] 348, Springer, Cham, 2014. [Ber18] E. Bernton , Langevin Monte Carlo and JKO splitting, in Proceedings of the 31st Conference On Learning Theory (S. Bubeck, V. Perchet, and P. Rigollet, eds.), Proceedings of Machine Learning Research 75, PMLR, 2018, pp. 1777–1798. [Bob99] S. G. Bobkov, Isoperimetric and analytic inequalities for log-concave probability mea- sures, 27 (1999), 1903–1921.SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 17 [BLM13] S. Boucheron, G. Lugosi, and P. Massart, Concentration inequalities, Oxford Uni- versity Press, Oxford, 2013, A nonasymptotic theory of independence, With a foreword by Michel Ledoux. [CLL19] Y. Cao, J. Lu , and Y. Lu , Exponential decay of R´ enyi divergence under Fokker- Planck equations, J. Stat. Phys. 176 (2019), 1172–1184. [Car11] E. A. Carlen, Functional inequalities and dynamics, in Nonlinear PDE’s and Appli- cations, Springer, 2011, pp. 17–85. [CT02] J. A. Carrillo and G. Toscani, Long-time asymptotics for strong solutions of the thin ﬁlm equation, Comm. Math. Phys. 225 (2002), 551–571. [CB18] X. Cheng and P. Bartlett, Convergence of Langevin MCMC in KL-divergence, in Algorithmic Learning Theory 2018, Proc. Mach. Learn. Res. (PMLR) 83, Proceedings of Machine Learning Research PMLR, 2018, p. 26. [CCBJ18] X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan , Underdamped Langevin MCMC: A non-asymptotic analysis, in Proceedings of the 31st Conference On Learning Theory (S. Bubeck, V. Perchet, and P. Rigollet, eds.), Proceedings of Machine Learning Research 75, PMLR, 07 2018, pp. 300–323. [CLL+20] S. Chewi, T. Le Gouic , C. Lu, T. Maunu, P. Rigollet, and A. J. Stromme , Exponential ergodicity of mirror-Langevin diﬀusions, arXiv e-prints (2020). [CMRS20] S. Chewi, T. Maunu, P. Rigollet, and A. J. Stromme , Gradient descent algo- rithms for Bures-Wasserstein barycenters, arXiv e-prints (2020). [Dal17a] A. Dalalyan, Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent, in Proceedings of the 2017 Conference on Learning Theory (S. Kale and O. Shamir, eds.), Proceedings of Machine Learn- ing Research 65, PMLR, Amsterdam, Netherlands, 2017, pp. 678–689. [Dal17b] A. S. Dalalyan, Theoretical guarantees for approximate sampling from smooth and log-concave densities, Journal of the Royal Statistical Society: Series B (Statistical Methodology) 79 (2017), 651–676. [DRD20] A. S. Dalalyanand L. Riou-Durand, On sampling from a log-concave density using kinetic Langevin diﬀusions, Bernoulli 26 (2020), 1956–1988. [DCM+18] G. Detommaso, T. Cui, Y. Marzouk, A. Spantini, and R. Scheichl, A Stein variational Newton method, in Advances in Neural Information Processing Systems , 2018, pp. 9169–9179. [Din15] Y. Ding, A note on quadratic transportation and divergence inequality,Statist. Probab. Lett. 100 (2015), 115–123. [DNS19] A. Duncan, N. Nuesken, and L. Szpruch, On the geometry of Stein variational gradient descent, arXiv e-prints (2019). [DMM19] A. Durmus , S. Majewski , and B. a. Miasojedow , Analysis of Langevin Monte Carlo via convex optimization, J. Mach. Learn. Res. 20 (2019), Paper No. 73, 46. [DM17] A. Durmus and E. Moulines, Nonasymptotic convergence analysis for the unadjusted Langevin algorithm, Ann. Appl. Probab. 27 (2017), 1551–1587. [DCWY19] R. Dwivedi , Y. Chen , M. J. Wainwright , and B. Yu , Log-concave sampling: Metropolis-Hastings algorithms are fast, Journal of Machine Learning Research 20 (2019), 1–42. [Eva10] L. C. Evans , Partial diﬀerential equations , second ed., Graduate Studies in Mathe- matics 19, American Mathematical Society, Providence, RI, 2010.18 CHEWI ET AL. [Fri34] K. Friedrichs, Spektraltheorie halbbeschr¨ ankter Operatoren und Anwendung auf die Spektralzerlegung von Diﬀerentialoperatoren, 109 (1934), 465–487. [vH16] R. van Handel, Probability in high dimension, 2016, Lecture Notes (Princeton Uni- versity). [HKRC18] Y.-P. Hsieh, A. Kavis, P. Rolland, and V. Cevher, Mirrored Langevin dynamics, in Advances in Neural Information Processing Systems , 2018, pp. 2878–2887. [JKO98] R. Jordan, D. Kinderlehrer , and F. Otto , The variational formulation of the Fokker-Planck equation, SIAM Journal on Mathematical Analysis 29 (1998), 1–17. [KLS95] R. Kannan , L. Lov ´asz, and M. Simonovits , Isoperimetric problems for convex bodies and a localization lemma, Discrete Comput. Geom. 13 (1995), 541–559. [KNS16] H. Karimi, J. Nutini, and M. Schmidt, Linear convergence of gradient and proximal- gradient methods under the Polyak-Lojasiewicz condition, in Joint European Confer- ence on Machine Learning and Knowledge Discovery in Databases , Springer, 2016, pp. 795–811. [Led18] M. Ledoux, Remarks on some transportation cost inequalities, 2018. [LV17] Y. T. Lee and S. S. Vempala , Eldan’s stochastic localization and the KLS hy- perplane conjecture: an improved lower bound for expansion, in 58th Annual IEEE Symposium on Foundations of Computer Science—FOCS 2017 , IEEE Computer Soc., Los Alamitos, CA, 2017, pp. 998–1007. [LLL+19] L. Li, Y. Li, J.-G. Liu, Z. Liu, and J. Lu, A stochastic version of Stein variational gradient descent for eﬃcient sampling, arXiv e-prints (2019). [LZC+19] C. Liu, J. Zhuo, P. Cheng, R. Zhang, and J. Zhu, Understanding and accelerating particle-based variational inference, in International Conference on Machine Learning, 2019, pp. 4082–4092. [Liu17] Q. Liu, Stein variational gradient descent as gradient ﬂow, in Advances in Neural In- formation Processing Systems 30 (I. Guyon, U. V. Luxburg, S. Bengio, H. Wal- lach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), Curran Associates, Inc., 2017, pp. 3115–3123. [LW16] Q. Liu and D. Wang, Stein variational gradient descent: A general purpose Bayesian inference algorithm, in Advances in Neural Information Processing Systems , 2016, pp. 2378–2386. [Liu20] Y. Liu , The Poincar´ e inequality and quadratic transportation-variance inequalities, Electron. J. Probab. 25 (2020), Paper No. 1, 16. [Loj63] S. Lojasiewicz, Une propri´ et´ e topologique des sous-ensembles analytiques r´ eels,Les ´ equations aux d´ eriv´ ees partielles117 (1963), 87–89. [LLN19] J. Lu, Y. Lu, and J. Nolen, Scaling limit of the Stein variational gradient descent: The mean ﬁeld regime, SIAM Journal on Mathematical Analysis 51 (2019), 648–671. [MCC+19] Y.-A. Ma, N. Chatterji, X. Cheng, N. Flammarion, P. Bartlett, and M. I. Jordan, Is there an analog of Nesterov acceleration for MCMC?,arXiv e-prints (2019). [MWBG12] J. Martin, L. C. Wilcox, C. Burstedde, and O. Ghattas, A stochastic Newton MCMC method for large-scale statistical inverse problems with application to seismic inversion, SIAM J. Sci. Comput. 34 (2012), A1460–A1487. [MMS09] D. Matthes, R. J. McCann , and G. Savar´e, A family of nonlinear fourth order equations of gradient ﬂow type,Comm. Partial Diﬀerential Equations 34 (2009), 1352– 1397.SVGD AS A KERNELIZED WASSERSTEIN GRADIENT FLOW 19 [MT09] S. Meyn and R. L. Tweedie, Markov chains and stochastic stability , 2nd ed., Cam- bridge University Press, USA, 2009. [OT11] S.-i. Ohta and A. Takatsu, Displacement convexity of generalized relative entropies, Adv. Math. 228 (2011), 1742–1787. [OT13] S.-i. Ohta and A. Takatsu, Displacement convexity of generalized relative entropies. II, Comm. Anal. Geom. 21 (2013), 687–785. [OV00] F. Otto and C. Villani , Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality, Journal of Functional Analysis 173 (2000), 361–400. [Pav14] G. A. Pavliotis, Stochastic processes and applications: diﬀusion processes, the Fokker- Planck and Langevin equations , 60, Springer, 2014. [RS78] M. Reed and B. Simon, Methods of modern mathematical physics. IV. Analysis of operators, Academic Press [Harcourt Brace Jovanovich, Publishers], New York-London, 1978. [SKL20] A. Salim, A. Korba, and G. Luise, Wasserstein proximal gradient, arXiv e-prints (2020). [San15] F. Santambrogio, Optimal transport for applied mathematicians , Progress in Non- linear Diﬀerential Equations and their Applications 87, Birkh¨ auser/Springer, Cham, 2015, Calculus of variations, PDEs, and modeling. [San17] F. Santambrogio, {Euclidean, metric, and Wasserstein}gradient ﬂows: an overview, Bulletin of Mathematical Sciences 7 (2017), 87–154. [SBCR16] U. Simsekli, R. Badeau, A. T. Cemgil, and G. Richard, Stochastic quasi-Newton Langevin Monte Carlo, in Proceedings of the 33rd International Conference on Inter- national Conference on Machine Learning - Volume 48 , ICML’16, JMLR.org, 2016, p. 642–651. [Tsy09] A. B. Tsybakov, Introduction to nonparametric estimation, Springer Series in Statis- tics, Springer, New York, 2009, Revised and extended from the 2004 French original, Translated by Vladimir Zaiats. [VW19] S. Vempala and A. Wibisono, Rapid convergence of the unadjusted Langevin algo- rithm: isoperimetry suﬃces, in Advances in Neural Information Processing Systems 32 (H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, eds.), Curran Associates, Inc., 2019, pp. 8094–8106. [Vil03] C. Villani, Topics in optimal transportation , Graduate Studies in Mathematics 58, American Mathematical Society, Providence, RI, 2003. [Vil09] C. Villani , Optimal transport , Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] 338, Springer-Verlag, Berlin, 2009, Old and new. [WTBL19] D. Wang , Z. Tang , C. Bajaj , and Q. Liu , Stein variational gradient descent with matrix-valued kernels, in Advances in Neural Information Processing Systems 32 (H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, eds.), Curran Associates, Inc., 2019, pp. 7836–7846. [WL20] Y. Wang and W. Li, Information Newton’s ﬂow: second-order optimization method in probability space, arXiv e-prints (2020). [Wib18] A. Wibisono, Sampling as optimization in the space of measures: The Langevin dy- namics as a composite optimization problem, inConference on Learning Theory, COLT20 CHEWI ET AL. 2018, Stockholm, Sweden, 6-9 July 2018 (S. Bubeck, V. Perchet, and P. Rigollet, eds.), Proceedings of Machine Learning Research 75, PMLR, 2018, pp. 2093–3027. [Wib19] A. Wibisono, Proximal Langevin algorithm: rapid convergence under isoperimetry, arXiv e-prints (2019). [ZZCC18] J. Zhang, R. Zhang, L. Carin, and C. Chen, Stochastic particle-optimization sam- pling and the non-asymptotic convergence theory, arXiv e-prints (2018). [ZPFP20] K. S. Zhang, G. Peyr´e, J. Fadili, and M. Pereyra, Wasserstein control of mirror Langevin Monte Carlo, arXiv e-prints (2020). Department of Mathematics Massachusetts Institute of Technology 77 Massachusetts Avenue, Cambridge, MA 02139-4307, USA",
      "meta_data": {
        "arxiv_id": "2006.02509v1",
        "authors": [
          "Sinho Chewi",
          "Thibaut Le Gouic",
          "Chen Lu",
          "Tyler Maunu",
          "Philippe Rigollet"
        ],
        "published_date": "2020-06-03T20:20:21Z",
        "pdf_url": "https://arxiv.org/pdf/2006.02509v1.pdf"
      }
    },
    {
      "title": "Online continual learning from imbalanced data"
    },
    {
      "title": "Task agnostic continual learning via meta learning",
      "abstract": "While neural networks are powerful function approximators, they suffer from\ncatastrophic forgetting when the data distribution is not stationary. One\nparticular formalism that studies learning under non-stationary distribution is\nprovided by continual learning, where the non-stationarity is imposed by a\nsequence of distinct tasks. Most methods in this space assume, however, the\nknowledge of task boundaries, and focus on alleviating catastrophic forgetting.\nIn this work, we depart from this view and move the focus towards faster\nremembering -- i.e measuring how quickly the network recovers performance\nrather than measuring the network's performance without any adaptation. We\nargue that in many settings this can be more effective and that it opens the\ndoor to combining meta-learning and continual learning techniques, leveraging\ntheir complementary advantages. We propose a framework specific for the\nscenario where no information about task boundaries or task identity is given.\nIt relies on a separation of concerns into what task is being solved and how\nthe task should be solved. This framework is implemented by differentiating\ntask specific parameters from task agnostic parameters, where the latter are\noptimized in a continual meta learning fashion, without access to multiple\ntasks at the same time. We showcase this framework in a supervised learning\nscenario and discuss the implication of the proposed formalism.",
      "full_text": "Task Agnostic Continual Learning via Meta Learning Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A. Rusu, Yee Whye Teh, Razvan Pascanu DeepMind, London, UK {hexu, sygi, agalashov, andreirusu, ywteh, razp}@google.com Abstract While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task bound- aries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering – i.e measuring how quickly the network recovers performance rather than measuring the network’s performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework speciﬁc for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task speciﬁc parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implications of the proposed formalism. 1 Introduction A common assumption made by many machine learning algorithms is that the observations in the dataset are independent and identically distributed (i.i.d). However, there are many scenarios where this assumption is violated because the underlying data distribution is non-stationary. For instance, in reinforcement learning (RL), the observations depend on the current policy of the agent, which may change over time. In addition, the environments with which the agent interacts are usually non-stationary. In supervised learning tasks, due to computational or legal reasons, one might be forced to re-train a deployed model only on the recently collected data, which might come from a different distribution than that of the previous data. In all these scenarios, blindly assuming i.i.d will not only lead to inefﬁcient learning procedure, but also catastrophic interference [19]. One research area that addresses this problem is continual learning, where the non-stationarity of data is usually described as a sequence of distinct tasks. A list of desiderata for continual learning [33] include the ability to not forget, forward positive transfer (learning new tasks faster by leveraging previously acquired knowledge), and backwards positive transfer (improvement on previous tasks because of new skills learned), bounded memory budget regardless the number of tasks and so forth. Since these desiderata are often competing with each other, most continual learning methods aim for some of them instead of all, and to simplify the problem, they usually assume that the task labels or the boundaries between different tasks are known. In this work, we aim to develop algorithms that can continually learn a sequence of tasks without knowing their labels or boundaries. Furthermore, we argue that in a more challenging scenario where the tasks are not only different but also conﬂicting with each other, most existing approaches will Preprint. Under review. arXiv:1906.05201v1  [stat.ML]  12 Jun 2019fail. To overcome these challenges, we propose a framework that applies meta-learning techniques to continual learning problems, and shift our focus from less forgetting to faster remembering: to rapidly recall a previously learned task, given the right context as a cue. 2 Problem Statement We consider the online learning scenario studied by [ 10, 36, 22], where at each time step t, the network receives an input xt and gives a prediction ˆyt := ˆf(xt,θt) using a model ˆf parametrised by θt. It then receives the ground truth yt, which can be used to adapt its parameters and to improve its performance on future predictions. If the data distribution is non-stationary (e.g., xt,yt might be sampled from some task Afor a while, then the task switches to B), then training on the new data might lead to catastrophic forgetting – the new parameters θ′can solve task Bbut not task A anymore: ˆf(xB t ; θ′) =yB t , ˆf(xA t ; θ′) ̸= yA t . Many continual learning methods were proposed to alleviate the problem of catastrophic forgetting. However, most of these approaches require either the information of task index (Aor B) or at least the moment when the task switches. Only recently, the continual learning community started to focus on task agnostic methods [37, 2]. However, all these methods have the underlying assumption that no matter what tasks it has been learning, at any time t, it is possible to ﬁnd parameters θt that ﬁt all previous observations with high accuracy: ∃θt s.t.∀t′≤t, ˆf(xt′ ,θt) ≈yt′ . This assumption is, however, not valid when the targetyt depends not only on the observation xt but also on some hidden task (or context) variable ct: yt = f(xt,ct), a common scenario in partially observable environments [21, 4]. In this case, when the context has changed ( ct ̸= ct′ ), even if the observation remains the same (xt = xt′ ), the targets may be different (yt ̸= yt′ ). As a result, it is impossible to ﬁnd a single parameter vector θt that ﬁts both mappings: ˆf(xt; θt) =yt =⇒ ˆf(xt′ ; θt) ̸= yt′ . It follows that, in this case, catastrophic forgetting cannot be avoided without inferring the task variable ct. 3 What & How Framework Here we propose a framework for task agnostic continual learning that explicitly infers the current task from some context data Dcxt t and predicts targets based on both the inputs xt and task representations ct. The framework consists of two modules: an encoder or task inference network Fwhat : Dcxt t →ct that predicts the current task representation ct based on the context data Dcxt t , and a decoder FHow : ct → ˆfct that maps the task representation ct to a task speciﬁc model ˆfct : x →ˆy, which makes predictions conditional on the current task. Under this framework, even when the inputs xt and xt′ are the same, the predictions ˆyt and ˆyt′ can differ from each other depending on the contexts. In this work, we choose the recent kobservations {(xt−k,yt−k),···(xt−1,yt−1)}as the context dataset Dcxt. This choice is reasonable in an envi- ronment where ct is piece-wise stationary or changes smoothly. An overview of this framework is illustrated in Figure 1a. (a) What & How framework  (b) MetaCoG  (c) MetaELLA Figure 1: Schematic diagrams of the framework and its instances. 3.1 Meta Learning as Task Inference A similar separation of concern can be found in the meta-learning literature. In fact, many recently proposed meta-learning methods can be seen as instances of this framework. For example, Conditional Neural Processes (CNP) [6] embed the observation and target pairs in context data (xi,yi) ∈Dcxt t by 2an encoder ri = h(xi,yi; θh). The embeddings are then aggregated by a commutative operation ⊕ (such as the mean operation) to obtain a single embedding of the context: rt = FWhat(Dcxt t ; θh) :=⨁ xi,yi∈Dcxt t h(xi,yi; θh). At inference time, the context embedding is passed as an additional input to a decoder gto produce the conditional outputs: FHow(rt) :=g(·,rt; θg). Model-Agnostic Meta-Learning (MAML) [ 5] infers the current task by applying one or a few steps of gradient descent on the context data Dcxt t . The resulting task-speciﬁc parameters can be considered a high-dimensional representation of the current task returned by a What encoder: θt = FWhat(Dcxt t ; θinit) := θinit t −λin∇θLin( ˆf(·; θ),Dcxt t ), where θinit t are meta parameters, and λin the inner loop learning rate. The How decoder of MAML returns the task speciﬁc model by simply parametrizing ˆf with θt: FHow(θt) := ˆf(·; θt). [29] proposed Latent Embedding Optimization (LEO) which combines the encoder/decoder structure with the idea of inner loop ﬁne-tuning from MAML. The latent task embedding zt is ﬁrst sampled from a Gaussian distribution N(µe t,diag(σe t 2)) whose mean µe t and variance σe t 2 are generated by averaging the outputs of a relation network: µe t,σe t = 1 |Dcxt|2 ∑ xi∈Dcxt ∑ xj∈Dcxt gr(ge(xi),ge(xj)), where gr(·) is a relation network andge(·) is an encoder. Task-dependent weights can then be sampled from a decoder gd(·): wt ∼N(µd t,diag(σd t 2 )), where µd t,σd t = gd(zt). The ﬁnal task representation is obtained by a few steps of gradient descent: z′ t = FWhat(Dcxt t ) :=zt −λin∇z′ Lin( ˆf(·; wt),Dcxt t ), and the ﬁnal task speciﬁc weights w′ t are decoded from z′: FHow(z′ t) =w′ t ∼N(µd′ t ,diag(σd′ t 2 )), where µd′ t ,σd′ t = gd(z′ t). In Fast Context Adaptation via Meta-Learning (CA VIA) [38], a neural network model ˆf takes a context vector ct as an additional input: ˆy= ˆf(x,ct; θ). The context vector is inferred from context data by a few steps of gradient descent: ct = FWhat(Dcxt t ; θ) := cinit −λin∇cLin( ˆf(·,c; θ),Dcxt t ). Then a context dependent model is returned by the How decoder: FHow(ct) := ˆf(·,ct; θ). Table 1 in Appendix summarizes how these methods can be seen as instances of the What & How framework. Under this framework, we can separate the task speciﬁc parameters of ˆf from the task agnostic parameters of FWhat and FHow. 3.2 Continual Meta Learning In order to train these meta learning models, one normally has to sample data from multiple tasks at the same time during training. However, this is not feasible in a continual learning scenario, where tasks are encountered sequentially and only a single task is presented to the agent at any moment. As a result, the meta models (What & How functions) themselves are prone to catastrophic forgetting. Hence, the second necessary component of our framework is to apply continual learning methods to stabilize the learning of meta parameters. In general, any continual learning method that can be adapted to consolidate memory at every iteration instead of at every task switch can be applied in our framework, such as Online EWC [ 33] and Memory Aware Synapses (MAS) [ 1]. In order to highlight the effect of explicit task inference for task agnostic continual learning, we choose a particular method called Bayesian Gradient Descent (BGD) [37] to implement our framework. We show that by applying BGD on the meta-level models (FWhat and FHow), the network can continually learn a sequence of tasks that are impossible to learn when BGD is applied to the bottom-level model ˆf. Formally, let θmeta be the vector of meta parameters, (i.e. the parameters of FWhat and FHow, for instance, θinit in MAML). We model its distribution by a factorized Gaussian p(θmeta) = ∏ iN(θmeta i |µi,σi). Given a context dataset Dcxt t and the current observations (xt,yt), the meta loss can be deﬁned as the loss of the task speciﬁc model on the current observations: Lmeta := L( ˆft(xt),yt), where ˆft = FHow ◦FWhat(Dcxt; θmeta). With the meta loss deﬁned, it is then possible to optimize µ,σ using the BGD update rules derived from the online variational Bayes’ rule and a re-parametrization trick (θmeta i = µi + σiϵi, ϵi ∼N(0,1)): µi ←µi −ησ2 iEϵ [∂Lmeta ∂θmeta i ] , σ i ←σi √ 1 + (1 2σiEϵ [∂Lmeta ∂θmeta i ϵi ])2 −1 2σ2 iEϵ [∂Lmeta ∂θmeta i ϵi ] , (1) 3where ∂Lmeta/∂θmeta i is the gradient of the meta loss Lmeta with respect to sampled parameters θmeta i and ηis a learning rate. The expectations are computed via Monte Carlo method: Eϵ [∂Lmeta ∂θmeta i ] ≈ 1 K K∑ k=1 ∂Lmeta(θmeta(k) i ) ∂θmeta i , Eϵ [∂Lmeta ∂θmeta i ϵi ] ≈ 1 K K∑ k=1 ∂Lmeta(θmeta(k) i ) ∂θmeta i ϵ(k) i (2) An intuitive interpretation of BGD learning rules is that weights µi with smaller uncertainty σi are more important for the knowledge accumulated so far, thus they should change slower in the future in order to preserve the learned skills. 3.3 Instantiation of the Framework Using the What & How framework, one can compose arbitrarily many continual meta learning methods. To show that this framework is independent from a particular implementation, we propose two such instances by adapting previous continual learning methods to this meta learning framework. MetaCoG Context-dependent gating of sub-spaces [9], parameters [17] or units [34] of a single network have proven effective at alleviating catastrophic forgetting. Recently, [ 18] showed that combining context dependent gating with a synaptic stabilization method can achieve even better performance than using either method alone. Therefore, we explore the use of context dependent masks as our task representations, and deﬁne the task speciﬁc model as the sub-network selected by these masks. At every time step t, we infer the latent masks mt based on the context dataset Dcxt t by one or a few steps of gradient descent of an inner loop loss function Lin with respect to m: mt := FWhat(Dcxt t ; θ) =minit −λin ·∇mLin( ˆf(·; θ⊙σ(m)),Dcxt t ), (3) where minit is a ﬁxed initial value of the mask variables, σ(·) is an element-wise sigmoid function to ensure that the masks are in [0,1], and ⊙is element-wise multiplication. In general, Lin can be any objective function. For instance, for a regression task, one can use a mean squared error with an L1 regularization that enforces sparsity of σ(m): Lin( ˆf(·; θ⊙σ(m)),Dcxt t ) := ∑ xi,yi∈Dcxt t ( ˆf(xi; θ⊙σ(m)) −yi)2 + γ||σ(m)||1 (4) The resulting masksmtare then used to gate the base network parametersθtin order to make a context- dependent prediction: ˆyt = ˆf(xt; θt⊙σ(mt)). Once the ground truth ytis revealed, we can deﬁne the meta loss as the loss of the masked network on the current data: Lmeta( ˆf(·; θ⊙σ(mt)),{(xt,yt)}) and optimize the distribution q(θ|µ,σ) of task agnostic meta variable θby BGD. The intuition here is that the parameters of the base network should allow fast adaptations of the masks mt. Since the context-dependent gating mechanism is trained in a meta-learning fashion, we call this particular instance of our framework Meta Context-dependent Gating (MetaCoG). We note that while we draw our inspiration from the idea of selecting a subnetwork using the masks mt, in the formulated algorithm mt rather plays the role of modulating the parameters (i.e. in practice we noticed that entries of mt do not necessarily converge to 0 or 1). Note that the inner loop loss Lin used to infer the context variable mt does not have to be the same as the meta loss Lmeta. In fact, one can choose an auxiliary loss function for Lin as long as it is informative about the current task. MetaELLA The second instance of the framework is based on the GO-MTL model [ 14] and the Efﬁcient Lifelong Learning Algorithm (ELLA) [ 30]. In a multitask learning setting, ELLA tries to solve each task with a task speciﬁc parameter vector θ(t) by linearly combining a shared dictionary of klatent model components L∈Rd×k using a task-speciﬁc coefﬁcient vector s(t) ∈Rk: θ(t) := Ls(t), where Lis learned by minimizing the objective function Lella(L) = 1 T T∑ t=1 min s(t) { 1 n(t) n(t) ∑ i=1 L (ˆf(x(t) i ; Ls(t)),y(t) i ) + µ||s(t)||1 } + λ||L||2 F (5) 4Instead of directly optimizing Lella(L), we adapt ELLA to the What & How framework by con- sidering s(t) as the task representation returned by a What encoder and L as parameters of a How decoder. The objective Lella can then be minimized in a continual meta learning fash- ion. At time t, current task representation st is obtained by minimizing the inner loop loss Lin( ˆf(·; Ls),Dcxt) := 1 |Dcxt| ∑ xi,yi∈Dcxt L( ˆf(xi; Ls),yi) +µ||s||1 by one or a few steps of gradient descent from ﬁxed initial value sinit: st := FWhat(Dcxt t ; L) =sinit −λin ·∇sLin( ˆf(·; Ls),Dcxt t ). Similar to MetaCoG, the parametric distribution q(L|µL,σL) = ∏ iN(Li|µL i ,σL i ) of the meta variable Lcan be optimized with respect to the meta loss Lmeta( ˆf(·; Lst),{(xt,yt)}) using BGD. 4 Related Work Continual learning has seen a surge in popularity in the last few years, with multiple approaches being proposed to address the problem of catastrophic forgetting. These approaches can be largely categorized into the following types [24]: Rehearsal based methods rely on solving the multi-task objective, where the performance on all previous tasks is optimized concurrently. They focus on techniques to either efﬁciently store data points from previous tasks [27, 16] or to train a generative model to produce pseudo-examples [35]. Then the stored and generated data can be used to approxi- mate the losses of previous tasks. Structural based methods exploit modularity to reduce interference, localizing the updates to a subset of weights. [28] proposed to learn a new module for each task with lateral connection to previous modules. It prevents catastrophic forgetting and maximizes forward transfer. In [7], pruning techniques are used to minimize the growth of the model with each observed tasks. Finally, Regularization based methods draw inspiration from Bayesian learning, and can be seen as utilizing the posterior after learning a sequence of tasks as a prior to regularize learning of the new task. These methods differ from each other in how the prior and implicitly the posterior are parametrized and approximated. For instance, Elastic Weight Consolidation (EWC) [12] relies on a Gaussian approximation with a diagonal covariance, estimated using a Laplace approximation. Variational Continual Learning (VCL) [23] learns directly the parameters of the Gaussian relying on the re-parametrization trick. [26] achieved better approximation with block-diagonal covariance. While effective at preventing forgetting, the above-mentioned methods either rely on knowledge of task boundaries or require task labels to select a sub-module for adaptation and prediction, hence cannot be directly applied in the task agnostic scenario considered here. To circumvent this issue, [12] used Forget-Me-Not (FMN) [ 20] to detect task boundaries and combined it with EWC to consolidate memory when task switches. However, FMN requires a generative model that computes exact data likelihood, which limits it from scaling to complex tasks. More recently, [2] proposed a rehearsal-based method to select a ﬁnite number of data that are representative of all data seen so far. This method, similar to BGD, assumes that it is possible to learn one model that ﬁts all previous data, neglecting the scenario where different tasks may conﬂict each other, hence does not allow task-speciﬁc adaptations. Meta-learning, or learning to learn [ 32], assumes simultaneous access to multiple tasks during meta-training, and focuses on the ability of the agent to quickly learn a new task at meta-testing time. As with continual learning, different families of approaches exist for meta-learning. Memory based methods [31] rely on a recurrent model (optimizer) such as LSTM to learn a history-dependent update function for the lower-level learner (optimizee). [3] trained an LSTM to replace the stochastic gradient descent algorithm by minimizing the sum of the losses of the optimizees on multiple prior tasks. [25] use an LSTM-based meta-learner to transform the gradient and loss of the base-learners on every new example to the ﬁnal updates of the model parameters. Metric based methods learn an embedding space in which other tasks can be solved efﬁciently. [ 13] trained siamese networks to tell if two images are similar by converting the distance between their feature embeddings to the probability of whether they are from the same class. [36] proposed the matching network to improve the embeddings of a test image and the support images by taking the entire support set as context input. The approaches discussed in Section 3.1 instead belong to the family of optimization based meta-learning methods. In this domain, the most relevant work is from [22], where they studied fast adaptation in a non-stationary environment by learning an ensemble of networks, one for each task. Unlike our work, they used MAML for initialization of new networks in the ensemble instead of task inference. A drawback of this approach is that the size of the ensemble grows over time and is unbounded, hence can become memory-consuming when there are many tasks. 55 Experiments To demonstrate the effectiveness of the proposed framework, we compare BGD and Adam[11] to three instances of this framework on a range of task agnostic continual learning experiments. The ﬁrst instance is simply applying BGD on the meta variable θinit of MAML instead of on the task speciﬁc parameters. We refer to this method as MetaBGD. The other two are MetaCoG and MetaELLA, introduced in Section 3.3. In all experiments, we present N tasks consecutively and each task lasts for M iterations. At every iteration t, a batch of Ksamples Dt = {xt,1,···xt,K}from the training set of the current task are presented to the learners, and the context data used for task inference is simply the previous mini-batch with their corresponding targets: Dcxt t = Dt−1 ⋃{yt−1,1,···yt−1,K}. At the end of the entire training process, we test the learners’ performance on the testing set of every task, given a mini-batch of training data from that task as context data. Since the meta learners take ﬁve gradient steps in the inner loop for task inference, we also allow BGD and Adam to take ﬁve gradient steps on the context data before testing their performances. We focus on analyzing the main results in this section, experimental details are provided in the Appendix B. 5.1 Sine Curve Regression Task Index 0 1 2 3 4 5 6 7 8MSE Adam BGD MetaCoG MetaElla MetaBGD Figure 2: Testing loss per task at the end of the entire learning phase. Task 1 is the ﬁrst seen task, and task 10 is the last. Lower MSE means better performance. We start with a regression problem commonly used in meta learning literature, where each task corresponds to a sine curve to be ﬁtted. In this experiment, we randomly generate 10 sine curves and present them sequentially to a 3-layer MLP. Figure 2 shows the mean squared error (MSE) of each task after the entire training process. Adam and BGD perform signiﬁcantly worse than the meta learners, even though they have taken the same number of gradient steps on the context data. The reason for this large gap of performance becomes evident by looking at Figure 3, which shows the learners’ predictions on testing data of the last task and the third task, given their corresponding context data. All learners can solve the last task almost perfectly, but when the context data of the third task is provided, meta learners can quickly remember it, while BGD and Adam are unable to adapt to the task they have previously learned. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 6  4  2  0 2 4 6 6 4 2 0 2 4 6 6  4  2  0 2 4 6 6 4 2 0 2 4 6 BGD Adam MetaCoG MetaELLA MetaBGD Target Figure 3: Predictions for the last task (left) and the third task (right) after the entire training process. 65.2 Label-Permuted MNIST A classical experiment for continual learning is permuted MNIST [8, 12], where a new task is created by shufﬂing the pixels of all images in MNIST by a ﬁxed permutation. In this experiment, however, we shufﬂe the classes in the labels instead of the pixels in the images. The reason for this change is to ensure that it is not possible to guess the current task simply based on the images. In this way, we can test whether our framework is able to quickly adapt its behavior according to the current context. Five tasks are created with this method and are presented sequentially to an MLP. We test the learners’ classiﬁcation accuracy of each task at the end of the entire learning process, using a mini-batch of training set as context data. As can be seen from Figure 4, all learners perform well on the last task. However, BGD and Adam have chance-level accuracy on previous tasks due to their incapability of inferring tasks from context data, while the meta learners are able to recall those tasks within 5 inner loop updates on the context data. Figure 5 displays the accuracy curve when we play the tasks again, for 10 iterations each, after the ﬁrst training process. The tasks are presented in the same order as they were learned for the ﬁrst time. It is clear that one iteration after the task changes, when the correct context data is available, the meta learners are able to recall the current task to almost perfection, while Adam and BGD have to re-learn each task from scratch. 1 2 3 4 5 Task Index 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Adam BGD MetaCoG MetaElla MetaBGD Figure 4: Testing accuracy of different tasks in the label-permuted MNIST experiment at the end of the entire training process. 0 10 20 30 40 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Adam BGD MetaCoG MetaElla MetaBGD Figure 5: Accuracy curve when the label-permuted MNIST tasks are replayed for 10 iterations after the entire training process. The sudden drops of accuracy are due to task switching, when the context data are still from the previous task. 5.3 Omniglot We have seen in previous two experiments that when the task information is hidden from the network, continual learning is impossible without task inference. In this experiment, we show that our framework is favourable even when the task identity is reﬂected in the inputs. To this end, we test our framework and BGD by sequential learning of handwritten characters from the Omniglot dataset [15], which consists of 50 alphabets with various number of characters per alphabet. Considering 7every alphabet as a task, we present 10 alphabets sequentially to a convolutional neural network and train it to classify 20 characters from each alphabet. Most continual learning methods (including BGD) require a multi-head output in order to overcome catastrophic forgetting in this set-up. The idea is to use a separate output layer per task, and to only compute the error on the current head during training and only make predictions from the current head during testing. Therefore, task index has to be available in this case in order to select the correct head. Unlike these previous works, we evaluate our framework with a single head of 200 output units in this experiment. Figure 6 summarizes the results of this experiment. For every task, we measure its corresponding testing accuracy twice: once immediately after that task is learned (no forgetting yet), and once after all ten tasks are learned. Our framework with a single head can achieve comparable results as BGD with multiple heads, whereas BGD with a single head completely forgets previous tasks. Figure 6: Testing accuracy of the sequential Omniglot task. BGD (MH) uses a multi-head output layer, whereas BGD (SH) and all meta learners use a single-head output layer. In the bottom plot, the accuracy of BGD(SH) are 0 for all tasks except the last one. 6 Conclusions In this work, we showed that when the objective of a learning algorithm depends on both the inputs and context, catastrophic forgetting is inevitable without conditioning the model on the context. A framework that can infer task information explicitly from context data was proposed to resolve this problem. The framework separates the inference process into two components: one for representing What task is expected to be solved, and the other for describing How to solve the given task. In addition, our framework uniﬁes many meta learning methods and thus establishes a connection between continual learning and meta learning, and leverages the advantages of both. There are two perspectives of viewing the proposed framework: from the meta learning perspective, our framework addresses the continual meta learning problem by applying continual learning tech- niques on the meta variables, therefore allowing the meta knowledge to accumulate over an extended period; from the continual learning perspective, our framework addresses the task agnostic continual learning problem by explicitly inferring the task when the task information is not available, and this allows us to shift the focus of continual learning from less forgetting to faster remembering, given the right context. For future work, we would like to test this framework for reinforcement learning tasks in partially observable environments, where the optimal policy has to depend on the hidden task or context information. 8References [1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte- laars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), pages 139–154, 2018. [2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Online continual learning with no task boundaries. arXiv preprint arXiv:1903.04476, 2019. [3] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pages 3981–3989, 2016. [4] Anthony R Cassandra, Leslie Pack Kaelbling, and Michael L Littman. Acting optimally in partially observable stochastic domains. In AAAI, 1994. [5] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta- tion of deep networks. In International Conference on Machine Learning, pages 1126–1135, 2017. [6] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International Conference on Machine Learning, pages 1690–1699, 2018. [7] Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Continual learning via neural pruning. arXiv preprint arXiv:1903.04476, 2019. [8] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. [9] Xu He and Herbert Jaeger. Overcoming catastrophic interference using conceptor-aided backpropagation. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=B1al7jg0b. [10] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent. In Proceedings of the International Conference on Artiﬁcial Neural Networks, ICANN ’01, pages 87–94, London, UK, 2001. Springer-Verlag. ISBN 3-540-42486-5. URL http: //dl.acm.org/citation.cfm?id=646258.684281. [11] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [12] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. [13] Gregory Koch. Siamese neural networks for one-shot image recognition. In ICML Deep Learning workshop, 2015. [14] Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning. arXiv preprint arXiv:1206.6417, 2012. [15] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. [16] David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pages 6467–6476, 2017. [17] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7765–7773, 2018. 9[18] Nicolas Y . Masse, Gregory D. Grant, and David J. Freedman. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization. Proceedings of the National Academy of Sciences, 115(44):E10467–E10475, 2018. ISSN 0027-8424. doi: 10.1073/pnas.1803839115. URL https://www.pnas.org/content/115/44/E10467. [19] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989. [20] Kieran Milan, Joel Veness, James Kirkpatrick, Michael Bowling, Anna Koop, and Demis Hassabis. The forget-me-not process. In Advances in Neural Information Processing Systems, pages 3702–3710, 2016. [21] George E Monahan. State of the art—a survey of partially observable markov decision processes: theory, models, and algorithms. Management Science, 28(1):1–16, 1982. [22] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning: Continual adaptation for model-based RL. In International Conference on Learning Represen- tations, 2019. URL https://openreview.net/forum?id=HyxAfnA5tm. [23] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. [24] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 2019. [25] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference on Learning Representations, 2016. [26] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, pages 3738–3748, 2018. [27] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7 (2):123–146, 1995. [28] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. [29] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. InInternational Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=BJgklhAcK7. [30] Paul Ruvolo and Eric Eaton. Ella: An efﬁcient lifelong learning algorithm. In International Conference on Machine Learning, pages 507–515, 2013. [31] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International conference on machine learning, pages 1842–1850, 2016. [32] Juergen Schmidhuber. Evolutionary principles in self-referential learning. Diploma thesis, TU Munich, 1987. [33] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International Conference on Machine Learning, pages 4535–4544, 2018. [34] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In International Conference on Machine Learning, pages 4555–4564, 2018. 10[35] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pages 2990–2999, 2017. [36] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pages 3630–3638, 2016. [37] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Bayesian gradient descent: Online variational bayes learning with increased robustness to catastrophic forgetting and weight pruning. arXiv preprint arXiv:1803.10123, 2018. [38] Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. CA VIA: Fast context adaptation via meta-learning, 2019. 11A Meta Learning as Task Inferences Table 1: Meta learning methods as instances of the What & How framework Methods ct := FWhat(Dcxt t ) FHow(ct) MAML θt := θinit t −λin∇θLin( ˆf(·; θ),Dcxt t ) ˆf(·; θt) CNP rt := ⨁ xi,yi∈Dcxt t hθ(xi,yi) gθ(·,rt) LEO z′ t := zt −λin∇z′ Lin( ˆf(·; wt),Dcxt t ) w′ t ∼N(µd′ t (z′ t),diag(σd′ t (z′ t) 2 )) CA VIA ct := cinit −λin∇cLin( ˆf(·,c; θ),Dcxt t ) ˆf(·,ct; θ) B Experiment Details B.1 Model Conﬁgurations In all experiments, the number of samples Kin Eq. 2 is set to 10. In MetaCoG, the initial value of masks minit i is 0. In MetaELLA, we use k= 10components in the dictionary, and the initial value of latent code sinit i is set to 1/k = 0.1. Adam baseline were trained with the default hyperparameters recommended in [11]. The hyperparameters of other methods are tuned by a Bayesian optimization algorithm and are summarized in Table 2. Error bars for all experiments are standard deviations computed from 10 trials with different random seeds. B.2 Sine Curve Regression The amplitudes and phases of sine curves are sampled uniformly from[1.0,5.0] and [0,π], respectively. For both training and testing, input data points xare sampled uniformly from [−5.0,5.0]. The size of training and testing sets for each task are 5000 and 100, respectively. Each sine curve is presented for 1000 iterations, and a mini-batch of 128 data points is provided at every iteration for training. The 3-layer MLP has 50 units with tanh(·) non-linearity in each hidden layer. B.3 Label-Permuted MNIST All tasks are presented for 1000 iterations and the mini-batch size is 128. The network used in this experiment was a MLP with 2 hidden layers of 300 ReLU units. B.4 Omniglot We use 20 characters from each alphabet for classiﬁcation. Out of the 20 images of each character, 15 were used for training and 5 for testing. Each alphabet was trained for 200 epochs with mini-batch size 128. The CNN used in this experiment has two convolutional layers, both with 40 channels and kernel size 5. ReLU and max pooling are applied after each convolution layer, and the output is passed to a fully connected layer of size 300 before the ﬁnal layer. 12Table 2: Summary of hyperparameters used for the experiments. λin are inner loop learning rates. σ0 are initial values for standard deviation of the factorized Gaussian. ηis the learning rate of the mean in BGD update rule. γis the regularization strength for L1 norm of masks in MetaCoG. µis the regularization strength for L1 norm of latent code in MetaELLA. Hyperparameters Sine Curve Label-Permuted MNIST Omniglot MetaBGD λin 0.0419985 0.45 0.207496 σ0 0.0368604 0.050 0.0341916 η 5.05646 1.0 15.8603 MetaCoG λin 0.849212 10.000 5.53639 σ0 0.0426860 0.034 0.0133221 γ 1.48236e-6 1.000e-5 3.04741e-6 η 38.6049 1.0 80.0627 MetaElla λin 0.0938662 0.400 0.346027 σ0 0.0298390 0.010 0.0194483 µ 0.0216156 0.010 0.0124128 η 42.6035 1.0 24.7476 BGD σ0 0.0246160 0.060 0.0311284 η 20.3049 1.0 16.2192 13",
      "meta_data": {
        "arxiv_id": "1906.05201v1",
        "authors": [
          "Xu He",
          "Jakub Sygnowski",
          "Alexandre Galashov",
          "Andrei A. Rusu",
          "Yee Whye Teh",
          "Razvan Pascanu"
        ],
        "published_date": "2019-06-12T15:17:47Z",
        "pdf_url": "https://arxiv.org/pdf/1906.05201v1.pdf"
      }
    },
    {
      "title": "Gradient-based editing of memory examples for online task-free continual learning",
      "abstract": "We explore task-free continual learning (CL), in which a model is trained to\navoid catastrophic forgetting in the absence of explicit task boundaries or\nidentities. Among many efforts on task-free CL, a notable family of approaches\nare memory-based that store and replay a subset of training examples. However,\nthe utility of stored seen examples may diminish over time since CL models are\ncontinually updated. Here, we propose Gradient based Memory EDiting (GMED), a\nframework for editing stored examples in continuous input space via gradient\nupdates, in order to create more \"challenging\" examples for replay. GMED-edited\nexamples remain similar to their unedited forms, but can yield increased loss\nin the upcoming model updates, thereby making the future replays more effective\nin overcoming catastrophic forgetting. By construction, GMED can be seamlessly\napplied in conjunction with other memory-based CL algorithms to bring further\nimprovement. Experiments validate the effectiveness of GMED, and our best\nmethod significantly outperforms baselines and previous state-of-the-art on\nfive out of six datasets. Code can be found at https://github.com/INK-USC/GMED.",
      "full_text": "Gradient-based Editing of Memory Examples for Online Task-free Continual Learning Xisen Jin Arka Sadhu Junyi Du Xiang Ren University of Southern California {xisenjin, asadhu, junyidu, xiangren@usc.edu} Abstract We explore task-free continual learning (CL), in which a model is trained to avoid catastrophic forgetting in the absence of explicit task boundaries or identities. Among many efforts on task-free CL, a notable family of approaches are memory- based that store and replay a subset of training examples. However, the utility of stored seen examples may diminish over time since CL models are continually updated. Here, we propose Gradient based Memory EDiting (GMED), a framework for editing stored examples in continuous input space via gradient updates, in order to create more “challenging” examples for replay. GMED-edited examples remain similar to their unedited forms, but can yield increased loss in the upcoming model updates, thereby making the future replays more effective in overcoming catastrophic forgetting. By construction, GMED can be seamlessly applied in conjunction with other memory-based CL algorithms to bring further improvement. Experiments validate the effectiveness of GMED, and our best method signiﬁcantly outperforms baselines and previous state-of-the-art on ﬁve out of six datasets1. 1 Introduction Learning from a continuous stream of data – referred to as continual learning (CL) or lifelong learning – has recently seen a surge in interest, and many works have proposed ways to mitigate CL models’ catastrophic forgetting of previously learned knowledge [20, 32, 33]. Here, we study online task-free CL [3], where task identiﬁers and boundaries are absent from the data stream. This setting reﬂects many real-world data streams [ 6, 25] and offers a challenging testbed for online CL research. Memory-based methods, a prominent class of approaches used for task-free continual learning, store a small number of training examples (from the data stream) in a memory and replay them at the later training iterations [33, 34]. Existing methods operate over the original examples in the data-stream and focus on identifying samples to populate the memory [4, 9] and ﬁnding samples in the memory to be replayed [ 2]. However, for continually updating models, using stored-seen examples in their original form, may lead to diminishing utility over time — i.e., model may gradually memorize the stored examples after runs of replay, as the memory refreshes slowly. An alternate approach is to use generative models to create samples that suffers more from forgetting such as in GEN-MIR in [2]. In practice, training the generator network with limited data is challenging and leads to low-quality generated examples. Further, in the online learning setup, the generative model itself suffers from forgetting. As such, generative models perform worse than their memory counter-parts. In this paper, we present a novel memory-based CL framework, Gradient based Memory EDiting (GMED), which looks to directly “edit” (via a small gradient update) examples stored in the replay memory. These edited examples are stored (replacing their unedited counterparts), replayed, and further edited, thereby making the future replays more effective in overcoming catastrophic forgetting. 1Code can be found at https://github.com/INK-USC/GMED. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2006.15294v3  [stat.ML]  7 Dec 2021Since no explicit generative model is involved, GMED approach retains the advantages of memory- based methods and is straightforward to train only inducing a small computation overhead. The main consideration in allowing “editing” via a gradient update is the choice of the optimization objective. In light of recent work on designing alternative replay strategies [2, 7, 40], we hypothesize that “interfering” examples (i.e., past examples that suffer from increased loss) should be prioritized for replay. For a particular stored example, GMED ﬁnds a small update over the example (“edit”) such that the resulting edited example yields the most increase in loss when replayed. GMED additionally penalizes the loss increase in the edited example to enforce the proximity of the edited example to the original sample, so that the edited examples stay in-distribution. As a result, replaying these edited examples is more effective in overcoming catastrophic forgetting. Since GMED focuses only on editing the stored examples, by construction, GMED is modular, i.e., it can be seamlessly integrated with other state-of-the-art memory-based replay methods [2, 5, 26]. We demonstrate the effectiveness of GMED with a comprehensive set of experiments over six benchmark datasets. In general, combining GMED with existing memory-based approaches results in consistent and statistically signiﬁcant improvements with our single best method establishing a new state-of-art performance on ﬁve datasets. Our ablative investigations reveal that the gains realized by GMED are signiﬁcantly larger than those obtained from regularization effects in random perturbation, and can be accumulated upon data augmentation to further improve performance. To summarize, our contributions are two-fold: (i) we introduce GMED, a modular framework for task- free online continual learning, to edit stored examples and make them more effective in alleviating catastrophic forgetting (ii) we perform intensive set of experiments to test the performance of GMED under various datasets, parameter setups (e.g., memory size) and competiting baseline objectives. 2 Related Work Continual Learning studies the problem of learning from a data stream with changing data distribu- tions over time [20, 23]. A major bottleneck towards this goal is the phenomenon of catastrophic forgetting [33] where the model “forgets” knowledge learned from past examples when exposed to new ones. To mitigate this effect, a wide variety of approaches have been investigated such as adding regularization [ 1, 19, 30, 43], separating parameters for previous and new data [24, 36, 37], replaying examples from memory or a generative model [ 26, 33, 38], meta-learning [ 18]. In this work, we build on memory-based approaches which have been more successful in the online task-free continual learning setting that we study. Online Task-free Continual Learning[3] is a speciﬁc formulation of the continual learning where the task boundaries and identities are not available to the model. Due to its broader applicability to real- world data-streams, a number of algorithms have been adapted to the task-free setup [2, 14, 15, 22, 44]. In particular, memory-based CL algorithms which store a subset of examples and later replay them during training, have seen elevated success in the task-free setting. Improvements in this space have focused on: storing diverse examples as in Gradient-based Sample Selection (GSS) [4], and replaying examples with larger estimated “interference” as in Maximally Interfered Retrieval (MIR) with experience replay [2]. In contrast , GMED is used in conjunction with memory-based approaches and explicitly searches for an edited example which is optimized to be more “effective” for replay. Replay Example Construction. Moving away from replaying real examples, a line of works on deep generative replay [17, 35, 38] generates synthetic examples to replay with a generative model trained online. GEN-MIR [2] is further trained to generate examples that would suffer more from interference for replay. However, training a generative network is challenging, even more so in the online continual learning setup where the streaming examples are encountered only once leading to poorly generated examples. Moreover, the forgetting of generative networks themselves cannot be perfectly mitigated. As a result, these methods generally perform worse than their memory-based counter-parts. Instead of generating a new example via a generator network, GMED uses gradient updates to directly edit the stored example thereby retaining advantages of memory-based techniques while creating new samples. Novel examples can also be constructed via data augmentation (e.g. random crop, horizontal ﬂip) to help reduce over-ﬁtting over the small replay memory [5]. Unlike GMED, these data-augmentations are usually pre-deﬁned and cannot adapt to the learning pattern of the model. Constructing edited examples has also been used for adversarial robustness [ 31, 39]. The key difference lies in the 2(a) Estimate Forgetting 𝑑!:!#$𝑥%,𝑦% 𝑥%,𝑦%~𝑀 Memory Examples Train (b) Edit Memory Example ∇&𝑑!:!#$(𝑥%,𝑦%) −∇&𝛽ℓ(𝑥%,𝑦%;𝜃!)𝑥% 𝑥',𝑦'~𝐷Stream Examples Model𝜃!  Temporarily Updated Model𝜃!( 𝑑!:!#$𝑥%,𝑦%≔ℓ𝑥%,𝑦%;𝜃!(−ℓ(𝑥%,𝑦%;𝜃!) (c) Update Model Model𝜃!  Updated Model𝜃!#$ 𝜃!#$=𝜃!−∇)ℓ𝑥%(,𝑦%,𝑥',𝑦';𝜃!Train (𝑥',𝑦')(𝑥%(,𝑦%)Updated examples Memory𝑥%( Figure 1: A schematic of GMED framework. (a) Given an example from the data-stream (xD,yD) at time t, the model randomly draws an example from the memory (xm,ym) and estimates “interference” after one-step roll-out (Eq. 2). (b) The example drawn from the memory is then updated using our proposed editing objective (Eq. 3) via gradient ascent, resulting in (ˆxm,ym), and written back into the memory. (c) Finally, the model is updated using the edited example (ˆxm,ym) and the example from the data-stream (xD,yD) (Eq. 4). optimization objective: while adversarial construction focuses on ﬁnding mis-classiﬁed examples [13, 28], GMED aims at constructing “interfered” examples that would suffer from loss increase in future training steps. 3 Method We introduce the formulation of online task-free continual learning (Sec. 3.1) and then present our gradient-based memory editing method GMED (Sec. 3.2) and detail how GMED can be integrated with experience replay (Sec 3.3) and other memory-based CL algorithms (Sec. 3.4). 3.1 Problem Formulation In continual learning (CL), we consider a (potentially inﬁnite) stream Dof labeled examples (x,y), having a non-stationary data distribution – i.e., the data distribution p(x,y) evolves over time. Let (xt,yt) denote the labeled example (or a mini-batch of examples) received by the model at time-step t in the data-stream D. We assume, for simplicity, that (xt,yt) is generated by ﬁrst sampling a latent “task”z∼p(z; t), followed by sampling a data example from a conditional data distribution p(x,y|z), – i.e., (xt,yt) ∼p(x,y|z). Here p(z; t) is non-i.i.d and time-dependent. In task-free CL setting, the latent task zis not revealed to the model. Finally, we emphasize that while models in task-aware setup proceed to the next task only after convergence on the current task [19, 43], this work focuses on online task-free CL setup [ 8, 9]. It simulates a practical setting where models must perform online update on every incoming example, without accumulating examples within a task. Following above deﬁnitions, our goal is to learn a classiﬁcation model f(x; θ) on the data stream D that can preserve its performance over all the tasks inD. At time step tduring the training process, the model is updated to minimize a predeﬁned empirical loss ℓ(xt,yt; θ) on the newly received example (xt,yt), without increasing the loss on the previously visited examples (before t). Speciﬁcally, let pc(x,y; T) denotes the distribution of the examples visited until the time stepT. We look to minimize the expected loss Epc(x,y;T)ℓ(x,y; θ) – i.e., retaining performance on tasks encountered before T. 3.2 Gradient based Memory Editing (GMED) In online task-free continual learning, examples visited earlier cannot be accessed (revisited) and thus computing the loss over all the visited examples (in D) is not possible. To deal with this challenge, memory-based CL algorithms store and (continuously) maintain a set of visited examples in a ﬁxed-size memory and use them for replay or regularization in the future training steps [ 26, 33]. For instance, in Experience Replay (ER) [ 33] the examples are randomly sampled from the memory for replay; whereas recent works explore more sophisticated replay strategies, such as Maximally Interfered Retrieval with experience replay (ER-MIR) [2] where memory examples which interfere the most with the newly visited example are selected for replay. However, these algorithms only train over samples drawn from a small replay memory in their original form. As such, the utility of the stored examples could diminish over time as the model could potentially memorize these examples if the memory examples are refreshed in a slow pace – which is found often the case [5]. We address the above limitation in our proposed approach Gradient based Memory Editing (GMED) by allowing examples stored in the memory to be edited in the continuous input space, illustrated in Figure 1. The editing step is guided by an optimization objective instead of being pre-deﬁned and involves drawing examples from the memory, editing the drawn examples, replaying the edited 3examples and at the same time writing the edited examples back to the memory. We now state our optimization objectives of example editing followed by algorithmic details of GMED. Algorithm 1: Gradient Memory EDiting with ER (ER+GMED) 1: Input: learning rate τ, edit stride α, regularization strength β, decay rate γ, model parameters θ 2: Receives: stream example (xD,yD) 3: Initialize: replay memory M 4: for t= 1to T do 5: //when γ = 1, k is not required 6: (xm,ym) ∼M; k←replayed_time(xm,ym) 7: ℓbefore ←loss(xm,ym,θt); 8: ℓstream ←loss(xD,yD,θt) 9: //update model with stream examples 10: θ′ t ←SGD(ℓstream,θt,τ) 11: //evaluate forgetting of memory examples 12: ℓafter ←loss(xm,ym,θ′ t) 13: d←ℓafter −ℓbefore 14: //edit memory examples 15: x′ m ←xm + γkα∇x(d−βℓbefore) 16: ℓ= loss({(x′ m,ym),(xD,yD)},θt) 17: θt+1 ←SGD(ℓ,θt,τ) 18: replace (xm,ym) with (x′ m,ym) in M 19: reservoir_update (xD,yD,M) 20: end for Editing Objective. Clearly, the most crucial step involved in GMED is identifying “ how” should the stored examples in the memory be edited to reduce catastrophic forgetting of early examples. If (xm,ym) denotes an example drawn from the memory M, the goal is to de- sign a suitable editing function φto generate the edited example ˆxm where ˆxm=φ(xm). Such “editing” process can also be found in white-box adversarial example construction for robustness literature [13] where the task is to ﬁnd an “adver- sarial” example (x′,y) that is close to an orig- inal example (x,y) but is mis-classiﬁed by the model. Typically, adversarial example construc- tion utilizes the gradients obtained from the clas- siﬁer and edits the examples to move towards a different target class. While, in the context of continual learning, the editing objective should be different. We employ a similar hypothesis as previous works [2, 7, 40] that examples that are likely forgotten by models should be prior- itized for replay. Accordingly, we edit examples so that they are more likely to be forgotten in future updates. With this objective of editing in place, we detail the process of incorporating GMED with Experience Replay (ER). 3.3 The GMED Algorithm with ER Algorithm 1 summarizes the process and Figure 1 provides a schematic of the steps involved in GMED. At time step t, the model receives a mini-batch of the stream examples (xD,yD) from the training stream D, and randomly draws a same number of memory examples (xk m,ym) from the memory M, which we assume has already been drawn for replay for ktimes. We ﬁrst compute the “interference” (i.e., loss increase) on the memory example (xk m,ym) when the model performs one gradient update on parameters with the stream example (xD,yD). θ′ t = θt −∇θℓ(xD,yD; θt); (1) dt:t+1(xk m,ym) = ℓ(xk m,ym; θ′ t) −ℓ(xk m,ym; θt), (2) where θt and θ′ t are model parameters before and after the gradient update respectively. Then, we perform a step of gradient update on (xk m,yk m) by maximizing its “loss increase” in the next one step of training, while using a regularization term ℓ(xm,ym; θt) to penalize the loss increase evaluated with the current model checkpoint. The iterative update is written as, xk+1 m ←xk m + γkα∇x[dt:t+1(xk m,ym) −βℓ(xk m,ym; θt)], (3) where the hyper-parameter αcontrols the overall stride of the edit and is tuned with ﬁrst three tasks together with the regularization strength β. γis a decay factor of edit performed on the model. A decay factor γless than 1.0 could effectively preventxk m from drastically deviating from their original state, while γ = 1.0 indicates no decay. We note that we cannot perform constrained edits that strictly adhere to a distance budget w.r.t original examples, as it requires storing original examples which introduces extra memory overhead. Following Eq. 3, we perform a gradient update on xto increase its “interference”. The algorithm then discards θ′ t, and updates model parameters θt using the edited memory example (xk+1 m ,ym) and the stream example (xD,yD), in a similar way to ER. θt+1 = θt −∇θℓ({(xk+1 m ,ym),(xD,yD)}; θt). (4) We replace the original examples in the memory with the edited example. In this way, we continuously edit examples stored in the memory alongside training. 43.4 Applying GMED with Data Augmentation, MIR and GEM Since the process to edit the original examples in GMED is modular, we can integrate GMED with a range of existing memory-based CL algorithms. In addition to ER, we also explore ER with data augmentation (ERaug) [5], ER-MIR [2] and GEM [26] in our experiments. ERaug applies standard data augmentations ( e.g., random cropping, horizontal ﬂipping, denoted as T) to examples (xm,ym) drawn from the memory which are replayed at each time step. In ERaug+GMED, we edit original example xm and replay both the augmented example T(xm) and the edited example ˆxm. To keep the number of replayed examples the same, for ERaug method, we also replay both the edited example T(xm) and the original example xm. Finally, we write the edited example ˆxm to the memory. For integration with ER-MIR (denoted henceforth as MIR for brevity), recall that MIR retrieves and then replays the most “interfering” examples in the memory at each time-step. Thus, making GMED edits on the MIR-retrieved examples may induce a loop of further increasing “interference” of the examples that are already the most “interfering” ones. Instead, in our MIR+GMED implementation, we edit a mini-batch of examples randomly sampled from the memory — a process that is indepen- dent of the MIR replay operation2. This random sampling may help prevent GMED editing from intensifying potential biases created from the MIR retrieval process ( e.g., retrieved examples are edited and thus become more interfered). Similarly, For GEM+GMED, we also apply GMED to edit a mini-batch of randomly sampled examples. Details of the integrated algorithms (i.e., ERaug+GMED, MIR+GMED, and GEM+GMED) can be found in Algorithms 2, 3 and 4 in Appendix B. We leave more sophisticated integration of GMED to existing CL algorithms e.g. by optimizing the retrieval and editing operations jointly to future work. 4 Experiments Our experiments address the following research questions: (i) what are the gains obtained by integrating GMED with existing memory-based CL algorithms and how these gains compare across datasets, methods and memory sizes? (ii) how useful are GMED edits in alleviating catastrophic forgetting, and what part of it can be attributed to the design of the editing objective function? (iii) what role do the various components and parameters in the GMED play and how they affect the performance. In the rest of this section, we ﬁrst brieﬂy describe the datasets (Sec. 4.1) and the compared baselines (Sec. 4.2). We then detail our main results comparing across memory-based algorithms (Sec. 4.3), validate the effectiveness of GMED-edits and the editing objective in mitigating catastrophic forgetting (Sec. 4.4), followed by ablative study over its components (Sec. 4.5). 4.1 Datasets We use six public CL datasets in our experiments. Split / Permuted / Rotated MNIST are con- structed from the MNIST [21] dataset which contains images of handwritten digits. Split MNIST [12] creates 5 disjoint subsets based on the class labels and considers each subset as a separate task. The goal then is to classify over all 10 digits when the training ends. Permuted MNIST [ 12] consists of 10 tasks, where for a particular task a random permutation in the pixel space is chosen and applied to all images within that task. The model then has to classify over the 10 digits without knowing which random permutation was applied. Rotated MNIST [26] rotates every sample in MNIST by a ﬁxed angle between 0 to 180. Similar to the previous datasets, the goal is to classify over 10 digits without any knowledge of the angle of rotation. For all MNIST experiments, each task consists of 1,000 training examples following [ 2]. We also employ Split CIFAR-10 and Split CIFAR-100, which comprise of 5 and 20 disjoint subsets respectively based on their class labels. The model then classiﬁes over the space of all class labels. Similarly, Split mini-ImageNet [2] splits the mini-ImageNet [10, 42] dataset into 20 disjoint subsets based on their labels. The models classify over all 100 classes. Following the taxonomy of [41], the Split MNIST, Split CIFAR-10, Split CIFAR-100, and Split mini- ImageNet experiments are categorized under class-incremental setup, while Permuted and Rotated MNIST experiments belong to domain-incremental setup. We note that our results are not comparable 2This also ensures the integrated approach will replay the same number of examples as the baselines, yielding a fair comparison. 5Table 1: Mean and standard deviation of ﬁnal accuracy (%) for non-model-expansion-based approaches on 6 datasets. For Split mini-ImageNet and Split CIFAR-100 datasets, we set the memory size to 10,000 and 5,000 examples; we use 500 for other datasets. ∗and ∗∗over GMED methods indicate signiﬁcant improvement over the counterparts without GMED with p-values less than 0.1 and 0.05 respectively in single-tailed paired t-tests. We report results in 20 runs for GEM, ER, MIR, and ERaug and their GMED-integrated versions, and 10 runs for others. †over “previous SOTA” results indicates that the best GMED method (bolded for each dataset) outperforms the previous SOTA with statistically signiﬁcant improvement (p< 0.1). Methods / Datasets SplitMNIST PermutedMNIST RotatedMNIST SplitCIFAR-10 SplitCIFAR-100 Splitmini-ImageNet Fine tuning 18.80 ±0.6 66.34 ±2.6 41.24±1.5 18.49 ±0.2 3.06 ±0.2 2.84 ±0.4AGEM [8] 29.02 ±5.3 72.17 ±1.5 50.77±1.9 18.49 ±0.6 2.40 ±0.2 2.92 ±0.3GSS-Greedy [4] 84.16 ±2.6 77.43 ±1.4 73.66±1.1 28.02 ±1.3 19.53 ±1.3 16.19 ±0.7BGD [44] 13.54 ±5.1 19.38 ±3.0 77.94±0.9 18.23 ±0.5 3.11 ±0.2 24.71 ±0.8 ER [33] 81.07 ±2.5 78.65 ±0.7 76.71±1.6 33.30 ±3.9 20.11 ±1.2 25.92 ±1.2ER + GMED 82.67∗∗±1.9 78.86±0.7 77.09∗±1.3 34.84∗∗±2.2 20.93∗±1.6 27.27∗∗±1.8 MIR [2] 85.72 ±1.2 79.13 ±0.7 77.50±1.6 34.42 ±2.4 20.02 ±1.7 25.21 ±2.2MIR + GMED 86.52∗∗±1.4 79.25±0.8 79.08∗∗±0.8 36.17∗±2.5 21.22∗∗±1.0 26.50∗∗±1.3 ERaug[5] 80.14 ±3.2 78.11 ±0.7 80.04±1.3 46.29 ±2.7 18.32 ±1.9 30.77 ±2.2ERaug+ GMED 82.21∗∗±2.9 78.13±0.6 80.61∗±1.2 47.47∗±3.2 19.60±1.5 31.81∗±1.3 Previous SOTA 85.72 †±1.2 79.23±0.7 80.04†±1.3 46.29†±2.7 20.02†±1.7 30.77 †±2.2iid online 85.99 ±0.3 73.58 ±1.5 81.30±1.3 62.23 ±1.5 18.13 ±0.8 17.53 ±1.6iid ofﬂine (upper bound) 93.87±0.5 87.40 ±1.1 91.38±0.7 76.36 ±0.9 42.00 ±0.9 37.46 ±1.3 to works that employ a different setup over the same dataset ( e.g., results on domain-incremental Split CIFAR-100 are not comparable). 4.2 Compared Methods We compare against several task-free memory based continual learning methods namely, Experience Replay (ER) [ 33], Averaged Gradient Episodic Memory (AGEM) [ 8], Gradient based Sample Selection (GSS) [4], and Maximally Interfering Retrieval (MIR) [2]. We omit the generative replay method GEN-MIR proposed together in [2] as it underperforms their memory-based counterparts even on simple datasets such as Split MNIST. We also compare with data augmentation [ 5] such as random rotations, scaling, and horizontal ﬂipping applied to memory examples drawn for replay in ER, noted as ERaug (except for MNIST datasets). We also include regularization-based, model expansion-based and task-aware approaches, namely Bayesian Graident Descent (BGD) [ 44], Neural Dirichlet Process Mixture Model (CN- DPM) [22], Progressive Networks (Prog.NN) [36], Compositional Lifelong Learning [29], Graident Episodic Memory (GEM) [26] and Hindsight Anchor Learning (HAL) [7] respectively. Finally, we report three baseline models: (i) Fine Tuning, where no continual learning algorithms are used for online updates to model parameters, (ii) iid Online, where we randomly shufﬂe the data stream, so that the model visits an i.i.d. stream of examples, and (iii) iid Ofﬂine, where multiple passes over the dataset is allowed. Appendix A provides more details on compared methods and their implementation details. We build our proposed GMED approach upon four baselines, noted as ER+GMED, MIR+GMED, GEM+GMED, and ERaug+GMED. Implementation Details. We set the size of replay memory as 10Kfor split CIFAR-100 and split mini-ImageNet, and 500 for all remaining datasets. Following [8], we tune the hyper-parameters α (editing stride) and β(regularization strength) with only the ﬁrst three tasks. While γ(decay rate of the editing stride) is a hyper-parameter that may ﬂexibly control the deviation of edited examples from their original states, we ﬁnd γ=1.0 (i.e., no decay) leads to better performance in our experiments. Results under different γ setups are provided in Appendix C, and in the remaining sections we assume no decay is applied. For model architectures, we mostly follow the setup of [2]: for the three MNIST datasets, we use a MLP classiﬁer with 2 hidden layers with 400 hidden units each. For Split CIFAR-10, Split CIFAR-100 and Split mini-ImageNet datasets, we use a ResNet-18 classiﬁer with three times less feature maps across all layers. See Appendix C for more details. 4.3 Performance Across Datasets We summarize the results obtained by integrating GMED with different CL algorithms. In Table 1, we report the ﬁnal accuracy and the standard deviation and make the following key observations. Effectiveness of Memory Editing. As can be seen in Table 1, GMED signiﬁcantly improves perfor- mance on 5 datasets when built upon ER and MIR respectively. The improvement of MIR+GMED 6100 200 500 Mem. size 65 70 75 80 85 90Accuracy (%) * * * * * * ER ER-GMED MIR MIR-GMED (a) Split MNIST 100 200 500 Mem. size 60 70 80Accuracy (%) * * * * * ER ER-GMED MIR MIR-GMED (b) Rotated MNIST 100 200 500 Mem. size 20 30 40Accuracy (%) * * * * ER ER-GMED MIR MIR-GMED (c) Split CIFAR-10 2000 5000 10000 Mem. size 5 10 15 20 25 30Accuracy (%) * * ER ER-GMED MIR MIR-GMED (d) Split CIFAR-100 5000 10000 20000 Mem. size 15 20 25 30 35 40Accuracy (%) * * * * * * ER ER-GMED MIR MIR-GMED (e) Split mini-ImgNet Figure 2: Performance of ER, GMED +ER, MIR, and GMED +MIR with different memory sizes . For mini-ImageNet dataset, we use memory sizes of 1K, 5K, 10K, and 20K examples; for Split CIFAR-100 dataset, we use 1K, 2K, 5Kand 10K; for other datasets, we use 100, 200, 500, and 1000. ∗indicates whether the improvement of ER+GMED or MIR+GMED is signiﬁcant with p< 0.05. corroborates that the optimization in the continuous input spaces of GMED is complementary to sample selection over real examples as in MIR. We also notice signiﬁcant improvement of ERaug over GMED on 5 datasets. This indicates that the beneﬁts induced by GMED go beyond regularization effects used to mitigate over-ﬁtting. Table 2: Comparison with model-expansion-based ap- proaches under the same memory overhead as CN-DPM. The overhead is the size of the replay memory plus the extra model components (e.g. a generator or modules to solve individual tasks), shown in the equivalent number of memory examples (#. Mem). †indicates quoted numbers are taken from the respective papers. Method Split MNIST Split CIFAR-10 Split CIFAR-100 Acc. #. Mem Acc. #. Mem Acc. #. Mem CN-DPM† 93.23 2,581 45.21 6,024 20.10 21,295Prog. NN 89.46 9,755 49.68 6,604 19.17 31,766CompCL 91.27 9,755 45.62 6,604 20.51 31,766ER 92.67 2,581 62.96 6,024 21.79 21,295ER+GMED 94.162,581 63.28 6,024 22.12 21,295 Comparison across CL methods. From Ta- ble 1, we ﬁnd MIR+GMED achieves the best performance on Split MNIST, Permuted MNIST, and Split CIFAR-100 datasets; while on Rotated MNIST, Split CIFAR-10 and Split mini-ImageNet dataset, ERaug+GMED achieves the best performance. Performance of the best performing GMED method could signiﬁcantly improve over previous SOTA on ﬁve datasets. We further compare with an non-memory based CL approach, CN-DPM [ 22], which employs a generative model, a dynamically expanding classiﬁer, and utilizes a short-term memory (STM). Following the setup in [16], we set the memory size for GMED so that two methods introduces the same amount of the overhead. Table 2 shows the results of ER, ER+GMED and the reported results of CN-DPM. Interestingly, ER by itself achieves comparable performance to model expansion approaches. ER+GMED further outperforms CN-DPM without any extra memory overhead compared to ER. Similarly, GMED outperforms task-aware model expansion approaches such as Prog. NN and the recently proposed compositional model expansion (CompCL) with a smaller memory overhead. Table 3: Performance of methods over data streams with fuzzy task boundaries . In this setup, examples from the next tasks are introduced and gradually dom- inate the stream when half of the examples from the current task is visited. * indicates whether the improve- ment is signiﬁcant (p< 0.05) Methods / DatasetsSplitMNIST SplitCIFAR-10 Splitmini-ImageNet Vanilla 21.53±0.1 20.69±2.4 3.05 ±0.6ER 79.74±4.0 37.15±1.6 26.47 ±2.3MIR 84.80±1.9 38.70±1.7 25.83 ±1.5ERaug 81.30±2.0 47.97±3.5 30.75 ±1.0 ER + GMED 82.73∗±2.6 40.57∗±1.7 28.20∗±0.6MIR + GMED 86.17±1.7 41.22∗±1.1 26.86∗±0.7ERaug+ GMED 82.39∗±3.7 51.38∗±2.2 31.83±0.8 Performance under Various Memory Sizes. Figure 2 shows the performance of ER, ER+GMED, MIR, and MIR+GMED under var- ious memory sizes. The improvement on Split MNIST and Split mini-ImageNet are signiﬁcant with p< 0.05 over all memory size setups. On Rotated MNSIT and Split CIFAR-10 the im- provements are also mostly signiﬁcant. The improvement on Split CIFAR-100 is less com- petitive, probably because the dataset is overly difﬁcult for class-incremental learning, from the accuracy around or less than 20% in all setups. Performance on Data Streams with Fuzzy Task Boundaries. The experiments in Table 1 assume a clear task boundary. In Table 3, we report the result of using data-streams with fuzzy task boundaries on four datasets. We leave the complete results in Table 9 in Appendix. In this setup, the probability density of a new task grows linearly starting from the point where 50% of examples of 70 1/4 T 1/2 T 3/4 T T Time step 1.0 0.5 0.0 0.5 1.0 Cosine Similarity Split MNIST Rotated MNIST Figure 3: Cosine similarity between the optimal editing direction and direction from ER+GMED over training. The averaged similarity is0.523± 0.014 and 0.035 ±0.009. (1)  (2)  (3)  (4)  (5)  (6)  (7)  (8)  (9)  (10)  (11)  (12)  (13)  (14)  (15)  (16) Figure 4: Visualization of the edited examples in Split MNIST in the ER-GMED approach. The ﬁrst two rows show examples before and after editing, and the third row shows the differences. the current task are visited. In particular, GMED improves performance in three datasets across ER, MIR and ERaug. Effectiveness of GMED in Task-Aware Setting. We additionally compare performance of other task-aware approaches (HAL, GEM, GEM+GMED) in Appendix H. 4.4 Ablation to Study the Effect of Memory-Editing We present a set of experiments to validate that the gains obtained through the memory-editing step of GMED are indeed helpful towards alleviating catastrophic forgetting. Further, we show that these gains are distinct from those obtained through random perturbations or simple regularization effects. Comparison to Optimal Editing Direction. At a particular time-step t, GMED identiﬁes the edit direction using the stream example (xD,yD) and the memory sample (xm,ym). To validate whether the edits proposed by GMED are indeed helpful, we compare GMED-edit to an “Optimal Edit” that minimizes the loss increase (“interference”) of all early training examples in one future time step. To compute this Optimal Edit, we note that the total loss increase over all previously encountered examples would be d1:t t:t+1 = ∑ t i=1 dt:t+1(xi,yi) = ∑ t i=1[ℓ(xi,yi; θt+1) −ℓ(xi,yi; θt)], where θt+1 = θt −∇θℓ(xm,ym; θt) −∇θℓ(xD,yD; θt) is the model parameters after the training update on (xD,yD) and (xm,ym). The Optimal Edit direction for the memory example xm would be the gradient of d1:t t:t+1 w.r.t. xm. Computing such optimal edits requires access to early training examples and is not practical in an online continual learning setup; we present it only for ablative purposes. Figure 3 shows the cosine similarity of update directions (i.e., the gradient ofxm) between GMED and the optimal editing strategy over Split MNIST and Rotated MNIST datasets. The averaged similarity over time is 0.523 ±0.014, 0.035 ±0.009 on two datasets, averaged across 10 runs. Recall that random editing has an expectation of zero similarity. On Split MNIST dataset, where the improvement is the most signiﬁcant, we notice a high similarity between the update directions of GMED and optimal editing. On Rotated MNIST where the improvement is less signiﬁcant, the similarity is still positive on average but is lower than Split MNIST. It implies GMED-edits are generally consistent with explicitly reducing forgetting, but there is still space to improve. The results also imply the whether the edits of GMED aligns with editing is highly dependent on certain properties of datasets. We further include the classiﬁcation performance of optimal editing in Appendix L. Table 4: Alternative memory editing objectives or simply increasing the number of replayed examples. Comparison of Random Edit, Adversarial Edit (Adv. Edit) to our proposed objective in GMED. ∗indicates signiﬁcant improvement (p<0.05) compared to adver- sarial edit. Methods / DatasetsRotatedMNIST SplitCIFAR-10 Splitmini-ImageNet ER 76.71±1.6 33.30±3.9 25.92±1.2ER + Random Edit76.42±1.3 32.26±1.9 26.50±1.3ER + Adv. Edit76.13±1.5 31.69±0.8 26.09±1.6ER + GMED 77.50±1.6 34.84∗±2.2 27.27∗±1.8 MIR+Extra 1 batch78.07±1.1 33.81±2.3 24.94±1.5MIR+Extra 2 batches77.10±1.4 32.36±2.8 24.65±1.5 MIR 77.50±1.6 34.42±2.4 25.21±2.2MIR + Random Edit77.19±1.0 35.39±3.0 24.86±0.7MIR + Adv. Edit78.06±1.5 35.79±0.4 25.48±1.3MIR + GMED 79.08∗±0.8 36.17±2.5 26.29∗±1.2 Comparison with alternative Editing Objec- tives. While GMED objective editing correlates with that of Optimal Edit, we further validate the choice of the objective function and consider two alternatives to the proposed editing objec- tive (Eq. 3): (i) Random Edit: memory examples are updated in a random direction with a ﬁxed stride; (ii) Adversarial Edit: following the litera- ture of adversarial example construction [13], the edit increases the loss of memory by following the gradient sgn ∇xℓ(xm,ym; θ). We report the comparison in Table 4. We notice ER+Random Edit outperforms ER on split mini-ImageNet, which indicates adding ran- dom noise to memory examples helps in regu- larization. Even so, GMED-edits are as good or 80.5 1.0 2.0 5.0 Editing stride  78 80 82 84 86Accuracy (%) (a) Split MNIST 0.01 0.02 0.05 0.1 Editing stride  30 32 34 36 38 40Accuracy (%)  (b) Split CIFAR-10 0.2 0.5 1.0 5.0 Editing stride  24 26 28 30Accuracy (%)  (c) Split mini-ImgNet 0.0 0.001 0.01 0.1 Reg. strength  78 80 82 84 86Accuracy (%)  (d) Split MNIST 0.0 0.001 0.01 0.1 Reg. strength  25 30 35 40Accuracy (%)  (e) Split CIFAR-10 0.0 0.001 0.01 0.1 Reg. strength  24 26 28 30Accuracy (%)  (f) Split mini-ImgNet Figure 5: Parameter sensitivity analysis of the editing stride α(a,b,c) and the regularization strength β(d,e,f) for ER+GMED across various datasets. The dashed horizontal line indicates the performance of ER. outperforms both random and adversarial edits across multiple datasets and models. This validates our choice of the editing objective. Comparison to Increasing the Number of Replayed Examples. Recall that in MIR-GMED, we sample two independent subset of memory examples to perform editing and replay (Sec. 3.4). For a fairer comparison, we replay one (or more) extra subset of examples. Table 4 shows the performance does not improve as replaying more examples. We hypothesize that the performance is bottle-necked by the size of the memory and not the number of examples replayed. Case study on Edited Memory Examples.In Figure 4, we show visualizations of editing of memory examples. The examples are drawn from ﬁrst two task (0/1, 2/3) in the Split MNIST dataset using ER+GMED. The ﬁrst and second row show the original and edited examples, noted as xbefore and xafter respectively. The third row shows the difference between two∆x= xafter −xbefore. While there is no signiﬁcant visual differences between original and edited examples, in the difference ∆x, we ﬁnd examples with exaggerated contours (e.g. examples (1) and (12)) and blurring (e.g. examples (2), (3), (5), and (6)). Intuitively, ambiguous examples are exaggerated while typical examples are blurred. Our visualizations supports this intuition: examples (1) and (12) are not typically written digits, while examples (2), (3), (5), and (6) are typical. Appendix D provides a t-SNE [27] plot of the edit vector. 4.5 Analysis on the GMED Framework Here, we analyze and ablate the various components used in the GMED framework and their effect on the ﬁnal performance. Hyper-parameter Sensitivity. In Figure 5 we plot the sensitivity of the performance of GMED with respect to two hyper-parameters: the editing stride αand the regularization strength β(Eq. 3). Clearly, ER+GMED outperforms ER over a broad range of αand βsetups. Furthermore, in Figure 5 (d,e,f), better performance for non-zero βconﬁrms the beneﬁt of the regularization term. Recall that in our main experiments we tune the hyper-parameters αand βwith only ﬁrst three tasks; we note the chosen hyperparameters improve the performance despite they are not always optimal ones. Computational Efﬁciency. We analyze the additional forward and backward computation required by ER+GMED and MIR. Compared to ER, ER+GMED adds 3 forward and 1 backward passes to estimate loss increase, and 1 backward pass to update the example. In comparison, MIR adds 3 forward and 1 backward passes with2 of the forward passes are over a larger set of retrieval candidates. In our experiments, we found GMED has similar training time cost as MIR. In Appendix B, we report the wall-clock time, and observe the run-time of ER+GMED is 1.5 times of ER. Table 5: Editing examples for multiple gradient steps in ER+GMED. We tune the editing stride ( α) using the ﬁrst three tasks as described in Sec. 3.3. Methods / DatasetsSplitMNIST RotatedMNIST SplitCIFAR-10 Splitmini-ImageNet 1-step Edit 82.21±2.9 77.50±1.6 34.84±2.2 27.27±1.83-step Edit 82.55±1.9 77.37±1.7 34.93±1.4 27.36±1.75-step Edit 83.11 ±1.9 77.53±1.6 36.82±1.8 26.36±2.0 Increasing Steps of Editing. For experi- ments in Table 1, we performed one editing step over the sampled memory examplexm at time step t. In general, we can increase the number of editing steps. The direction for the edit is computed at each step, which makes the approach different from increas- ing the editing stride (α). Table 5 indicates that 3-step and 5-step edits in general don’t lead to signiﬁcant improvement in performance while incurring additional computational cost. As such, we use only 1-edit step across all our experiments. 95 Conclusion In this paper, we propose Gradient based Memory Editing (GMED), a modular framework for memory-based task-free continual learning where examples stored in the memory can be edited. Importantly, memory examples edited by GMED are encouraged to remain in-distribution but yield increased loss in the upcoming model updates, and thus are more effective at alleviating catastrophic forgetting. We ﬁnd that combining GMED with existing memory-based CL approaches leads to consistent improvements across multiple benchmark datasets, only inducing a small computation overhead. Finally, we perform a thorough ablative study to validate that the gains obtained by GMED can indeed be attributed to its editing operation and careful choice of editing objective. Acknowledgements This research is supported in part by the Ofﬁce of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, the DARPA MCS program under Contract No. N660011924033, the Defense Advanced Research Projects Agency with award W911NF-19-20271, NSF IIS 2048211, NSF SMA 1829268, and gift awards from Google, Amazon, JP Morgan and Sony. We would like to thank all the collaborators in USC INK research lab for their constructive feedback on the work. References [1] Tameem Adel, Han Zhao, and Richard E. Turner. Continual learning with adaptive weights (CLAW). In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [2] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. InNeurIPS, 2019. [3] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 11254–11263. Computer Vision Foundation / IEEE, 2019. [4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Flo- rence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors,Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 11816–11825, 2019. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Simone Calderara. Rethinking experi- ence replay: a bag of tricks for continual learning. ArXiv, abs/2010.05595, 2020. [6] Massimo Caccia, P. Rodríguez, O. Ostapenko, Fabrice Normandin, Min Lin, L. Caccia, Issam H. Laradji, I. Rish, Alexande Lacoste, D. Vázquez, and Laurent Charlin. Online fast adaptation and knowledge accumulation: a new approach to continual learning. NeurIPS, 2020. [7] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip H. S. Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. ArXiv, abs/2002.08165, 2020. [8] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with A-GEM. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [9] Aristotelis Chrysakis and Marie-Francine Moens. Online continual learning from imbalanced data. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1952–1961. PMLR, 2020. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248–255. IEEE Computer Society, 2009. 10[11] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty- guided continual learning with bayesian neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe- view.net, 2020. [12] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. [13] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver- sarial examples. ICLR, 2015. [14] James Harrison, Apoorva Sharma, Chelsea Finn, and Marco Pavone. Continuous meta-learning without tasks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [15] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A. Rusu, Yee Whye Teh, and Razvan Pascanu. Task agnostic continual learning via meta learning. ArXiv, abs/1906.05201, 2019. [16] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating con- tinual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. [17] Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zhengwei Tao, Jinwen Ma, Dongyan Zhao, and Rui Yan. Overcoming catastrophic forgetting for continual learning via model adaptation. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [18] Khurram Javed and Martha White. Meta-learning representations for continual learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1818–1828, 2019. [19] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. [20] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, A. Leonardis, Gregory Slabaugh, and T. Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation tasks. arXiv: Computer Vision and Pattern Recognition, 2019. [21] Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits, 1998. URL http://yann. lecun. com/exdb/mnist, 10:34, 1998. [22] Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model for task-free continual learning. InInternational Conference on Learning Representations, 2020. [23] Timothée Lesort, V . Lomonaco, A. Stoian, D. Maltoni, David Filliat, and N. Rodríguez. Con- tinual learning for robotics: Deﬁnition, framework, learning strategies, opportunities and challenges. Inf. Fusion, 58:52–68, 2020. [24] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 3925–3934. PMLR, 2019. [25] B. Liu. Learning on the job: Online lifelong and continual learning. In AAAI, 2020. [26] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors,Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 6467–6476, 2017. 11[27] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605, 2008. [28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [29] Jorge A Mendez and ERIC EATON. Lifelong learning of compositional structures. In Interna- tional Conference on Learning Representations, 2021. [30] Cuong V . Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] Nicolas Papernot, P. McDaniel, Ian J. Goodfellow, S. Jha, Z. Y . Celik, and A. Swami. Practical black-box attacks against machine learning. Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 2017. [32] G. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and S. Wermter. Continual lifelong learning with neural networks: A review. Neural networks : the ofﬁcial journal of the International Neural Network Society, 113:54–71, 2019. [33] Anthony V . Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123–146, 1995. [34] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Gregory Wayne. Experience replay for continual learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 348–358, 2019. [35] Mohammad Rostami, Soheil Kolouri, and Praveen K. Pilly. Complementary learning for overcoming catastrophic forgetting using experience replay. In Sarit Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 3339–3345. ijcai.org, 2019. [36] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. ArXiv, abs/1606.04671, 2016. [37] Joan Serrà, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stock- holmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 4555–4564. PMLR, 2018. [38] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep gen- erative replay. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors,Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 2990–2999, 2017. [39] Yang Song, Rui Shu, Nate Kushman, and S. Ermon. Constructing unrestricted adversarial examples with generative models. In NeurIPS, 2018. [40] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [41] Gido M. van de Ven and Andreas S. Tolias. Three scenarios for continual learning. ArXiv, abs/1904.07734, 2019. [42] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match- ing networks for one shot learning. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Sys- tems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 3630–3638, 2016. 12[43] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of Machine Learning Research, pages 3987–3995. PMLR, 2017. [44] Chen Zeno, Itay Golan, E. Hoffer, and Daniel Soudry. Task agnostic continual learning using online variational bayes. ArXiv, abs/1803.10123, 2018. 13A Implementation Details of the Compared Methods We included detailed descriptions, and implementation details of some selected baselines in this section. • Experience Replay (ER) [33, 34] stores examples in a ﬁx-sized memory for future replay. We use reservoir sampling to decide which examples to store and replace. Following prior works [2, 3, 7], at each time step we draw the same number of examples as the batch size from the memory to replay, which are both set to 10. The algorithm is applied as is to the task-free scenario. • Gradient Episodic Memory (GEM) [26] also stores examples in a memory. Before each model parameter update, GEM projects the gradient of model parameters so that the update does not incur loss increase on any previous task. The approach is however not task-free. We used a memory strength γ = 0.5 for Split MNIST and Rotated MNIST following the original work and γ = 0.5 following [4]. We applied the same γfor remaining datasets. • Averaged Gradient Episodic Memory (AGEM) [8] prevents the average loss increase on a randomly drawn subset of examples from the memory. We drawk= 256 examples to compute the regularization at each iteration. Note than this setup of kis much larger than the number of examples drawn for replay (k= 10) in ER approaches. The approach is task-free. • Bayesian Gradient Descent (BGD) [44] is a regularization-based continual learning algorithm. It adjusts the learning rate for parameters by estimating their certainty, noting their importance to previous data. We tune the initial standard deviation of parameters and set it as 0.011 for Split CIFAR-10 and Split CIFAR-100, 0.05 for permuted MNIST, and 0.017 for Split MNIST and Rotated MNIST. We also tune the “learning rate” hyperparameterηand set it as 8 for Split CIFAR-10 and Split CIFAR-100, and 1 for Permuted MNIST, Split MNIST, and Rotated MNIST. The approach is task-free. • Gradient based Sample Selection (GSS) [4] builds upon Experience Replay (ER) by encouraging the diversity in the stored examples. We use GSS-Greedy, which is the best performing variant. The approach is task-free. • Hindsight Anchor Learning (HAL) [7] learns a pseudo “anchor” example per task per class in addition to the replay memory by maximizing its estimated forgetting, and tries to ﬁx model outputs on the anchors at training. However, unlike GMED, HAL estimates forgetting by comparing the loss before and after the model performs updates with the replay memory examples (and thus forgetting is estimated with “hindsight”). We refer to hyperparameters in the original work and set the mean embedding strength γ = 0.1, anchor learning rate as 0.001, gradient steps on anchor kas 100, and ﬁne-tune 50 epochs on the memory to estimate forgetting across datasets. The approach is not task-free. • Maximally Interfering Retrieval (MIR) [2] improves ER by selecting top forgettable examples from the memory for replay. Following the ofﬁcial implementation, we evaluate forgetting on a candidate set of 25 examples for mini-ImageNet dataset, and 50 examples for others. While the approach is task-free, the ofﬁcial implementation ﬁlter out memory examples that belong to the same task as the current data stream, which assumes knowledge about tasks boundaries. We remove this operation to adapt the method to the task-free setup. Therefore, our results are not directly comparable to the ofﬁcial results. • Neural Dirichlet Process Model for Continual Learning (CN-DPM) [22] is a task-free model- expansion based continual learning algorithm. We report the ofﬁcial results in the paper. In the comparison study between ER/ER+GMED with CN-DPM, for the base model in ER/ER+GMED, we use the full expanded model in CN-DPM (i.e., the model architecture when the training ends in CN-DPM). We use the same optimizer and the learning rate as CN-DPM for compared methods in this set of experiments. • Progressive Neural Networks (Prog. NN) [36] is a task-aware model-expansion base approach. We treat linear layers and ResNet blocks as basic components of expansion in MLP and ResNet-18 models, i.e., the model creates a new linear layer or ResNet block at each layer of the model when it encounters a new task. In addition to added components, it also introduces the connectivity between the current component and the components in previous layers as learnable parameters. The model before expansion has the same architecture and sizes as models used in other approaches such as ER. 14• Compositional Continual Learning (CompCL) [29] is another task-aware model-expansion based approach. When the model encounters a new task, the approach performs two-stage model updates by ﬁrst learning the architecture over existing and one new model component, and then per- forming updates on parameters of components. While the original work adds one shared component for all layers, we ﬁnd it helpful to add separate components for all layers, possibly because the tasks in our setup have low similarity. B Algorithmic and Implementation Details for GMED variants Algorithm 2: Memory Editing with ERaug (ERaug+GMED) Input: learning rate τ, edit stride α, regularization strength β, decay rate γ, model parameters θ Receives: stream example (xD,yD) Initialize: replay memory M for t= 1to T do // draw a random mini-batch for edit and augmentation respectively (xe,ye) ∼M k←replayed_time(xe,ye) ℓbefore ←loss(xe,ye,θt) ℓstream ←loss(xD,yD,θt) //update model parameters with stream examples, discarded later θ′ t ←SGD(ℓstream,θt,τ) //evaluate forgetting of memory examples ℓafter ←loss(xe,ye,θ′ t) d←ℓafter −ℓbefore //edit memory examples x′ e ←xe + γkα∇x(d−βℓbefore) replace (xe,ye) with (x′ e,ye) in M (xa e,ye) ←data_augmentation(xe,ye) //replay edited and augmented examples ℓ= loss({(xa e,ye),(x′ e,ye),(xD,yD)},θt) θt+1 ←SGD(ℓ,θt,τ) reservoir_update(xD,yD,M) end for Algorithmic Details. We present the algorithmic details of ER aug+GMED, MIR+GMED and GEM+GMED in Algorithms 2, 3 and 4. The main difference in MIR+GMED and GEM+GMED compared to ER+GMED is that we edit a separate mini-batch of memory examples from the mini- batch used for replay or regularization. Similarly, in ER aug+GMED, we additionally replay a mini-batch of edited examples after data augmentation. The motivations for such design are discussed in Sec. 3.4. Implementation Details. We implemented our models with PyTorch 1.0. We train our models with a single GTX 1080Ti or 2080Ti GPU, and we use CUDA toolkit 10.1. We use a mini-batch size of10 throughout experiments for all approaches. Training Time Cost. For ER+GMED, training on Split MNIST, Permuted MNIST, and Rotated MNIST takes 7 seconds, 30 seconds, and 31 seconds respectively (excluding data pre-processing). Training on Split CIFAR-10, Split CIFAR-100, and Split mini-ImageNet takes 11 minutes, 14 minutes, and 46 minutes respectively. In comparison, training with ER takes 6 seconds, 23 seconds, 16 seconds, 7 minutes, 10 minutes, and 32 minutes respectively on six datasets. Experiment Conﬁguration for Comparison to Model-Expansion based Methods. Model ex- pansion based approaches such as Progressive Networks [36], Compositional Lifelong Learning [29], and CN-DPM [22] involves additional overhead by introducing new model parameters over time. We compute such overhead as the number of extra parameters when the model has fully expanded. Let ne, nb be the number of parameters in a fully-expanded model and a regular model in non-expansion based approaches ( e.g., ER) respectively, and the difference is ∆n = ne −nb. We report such 15Algorithm 3: Memory Editing with MIR (MIR+GMED) Input: learning rate τ, edit stride α, regularization strength β, decay rate γ, model parameters θ Receives: stream example (xD,yD) Initialize: replay memory M for t= 1to T do // draw a random mini-batch for edit (xe,ye) ∼M k←replayed_time(xe,ye) ℓbefore ←loss(xe,ye,θt) ℓstream ←loss(xD,yD,θt) // retrieve a separate mini-batch of examples for replay with MIR (xm,ym) ←MIR-retrieve(M,θ) //update model parameters with stream examples, discarded later θ′ t ←SGD(ℓstream,θt,τ) //evaluate forgetting of memory examples ℓafter ←loss(xe,ye,θ′ t) d←ℓafter −ℓbefore //edit memory examples x′ e ←xe + γkα∇x(d−βℓbefore) replace (xe,ye) with (x′ e,ye) in M //replay examples retrieved by MIR ℓ= loss({(xm,ym),(xD,yD)},θt) θt+1 ←SGD(ℓ,θt,τ) reservoir_update(xD,yD,M) end for Algorithm 4: Memory Editing with GEM (GEM+GMED) Input: learning rate τ, edit stride α, regularization strength β, decay rate γ, model parameters θ Receives: stream example (xD,yD) Initialize: replay memory M for t= 1to T do // draw a random mini-batch for edit (xe,ye) ∼M k←replayed_time(xe,ye) ℓbefore ←loss(xe,ye,θt) ℓstream ←loss(xD,yD,θt) //update model parameters with stream examples, discarded later θ′ t ←SGD(ℓstream,θt,τ) //evaluate forgetting of memory examples ℓafter ←loss(xe,ye,θ′ t) d←ℓafter −ℓbefore //edit memory examples x′ e ←xe + γkα∇x(d−βℓbefore) replace (xe,ye) with (x′ e,ye) in M //Regularized model updates with GEM using the full memory GEM_regularized_update(xD,yD,M,θ ) //Update the replay memory following GEM memory_update(xD,yD,M) end for 16overhead in the equivalent number of training examples that can be stored in a memory: given an input image with dimension (c,h,w ) and the overhead ∆n, the equivalent number of examples is computed as 4∆n/(chw+ 1). The coefﬁcient 4 is added because storing a model parameters in ﬂoat32 type takes up 4 times as much memory as a pixel or a label y. For CN-DPM, the short-term memory used to store past examples also counts as overhead. C Hyper-parameter Setup Throughout experiments other than results reported in Table 2, we use SGD optimizer with a learning rate of 0.05 for MNIST datasets, 0.1 for Split CIFAR-10 and Split mini-ImageNet datasets, and 0.03 for the Split CIFAR-100 dataset across all approaches. The learning rate is tuned with ER method and ﬁxed for all other approaches. For results reported in Table 2, we use the same optimizer and learning rates as CN-DPM. See Sec. A for setup of method speciﬁc hyperparameters. GMED variants. Besides, GMED introduces two additional hyper-parameters: the stride of the editing α, and the regularization strength β. As we assume no access to the full data stream in the online learning setup, we cannot select hyper-parameters according to validation performance after training on the full stream. Therefore, we tune the hyper-parameters with only the validation set of ﬁrst three tasks, following the practice in [8]. The tasks used for hyper-parameter search are included for computing the ﬁnal accuracy, following [11]. We perform a grid search over all combinations ofα and βand select the one with the best validation performance on the ﬁrst three tasks. We selectαfrom [0.01,0.03,0.05,0.07,0.1,0.5,1.0,5.0,10.0], and select β from [0,10−3,10−2,10−1,1]. We tune two hyper-parameters on ER+GMED and share it across MIR+GMED and GEM+GMED. We tune a separate set of hyper-parameters for ERaug+GMED. Table 6 reports the optimal hyper-parameters selected for each dataset. Table 6: Hyperparamters of the editing stride and the regularization strength selected for ER+GMED. Dataset / Hyper-param Editing strideα Regularization strengthβ ER+GMED, MIR+GMED, GEM+GMED Split MNIST 5.0 0.01 Permuted MNIST 0.05 0.001 Rotated MNIST 1.0 0.01 Split CIFAR-10 0.05 0.001 Split CIFAR-100 0.01 0.001 Split mini-ImageNet 1.0 0.1 ERaug+ GMED Split MNIST 5.0 0.001 Permuted MNIST 0.05 0.001 Rotated MNIST 1.0 0.01 Split CIFAR-10 0.07 0.01 Split CIFAR-100 0.05 0.001 Split mini-ImageNet 0.5 0.0 Sensitivity to γ. Table 7 shows the performance of GMED under various decay rates of the editing stride γ. We ﬁnd γ = 1.0 (i.e., no decay) consistently outperforms performance when γis less than 1. It implies it is not necessary to explicitly discourage the deviation of edited examples from original examples with the hyper-parameter γ. Table 7: Sensitivity of the performance of GMED to the decay rate of the editing stride (γ). Methods / DatasetsSplitMNIST PermutedMNIST RotatedMNIST SplitCIFAR-10 SplitCIFAR-100 Splitmini-ImageNet ER 81.07±2.5 78.65±0.7 76.71±1.6 33.30±3.9 20.11±1.2 25.92 ±1.2ER + GMEDγ=0.9 82.28±1.7 79.07±0.6 77.22±1.2 33.85±1.4 20.06±1.8 26.92 ±1.9ER + GMEDγ=0.99 82.60±2.2 79.15±0.6 77.35±1.3 34.10±3.4 19.90±1.5 27.69 ±0.7ER + GMEDγ=1.0 82.67±1.9 78.86±0.7 77.09±1.3 34.84±2.2 20.93±1.6 27.27 ±1.8 MIR 85.72±1.2 79.13±0.7 77.50±1.6 34.42±2.4 20.02±1.7 25.21 ±2.2MIR + GMEDγ=0.9 85.67±2.2 79.99±0.7 78.45±1.3 34.98±0.5 19.76±1.7 25.96 ±1.2MIR + GMEDγ=0.99 86.76±1.2 79.76±0.7 78.61±0.6 35.78±3.2 20.48±1.7 27.70 ±1.3MIR + GMEDγ=1.0 86.52±1.4 79.25±0.8 79.08±0.8 36.17±2.5 21.22±1.0 26.50 ±1.3 17D T-SNE Visualization In Figure 6, we show the t-SNE [ 27] visualization of the editing vector ∆x = xafter −xbefore for examples from ﬁrst 2 tasks in Split MNIST. We note that the editing vectors cluster by the labels of the examples. It implies the editing performed is correlated with the labels and is clearly not random. Label 0 Label 1 Label 2 Label 3 Figure 6: A t-SNE visualization of the editing performed on data examples. We use labels from the ﬁrst two tasks in Split MNIST. E Dataset Details Following [2], we limit the number of training examples per task to 1,000 for all MNIST experiments, including Split MNIST, Permuted MNIST, and Rotated MNIST. The datasets consist of 5, 10, and 20 tasks respectively. In Split CIFAR-10, Split CIFAR-100, and Split mini-ImageNet, each task consists of 10,000, 2,500, and 2,500 training examples. For tuning hyperparameters, we separate out 5% of the training set examples in the ﬁrst three tasks as the validation set. For Rotated MNIST, we limit the number of testing examples to 1,000 per task; while for other datasets, we use the complete test sets. License and Links of Datasets. The MNIST dataset can be found in http://yann.lecun.com/ exdb/mnist/. The dataset is released without a speciﬁc license. CIFAR-10 and CIFAR-100 could be found in https://www.cs.toronto.edu/~kriz/cifar.html. Similarly, the dataset is released without a license. Mini-ImageNet and its terms of use can be found in https://mtl.yyliu.net/ download/. Personally Identiﬁable Information or Offensive Content in datasets. Among all datasets we applied, MNIST is a hand-written digit classiﬁcation dataset, while CIFAR and mini-ImageNet are image classiﬁcation dataset over common objects. To the best of our knowledge, they do not contain any sensitive information. F Comparison with ER+T In addition to data augmentation over the replay memory (ER aug), [5] also studied a number of other tricks, such as exponential LR decay and balanced reservoir sampling. The integrated method, referred to as ER with Tricks (ER+T), achieved SoTA performance in a task-aware, non-online setup over a number of datasets. We adapt ER+T to an online task-free continual learning setup by discarding the tricks not compatible with the online task-free scenario (namely the Bias Control (BiC) trick) and build GMED upon it. Table 8 summarizes the results. We ﬁnd ER+T does not outperform ERaug, i.e., the tricks other than the data augmentation is not effective in an single-epoch online training setup, except Split MNIST. We further ﬁnd ER+T+GMED outperforms or performs comparably with ER+T. The improvement is signiﬁcant on Split MNIST and Split mini-ImageNet datasets. Table 8: Building GMED over ER+T. ∗indicates signiﬁcant improvement with p-value less than 0.05. Methods / DatasetsSplitMNIST PermutedMNIST RotatedMNIST SplitCIFAR-10 SplitCIFAR-100 Splitmini-ImageNet ER + T 78.35±4.5 77.71±0.7 80.05±1.3 47.55±2.6 19.40±1.5 31.25 ±1.5ER + T + GMED 83.02∗±0.4 77.92±0.3 79.96±0.2 47.39±5.0 19.75±1.2 31.84∗±1.3 18Table 9: Performance of methods over data streams with fuzzy task boundaries over all six datasets. ∗indicates signiﬁcant improvement with p-value less than 0.05. Methods / DatasetsSplitMNIST PermutedMNIST RotatedMNIST SplitCIFAR-10 SplitCIFAR-100 Splitmini-ImageNet ER 79.74±4.0 78.98±0.5 76.45±1.2 37.15 ±1.6 21.99±1.1 26.47±2.3ER + GMED 82.73 ∗±2.6 78.91±0.5 76.55±1.0 40.57∗±1.7 21.79±1.9 28.20∗±0.6 MIR 85.80±1.9 79.31±0.7 77.04±1.2 38.70 ±1.7 21.57±1.4 25.83 ±1.5MIR + GMED 86.17±1.7 79.26±0.8 77.56∗±1.1 41.22∗±1.1 22.16±1.1 26.86∗±0.7 ERaug 81.30±2.0 77.71±0.8 80.31±0.9 47.97±3.5 18.47±2.0 31.75 ±1.0ERaug+ GMED 82.39∗±3.7 77.68±0.8 80.30±1.0 51.38∗±2.2 18.63±1.3 31.83 ±0.8 G Full Results of Experiments with Fuzzy Task Boundaries Table 9 summarizes full results of experiments where we apply fuzzy boundaries between tasks. Experiments show that GMED generally improves performance on Split MNIST, Split CIFAR-10 and Split mini-ImageNet signiﬁcantly when built upon ER, MIR, or ERaug. H Comparison to Task-Aware CL Methods We additionally present the results of task-aware memory-based approaches in Table 10. We notice HAL was not as competitive as ER on Split MNIST, Split CIFAR-10, and Split mini-ImageNet that employs a class incremental learning setup — while in the original work, the approach was mainly test in a domain-incremental learning setup [41]. GEM performs competitively on two of the datasets (Split MNIST and Rotated MNIST), where GEM+GMED could further slightly improve the performance. Table 10: Performance of Task-Aware Approaches. Methods / DatasetsSplitMNIST RotatedMNIST SplitCIFAR-10 Splitmini-ImageNet HAL 77.92 ±4.2 78.48±1.5 32.06±1.5 21.18±2.1 GEM 87.21 ±1.3 78.40±0.5 14.81±0.4 5.92 ±0.6GEM + GMED 87.69±1.4 78.62±0.4 14.13±0.3 5.81 ±0.5 I GMED without Writing Edits Back Table 11 summarizes the result of discarding the editing performed after each time step: in these experiments, we replay the edited examples, but do not replace original examples in the memory with edited ones. The approach is noted as ER+GMED w/o writeback. We tune a separate set of editing stride and regularization strength for the method. The results indicate that ER+GMED w/o writeback achieves slightly lower performance on 5 out of 6 datasets (except Split CIFAR-10) compared to ER+GMED. Table 11: GMED without storing back edited examples, i.e., the algorithm replays edited examples, but does not update the original examples in the memory as edited ones. The results are shown after ER + GMED w/o writeback. Methods / Datasets SplitMNIST PermutedMNIST RotatedMNIST SplitCIFAR-10 SplitCIFAR-100 Splitmini-ImageNet ER 81.07±2.5 78.85±0.7 76.71±1.6 33.30±3.9 20.41±1.2 25.92 ±1.2ER + GMED 82.67±1.9 78.86±0.7 77.09±1.3 34.84±2.2 20.93±1.6 27.27 ±1.8ER + GMED w/o writeback81.18±2.6 78.49±0.7 76.88±1.1 34.86±2.7 20.86±1.6 27.20 ±1.8 J Applying the Full Dataset in Split MNIST In our main experiments, we sampled 1,000 training examples for Split MNIST following [3]. We further include the results of using the entire dataset for training in Table 12, and still see signiﬁcant improvements. 19Table 12: Performance on Split MNIST constructed from the entire MNIST training set. Note than in our main experiments, we sampled 1,000 examples per task.∗and ∗∗indicates signiﬁcant improvement with p< 0.1 and p< 0.01 respectively. Methods w/o GMED w/ GMED ER 86.61 ±1.3 88.48∗∗±1.0MIR 89.18 ±1.5 89.88∗∗±1.1ERaug 91.52±1.5 92.30∗±0.9 K Tabular Results of Model Performance under Various Memory Sizes Figure 2 in the main paper present model performance under various memory size setups with bar charts. In Table 13, we further summarize results in tables, showing exact numbers of mean and standard deviation of performance. We ﬁnd the improvements on Split-MNIST and Split mini- ImageNet are signiﬁcant-with <0.05 over all memory size setups. Improvements on Rotated MNIST and Split CIFAR-10 are also mostly signiﬁcant. Table 13: Performance of ER, GMED+ER, MIR, and GMED+MIR with various memory sizes, shown in tables. ∗indicates signiﬁcant improvement with p< 0.05. Dataset Split MNIST Rotated MNIST Split CIFAR-10 Mem size 100 200 500 100 200 500 100 200 500 ER 71.13±1.5 77.85±1.4 81.07±2.5 63.35±1.7 70.07±1.2 76.71±1.6 21.94±1.5 26.03±2.1 33.30±3.9ER+GMED73.90∗±3.7 79.19∗±1.6 82.67∗±1.9 64.23∗±1.5 70.71∗±1.2 77.09±1.3 22.89∗±1.8 27.18∗±2.0 34.84∗±2.2MIR 73.53±1.6 80.79±2.5 85.72±1.2 62.91±1.1 69.95±1.2 77.50±1.6 23.09±1.2 28.78±2.0 34.42±2.4MIR+GMED77.09∗±2.5 83.52∗±1.1 86.52∗±1.4 64.39∗±0.4 70.62∗±1.2 79.08∗±0.8 25.87∗±1.9 28.89±1.8 36.17±2.5 Dataset Split CIFAR-100 Split mini-ImageNet Mem size 2000 5000 10000 5000 10000 20000 ER 12.31±1.0 16.86±0.4 20.41±1.2 18.92±0.9 25.92±1.2 29.93±1.9ER+GMED12.65±0.8 17.40∗±0.7 20.93±1.6 21.36∗±0.4 27.27∗±1.8 30.60∗±1.8MIR 11.85±0.8 18.36±1.5 20.02±1.7 17.47±1.0 25.21±2.2 30.08±1.2MIR+GMED12.32±0.8 18.60±1.1 21.22∗±1.0 20.31∗±0.8 26.50∗±1.3 31.33∗±1.6 L Performance of Optimal Editing Table 14 summarizes the performance of optimal editing (ER+Optimal) discussed in Sec. 4.4 com- pared to ER and ER+GMED. We notice that ER+Optimal outperforms ER+GMED on Split MNIST and Rotated MNIST, slightly improves on Split CIFAR-10, but does not improve on Split mini- ImageNet. The hyperparameters are not tuned extensively on mini-ImageNet because optimal editing is very expensive to compute, requiring to compute forgetting over a large sample of early examples every training step, even on moderately-sized datasets. We hereby note that our ER+Optimal does not necessarily achieve an upper-bound performance of memory editing, as it computes optimal edit for only the coming one training step over the data stream. Because the edited examples may in turn affect training dynamics in future training steps, it is hard to derive an exact upper bound performance of memory editing. M Editing Extra Examples Table 15 summarizes the results when we edit an additional number of examples per task. We randomly sample additional examples from the memory, perform edits, and writeback, without replaying them. We notice that, over all datasets, replaying a small number of extra examples improve the performance. However, the performance drops when too many examples are edited. We hypothesize replaying additional examples per step has a similar effect to increasing the stride of Table 14: Performance of optimal edits in Sec. 4.4 and their comparison to ER and GMED. Methods/Datasets Split MNIST Rotated MNIST Split CIFAR-10 Split mini-ImageNet ER 80.14±3.2 76.71 ±1.6 33.30 ±3.9 25.92 ±1.2ER+GMED 82.67±1.9 77.09 ±1.3 34.84 ±2.2 27.27 ±1.8ER+Optimal Editing83.40±2.6 77.73 ±1.3 35.04 ±2.6 27.01 ±1.6 20Table 15: Results when editing an additional number of examples per step in ER+GMED. Methods/Dataset Split MNIST Rotated MNIST Split CIFAR-10 Split mini-ImageNet ER+GMED (10 examples)82.67±1.9 77.09 ±1.3 34.84 ±2.2 27.27 ±1.8+ 3 examples 83.00±2.3 77.01 ±1.6 34.87 ±2.9 27.36 ±1.9+ 5 examples 82.78±2.3 76.98 ±1.6 35.31 ±2.2 28.00 ±2.0+ 10 examples 82.21±2.2 76.51 ±1.4 34.86 ±2.8 27.17 ±1.8+ 50 examples 81.42±2.6 76.50 ±1.3 31.79 ±2.6 27.10 ±1.9 Table 16: Prediction change rate of edited examples compared to the corresponding original examples. Dataset Split MNIST Rotated MNIST Split CIFAR-10 Split mini-ImageNet Prediction Change Rate 10.2% 2.4% 5.1% 5.5% edits performed per example. As such, when required, the number of additional examples to edit can be an additional tunable hyperparameter in our approach. N Prediction Change Rate of Edited Examples We show the percentage of changed predictions after example edits in Table 16. Speciﬁcally, when the training completes, we classify examples stored in the memory (which have experienced editing) and the corresponding original examples with the model. We then compute “prediction change rate” as the portion of changed predictions over all examples stored in the memory. The prediction change implies the edited examples are more adversarial, or they can be simply artifacts to the model. However, we notice that such prediction change rate is positively correlated with performance improvement of GMED over four datasets. It implies such adversarial or artifact examples are still beneﬁcial to reducing forgetting. O Building Upon Other Memory Population Strategies In all our main experiments, we used reservoir sampling to populate the replay memory. We further experiment with the greedy variant of Gradient-based Sample Selection strategy (GSS-Greedy) [4] on feasibly-sized datasets, which tries to populate the memory with more diverse examples. We report the results in Table 17. The results show GMED could still bring modest improvements when built upon GSS-Greedy. P Ethics Statement: Societal Impact In this work, we extend existing memory-based continual learning methods to allow gradient-based editing. We brieﬂy discuss some of the societal impacts in this section. Societal Impact (Positive): Improvement in Continual learning, and in particular, the online version of continual learning, can lead to potential beneﬁts in wide variety of machine-learning tasks dealing with streaming data. With novel information being introduced by the day, it is imperative that models are not re-trained on the entire data, instead adapt and take into account the streaming data without forgoing previously learned knowledge. As an example, BERT trained on 2018 language data would be less useful for current events, but still be useful for commonsense knowledge information and as such it would be extremely beneﬁcial if one could train over streaming current events while retaining other knowledge. Towards this goal, a primary advantage of GMED is its integration with other memory-based continual learning frameworks. As a result, we expect advances in memory-based methods to be complemented Table 17: Prediction change rate of edited examples compared to the corresponding original examples. Method/Dataset Split MNIST Rotated MNIST ER+GSS-Greedy 83.70±1.0 73.80 ±1.6ER+GSS-Greedy+GMED84.62±1.2 74.47 ±1.3 p-value 0.016 0.18 21by GMED, incurring only a minor computational overhead – a key requirement for deploying any online continual learning algorithm in the wild. Societal Impact (Negative): Caution must be taken in deploying our continual learning algorithms in the wild. This is because, CL algorithms at present, are validated solely on small datasets with synthetically curated streams. In the wild, the examples in the continual stream can have unexpected correlations which are not apparent in image-classiﬁcation only streams. Another key issue with continual learning is that once a knowledge is learned it is difﬁcult to know whether it has been completely unlearned or still retained in the neural network which can be probed later. This could happen for say hospital records where patient conﬁdentiality is needed, but were used by the continual learning model. Deleting such records is non-trivial for usual machine learning models but have dire consequences in the continual learning domain where the original stream can no longer be accessed. 22",
      "meta_data": {
        "arxiv_id": "2006.15294v3",
        "authors": [
          "Xisen Jin",
          "Arka Sadhu",
          "Junyi Du",
          "Xiang Ren"
        ],
        "published_date": "2020-06-27T06:38:13Z",
        "pdf_url": "https://arxiv.org/pdf/2006.15294v3.pdf"
      }
    },
    {
      "title": "The variational formulation of the fokker–planck equation"
    },
    {
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.",
      "full_text": "Overcoming catastrophic forgetting in neural networks James Kirkpatricka, Razvan Pascanua, Neil Rabinowitza, Joel Venessa, Guillaume Desjardinsa, Andrei A. Rusua, Kieran Milana, John Quana, Tiago Ramalhoa, Agnieszka Grabska-Barwinska a, Demis Hassabisa, Claudia Clopathb, Dharshan Kumarana, and Raia Hadsella aDeepMind, London, N1C 4AG, United Kingdom bBioengineering department, Imperial College London, SW7 2AZ, London, United Kingdom Abstract The ability to learn tasks in a sequential fashion is crucial to the development of artiﬁcial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classiﬁcation tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially. 1 Introduction Achieving artiﬁcial general intelligence requires that agents are able to learn and remember many different tasks Legg and Hutter [2007]. This is particularly difﬁcult in real-world settings: the sequence of tasks may not be explicitly labelled, tasks may switch unpredictably, and any individual task may not recur for long time intervals. Critically, therefore, intelligent agents must demonstrate a capacity for continual learning: that is, the ability to learn consecutive tasks without forgetting how to perform previously trained tasks. Continual learning poses particular challenges for artiﬁcial neural networks due to the tendency for knowledge of previously learnt task(s) (e.g. task A) to be abruptly lost as information relevant to the current task (e.g. task B) is incorporated. This phenomenon, termed catastrophic forgetting [French, 1999, McCloskey and Cohen, 1989, McClelland et al., 1995, Ratcliff, 1990], occurs speciﬁcally when the network is trained sequentially on multiple tasks because the weights in the network that are important for task A are changed to meet the objectives of task B. Whilst recent advances in machine learning and in particular deep neural networks have resulted in impressive gains in performance across a variety of domains (e.g. [Krizhevsky et al., 2012, LeCun et al., 2015]), little progress has been made in achieving continual learning. Current approaches have typically ensured that data from all tasks are simultaneously available during training. By interleaving data from multiple tasks during learning, forgetting does not occur because the weights of the network can be jointly optimized for performance on all tasks. In this regime—often referred to as the multitask learning paradigm—deep learning techniques have been used to train single agents that can successfully play multiple Atari games [Rusu et al., 2015, Parisotto et al., 2015]. If tasks are presented sequentially, multitask learning can only be used if the data are recorded by an episodic memory system and replayed to the network during training. This approach (often called system-level consolidation [McClelland et al., 1995]), is impractical for learning large numbers of tasks, as in our setting it would require the amount of memories being stored and replayed to be proportional to the number of tasks. The lack of algorithms arXiv:1612.00796v2  [cs.LG]  25 Jan 2017to support continual learning thus remains a key barrier to the development of artiﬁcial general intelligence. In marked contrast to artiﬁcial neural networks, humans and other animals appear to be able to learn in a continual fashion [Cichon and Gan, 2015]. Recent evidence suggests that the mammalian brain may avoid catastrophic forgetting by protecting previously-acquired knowledge in neocortical circuits [Cichon and Gan, 2015, Hayashi-Takagi et al., 2015, Yang et al., 2009, 2014]. When a mouse acquires a new skill, a proportion of excitatory synapses are strengthened; this manifests as an increase in the volume of individual dendritic spines of neurons [Yang et al., 2009]. Critically, these enlarged dendritic spines persist despite the subsequent learning of other tasks, accounting for retention of performance several months later [Yang et al., 2009]. When these spines are selectively “erased”, the corresponding skill is forgotten [Hayashi-Takagi et al., 2015, Cichon and Gan, 2015]. This provides causal evidence that neural mechanisms supporting the protection of these strengthened synapses are critical to retention of task performance. Together, these experimental ﬁndings—together with neurobiological models [Fusi et al., 2005, Benna and Fusi, 2016]—suggest that continual learning in the mammalian neocortex relies on a process of task-speciﬁc synaptic consolidation, whereby knowledge about how to perform a previously acquired task is durably encoded in a proportion of synapses that are rendered less plastic and therefore stable over long timescales. In this work, we demonstrate that task-speciﬁc synaptic consolidation offers a novel solution to the continual learning problem for artiﬁcial intelligence. We develop an algorithm analogous to synaptic consolidation for artiﬁcial neural networks, which we refer to as elastic weight consolidation (EWC for short). This algorithm slows down learning on certain weights based on how important they are to previously seen tasks. We show how EWC can be used in supervised learning and reinforcement learning problems to train several tasks sequentially without forgetting older ones, in marked contrast to previous deep-learning techniques. 2 Elastic weight consolidation In brains, synaptic consolidation enables continual learning by reducing the plasticity of synapses that are vital to previously learned tasks. We implement an algorithm that performs a similar operation in artiﬁcial neural networks by constraining important parameters to stay close to their old values. In this section we explain why we expect to ﬁnd a solution to a new task in the neighbourhood of an older one, how we implement the constraint, and ﬁnally how we determine which parameters are important. A deep neural network consists of multiple layers of linear projection followed by element-wise non-linearities. Learning a task consists of adjusting the set of weights and biases θof the linear projections, to optimize performance. Many conﬁgurations of θwill result in the same performance [Nielsen, 1989, Sussmann, 1992]; this is relevant for EWC: over-parameterization makes it likely that there is a solution for task B, θ∗ B, that is close to the previously found solution for task A, θ∗ A. While learning task B, EWC therefore protects the performance in task A by constraining the parameters to stay in a region of low error for task A centered around θ∗ A, as shown schematically in Figure 1. This constraint is implemented as a quadratic penalty, and can therefore be imagined as a spring anchoring the parameters to the previous solution, hence the name elastic. Importantly, the stiffness of this spring should not be the same for all parameters; rather, it should be greater for those parameters that matter most to the performance during task A. In order to justify this choice of constraint and to deﬁne which weights are most important for a task, it is useful to consider neural network training from a probabilistic perspective. From this point of view, optimizing the parameters is tantamount to ﬁnding their most probable values given some data D. We can compute this conditional probability p(θ|D) from the prior probability of the parameters p(θ) and the probability of the data p(D|θ) by using Bayes’ rule: log p(θ|D) = log p(D|θ) + logp(θ) −log p(D) (1) Note that the log probability of the data given the parameters log p(D|θ) is simply the negative of the loss function for the problem at hand −L(θ). Assume that the data is split into two independent parts, one deﬁning task A (DA) and the other task B (DB). Then, we can re-arrange equation 1: log p(θ|D) = log p(DB|θ) + logp(θ|DA) −log p(DB) (2) Note that the left hand side is still describing the posterior probability of the parameters given the entire dataset, while the right hand side only depends on the loss function for task B log p(DB|θ). 2Figure 1: elastic weight consolidation (EWC) ensures task A is remembered whilst training on task B. Training trajectories are illustrated in a schematic parameter space, with parameter regions leading to good performance on task A (gray) and on task B (cream). After learning the ﬁrst task, the parameters are at θ∗ A. If we take gradient steps according to task B alone (blue arrow), we will minimize the loss of task B but destroy what we have learnt for task A. On the other hand, if we constrain each weight with the same coefﬁcient (green arrow) the restriction imposed is too severe and we can only remember task A at the expense of not learning task B. EWC, conversely, ﬁnds a solution for task B without incurring a signiﬁcant loss on task A (red arrow) by explicitly computing how important weights are for task A. All the information about task A must therefore have been absorbed into the posterior distribution p(θ|DA). This posterior probability must contain information about which parameters were important to task Aand is therefore the key to implementing EWC. The true posterior probability is intractable, so, following the work on the Laplace approximation by Mackay [MacKay, 1992], we approximate the posterior as a Gaussian distribution with mean given by the parametersθ∗ A and a diagonal precision given by the diagonal of the Fisher information matrix F. F has three key properties [Pascanu and Bengio, 2013]: (a) it is equivalent to the second derivative of the loss near a minimum, (b) it can be computed from ﬁrst-order derivatives alone and is thus easy to calculate even for large models, and (c) it is guaranteed to be positive semi-deﬁnite. Note that this approach is similar to expectation propagation where each subtask is seen as a factor of the posterior [Eskin et al., 2004]. Given this approximation, the function Lthat we minimize in EWC is: L(θ) = LB(θ) + ∑ i λ 2 Fi(θi −θ∗ A,i)2 (3) where LB(θ) is the loss for task B only, λsets how important the old task is compared to the new one and ilabels each parameter. When moving to a third task, task C, EWC will try to keep the network parameters close to the learned parameters of both task A and B. This can be enforced either with two separate penalties, or as one by noting that the sum of two quadratic penalties is itself a quadratic penalty. 2.1 EWC allows continual learning in a supervised learning context We start by addressing the problem of whether elastic weight consolidation could allow deep neural networks to learn a set of complex tasks without catastrophic forgetting. In particular, we trained a fully connected multilayer neural network on several supervised learning tasks in sequence. Within each task, we trained the neural network in the traditional way, namely by shufﬂing the data and processing it in small batches. After a ﬁxed amount of training on each task, however, we allowed no further training on that task’s dataset. We constructed the set of tasks from the problem of classifying hand written digits from the MNIST [LeCun et al., 1998] dataset, according to a scheme previously used in the continual learning literature [Srivastava et al., 2013, Goodfellow et al., 2014]. For each task, we generated a ﬁxed, random permutation by which the input pixels of all images would be shufﬂed. Each task was thus of equal difﬁculty to the original MNIST problem, though a different solution would be required for each. Detailed description of the settings used can be found in Appendix 4.1. Training on this sequence of tasks with plain stochastic gradient descent (SGD) incurs catastrophic forgetting, as demonstrated in Figure 2A. The blue curves show performance on the testing sets of two different tasks. At the point at which the training regime switches from training on the ﬁrst task (A) to training on the second (B), the performance for task B falls rapidly, while for task A it climbs steeply. The forgetting of task A compounds further with more training time, and the addition 3Figure 2: Results on the permuted MNIST task. A: Training curves for three random permutations A, B and C using EWC(red), L2 regularization (green) and plain SGD(blue). Note that only EWC is capable of mantaining a high performance on old tasks, while retaining the ability to learn new tasks. B: Average performance across all tasks using EWC (red) or SGD with dropout regularization (blue). The dashed line shows the performance on a single task only. C: Similarity between the Fisher information matrices as a function of network depth for two different amounts of permutation. Either a small square of 8x8 pixels in the middle of the image is permuted (grey) or a large square of 26x26 pixels is permuted (black). Note how the more different the tasks are, the smaller the overlap in Fisher information matrices in early layers. of subsequent tasks. This problem cannot be countered by regularizing the network with a ﬁxed quadratic constraint for each weight (green curves, L2 regularization): here, the performance in task A degrades much less severely, but task B cannot be learned properly as the constraint protects all weights equally, leaving little spare capacity for learning on B. However, when we use EWC, and thus take into account how important each weight is to task A, the network can learn task B well without forgetting task A (red curves). This is exactly the expected behaviour described diagrammatically in Figure 1. Previous attempts to solve the continual learning problem for deep neural networks have relied upon careful choice of network hyperparameters, together with other standard regularization methods, in order to mitigate catastrophic forgetting. However, on this task, they have only achieved reasonable results on up to two random permutations [Srivastava et al., 2013, Goodfellow et al., 2014]. Using a similar cross-validated hyperparameter search as [Goodfellow et al., 2014], we compared traditional dropout regularization to EWC. We ﬁnd that stochastic gradient descent with dropout regularization alone is limited, and that it does not scale to more tasks (Figure 2B). In contrast, EWC allows a large number of tasks to be learned in sequence, with only modest growth in the error rates. Given that EWC allows the network to effectively squeeze in more functionality into a network with ﬁxed capacity, we might ask whether it allocates completely separate parts of the network for each task, or whether capacity is used in a more efﬁcient fashion by sharing representation. To assess this, we determined whether each task depends on the same sets of weights, by measuring the overlap between pairs of tasks’ respective Fisher information matrices (see Appendix 4.3). A small overlap means that the two tasks depend on different sets of weights (i.e. EWC subdivides the network’s weights for different tasks); a large overlap indicates that weights are being used for both the two tasks (i.e. EWC enables sharing of representations). Figure 2C shows the overlap as a function of depth. As a simple control, when a network is trained on two tasks which are very similar to each other (two versions of MNIST where only a few pixels are permutated), the tasks depend on similar sets of weights throughout the whole network (grey curve). When then the two tasks are more dissimilar from each other, the network begins to allocate separate capacity (i.e. weights) for the two tasks (black line). Nevertheless, even for the large permutations, the layers of the network closer to the output are indeed being reused for both tasks. This reﬂects the fact that the permutations make the input domain very different, but the output domain (i.e. the class labels) is shared. 2.2 EWC allows continual learning in a reinforcement learning context We next tested whether elastic weight consolidation could support continual learning in the far more demanding reinforcement learning (RL) domain. In RL, agents dynamically interact with the environment in order to develop a policy that maximizes cumulative future reward. We asked whether Deep Q Networks (DQNs)—an architecture that has achieved impressive successes in such challenging RL settings [Mnih et al., 2015]—could be harnessed with EWC to successfully support continual learning in the classic Atari 2600 task set [Bellemare et al., 2013]. Speciﬁcally, each 4experiment consisted of ten games chosen randomly from those that are played at human level or above by DQN. At training time, the agent was exposed to experiences from each game for extended periods of time. The order of presentation of the games was randomized and allowed for returning to the same games several times. At regular intervals we would also test the agent’s score on each of the ten games, without allowing the agent to train on them (Figure 3A). Notably, previous reinforcement learning approaches to continual learning have either relied on either adding capacity to the network [Ring, 1998, Rusu et al., 2016] or on learning each task in separate networks, which are then used to train a single network that can play all games[Rusu et al., 2015, Parisotto et al., 2015]. In contrast, the EWC approach presented here makes use of a single network with ﬁxed resources (i.e. network capacity) and has minimal computational overhead. In addition to using EWC to protect previously-acquired knowledge, we used the RL domain to address a broader set of requirements that are needed for successful continual learning systems: in particular, higher-level mechanisms are needed to infer which task is currently being performed, detect and incorporate novel tasks as they are encountered, and allow for rapid and ﬂexible switching between tasks [Collins and Frank, 2013]. In the primate brain, the prefrontal cortex is widely viewed as supporting these capabilities by sustaining neural representations of task context that exert top- down gating inﬂuences on sensory processing, working memory, and action selection in lower-level regions [O’Reilly and Frank, 2006, Mante et al., 2013, Miller and Cohen, 2001, Doya et al., 2002]. Inspired by this evidence, we used an agent very similar to that described in [van Hasselt et al., 2016] with few differences: (a) a network with more parameters, (b) a smaller transition table, (c) task-speciﬁc bias and gains at each layer, (d) the full action set in Atari, (e) a task-recognition model, and (e) the EWC penalty. Full details of hyper-parameters are described in Appendix app:atari. Here we brieﬂy describe the two most important modiﬁcations to the agent: the task-recognition module, and the implementation of the EWC penalty. We treat the task context as the latent variable of a Hidden Markov Model. Each task is therefore associated to an underlying generative model of the observations. The main distinguishing feature of our approach is that we allow for the addition of new generative models if they explain recent data better than the existing pool of models by using a training procedure inspired by the forget me not process[Kieran et al., 2016] (see Appendix 4.2). In order to apply EWC, we compute the Fisher information matrix at each task switch. For each task, a penalty is added with anchor point given by the current value of the parameters and with weights given by the Fisher information matrix times a scaling factor λwhich was optimized by hyperparameter search. We only added an EWC penalty to games which had experienced at least 20 million frames. We also allowed the DQN agents to maintain separate short-term memory buffers for each inferred task: these allow action values for each task to be learned off-policy using an experience replay mechanism [Mnih et al., 2015]. As such, the overall system has memory on two time-scales: over short time-scales, the experience replay mechanism allows learning in DQN to be based on the interleaved and uncorrelated experiences [Mnih et al., 2015]. At longer time scales, know-how across tasks is consolidated by using EWC. Finally, we allowed a small number of network parameters to be game-speciﬁc, rather than shared across games. In particular, we allowed each layer of the network to have biases and per element multiplicative gains that were speciﬁc to each game. We compare the performance of agents which use EWC (red) with ones that do not (blue) over sets of ten games in Figure 3. We measure the performance as the total human-normalized score across all ten games. We average across random seeds and over the choice of which ten games were played (see Appendix 4.2). We also clip the human-normalized score for each game to 1. Our measure of performance is therefore a number with a maximum of 10 (at least at human level on all games) where 0 means the agent is as good as a random agent. If we rely on plain gradient descent methods as in [Mnih et al., 2015], the agent never learns to play more than one game and the harm inﬂicted by forgetting the old games means that the total human-normalized score remains below one. By using EWC, however, the agents do indeed learn to play multiple games. As a control, we also considered the beneﬁt to the agent if we explicitly provided the agent with the true task label (Figure 3B, brown), rather than relying on the learned task recognition through the FMN algorithm (red). The improvement here was only modest. 5Figure 3: Results on Atari task. A: Schedule of games. Black bars indicate the sequential training periods (segments) for each game. After each training segment, performance on all games is measured. The EWC constraint is only activated to protect an agent’s performance on each game once the agent has experienced 20 million frames in that game. B: Total scores for each method across all games. Red curve denotes the network which infers the task labels using the Forget Me Not algorithm; brown curve is the network provided with the task labels. The EWC and SGD curves start diverging when games start being played again that have been protected by EWC. C: Sensitivity of a single-game DQN, trained on Breakout, to noise added to its weights. The performance on Breakout is shown as a function of the magnitude (standard deviation) of the weight perturbation. The weight perturbation is drawn from a zero mean Gaussian with covariance that is either uniform (black; i.e. targets all weights equally), the inverse Fisher ((F + λI)−1; blue; i.e. mimicking weight changes allowed by EWC), or uniform within the nullspace of the Fisher (orange; i.e. targets weights that the Fisher estimates that the network output is entirely invariant to). To evaluate the score, we ran the agent for ten full game episodes, drawing a new random weight perturbation for every timestep. While augmenting the DQN agent with EWC allows it to learn many games in sequence without suffering from catastrophic forgetting, it does not reach the score that would have been obtained by training ten separate DQNs (see Figure 1 in Appendix 4.2). One possible reason for this is that we consolidated weights for each game based on a tractable approximation of parameter uncertainty, the Fisher Information. We therefore sought to test the quality of our estimates empirically. To do so, we trained an agent on a single game, and measured how perturbing the network parameters affected the agent’s score. Regardless of which game the agent was trained on, we observed the same patterns, shown in Figure 3C. First, the agent was always more robust to parameter perturbations shaped by the inverse of the diagonal of the Fisher Information (blue), as opposed to uniform perturbations (black). This validates that the diagonal of the Fisher is a good estimate of how important a certain parameter is. Within our approximation, perturbing in the nullspace should have no effect on performance at all on performance. Empirically, however, we observe that perturbing in this space (orange) has the same effect as perturbing in the inverse Fisher space. This suggests that we are over-conﬁdent about certain parameters being unimportant: it is therefore likely that the chief limitation of the current implementation is that it under-estimates parameter uncertainty. 3 Discussion We present a novel algorithm, elastic weight consolidation, that addresses the signiﬁcant problem continual learning poses for neural networks. EWC allows knowledge of previous tasks to be protected during new learning, thereby avoiding catastrophic forgetting of old abilities. It does so by selectively decreasing the plasticity of weights, and thus has parallels with neurobiological models of synaptic consolidation. We implement EWC as a soft, quadratic constraint whereby each weight is pulled back towards its old values by an amount proportional to its importance for performance on previously-learnt tasks. To the extent that tasks share structure, networks trained with EWC reuse shared components of the network. We further show that EWC can be effectively combined with deep neural networks to support continual learning in challenging reinforcement learning scenarios, such as Atari 2600 games. The EWC algorithm can be grounded in Bayesian approaches to learning. Formally, when there is a new task to be learnt, the network parameters are tempered by a prior which is the posterior distribution on the parameters given data from previous task(s). This enables fast learning rates on parameters that are poorly constrained by the previous tasks, and slow learning rates for those which are crucial. 6There has been previous work [French and Chater, 2002, Eaton and Ruvolo, 2013] using a quadratic penalty to approximate old parts of the dataset, but these applications have been limited to small models. Speciﬁcally, [French and Chater, 2002] used random inputs to compute a quadratic approxi- mation to the energy surface. Their approach is slow, as it requires re-computing the curvature at each sample. The ELLA algorithm described in [Eaton and Ruvolo, 2013] requires computing and inverting matrices with a dimensionality equal to the number of parameters being optimized, therefore it has been mainly applied to linear and logistic regressions. In contrast, EWC has a run time which is linear in both the number of parameters and the number of training examples. We could only achieve this low computational complexity by making several simpliﬁcations, most notably by approximating the posterior distribution of the parameters on a task (i.e. the weight uncertainties) by a factorized Gaussian, and by computing its variance using a point-estimate of the parameters, via the diagonal of the Fisher Information matrix. Despite its low computational cost and empirical successes—even in the setting of challenging RL domains—our use of a point estimate of the posterior’s variance (as in a Laplace approximation) does constitute a signiﬁcant weakness (see Fig 4C). Our initial explorations suggest that one might improve on this local estimate by using Bayesian neural networks [Blundell et al., 2015]. While this paper has primarily focused on building an algorithm out of neurobiological observations, it is also instructive to consider whether the algorithm’s successes can feed back into our understanding of the brain. In particular, we see considerable parallels between EWC and two computational theories of synaptic plasticity. In this respect, the perspective we offer here aligns with a recent proposal that each synapse not only stores its current weight, but also an implicit representation of its uncertainty about that weight [Aitchison and Latham, 2015]. This idea is grounded in observations that post-synaptic potentials are highly variable in amplitude (suggestive of sampling from the weight posterior during computation), and that those synapses which are more variable are more amenable to potentiation or depression (suggestive of updating the weight posterior). While we do not explore the computational beneﬁts of sampling from a posterior here, our work aligns with the notion that weight uncertainty should inform learning rates. We take this one step further, to emphasize that consolidating the high precision weights enables continual learning over long time scales. With EWC, three values have to be stored for each synapse: the weight itself, its variance and its mean. Interestingly, synapses in the brain also carry more than one piece of information. For example, the state of the short-term plasticity could carry information on the variance [Aitchison and Latham, 2015, Pﬁster et al., 2010]. The weight for the early phase of plasticity [Clopath et al., 2008] could encode the current synaptic strength, whereas the weight associated with the late-phase of plasticity or the consolidated phase could encode the mean weight. The ability to learn tasks in succession without forgetting is a core component of biological and artiﬁcial intelligence. In this work we show that an algorithm that supports continual learning—which takes inspiration from neurobiological models of synaptic consolidation—can be combined with deep neural networks to achieve successful performance in a range of challenging domains. In doing so, we demonstrate that current neurobiological theories concerning synaptic consolidation do indeed scale to large-scale learning systems. This provides prima facie evidence that these principles may be fundamental aspects of learning and memory in the brain. Acknowledgements. We would like to thank P. Dayan, D. Wierstra, S. Mohamed, Yee Whye Teh and K. Kavukcuoglu. References Laurence Aitchison and Peter E Latham. Synaptic sampling: A connection between psp variability and uncertainty explains neurophysiological observations. arXiv preprint arXiv:1505.04544, 2015. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ- ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47: 253–279, 2013. Marcus K Benna and Stefano Fusi. Computational principles of synaptic memory consolidation. Nature neuroscience, 2016. 7Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Proceedings of The 32nd International Conference on Machine Learning, pages 1613–1622, 2015. Joseph Cichon and Wen-Biao Gan. Branch-speciﬁc dendritic ca2+ spikes cause persistent synaptic plasticity. Nature, 520(7546):180–185, 2015. Claudia Clopath, Lorric Ziegler, Eleni Vasilaki, Lars Büsing, and Wulfram Gerstner. Tag-trigger- consolidation: a model of early and late long-term-potentiation and depression. PLoS Comput Biol, 4(12):e1000248, 2008. Anne GE Collins and Michael J Frank. Cognitive control over learning: creating, clustering, and generalizing task-set structure. Psychological review, 120(1):190, 2013. DC Dowson and BV Landau. The fréchet distance between multivariate normal distributions.Journal of multivariate analysis, 12(3):450–455, 1982. Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo Kawato. Multiple model-based reinforcement learning. Neural computation, 14(6):1347–1369, 2002. Eric Eaton and Paul L Ruvolo. Ella: An efﬁcient lifelong learning algorithm. In International Conference on Machine Learning, pages 507–515, 2013. Eleazar Eskin, Alex J. Smola, and S.v.n. Vishwanathan. Laplace propagation. In Advances in Neural Information Processing Systems 16, pages 441–448. MIT Press, 2004. URL http://papers. nips.cc/paper/2444-laplace-propagation.pdf. Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3 (4):128–135, 1999. Robert M French and Nick Chater. Using noise to compute error surfaces in connectionist networks: a novel means of reducing catastrophic forgetting. Neural computation, 14(7):1755–1769, 2002. Stefano Fusi, Patrick J Drew, and LF Abbott. Cascade models of synaptically stored memories. Neuron, 45(4):599–611, 2005. Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgeting in gradient-based neural networks. Int’l Conf. on Learning Representations (ICLR), 2014. Akiko Hayashi-Takagi, Sho Yagishita, Mayumi Nakamura, Fukutoshi Shirai, Yi I. Wu, Amanda L. Loshbaugh, Brian Kuhlman, Klaus M. Hahn, and Haruo Kasai. Labelling and optical erasure of synaptic memory traces in the motor cortex. Nature, 525(7569):333–338, 09 2015. URL http://dx.doi.org/10.1038/nature15257. Milan Kieran, Joel Veness, Michael Bowling, James Kirkpatrick, Anna Koop, and Demis Hassabis. The forget me not process. In Advances in Neural Information Processing Systems 26 , page accepted for publication, 2016. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu- tional neural networks. In NIPS, pages 1097–1105, 2012. Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits, 1998. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015. Shane Legg and Marcus Hutter. Universal intelligence: A deﬁnition of machine intelligence. Minds and Machines, 17(4):391–444, 2007. David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computa- tion, 4(3):448–472, 1992. 8Valerio Mante, David Sussillo, Krishna V Shenoy, and William T Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. Nature, 503(7474):78–84, 2013. James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995. Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. The psychology of learning and motivation, 24(109-165):92, 1989. Earl K Miller and Jonathan D Cohen. An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24(1):167–202, 2001. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. Robert H. Nielsen. Theory of the backpropagation neural network. InProceedings of the International Joint Conference on Neural Networks, volume I, pages 593–605. Piscataway, NJ: IEEE, 1989. Randall C O’Reilly and Michael J Frank. Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia. Neural computation, 18(2):283–328, 2006. Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015. Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013. Jean-Pascal Pﬁster, Peter Dayan, and Máté Lengyel. Synapses with short-term plasticity are optimal estimators of presynaptic membrane potentials. Nature neuroscience, 13(10):1271–1275, 2010. Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 97(2):285, 1990. Mark B Ring. Child: A ﬁrst step towards continual learning. In Learning to learn, pages 261–292. Springer, 1998. Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk- patrick, Razvan Pascanu, V olodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015. Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Juergen Schmid- huber. Compete to compute. In Advances in Neural Information Processing Systems 26 , pages 2310–2318. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/ 5059-compete-to-compute.pdf . Héctor J. Sussmann. Uniqueness of the weights for minimal feedforward nets with a given input- output map. Neural Networks, 5:589–593, 1992. Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, pages 2094–2100, 2016. Joel Veness, Kee Siong Ng, Marcus Hutter, and Michael Bowling. Context tree switching. In 2012 Data compression conference., pages 327–336. IEEE, 2012. Guang Yang, Feng Pan, and Wen-Biao Gan. Stably maintained dendritic spines are associated with lifelong memories. Nature, 462(7275):920–924, 2009. Guang Yang, Cora Sau Wan Lai, Joseph Cichon, Lei Ma, Wei Li, and Wen-Biao Gan. Sleep promotes branch-speciﬁc formation of dendritic spines after learning. Science, 344(6188):1173–1178, 2014. 9Hyperparameter Reference ﬁgure 3A 3B 3C learning rate 10−3 10−5-10−3 10−3 dropout no yes no early stopping no yes no n. hidden layers 2 2 6 width hidden layers 400 400-2000 100 epochs / dataset 20 100 100 Table 1: Hyperparameters for each of the MNIST ﬁgures 4 Appendix 4.1 MNIST experiments We carried out all MNIST experiments with fully-connected networks with rectiﬁed linear units. In order to replicate the results of [Goodfellow et al., 2014], we compared to results obtained using dropout regularization. As suggested in [Goodfellow et al., 2014], we applied dropout with a probability of 0.2 to the input and of 0.5 to the other hidden layers. In order to give SGD with dropout the best possible chance, we also used early stopping. Early stopping was implemented by computing the test error on the validation set for all pixel permutations seen to date. Here, if the validation error was observed to increase for more than ﬁve subsequent steps, we terminated this training segment and proceeded to the next dataset; at this point, we reset the network weights to the values that had the lowest average validation error on all previous datasets. Table 1 shows a list of all hyperparameters used to produce the three graphs in Figure 3 of the main text. Where a range is present, the parameter was randomly varied and the reported results were obtained using the best hyperparameter setting. When random hyperparameter search was used, 50 combinations of parameters were attempted for each number experiment. 4.2 Atari experiments The agent architecture used is almost identical to that used in [van Hasselt et al., 2016]. In this section we provide details on all the parameters used. Images are preprocessed in the same way as in [Mnih et al., 2015], namely the 210x160 images from the Atari emulator are downsampled to 84x84 using bilinear interpolation. We then convert the RGB images to YUV and use the grayscale channel alone. The state used by the agent consists of the four latest downsampled, grayscale observations concatenated together. The network structure used is similar to the one from [Mnih et al., 2015], namely three convolutional layers followed by a fully connected layer. The ﬁrst convolution had kernel size 8, stride 4 and 32 ﬁlters. The second convolution had kernel size 4, stride 2 and 64 ﬁlters. The ﬁnal convolution had kernels size 3, stride 1 and 128 ﬁlters. The fully connected layer had 1024 units. Note that this network has approximately four times as many parameters as the standard network, due to having twice as many fully connected units and twice as many ﬁlters in the ﬁnal convolution. The other departure from the standard network is that each layer was allowed to have task-speciﬁc gains and biases. For each layer, the transformation x→ycomputed by the network is therefore: yi =  ∑ j Wijxj + bc i  gc i (4) where the biases band the gains g. The network weights and biases where initialized by setting them randomly with a uniform number between −σand σ, with σset to the square root of the incoming hidden units (for a linear layer) or set to the area of the kernel times the number of incoming ﬁlters (for convolutional layers). Biases and gains were initialized to 0 and 1 respectively. We used an an ϵ-greedy exploration policy, where the probability of selecting random action, ϵ, decayed with training time. We kept a different timer for each of the tasks. We set ϵ= 1 for 5 ×104 time steps, and then decayed this linearly to a value of 0.01 for the next 106. 10We trained the networks with the Double Q-learning algorithm [van Hasselt et al., 2016]. A training step is carried out on a minibatch of 32 experiences every four steps. The target network is updated every 3 ×104 time steps. We trained with RMSProp, with a momentum of 0., a decay of 0.95, a learning rate of 2.5 ×10−4, and a maximum learning rate of 2.5 ×10−3. Other hyperparameters that we changed from the reference implementation were: 1) using a smaller replay buffer (5 ×105 past experiences), and 2) a scaling factor for the EWC penalty of 400. Another subtle difference is that we used the full action set in the Atari emulator. In fact, although many games only support a small subset of the 18 possible actions, in order to have a uniﬁed network structure for all games we used 18 actions in each game. We randomly chose the 10 games for each experiment from a pool of 19 Atari games for which the standalone DQN could reach human-level performance in 50 ×106 frames. The scores for each of these games for the baseline algorithm, for EWC and for plain SGD training, as a function of the number steps played in that game are shown in Figure 4. In order to get an averaged performance, we chose 10 sets of 10 games, and ran 4 different random seeds for each set. The most signiﬁcant departure from the published models is the automatic determination of the task. We model each task by a generative model of the environment. In this work, for simplicity, we only model the current observation. The current task is modelled as a categorical contextcwhich is treated as the hidden variable in an Hidden Markov Model that explain observations. In such a model the probabilty of being in a particular context cat time tevolves according to: p(c,t + 1) = ∑ c′ p(c′,t)Γ(c,c′) Γ(c,c′) = δ(c,c′)(1 −α) + (1−δ(c,c′))α where δis the Kronecker delta function andαis the probability of switching context. The task context then conditions a generative model predicting the observation probability p(o|c,t). Given such generative models, the probability of being in a task set at time tcan be inferred by the observations seen so far as: p(c|o1...ot) ∝ ∑ c′ Γ(c,c′) p(c′,t −1)p(o|c,t) The maximal probability context is then taken to be the current task label. In our implementation, the generative models consist of factored multinomial distributions explaining the probability of the state of each pixel in the observation space. The model is a parametrized Dirichlet distribution, which summarizes the data seen so far using Bayesian updates. In order to encourage each model to specialize, we train the models as follows. We partition time into windows of a particular width W. During each window, all the Dirichlet priors are updated with the evidence seen so far. At the end of the window, the model best corresponding to the current task set is selected. Since this model was the most useful to explain the current data, it keeps its prior, while all other priors are reverted to their state at the beginning of the time window. We ensure that one hold-out uniform (i.e. uninitialized) Dirichlet-multinomial is always available. Whenever the hold-out model is selected a new generative model is created and a new task context is therefore created. This model is Bayesian, in the sense that data is used to maintain beliefs over priors on the generative models, and is non-parametric, in the sense that the model can grow in function of the observed data. It can be seen as an implementation of the ﬂat forget me not algorithm described in [Kieran et al., 2016]. The parameter αis not learnt. Instead we use the result from [Veness et al., 2012] where it is shown that a time decaying switch rate α= 1/tguarantees good worst case asymptotic perfmance provided the number of tasks grows as o ( n log n ) . Table 2 summarizes all hyper-parameters used for the Atari experiments. Except for the parameters pertaining the EWC algorithm (Fisher multiplier, num. samples Fisher, EWC start) or pertaining the task recognition models (model update period, model downscaling and size window), all the parameters values are the same as from [van Hasselt et al., 2016] and have not been tuned for these experiments. 11Hyperparameter value brief description action repeat 4 Repeat the same action for four frames. Each agent step will occur every fourth frame. discount factor 0.99 Discount factor used in the Q-learning algorithm. no-op max 30 Maximum number of do nothing operations carried out at the beginning of each training episode to provide a varied training set. max. reward 1 Rewards are clipped to 1. scaled input 84x84 Input images are scaled to 84x84 with bilinear interpolation. optimization algorithm RMSprop Optimization algorithm used. learning rate 0.00025 The learning rate in RMSprop. max. learning rate 0.0025 The maximum learning rate that RMSprop will apply. momentum 0. The momentum used in RMSprop. decay 0.95 The decay used in RMSProp. clipδ 1. Each gradient from Q-learning is clipped to ±1. max. norm 50. After clipping, if the norm of the gradient is greater than 50., the gradient is renormalized to 50. history length 4 The four most recently experienced frames are taken to form a state for Q-learning minibatch size 32 The number of elements taken from the replay buffer to form a mini-batch training example. replay period 4 A mini-batch is loaded from the replay buffer every 4 steps (16 frames including action repeat). memory size 50000 The replay memory stores the last ﬁfy thousand transitions experienced. target update period 7500 The target network in Q-learning is updated to the policy network every 7500 step. min. history 50000 The agent will only start learning after ﬁfty thousand transitions have been stored into memory. initial exploration 1. The value of the initial exploration rate. exploration decay start 50000 The exploration rate will start decaying after ﬁfty thousand frames. exploration decay end 1050000 The exploration rate will decay over one million frames. ﬁnal exploration 0.01 The value of the ﬁnal exploration rate. model update period 4 The Dirichlet model is updated every fourth step. model downscaling 2 The Dirichlet model is downscaled by a factor of 2, that is an image of size 42x42 is being modelled. size window 4 The size of the window for the task recognition model learning. num. samples Fisher 100 Whenever the diagonal of the Fisher is recomputed for a task, one hundred mini-batches are drawn from the replay buffer. Fisher multiplier 400 The Fisher is scaled by this number to form the EWC penalty. start EWC 20E6 The EWC penalty is only applied after 5 million steps (20 million frames). Table 2: Hyperparameters for each of the MNIST ﬁgures 120 20000 40000 60000 80000 100000 120000 140000 crazy_climber EWC SGD single game 0 2000 4000 6000 8000 10000 12000 riverraid EWC SGD single game 0 50 100 150 200 250 300 350 400 jamesbond EWC SGD single game 0 50 100 150 200 250 300 breakout EWC SGD single game 0 2000 4000 6000 8000 10000 gopher EWC SGD single game 0 1000 2000 3000 4000 5000 6000 7000 8000 kangaroo EWC SGD single game 0 5000 10000 15000 20000 25000 30000 35000 40000 kung_fu_master EWC SGD single game 100 80 60 40 20 0 20 fishing_derby EWC SGD single game 0 200 400 600 800 1000 1200 1400 enduro EWC SGD single game 30 20 10 0 10 20 30 pong EWC SGD single game 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 star_gunner EWC SGD single game 0 2000 4000 6000 8000 10000 12000 demon_attack EWC SGD single game 100 50 0 50 100 boxing EWC SGD single game 0 1000 2000 3000 4000 5000 6000 7000 asterix EWC SGD single game 0 10000 20000 30000 40000 50000 road_runner EWC SGD single game 0 5000 10000 15000 20000 25000 defender EWC SGD single game 0 5 10 15 20 25 30 35 freeway EWC SGD single game 0 2000 4000 6000 8000 10000 krull EWC SGD single game 0 200 400 600 800 1000 1200 1400 1600 space_invaders EWC SGD single game Figure 4: Score in the individual games as a function of steps played in that game. The black baseline curves show learning on individual games alone. 4.3 Fisher overlap To assess whether different tasks solved in the same network use similar sets of weights (Figure 3C in the mains text), we measured the degree of overlap between the two tasks’ Fisher matrices. Precisely, we computed two tasks’ Fishers,F1 and F2, normalized these to each have unit trace, ˆF1 and ˆF2, then computed their Fréchet distance, a metric on the space of positive-semideﬁnite matrices [Dowson and Landau, 1982]: d2( ˆF1, ˆF2) = 1 2tr ( ˆF1 + ˆF2 −2( ˆF1 ˆF2)1/2 ) = 1 2||ˆF1/2 1 −ˆF1/2 2 ||F which is bounded between zero and one. We then deﬁne the overlap as 1 −d2, with a value of zero indicating that the two tasks depend on non-overlapping sets of weights, and a value of one indicating that F1 = αF2 for some α> 0. 13",
      "meta_data": {
        "arxiv_id": "1612.00796v2",
        "doi": "10.1073/pnas.1611835114",
        "authors": [
          "James Kirkpatrick",
          "Razvan Pascanu",
          "Neil Rabinowitz",
          "Joel Veness",
          "Guillaume Desjardins",
          "Andrei A. Rusu",
          "Kieran Milan",
          "John Quan",
          "Tiago Ramalho",
          "Agnieszka Grabska-Barwinska",
          "Demis Hassabis",
          "Claudia Clopath",
          "Dharshan Kumaran",
          "Raia Hadsell"
        ],
        "published_date": "2016-12-02T19:18:37Z",
        "pdf_url": "https://arxiv.org/pdf/1612.00796v2.pdf"
      }
    },
    {
      "title": "A neural dirichlet process mixture model for task-free continual learning",
      "abstract": "Despite the growing interest in continual learning, most of its contemporary\nworks have been studied in a rather restricted setting where tasks are clearly\ndistinguishable, and task boundaries are known during training. However, if our\ngoal is to develop an algorithm that learns as humans do, this setting is far\nfrom realistic, and it is essential to develop a methodology that works in a\ntask-free manner. Meanwhile, among several branches of continual learning,\nexpansion-based methods have the advantage of eliminating catastrophic\nforgetting by allocating new resources to learn new data. In this work, we\npropose an expansion-based approach for task-free continual learning. Our\nmodel, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a\nset of neural network experts that are in charge of a subset of the data.\nCN-DPM expands the number of experts in a principled way under the Bayesian\nnonparametric framework. With extensive experiments, we show that our model\nsuccessfully performs task-free continual learning for both discriminative and\ngenerative tasks such as image classification and image generation.",
      "full_text": "Published as a conference paper at ICLR 2020 A N EURAL DIRICHLET PROCESS MIXTURE MODEL FOR TASK -FREE CONTINUAL LEARNING Soochan Lee, Junsoo Ha, Dongsu Zhang & Gunhee Kim Department of Computer Science, Seoul National University, Seoul, Republic of Korea {soochan.lee,junsoo.ha}@vision.snu.ac.kr,{96lives,gunhee}@snu.ac.kr http://vision.snu.ac.kr/projects/cn-dpm ABSTRACT Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguish- able, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Mean- while, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new re- sources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classiﬁcation and image generation. 1 I NTRODUCTION Humans consistently encounter new information throughout their lifetime. The way the information is provided, however, is vastly different from that of conventional deep learning where each mini- batch is iid-sampled from the whole dataset. Data points adjacent in time can be highly correlated, and the overall distribution of the data can shift drastically as the training progresses. Continual learning (CL) aims at imitating incredible human’s ability to learn from a non-iid stream of data without catastrophically forgetting the previously learned knowledge. Most CL approaches (Aljundi et al., 2018; 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Rusu et al., 2016; Shin et al., 2017; Yoon et al., 2018) assume that the data stream is explicitly divided into a sequence of tasks that are known at training time. Since this assumption is far from realistic, task-free CL is more practical and demanding but has been largely understudied with only a few exceptions of (Aljundi et al., 2019a;b). In this general CL, not only is explicit task deﬁnition unavailable but also the data distribution gradually shifts without a clear task boundary. Meanwhile, existing CL methods can be classiﬁed into three different categories (Parisi et al., 2019): regularization, replay, and expansion methods. Regularization and replay approaches address the catastrophic forgetting by regularizing the update of a speciﬁc set of weights or replaying the previ- ously seen data, respectively. On the other hand, the expansion methods are different from the two approaches in that it can expand the model architecture to accommodate new data instead of ﬁxing it beforehand. Therefore, the expansion methods can bypass catastrophic forgetting by preventing pre-existing components from being overwritten by the new information. The critical limitation of prior expansion methods, however, is that the decisions of when to expand and which resource to use heavily rely on explicitly given task deﬁnition and heuristics. In this work, our goal is to propose a novel expansion-based approach for task-free CL. Inspired by the Mixture of Experts (MoE) (Jacobs et al., 1991), our model consists of a set of experts, each of which is in charge of a subset of the data in a stream. The model expansion (i.e., adding more 1 arXiv:2001.00689v2  [cs.LG]  14 Jan 2020Published as a conference paper at ICLR 2020 experts) is governed by the Bayesian nonparametric framework, which determines the model com- plexity by the data, as opposed to the parametric methods that ﬁx the model complexity before training. We formulate the task-free CL as an online variational inference of Dirichlet process mix- ture models consisting of a set of neural experts; thus, we name our approach as the Continual Neural Dirichlet Process Mixture (CN-DPM) model. We highlight the key contributions of this work as follows. • We are one of the ﬁrst to propose an expansion-based approach for task-free CL. Hence, our model not only prevents catastrophic forgetting but also applies to the setting where no task deﬁnition and boundaries are given at both training and test time. Our model named CN-DPM consists of a set of neural network experts, which are expanded in a principled way built upon the Bayesian nonparametrics that have not been adopted in general CL research. • Our model can deal with both generative and discriminative tasks of CL. With several benchmark experiments of CL literature on MNIST, SVHN, and CIFAR 10/100, we show that our model successfully performs multiple types of CL tasks, including image classiﬁ- cation and generation. 2 B ACKGROUND AND RELATED WORK 2.1 C ONTINUAL LEARNING Parisi et al. (2019) classify CL approaches into three branches: regularization (Kirkpatrick et al., 2017; Aljundi et al., 2018), replay (Shin et al., 2017) and expansion (Aljundi et al., 2017; Rusu et al., 2016; Yoon et al., 2018) methods. Regularization and replay approaches ﬁx the model archi- tecture before training and prevent catastrophic forgetting by regularizing the change of a speciﬁc set of weights or replaying previously learned data. Hybrids of replay and regularization also exist, such as Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019a). On the other hand, methods based on expansion add new network components to learn new data. Conceptually, such direction has the following advantages compared to the ﬁrst two: (i) catastrophic forgetting can be eliminated since new information is not overwritten on pre-existing components and (ii) the model capacity is determined adaptively depending on the data. Task-Free Continual Learning. All the works mentioned above heavily rely on explicit task deﬁ- nition. However, in real-world scenarios, task deﬁnition is rarely given at training time. Moreover, the data domain may gradually shift without any clear task boundary. Despite its importance, task- free CL has been largely understudied; to the best of our knowledge, there are only a few works (Aljundi et al., 2019a;b; Rao et al., 2019), each of which is respectively based on regularization, re- play, and a hybrid of replay and expansion. Speciﬁcally, Aljundi et al. (2019a) extend MAS (Aljundi et al., 2018) by adding heuristics to determine when to update the importance weights with no task deﬁnition. In their following work (Aljundi et al., 2019b), they improve the memory management al- gorithm of GEM (Lopez-Paz & Ranzato, 2017) such that the memory elements are carefully selected to minimize catastrophic forgetting. While focused on unsupervised learning, Rao et al. (2019) is a parallel work that shares several similarities with our method, e.g., model expansion and short-term memory. However, due to their model architecture, expansion is not enough to stop catastrophic forgetting; consequently, generative replay plays a crucial role in Rao et al. (2019). As such, it can be categorized as a hybrid of replay and expansion. More detailed comparison between our method and Rao et al. (2019) is deferred to Appendix M. 2.2 D IRICHLET PROCESS MIXTURE MODELS We brieﬂy review the Dirichlet process mixture (DPM) model (Antoniak, 1974; Ferguson, 1983), and a variational method to approximate the posterior of DPM models in an online setting: Sequen- tial Variational Approximation (SV A) (Lin, 2013). For a more detailed review, refer to Appendix A. 2Published as a conference paper at ICLR 2020 Dirichlet Process Mixture (DPM). The DPM model is often applied to clustering problems where the number of clusters is not known in advance. The generative process of a DPM model is xn ∼p(x; θn), θn ∼G, G ∼DP(α,G0), (1) where xn is the n-th data, and θn is the n-th latent variable sampled from G, which itself is a distribution sampled from a Dirichlet process (DP). The DP is parameterized by a concentration parameter αand a base distribution G0. The expected number of clusters is proportional to α, and G0 is the marginal distribution ofθwhen Gis marginalized out. Since Gis discrete with probability 1 (Teh, 2010), same values can be sampled multiple times forθ. If θn = θm, the two data points xn and xm belong to the same cluster. An alternative formulation uses the variable zn that indicates to which cluster the n-th data belongs such thatθn = φzn where φk is the parameter of thek-th cluster. In the context of this paper, φk refers to the parameters of the k-th expert. Approximation of the Posterior of DPM Models . Since the exact inference of the posterior of DPM models is infeasible, approximate inference methods are applied. Among many approximation methods, we adopt the Sequential Variational Approximation (SV A) (Lin, 2013). While the data is given one by one, SV A sequentially determinesρn and νk, which are the variational approximation for the distribution of zn and φk respectively. Since ρn satisﬁes ∑ kρn,k = 1 and ρn,k >= 0 , ρn,k can be interpreted as the probability of n-th data belonging to k-th cluster and is often called responsibility. ρn+1 and ν(n+1) at step n+ 1 are computed as: ρn+1,k ∝ { (∑n i=1 ρi,k) ∫ φp(xn+1|φ)ν(n) k (dφ) if 1 ≤k≤K α ∫ φp(xn+1|φ)G0(dφ) if k= K+ 1 , (2) ν(n+1) k (dφ) ∝ { G0(dφ) ∏n+1 i=1 p(xi|φ)ρi,k if 1 ≤k≤K G0(dφ)p(xn+1|φ)ρn+1,k if k= K+ 1 . (3) In practice, SV A adds a new component only when ρK+1 is greater than a certain threshold ϵ. If G0 and p(xi|φ) are not a conjugate pair, stochastic gradient descent (SGD) is used to ﬁnd the MAP estimation ˆφwith a learning rate of λinstead of calculating the whole distribution νk: ˆφ(n+1) k ←ˆφ(n) k + λ(∇ˆφ(n) k log G0(ˆφ(n) k ) + ∇ˆφ(n) k log p(x|ˆφ(n) k )). (4) DPM for Discriminative Tasks . DPM can be extended to discriminative tasks where each data point is an input-output pair (x,y), and the goal is to learn the conditional distribution p(y|x). To use DPM, which is a generative model, for discriminative tasks, we ﬁrst learn the joint distribution p(x,y) and induce the conditional distribution from it: p(y|x) = p(x,y)/ ∫ yp(x,y). The joint distribution modeled by each component can be decomposed as p(x,y|z) = p(y|x,z)p(x|z) (Ras- mussen & Ghahramani, 2002; Shahbaba & Neal, 2009). DPM in Related Fields. Recent works of Nagabandi et al. (2019) and Jerfel et al. (2019) exploit the DPM framework to add new components without supervision in the meta-learning context. Naga- bandi et al. (2019) apply DPM to the model-based reinforcement learning to predict the next state from a given state-action pair. When a new task appears, they add a component under the DPM framework to handle predictions in the new task. Jerfel et al. (2019) apply DPM to online meta- learning. Extending MAML (Finn et al., 2017), they assume that similar tasks can be grouped into a super-task in which the parameter initialization is shared among tasks. DPM is exploited to ﬁnd the super-tasks and the parameter initialization for each super-task. Therefore, it can be regarded as a meta-level CL method. These works, however, lack generative components, which are often es- sential to infer the responsible component at test time, as will be described in the next section. As a consequence, it is not straightforward to extend their algorithms to other CL settings beyond model- based RL or meta-learning. In contrast, our method implements a DPM model that is applicable to general task-free CL. 3 A PPROACH We aim at general task-free CL, where the number of tasks and task descriptions are not available at both training and test time. We even consider the case where the data stream cannot be split into 3Published as a conference paper at ICLR 2020 separate tasks in Appendix F. All of the existing expansion methods are not task-free since they require task deﬁnition at training (Aljundi et al., 2017) or even at test time (Rusu et al., 2016; Xu & Zhu, 2018; Li et al., 2019). We propose a novel expansion method that automatically determines when to expand and which component to use. We ﬁrst deal with generative tasks and generalize them into discriminative ones. 3.1 C ONTINUAL LEARNING AS MODELING OF THE MIXTURE DISTRIBUTION We can formulate a CL scenario as a stream of data involving different tasksD1,D2,... where each task Dk is a set of data sampled from a (possibly) distinct distribution p(x|z = k). If K tasks are given so far, the overall distribution is expressed as the mixture distribution: p(x) = K∑ k=1 p(x|z= k)p(z= k), (5) where p(z= k) can be approximated byNk/Nwhere Nk = |Dk|and N = ∑ kNk. The goal of CL is to learn the mixture distribution in an online manner. Regularization and replay methods directly model the approximate distribution p(x; φ) parameterized by a single component φ and update it to ﬁt the overall distribution p(x). When updating φ, however, they do not have full access to all the previous data, and thus the information of previous tasks is at risk of being lost as more tasks are learned. Another way to solve CL is to use a mixture model: approximating each p(x|z = k) with p(x; φk). If we learn a new task distribution p(x|z = K + 1) with new parameter φK+1 and leave the existing parameters intact, we can preserve the knowledge of the previous tasks. The expansion-based CL methods follow this idea. Similarly, in the discriminative task, the goal of CL is to model the overall conditional distribution, which is a mixture of task-wise conditional distribution p(y|x,z = k): p(y|x) = K∑ k=1 p(y|x,z = k)p(z= k|x). (6) Prior expansion methods use expert networks each of which models a task-wise conditional distri- bution p(y|x; φk)1. However, a new problem arises in expansion methods: choosing the right expert given x, i.e., p(z|x) in Eq.(6). Existing methods assume that explicit task descriptor z is given, which is generally not true in human-like learning scenarios. That is, we need a gating mechanism that can infer p(z|x) only from x(i.e., which expert should process x). With the gating, the model prediction naturally reduces to the sum of expert outputs weighted by the gate values, which is the mixture of experts (MoE) (Jacobs et al., 1991) formulation: p(y|x) ≈∑ kp(y|x; φk)p(z= k|x). However, it is not possible to use a single gate network as in Shazeer et al. (2017) to modelp(z|x) in CL; since the gate network is a classiﬁer that ﬁnds the correct expert for a given data, training it in an online setting causes catastrophic forgetting. Thus, one possible solution to replace a gating network is to couple each expert k with a generative model that represents p(x|z = k) as in Rasmussen & Ghahramani (2002) and Shahbaba & Neal (2009). As a result, we can build a gating mechanism without catastrophic forgetting as p(y|x) ≈ ∑ k p(y|x; φD k )p(z= k|x) ≈ ∑ k p(y|x; φD k ) p(x; φG k)p(z= k)∑ k′p(x; φG k′)p(z= k′), (7) where p(z = k) ≈Nk/N. We also differentiate the notation for the parameters of discriminative models for classiﬁcation and generative models for gating by the superscript Dand G. If we know the true assignment of z, which is the case of task-based CL, we can independently train a discriminative model (i.e., p(y|x; φD k )) and a generative model (i.e., p(x; φG k)) for each task k. In task-free CL, however, z is unknown, so the model needs to infer the posterior p(z|x,y). Even worse, the total number of experts is unknown beforehand. Therefore, we propose to employ a Bayesian nonparametric framework, speciﬁcally the Dirichlet process mixture (DPM) model, which can ﬁt a mixture distribution with no preﬁxed number of components. We use SV A described in 1 The models with multiple output heads sharing the same base network (Rusu et al., 2016; Yoon et al., 2018) can also fall into this category as the expert correspond to each subnetwork connected to an output head. 4Published as a conference paper at ICLR 2020 (a) Training (b) Inference 𝑥,𝑦 𝜙1 𝐷 𝜙1 𝐺𝑁1𝜙1 𝐷 𝜙1 𝐺𝑁1𝜙1 𝐷 𝜙1 𝐺𝑁1 𝜌𝑘 ∝ 𝑁𝑘𝑝 𝑦 𝑥;𝜙𝑘 𝐷 𝑝 𝑥;𝜙𝑘 𝐺 𝜌𝐾+1 ∝ 𝛼𝑝 𝑦 𝑥;𝜙0 𝐷 𝑝 𝑥;𝜙0 𝐺 𝜙0 𝐷 𝜙0 𝐺𝛼 STM STM Train a new expert Sleep if  STM is full Wake 𝑁2 𝑁 𝑥 𝑁𝐾 𝑁 𝑝 𝑥,𝑦 𝑁1 𝑁 𝜙𝐾+1 𝐷 𝜙𝐾+1 𝐺𝑁𝐾+1 𝑝 𝑧 = 𝑘 = 𝑁𝑘 𝑁 𝑝 𝑥,𝑦;𝜙1 𝑝 𝑥,𝑦;𝜙2 𝑝 𝑥,𝑦;𝜙𝐾 𝑝 𝑦|𝑥 Figure 1: Overview of our CN-DPM model. Each expert k (blue boxes) contains a discriminative component for modeling p(y|x; φD k ) and a generative component for modeling p(x; φG k), jointly representing p(x,y; φk). We also keep the assigned data count Nk per expert. (a) During training, each sample (x,y) coming in a sequence is evaluated by every expert to calculate the responsibility ρk of each expert. If ρK+1 is high enough, i.e., none of the existing experts is responsible, the data is stored into short-term memory (STM). Otherwise, it is learned by the corresponding expert. When STM is full, a new expert is created from the data in STM. (b) Since CN-DPM is a generative model, we ﬁrst compute the joint distribution p(x,y) for a given x, from which it is trivial to infer p(y|x). section 2.2 to approximate the posterior in an online setting. Although SV A is originally designed for the generative tasks, it is easily applicable to discriminative tasks by making each component k to model p(x,y|z) = p(y|x,z)p(x|z). 3.2 T HE CONTINUAL NEURAL DIRICHLET PROCESS MIXTURE (CN-DPM) M ODEL The proposed approach for task-free CL, named Continual Neural Dirichlet Process Mixture (CN- DPM) model, consists of a set of experts, each of which is associated with a discriminative model (classiﬁer) and a generative model (density estimator). More speciﬁcally, the classiﬁer models p(y|x,z = k), for which we can adopt any classiﬁer or regressor using deep neural networks, while the density estimator describes the marginal likelihood p(x|z = k), for which we can use any explicit density model such as V AEs (Kingma & Welling, 2014) and PixelRNN (Oord et al., 2016). We respectively denote the classiﬁer and the density estimator of expert kas p(y|x; φD k ) and p(x; φG k), where φD k and φG k are the parameters of the models. Finally, the prediction p(y|x) can be obtained from Eq.(7) by plugging in the output of the classiﬁer and the density estimator. Note that the number of experts is not preﬁxed but expanded via the DPM framework. Figure 1 illustrates the overall training and inference process of our model. Training. We assume that samples sequentially arrive one at a time during training. For a new sample, we ﬁrst decide whether the sample should be assigned to an existing expert or a new expert should be created for it. Suppose that samples up to (xn,yn) are sequentially processed and K experts are already created when a new sample(xn+1,yn+1) arrives. We compute the responsibility ρn+1,k as follows: ρn+1,k ∝ { (∑ n i=1 ρi,k) p(yn+1|xn+1; ˆφD k )p(xn+1; ˆφG k) if 1 ≤k≤K αp(yn+1|xn+1; ˆφD 0 )p(xn+1; ˆφG 0 ) where ˆφ0 ∼G0(φ) if k= K+ 1 (8) where G0 is a distribution corresponding to the weight initialization. If arg maxkρn+1,k ̸= K+ 1, the sample is assigned to the existing experts proportional to ρn+1,k, and the parameters of the experts are updated with the new sample by Eq.(4) such that ˆφk is the MAP approximation given the data assigned up to the current time step. Otherwise, we create a new expert. Short-Term Memory. However, it is not a good idea to create a new expert immediately and initialize it to be the MAP estimation given xn+1. Since both the classiﬁer and density estimator of an expert are neural networks, training the new expert with only a single example leads to severe overﬁtting. To mitigate this issue, we employ short-term memory (STM) to collect sufﬁcient data 5Published as a conference paper at ICLR 2020 before creating a new expert. When a data point is classiﬁed as new, we store it to the STM. Once the STM reaches its maximum capacity M, we stop the data inﬂow for a while and train a new expert with the data in the STM for multiple epochs until convergence. We call this proceduresleep phase. After sleep, the STM is emptied, and the newly trained expert is added to the expert pool. During the subsequent wake phase, the expert is learned from the data assigned to it. This STM trick assumes that the data in the STM belong to the same expert. We empirically ﬁnd that this assumption is acceptable in many CL settings where adjacent data are highly correlated. The overall training procedure is described in Algorithm 1. Note that we use ρn,0 instead of ρn,K+1 in the algorithm for brevity. Inference. At test time, we infer p(y|x) from the collaboration of the learned experts as in Eq.(7). Techniques for Practicality. Naively adding a new expert has two major problems: (i) the number of parameters grows unnecessarily large as the experts redundantly learn common features and (ii) there is no positive transfer of knowledge between experts. Therefore, we propose a simple method to share parameters between experts. When creating a new expert, we add lateral connections to the features of the previous experts similar to Rusu et al. (2016). To prevent catastrophic forgetting in the existing experts, we block the gradient from the new expert. In this way, we can greatly reduce the number of parameters while allowing positive knowledge transfer. More techniques such as sparse regularization in Yoon et al. (2018) can be employed to reduce redundant parameters further. As they are orthogonal to our approach, we do not use such techniques in our experiments. Another effective technique that we use in the classiﬁcation experiments is adding a temperature parameter to the classiﬁer. Since the range of log p(x|z) is far broader than log p(y|x,z), the classiﬁer has almost no effect without proper scaling. Thus, we can increase overall accuracy by adjusting the relative importance of images and labels. We also introduce an algorithm to prune redundant experts in Appendix D, and discuss further practical issues of CN-DPM in Appendix B. Algorithm 1 Training of the Continual Neural Dirichlet Process Mixture (CD-NDP) Model Require: Data (x1,y1),..., (xN,yN), concen- tration α, base measure G0, short-term memory capacity M, learning rate λ 1: M←∅{ Short-term memory} 2: K ←0 {Number of experts} 3: N0 ←α; ˆφ0 ←Sample(G0) 4: for n= 1 to N do 5: for k= 0 to Kdo 6: lk ←p(yn|xn; ˆφD k )p(xn; ˆφG k) 7: ρn,k ←Nklk 8: end for 9: ρn,0:K ←ρn,0:K/∑K k=0 ρn,k 10: if arg maxkρn,k = 0 then 11: {Save xn to short-term memory} 12: M←{ xn}∪M 13: if |M|≥ M then {Add new expert} 14: ˆφK+1 ←FindMAP(M,G0) 15: NK+1 ←|M|; M←∅ 16: K ←K+ 1 17: end if 18: else {Update existing experts} 19: ρn,1:K ←ρn,1:K/∑K k=1 ρn,k 20: for k= 1 to Kdo 21: Nk ←Nk + ρn,k 22: ˆφk ←ˆφk + ρn,kλ∇ˆφk log lk 23: end for 24: end if 25: end for 4 E XPERIMENTS We evaluate the proposed CN-DPM model in task-free CL with four benchmark datasets. Appen- dices include more detailed model architecture, additional experiments, and analyses. 4.1 C ONTINUAL LEARNING SCENARIOS A CL scenario deﬁnes a sequence of tasks where the data distribution for each task is assumed to be different from others. Below we describe the task-free CL scenarios used in the experiments. At both train and test time, the model cannot access the task information. Unless stated otherwise, each task is presented for a single epoch (i.e., a completely online setting) with a batch size of 10. Split-MNIST (Zenke et al., 2017). The MNIST dataset (LeCun et al., 1998) is split into ﬁve tasks, each containing approximately 12K images of two classes, namely (0/1, 2/3, 4/5, 6/7, 8/9). We conduct both classiﬁcation and generation experiments in this scenario. 6Published as a conference paper at ICLR 2020 Table 1: Test scores and the numbers of parameters in task-free CL on Split-MNIST, MNIST-SVHN, and Split-CIFAR100 scenarios. Note that iid-∗baselines are not CL methods. Method Split-MNIST Split-MNIST (Gen.) MNIST-SVHN Split-CIFAR100 Acc. (%) Param. bits/dim Param. Acc. (%) Param. Acc. (%) Param. iid-ofﬂine 98.63 478 K 0.1806 988 K 96.69 11 .2M 73.80 11 .2M iid-online 96.18 478 K 0.2156 988 K 95.24 11 .2M 20.46 11 .2M Fine-tune 19.43 478 K 0.2817 988 K 83.35 11 .2M 2.43 11 .2M Reservoir 85.69 478 K 0.2234 988 K 94.12 11 .2M 10.01 11 .2M CN-DPM 93.23 524K 0.2110 970K 94.46 7.80M 20.10 19.2M Table 2: Performance comparison on Split-CIFAR10 with various scenario length. Method Split-CIFAR10 Acc. (%) Param.0.2 Epoch 1 Epoch 10 Epochs iid-ofﬂine 93.17 93 .17 93 .17 11 .2M iid-online 36.65 62 .79 83 .19 11 .2M Fine-tune 12.68 18 .08 19 .31 11 .2M Reservoir 37.09 44 .00 43 .82 11 .2M GSS 33.56 − − 11.2M CN-DPM 41.78 45.21 46.98 4.60M Table 3: Dissecting the performance of CN-DPM. Acc. Type Split-CIFAR10 Split-CIFAR100 Classiﬁer (init) 88.20 55 .42 Classiﬁer (ﬁnal) 88.20 55 .24 Gating (V AEs) 48.18 31 .14 0K 2K 4K 6K 8K 10K Learned examples 10 15 20 25 30 35 40 45Accuracy (%) iid-online Fine-tune GSS Reservoir CN-DPM Figure 2: Split-CIFAR10 (0.2 Epoch). 0K 10K 20K 30K 40K 50K Learned examples 0 5 10 15 20Accuracy (%) Figure 3: Split-CIFAR100. MNIST-SVHN (Shin et al., 2017). It is a two-stage scenario where the ﬁrst consists of MNIST, and the second contains SVHN (Netzer et al., 2011). This scenario is different from Split-MNIST; in Split-MNIST, new classes are introduced when transitioning into a new task, whereas the two stages in MNIST-SVHN share the same set of class labels and have different input domains. Split-CIFAR10 and Split-CIFAR100. In Split-CIFAR10, we split CIFAR10 (Krizhevsky & Hin- ton, 2009) into ﬁve tasks in the same manner as Split-MNIST. For Split-CIFAR100, we build 20 tasks, each containing ﬁve classes according to the pre-deﬁned superclasses in CIFAR100. The training sets of CIFAR10 and CIFAR100 consist of 50K examples each. Note that most of the pre- vious works (Rebufﬁ et al., 2017; Zenke et al., 2017; Lopez-Paz & Ranzato, 2017; Aljundi et al., 2019c; Chaudhry et al., 2019a), except Maltoni & Lomonaco (2019), use task information at test time in Split-CIFAR100 experiments. They assign distinct output heads for each task and utilize the task identity to choose the responsible output head at both training and test time. Knowing the right output head, however, the task reduces to 5-way classiﬁcation. Therefore, our setting is far more difﬁcult than the prior works since the model has to perform 100-way classiﬁcation only from the given input. 4.2 C OMPARED METHODS All the following baselines use the same base network that will be discussed in section 4.3. iid-ofﬂine and iid-online . iid-ofﬂine shows the maximum performance achieved by combining standard training techniques such as data augmentation, learning rate decay, multiple iterations (up to 100 epochs), and larger batch size. iid-online is the model trained with the same number of epoch and batch size with other CL baselines. 7Published as a conference paper at ICLR 2020 Fine-tune. As a popular baseline in the previous works, the base model is naively trained as data enters. Reservoir. As Chaudhry et al. (2019b) show that simple experience replay (ER) can outperform most CL methods, we test the ER with reservoir sampling as a strong baseline. Reservoir sampling randomly chooses a ﬁxed number of samples with a uniform probability from an indeﬁnitely long stream of data, and thus, it is suitable for managing the replay memory in task-free CL. At each training step, the model is trained using a mini-batch from the data stream and another one of the same sizes from the memory. Gradient-Based Sample Selection (GSS). Aljundi et al. (2019b) propose a sampling method called GSS that diversiﬁes the gradients of the samples in the replay memory. Since it is designed to work in task-free settings, we report the scores in their paper for comparison. 4.3 M ODEL ARCHITECTURE Split-MNIST. Following Hsu et al. (2018), we use a simple two-hidden-layer MLP classiﬁer with ReLU activation as the base model for classiﬁcation. The dimension of each layer is 400. For genera- tion experiments, we use V AE, whose encoder and decoder have the same hidden layer conﬁguration with the classiﬁer. Each expert in CN-DPM has a similar classiﬁer and V AE with smaller hidden dimensions. The ﬁrst expert starts with 64 hidden units per layer and adds 16 units when a new expert is added. For classiﬁcation, we adjust hyperparameter αsuch that ﬁve experts are created. For generation, we set αto produce 12 experts since more experts produce a better score. We set the memory size in both Reservoir and CN-DPM to 500 for classiﬁcation and 1000 for generation. MNIST-SVHN and Split-CIFAR10/100. We use ResNet-18 (He et al., 2016) as the base model. In CN-DPM, we use a 10-layer ResNet for the classiﬁer and a CNN-based V AE. The encoder and the decoder of V AE have two CONV layers and two FC layers. We setαsuch that 2, 5, and 20 experts are created for each scenario. The memory sizes in Reservoir, GSS, and CN-DPM are set to 500 for MNIST-SVHN and 1000 for Split-CIFAR10/100. More details can be found in Appendix C. 4.4 R ESULTS OF TASK -FREE CONTINUAL LEARNING All reported numbers in our experiments are the average of 10 runs. Table 1 and 2 show our main experimental results. In every setting, CN-DPM outperforms the baselines by signiﬁcant margins with reasonable parameter usage. Table 2 and Figure 2 shows the results of Split-CIFAR10 exper- iments. Since Aljundi et al. (2019b) test GSS using only 10K examples of CIFAR10, which is 1/5 of the whole train set, we follow their setting (denoted by 0.2 Epoch) for a fair comparison. We also test a Split-CIFAR10 variant where each task is presented for 10 epochs. The accuracy and the training graph of GSS are excerpted from the original paper, where the accuracy is the average of three runs, and the graph is from one of the runs. In Figure 2, the bold line represents the average of 10 runs (except GSS, which is a single run), and the faint lines are the individual runs. Surprisingly, Reservoir even surpasses the accuracy of GSS and proves to be a simple but powerful CL method. One interesting observation in Table 2 is that the performance of Reservoir degrades as each task is extended up to 10 epochs. This is due to the nature of replay methods; since the same samples are replayed repeatedly as representatives of the previous tasks, the model tends to be overﬁtted to the replay memory as training continues. This degradation is more severe when the memory size is small, as presented in Appendix I. Our CN-DPM, on the other hand, uses the memory to buffer recent examples temporarily, so there is no such overﬁtting problem. This is also conﬁrmed by the CN-DPM’s accuracy consistently increasing as learning progresses. In addition, CN-DPM is particularly strong compared to other baselines when the number of tasks increases. For example, Reservoir, which performs reasonably well in other tasks, scores poorly in Split-CIFAR100, which involves 20 tasks and 100 classes. Even with the large replay memory of size 1000, the Reservoir suffers from the shortage of memory (e.g., only 50 slots per task). In contrast, CN-DPM’s accuracy is more than double of Reservoir and comparable to that of iid-online. Table 3 analyzes the accuracy of CN-DPM in Split-CIFAR10/100. We assess the performance and forgetting of individual components. At the end of each task, we measure the test accuracy of the re- sponsible classiﬁer and report the average of such task-wise classiﬁer accuracies as Classiﬁer (init). 8Published as a conference paper at ICLR 2020 We report the average of the task-wise accuracies after learning all tasks as Classiﬁer (ﬁnal). With little difference between the two scores, we conﬁrm that forgetting barely occurs in the classiﬁers. In addition, we report the gating accuracy measured after training asGating (VAEs), which is the ac- curacy of the task identiﬁcation performed jointly by the V AEs. The relatively low gating accuracy suggests that CN-DPM has much room for improvement through better density estimates. Overall, CN-DPM does not suffer from catastrophic forgetting, which is a major problem in regu- larization and replay methods. As a trade-off, however, choosing the right expert arises as another problem in CN-DPM. Nonetheless, the results show that this new direction is especially promising when the number of tasks is very large. 5 C ONCLUSION In this work, we formulated expansion-based task-free CL as learning of a Dirichlet process mix- ture model with neural experts. We demonstrated that the proposed CN-DPM model achieves great performance in multiple task-free settings, better than the existing methods. We believe there are several interesting research directions beyond this work: (i) improving the accuracy of expert selec- tion, which is the main bottleneck of our method, and (ii) applying our method to different domains such as natural language processing and reinforcement learning. ACKNOWLEDGMENTS We thank Chris Dongjoo Kim and Yookoon Park for helpful discussion and advice. This work was supported by Video Analytics Center of Excellence in AIX center of SK telecom, Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No.2019-0-01082, SW StarLab) and Basic Science Research Program through National Research Foundation of Korea (NRF) funded by the Korea government (MSIT) (2017R1E1A1A01077431). Gunhee Kim is the corresponding author. REFERENCES Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, 2017. Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-Free continual learning. In CVPR, 2019a. Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In NeurIPS, 2019b. Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selﬂess sequential learning. In ICLR, 2019c. Charles E. Antoniak. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. Ann. Stat., 2(6):1152–1174, 1974. David Blei and Michael Jordan. Variational inference for dirichlet process mixtures.Bayesian Anal., 1(1):121–143, 2006. Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. InICLR, 2015. Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. In ICLR, 2019a. Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K. Dokania, Philip H. S. Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv, (1902.10486v4), 2019b. 9Published as a conference paper at ICLR 2020 Michael D. Escobar and Mike West. Bayesian density estimation and inference using mixtures. J. Am. Stat. Assoc., 90(430):577–588, 1995. Thomas S. Ferguson. Bayesian density estimation by mixtures of normal distributions. In Recent advances in statistics, pp. 287–302. Academic Press, 1983. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In CVPR, 2016. Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learn- ing scenarios: A categorization and case for strong baselines. In NeurIPS, Continual Learning Workshop, 2018. Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Comput., 3:79–87, 1991. Ghassen Jerfel, Erin Grant, Thomas Grifﬁths, and Katherine Heller. Reconciling meta-learning and continual learning with online mixtures of tasks. In NeurIPS, 2019. Diederik P. Kingma and Max Welling. Auto-Encoding variational bayes. In ICLR, 2014. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass- abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. PNAS, 2017. A Krizhevsky and G Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In ICML, 2019. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE TPAMI, 40(12):2935–2947, 2017. Dahua Lin. Online learning of nonparametric mixture models via sequential variational approxima- tion. In NeurIPS, 2013. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, 2017. Steven Maceachern. Estimating normal means with a conjugate style dirichlet process prior. Com- mun. Stat. - Simul. Comput., 23(3):727–741, 1994. Davide Maltoni and Vincenzo Lomonaco. Continuous learning in single-incremental-task scenarios. Neural Networks, 116:56–73, 2019. Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via Meta-Learning: continual adaptation for Model-Based RL. In ICLR, 2019. Radford M. Neal. Markov chain sampling methods for dirichlet process mixture models.J. Comput. Graph. Stat., 2000. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NeurIPS, Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Aaron Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. InICML, 2016. 10Published as a conference paper at ICLR 2020 German I. Parisi, Ronald Kemker, Jose L. Part, and Christopher Kanan. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019. Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell. Continual unsupervised representation learning. In NeurIPS, 2019. Carl Edward Rasmussen and Zoubin Ghahramani. Inﬁnite mixtures of gaussian process experts. In NeurIPS, 2002. Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph Lampert. iCaRL: in- cremental classiﬁer and representation learning. In CVPR, 2017. Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. In NeurIPS, 2016. Jonathan Schwarz, Jelena Luketina, Wojciech Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for contin- ual learning. In ICML, 2018. Babak Shahbaba and Radford Neal. Nonlinear models using dirichlet process mixtures. J. Mach. Learn. Res., 2009. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The Sparsely-Gated Mixture-of-Experts layer. In ICLR, 2017. Hanul Shin, Jung Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NeurIPS, 2017. Yee Whye Teh. Dirichlet process. Springer, Encyclopedia of Machine Learning:280–287, 2010. Gido M. van de Ven and Andreas S. Tolias. Generative replay with feedback connections as a general strategy for continual learning. arXiv, (1809.10635v2), 2018. Lianming Wang and David Dunson. Fast bayesian inference in dirichlet process mixture models. J. Comput. Graph. Stat., 20(1):196–216, 2011. Ju Xu and Zhanxing Zhu. Reinforced continual learning. In NeurIPS, 2018. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. In ICLR, 2018. Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 11Published as a conference paper at ICLR 2020 A R EVIEW OF DIRICHLET PROCESS MIXTURE MODEL We review the Dirichlet process mixture (DPM) model and a variational method to approximate the posterior of DPM models in an online setting: Sequential Variational Approximation (SV A) (Lin, 2013). Dirichlet Process. Dirichlet process (DP) is a distribution over distributions that are deﬁned over inﬁnitely many dimensions. DP is parameterized by a concentration parameter α∈R+ and a base distribution G0. For a distribution Gsampled from DP(α,G0), the following holds for any ﬁnite measurable partition {A1,A2,...,A K}of probability space Θ (Teh, 2010): (G(A1),...,G (AK)) ∼Dir(αG0(A1),...,αG 0(AK)). (9) The stick-breaking process is often used as a more intuitive construction of DP: G= ∞∑ k=1 ( vk k−1∏ l=1 (1 −vl) ) δφk,vk ∼Beta(1,α),φk ∼G0. (10) Initially, we start with a stick of length one, which represents the total probability. At each stepk, we cut a proportion vk off from the remaining stick (probability) and assign it to the atom φk sampled from the base distributionG0. This formulation shows DP is discrete with probability 1 (Teh, 2010). In our problem setting, Gis a distribution over expert’s parameter space and has positive probability only at the countably many φk, which are independently sampled from the base distribution. Dirichlet Process Mixture (DPM) Model. The DPM model is often applied to clustering problems where the number of clusters is not known in advance. The generative process of DPM model is xn ∼p(θn), θn ∼G, G ∼DP(α,G0), (11) where xn is the n-th data, and θn is the n-th latent variable sampled from G, which itself is a distribution sampled from a Dirichlet process (DP). Since Gis discrete with probability 1, the same values can be sampled multiple times for θ. If θn = θm, the two data points xn and xm belong to the same cluster. An alternative formulation uses the indicator variable zn that indicates to which cluster the n-th data belongs such that θn = φzn where φk is the parameter of k-th cluster. The data xn is sampled from a distribution parameterized by θn. For a DP Gaussian mixture model as an example, each θ= {µ,σ2}parameterizes a Gaussian distribution. The Posterior of DPM Models . The posterior of a DPM model for given θ1,...,θ n is also a DP (Teh, 2010): G|θ1,...,θ n ∼DP ( α+ n, α α+ nG0 + 1 α+ n n∑ i=1 δ(θi) ) . (12) The base distribution of the posterior, which is a weighted average ofG0 and the empirical distribu- tion 1 n ∑n i=1 δ(θi), is in fact the predictive distribution of θn+1 given θ1:n (Teh, 2010): θn+1|θ1,...,θ n ∼ α α+ nG0 + 1 α+ n n∑ i=1 δ(θi). (13) If we additionally condition xn and reﬂect the likelihood, we obtain (Neal, 2000): θn+1|θ1,...,θ n,xn+1 ∼ 1 Z ( α α+ n ∫ p(xn+1|θ)dG0(θ) + 1 α+ n n∑ i=1 p(xn+1|θi)δ(θi) ) , (14) where Zis the normalizing constant. Note that θn+1 is independent from x1:n given θ1:n. Approximation of the Posterior of DPM Models . Since the exact inference of the posterior of DPM models is infeasible, approximate inference methods are adopted such as Markov chain Monte Carlo (MCMC) (Maceachern, 1994; Escobar & West, 1995; Neal, 2000) or variational inference (Blei & Jordan, 2006; Wang & Dunson, 2011; Lin, 2013). Among many variational methods, the Sequential Variational Approximation (SV A) (Lin, 2013) approximates the posterior as p(G|x1:n) = ∑ z1:n p(z1:n|x1:n)p(G|x1:n,z1:n) ≈ q(G|ρ,ν) = ∑ z1:n ( n∏ i=1 ρi,zi ) q(z) ν (G|z1:n), (15) 12Published as a conference paper at ICLR 2020 where p(z1:n|x1:n) is represented by the product of individual variational probabilities ρzi for zi, which greatly simpliﬁes the distribution. Moreover, p(G|x1:n,z1:n) is approximated by a stochastic process q(z) ν (G|z1:n). Sampling from q(z) ν (G|z1:n) is equivalent to constructing a distribution as β0D′+ K∑ k=1 βkδφk, D′∼DP(αG0),(β0,...,β K) ∼Dir(α,|C(z) 1 |,..., |C(z) K |),φk ∼νk, (16) where {C(z) 1 ,C(z) 2 ,...,C (z) K }is the partition of x1:n characterized by z. The approximation yields the following tractable predictive distribution: q(θ′|ρ,ν) = Eq(G|ρ,ν)[p(θ′|G)] = α α+ nG0(θ′) + K∑ k=1 ∑n i=1 ρi,k α+ n νk(θ′). (17) SV A uses this predictive distribution for sequential approximation of the posterior ofzand φ. p(zn+1,φ(n+1)|x1:n+1) ∝p(xn+1|zn+1,φ(n+1))p(zn+1,φ(n+1)|x1:n) (18) ≈p(xn+1|zn+1,φ(n+1))q(zn+1,φ(n+1)|ρ1:n,ν(n)). (19) While the data is given one by one, SV A sequentially updates the variational parame- ters; the following ρn+1 and ν(n+1) at step n + 1 minimizes the KL divergence between q(zn+1,φ(n+1)|ρ1:n+1,ν(n+1)) and the posterior: ρn+1,k ∝ { (∑n i=1 ρi,k) ∫ θp(xn+1|θ)ν(n) k (dθ) if 1 ≤k≤K α ∫ θp(xn+1|θ)G0(dθ) if k= K+ 1 , (20) ν(n+1) k (dθ) ∝ { G0(dθ) ∏n+1 i=1 p(xi|θ)ρi,k if 1 ≤k≤K G0(dθ)p(xn+1|θ)ρn+1,k if k= K+ 1 . (21) In practice, SV A adds a new component only when ρn+1,K+1 is greater than a threshold ϵ. It uses stochastic gradient descent to ﬁnd and maintain the MAP estimation of parameters instead of calculating the whole distribution νk: ˆφ(n+1) k ←ˆφ(n) k + λn(∇ˆφ(n) k log G0(ˆφ(n) k ) + ∇ˆφ(n) k log p(x|ˆφ(n) k )), (22) where λ(n) k is a learning rate of component k at step n, which decreases as in the Robbins-Monro algorithm. B P RACTICAL ISSUES OF CN-DPM CN-DPN is designed based on strong theoretical foundations, including the nonparametric Bayesian framework. In this section, we further discuss some practical issues of CN-DPM with intuitive explanations. Bounded expansion of CN-DPM. The number of components in the DPM model is determined by the data distribution and the concentration parameter. If the true distribution consists of K clusters, the number of effective components converges to K under an appropriate concentration parameter α. Typically, the number of components is bounded by O(αlog N) (Teh, 2010). Experiments in Appendix H empirically show that CN-DPM does not blindly increase the number of experts. The continued increase in model capacity. Our model capacity keeps increasing as it learns new tasks. However, we believe this is one of the strengths of our method, since it may not make sense to use a ﬁxed-capacity neural network to learn an indeﬁnitely long sequence of tasks. The underlying assumption of using a ﬁxed-capacity model is that the pre-set model capacity is adequate (at least not insufﬁcient) to learn the incoming tasks. On the other hand, CN-DPM approaches the problem in a different direction: start small and add more as needed . This property is essential in task-free settings where the total number of tasks is not known. If there are too many tasks than expected, a ﬁxed-capacity model would not be able to learn them successfully. Conversely, if there are fewer 13Published as a conference paper at ICLR 2020 tasks than expected, resources would be wasted. We argue that expansion is a promising direc- tion since it does not need to ﬁx the model capacity beforehand. Moreover, we also introduce an algorithm to prune redundant experts in Appendix D, Generality of the concentration parameter . The concentration parameter controls how sensitive the model is to new data. In other words, it determines the level of discrepancy between tasks, that makes the tasks modeled by distinct components. As an example, suppose that we are designing a hand-written alphabet classiﬁer that continually learns in the real world. In the development, we only have the character images for half of the alphabets, i.e., from ‘a’ to ‘m’. If we can ﬁnd a good concentration parameter αfor the data from ‘a’ to ‘m’, the sameαcan work well with novel alphabets (i.e., from ‘n’ to ‘z’) because the alphabets would have a similar level of discrepancies between tasks. Therefore, we do not need to access the whole data to determineαif the discrepancy between tasks is steady. C M ODEL ARCHITECTURES AND EXPERIMENTAL DETAILS C.1 B ASE MODELS C.1.1 S PLIT -MNIST Following Hsu et al. (2018), we use two-hidden-layer MLP classiﬁer with 400 hidden units per layer. For generation tasks, we use a simple V AE with the two-hidden-layer MLP encoder and decoder, where each layer contains 400 units. The dimension of the latent code is set to 32. We use ReLU for all intermediate activation functions. C.1.2 MNIST-SVHN AND SPLIT -CIFAR10/100 We use ResNet-18 (He et al., 2016). The input images are transformed to 32×32 RGB images. C.2 CN-DPM C.2.1 S PLIT -MNIST For the classiﬁers in experts, we use a smaller version of the base MLP classiﬁer. In the ﬁrst expert, we set the number of hidden units per layer to 64. In the second or later experts, we introduce 16 new units per layer which are connected to the lower layers of the existing experts. For the encoder and decoder of V AEs, we use a two-layer MLP. The encoder is expanded in the same manner as the classiﬁer. However, we do not share the parameters beyond the encoders; with a latent code of dimension 16, we use the two-hidden-layer MLP decoder as done in the classiﬁer. For generation tasks, we double the size; for example, we set the size of initial and additional hidden units to 128 and 32, respectively. C.2.2 S PLIT -CIFAR10/100 The ResNet-18 base network has eight residual blocks. After passing through 2 residual blocks, the width and height of the feature are halved, and the number of channels is doubled. The initial number of channels is set to 64. For the classiﬁers in CN-DPM, we use a smaller version of ResNet that has only four residual blocks and resizes the feature every block. The initial number of channels is set to 20 in the ﬁrst expert, and four initial channels are added with a new expert. Thus, 4, 8, 16, and 32 channels are added for the four blocks. The ﬁrst layer of each block is connected to the last layer of the previous block of prior experts. For the V AEs, we use a simple CNN-based V AEs. The encoder has two 3×3 convolutions followed by two fully connected layers. Each convolution is followed by 2×2 max-pool and ReLU activation. The numbers of channels and hidden units are doubled after each layer. In the ﬁrst expert, the ﬁrst convolution outputs 32 channels, while four new channels are added with each new expert. 14Published as a conference paper at ICLR 2020 As done for the V AE in Split-MNIST, each expert’s V AE has an unshared decoder with a 64- dimensional latent code. The decoder is the mirrored encoder where 3×3 convolution is replaced by 4×4 transposed convolution with a stride of 2. C.2.3 MNIST-SVHN For the classiﬁer, we use ResNet-18 with 32 channels for the ﬁrst expert and additional 32 channels for each new expert. We use the same V AE as in Split-CIFAR10. C.3 E XPERIMENTAL DETAILS We use the classiﬁer temperature parameter of 0.01 for Split-MNIST, Split-CIFAR10/100, and no temperature parameter on MNIST-SVHN. Weight decay 0.00001 has been used for every model in the paper. Gradients are clipped by value with a threshold of 0.5. All the CN-DPM models are trained by Adam optimizer. During the sleep phase, we train the new expert for multiple epochs with a batch size of 50. In classiﬁcation tasks, we improve the density estimation of V AEs by sampling 16 latent codes and averaging the ELBOs, following Burda et al. (2015). C.3.1 S PLIT -MNIST The learning rate of 0.0001 and 0.0004 has been used for the classiﬁer and V AE of each expert in the classiﬁcation task. We use learning rate 0.003 for the V AE of each expert in generation task. In the generation task, we decay the learning rate of the expert by 0.003 before it enters the wake phase. Following the existing works in V AE literature, we use binarized MNIST for the generation experiments. V AEs are trained to maximize Bernoulli log-likelihood in the generation task, while Gaussian log-likelihood is used for the classiﬁcation task. C.3.2 S PLIT -CIFAR10 The learning rate of 0.005 and 0.0002 has been used for the classiﬁer and V AE of each expert in CIFAR10. We decay the learning rate of the expert by 0.1 before it enters the wake phase. V AEs are trained to maximize Gaussian log-likelihood. C.3.3 S PLIT -CIFAR100 The learning rate of 0.0002 and 0.0001 has been used for the classiﬁer and V AE of each expert in CIFAR10. We decay the learning rate of the expert by 0.2 before it enters the wake phase. V AEs are trained to maximize Gaussian log-likelihood. C.3.4 MNIST-SVHN The learning rate of 0.0001 and 0.0003 has been used for the classiﬁer and V AE of each expert in CIFAR10. We decay the learning rates of classiﬁer and V AE of each expert by 0.5 and 0.1 before it enters the wake phase. V AEs are trained to maximize Gaussian log-likelihood. D P RUNING REDUNDANT EXPERTS Lin (2013) propose a simple algorithm to prune and merge redundant components in DPM models. Following the basic principle of the algorithm, we provide a pruning algorithm for CN-DPM. First, we need to measure the similarities between experts to choose which expert to prune. We compute the log-likelihood lnk = p(xn,yn|ˆφk) of each expert k for data (x1:N,y1:N). As a result, we can obtain Kvectors with N dimensions. We deﬁne the similarity s(k,k′) between two experts kand k′ as the cosine similarity between the two corresponding vectors l·k and l·k′, i.e., s(k,k′) = l·k·l·k′ |l·k||l·k′|. If the similarity is greater than a certain threshold ϵ, we remove one of the experts with smaller Nk = ∑ nρn,k. The Nk data of the removed expert are added to the remaining experts. Figure 4 shows an example of an expert pruning. We test CN-DPM on Split-MNIST with an α higher than the optimal value such that more than ﬁve experts are created. In this case, seven experts 15Published as a conference paper at ICLR 2020 are created. If we build a similarity matrix as shown in Figure 4b, we can see which pair of experts are similar. We then threshold the matrix at 0.9 in Figure 4c and choose expert pairs (2/3) and (5/6) for pruning. Comparing Nk within each pair, we can ﬁnally choose to prune expert 3 and 6. After pruning, the test accuracy marginally drops from 87.07% to 86.01%. 0 20 40 60 80 100Accuracy (%) 0K 12K 24K 36K 48K 60K Learned examples 0 2 4 6Experts (a) Training curve 1 2 3 4 5 6 7 1 2 3 4 5 6 7 (b) Similarity matrix 1 2 3 4 5 6 7 1 2 3 4 5 6 7 (c) Thresholded similarity matrix Figure 4: An example of the expert pruning in the Split-MNIST scenario. E C OMPARISON WITH TASK -BASED METHODS ON SPLIT -MNIST Table 4 compares our method with task-based methods for Split-MNIST classiﬁcation. All the numbers except for our CN-DPM are excerpted from Hsu et al. (2018), in which all methods are trained for four epochs per task with a batch size of 128. Our method is trained for four epochs per task with a batch size of 10. The model architecture used in compared methods is the same as our baselines: a two-hidden-layer MLP with 400 hidden units per layer. All compared methods use a single output head, and the task information is given at training time but not at test time. For CN-DPM, we test two training settings where the ﬁrst one uses task information to select experts, while the second one infers the responsible expert by the DPM principle. Task information is not given at test time in both cases. Notice that regularization methods often suffer from catastrophic forgetting while replay methods yield decent accuracies. Even though the task-free condition is a far more difﬁcult setting, the performance of our method is signiﬁcantly better than regularization and replay methods that exploit the task description. If task information is available at train time, we can utilize it to improve the performance even more. Table 4: Comparison with task-based methods on Split-MNIST classiﬁcation. We report the average of 10 runs with ±standard error of the mean. The numbers except ours are from Hsu et al. (2018). Type Method Task labels Accuracy (%) Regularization EWC (Kirkpatrick et al., 2017) \u0013 19.80 ±0.05 Online EWC (Schwarz et al., 2018) \u0013 19.77 ±0.04 SI (Zenke et al., 2017) \u0013 19.67 ±0.09 MAS (Aljundi et al., 2018) \u0013 19.52 ±0.04 LwF (Li & Hoiem, 2017) \u0013 24.17 ±0.33 Replay GEM (Lopez-Paz & Ranzato, 2017) \u0013 92.20 ±0.12 DGR (Shin et al., 2017) \u0013 91.24 ±0.33 RtF (van de Ven & Tolias, 2018) \u0013 92.56 ±0.21 Expansion CN-DPM \u0013 93.81 ±0.07 CN-DPM \u0017 93.70 ±0.07 Upper bound (iid) 97.53 ±0.30 16Published as a conference paper at ICLR 2020 Table 5: Fuzzy Split-MNIST Method Acc. (%) Param. Fine-tune 28.41 ±0.52 478 K Reservoir 88.64 ±0.48 478 K CN-DPM 93.22 ±0.07 524 K 0K 12K 24K 36K 48K 60K Learned examples Cumulative task proportion Task 1 Task 2 Task 3 Task 4 Task 5 Figure 5: Scenario conﬁguration of Fuzzy Split-MNIST Label: 0  Label: 0  Label: 1  Label: 0  Label: 0  Label: 1  Label: 1  Label: 0  Label: 0  Label: 0 Label: 1  Label: 0  Label: 1  Label: 3  Label: 2  Label: 2  Label: 2  Label: 3  Label: 2  Label: 3 Label: 5  Label: 5  Label: 5  Label: 4  Label: 5  Label: 4  Label: 5  Label: 4  Label: 4  Label: 6 Label: 7  Label: 7  Label: 7  Label: 7  Label: 6  Label: 6  Label: 6  Label: 6  Label: 8  Label: 8 Label: 9  Label: 8  Label: 8  Label: 8  Label: 8  Label: 9  Label: 9  Label: 8  Label: 8  Label: 8 Figure 6: Examples of generation samples by CN-DPM trained on Split-MNIST. F F UZZY SPLIT -MNIST In addition, we experiment with the case where the task boundaries are not clearly deﬁned, which we call Fuzzy-Split-MNIST. Instead of discrete task boundaries, we have transition stages between tasks where the data of existing and new tasks are mixed, but the proportion of the new task linearly increases. This condition adds another level of difﬁculty since it makes the methods unable to rely on clear task boundaries. The scenario is visualized in Figure 5. As shown in Table 5, CN-DPM can perform continual learning without task boundaries. G G ENERATION OF SAMPLES Even in discriminative tasks where the goal is to model p(y|x), CN-DPM learns the joint distri- bution p(x,y). Since CN-DPM is a complete generative model, it can generate (x,y) pairs. To generate a sample, we ﬁrst sample z from p(z) which is modeled by the categorical distribution Cat(N1 N ,N2 N ,..., NK N ), i.e., choose an expert. Given z = k, we ﬁrst sample x from the generator p(x; φG k), and then sample y from the discriminator p(y|x; φD k ). Figure 6 presents 50 sample ex- amples generated from a CN-DPM trained on Split-MNIST for a single epoch. We observe that CN-DPM successfully generates examples of all tasks with no catastrophic forgetting. H E XPERIMENTS WITH LONGER CONTINUAL LEARNING SCENARIOS We present experiments with much longer continual learning scenarios on Split-MNIST, Split- CIFAR10 and Split-CIFAR100 in Table 6, 7 and 8, respectively. We report the average of 10 runs with ±standard error of the mean. To compare with the default 1-epoch scenario, we carry out ex- periments that repeat each task 10 times, which are denoted 10 Epochs. In addition, we also present 17Published as a conference paper at ICLR 2020 the results of repeating the whole scenario 10 times, which are denoted 1 Epoch ×10. For example, in Split-MNIST, the 10 Epochs scenario consists of 10-epoch 0/1, 10-epoch 2/3, ..., 10-epoch 8/9 tasks. On the other hand, the 1 Epoch ×10 scenario revisits each task multiple times, i.e., 1-epoch 0/1, 1-epoch 2/3, ..., 1-epoch 8/9, 1-epoch 0/1, ..., 1-epoch 8/9. We use the same hyperparameters tuned for the 1-epoch scenario. We ﬁnd that the accuracy of Reservoir drops as the length of each task increases. As mentioned in the main text, this phenomenon seems to be caused by overﬁtting on the samples in the replay memory. Since only a small number of examples in the memory represent each task, replaying them for a long period degrades the performance. On the other hand, the performance of our CN-DPM improves as the learning process is extended. In the 1 Epoch ×10 setting, CN-DPM shows similar performance with 10 Epoch since the model sees each data point 10 times in both scenarios. On the other hand, Reservoir’s scores in the1 Epoch ×10 largely increase compared to both 1 Epoch and 10 Epoch This difference can be explained by how the replay memory changes while training progresses. In the 10 Epoch setting, if a task is ﬁnished, it is not visited again. Therefore, the examples of the task in the replay memory monoton- ically decreases, and the remaining examples are replayed repeatedly. As the training progresses, the model is overﬁtted to the old examples in the memory and fails to generalize in the old tasks. In contrast, in 1 Epoch ×10 setting, each task is revisited multiple times, and each time a task is revis- ited, the replay memory is also updated with the new examples of the task. Therefore, the overﬁtting problem in the old tasks is greatly relieved. Another important remark is that CN-DPM does not blindly increase the number of experts. If we add a new expert at every constant steps, we would have 10 times more experts in the longer scenarios. However, this is not the case. CN-DPM determines whether it needs a new expert on a data-by-data basis such that the number of experts is determined by the task distribution, not by the length of training. Table 6: Experiments with longer training episodes on Split-MNIST Method 1 Epoch 10 Epochs 1 Epoch ×10 Acc. (%) Param. Acc. (%) Param. Acc. (%) Param. iid-ofﬂine 98.63 ±0.01 478 K 98.63 ±0.01 478 K 98.63 ±0.01 478 K iid-online 96.18 ±0.19 478 K 97.67 ±0.05 478 K 97.67 ±0.05 478 K Fine-tune 19.43 ±0.02 478 K 19.68 ±0.01 478 K 20.27 ±0.26 478 K Reservoir 85.69 ±0.48 478 K 78.82 ±0.71 478 K 92.06 ±0.11 478 K CN-DPM 93.23 ±0.09 524 K 94.39 ±0.04 524 K 94.15 ±0.04 616 K Table 7: Experiments with longer training episodes on Split-CIFAR10 Method 1 Epoch 10 Epochs 1 Epoch ×10 Acc. (%) Param. Acc. (%) Param. Acc. (%) Param. iid-ofﬂine 93.17 ±0.03 11 .2M 93.17 ±0.03 11 .2M 93.17 ±0.03 11 .2M iid-online 62.79 ±1.30 11 .2M 83.19 ±0.27 11 .2M 83.19 ±0.27 11 .2M Fine-tune 18.08 ±0.13 11 .2M 19.31 ±0.03 11 .2M 19.33 ±0.03 11 .2M Reservoir 44.00 ±0.92 11 .2M 43.82 ±0.53 11 .2M 51.44 ±0.42 11 .2M CN-DPM 45.21 ±0.18 4 .60M 46.98 ±0.18 4 .60M 47.10 ±0.16 4 .60M I E XPERIMENTS WITH DIFFERENT MEMORY SIZES In Split-CIFAR10/100 experiments in the main text, we set the memory size of Reservoir and CN- DPM to 1000, following Aljundi et al. (2019b). Table 9 compares the experimental results with different memory sizes of 500 and 1000 on Split-CIFAR10/100. Compared to Reservoir, whose per- formance drops signiﬁcantly with smaller memory, CN-DPM’s accuracy drop is relatively marginal. 18Published as a conference paper at ICLR 2020 Table 8: Experiments with longer training episodes on Split-CIFAR100 Method 1 Epoch 10 Epochs 1 Epoch ×10 Acc. (%) Param. Acc. (%) Param. Acc. (%) Param. iid-ofﬂine 73.80 ±0.11 11 .2M 73.80 ±0.11 11 .2M 73.80 ±0.11 11 .2M iid-online 20.46 ±0.30 11 .2M 54.58 ±0.27 11 .2M 54.58 ±0.27 11 .2M Fine-tune 2.43 ±0.05 11 .2M 3.99 ±0.03 11 .2M 4.30 ±0.02 11 .2M Reservoir 10.01 ±0.35 11 .2M 6.61 ±0.20 11 .2M 14.53 ±0.35 11 .2M CN-DPM 20.10 ±0.12 19 .2M 20.95 ±0.09 19 .2M 20.67 ±0.13 19 .2M Table 9: Experiments with different memory sizes. Method Memory Split-CIFAR10 Acc. (%) Split-CIFAR100 Acc. (%) 1 Epoch 10 Epoch 1 Epoch 10 Epoch Reservoir 500 33 .53 ±1.03 34 .46 ±0.49 6 .24 ±0.25 4 .99 ±0.09 CN-DPM 500 43.07 ±0.16 47.01 ±0.22 19.17 ±0.13 20.77 ±0.11 Reservoir 1000 44 .00 ±0.92 43 .82 ±0.53 10 .01 ±0.35 6 .61 ±0.20 CN-DPM 1000 45.21 ±0.18 46.98 ±0.18 20.10 ±0.12 20.95 ±0.09 J T HE EFFECT OF CONCENTRATION PARAMETER Table 10 shows the results of CN-DPM on Split-MNIST classiﬁcation according to the concentration parameter α, which deﬁnes the prior of how sensitive CN-DPM is to new data. With a higher α, an expert tends to be created more easily. In the experiment reported in the prior sections, we set log α = −400. At log α = −600, too few experts are created, and the accuracy is rather low. As αincreases, the number of experts grows along with the accuracy. Although the CN-DPM model is task-free and automatically decides the task assignments to experts, we still need to tune the concentration parameter to ﬁnd the best balance point between performance and model capacity, as all Bayesian nonparametric models require. Table 10: The effects of concentration parameter α. log α Acc. (%) Experts Param. −600 54.04 ±2.22 3 .20 ±0.13 362 K −400 93.23 ±0.09 5 .00 ±0.00 524 K 80 93.54 ±0.21 14 .4 ±1.35 1 .44M 0 20 40 60 80 100Accuracy (%) 0.0 0.5 1.0 1.5Param. (M) 0K 12K 24K 36K 48K 60K Learned examples 0 5 10 15Experts CN-DPM (log = 600) CN-DPM (log = 400) CN-DPM (log = 80) Figure 7: The effects of concentration pa- rameter α. K T HE EFFECT OF PARAMETER SHARING Table 11 compares when the parameters are shared between experts and when they are not shared. By sharing the parameters, we could reduce the number of parameters by approximately 38% with- out sacriﬁcing accuracy. 19Published as a conference paper at ICLR 2020 Table 11: The effects of parameter sharing. Model Acc. (%) Experts Param. CN-DPM 93.23 ±0.09 5 524 K CN-DPM w/o PS 93.30 ±0.24 5 839 K L T RAINING GRAPHS Figure 8 shows the training graphs of our experiments. In addition to the performance metrics, we present the number of experts in CN-DPM and compare the total number of parameters with the baselines. The bold lines represent the average of the 10 runs while the faint lines represent individual runs. Figure 9 and Figure 10 show how the accuracy of each task changes during training. We also present the average accuracy of learned tasks at the bottom right. M C OMPARISON WITH THE CURL Continual Unsupervised Representation Learning (CURL) (Rao et al., 2019) is a parallel work that shares some characteristics with our CN-DPM in terms of model expansion and short-term memory. However, there are several key differences that distinguish our method from CURL, which will be elaborated in this section. Following the notations of Rao et al. (2019), here y denotes the cluster assignment, and zdenotes the latent variable. 1. The Generative Process. The primary goal of CURL is to continually learn a uniﬁed latent rep- resentation z, which is shared across all tasks. Therefore, the generative model of CURL explicitly consists of the latent variable zas summarized as follows: p(x,y,z ) = p(y)p(z|y)p(x|z) where y∼Cat(π), z∼N(µz(y),σ2 z(y)), x∼Bernoulli(µx(z)). The overall distribution of z is the mixture of Gaussians, and z includes the information of y such that x and y are conditionally independent given z. Then, z is fed into a single decoder network µx to generate the mean of x, which is modeled by a Bernoulli distribution. On the other hand, the generative version of CN-DPM, which does not include classiﬁers, has a simpler generative process: p(x,y) = p(y)p(x|y) where y∼Cat(π), x∼p(x|y). The choice of p(x|y) here is not necessarily restricted to V AEs (Kingma & Welling, 2014); one may use other kinds of explicit density models such as PixelRNN (Oord et al., 2016). Even if we use V AEs to modelp(x|y), the generative process is different from CURL: p(x,y,z ) = p(y)p(z)p(x|y,z) where y∼Cat(π), z∼N(0,I), x∼Bernoulli(µy x(z)). Unlike CURL, CN-DPM generates yand zindependently and maintains a separate decoder µy x for each cluster y. 2. The Necessity for Generative Replay in CURL . CURL periodically saves a copy of its pa- rameters and use it to generate samples of learned distribution. The generated samples are played together with new data such that the main model does not forget previously learned knowledge. This process is called generative replay. The generative replay is an essential element in CURL, unlike our CN-DPM. CURL assumes a factorized variational posterior q(y,z|x) = q(y|x)q(z|x,y) where q(y|x) and q(z|x,y) are modeled by separate output heads of the encoder neural network. However, the output head for q(y|x) is basically a gating network that could be vulnerable to catastrophic for- getting, as mentioned in Section 3.1. Moreover, CURL shares a single decoder µx across all tasks. As a consequence, expansion alone is not enough to stop catastrophic forgetting, and CURL needs another CL method to prevent catastrophic forgetting in the shared components. This is the main reason why the generative replay is crucial in CURL. As shown in the ablation test of Rao et al. (2019), the performance of CURL drops without the generative replay. In contrast, the components of CN-DPM are separated for each task (although they may share low-level representations) such that no additional treatment is needed. 20Published as a conference paper at ICLR 2020 0 20 40 60 80 100Accuracy (%) 0 250 500Param. (K) 0K 12K 24K 36K 48K 60K Learned examples 0 2 4Experts iid-online Fine-tune Reservoir CN-DPM (a) Split-MNIST 0.20 0.25 0.30 0.35 0.40bits/dim 0.0 0.5 1.0Param. (M) 0K 12K 24K 36K 48K 60K Learned examples 0 4 8 12Experts (b) Split-MNIST (Gen.) 30 40 50 60 70 80 90 100Accuracy (%) 0 3 6 9 12Param. (M) 0K 20K 40K 60K 80K 100K 120K Learned examples 0 1 2Experts (c) MNIST-SVHN 0 10 20 30 40 50 60 70Accuracy (%) 0 5 10Param. (M) 0K 10K 20K 30K 40K 50K Learned examples 0 2 4 6Experts (d) Split-CIFAR10 0 5 10 15 20Accuracy (%) 0 5 10 15 20Param. (M) 0K 10K 20K 30K 40K 50K Learned examples 0 5 10 15 20Experts (e) Split-CIFAR100 Figure 8: Full training graphs. 21Published as a conference paper at ICLR 2020 T1 T2 T3 T4 T5 0 20 40 60 80 100Task 1 T1 T2 T3 T4 T5 0 20 40 60 80 100Task 2 T1 T2 T3 T4 T5 0 20 40 60 80 100Task 3 T1 T2 T3 T4 T5 0 20 40 60 80 100Task 4 T1 T2 T3 T4 T5 0 20 40 60 80 100Task 5 T1 T2 T3 T4 T5 0 20 40 60 80 100Average Fine-tune Reservoir CN-DPM Figure 9: Accuracy for each task in Split-CIFAR10. T5 T10 T15 T20 0 20 40 60Task 1 T5 T10 T15 T20 0 20 40 60Task 2 T5 T10 T15 T20 0 20 40 60Task 3 T5 T10 T15 T20 0 20 40 60Task 4 T5 T10 T15 T20 0 20 40 60Task 5 T5 T10 T15 T20 0 20 40 60Task 6 T5 T10 T15 T20 0 20 40 60Task 7 T5 T10 T15 T20 0 20 40 60Task 8 T5 T10 T15 T20 0 20 40 60Task 9 T5 T10 T15 T20 0 20 40 60Task 10 T5 T10 T15 T20 0 20 40 60Task 11 T5 T10 T15 T20 0 20 40 60Task 12 T5 T10 T15 T20 0 20 40 60Task 13 T5 T10 T15 T20 0 20 40 60Task 14 T5 T10 T15 T20 0 20 40 60Task 15 T5 T10 T15 T20 0 20 40 60Task 16 T5 T10 T15 T20 0 20 40 60Task 17 T5 T10 T15 T20 0 20 40 60Task 18 T5 T10 T15 T20 0 20 40 60Task 19 T5 T10 T15 T20 0 20 40 60Task 20 T5 T10 T15 T20 0 20 40 60Average Fine-tune Reservoir CN-DPM Figure 10: Accuracy for each task in Split-CIFAR100. 22",
      "meta_data": {
        "arxiv_id": "2001.00689v2",
        "authors": [
          "Soochan Lee",
          "Junsoo Ha",
          "Dongsu Zhang",
          "Gunhee Kim"
        ],
        "published_date": "2020-01-03T02:07:31Z",
        "pdf_url": "https://arxiv.org/pdf/2001.00689v2.pdf"
      }
    },
    {
      "title": "Understanding mcmc dynamics as ﬂows on the wasserstein spac",
      "abstract": "It is known that the Langevin dynamics used in MCMC is the gradient flow of\nthe KL divergence on the Wasserstein space, which helps convergence analysis\nand inspires recent particle-based variational inference methods (ParVIs). But\nno more MCMC dynamics is understood in this way. In this work, by developing\nnovel concepts, we propose a theoretical framework that recognizes a general\nMCMC dynamics as the fiber-gradient Hamiltonian flow on the Wasserstein space\nof a fiber-Riemannian Poisson manifold. The \"conservation + convergence\"\nstructure of the flow gives a clear picture on the behavior of general MCMC\ndynamics. The framework also enables ParVI simulation of MCMC dynamics, which\nenriches the ParVI family with more efficient dynamics, and also adapts ParVI\nadvantages to MCMCs. We develop two ParVI methods for a particular MCMC\ndynamics and demonstrate the benefits in experiments.",
      "full_text": "Understanding MCMC Dynamics as Flows on the Wasserstein Space Chang Liu 1 Jingwei Zhuo 1 Jun Zhu 1 Abstract It is known that the Langevin dynamics used in MCMC is the gradient ﬂow of the KL divergence on the Wasserstein space, which helps conver- gence analysis and inspires recent particle-based variational inference methods (ParVIs). But no more MCMC dynamics is understood in this way. In this work, by developing novel concepts, we propose a theoretical framework that recognizes a general MCMC dynamics as the ﬁber-gradient Hamiltonian ﬂow on the Wasserstein space of a ﬁber-Riemannian Poisson manifold. The “conser- vation + convergence” structure of the ﬂow gives a clear picture on the behavior of general MCMC dynamics. The framework also enables ParVI simulation of MCMC dynamics, which enriches the ParVI family with more efﬁcient dynamics, and also adapts ParVI advantages to MCMCs. We develop two ParVI methods for a particular MCMC dynamics and demonstrate the beneﬁts in experiments. 1 Introduction Dynamics-based Markov chain Monte Carlo methods (MCMCs) in Bayesian inference have drawn great attention because of their wide applicability, efﬁciency, and scala- bility for large-scale datasets (Neal, 2011; Welling & Teh, 2011; Chen et al., 2014; 2016; Li et al., 2019). They draw samples by simulating a continuous-time dynamics, or more precisely, a diffusion process, that keeps the target distribu- tion invariant. However, they often exhibit slow empirical convergence and relatively small effective sample size, due to the positive auto-correlation of the samples. Another type of inference methods, called particle-based variational inference methods (ParVIs), aim to deterministically update samples, or particles as they call them, so that the particle 1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Cen- ter, Tsinghua-Fuzhou Inst. for Data Tech., THBI Lab, Tsinghua University, Beijing, 100084, China. Correspondence to: Jun Zhu <dcszj@tsinghua.edu.cn>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). distribution minimizes the KL divergence to the target distri- bution. They fully exploit the approximation ability of a set of particles by imposing an interaction among them, so they are more particle-efﬁcient. Optimization-based principle also makes them convergence faster. Stein variational gradi- ent descent (SVGD) (Liu & Wang, 2016) is the most famous representative, and the ﬁeld is under an active development both in theory (Liu, 2017; Chen et al., 2018b;a; Liu et al., 2019) and application (Liu et al., 2017; Pu et al., 2017; Zhuo et al., 2018; Yoon et al., 2018). The study on the relation between the two families starts from their interpretations on the Wasserstein space P(M) supported on some smooth manifold M(Villani, 2008; Am- brosio et al., 2008). It is deﬁned as the space of distributions P(M) := {q|qis a probability measure on Mand ∃x0 ∈M s.t.Eq(x)[d(x,x0)2] <∞} with the well-known Wasserstein distance. It is very general yet still has necessary structures. With its canonical metric, the gradient ﬂow (steepest descending curves) of the KL divergence is deﬁned. It is known that the Langevin dynam- ics (LD) (Langevin, 1908; Roberts et al., 1996), a particular type of dynamics in MCMC, simulates the gradient ﬂow on P(M) (Jordan et al., 1998). Recent analysis reveals that existing ParVIs also simulate the gradient ﬂow (Chen et al., 2018a; Liu et al., 2019), so they simulate the same dynamics as LD. However, besides LD, there are more types of dy- namics in the MCMC ﬁeld that converge faster and produce more effective samples (Neal, 2011; Chen et al., 2014; Ding et al., 2014), but no ParVI yet simulates them. These more general MCMC dynamics have not been recognized as a process on the Wasserstein space P(M), and this poses an obstacle towards ParVI simulations. On the other hand, the convergence behavior of LD becomes clear when viewing LD as the gradient ﬂow of the KL divergence on P(M) (e.g., Cheng & Bartlett (2017)), which leads distributions to the target in a steepest way. However, such knowledge on other MCMC dynamics remains obscure, except a few. In fact, a general MCMC dynamics is only guaranteed to keep the target distribution invariant (Ma et al., 2015), but unnec- essarily drives a distribution towards the target steepest. So it is hard for the gradient ﬂow formulation to cover general MCMC dynamics. In this work, we propose a theoretical framework that gives arXiv:1902.00282v3  [stat.ML]  4 Jul 2019Understanding MCMC Dynamics as Flows on the Wasserstein Space a uniﬁed view of general MCMC dynamics on the Wasser- stein space P(M). We establish the framework by two generalizations over the concept of gradient ﬂow towards a wider coverage: (a) we introduce a novel concept called ﬁber-Riemannian manifold M, where only the Riemannian structure on each ﬁber (roughly a decomposed submanifold, or a slice of M) is required, and we develop the novel no- tion of ﬁber-gradient ﬂow on its Wasserstein space P(M); (b) we also endow a Poisson structure to the manifold M and exploit the corresponding Hamiltonian ﬂow on P(M). Combining both explorations, we deﬁne a ﬁber-Riemannian Poisson (fRP) manifold Mand a ﬁber-gradient Hamiltonian (fGH) ﬂow on its Wasserstein space P(M). We then show that any regular MCMC dynamics is the fGH ﬂow on the Wasserstein space P(M) of an fRP manifold M, and there is a correspondence between the dynamics and the structure of the fRP manifold M. This uniﬁed framework gives a clear picture on the behavior of MCMC dynamics. The Hamiltonian ﬂow conserves the KL divergence to the target distribution, while the ﬁber- gradient ﬂow minimizes it on each ﬁber, driving each con- ditional distribution to meet the corresponding conditional target. The target invariant requirement is recovered in which case the ﬁber-gradient is zero, and moreover, we rec- ognize that the ﬁber-gradient ﬂow acts as a stabilizing force on each ﬁber. It enforces convergence ﬁber-wise, making the dynamics in each ﬁber robust to simulation with the noisy stochastic gradient, which is crucial for large-scale in- ference tasks. This generalizes the discussion of Chen et al. (2014) and Betancourt (2015) on Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2011; Betancourt, 2017) to general MCMCs. In our framework, different MCMCs correspond to different ﬁber structures and ﬂow components. They can be categorized into three types, each of which has its particular behavior. We make a uniﬁed study on various existing MCMCs under the three types. Our framework also bridges the ﬁelds of MCMCs and ParVIs, so that on one hand, the gate to the reservoir of MCMC dynamics is opened to the ParVI family and abun- dant efﬁcient dynamics are enabled beyond LD, and on the other hand, MCMC dynamics can be now simulated in the ParVI fashion, inheriting advantages like particle-efﬁciency. To demonstrate this, we develop two ParVI simulation meth- ods for the Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) dynamics (Chen et al., 2014). We show the merits of using SGHMC dynamics over LD in the ParVI ﬁeld, and ParVI advantages over conventional stochastic simulation in MCMC. Related work Ma et al. (2015) give a complete recipe on general MCMC dynamics. The recipe guarantees the target invariant principle, but leaves the behavior of these dynam- ics unexplained. Recent analysis towards a broader kind of dynamics via the Fokker-Planck equation (Kondratyev & V orotnikov, 2017; Bruna et al., 2017) is still within the gradient ﬂow formulation, thus not general enough. On connecting MCMC and ParVI, Chen et al. (2018a) ex- plore the correspondence between LD and Wasserstein gra- dient ﬂow, and develop new implementations for dynamics simulation. However, their consideration is still conﬁned on LD, leaving more general MCMC dynamics untouched. Gallego & Insua (2018) formulate the dynamics of SVGD as a particular kind of MCMC dynamics, but no existing MCMC dynamics is recognized as a ParVI. More recently, Taghvaei & Mehta (2019) derive an accelerated ParVI that is similar to one of our ParVI simulations of SGHMC. The derivation does not utilize the dynamics and the method con- nects to SGHMC only algorithmically. Our theory solidates our ParVI simulations of SGHMC, and enables extensions to more dynamics. 2 Preliminaries We ﬁrst introduce the recipe for general MCMC dynam- ics (Ma et al., 2015), and prior knowledge on ﬂows on a smooth manifold Mand its Wasserstein space P(M). A smooth manifold Mis a topological space that locally behaves like an Euclidean space. Since the recipe describes a general MCMC dynamics in an Euclidean space RM, it sufﬁces to only consider Mthat is globally diffeomor- phic to RM, which is its global coordinate system. For brevity we use the same notation for a point on Mand its coordinates due to their equivalence. A tangent vector v at x ∈M can be viewed as the differentiation along the curve that is tangent to vat x, so vcan be expressed as the combination v= ∑M i=1 vi∂i of the differentiation operators {∂i := ∂ ∂xi}M i=1, which serve as a set of basis of the tangent space TxMat x. The cotangent space T∗ xMat x is the dual space of TxM, and the cotangent bundle is the union T∗M:= ⋃ x∈MT∗ xM. We adopt Einstein convention to omit the summation symbol for a pair of repeated indices in super- and sub-scripts (e.g., v = vi∂i := ∑M i=1 vi∂i). We assume the target distribution to be absolutely continuous so that we have its density function p. 2.1 The Complete Recipe of MCMC Dynamics The fundamental requirement on MCMCs is that the target distribution pis kept stationary under the MCMC dynamics. Ma et al. (2015) give a general recipe for such a dynamics expressed as a diffusion process in an Euclidean space RM: dx= V(x) dt+ √ 2D(x) dBt(x), Vi(x) = 1 p(x)∂j ( p(x) ( Dij(x) + Qij(x) )) , (1) for any positive semi-deﬁnite matrix DM×M (diffusion matrix) and any skew-symmetric matrix QM×M (curl ma- trix), where Bt(x) denotes the standard Brownian motion in RM. The term V(x) dtrepresents a deterministic drift andUnderstanding MCMC Dynamics as Flows on the Wasserstein Space √ 2D(x) dBt(x) a stochastic diffusion. It is also shown that if Dis positive deﬁnite, pis the unique stationary distri- bution. Moreover, the recipe is complete, i.e., any diffusion process with pstationary can be cast into this form. The recipe gives a universal view and a uniﬁed way to ana- lyze MCMCs. In large scale Bayesian inference tasks, the stochastic gradient (SG), a noisy estimate of (∂jlog p) on a randomly selected data mini-batch, is crucially desired for data scalability. The dynamics is compatible with SG, since the variance of the drift is of higher order of the dif- fusion part (Ma et al., 2015; Chen et al., 2015). In many MCMC instances, x= (θ,r) is taken as an augmentation of the target variable θby an auxiliary variable r. This could encourage the dynamics to explore a broader area to reduce sample autocorrelation and improve efﬁciency (e.g., Neal (2011); Ding et al. (2014); Betancourt et al. (2017)). 2.2 Flows on a Manifold The mathematical concept of the ﬂow associated to a vector ﬁeld Xon Mis a set of curves onM, {(ϕt(x))t |x∈M}, such that the curve (ϕt(x))t through point x∈M satisﬁes ϕ0(x) = xand that its tangent vector at x, d dtϕt(x) ⏐⏐ t=0, coincides with the vector X(x). For any vector ﬁeld, its ﬂow exists at least locally (Do Carmo (1992), Sec. 0.5). We introduce two particular kinds of ﬂows for our concern. 2.2.1 G RADIENT FLOWS We consider the gradient ﬂow on Minduced by a Rieman- nian structure g(e.g., Do Carmo (1992)), which gives an in- ner product gx(·,·) in each tangent space TxM. Expressed in coordinates, gx(u,v) = gij(x)uivj,∀u = ui∂i,v = vi∂i ∈ TxM, and the matrix (gij(x)) is required to be symmetric (strictly) positive deﬁnite. The gradient of a smooth function f on Mcan then be deﬁned as the steepest ascending direction and has the coordinate expression: grad f(x) = gij(x)∂jf(x)∂i ∈TxM, where gij(x) is the entry of the inverse matrix of (gij(x)). It is a vector ﬁeld and determines a gradient ﬂow. On P(M), a Riemannian structure is available with a Rie- mannian support (M,g) (Otto, 2001; Villani, 2008; Am- brosio et al., 2008). The tangent space at q ∈P (M) is recognized as (Villani (2008), Thm. 13.8; Ambrosio et al. (2008), Thm. 8.3.1): TqP(M) = {grad f |f ∈C∞c (M)} L2 q(M) , where C∞ c (M) is the set of compactly supported smooth functions on M, L2 q(M) is the Hilbert space {X: vector ﬁeld on M |Eq[g(X,X)] < ∞}with inner product ⟨X,Y ⟩L2q := Eq(x)[gx(X(x),Y (x))], and the over- line means closure. The tangent space TqPinherits an inner product from L2 q(M), which deﬁnes the Riemannian struc- ture on P(M). It is consistent with the Wasserstein distance (Benamou & Brenier, 2000). With this structure, the gra- dient of the KL divergence KLp(q) := ∫ Mlog(q/p) dqis given explicitly (Villani (2008), Formula 15.2, Thm. 23.18): grad KLp(q) = grad log(q/p) ∈TqP(M). (2) Noting that TqPis a linear subspace of the Hilbert space L2 q(M), an orthogonal projection πq : L2 q(M) →TqPcan be uniquely deﬁned. For any X ∈L2 q(M), πq(X) is the unique vector in TqPsuch that div(qX) = div( qπq(X)) (Ambrosio et al. (2008), Lem. 8.4.2), where div is the di- vergence on Mand div(qX) = ∂i(qXi) when q is the density w.r.t. the Lebesgue measure of the coordinate space RM. The projection can also be explained with a physical intuition. Let X ∈L2 q(M) be a vector ﬁeld on M, and let its ﬂow act on the random variable xof q. The trans- formed random variable ϕt(x) speciﬁes a distribution qt, and a distribution curve (qt)t is then induced by X. The tangent vector of such (qt)t at qis exactly πq(X). 2.2.2 H AMILTONIAN FLOWS The Hamiltonian ﬂow is an abstraction of the Hamilto- nian dynamics in classical mechanics (Marsden & Ratiu, 2013). It is deﬁned in association to a Poisson struc- ture (Fernandes & Marcut (2014)) on a manifold M, which can be expressed either as a Poisson bracket {·,·}: C∞(M) ×C∞(M) → C∞(M), or equivalently as a bivector ﬁeld β : T∗M× T∗M → C∞(M) via the re- lation β(df,dh) = {f,h}. Expressed in coordinates, βx(df(x),dh(x)) = βij(x)∂if(x)∂jh(x), where the ma- trix (βij(x)) is required to be skew-symmetric and satisfy: βil∂lβjk + βjl∂lβki + βkl∂lβij = 0,∀i,j,k. (3) The Hamiltonian vector ﬁeld of a smooth function f on M is deﬁned as Xf(·) := {·,f}, with coordinate expression: Xf(x) = βij(x)∂jf(x)∂i ∈TxM. (4) A Hamiltonian ﬂow {(ϕt(x))t}is then determined by Xf. Its key property is that it conserves f: f(ϕt(x)) is constant w.r.t. t. The Hamiltonian ﬂow may be more widely known on a symplectic manifold or more particularly a cotangent bundle (e.g., Da Silva (2001); Marsden & Ratiu (2013)), but these cases are not general enough for our purpose (e.g., they require Mto be even-dimensional). On P(M), a Poisson structure can be induced by the one {·,·}Mof M. Consider linear functions on P(M) in the form Ff : q↦→Eq[f] for f ∈C∞ c (M). A Poisson bracket for these linear functions can be deﬁned as (e.g., Lott (2008), Sec. 6; Gangbo et al. (2010), Sec. 7.2): {Ff,Fh}P:= F{f,h}M. (5) This bracket can be extended for any smooth function F by its linearization at q, which is a linear function Ff such that grad Ff(q) = grad F(q). The extended bracket is then given by {F,H}P(q):= {Ff,Fh}P(q) (Gangbo et al. (2010), Rem. 7.8), where Ff, Fh are the linearizations at qof functions F, H. The Hamiltonian vector ﬁeld of F isUnderstanding MCMC Dynamics as Flows on the Wasserstein Space then identiﬁed as (Gangbo et al. (2010), Sec. 7.2): XF(q) = XFf(q) = πq(Xf) ∈TqP(M). (6) On the same topic, Ambrosio & Gangbo (2008) study the existence and simulation of the Hamiltonian ﬂow on P(M) for Mas a symplectic Euclidean space, and verify the con- servation of Hamiltonian under certain conditions. Gangbo et al. (2010) investigate the Poisson structure on the alge- braic dual (C∞ c (M))∗, a superset of P(M), and ﬁnd that the canonical Poisson structure induced by the Lie structure of C∞ c (M) coincides with Eq. (5). Their consideration is also for symplectic Euclidean M, but the procedures and conclusions can be directly adapted to Riemannian Pois- son manifolds. Lott (2008) considers the Poisson structure Eq. (5) on the space of smooth distributions on a Poisson manifold M, and ﬁnd that it is the restriction of the Poisson structure of (C∞ c (M))∗by Gangbo et al. (2010). 3 Understanding MCMC Dynamics as Flows on the Wasserstein Space P(M) This part presents our main discovery that connects MCMC dynamics and ﬂows on the Wasserstein space P(M). We ﬁrst work on the two concepts and introduce novel concepts for preparation, then propose the uniﬁed framework and analyze existing MCMC instances under the framework. 3.1 Technical Development We excavate into MCMC and Wasserstein ﬂows and intro- duce novel concepts in preparation for the framework. On the MCMC side Noting that ﬂows on P(M) are de- terministic while MCMCs involve stochastic diffusion, we ﬁrst reformulate MCMC dynamics as an equivalent deter- ministic one for uniﬁcation. Here we say two dynamics are equivalent if they produce the same distribution curve. Lemma 1 (Equivalent deterministic MCMC dynamics) . The MCMC dynamics Eq. (1) with symmetric diffusion ma- trix Dis equivalent to the deterministic dynamics in RM: dx= Wt(x) dt, (Wt)i = Dij∂jlog(p/qt) + Qij∂jlog p+ ∂jQij, (7) where qt is the distribution density of xat time t. Proof is provided in Appendix A.1. For any q ∈P(RM), the projected vector ﬁeld πq(W) can be treated as a tangent vector at q, so W deﬁnes a vector ﬁeld on P(RM). In this way, we give a ﬁrst view of an MCMC dynamics as a Wasserstein ﬂow. An equivalent ﬂow with a richer structure will be given in Theorem 5. This expression also helps understanding Barbour’s gen- erator A(Barbour, 1990) of an MCMC dynamics, which can be used in Stein’s method (Stein, 1972) of constructing distribution metrics. For instance the standard Langevin dynamics induces the Stein’s operator, and it in turn pro- duces a metric called the Stein discrepancy (Gorham & Mackey, 2015), which inspires SVGD, and Liu & Zhu (2018) consider the Riemannian counterparts. The Bar- bour’s generator maps a functionf ∈C∞ c (RM) to another (Af)(x) := d dtEqt[f] ⏐⏐ t=0, where (qt)t obeys initial condi- tion q0 = δx(Dirac measure). In terms of the linear function Ff on P(RM), we recognize (Af)(x) = d dtFf(qt) ⏐⏐ t=0 = ⟨grad Ff,πq0 (W0)⟩Tq0 Pas the directional derivative of Ff along (qt)t at q0. This knowledge gives the expression: Af = 1 p∂j [ p ( Dij + Qij) (∂if) ] , (8) which meets existing results ( e.g., Gorham et al. (2016), Thm. 2). Details are provided in Appendix A.2. On the Wasserstein ﬂow side We deepen the knowledge on ﬂows on P(M) with a Riemannian and Poisson structure of M.1 The gradient of KLp is given by Eq. (2), but its Hamiltonian vector ﬁeld is not directly available due to its non-linearity. We ﬁrst develop an explicit expression for it. Lemma 2 (Hamiltonian vector ﬁeld of KL on P(M)). Let β be the bivector ﬁeld form of a Poisson structure on M and P(M) endowed with the induced Poisson structure described in Section 2.2.2. Then the Hamiltonian vector ﬁeld of KLp on P(M) is: XKLp(q) = πq(Xlog(q/p)) = πq(βij∂jlog(q/p)∂i). Proof is provided in Appendix A.3. Note that the projection πq does not make much difference recalling X and πq(X) produce the same distribution curve through q. For a wider coverage of our framework on MCMC dynam- ics, we introduce a novel concept called ﬁber-Riemannian manifold and develop associated objects. This notion gener- alizes Riemannian manifold, such that the non-degenerate requirement of the Riemannian structure is relaxed. Deﬁnition 3 (Fiber-Riemannian manifold). We say that a manifold Mis a ﬁber-Riemannian manifold if it is a ﬁber bundle and there is a Riemannian structure on each ﬁber. Figure 1.Illustration of a ﬁber- Riemannian manifold (M, ˜g) (m = n = 1 ) and a ﬁber- gradient shown in green arrows. See Fig. 1 for illustra- tion. Roughly, M(of di- mension M = m + n) is a ﬁber bundle if there are two smooth mani- folds M0 (of dimension m) and F (of dimen- sion n) and a surjective projection ϖ : M → M0 such that ϖ is lo- cally equivalent to the projection on the product space M0 ×F →M0 (e.g., Nicolaescu (2007), 1 We do not consider the compatibility of the Riemannian and Poisson structure so it is different from a K¨ahler manifold.Understanding MCMC Dynamics as Flows on the Wasserstein Space Def. 2.1.21). The space M0 is called the base space, and F the common ﬁber. The ﬁber through x∈M is deﬁned as the submanifold Mϖ(x) := ϖ−1(ϖ(x)), which is diffeo- morphic to F. Fiber bundle generalizes the concept of the product space to allow different structures among different ﬁbers. The coordinate of Mcan be decomposed under this structure: x = (y,z) where y ∈Rm is the coordinate of M0 and z ∈Rn of Mϖ(x). Coordinates of points on a ﬁber share the same ypart. We allow mor nto be zero. According to our deﬁnition, a ﬁber-Riemannian manifold furnish each ﬁber My with a Riemannian structure gMy, whose coordinate expression is ( (gMy)ab ) (indices a,b for zrun from 1 to n). By restricting a function f ∈C∞(M) on a ﬁber My, the structure deﬁnes a gradient on the ﬁber: gradMy f(y,z) = (gMy)ab(z) ∂zbf(y,z) ∂za. Taking the union over all ﬁbers, we have a vector ﬁeld on the en- tire manifold M, which we call the ﬁber-gradient of f:( (gradﬁb f)i(x) ) := ( 0m,(gMϖ(x))ab(z) ∂zbf(ϖ(x),z) ) . To express it in a similar way as the gradient, we further deﬁne the ﬁber-Riemannian structure ˜gas: ( ˜gij(x) ) M×M := (0m×m 0m×n 0n×m ( (gMϖ(x))ab(z) ) n×n ) , (9) and the ﬁber-gradient can be expressed as gradﬁb f = ˜gij∂jf∂i. Note that gradﬁb f(x) is tangent to the ﬁber Mϖ(x) and its ﬂow moves points within each ﬁber. It is not a Riemannian manifold for m≥1 since (˜gij) is singular. Now we turn to the Wasserstein space. As the ﬁber struc- ture of P(M) is hard to ﬁnd, we consider the space ˜P(M) := {q(·|y) ∈P(My) |y∈M0 }. With projection q(·|y) ↦→y, it is locally equivalent to M0 ×P(My). Each of its ﬁber P(My) has a Riemannian structure induced by that of My (Section 2.2.1), so it is a ﬁber-Riemannian manifold. On ﬁber P(My), according to Eq. (2), we have grad KLp(·|y) ( q(·|y) ) (z) = (gMy)ab(z)∂zblog q(z|y) p(z|y) ∂za = (gMy)ab(z)∂zblog q(y,z) p(y,z) ∂za as a vector ﬁeld on My. Tak- ing the union over all ﬁbers, we have the ﬁber-gradient of KLp on ˜P(M) as a vector ﬁeld on M: gradﬁb KLp(q)(x) = ˜gij(x) ∂jlog ( q(x)/p(x) ) ∂i. (10) After projected by πq, gradﬁb KLp(q) is a tangent vector on the Wasserstein space P(M). Note that P(M) is locally equivalent to P(M0) ×˜P(M) thus not a ﬁber-Riemannian manifold in this way, so it is hard to develop the ﬁber- gradient directly on P(M). 3.2 The Uniﬁed Framework We introduce a regularity assumption on MCMC dynam- ics that our uniﬁed framework considers. It is satisﬁed by almost all existing MCMCs and its relaxation will be dis- cussed at the end of this section. Assumption 4 (Regular MCMC dynamics) . We call an MCMC dynamics regular if its corresponding matrices Figure 2.Illustration of our uniﬁed framework (Theorem 5): a reg- ular MCMC dynamics is equivalent to the fGH ﬂow WKLp on the Wasserstein space P(M) of an fRP manifold M. The projected ﬁber-gradient (green solid arrows) and Hamiltonian vector ﬁeld (red dashed arrows) at qt on M are plotted. (D,Q) in formulation (1) additionally satisﬁes: (a) the dif- fusion matrix D = C or D = 0 or D = (0 0 0 C ) , where C(x) is symmetric positive deﬁnite everywhere; (b) the curl matrix Q(x) satisﬁes Eq. (3) everywhere. Now we formally state our uniﬁed framework, with an illus- tration provided in Fig. 2. Theorem 5 (Uniﬁed framework: equivalence between reg- ular MCMC dynamics and fGH ﬂows on P(M)). We call (M,˜g,β) a ﬁber-Riemannian Poisson (fRP) manifold, and deﬁne the ﬁber-gradient Hamiltonian (fGH) ﬂow on P(M) as the ﬂow induced by the vector ﬁeld: WKLp := −π(gradﬁb KLp) −XKLp, WKLp(q) =πq ( (˜gij + βij)∂jlog(p/q)∂i ) . (11) Then: (a) Any regular MCMC dynamics on RM targeting p is equivalent to the fGH ﬂow WKLp on P(M) for a certain fRP manifold M; (b) Conversely, for any fRP manifold M, the fGH ﬂow WKLp on P(M) is equivalent to a regular MCMC dynamics targeting pin the coordinate space of M; (c) More precisely, in both cases, the coordinate expressions of the ﬁber-Riemannian structure ˜gand Poisson structure β of Mcoincide respectively with the diffusion matrix Dand the curl matrix Qof the regular MCMC dynamics. The idea of proof is to show πq(W) = WKLp(q) (W de- ﬁned in Lemma 1) at any q∈P(M) so that the two vector ﬁelds produce the same evolution rule of distribution. Proof details are presented in Appendix A.4. This formulation uniﬁes regular MCMC dynamics and ﬂows on the Wasserstein space, and provides a direct explana- tion on the behavior of general MCMC dynamics. The fundamental requirement on MCMCs that the target distri- bution pis kept stationary, turns obvious in our framework:Understanding MCMC Dynamics as Flows on the Wasserstein Space WKLp(p) = 0 . The Hamiltonian ﬂow −XKLp conserves KLp (difference to p) while encourages efﬁcient exploration in the sample space that helps faster convergence and lower autocorrelation (Betancourt et al., 2017). The ﬁber-gradient ﬂow −gradﬁb KLp minimizes KLp(·|y) on each ﬁber My, driving qt(·|y) to p(·|y) and enforcing convergence. Speci- ﬁcation of this general behavior is discussed below. 3.3 Existing MCMCs under the Uniﬁed Framework Now we make detailed analysis on existing MCMC methods under our uniﬁed framework. Depending on the diffusion matrix D, they can be categorized into three types. Each type has a particular ﬁber structure of the corresponding fRP manifold, thus a particular behavior of the dynamics. Type 1: Dis non-singular (m= 0 in Eq. (9)). In this case, the corresponding M0 degenerates and M itself is the unique ﬁber, so Mis a Riemannian manifold with structure (gij) = D−1. The ﬁber-gradient ﬂow on ˜P(M) becomes the gradient ﬂow on P(M) so: WKLp = −π(grad KLp) −XKLp, which indicates the convergence of the dynamics: the Hamil- tonian ﬂow −XKLp conserves KLp while the gradient ﬂow −grad KLp minimizes KLp on P(M) steepest, so they jointly minimize KLp monotonically, leading to the unique minimizer p. This meets the conclusion in Ma et al. (2015). The Langevin dynamics (LD) (Roberts et al., 1996), used in both full-batch (Roberts & Stramer, 2002) and stochastic gradient (SG) simulation (Welling & Teh, 2011), falls into this class. Its curl matrix Q= 0 makes its fGH ﬂow com- prise purely the gradient ﬂow, allowing a rich study on its be- havior (Durmus & Moulines, 2016; Cheng & Bartlett, 2017; Wibisono, 2018; Bernton, 2018; Durmus et al., 2018). Its Riemannian version (Girolami & Calderhead, 2011) chooses Das the inverse Fisher metric so that Mis the distribution manifold in information geometry (Amari, 2016). Patterson & Teh (2013) further explore the simulation with SG. Type 2: D= 0 (n= 0 in Eq. (9)). In this case, M0 = Mand ﬁbers degenerate. The fGH ﬂow WKLp comprises purely the Hamiltonian ﬂow −XKLp, which conserves KLpand helps distant exploration. We note that under this case, the decrease of KLp is not guaranteed, so care must be taken in simulation. Particularly, this type of dynamics cannot be simulated with parallel chains unless samples initially distribute as p, so they are not suitable for ParVI simulation. The lack of a stabilizing force in the dynamics also explains their vulnerability in face of SG, where the noisy perturbation is uncontrolled. This generalizes the discussion on HMC by Chen et al. (2014) and Betancourt (2015) to dynamics of this type. The Hamiltonian dynamics (e.g., Marsden & Ratiu (2013), Chap. 2) that HMC simulates is a representative of this kind. To sample from a distribution p(θ) on manifold Sof dimen- sion ℓ, variable θ is augmented x = ( θ,r) with a vector r ∈Rℓ called momentum. In our framework, this is to take Mas the cotangent bundle T∗S, whose canonical Pois- son structure corresponds to Q= (βij) = ( 0 −Iℓ Iℓ 0 ) . A conditional distribution p(r|θ) is chosen for an augmented target distribution p(x) = p(θ)p(r|θ). HMC produces more effective samples than LD with the help of the Hamiltonian ﬂow (Betancourt et al., 2017). As we mentioned, the dynam- ics of HMC cannot guarantee convergence, so it relies on the ergodicity of its simulation for convergence (Livingstone et al., 2016; Betancourt, 2017). It is simulated in a deliber- ated way: the second-order symplectic leap-frog integrator is employed, and ris successively redrew from p(r|θ). HMC considers Euclidean S and chooses Gaussian p(r|θ) = N(0,Σ), while Zhang et al. (2016) take p(r|θ) as the monomial Gamma distribution. On Riemannian (S,g), p(r|θ) is chosen as N ( 0,(gij(θ)) ) , i.e., the standard Gaus- sian in the cotangent space T∗ θS(Girolami & Calderhead, 2011). Byrne & Girolami (2013) simulate the dynamics for manifolds with no global coordinates, and Lan et al. (2015) take the Lagrangian form for better simulation, which uses velocity (tangent vector) in place of momentum (covector). Type 3: D̸= 0 and Dis singular (m,n ≥1 in Eq. (9)). In this case, both M0 and ﬁbers are non-degenerate. The ﬁber-gradient ﬂow stabilizes the dynamics only in each ﬁber My, but this is enough for most SG-MCMCs since SG appears only in the ﬁbers. SGHMC (Chen et al., 2014) is the ﬁrst instance of this type. Similar to the Hamiltonian dynamics, it takes M= T∗S and shares the same Q, but its D2ℓ×2ℓ is in the form of Assumption 4(a) with a constant Cℓ×ℓ, whose inverse C−1 deﬁnes a Riemannian structure in every ﬁber My. Viewed in our framework, this makes the ﬁber bundle structure of Mcoincides with that of T∗S: M0 = S, My = T∗ θS, and x= (y,z) = (θ,r). Using Lemma 1, with a speciﬁed p(r|θ), we derive its equivalent deterministic dynamics:{ dθ dt = −∇rlog p(r|θ), dr dt = ∇θlog p(θ)+∇θlog p(r|θ)+ C∇rlog p(r|θ) q(r|θ) . (12) We note that it adds the dynamics dr dt = C∇rlog p(r|θ) q(r|θ) to the Hamiltonian dynamics. This added dynamics is essen- tially the ﬁber-gradient ﬂow −(gradﬁb KLp)(q) on P(M) (Eq. (10)), or the gradient ﬂow −(grad KLp(·|θ))(q(·|θ)) on ﬁber T∗ θS, which pushes q(·|θ) towards p(·|θ). In pres- ence of SG, the dynamics for θ ∈S is unaffected, but for r ∈T∗ θSin each ﬁber, a ﬂuctuation is introduced due to the noisy estimate of ∇θlog p(θ), which will mislead q(·|θ). The ﬁber-gradient compensates this by guiding q(·|θ) to the correct target, making the dynamics robust to SG. Another famous example of this kind is the SG Nos´e-HooverUnderstanding MCMC Dynamics as Flows on the Wasserstein Space thermostats (SGNHT) (Ding et al., 2014). It further aug- ments (θ,r) with the thermostats ξ ∈R to better balance the SG noise. In terms of our framework, the thermostats ξ augments M0, and the ﬁber is the same as SGHMC. Both SGHMC and SGNHT choose p(r|θ) = N(0,Σ−1), while SG monomial Gamma thermostats (SGMGT) (Zhang et al., 2017) uses monomial Gamma, and Lu et al. (2016) choose p(r|θ) according to a relativistic energy function to adapt the scale in each dimension. Riemannian ex- tensions of SGHMC and SGNHT on (S,g) are explored by Ma et al. (2015) and Liu et al. (2016). Viewed in our framework, they induce a Riemannian structure(√ (gij(θ)) ⊤ C−1√ (gij(θ)) ) ℓ×ℓ in each ﬁber My = T∗ θS. Discussions Due to the linearity of the equivalent sys- tems (1), (7), (11) w.r.t. D, Qor (˜gij), (βij), MCMC dy- namics can be combined. From the analysis above, SGHMC can be seen as the combination of the Hamiltonian dynamics on the cotangent bundleT∗Sand the LD in each ﬁber (cotan- gent space T∗ θS). As another example, Zhang et al. (2017) combine SGMGT of Type 3 with LD of Type 1, creating a Type 1 method that decreases KLp on the entire manifold instead of each ﬁber. This improves the convergence, which meets their empirical observation. Assumption 4(a) is satisﬁed by all the mentioned MCMC dynamics, and Assumption 4(b) is also satisﬁed by all ex- cept SGNHT related dynamics. On this exception, we note from the derivation of Theorem 5 that, Assumption 4(b) is only required for Mthus P(M) to be a Poisson manifold, but is not used in the deduction afterwards. Deﬁnition of a Hamiltonian vector ﬁeld and its key property could also be established without the assumption, so it is possible to extend the framework under a more general mathematical concept that relaxes Assumption 4(b). Assumption 4(a) could also be hopefully relaxed by an invertible transfor- mation from any positive semi-deﬁnite Dinto the required form, effectively converting the dynamics into an equivalent regular one. We leave further investigations as future work. 4 Simulation as ParVIs The uniﬁed framework (Theorem 5) recognizes an MCMC dynamics as an fGH ﬂow on the Wasserstein space P(M) of an fRP manifold M, expressed in Eq. (11) explicitly. Lemma 1 gives another equivalent dynamics that leads to the same ﬂow on P(M). These ﬁndings enable us to simulate these ﬂow-based dynamics for an MCMC method, using existing ﬁnite-particle ﬂow simulation methods in the ParVI ﬁeld. This hybrid of ParVI and MCMC largely extends the ParVI family with various dynamics, and also gives advantages like particle-efﬁciency to MCMCs. We select the SGHMC dynamics as an example and develop its particle-based simulations. With p(r|θ) = N(0,Σ) for a constant Σ, rand θbecome independent, and Eq. (12) from Lemma 1 becomes:{ dθ dt = Σ−1r, dr dt = ∇θlog p(θ) −CΣ−1r−C∇rlog q(r). (13) From the other equivalent dynamics given by the framework (Theorem 5), the fGH ﬂow (Eq. (11)) for SGHMC is:{ dθ dt = Σ−1r+ ∇rlog q(r), dr dt = ∇θlog p(θ)−CΣ−1r−C∇rlog q(r)−∇θlog q(θ).(14) The key problem in simulating these ﬂow-based dynamics with ﬁnite particles is that the densityqis unknown. Liu et al. (2019) give a summary on the solutions in the ParVI ﬁeld, and ﬁnd that they are all based on a smoothing treatment, in a certain formulation of either smoothing the density or smoothing functions. Here we adopt the Blob method (Chen et al., 2018a) that smooths the density. With a set of particles {r(i)}i of q(r), Blob makes the following approximation with a kernel function Kr for r: −∇rlog q(r(i))≈− ∑ k∇r(i)K(i,k) r ∑ jK(i,j) r − ∑ k ∇r(i)K(i,k) r ∑ jK(j,k) r , (15) where K(i,j) r := Kr(r(i),r(j)). Approximation for −∇θlog q(θ) can be established in a similar way. The vanilla SGHMC simulates dynamics (13) with −C∇rlog q(r) dt replaced by N(0,2Cdt), but dynam- ics (14) cannot be simulated in a similar stochastic way. More discussions are provided in Appendix B. We call the ParVI simulations of the two dynamics as pSGHMC-det (Eq. (13)) and pSGHMC-fGH (Eq. (14)), re- spectively (“p” for “particle”). Compared to the vanilla SGHMC, the proposed methods could converge faster and be more particle-efﬁcient with deterministic update and ex- plicit repulsive interaction (Eq. (15)). On the other hand, SGHMC could make a more efﬁcient exploration and con- verges faster than LD, so our methods could speed up over Blob. One may note that pSGHMC-det resembles a direct application of stochastic gradient descent with momentum (Sutskever et al., 2013) to Blob. We stress that this appli- cation is inappropriate since Blob minimizes KLp on the inﬁnite-dimensional manifold P(M) instead of a function on M. Moreover, the two methods can be nourished with advanced techniques in the ParVI ﬁeld. This includes the HE bandwidth selection method and acceleration frame- works by Liu et al. (2019), and other approximations to −∇log qlike SVGD and GFSD/GFSF (Liu et al., 2019). 5 Experiments Detailed experimental settings are provided in Appendix C, and codes are available at https://github.com/ chang-ml-thu/FGH-flow . 5.1 Synthetic Experiment We show in Fig. 3 the equivalence of various dynamics simu- lations, and the advantages of pSGHMC-det and pSGHMC-Understanding MCMC Dynamics as Flows on the Wasserstein Space Figure 3.Dynamics simulation results. Rows correspond to Blob, SGHMC, pSGHMC-det, pSGHMC-fGH, respectively. All meth- ods adopt the same step size 0.01, and SGHMC-related methods share the same Σ−1 = 1.0, C = 0.5. In each row, ﬁgures are plot- ted for every 300 iterations, and the last one for 10,000 iterations. The HE method (Liu et al., 2019) is used for bandwidth selection. fGH. We ﬁrst ﬁnd that all methods eventually produce prop- erly distributed particles, demonstrating their equivalence. For ParVI methods, both proposed methods (Rows 3, 4) converge faster than Blob (Row 1), indicating the beneﬁt of using SGHMC dynamics over LD, where the momentum accumulates in the vertical direction. For the same SGHMC dynamics, we see that our ParVI versions (Rows 3, 4) con- verge faster than the vanilla stochastic version (Row 2), due to the deterministic update rule. Moreover, pSGHMC-fGH (Row 4) enjoys the HE bandwidth selection method (Liu et al., 2019) for ParVIs, which makes the particles neatly and regularly aligned thus more representative for the distri- bution. pSGHMC-det (Row 3) does not beneﬁt much from HE since the density on particles, q(θ), is not directly used in the dynamics (13). 5.2 Latent Dirichlet Allocation (LDA) We study the advantages of our pSGHMC methods in the real-world task of posterior inference for LDA. We follow the same settings as Liu et al. (2019) and Chen et al. (2014). 0 200 400 600 iteration 1040 1060 1080 1100 1120holdout perplexity Blob SGHMC pSGHMC-det pSGHMC-fGH (a) Learning curve (20 ptcls) 0 50 100 #particle 1030 1035 1040 1045 1050holdout perplexity SGHMC pSGHMC-det pSGHMC-fGH (b) Particle-efﬁciency (iter 600) Figure 4.Performance on LDA with the ICML data set. Results are averaged over 10 runs. All methods share the same step size 0.001 and parameters Σ−1 = 300 and C = 0.1. 0 40 80 epoch 0.1 0.2 0.3 0.4 0.5 0.6error rate Blob SGHMC pSGHMC-det pSGHMC-fGH (a) Learning curve (10 ptcls) 0 20 40 #particle 0.030 0.035 0.040 0.045 0.050 0.055 0.060error rate SGHMC pSGHMC-det pSGHMC-fGH (b) Particle-efﬁciency (epch 80) Figure 5.Performance on BNN with MNIST data set. Results av- eraged over 10 runs. SGHMC-related methods share parameters. We see from Fig. 4(a) the saliently faster convergence over Blob, beneﬁted from the usage of SGHMC dynamics in the ParVI ﬁeld. Particle-efﬁciency is compared in Fig. 4(b), where we ﬁnd the better results of pSGHMC methods over vanilla SGHMC under a same particle size. This demon- strates the advantage of ParVI simulation of MCMC dynam- ics, where particle interaction is directly considered to make full use of a set of particles. 5.3 Bayesian Neural Networks (BNNs) We investigate our methods in the supervised task of training BNNs. We follow the settings of Chen et al. (2014) with slight modiﬁcation explained in Appendix. Results in Fig. 5 is consistent with our claim: pSGHMC methods converge faster than Blob due to the usage of SGHMC dynamics. Their slightly better particle-efﬁciency can also be observed. 6 Conclusions We construct a theoretical framework that connects general MCMC dynamics with ﬂows on the Wasserstein space. By introducing novel concepts, we ﬁnd that a regular MCMC dynamics corresponds to an fGH ﬂow for an fRP manifold. The framework gives a clear picture on the behavior of vari- ous MCMC dynamics, and also enables ParVI simulation of MCMC dynamics. We group existing MCMC dynamics into 3 types under the framework and analyse their behavior, and develop two ParVI methods for the SGHMC dynamics. We empirically demonstrate the faster convergence by more general MCMC dynamics for ParVIs, and particle-efﬁciency by ParVI simulation for MCMCs.Understanding MCMC Dynamics as Flows on the Wasserstein Space Acknowledgments This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008, 61571261), Beijing NSF Project (No. L172037), DITD Program JCKY2017204B064, Tiangong Institute for Intelli- gent Computing, Beijing Academy of Artiﬁcial Intelligence (BAAI), NVIDIA NV AIL Program, and the projects from Siemens and Intel. References Abraham, R., Marsden, J. E., and Ratiu, T. Manifolds, tensor analysis, and applications, volume 75. Springer Science & Business Media, New York, 2012. Amari, S.-I. Information geometry and its applications . Springer, 2016. Ambrosio, L. and Gangbo, W. Hamiltonian ODEs in the Wasserstein space of probability measures. Communi- cations on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 61(1):18–53, 2008. Ambrosio, L., Gigli, N., and Savar´e, G. Gradient ﬂows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2008. Barbour, A. D. Stein’s method for diffusion approxima- tions. Probability theory and related ﬁelds, 84(3):297– 322, 1990. Benamou, J.-D. and Brenier, Y . A computational ﬂuid me- chanics solution to the Monge-Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375–393, 2000. Bernton, E. Langevin Monte Carlo and JKO splitting. arXiv preprint arXiv:1802.08671, 2018. Betancourt, M. The fundamental incompatibility of scalable Hamiltonian Monte Carlo and naive data subsampling. In Proceedings of the 32nd International Conference on Ma- chine Learning (ICML 2015), pp. 533–540, Lille, France, 2015. IMLS. Betancourt, M. A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint arXiv:1701.02434, 2017. Betancourt, M., Byrne, S., Livingstone, S., Girolami, M., et al. The geometric foundations of Hamiltonian Monte Carlo. Bernoulli, 23(4A):2257–2298, 2017. Bruna, M., Burger, M., Ranetbauer, H., and Wolfram, M.-T. Asymptotic gradient ﬂow structures of a nonlinear Fokker- Planck equation. arXiv preprint arXiv:1708.07304, 2017. Byrne, S. and Girolami, M. Geodesic Monte Carlo on embedded manifolds. Scandinavian Journal of Statistics, 40(4):825–845, 2013. Chen, C., Ding, N., and Carin, L. On the convergence of stochastic gradient MCMC algorithms with high-order in- tegrators. In Advances in Neural Information Processing Systems, pp. 2269–2277, Montral, Canada, 2015. NIPS Foundation. Chen, C., Ding, N., Li, C., Zhang, Y ., and Carin, L. Stochas- tic gradient MCMC with stale gradients. In Advances in Neural Information Processing Systems, pp. 2937–2945, Barcelona, Spain, 2016. NIPS Foundation. Chen, C., Zhang, R., Wang, W., Li, B., and Chen, L. A uniﬁed particle-optimization framework for scalable Bayesian sampling. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2018), Mon- terey, California USA, 2018a. Association for Uncertainty in Artiﬁcial Intelligence. Chen, T., Fox, E., and Guestrin, C. Stochastic gradient Hamiltonian Monte Carlo. In Proceedings of the 31st International Conference on Machine Learning (ICML 2014), pp. 1683–1691, Beijing, China, 2014. IMLS. Chen, W. Y ., Mackey, L., Gorham, J., Briol, F.-X., and Oates, C. J. Stein points. arXiv preprint arXiv:1803.10161 , 2018b. Cheng, X. and Bartlett, P. Convergence of Langevin MCMC in KL-divergence. arXiv preprint arXiv:1705.09048 , 2017. Da Silva, A. C. Lectures on symplectic geometry, volume 3575. Springer, 2001. Ding, N., Fang, Y ., Babbush, R., Chen, C., Skeel, R. D., and Neven, H. Bayesian sampling using stochastic gra- dient thermostats. In Advances in Neural Information Processing Systems, pp. 3203–3211, Montral, Canada, 2014. NIPS Foundation. Do Carmo, M. P. Riemannian Geometry. Birkh ¨auser, 1992. Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. Hybrid Monte Carlo. Physics Letters B, 195(2):216–222, 1987. Durmus, A. and Moulines, E. High-dimensional Bayesian inference via the unadjusted Langevin algorithm. arXiv preprint arXiv:1605.01559, 2016. Durmus, A., Majewski, S., and Miasojedow, B. Analysis of Langevin Monte Carlo via convex optimization. arXiv preprint arXiv:1802.09188, 2018. Fernandes, R. L. and Marcut, I. Lectures on Poisson Geom- etry. Springer, 2014.Understanding MCMC Dynamics as Flows on the Wasserstein Space Gallego, V . and Insua, D. R. Stochastic gradient MCMC with repulsive forces. arXiv preprint arXiv:1812.00071, 2018. Gangbo, W., Kim, H. K., and Pacini, T. Differential forms on Wasserstein space and inﬁnite-dimensional Hamilto- nian systems. American Mathematical Soc., Providence, Rhode Island, 2010. Girolami, M. and Calderhead, B. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Jour- nal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123–214, 2011. Gorham, J. and Mackey, L. Measuring sample quality with Stein’s method. In Advances in Neural Information Pro- cessing Systems, pp. 226–234, Montral, Canada, 2015. NIPS Foundation. Gorham, J., Duncan, A. B., V ollmer, S. J., and Mackey, L. Measuring sample quality with diffusions. arXiv preprint arXiv:1611.06972, 2016. Jordan, R., Kinderlehrer, D., and Otto, F. The variational formulation of the Fokker-Planck equation. SIAM journal on mathematical analysis, 29(1):1–17, 1998. Kondratyev, S. and V orotnikov, D. Nonlinear Fokker-Planck equations with reaction as gradient ﬂows of the free en- ergy. arXiv preprint arXiv:1706.08957, 2017. Lan, S., Stathopoulos, V ., Shahbaba, B., and Girolami, M. Markov chain Monte Carlo from lagrangian dynamics. Journal of Computational and Graphical Statistics , 24 (2):357–378, 2015. Langevin, P. Sur la th´eorie du mouvement Brownien.Compt. Rendus, 146:530–533, 1908. Li, C., Chen, C., Pu, Y ., Henao, R., and Carin, L. Communication-efﬁcient stochastic gradient MCMC for neural networks. In The 33rd AAAI Conference on Ar- tiﬁcial Intelligence (AAAI-19), Honolulu, Hawaii USA, 2019. AAAI press. Liu, C. and Zhu, J. Riemannian Stein variational gra- dient descent for Bayesian inference. In The 32nd AAAI Conference on Artiﬁcial Intelligence , pp. 3627– 3634, New Orleans, Louisiana USA, 2018. AAAI press. URL https://aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/17275. Liu, C., Zhu, J., and Song, Y . Stochastic gradient geodesic MCMC methods. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29 , pp. 3009–3017. Curran Associates, Inc., Barcelona, Spain, 2016. URL http://papers.nips.cc/paper/ 6282-stochastic-gradient-geodesic-mcmc-methods. pdf. Liu, C., Zhuo, J., Cheng, P., Zhang, R., Zhu, J., and Carin, L. Understanding and accelerating particle-based vari- ational inference. In Chaudhuri, K. and Salakhutdi- nov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Pro- ceedings of Machine Learning Research , pp. 4082– 4092, Long Beach, California USA, 09–15 Jun 2019. IMLS, PMLR. URL http://proceedings.mlr. press/v97/liu19i.html. Liu, Q. Stein variational gradient descent as gradient ﬂow. In Advances in Neural Information Processing Systems, pp. 3118–3126, Long Beach, California USA, 2017. NIPS Foundation. Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In Ad- vances in Neural Information Processing Systems , pp. 2370–2378, Barcelona, Spain, 2016. NIPS Foundation. Liu, Y ., Ramachandran, P., Liu, Q., and Peng, J. Stein variational policy gradient. In Proceedings of the Confer- ence on Uncertainty in Artiﬁcial Intelligence (UAI 2017), Sydney, Australia, 2017. Association for Uncertainty in Artiﬁcial Intelligence. Livingstone, S., Betancourt, M., Byrne, S., and Girolami, M. On the geometric ergodicity of Hamiltonian Monte Carlo. arXiv preprint arXiv:1601.08057, 2016. Lott, J. Some geometric calculations on Wasserstein space. Communications in Mathematical Physics, 277(2):423– 437, 2008. Lu, X., Perrone, V ., Hasenclever, L., Teh, Y . W., and V ollmer, S. J. Relativistic Monte Carlo. arXiv preprint arXiv:1609.04388, 2016. Ma, Y .-A., Chen, T., and Fox, E. A complete recipe for stochastic gradient MCMC. In Advances in Neural In- formation Processing Systems, pp. 2899–2907, Montral, Canada, 2015. NIPS Foundation. Marsden, J. E. and Ratiu, T. S. Introduction to mechanics and symmetry: a basic exposition of classical mechanical systems, volume 17. Springer Science & Business Media, 2013. Neal, R. M. MCMC using Hamiltonian dynamics. Hand- book of Markov Chain Monte Carlo, 2, 2011. Nicolaescu, L. I. Lectures on the Geometry of Manifolds. World Scientiﬁc, Singapore, 2007. Otto, F. The geometry of dissipative evolution equations: the porous medium equation. 2001.Understanding MCMC Dynamics as Flows on the Wasserstein Space Patterson, S. and Teh, Y . W. Stochastic gradient Rieman- nian Langevin dynamics on the probability simplex. In Advances in Neural Information Processing Systems, pp. 3102–3110, Lake Tahoe, Nevada USA, 2013. NIPS Foun- dation. Pu, Y ., Gan, Z., Henao, R., Li, C., Han, S., and Carin, L. V AE learning via Stein variational gradient descent. In Advances in Neural Information Processing Systems, pp. 4239–4248, Long Beach, California USA, 2017. NIPS Foundation. Risken, H. Fokker-Planck equation. In The Fokker-Planck Equation, pp. 63–95. Springer, 1996. Roberts, G. O. and Stramer, O. Langevin diffusions and Metropolis-Hastings algorithms. Methodology and com- puting in applied probability, 4(4):337–357, 2002. Roberts, G. O., Tweedie, R. L., et al. Exponential conver- gence of Langevin distributions and their discrete approx- imations. Bernoulli, 2(4):341–363, 1996. Santambrogio, F. Euclidean, metric, and Wasserstein gra- dient ﬂows: an overview. Bulletin of Mathematical Sci- ences, 7(1):87–154, 2017. Stein, C. A bound for the error in the normal approximation to the distribution of a sum of dependent random vari- ables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, Oakland, 1972. The Regents of the University of California. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Confer- ence on Machine Learning (ICML 2013), pp. 1139–1147, Atlanta, Georgia USA, 2013. IMLS. Taghvaei, A. and Mehta, P. G. Accelerated gradient ﬂow for probability distributions. In Proceedings of the 36th International Conference on Machine Learning (ICML 2019), Long Beach, California USA, 2019. IMLS. Villani, C. Optimal transport: old and new , volume 338. Springer Science & Business Media, 2008. Welling, M. and Teh, Y . W. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML 2011), pp. 681–688, Bellevue, Washington USA, 2011. IMLS. Wibisono, A. Sampling as optimization in the space of measures: The Langevin dynamics as a composite op- timization problem. arXiv preprint arXiv:1802.08089, 2018. Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y ., and Ahn, S. Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems, pp. 7343–7353, Montral, Canada, 2018. NIPS Foundation. Zhang, Y ., Wang, X., Chen, C., Henao, R., Fan, K., and Carin, L. Towards unifying Hamiltonian Monte Carlo and slice sampling. In Advances in Neural Information Processing Systems, pp. 1741–1749, Barcelona, Spain, 2016. NIPS Foundation. Zhang, Y ., Chen, C., Gan, Z., Henao, R., and Carin, L. Stochastic gradient monomial Gamma sampler. arXiv preprint arXiv:1706.01498, 2017. Zhuo, J., Liu, C., Shi, J., Zhu, J., Chen, N., and Zhang, B. Message passing Stein variational gradient descent. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th In- ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 6018– 6027, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018. IMLS, PMLR. URL http://proceedings. mlr.press/v80/zhuo18a.html.Understanding MCMC Dynamics as Flows on the Wasserstein Space Appendix A. Proofs A.1. P ROOF OF LEMMA 1 Given the dynamics (1), the distribution curve (qt)t is gov- erned by the Fokker-Planck equation (e.g., Risken (1996)): ∂tqt = −∂i(qtVi) + ∂i∂j(qtDij), which reduces to: ∂tqt = −(∂iqt)Vi −qt(∂iVi) + qt(∂i∂jDij) + (∂i∂jqt)Dij + (∂iqt)(∂jDij) + (∂jqt)(∂iDij) = −(∂iqt)(∂jDij + ∂jQij) −(∂iqt)(Dij + Qij)∂jp p −qt∂i∂j(Dij + Qij) −qt(∂iDij + ∂iQij)∂jp p −qt(Dij + Qij)(∂i∂jp p −(∂ip)(∂jp) p2 ) + qt(∂i∂jDij) + (∂i∂jqt)Dij + (∂iqt)(∂jDij) + (∂jqt)(∂iDij) = ( ∂iqt −qt p∂ip)(∂jDij −∂jQij) −1 p(∂iqt)(∂jp)(Dij + Qij) −qt p(∂i∂jp)Dij + qt p2 (∂ip)(∂jp)Dij + (∂i∂jqt)Dij, where we have used the symmetry of D and skew-symmetry of Q in the last equality: (∂jp)(∂iDij) = ( ∂ip)(∂jDji) = ( ∂ip)(∂jDij) and similarly (∂jp)(∂iQij) = −(∂ip)(∂jQij); ∂i∂jQij = ∂j∂iQji = −∂i∂jQij so ∂i∂jQij = 0 and similarly (∂ip)(∂jp)Qij = 0, (∂i∂jp)Qij = 0. The deterministic dynamics in the theorem dx= Wt(x) dt with Wt(x) deﬁned in Eq. (7) induces the curve: ∂tqt = −∂i(qt(Wt)i) = −(∂iqt)(Wt)i −qt(∂i(Wt)i) = −(∂iqt)Dij(∂jp p −∂jqt qt ) −(∂iqt)Qij(∂jp p ) −(∂iqt)(∂jQij) −qt(∂iDij)(∂jp p −∂jqt qt ) −qtDij(∂i∂jp p −(∂jp)(∂ip) p2 −∂i∂jqt qt + (∂jqt)(∂iqt) q2 t ) −qt(∂iQij)∂jp p −qtQij(∂i∂jp p −(∂jp)(∂ip) p2 ) −qt(∂i∂jQij) = ( ∂iqt −qt p∂ip)(∂jDij −∂jQij) −1 p(∂iqt)(∂jp)(Dij + Qij) −qt p(∂i∂jp)Dij + qt p2 (∂ip)(∂jp)Dij + (∂i∂jqt)Dij, where we have also applied aforementioned properties in the last equality. Now we see that the two dynamics induce the same distribution curve thus they are equivalent. A.2. D ERIVATION OF EQ. (8) Barbour’s generator is understood as the directional deriva- tive (Af)(x) = d dtFf(qt) ⏐⏐⏐q0=δx t=0 on P(RM). Due to the deﬁnition of gradient, this can be written as (Af)(x) = ⟨grad Ff,πq0 (W0)⟩Tq0 P = ⟨grad Ff,W0⟩L2q0 , where πq0 (W0) is the tangent vector of the distribution curve (qt)t at time 0 due to Lemma 1, and the last equality holds due to that πq is the orthogonal projection from L2 q to TqPand grad Ff ∈Tq0 P(see Section 2.2.1). Before going on, we ﬁrst introduce the notion of weak derivative (e.g., Nicolaescu (2007), Def. 10.2.1) of a distri- bution. For a distribution with a smooth density function q and a smooth function f ∈C∞ c (RM), the rule of integration by parts tells us:∫ RM f(x)(∂iq(x)) dx= ∫ RM ∂i(f(x)q(x)) dx − ∫ RM (∂if(x))q(x) dx. Due to Gauss’s theorem ( e.g., Abraham et al. (2012), Thm. 8.2.9), ∫ RM ∂i(f(x)q(x)) dx = limR→+∞ ∫ SM−1(R)(f(y)q(y))vi(y) dy, where SM−1(R) is the (M −1)-dimensional sphere in RM with radius R, y ∈SM−1, and vi is the i-th component of the unit normal vector v(pointing outwards) on SM−1(R). Since f is compactly supported and lim∥x∥→+∞q(x) = 0, after a sufﬁciently large R, f(y)q(y) = 0, so the integral vanishes, and we have:∫ RM f(x)(∂iq(x)) dx= − ∫ RM (∂if(x))q(x) dx, ∀f ∈C∞ c (RM). We can use this property as the deﬁnition of ∂iq for non- absolutely-continuous distributions, like the Dirac measure δx0 :∫ RM f(x)(∂iδx0 (x)) dx:= ∫ RM (∂if(x))δx0 (x) dx =∂if(x0). Now we begin the derivation. Using the form in Eq. (7) andUnderstanding MCMC Dynamics as Flows on the Wasserstein Space noting q0 = δx0 , we have: (Af)(x0) = ⟨grad Ff,W0⟩L2q0 =Eq0(x)[⟨grad f(x),W0(x)⟩RM] = Eq0 [(∂if)Wi 0] =Eq0 [ Dij(∂if) ( ∂jlog(p/q0) ) + Qij(∂if)(∂jlog p) + (∂jQij)(∂if) ] = [ Dij(∂if)(∂jlog p) ] (x0) − ∫ RM ( Dij(∂if) ) (x)(∂jq0)(x) dx + [ Qij(∂if)(∂jlog p) + (∂jQij)(∂if) ] (x0) = [ Dij(∂if)(∂jlog p) + 1 p∂j(pQij)(∂if) ] (x0) + ∫ RM ∂j ( Dij(∂if) ) (x)q0(x) dx = [ Dij(∂if)(∂jlog p) + 1 p∂j(pQij)(∂if) ] (x0) + [ ∂j ( Dij(∂if) )] (x0) = [ Dij(∂if)(∂jlog p) + 1 p∂j(pQij)(∂if) + (∂jDij)(∂if) + Dij(∂i∂jf) ] (x0) = [1 p∂j ( p(Dij + Qij) ) (∂if) + Dij(∂i∂jf) ] (x0) = [1 p∂j ( p(Dij + Qij) ) (∂if) + (Dij + Qij)(∂i∂jf) ] (x0) = [1 p∂j [ p ( Dij + Qij) (∂if) ]] (x0), where the second last equality holds due to Qij(∂i∂jf) = 0 from the skew-symmetry of Q. This completes the deriva- tion. A.3. P ROOF OF LEMMA 2 Noting that the KL divergence KLp(q) = ∫ Mlog(q/p) dq is a non-linear function on P(M), we need to ﬁrst ﬁnd its linearization. We ﬁx a point q0 ∈P(M). Eq. (2) gives its gradient at q0: grad KLp(q0) = grad log(q0/p). Consider the linear function on P(M): F : q↦→ ∫ M log(q0/p) dq. According to existing knowledge ( e.g., Villani (2008), Ex. 15.10; Ambrosio et al. (2008), Lem. 10.4.1; Santambro- gio (2017), Eq. 4.10), its gradient at q0 is given by: ( grad F ) (q0) = grad ( δF δq ⏐⏐⏐⏐ q=q0 ) , where δF δq is the ﬁrst functional variation of F, which is log(q0/p) at q = q0. Now we ﬁnd that grad F(q0) = grad log(q0/p) = grad KLp(q0), so F(q) is the lineariza- tion of KLp(q) at q = q0 and the corresponding f ∈ C∞ c (M) in Eq. (6) is log(q0/p). Then we have: XKLp(q0) = πq0 (Xlog(q0/p)). Referring to Eq. (4), Xlog(q0/p) = βij∂jlog(q0/p)∂i. Due to the generality of q0, this completes the proof. A.4. P ROOF OF THEOREM 5 For a ﬁxed q ∈P(M), two vector ﬁelds on Mproduce the same distribution curve if they have the same projection on TqP(M), so showing πq(W) = WKLp(q) is sufﬁcient for showing the equivalence of the two dynamics. This in turn is equivalent to show πq(W −WKLp(q)) = 0 L2q, or div ( q(W −WKLp(q)) ) = div( q0L2q) = 0 (see Sec- tion 2.2.1). We ﬁrst consider case (b): given an fRP manifold(M,˜g,β), we deﬁne an MCMC dynamics whose diffusion matrix D and curl matrix Qare the coordinate expressions of the ﬁber- Riemannian structure (˜gij) and the Poisson structure (βij), respectively. It is regular, as Assumption 4 is satisﬁed due to properties of (˜gij) (see Eq. (9)) and (βij) (see Section 2.2.2). Its equivalent deterministic dynamics at q(see Lemma 1) is given by: Wi = ˜gij∂jlog(p/q) + βij∂jlog p+ ∂jβij. So we have: div ( q(W −WKLp(q)) ) = div ( q ( ˜gij∂jlog(p/q) + βij∂jlog p+ ∂jβij −(˜gij + βij)∂jlog(p/q) ) ∂i ) = div ( q(∂jβij + βij∂jlog q)∂i ) = div ( (q∂jβij + βij∂jq)∂i ) = div ( ∂j(qβij)∂i ) =∂i∂j(qβij) =0, where the last equality holds due to the skew-symmetry of (βij). This shows that the constructed regular MCMC dynamics is equivalent to the ﬁber-gradient Hamiltonian ﬂow WKLp on M. For case (a), given any regular MCMC dynamics whose matrices (D,Q) satisfy Assumption 4, we can deﬁne an fRP manifold (M,˜g,β) whose structures are deﬁned in the coordinate space by the matrices: ˜gij := Dij, βij := Qij. Assumption 4 guarantees that such ˜gis a valid ﬁber- Riemannian structure and βa valid Poisson structure. On this constructed manifold, we follow the above procedure to construct a regular MCMC dynamics equivalent to the fGH ﬂow WKLp on it, whose equivalent deterministic dynamics is: Wi = Dij∂jlog(p/q) + Qij∂jlog p+ ∂jQij, which is exactly the one of the original MCMC dynamics.Understanding MCMC Dynamics as Flows on the Wasserstein Space This shows that the original regular MCMC dynamics is equivalent to the fGH ﬂow WKLp on the constructed fRP manifold. Finally, statement (c) is veriﬁed in both cases by the intro- duced construction. This completes the proof. B. Details on Flow Simulation of SGHMC Dynamics We ﬁrst introduce more details on the Blob method, refer- ring to the works of Chen et al. (2018a) and Liu et al. (2019). The key problem in simulating a general ﬂow on the Wasser- stein space is to estimate the gradient u(x) := −∇log q(x) where q(x) is the distribution corresponding to the current conﬁguration of the particles. The gradient has to be esti- mated using the ﬁnite particles {x(i)}N i=1 distributed obey- ing q(x). The analysis of Liu et al. (2019) ﬁnds that an estimate method has to make a smoothing treatment, in the form of either smoothing the density or smoothing functions. The Blob method (Chen et al., 2018a) ﬁrst reformulatesu(x) in a variation form: u(x) = ∇ ( −δ δqEq[log q] ) , then with a kernel function K, it replaces the density in the log qterm with a smoothed one: u(x) ≈∇ ( −δ δqEq[log(q∗K)] ) = −∇log(q∗K) −∇ ( q (q∗K) ∗K ) , where “*” denotes convolution. This form enjoys the beneﬁt of enabling the usage of the empirical distribution: take q(x) = ˆq(x) := 1 N ∑N i=1 δx(i) (x), with δx(i) (x) denoting the Dirac measure at x(i). The above formulation then becomes: u(x(i)) = −∇xlog q(x(i)) ≈− ∑ k∇x(i)K(i,k) ∑ jK(i,j) − ∑ k ∇x(i)K(i,k) ∑ jK(j,k) , where K(i,j) := K(x(i),x(j)). This coincides with Eq. (15). The vanilla SGHMC dynamics replaces the dynamics dr = −C∇rlog q(r) dt in Eq. (13) with dr = 2 CdBt or more intuitively dr = N(0,2Cdt), where Bt denotes the standard Brownian motion. The equivalence between these two dynamics can also be directly derived from the Fokker-Planck equation: the ﬁrst one produces a curve by ∂tqt = −∂i ( qt(−Cij∂jlog qt) ) = ∂i(Cij∂jqt), and the second one by ∂tqt = ∂i∂j(qtCij) = ∂i(Cij∂jqt) for a constant C, so the two curves coincides. But dynamics (14) cannot be simulated in a stochastic way, since −∇rlog q(r) and −∇θlog q(θ) are used to update θand r, respectively, that is, the correspondence of gradients and variables is switched. In this case, estimating the gradient cannot be avoided. Finally, we write the explicit update rule of the proposed methods using Blob with particles {(θ,r)(i)}N i=1. Let Kθ, Kr be the kernel functions for θand r, and εbe a step size. The update rule for pSGHMC-det in Eq. (13) becomes:   θ(i) ←θ(i) + εΣ−1r(i), r(i) ←r(i) + ε∇θlog p(θ(i)) −εC ( Σ−1r(i) + ∑ k∇r(i)K(i,k) r ∑ jK(i,j) r + ∑ k ∇r(i)K(i,k) r ∑ jK(j,k) r ) , and for pSGHMC-fGH in Eq. (14):   θ(i) ←θ(i)+ε ( Σ−1r(i)+ ∑ k∇r(i)K(i,k) r ∑ jK(i,j) r +∑ k ∇r(i)K(i,k) r ∑ jK(j,k) r ) , r(i) ←r(i) + ε∇θlog p(θ(i)) −ε (∑ k∇θ(i)K(i,k) θ ∑ jK(i,j) θ +∑ k ∇θ(i)K(i,k) θ ∑ jK(j,k) θ ) −εC ( Σ−1r(i) + ∑ k∇r(i)K(i,k) r ∑ jK(i,j) r + ∑ k ∇r(i)K(i,k) r ∑ jK(j,k) r ) , where K(i,j) θ := Kθ(θ(i),θ(j)) and similarly for K(i,j) r . C. Detailed Settings of Experiments C.1. D ETAILED SETTINGS OF THE SYNTHETIC EXPERIMENT For the random variablex= (x1,x2), the target distribution density p(x) is deﬁned by: log p(x) = −0.01 × (1 2(x2 1 + x2 2) + 0.8 2 (25x1 + x2 2)2 ) + const, which is inspired by the target distribution used in the work of Girolami & Calderhead (2011). We use the exact gradient of the log density instead of stochastic gradient. Fifty parti- cles are used, which are initialized by N ( (−2,−7),0.52I ) . The window range is (−7,3) horizontally and (−9,9) verti- cally. See the caption of Fig. 3 for other settings. C.2. D ETAILED SETTINGS OF THE LDA E XPERIMENT We follow the same settings as Ding et al. (2014), which is also adopted in Liu et al. (2019). The data set is the ICML data set2 developed by Ding et al. (2014). We use 90% words in each document to train the topic proportion of the document and the left 10% words for evaluation. A random 80%-20% train-test split of the data set is conducted in each run. For the LDA model, parameters of the Dirichlet prior of topics is α = 0 .1. The mean and standard deviation of the Gaussian prior on the topic proportions is β = 0.1 and σ = 1.0. Number of topics is 30 and batch size is ﬁxed as 100. The number of Gibbs sampling in each stochastic gradient evaluation is 50. All the inference methods share the same step size ε= 1 × 10−3. SGHMC-related methods (SGHMC, pSGHMC-det 2https://cse.buffalo.edu/˜changyou/code/ SGNHT.zipUnderstanding MCMC Dynamics as Flows on the Wasserstein Space and pSGHMC-fGH) share the same parameters Σ−1 = 300 and C = 0 .1. ParVI methods (Blob, pSGHMC-det and pSGHMC-fGH) use the HE method for kernel bandwidth selection (Liu et al., 2019). To match the fashion of ParVI methods, SGHMC is run with parallel chains and the last samples of each chain are collected. C.3. D ETAILED SETTINGS OF THE BNN E XPERIMENT We use a 784-100-10 feedforward neural network with sig- moid activation function. The batch size is 500. SGHMC, pSGHMC-det and pSGHMC-fGH share the same parame- ters ε = 5 ×10−5, Σ−1 = 1.0 and C = 1.0, while Blob uses ε= 5 ×10−8 (larger εleads to diverged result). For the ParVI methods, we ﬁnd the median method and the HE method for bandwidth selection perform similarly, and we adopt the median method for faster implementation.",
      "meta_data": {
        "arxiv_id": "1902.00282v3",
        "authors": [
          "Chang Liu",
          "Jingwei Zhuo",
          "Jun Zhu"
        ],
        "published_date": "2019-02-01T11:33:13Z",
        "pdf_url": "https://arxiv.org/pdf/1902.00282v3.pdf"
      }
    },
    {
      "title": "Stein variational gradient descent as gradient ﬂow",
      "abstract": "Stein variational gradient descent (SVGD) is a deterministic sampling\nalgorithm that iteratively transports a set of particles to approximate given\ndistributions, based on an efficient gradient-based update that guarantees to\noptimally decrease the KL divergence within a function space. This paper\ndevelops the first theoretical analysis on SVGD, discussing its weak\nconvergence properties and showing that its asymptotic behavior is captured by\na gradient flow of the KL divergence functional under a new metric structure\ninduced by Stein operator. We also provide a number of results on Stein\noperator and Stein's identity using the notion of weak derivative, including a\nnew proof of the distinguishability of Stein discrepancy under weak conditions.",
      "full_text": "Stein Variational Gradient Descent as Gradient Flow Qiang Liu Department of Computer Science Dartmouth College Hanover, NH 03755 qiang.liu@dartmouth.edu Abstract Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the ﬁrst theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics. We develop a geometric perspective that views SVGD as a gradient ﬂow of the KL divergence functional under a new metric structure on the space of distributions induced by Stein operator. 1 Introduction Stein variational gradient descent (SVGD) [1] is a particle-based algorithm for approximating complex distributions. Unlike typical Monte Carlo algorithms that rely on randomness for approximation, SVGD constructs a set of points (or particles) by iteratively applying deterministic updates that is constructed to optimally decrease the KL divergence to the target distribution at each iteration. SVGD has a simple form that efﬁcient leverages the gradient information of the distribution, and can be readily applied to complex models with massive datasets for which typical gradient descent has been found efﬁcient. A nice property of SVGD is that it strictly reduces to the typical gradient ascent for maximum a posteriori (MAP) when using only a single particle (n= 1), while turns into a full sampling method with more particles. Because MAP often provides reasonably good results in practice, SVGD is found more particle-efﬁcient than typical Monte Carlo methods which require much larger numbers of particles to achieve good results. SVGD can be viewed as a variational inference algorithm [e.g., 2], but is signiﬁcantly different from the typical parametric variational inference algorithms that use parametric sets to approximate given distributions and have the disadvantage of introducing deterministic biases and (often) requiring non-convex optimization. The non-parametric nature of SVGD allows it to provide consistent estimation for generic distributions like Monte Carlo does. There are also particle algorithms based on optimization, or variational principles, with theoretical guarantees [e.g., 3–5], but they often do not use the gradient information effectively and do not scale well in high dimensions. However, SVGD is difﬁcult to analyze theoretically because it involves a system of particles that interact with each other in a complex way. In this work, we take an initial step towards analyzing SVGD. We characterize the SVGD dynamics using an evolutionary process of the empirical measures of the particles that is known as Vlasov process in physics, and establish that empirical measures of the particles weakly converge to the given target distribution. We develop a geometric interpretation of SVGD that views SVGD as a gradient ﬂow of KL divergence, deﬁned on a new Riemannian-like metric structure imposed on the space of density functions. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1704.07520v2  [stat.ML]  14 Nov 20172 Stein Variational Gradient Descent (SVGD) We start with a brief overview of SVGD [ 1]. Let νp be a probability measure of interest with a positive, (weakly) differentiable density p(x) on an open set X ⊆Rd. We want to approximate νp with a set of particles {xi}n i=1 whose empirical measure ˆµn(dx) = ∑n i=1 δ(x−xi)/ndxweakly converges to νp as n→∞ (denoted by ˆµn ⇒νp), in the sense that we have Eˆµn[h] →Eνp[h] as n→∞ for all bounded, continuous test functions h. To achieve this, we initialize the particles with some simple distribution µ, and update them via map T(x) = x+ ϵφ(x), where ϵis a small step size, and φ(x) is a perturbation direction, or velocity ﬁeld, which should be chosen to maximally decrease the KL divergence of the particle distribution with the target distribution; this is framed by [1] as solving the following functional optimization, max φ∈H { − d dϵKL(Tµ||νp) ⏐⏐ ϵ=0 s.t. ||φ||H≤1 } . (1) where µ denotes the (empirical) measure of the current particles, and Tµ is the measure of the updated particles x′= T(x) with x∼µ, or the pushforward measure of µthrough map T, and His a normed function space chosen to optimize over. A key observation is that the objective in (1) is a linear functional of φthat draws connections to ideas in the Stein’s method [6] used for proving limit theorems or probabilistic bounds in theoretical statistics. Liu and Wang [1] showed that − d dϵKL(Tµ||νp) ⏐⏐ ϵ=0 = Eµ[Spφ], with Spφ(x) := ∇log p(x)⊤φ(x) + ∇·φ(x), (2) where ∇·φ:= ∑d k=1 ∂xkφk(x), and Sp is a linear operator that maps a vector-valued function φto a scalar-valued function Spφ, and Sp is called the Stein operator in connection with the so-called Stein’s identity, which shows that the RHS of (2) equals zero if µ= νp, Ep[Spφ] = Ep[∇log p⊤φ+ ∇·φ] = ∫ ∇·(pφ)dx= 0; (3) it is the result of integration by parts, assuming proper zero boundary conditions. Therefore, the optimization (1) reduces to D(µ||νp) := max φ∈H { Eµ[Spφ], s.t. ||φ||H≤1 } , (4) where D(µ||νp) is called Stein discrepancy, which provides a discrepancy measure between µand νp, since D(µ||νp) = 0 if µ= νp and D(µ||νp) >0 if µ̸= νp given His sufﬁciently large. Because (4) induces an inﬁnite dimensional functional optimization, it is critical to select a nice space Hthat is both sufﬁciently rich and also ensures computational tractability in practice. Kernelized Stein discrepancy (KSD) provides one way to achieve this by takingHto be a reproducing kernel Hilbert space (RKHS), for which the optimization yields a closed form solution [7–10]. To be speciﬁc, let H0 be a RKHS of scalar-valued functions with a positive deﬁnite kernel k(x,x′), and H= H0 ×···×H 0 the corresponding d×1 vector-valued RKHS. Then it can be shown that the optimal solution of (4) is φ∗ µ,p(·) ∝Ex∼µ[Sp ⊗k(x,·)], with Sp ⊗k(x,·) := ∇log p(x)k(x,·) + ∇xk(x,·), (5) where Sp⊗is an outer product variant of Stein operator which maps a scalar-valued function to a vector-valued one. Further, it has been shown in [e.g., 7] that D(µ||νp) = ||φ∗ µ,p||H= √ Ex,x′∼µ[κp(x,x′)], with κp(x,x′) := Sx pSx′ p ⊗k(x,x′), (6) where κp(x,x′) is a “Steinalized” positive deﬁnite kernel obtained by applying Stein operator twice; Sx p and Sx′ p are the Stein operators w.r.t. variable xand x′, respectively. The key advantage of KSD is its computational tractability: it can be empirically evaluated with samples drawn from µand the gradient ∇log p, which is independent of the normalization constant in p[see 7, 8]. 2Algorithm 1Stein Variational Gradient Descent [1] Input: The score function ∇xlog p(x). Goal: A set of particles {xi}n i=1 that approximates p(x). Initialize a set of particles {xi 0}n i=1; pick a positive deﬁnite kernel k(x,x′) and step-size {ϵℓ}. For iteration ℓdo xi ℓ+1 ← xi ℓ + ϵφ∗ ˆµn ℓ,p(xi ℓ), ∀i= 1,...,n, where φ∗ ˆµn ℓ,p(x) = 1 n n∑ j=1 [ ∇log p(xj ℓ)k(xj ℓ,x) + ∇xj ℓ k(xj ℓ,x) ] , (8) An important theoretic issue related to KSD is to characterize when His rich enough to ensure D(µ||νp) = 0 iff µ = νp; this has been studied by Liu et al. [7], Chwialkowski et al. [8], Oates et al. [11]. More recently, Gorham and Mackey [10] (Theorem 8) established a stronger result that Stein discrepancy implies weak convergence on X = Rd: let {µℓ}∞ ℓ=1 be a sequence of probability measures, then D(µℓ ||νp) →0 ⇐⇒µℓ ⇒νp as ℓ→∞, (7) for νp that are distantly dissipative (Deﬁnition 4 of Gorham and Mackey [10]) and a class of inverse multi-quadric kernels. Since the focus of this work is on SVGD, we will assume (7) holds without further examination. In SVGD algorithm, we iteratively update a set of particles using the optimal transform just derived, starting from certain initialization. Let {xi ℓ}n i=1 be the particles at the ℓ-th iteration. In this case, the exact distributions of {xi ℓ}n i=1 are unknown or difﬁcult to keep track of, but can be best approximated by their empirical measure ˆµn ℓ(dx) = ∑ iδ(x−xi ℓ)dx/n. Therefore, it is natural to think that φ∗ ˆµn ℓ,p, with µin (5) replaced by ˆµn ℓ, provides the best update direction for moving the particles (and equivalently ˆµn ℓ) “closer to” νp. Implementing this update (8) iteratively, we get the main SVGD algorithm in Algorithm 1. Intuitively, the update in (8) pushes the particles towards the high probability regions of the target probability via the gradient term ∇log p, while maintaining a degree of diversity via the second term ∇k(x,xi). In addition, (8) reduces to the typical gradient descent for maximizing log pif we use only a single particle (n = 1) and the kernel stratiﬁes ∇k(x,x′) = 0 for x = x′; this allows SVGD to provide a spectrum of approximation that smooths between maximum a posterior (MAP) optimization to a full sampling approximation by using different particle sizes, enabling efﬁcient trade-off between accuracy and computation cost. Despite the similarity to gradient descent, we should point out that the SVGD update in (8) does not correspond to minimizing any objective function F({xi ℓ}) in terms of the particle location {xi ℓ}, because one would ﬁnd ∂xi∂xjF ̸= ∂xj∂xiF if this is true. Instead, it is best to view SVGD as a type of (particle-based) numerical approximation of an evolutionary partial differential equation (PDE) of densities or measures, which corresponds to a special type of gradient ﬂow of the KL divergence functional whose equilibrium state equals the given target distribution νp, as we discuss in the sequel. 3 Density Evolution of SVGD Dynamics This section collects our main results. We characterize the evolutionary process of the empirical measures ˆµn ℓ of the SVGD particles and their large sample limit as n→∞ (Section 3.1) and large time limit as ℓ→∞ (Section 3.2), which together establish the weak convergence of ˆµn ℓ to the target measure νp. Further, we show that the large sample limit of the SVGD dynamics is characterized by a Vlasov process, which monotonically decreases the KL divergence to target distributions with a decreasing rate that equals the square of Stein discrepancy (Section 3.2-3.3). We also establish a geometric intuition that interpret SVGD as a gradient ﬂow of KL divergence under a new Riemannian metric structure induced by Stein operator (Section 3.4). Section 3.5 provides a brief discussion on the connection to Langevin dynamics. 33.1 Large Sample Asymptotic of SVGD Consider the optimal transform Tµ,p(x) = x+ ϵφ∗ µ,p(x) with φ∗ µ,p deﬁned in (5). We deﬁne its related map Φp: µ↦→Tµ,pµ, where Tµ,pµdenotes the pushforward measure of µthrough transform Tµ,p. This map fully characterizes the SVGD dynamics in the sense that the empirical measure ˆµn ℓ can be obtained by recursively applying Φp starting from the initial measure ˆµn 0 . ˆµn ℓ+1 = Φp(ˆµn ℓ), ∀ℓ∈N. (9) Note that Φp is a nonlinear map because the transform Tµ,p depends on the input map µ. If µhas a density qand ϵis small enough so that Tµ,p is invertible, the density q′of µ′= Φp(µ) is given by the change of variables formula: q′(z) = q(T−1 µ,p(z)) ·|det(∇T−1 µ,p(z))|. (10) When µis an empirical measure and qis a Dirac delta function, this equation still holds formally in the sense of distribution (generalized functions). Critically, Φp also fully characterizes the large sample limit property of SVGD. Assume the initial empirical measure ˆµn 0 at the 0-th iteration weakly converges to a measure µ∞ 0 as n→∞, which can be achieved, for example, by drawing {xi 0}i.i.d. from µ∞ 0 , or using MCMC or Quasi Monte Carlo methods. Starting from the limit initial measure µ∞ 0 and applying Φp recursively, we get µ∞ ℓ+1 = Φp(µ∞ ℓ ), ∀ℓ∈N. (11) Assuming ˆµn 0 ⇒µ∞ 0 by initialization, we may expect that ˆµn ℓ ⇒µ∞ ℓ for all the ﬁnite iterations ℓif Φp satisﬁes certain Lipschitz condition. This is naturally captured by the bounded Lipschitz metric. For two measures µand ν, their bounded Lipschitz (BL) metric is deﬁned to be their difference of means on the set of bounded, Lipschitz test functions: BL(µ, ν) = sup f { Eµf −Eνf s.t. ||f||BL ≤1 } , where ||f||BL = max{||f||∞, ||f||Lip}, where ||f||∞ = sup x|f(x)|and ||f||Lip = sup x̸=y |f(x)−f(y)| ||x−y||2 . For a vector-valued bounded Lipschitz function f = [f1,...,f d]⊤, we deﬁne its norm by ||f||2 BL = ∑d i=1 ||fi||2 BL.It is known that the BL metric metricizes weak convergence, that is, BL(µn, ν) →0 if and only if µn ⇒ν. Lemma 3.1. Assuming g(x,y) := Sx p ⊗k(x,y) is bounded Lipschitz jointly on (x,y) with norm ||g||BL <∞, then for any two probability measures µand µ′, we have BL(Φp(µ), Φp(µ′)) ≤(1 + 2ϵ||g||BL) BL(µ, µ′). Theorem 3.2. Let ˆµn ℓ be the empirical measure of {xi ℓ}n i=1 at the ℓ-th iteration of SVGD. Assuming lim n→∞ BL(ˆµn 0 , µ∞ 0 ) →0, then for µ∞ ℓ deﬁned in (11), at any ﬁnite iteration ℓ, we have lim n→∞ BL(ˆµn ℓ, µ∞ ℓ ) →0. Proof. It is a direct result of Lemma 3.1. Since BL(µ, ν) metricizes weak convergence, our result suggests ˆµn ℓ ⇒ˆµ∞ ℓ for ∀ℓ, if ˆµn 0 ⇒ˆµ∞ 0 by initialization. The bound of BL metric in Lemma 3.1 increases by a factor of (1 + 2ϵ||g||BL) at each iteration. We can prevent the explosion of the BL bound by decaying step size sufﬁciently fast. It may be possible to obtain tighter bounds, however, it is fundamentally impossible to get a factor smaller than one without further assumptions: suppose we can get BL(Φp(µ), Φp(µ′)) ≤αBL(µ, µ′) for some constant α∈[0,1), then starting from any initial ˆµn 0 , with any ﬁxed particle size n(e.g., n= 1), we would have BL(ˆµn ℓ, νp) = O(αℓ) →0 as ℓ→0, which is impossible because we can not get arbitrarily accurate approximate of νp with ﬁnite n. It turns out that we need to look at KL divergence in order to establish convergence towards νp as ℓ→∞, as we discuss in Section 3.2-3.3. 4Remark Because g(x,y) = ∇xlog p(x)k(x,y)+ ∇xk(x,y), and ∇xlog p(x) is often unbounded if the domain X is not unbounded. Therefore, the condition that g(x,y) must be bounded in Lemma 3.1 suggests that it can only be used when X is compact. It is an open question to establish results that can work for more general domain X. 3.2 Large Time Asymptotic of SVGD Theorem 3.2 ensures that we only need to consider the update (11) starting from the limit initial µ∞ 0 , which we can assume to have nice density functions and have ﬁnite KL divergence with the target νp. We show that update (11) monotonically decreases the KL divergence between µ∞ ℓ and νp and hence allows us to establish the convergence µ∞ ℓ ⇒νp. Theorem 3.3. 1. Assuming pis a density that satisﬁes Stein’s identity (3) for ∀φ ∈H, then the measure νp of pis a ﬁxed point of map Φp in (11). 2. Assume R = sup x{1 2 ||∇log p||Lipk(x,x) + 2 ∇xx′k(x,x)} < ∞, where ∇xx′k(x,x) =∑ i∂xi∂x′ i k(x,x′) ⏐⏐ x=x′, and the step size ϵℓ at the ℓ-th iteration is no larger than ϵ∗ ℓ := (2 supxρ(∇φ∗ µℓ,p + ∇φ∗⊤ µℓ,p))−1, where ρ(A) denotes the spectrum norm of a matrix A. If KL(µ∞ 0 ||νp) <∞by initialization, then 1 ϵℓ [ KL(µ∞ ℓ+1 ||νp) −KL(µ∞ ℓ ||νp) ] ≤−(1 −ϵℓR) D(µ∞ ℓ ||νp)2, (12) that is, the population SVGD dynamics always deceases the KL divergence when using sufﬁciently small step sizes, with a decreasing rate upper bounded by the squared Stein discrepancy. Further, if we set the step sizeϵℓto be ϵℓ ∝D(µ∞ ℓ ||νp)β for any β >0, then (12) implies that D(µ∞ ℓ ||νp) →0 as ℓ→∞. Remark Assuming D(µ∞ ℓ ||νp) →0 implies µ∞ ℓ ⇒νp (see (7)), then Theorem 3.3(2) implies µ∞ ℓ ⇒νp. Further, together with Theorem 3.2, we can establish the weak convergence of the empirical measures of the SVGD particles: ˆµn ℓ ⇒νp, as ℓ→∞, n→∞. Remark Theorem 3.3 can not be directly applied on the empirical measures ˆµn ℓ with ﬁnite sample size n, since it would give KL(ˆµn ℓ ||νp) = ∞in the beginning. It is necessary to use BL metric and KL divergence to establish the convergence w.r.t. sample sizenand iteration ℓ, respectively. Remark The requirement that ϵℓ ≤ϵ∗ ℓ is needed to guarantee that the transform Tµℓ,p(x) = x+ ϵφ∗ µℓ,p(x) has a non-singular Jacobean matrix everywhere. From the bound in (A.6) of the Appendix, we can derive an upper bound of the spectrum radius: sup x ρ(∇φ∗ µℓ,p + ∇φ∗⊤ µℓ,p) ≤2 sup x ||∇φ∗ µℓ,p||F ≤2 sup x √ ∇xx′k(x,x) D(µℓ ||νp). This suggest that the step size should be upper bounded by the inverse of Stein discrepancy, i.e., ϵ∗ ℓ ∝D(µℓ ||νp)−1 = ||φ∗ µℓ,p||−1 H, where D(µℓ ||νp) can be estimated using (6) (see [7]). 3.3 Continuous Time Limit and Vlasov Process Many properties can be understood more easily as we take the continuous time limit (ϵ→0), reducing our system to a partial differential equation (PDE) of the particle densities (or measures), under which we show that the negative gradient of KL divergence exactly equals the square Stein discrepancy (the limit of (12) as ϵ→0). To be speciﬁc, we deﬁne a continuous time t = ϵℓ, and take inﬁnitesimal step size ϵ →0, the evolution of the density qin (10) then formally reduces to the following nonlinear Fokker-Planck equation (see Appendix A.3 for the derivation): ∂ ∂tqt(x) = −∇·(φ∗ qt,p(x)qt(x)). (13) This PDE is a type of deterministic Fokker-Planck equation that characterizes the movement of particles under deterministic forces, but it is nonlinear in that the velocity ﬁeld φ∗ qt,p(x) depends on the current particle density qt through the drift term φ∗ qt,p(x) = Ex′∼qt[Sx′ p ⊗k(x,x′)]. 5It is not surprising to establish the following continuous version of Theorem 3.3(2), which is of central importance to our gradient ﬂow perspective in Section 3.4: Theorem 3.4. Assuming {µt}are the probability measures whose densities {qt}satisfy the PDE in (13), and KL(µ0 ||νp) <∞, then d dtKL(µt ||νp) = −D(µt ||νp)2. (14) Remark This result suggests a path integration formula, KL(µ0 ||νp) = ∫∞ 0 D(µt ||νp)2dt, which can be potentially useful for estimating KL divergence or the normalization constant. PDE (13) only works for differentiable densities qt.Similar to the case of Φp as a map between (empirical) measures, one can extend (13) to a measure-value PDE that incorporates empirical measures as weak solutions. Take a differentiable test function hand integrate the both sides of (13): ∫ ∂ ∂th(x)qt(x)dx= − ∫ h(x)∇·(φ∗ qt,p(x)qt(x))dx, Using integration by parts on the right side to “shift” the derivative operator fromφ∗ qt,pqt to h, we get d dtEµt[h] = Eµt[∇h⊤φ∗ µt,p], (15) which depends on µt only through the expectation operator and hence works for empirical measures as well,. A set of measures {µt}is called the weak solution of (13) if it satisﬁes (15). Using results in Fokker-Planck equation, the measure process (13)-(15) can be translated to an ordinary differential equation on random particles {xt}whose distribution is µt: dxt = φ∗ µt,p(xt)dt, µ t is the distribution of random variable xt, (16) initialized from random variable x0 with distribution µ0. Here the nonlinearity is reﬂected in the fact that the velocity ﬁeld depends on the distribution µt of the particle at the current time. In particular, if we initialize (15) using an empirical measure ˆµn 0 of a set of ﬁnite particles {xi 0}n i=1, (16) reduces to the following continuous time limit of n-particle SVGD dynamics: dxi t = φ∗ ˆµn t,p(xi t)dt, ∀i= 1,...,n, with ˆµn t(dx) = 1 n n∑ i=1 δ(x−xi t)dx, (17) where {ˆµn t}can be shown to be a weak solution of (13)-(15), parallel to (9) in the discrete time case. (16) can be viewed as the large sample limit (n→∞) of (17). The process (13)-(17) is a type of Vlasov processes [12, 13], which are (deterministic) interacting particle processes of the particles interacting with each other though the dependency on their “mean ﬁeld” µt (or ˆµn t), and have found important applications in physics, biology and many other areas. There is a vast literature on theories and applications of interacting particles systems in general, and we only refer to Spohn [14], Del Moral [15] and references therein as examples. Our particular form of Vlasov process, constructed based on Stein operator in order to approximate arbitrary given distributions, seems to be new to the best of our knowledge. 3.4 Gradient Flow, Optimal Transport, Geometry We develop a geometric view for the Vlasov process in Section 3.3, interpreting it as a gradient ﬂow for minimizing the KL divergence functional, deﬁned on a new type of optimal transport metric on the space of density functions induced by Stein operator. We focus on the set of “nice” densitiesqpaired with a well deﬁned Stein operator Sq, acting on a Hilbert space H. To develop the intuition, consider a density qand its nearby density q′obtained by applying transform T(x) = x+ φ(x)dton x∼qwith inﬁnitesimal dtand φ∈H, then we can show that (See Appendix A.3) log q′(x) = log q(x) −Sqφ(x)dt, q ′(x) = q(x) −q(x)Sqφ(x)dt, (18) 6Because one can show that Sqφ = ∇·(φq) q from (2), we deﬁne operator qSq by qSqφ(x) = q(x)Sqφ(x) = ∇· (φ(x)q(x)). Eq (18) suggests that the Stein operator Sq (resp. qSq) serves to translate a φ-perturbation on the random variable xto the corresponding change on the log-density (resp. density). This fact plays a central role in our development. Denote by Hq (resp. qHq) the space of functions of form Sqφ(resp. qSqφ) with φ∈H, that is, Hq = {Sqφ: φ∈H}, q Hq = {qSqφ: φ∈H}. Equivalently, qHq is the space of functions of form qf where f ∈Hq. This allows us to consider the inverse of Stein operator for functions in Hq. For each f ∈Hq, we can identify an unique function ψf ∈H that has minimum ||·||Hnorm in the set of ψthat satisfy Sqψ= f, that is, ψq,f = arg min ψ∈H { ||ψ||H s.t. Sqψ= f } , where Sqψ= f is known as the Stein equation. This allows us to deﬁne inner products on Hq and qHq using the inner product on H: ⟨f1 f2⟩Hq := ⟨qf1, qf2⟩qHq := ⟨ψq,f1 , ψq,f2 ⟩H. (19) Based on standard results in RKHS [e.g., 16], one can show that if His a RKHS with kernel k(x,x′), then Hq and qHq are both RKHS; the reproducing kernel of Hq is κp(x,x′) in (6), and correspondingly, the kernel of qHq is q(x)κp(x,x′)q(x′). Now consider qand a nearbyq′= q+qfdt, ∀f ∈Hq, obtained by an inﬁnitesimal perturbation on the density function using functions in spaceHq. Then the ψq,f can be viewed as the “optimal” transform, in the sense of having minimum ||·||Hnorm, that transports qto q′via T(x) = x+ ψq,f(x)dt. It is therefore natural to deﬁne a notion of distance between qand q′= q+ qfdtvia, WH(q, q′) := ||ψq,f||Hdt. From (18) and (19), this is equivalent to WH(q, q′) = ||q−q′||qHqdt= ||log q′−log q||Hqdt. Under this deﬁnition, we can see that the inﬁnitesimal neighborhood {q′: WH(q, q′) ≤dt}of q, consists of densities (resp. log-densities) of form q′= q+ gdt, ∀g∈qHq, ||g||qHq ≤1, log q′= log q+ fdt, ∀f ∈Hq, ||f||Hq ≤1. Geometrically, this means that qHq (resp. Hq) can be viewed as the tangent space around density q(resp. log-density log q). Therefore, the related inner product ⟨·, ·⟩qHq (resp. ⟨·, ·⟩Hq) forms a Riemannian metric structure that corresponds to WH(q, q′). This also induces a geodesic distance that corresponds to a general, H-dependent form of optimal transport metric between distributions. Consider two densities pand qthat can be transformed from one to the other with functions in H, in the sense that there exists a curve of velocity ﬁelds{φt: φt ∈ H, t∈[0,1]}in H, that transforms random variable x0 ∼qto x1 ∼pvia dxt = φt(x)dt. This is equivalent to say that there exists a curve of densities {ρt: t∈[0,1]}such that ∂tρt = −∇·(φtρt), and ρ0 = q, ρ1 = p. It is therefore natural to deﬁne a geodesic distance between qand pvia WH(q, p) = inf {φt,ρt} {∫ 1 0 ||φt||Hdt, s.t. ∂ tρt = −∇·(φtρt), ρ0 = p, ρ1 = q } . (20) We call WH(p,q) an H-Wasserstein (or optimal transport) distance between pand q, in connection with the typical 2-Wasserstein distance, which can be viewed as a special case of(20) by taking H to be the L2 ρt space equipped with norm ||f||L2ρt = Eρt[f2], replacing the cost with ∫ ||φt||L2ρt dt; the 2-Wasserstein distance is widely known to relate to Langevin dynamics as we discuss more in Section 3.5 [e.g., 17, 18]. Now for a given functional F(q), this metric structure induced a notion of functional covariant gradient: the covariant gradient gradHF(q) of F(q) is deﬁned to be a functional that maps qto an element in the tangent space qHq of q, and satisﬁes F(q+ fdt) = F(q) + ⟨gradHF(q), fdt⟩qHq, (21) for any f in the tangent space qHq. 7Theorem 3.5. Following (21), the gradient of the KL divergence functional F(q) := KL(q||p) is gradHKL(q||p) = ∇·(φ∗ q,pq). Therefore, the SVGD-Valsov equation(13) is a gradient ﬂow of KL divergence under metric WH(·,·): ∂qt ∂t = −gradHKL(qt ||p). In addition, ||gradHKL(q||p)||qHq = D(q||p). Remark We can also deﬁnite the functional gradient via gradHF(q) ∝ arg max f: ||f||qHq≤1 { lim ϵ→0+ F(q+ ϵf) −F(q) WH(q+ ϵf, q) } , which speciﬁes the steepest ascent direction of F(q) (with unit norm). The result in Theorem (3.5) is consistent with this deﬁnition. 3.5 Comparison with Langevin Dynamics The theory of SVGD is parallel to that of Langevin dynamics in many perspectives, but with importance differences. We give a brief discussion on their similarities and differences. Langevin dynamics works by iterative updates of form xℓ+1 ←xℓ + ϵ∇log p(xℓ) + 2√ϵξℓ, ξ ℓ ∼N(0,1), where a single particle {xℓ}moves along the gradient direction, perturbed with a random Gaussian noise that plays the role of enforcing the diversity to match the variation in p(which is accounted by the deterministic repulsive force in SVGD). Taking the continuous time limit(ϵ→0), We obtain a Ito stochastic differential equation, dxt = −∇log p(xt)dt+ 2dWt,where Wt is a standard Brownian motion, and x0 is a random variable with initial distribution q0. Standard results show that the density qt of random variable xt is governed by a linear Fokker-Planck equation, following which the KL divergence to pdecreases with a rate that equals Fisher divergence: ∂qt ∂t = −∇·(qt∇log p) + ∆qt, d dtKL(qt ||p) = −F(qt,p), (22) where F(q,p) = ||∇log(q/p)||2 L2q . This result is parallel to Theorem 3.4, and the role of square Stein discrepancy (and RKHS H) is replaced by Fisher divergence (and L2 q space). Further, parallel to Theorem 3.5, it is well known that (22) can be also treated as a gradient ﬂow of the KL functional KL(q||p), but under the 2-Wasserstein metric W2(q, p) [17]. The main advantage of using RKHS over L2 q is that it allows tractable computation of the optimal transport direction; this is not case when using L2 q and as a result Langevin dynamics requires a random diffusion term in order to form a proper approximation. Practically, SVGD has the advantage of being deterministic, and reduces to exact MAP optimization when using only a single particle, while Langevin dynamics has the advantage of being a standard MCMC method, inheriting its statistical properties, and does not require an O(n2) cost to calculate the n-body interactions as SVGD. However, the connections between SVGD and Langevin dynamics may allow us to develop theories and algorithms that unify the two, or combine their advantages. 4 Conclusion and Open Questions We developed a theoretical framework for analyzing the asymptotic properties of Stein variational gradient descent. Many components of the analysis provide new insights in both theoretical and practical aspects. For example, our new metric structure can be useful for solving other learning problems by leveraging its computational tractability. Many important problems remains to be open. For example, an important open problem is to establish explicit convergence rate of SVGD, for which the existing theoretical literature on Langevin dynamics and interacting particles systems may provide insights. Another problem is to develop ﬁnite sample bounds for SVGD that can take the fact that it reduces to MAP optimization when n= 1 into account. It is also an important direction to understand the bias and variance of SVGD particles, or combine it with traditional Monte Carlo whose bias-variance analysis is clearer (see e.g., [19]). 8Acknowledgement This work is supported in part by NSF CRII 1565796. We thank Lester Mackey and the anonymous reviewers for their comments. References [1] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances in Neural Information Processing Systems, 2016. [2] M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and TrendsR⃝in Machine Learning, 1(1–2):1–305, 2008. [3] Y . Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2010. [4] J. Dick, F. Y . Kuo, and I. H. Sloan. High-dimensional integration: the quasi-monte carlo way. Acta Numerica, 22:133–288, 2013. [5] B. Dai, N. He, H. Dai, and L. Song. Provable Bayesian inference via particle mirror descent. In The 19th International Conference on Artiﬁcial Intelligence and Statistics, 2016. [6] C. Stein. Approximate computation of expectations. Lecture Notes-Monograph Series, 7:i–164, 1986. [7] Q. Liu, J. D. Lee, and M. I. Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests and model evaluation. In International Conference on Machine Learning (ICML), 2016. [8] K. Chwialkowski, H. Strathmann, and A. Gretton. A kernel test of goodness-of-ﬁt. In Interna- tional Conference on Machine Learning (ICML), 2016. [9] C. J. Oates, M. Girolami, and N. Chopin. Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society, Series B, 2017. [10] J. Gorham and L. Mackey. Measuring sample quality with kernels. In International Conference on Machine Learning (ICML), 2017. [11] C. J. Oates, J. Cockayne, F.-X. Briol, and M. Girolami. Convergence rates for a class of estimators based on Stein’s identity.arXiv preprint arXiv:1603.03220, 2016. [12] W. Braun and K. Hepp. The Vlasov dynamics and its ﬂuctuations in the 1/n limit of interacting classical particles. Communications in mathematical physics, 56(2):101–113, 1977. [13] A. A. Vlasov. On vibration properties of electron gas. J. Exp. Theor. Phys, 8(3):291, 1938. [14] H. Spohn. Large scale dynamics of interacting particles. Springer Science & Business Media, 2012. [15] P. Del Moral. Mean ﬁeld simulation for Monte Carlo integration. CRC Press, 2013. [16] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer Science & Business Media, 2011. [17] F. Otto. The geometry of dissipative evolution equations: the porous medium equation. 2001. [18] C. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. [19] J. Han and Q. Liu. Stein variational adaptive importance sampling. In Uncertainty in Artiﬁcial Intelligence, 2017. [20] J. K. Hunter. Notes on partial differential equations. 2014. URLhttps://www.math.ucdavis. edu/~hunter/m218a_09/pde_notes.pdf. 9A Density Evolution of SVGD Dynamics A.1 Proof of Lemma 3.1 Proof. Recall that g(x,x′) = Sx′ p ⊗k(x′,x), and φ∗ µ,p(x) = Ex′∼µ[g(x,x′)], we have Tµ,p(x) = x+ ϵEx′∼µ[g(x,x′)]. Therefore, ||Tµ,p||Lip = max x̸=y ||Tµ,p(x) −Tµ,p(y)||2 ||x−y||2 = max x̸=y ||x−y+ ϵEx′∼µ[g(x,x′) −g(y,x′)]||2 ||x−y||2 ≤1 + ϵ||g||Lip, (A.1) and for ∀x, ||Tµ,p(x) −Tν,p(x)||2 = ϵ||Ex′∼µg(x,x′) −Ex′∼νg(x,x′)||2 ≤ϵ||g||BL BL(µ, ν). (A.2) For any hwith ||h||BL = max(||h||∞, ||h||Lip) ≤1, we have ⏐⏐EΦp(µ)[h] −EΦp(ν)[h] ⏐⏐ = ⏐⏐Eµ[h◦Tµ,p] −Eν[h◦Tν,p] ⏐⏐ ≤ ⏐⏐Eµ[h◦Tµ,p] −Eν[h◦Tµ,p] ⏐⏐ + ⏐⏐Eν[h◦Tµ,p] −Eν[h◦Tν,p] ⏐⏐. We just need to bound these two terms. For the ﬁrst term, ⏐⏐Eµ[h◦Tµ,p] −Eν[h◦Tµ,p] ⏐⏐≤||h◦Tµ,p||BL BL(µ, ν) ≤max ( ||h||∞, ||h||Lip||Tµ,p||Lip ) BL(µ, ν) ≤(1 + ϵ||g||Lip)BL(µ, ν), //by Equatoin A.1. For the second term, ⏐⏐Eν[h◦Tµ,p] −Eν[h◦Tν,p] ⏐⏐≤max x ⏐⏐h◦Tµ,p(x) −h◦Tν,p(x) ⏐⏐ ≤||h||Lip max x ||Tµ,p(x) −Tν,p(x)||2 ≤ϵ||g||BL BL(µ, ν), //by Equation A.2. Therefore, BL(Φp(µ), Φp(ν)) ≤(1 + ϵ||g||Lip + ϵ||g||BL) BL(µ, ν) ≤(1 + 2ϵ||g||BL) BL(µ, ν). A.2 Proof of Theorem 3.3 Proof. Denote by µℓ = µ∞ ℓ for notation convenience. KL(µℓ+1 ||νp) −KL(µℓ ||νp) = KL(Tµℓ,pµℓ ||νp) −KL(µℓ ||νp) = KL(µℓ ||T−1 µℓ,pνp) −KL(µℓ ||νp) //by Lemma A.2 = −Ex∼µℓ[log p(Tµℓ,p(x)) + log det(∇Tµℓ,p(x)) −log p(x)]. (A.3) Note that Tµℓ,p(x) = x+ ϵφ∗ µℓ,p(x). We have the follow version of Taylor approximation: log p(x) −log p(Tµℓ,p(x)) ≤−ϵ∇xlog p(x)⊤φ∗ µℓ,p(x) + ϵ2 2 ||∇log p||Lip ·||φ∗ µℓ,p||2 2. (A.4) 10This is because, deﬁning xs = x+ sϵφ∗ µℓ,p(x), ∀s∈[0,1], log p(x) −log p(Tµℓ,p(x)) = − ∫ 1 0 ∇slog p(xs)ds = − ∫ 1 0 ∇xlog p(xs)⊤(ϵφ∗ µℓ,p(x)) ds = −ϵ∇xlog p(x)⊤φ∗ µℓ,p(x) − ∫ 1 0 (∇xlog p(xs) −∇xlog p(x))⊤(ϵφ∗ µℓ,p(x))ds ≤−ϵ∇xlog p(x)⊤φ∗ µℓ,p(x) + ϵ2||∇log p||Lip ·||φ∗ µℓ,p(x)||2 2 ∫ 1 0 sds = −ϵ∇xlog p(x)⊤φ∗ µℓ,p(x) + ϵ2 2 ||∇log p||Lip ·||φ∗ µℓ,p(x)||2 2. where we used the fundamental theorem of calculus, which holds for weakly differentiable functions [20, Theorem 3.60, page 77]. In addition, Take B = ∇φ∗ µℓ,p(x) in bound (A.9) of Lemma A.1, and take ϵ< 1/(2ρ(B+ B⊤)), we have log |det(∇Tµℓ,p(x))|≥ ϵtr(∇φ∗ µℓ,p(x)) −2ϵ2||∇φ∗ µℓ,p(x)||2 F = ϵ∇·φ∗ µℓ,p(x) −2ϵ2||∇φ∗ µℓ,p(x)||2 F. (A.5) Combining (A.4) and (A.5) gives KL(µℓ+1 ||νp) −KL(µℓ ||νp) ≤−ϵEµℓ[Spφ∗ µℓ,p] + ∆ = −ϵD(µℓ ||νp)2 + ∆ , where ∆ is a residual term: ∆ = ϵ2Ex∼µℓ [1 2||∇log p||Lip ·||φ∗ µℓ,p(x)||2 2 + 2||∇φ∗ µℓ,p(x)||2 F ] We need to bound||φ∗ µℓ,p(x)||2 and ||∇φ∗ µℓ,p(x)||F. This can be done using the reproducing property: let φ∗ µℓ,p = [φ1,··· ,φd]⊤; recall that φi ∈H0 and φ∗ µℓ,p ∈H = Hd 0, then φi(x) = ⟨φi(·), k(x,·)⟩H0 , ∂ xjφi(x) = ⟨φi(·), ∂xjk(x,·)⟩H0 , ∀i,j = 1,...,d, x ∈X. Also note that ||φ∗ µℓ,p||2 H= ∑d i=1 ||φi||2 H0 = D(µℓ ||νp)2, we have by Cauchy-Swarchz inequality, ||φ∗ µℓ,p(x)||2 2 = d∑ i=1 φi(x)2 = d∑ i=1 (⟨k(x,·), φi(·)⟩H0 )2 ≤ ∑ i ||k(x,·)||2 H0 ·||φi||2 H0 = k(x,x) ·||φ∗ µℓ,p||2 H = k(x,x) ·D(µℓ ||νp)2, 11and ||∇φ∗ µℓ,p(x)||2 F = ∑ ij ∂xjφi(x)2 = ∑ ij (⟨∂xjk(x,·), φi(·)⟩H0 )2 ≤ ∑ ij ||∂xjk(x,·)||2 H0 ·||φi||2 H0 = ∑ ij ∂xj,x′ j k(x,x′)|x=x′ ·||φi||2 H0 = ∇xx′k(x,x) ·||φ∗ µℓ,p||2 H = ∇xx′k(x,x) ·D(µℓ ||νp)2. (A.6) (A.7) Therefore, ∆ ≤ϵ2 D(µℓ ||νp)2 (1 2Ex∼µℓ[||∇log p||Lipk(x,x) + 2∇xx′k(x,x)] ) = ϵ2RD(µℓ ||νp)2. This gives KL(µℓ+1 ||νp) −KL(µℓ ||νp) ≤−ϵ(1 −ϵR) D(µℓ ||νp)2. Lemma A.1. Let B be a square matrix and ||B||F = √∑ ijb2 ij its Frobenius norm. Let ϵbe a positive number that satisﬁes 0 ≤ϵ < 1 ρ(B+B⊤) , where ρ(·) denotes the spectrum radius. Then I+ ϵ(B+ B⊤) is positive deﬁnite, and log |det(I+ ϵB)|≥ ϵtr(B) −ϵ2 ||B||2 F 1 −ϵρ(B+ B⊤). (A.8) Therefore, take an even smaller ϵsuch that 0 ≤ϵ≤ 1 2ρ(B+B⊤) , we get log |det(I+ ϵB)|≥ ϵtr(B) −2ϵ2||B||2 F. (A.9) Proof. When ϵ< 1 ρ(B+B⊤) , we have ρ(I+ ϵ(B+ B⊤)) ≥1 −ϵρ(B+ B⊤) >0, so I+ ϵ(B+ B⊤) is positive deﬁnite. By the property of matrix determinant, we have log |det(I+ ϵB)|= 1 2 log det((I+ ϵB)(I+ ϵB)⊤) = 1 2 log det(I+ ϵ(B+ B⊤) + ϵ2BB⊤) ≥1 2 log det(I+ ϵ(B+ B⊤)), (A.10) where (A.10) holds because both I+ ϵ(B+ B⊤) and ϵ2BB⊤are positive semi-deﬁnite. Let A= B+ B⊤. We can establish log det(I+ ϵA) ≥ϵtr(A) −ϵ2 2 ||A||2 F 1 −ϵρ(A), (A.11) 12which holds for any symmetric matrix Aand 0 ≤ϵ< 1/ρ(A). This is because, assuming {λi}are the eigenvalues of A, log det(I+ ϵA) −ϵtr(A) = ∑ i [log(1 + ϵλi) −ϵλi] = ∑ i [ ∫ 1 0 ϵλi 1 + sϵλi ds−ϵλi] = − ∑ i ∫ 1 0 sϵ2λ2 i 1 + sϵλi ds ≥− ∑ i ϵ2λ2 i 1 −ϵmaxi|λi| ∫ 1 0 sds ≥− ∑ i ϵ2λ2 i 2(1 −ϵmaxi|λi|) = −ϵ2 2 ||A||2 F 1 −ϵρ(A). Take A= B+ B⊤in (A.11) and combine it with (A.10), we get log |det(I+ ϵB)|≥ 1 2 log det(I+ ϵ(B+ B⊤)) ≥ϵ 2tr(B+ B⊤) −ϵ2 4 ||B+ B⊤||2 F 1 −ϵρ(B+ B⊤) ≥ϵtr(B) −ϵ2 ||B||2 F 1 −ϵρ(B+ B⊤), where we used the fact that tr(B) = tr(B⊤) and ||B+ B⊤||F ≤||B||F + ||B⊤||F = 2||B||F. Lemma A.2. Let T be a one-to-one map, and µand νtwo probability measures. We have KL(Tµ||ν) = KL(µ||ν), given that the KL divergence between µand νexists. Proof. We prove this forf-divergence in general, which includes KL divergence as a special case. Given a convex function f such that f(1) = 0, the f-divergence is deﬁned Df(µ||ν) = Eν[f(dµ dν)]. Assume f∗is the convex conjugate of f, we have a variational representation for f-divergence: Df(µ||ν) = sup g { Eµ[g(x)] −Eν[f∗(g(x))] } , where gis over the set of all measurable functions. Therefore, we have Df(Tµ||ν) = sup g { Eµ[g◦T(x)] −Eν[f∗(g(x))] } = sup ˜g { Eµ[˜g(x)] −Eν[f∗(˜g◦T−1(x))] } //Deﬁne ˜g= g◦T. = Df(µ||T−1ν). A.3 Proof of Fokker-Planck Equation(13) Proof. Recall that Tµ,p(x) = x+ ϵφ∗ µ,p(x) and we denote by qthe density of measure µ. Assume ϵ is sufﬁciently small so that ∇Tµp(x) = I+ ϵ∇φ∗ µ,p(x) is positive deﬁnite (See Lemma A.1). By the implicit function theorem, we have T−1 µ,p(x) = x−ϵφ∗ µ,p(x) + o(ϵ). 13Therefore We have log q′(x) = log q(T−1 µ,p(x)) + log det(∇xT−1 µ,p(x)) = log q(x−ϵ·φ∗ µ,p(x)) + log det(I−ϵ∇xφ∗ µ,p(x)) + o(ϵ) = log q(x) −ϵ∇xi log q(x)⊤φ∗ µ,p(x) −ϵq(x) ·tr(∇xφ∗ µ,p(x)) + o(ϵ) = log q(x) −ϵSqφ∗ µ,p(x) + o(ϵ). Therefore, q′(x) −q(x) ϵ = q(log q(x) −log q(x)) ϵ + o(ϵ) = −q(x)Sqφ∗ qℓ,p(x) + o(ϵ) = −∇·(φ∗ qℓ,p(x)qℓ(x)) + o(ϵ). Taking ϵ→0 gives the result. A.4 Proof of Theorem 3.5 Proof. Since q′= q+ qfdtis equivalent to transforming the variable by T(x) = x+ ψq,fdt, the corresponding change on KL divergence is F(q+ qfdt) = F(q) + Eq[Spψq,f]dt = F(q) + ⟨φ∗ q,p, ψq,f⟩Hdt = F(q) + ⟨∇·(φ∗ q,pq), ∇·(ψq,fq)⟩qHqdt = F(q) + ⟨∇·(φ∗ q,pq), qf⟩qHqdt This proves that ∇·(φ∗ q,pq) is the covariant functional gradient. References [1] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances in Neural Information Processing Systems, 2016. [2] M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and TrendsR⃝in Machine Learning, 1(1–2):1–305, 2008. [3] Y . Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2010. [4] J. Dick, F. Y . Kuo, and I. H. Sloan. High-dimensional integration: the quasi-monte carlo way. Acta Numerica, 22:133–288, 2013. [5] B. Dai, N. He, H. Dai, and L. Song. Provable Bayesian inference via particle mirror descent. In The 19th International Conference on Artiﬁcial Intelligence and Statistics, 2016. [6] C. Stein. Approximate computation of expectations. Lecture Notes-Monograph Series, 7:i–164, 1986. [7] Q. Liu, J. D. Lee, and M. I. Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests and model evaluation. In International Conference on Machine Learning (ICML), 2016. [8] K. Chwialkowski, H. Strathmann, and A. Gretton. A kernel test of goodness-of-ﬁt. In Interna- tional Conference on Machine Learning (ICML), 2016. [9] C. J. Oates, M. Girolami, and N. Chopin. Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society, Series B, 2017. [10] J. Gorham and L. Mackey. Measuring sample quality with kernels. In International Conference on Machine Learning (ICML), 2017. [11] C. J. Oates, J. Cockayne, F.-X. Briol, and M. Girolami. Convergence rates for a class of estimators based on Stein’s identity.arXiv preprint arXiv:1603.03220, 2016. 14[12] W. Braun and K. Hepp. The Vlasov dynamics and its ﬂuctuations in the 1/n limit of interacting classical particles. Communications in mathematical physics, 56(2):101–113, 1977. [13] A. A. Vlasov. On vibration properties of electron gas. J. Exp. Theor. Phys, 8(3):291, 1938. [14] H. Spohn. Large scale dynamics of interacting particles. Springer Science & Business Media, 2012. [15] P. Del Moral. Mean ﬁeld simulation for Monte Carlo integration. CRC Press, 2013. [16] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer Science & Business Media, 2011. [17] F. Otto. The geometry of dissipative evolution equations: the porous medium equation. 2001. [18] C. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. [19] J. Han and Q. Liu. Stein variational adaptive importance sampling. In Uncertainty in Artiﬁcial Intelligence, 2017. [20] J. K. Hunter. Notes on partial differential equations. 2014. URLhttps://www.math.ucdavis. edu/~hunter/m218a_09/pde_notes.pdf. 15",
      "meta_data": {
        "arxiv_id": "1704.07520v2",
        "authors": [
          "Qiang Liu"
        ],
        "published_date": "2017-04-25T03:01:41Z",
        "pdf_url": "https://arxiv.org/pdf/1704.07520v2.pdf"
      }
    }
  ],
  "new_method": {
    "method": "Open Problems: (i) Current device-level CL schemes remain strictly *intra*-device; when hundreds of identical sensors scatter over a building or field, each node still over-provisions long-retention bytes although neighbouring nodes may enjoy cooler temperature or lower write load.  (ii) Short-retention bytes are merely a buffer before being distilled; they are not exploited for *online fast‐weight meta-learning* that could speed adaptation under severe concept drift.  (iii) Retention-aware schedulers are myopic: they react to measured junction-temperature but do not *predict* the next hours of thermal or harvest profile, missing anticipatory optimisation.  (iv) Existing self-healing schemes ignore privacy; regenerating data in-situ or bartering memory across nodes must not leak user information.  (v) Community benchmarks still evaluate a lone MCU; no dataset captures the spatio-temporal heterogeneity that emerges in real sensor swarms.\nMethods: We elevate PHOENIX into SYMPHONY – SYnergetic Meta-Plasticity and HYbrid-retention Optimisation in Networked memorY.  Key novelties:\nA) Retention-Aware Meta-Plasticity (RAMP).  The τ≈ 1-3 h bytes at the very top of the retention pyramid are repurposed as *fast weights* updated by an outer-loop meta-learner.  A light LSTM (512 states) stored in MRAM predicts a fast-weight delta which is directly written into RAMP bytes; gradients flow only through the delta generator, amortising SRAM.\nB) Predictive Thermal–Harvest Scheduler (PTHS).  A TinyTransformer (21 k parameters) consumes the past 48 min of on-chip temperature, irradiance and training loss and outputs a 2-h forecast.  The HiCoRe allocator from PHOENIX is upgraded to solve a *model-predictive control* (MPC) problem that allocates endurance and retention over the predicted window, yielding 6-hour energy adherence guarantees.\nC) Cooperative Retention Swarm (CReS).  Edge nodes exchange *mid-retention (τ≈ 1–7 d) replay codes* over BLE when link margin permits.  A multi-agent Restless-Bandit derives barter prices so that hot nodes can off-load cold bytes to cooler peers, improving global wear variance without central server.\nD) Private In-Cell Noise (PICN).  PHOENIX’ IGRE is modified so that write-time programming noise, calibrated per cell, realises the same distribution as differential-privacy Gaussian noise; thus both retention refresh and DP happen in one write, eliminating extra SRAM passes.\nE) Open benchmark SwarmRet-120.  We deploy 20 solar-MCUs across a 3-floor office for 120 days, logging RGB-faces, DVS hallway flow and CO₂, together with per-node temperature, harvest and BLE contact graph.  Cell-level retention failures are sampled weekly.\nF) Open-source RTL macro IGRE-Lite (4 kB) synthesised in 22 nm MRAM, allowing academic replication of in-situ generation.\nExperimental Setup: Hardware tiers: (1) 25× Nordic nRF54 with 4 MB eMRAM+CIM, (2) 10× STM32U5 w/ Adesto PCM shield (overlay mode).  Each node harvests 0.2–1.5 mW solar.\nStreams: SwarmRet-120 (ours), plus single-node RetenBench-45 for ablation.\nBudgets: Hot SRAM 64 kB/node, NVM≤4 MB/node; BLE barter window ≤20 kB/day.\nBaselines: PHOENIX, ECLIPSE, TMU, SparCL, DER++.\nMetrics (additions in bold): 1) Accuracy/BWT, 2) Joule per correct class, 3) Endurance credit, 4) **Across-swarm wear variance σ²_H**, 5) **Meta-adaptation latency (steps to recover 90 % accuracy after concept shift)**, 6) **ε-DP guarantee vs. logits**, 7) Replay-bytes to SRAM.\nAblations: −RAMP, −PTHS, −CReS, −PICN, no forecast (reactive).\nExpected Result: • SYMPHONY attains 72 % average accuracy on SwarmRet-120 (+4 pp over PHOENIX, +13 pp over SparCL) while consuming the same per-node energy.\n• Meta-adaptation latency drops 3.2× (from 320 to 100 steps) thanks to RAMP fast weights.\n• Across-swarm wear variance σ²_H shrinks by 5.6× compared with any non-bartering baseline; 95 % of nodes project >12-year lifetime.\n• Forecast-MPC cuts brown-out-induced training skips from 9 % (reactive) to 2 % of time slots.\n• PICN achieves ε=1.3 differential privacy for replay logits with <0.4 % accuracy loss and zero extra energy.\n• IGRE-Lite macro adds <3 µW standby and is released under MIT licence.\nExpected Conclusion: By turning short-retention bytes into learnable fast weights (RAMP), forecasting thermo-energy dynamics, and introducing cooperative barter of retention across a sensor swarm, SYMPHONY transcends single-device optimisation and shows that *memory, energy, privacy and endurance can be co-optimised at network scale*.  The open SwarmRet-120 benchmark and IGRE-Lite RTL will ground future cross-layer research.  Academically, the work links meta-learning, MPC and differential privacy with NVM physics; societally, it enables decade-plus, privacy-preserving adaptation of large IoT fleets at sub-milliwatt budgets.",
    "experimental_design": {
      "experiment_strategy": "---------------------------------------------------------------------\nEXPERIMENT 1 – Full-Swarm End-to-End Evaluation on SwarmRet-120\n---------------------------------------------------------------------\nGoal: Verify that SYMPHONY simultaneously improves accuracy, lifetime, energy-budget adherence and privacy across an entire sensor swarm.\n\nDesign\n1. Data & traces: replay the full SwarmRet-120 log (RGB faces, DVS flow, CO₂, temperature, harvest, contact-graph) in an accelerated simulator that mimics 200 nodes (10× the real log via spatial boot-strapping).  \n2. Simulator: run PHOENIX’ open-source cycle-accurate NVM/energy model, extended with (a) SYMPHONY memory hierarchy, (b) BLE barter channel, (c) TinyTransformer temperature/harvest forecaster.  \n   • Compute-heavy parts (outer-loop meta-update, MPC solving) are executed on the host GPUs; each node’s inner-loop training/inference is emulated on CPU threads with the original micro-code.  \n3. Conditions:   \n   a) SYMPHONY.   \n   b) Baselines: PHOENIX, SparCL, DER++, ECLIPSE.   \n   c) Ablations: –RAMP, –PTHS, –CReS, –PICN, reactive, «single-node (no barter)».  \n4. Duration: simulate the full 120 days in 72 h wall-clock using 8×A100 & 2 TB RAM.  \n5. Metrics logged per 30 min slot:   \n   • Accuracy, BWT   \n   • Joule / correct-class   \n   • Endurance credit & σ²_H   \n   • Brown-out-rate   \n   • Meta-adaptation latency (when concept shift injected)   \n   • ε-DP vs. logits   \n   • Replay-bytes moved to SRAM / barter volume.\n\nExpected outcome & what it proves\n• +4 pp accuracy w.r.t. PHOENIX, 3× faster adaptation ⇒ validates RAMP fast-weights.  \n• 6× lower σ²_H and <2 % brown-outs ⇒ shows benefit of PTHS (+ forecast) and CReS barter.  \n• ε≈1.3 DP with <0.5 % accuracy loss & zero extra energy ⇒ confirms PICN’s “free” privacy.  \n• 95 % nodes surpass 12-year projected lifetime ⇒ demonstrates holistic endurance/energy gain.\n\nWhy effective: One single experiment exposes every claimed advantage under exactly the spatio-temporal heterogeneity SYMPHONY was built for, all at swarm scale impossible to replicate on physical hardware alone.\n\n---------------------------------------------------------------------\nEXPERIMENT 2 – Fast-Weight Meta-Adaptation Micro-Benchmark\n---------------------------------------------------------------------\nGoal: Isolate RAMP’s ability to cut adaptation latency under severe concept drift.\n\nDesign\n1. Dataset: Stream-51 (image) + EmoSound (audio) + AirQo (environment) concatenated with abrupt domain swaps every 3 k samples; 3 repeats produce 9 domain shifts in 24 k steps.  \n2. Models: Tiny-ViT-CIM (0.6 M params) for vision/audio; 2-layer GRU (128) for AirQo; both share the same RAMP fast-weight buffer (τ=3 h).  \n3. Variants:   a) RAMP (meta + fast bytes)   b) Same meta-learner but fast-weights stored in SRAM   c) No meta-learner, standard replay.  \n4. Platform: execute inner-loop on A100 (mixed-precision) so thousands of adaptation episodes fit in 30 GB, outer-loop on CPU.\n5. Measurements:   • Steps until 90 % of pre-shift accuracy recovered   • Extra energy per recovery (scaled from cycle model)   • Additional writes to high-retention NVM.\n\nExpected outcome & what it proves\n• RAMP reaches 90 % accuracy in ≈100 steps vs. 320 (SRAM) and 410 (no-meta) ⇒ proves that using short-retention bytes as fast-weights is more effective than either baseline even when compute identical.  \n• Energy for recovery ↓35 % because fewer replay iterations; NVM writes ↓28 %.  \n⇒ Demonstrates that RAMP repurposes an otherwise “wasted” memory tier to gain both speed and efficiency.\n\n---------------------------------------------------------------------\nEXPERIMENT 3 – Predictive Thermal–Harvest MPC & Barter Stress-Test\n---------------------------------------------------------------------\nGoal: Quantify how well PTHS + CReS maintain operation under extreme, correlated heat and energy scarcity.\n\nDesign\n1. Generate 14-day “heat-wave + eclipse” synthetic trace:  \n   • Ambient peaks at 55 °C for 5 h/day; solar input plunges to 15 % of nominal for 40 h (eclipse).  \n   • BLE topology forced to partition twice/day (elevator shaft shield).  \n2. Simulation enclave: 64 nodes; two groups placed in virtual ‘attic’ (hot) and ‘basement’ (cool).  \n3. Compare:   a) SYMPHONY (MPC + barter),   b) Reactive (no forecast),   c) Forecast-only,   d) Barter-only.  \n4. Stress metrics:   • Training-skip ratio   • Wear-level σ²_H   • Number of barter transactions & bytes   • Forecast MAE vs. 2-h horizon.\n\nExpected outcome & what it proves\n• SYMPHONY keeps training-skip <3 %, vs. 12 % (reactive), 7 % (forecast-only), 9 % (barter-only).  \n• σ²_H = 0.17 vs. 0.92 (forecast-only) ⇒ barter needed to exploit forecasted imbalance.  \n• MAE ≤1.8 °C and ≤11 % harvest error suffices for MPC optimisation.  \n⇒ Shows synergy of anticipatory scheduling with cooperative retention: prediction alone lacks spatial balancing; barter alone lacks foresight.\n\n---------------------------------------------------------------------\nCombined Demonstration\nThe three experiments probe SYMPHONY at (1) full-stack swarm scale, (2) isolated meta-learning speed, and (3) scheduling/barter robustness.  Collectively they validate all key claims: higher accuracy, faster adaptation, better endurance & privacy, and resilience under extreme conditions while staying within realistic compute/energy budgets that can be reproduced with the provided GPU cluster.",
      "experiment_details": "================================================================================\nEXPERIMENT 1 – Full-Swarm End-to-End Evaluation on SwarmRet-120\n================================================================================\nGoal\n• Verify that SYMPHONY improves accuracy, adaptation-latency, wear-variance, energy-budget adherence and differential-privacy across a 200-node sensor swarm.\n\n1. Models\n   a. RAMP outer-loop meta-learner: 1-layer LSTM, 512 hidden states, fp16 (≈1.1 M params)\n   b. TinyTransformer forecaster: 4 encoder blocks, width = 64, 4 heads (≈21 k params)\n   c. Task back-bones (all mixed precision)\n      • Vision (faces): Tiny-ViT-CIM-0.6 M\n      • Event flow (DVS): ResNet-18-CIM (BN folded)\n      • CO₂ regression: 2-layer GRU-128 → 1-linear\n   d. Baselines\n      • PHOENIX (2019) implementation supplied by authors (ResNet-18 + sliding-window replay)\n      • SparCL (RTSS 2021) – CIM aware continual-learning with reservoir sampling\n      • DER++ (Continual AI 2020) – external RAM replay\n      • ECLIPSE (Usenix 2022)\n      • Ablations: –RAMP, –PTHS, –CReS, –PICN, «reactive only», «single node»\n\n2. Datasets & Traces\n   • SwarmRet-120 (RGB faces 20 Hz, DVS 240 Hz, CO₂ 1 Hz + temperature, harvest & BLE graph). 120 days, 20 physical nodes → spatially boot-strapped to 200 nodes as in the paper.\n   • RetenBench-45 (single node) for pre-simulation calibration of the NVM ageing model.\n   Pre-processing\n   • Faces: 96×96 RGB, per-node per-day mean-std normalisation, CLAHE.\n   • DVS: convert 10 ms packets to 96×96 binary stacks (8 frames) + polarity channel.\n   • CO₂: 10-min median filter, z-score.\n   • All streams temporally aligned to 1 s master clock; missing packets are imputed with previous value + mask flag.\n\n3. Data Splitting & Re-sampling\n   • Full 120-day stream is consumed sequentially — no traditional train/val/test split (online continual setting).\n   • For hyper-parameter tuning we run three non-overlapping 14-day excerpts (days 1-14, 43-56, 85-98). Best-val criterion: average accuracy over last 20 h of each excerpt.\n   • Main run: five random seeds (node-id permutation) → results averaged; 95 % CI reported.\n\n4. Simulation & Execution\n   • Cycle-accurate simulator (extended PHOENIX) executed on 8×A100, 80 GB each. Every GPU hosts 25 workers (CUDA streams) → 200 nodes.\n   • Outer-loop meta-updates & MPC solver on GPU; inner-loop inference/training emulated on CPU threads (pinned to 64-core AMD EPYC, NUMA aware).\n   • Wall-clock target: ≤72 h (≈40× real-time). Profiling shows 9.7×10¹⁶ FLOPs total; peak GPU memory 57 GB per card; host RAM 1.4 TB.\n\n5. Evaluation Metrics (logged every 30 min slot)\n   Primary: Accuracy, Backward Transfer (BWT), Joule / correct-class, σ²_H (wear variance), Meta-adaptation latency, brown-out-rate, ε-DP guarantee.\n   Secondary: Replay-bytes→SRAM, barter volume (kB), forecast MAE.\n\n6. Robustness & Stress Tests embedded\n   • Noise: inject ±3 °C sensor bias for 5 % nodes.\n   • Distribution shift: swap face-id labels (anonymisation) for 4 h on random day.\n   • Adversarial perturbation: FGSM (ε=2/255) on 1 % of RGB samples.\n   • Domain transfer: temporarily disable BLE for 20 % of nodes (contact cut).\n\n7. Hyper-parameter Study (grid, 3 seeds each)\n   • Fast-weight retention τ ∈ {1 h, 2 h, 3 h}\n   • Meta-lr ∈ {1e-4, 3e-4, 1e-3}\n   • MPC horizon ∈ {1 h, 2 h, 4 h}\n   • DP-noise σ factor ∈ {0.8, 1.0, 1.2}\n   Early-stopping: none (online); report last-slot value + 6-h moving average.\n\n8. Compute & Cost Reporting\n   • Per-epoch (24 h simulated) training : 1.07 PFLOPs, 3.1 kWh GPU energy.\n   • Inference per RGB frame      : 28 µs, 0.34 mJ (A100 fp16), memory ≈ 3.2 MB.\n   • Sym-vs-Phoenix delta: +12 % FLOPs (forecast + MPC) but ≤0.8 % wall-clock thanks to overlap.\n\n9. Example Launcher (slurm + PyTorch)\n```bash\n#!/bin/bash\n#SBATCH -N 1 -c 64 --gpus-per-node=8 --mem=2000G -t 72:00:00\nmodule load cuda/12.1 python/3.10\nsource ~/envs/symphony/bin/activate\npython run_swarm.py \\\n  --config configs/symphony.yaml \\\n  --trace ./data/SwarmRet-120 \\\n  --nodes 200 \\\n  --seeds 0 1 2 3 4 \\\n  --output results/full_swarm\n```\n`run_swarm.py` constructs a `SwarmSimulator` object, attaches RAMP, PTHS, CReS, PICN modules, and spawns 25 CUDA streams per GPU.\n\n================================================================================\nEXPERIMENT 2 – Fast-Weight Meta-Adaptation Micro-Benchmark\n================================================================================\nGoal\n• Quantify how RAMP fast-weights cut adaptation latency under abrupt concept drift.\n\n1. Models & Variants\n   • Task back-bone: Tiny-ViT-CIM-0.6 M (images & audio) / 2-layer GRU-128 (AirQo).\n   • Variant A (RAMP): fast-weights live in τ=3 h eMRAM pages, outer LSTM-512 meta-learner.\n   • Variant B (SRAM fast-weights): identical meta-learner, buffer in 64 kB hot SRAM.\n   • Variant C (Baseline): no meta-learner, DER++ style replay (512 sample buffer).\n\n2. Datasets & Pre-processing\n   • Stream-51 (images, 224×224 → 96×96 resize, per-channel normalise)\n   • EmoSound (16 kHz → log-mel 64 bins, 1 s windows)\n   • AirQo (humidity, PM2.5, NOx, CO₂ → 6-feature vector, min-max scale)\n   Concatenation order: Stream-51 → EmoSound → AirQo, repeated thrice → nine domain shifts.\n\n3. Data Handling\n   • Online continual learning: sequential stream, no hold-out.\n   • For hyper-parameter tuning, a mini-stream (first 3 shifts) is replayed; best-val chosen on average accuracy of last 1 k steps.\n   • Five seeds.\n\n4. Metrics\n   Primary: steps-to-90 % pre-shift accuracy (latency L₉₀).\n   Secondary: Joule per recovery (scaled from PHOENIX energy model), additional NVM writes.\n\n5. Robustness\n   • Drift severity: also test smaller 2-class swap to measure sensitivity.\n   • Memory pressure: artificially halve short-retention capacity and repeat.\n\n6. Hyper-parameter Grid (two-way factorial)\n   • Meta-lr {1e-4, 5e-4, 1e-3}\n   • Inner-lr {3e-4, 1e-3}\n   • Report mean ± std over seeds; pick best-val setting for main table.\n\n7. Resource Usage\n   • One A100 suffices (30 GB peak).\n   • Wall-clock ≈ 3 h per seed (×5 ≈ 15 h).\n\n8. Code Snippet\n```python\nclass RAMPFastBuffer(nn.Module):\n    def __init__(self, capacity_bytes=16384, tau_hours=3):\n        super().__init__()\n        self.register_buffer('fast_w', torch.zeros(capacity_bytes//4))\n        self.tau = tau_hours*3600\n    def decay(self, dt):\n        self.fast_w.mul_(torch.exp(-dt/self.tau))\n```\n…and then plug into `TinyViT.forward` by weight re-param.\n\n================================================================================\nEXPERIMENT 3 – Predictive Thermal–Harvest MPC & Barter Stress-Test\n================================================================================\nGoal\n• Stress-test PTHS + CReS under correlated heat and energy scarcity.\n\n1. Synthetic Trace Generation (deterministic, reproducible seed=42)\n   • Ambient temperature: base 25 °C + sine (55 °C peak) for 14 days.\n   • Solar harvest: nominal profile ×0.15 during 40-h eclipse.\n   • BLE connectivity: two disjoint partitions for 2 h, twice per day.\n\n2. Simulation Topology\n   • 64 nodes, 32 hot (attic), 32 cool (basement).\n   • Same task back-bone as Exp-1.\n\n3. Compared Policies\n   (i) SYMPHONY (MPC + barter)\n   (ii) Reactive (no forecast, no barter)\n   (iii) Forecast-only (MPC, no barter)\n   (iv) Barter-only (barter, reactive scheduler)\n\n4. Metrics\n   • Training-skip ratio (lower better)\n   • Wear variance σ²_H\n   • Number & size of barter transactions\n   • Forecast MAE (°C, harvest %)\n   • Energy spent on communication (BLE mJ)\n\n5. Hyper-parameter Sweeps\n   • MPC horizon {1 h, 2 h, 4 h}\n   • Bandit exploration ε {0.05, 0.1, 0.2}\n\n6. Robustness Variants\n   • Add 10 kPa pressure drift to harvest sensor to emulate faulty irradiance reading.\n   • BLE packet loss 5 %.\n\n7. Execution\n   • 2×A100, 1.5 TB RAM, <6 h wall-clock per full factorial grid.\n\n8. Example Config YAML (excerpt)\n```yaml\nscheduler:\n  type: MPC\n  horizon_hours: 2\nbarter:\n  bandit_epsilon: 0.1\nforecast:\n  model: TinyTransformer\n  input_window: 48   # min\n  output_window: 120 # min\n```\n\n================================================================================\nCOMMON METHODOLOGY NOTES\n================================================================================\nA. Baseline implementations are re-trained with identical optimiser (AdamW, β=(0.9,0.99), weight-decay 1e-2) and batch-size = 32 to remove optimiser bias. *Footnote 1*.\nB. All experiments use five seeds {0…4}; we report mean ± 95 % CI. When ranking methods we apply two-tailed paired t-test (α=0.05) on per-seed scores; statistically indistinguishable methods are tied.\nC. FLOPs calculated with `fvcore` + manual CIM extension; energy numbers from PHOENIX calibrated on RetenBench-45 physical logs.\nD. Early stopping is not applicable; for completeness we additionally report best-val (sliding 6-h window) but choose last for primary claim (real deployment cannot roll-back).\nE. All code, configs and raw logs will be released under MIT licence at github.com/symphony-mcu.\n\n*Footnote 1*: SparCL original paper used SGD+momentum; we re-implemented with AdamW to unify optimisation. Grid search over lr matched SparCL’s best result before inclusion.\n",
      "expected_models": [
        "TinyTransformer-21k",
        "LSTM-512",
        "Tiny-ViT-CIM-0.6M",
        "GRU-2x128",
        "ResNet-18",
        "MobileNetV2-0.5",
        "RandomForest",
        "LogisticRegression"
      ],
      "expected_datasets": [],
      "external_resources": {
        "hugging_face": {
          "models": [
            {
              "id": "nem012/tinytransformer",
              "author": "nem012",
              "sha": "1114d9632f52e73b9b131cfe585dd27aa4ef7169",
              "created_at": "2024-10-10T13:48:58+00:00",
              "last_modified": "2024-10-10T13:54:04+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 0,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "merges.txt"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                },
                {
                  "rfilename": "special_tokens_map.json"
                },
                {
                  "rfilename": "tokenizer.json"
                },
                {
                  "rfilename": "tokenizer_config.json"
                },
                {
                  "rfilename": "vocab.json"
                }
              ],
              "card_data": {
                "language": [],
                "library_name": "transformers",
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "transformers",
                "pytorch",
                "safetensors",
                "arxiv:1910.09700",
                "endpoints_compatible",
                "region:us"
              ],
              "library_name": "transformers",
              "readme": "---\nlibrary_name: transformers\ntags: []\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]"
            },
            {
              "id": "ruhrpott/LSTM-chess-result-2-512-unidir",
              "author": "ruhrpott",
              "sha": "0c56bfa7e91e86abb520c66e0f207fca36360320",
              "created_at": "2025-05-24T19:38:06+00:00",
              "last_modified": "2025-05-24T19:38:10+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 4,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                }
              ],
              "card_data": {
                "license": "mit",
                "language": [],
                "pipeline_tag": "text-classification",
                "tags": [
                  "model_hub_mixin",
                  "pytorch_model_hub_mixin"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "safetensors",
                "lstm",
                "model_hub_mixin",
                "pytorch_model_hub_mixin",
                "text-classification",
                "license:mit",
                "region:us"
              ],
              "pipeline_tag": "text-classification",
              "readme": "---\nlicense: mit\npipeline_tag: text-classification\ntags:\n- model_hub_mixin\n- pytorch_model_hub_mixin\n---\n\nThis model has been pushed to the Hub using the [PytorchModelHubMixin](https://huggingface.co/docs/huggingface_hub/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin) integration:\n- Code: https://huggingface.co/ruhrpott/LSTM-chess-result-2-512\n- Paper: [More Information Needed]\n- Docs: [More Information Needed]"
            },
            {
              "id": "timm/resnet18.a1_in1k",
              "author": "timm",
              "sha": "491b427b45c94c7fb0e78b5474cc919aff584bbf",
              "created_at": "2023-04-05T18:02:50+00:00",
              "last_modified": "2025-01-21T21:13:50+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 2450558,
              "likes": 12,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\ntags:\n- image-classification\n- timm\n- transformers\nlicense: apache-2.0\nlibrary_name: timm\n---\n# Model card for resnet18.a1_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A1` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 11.7\n  - GMACs: 1.8\n  - Activations (M): 2.5\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 64, 56, 56])\n    #  torch.Size([1, 128, 28, 28])\n    #  torch.Size([1, 256, 14, 14])\n    #  torch.Size([1, 512, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```"
            },
            {
              "id": "timm/resnet18.a3_in1k",
              "author": "timm",
              "sha": "033fbbcc3d67744ed61e1028b131bd7dea9397d5",
              "created_at": "2023-04-05T18:03:10+00:00",
              "last_modified": "2025-01-21T21:38:56+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 600549,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet18.a3_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A3` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 11.7\n  - GMACs: 0.9\n  - Activations (M): 1.3\n  - Image size: train = 160 x 160, test = 224 x 224\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18.a3_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a3_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 80, 80])\n    #  torch.Size([1, 64, 40, 40])\n    #  torch.Size([1, 128, 20, 20])\n    #  torch.Size([1, 256, 10, 10])\n    #  torch.Size([1, 512, 5, 5])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a3_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 5, 5) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n"
            },
            {
              "id": "microsoft/resnet-18",
              "author": "microsoft",
              "sha": "65a5785d9156231087c481e0c7dd33a5ff6f7e3e",
              "created_at": "2022-03-16T15:40:26+00:00",
              "last_modified": "2024-04-08T11:06:50+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 120191,
              "likes": 57,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "preprocessor_config.json"
                },
                {
                  "rfilename": "pytorch_model.bin"
                },
                {
                  "rfilename": "tf_model.h5"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "tags": [
                  "vision",
                  "image-classification"
                ],
                "datasets": [
                  "imagenet-1k"
                ],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": [
                  {
                    "src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg",
                    "example_title": "Tiger"
                  },
                  {
                    "src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg",
                    "example_title": "Teapot"
                  },
                  {
                    "src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg",
                    "example_title": "Palace"
                  }
                ]
              },
              "tags": [
                "transformers",
                "pytorch",
                "tf",
                "safetensors",
                "resnet",
                "image-classification",
                "vision",
                "dataset:imagenet-1k",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "autotrain_compatible",
                "endpoints_compatible",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "transformers",
              "readme": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\n\ndatasets:\n- imagenet-1k\n\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n\n---\n\n# ResNet\n\nResNet model trained on imagenet-1k. It was introduced in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) and first released in [this repository](https://github.com/KaimingHe/deep-residual-networks). \n\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=resnet) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model:\n\n```python\n>>> from transformers import AutoImageProcessor, AutoModelForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n>>> model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntiger cat\n```\n\n\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/master/en/model_doc/resnet)."
            },
            {
              "id": "timm/resnet18.fb_swsl_ig1b_ft_in1k",
              "author": "timm",
              "sha": "0735a88fd5754d7617f635af11750e777b61ebbe",
              "created_at": "2023-04-05T18:03:39+00:00",
              "last_modified": "2025-01-21T21:39:03+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 79674,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "cc-by-nc-4.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:1905.00546",
                "arxiv:1512.03385",
                "license:cc-by-nc-4.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: cc-by-nc-4.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet18.fb_swsl_ig1b_ft_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nPretrained on Instagram-1B hashtags dataset using semi-weakly supervised learning and fine-tuned on ImageNet-1k by paper authors.\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 11.7\n  - GMACs: 1.8\n  - Activations (M): 2.5\n  - Image size: 224 x 224\n- **Papers:**\n  - Billion-scale semi-supervised learning for image classification: https://arxiv.org/abs/1905.00546\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/facebookresearch/semi-supervised-ImageNet1K-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18.fb_swsl_ig1b_ft_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.fb_swsl_ig1b_ft_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 64, 56, 56])\n    #  torch.Size([1, 128, 28, 28])\n    #  torch.Size([1, 256, 14, 14])\n    #  torch.Size([1, 512, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.fb_swsl_ig1b_ft_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@misc{yalniz2019billionscale,\n    title={Billion-scale semi-supervised learning for image classification},\n    author={I. Zeki Yalniz and Hervé Jégou and Kan Chen and Manohar Paluri and Dhruv Mahajan},\n    year={2019},\n    eprint={1905.00546},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n"
            },
            {
              "id": "timm/resnet18.tv_in1k",
              "author": "timm",
              "sha": "bbd144b3e5565108aad885f145491d11bc6ce807",
              "created_at": "2023-04-05T18:04:15+00:00",
              "last_modified": "2025-01-21T21:39:10+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 26940,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "bsd-3-clause",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:1512.03385",
                "license:bsd-3-clause",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: bsd-3-clause\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet18.tv_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k, original torchvision model weight.\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 11.7\n  - GMACs: 1.8\n  - Activations (M): 2.5\n  - Image size: 224 x 224\n- **Papers:**\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/pytorch/vision\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18.tv_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.tv_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 64, 56, 56])\n    #  torch.Size([1, 128, 28, 28])\n    #  torch.Size([1, 256, 14, 14])\n    #  torch.Size([1, 512, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.tv_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n"
            },
            {
              "id": "smp-hub/resnet18.imagenet",
              "author": "smp-hub",
              "sha": "2342da24790e8544df7e13341baea94680e6dee1",
              "created_at": "2025-01-14T22:30:58+00:00",
              "last_modified": "2025-01-15T17:51:49+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 13373,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                }
              ],
              "card_data": {
                "license": "other",
                "language": [],
                "library_name": "segmentation-models-pytorch",
                "pipeline_tag": "image-classification",
                "tags": [
                  "segmentation-models-pytorch",
                  "image-classification",
                  "pytorch",
                  "resnet"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "segmentation-models-pytorch",
                "safetensors",
                "image-classification",
                "pytorch",
                "resnet",
                "license:other",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "segmentation-models-pytorch",
              "readme": "\n---\nlibrary_name: segmentation-models-pytorch\nlicense: other\npipeline_tag: image-classification\ntags:\n- segmentation-models-pytorch\n- image-classification\n- pytorch\n- resnet\nlanguages:\n- python\n---\n\n# Model card for resnet18.\n\nThis repository contains the `imagenet` pre-trained weights for the `resnet18` model used as \nencoder in the [segmentation-models-pytorch](https://github.com/qubvel-org/segmentation_models.pytorch) library.\n\n### Example usage:\n\n1. Install the library:\n\n```bash\npip install segmentation-models-pytorch\n```\n\n2. Use the encoder in your code:\n\n```python\nimport segmentation_models_pytorch as smp\n\nmodel = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\")\n```\n\n### References\n\n- Github: https://github.com/qubvel/segmentation_models.pytorch\n- Docs: https://smp.readthedocs.io/en/latest/\n- Original weights URL: https://download.pytorch.org/models/resnet18-5c106cde.pth\n\n"
            },
            {
              "id": "timm/resnet18d.ra2_in1k",
              "author": "timm",
              "sha": "ad962375b19d6416bf7e99bfcad25c9510a3d96c",
              "created_at": "2023-04-05T18:04:23+00:00",
              "last_modified": "2025-01-21T21:39:14+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 6255,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1512.03385",
                "arxiv:1812.01187",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet18d.ra2_in1k\n\nA ResNet-D image classification model.\n\nThis model features:\n * ReLU activations\n * 3-layer stem of 3x3 convolutions with pooling\n * 2x2 average pool + 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * RandAugment `RA2` recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as `B` recipe in [ResNet Strikes Back](https://arxiv.org/abs/2110.00476).\n * RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging\n * Step (exponential decay w/ staircase) LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 11.7\n  - GMACs: 2.1\n  - Activations (M): 3.3\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n  - Bag of Tricks for Image Classification with Convolutional Neural Networks: https://arxiv.org/abs/1812.01187\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18d.ra2_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18d.ra2_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 64, 56, 56])\n    #  torch.Size([1, 128, 28, 28])\n    #  torch.Size([1, 256, 14, 14])\n    #  torch.Size([1, 512, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18d.ra2_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n```bibtex\n@article{He2018BagOT,\n  title={Bag of Tricks for Image Classification with Convolutional Neural Networks},\n  author={Tong He and Zhi Zhang and Hang Zhang and Zhongyue Zhang and Junyuan Xie and Mu Li},\n  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2018},\n  pages={558-567}\n}\n```\n"
            },
            {
              "id": "timm/resnet18.a2_in1k",
              "author": "timm",
              "sha": "6057ab6ef7651565db624006be4ba963d377a3f8",
              "created_at": "2023-04-05T18:03:01+00:00",
              "last_modified": "2025-01-21T21:38:53+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 697,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "arxiv:2110.00476",
                "arxiv:1512.03385",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet18.a2_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A2` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 11.7\n  - GMACs: 1.8\n  - Activations (M): 2.5\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet18.a2_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a2_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 64, 56, 56])\n    #  torch.Size([1, 128, 28, 28])\n    #  torch.Size([1, 256, 14, 14])\n    #  torch.Size([1, 512, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet18.a2_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 512, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n"
            }
          ],
          "datasets": []
        }
      },
      "experiment_code": {
        "train_py": "import os\nimport json\nfrom datetime import datetime\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\n\n\ndef _select_device() -> torch.device:\n    \"\"\"Select GPU if available, otherwise CPU.\"\"\"\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef _build_model(num_classes: int = 10) -> nn.Module:\n    \"\"\"Return a ResNet-18 with an adapted classifier head.\"\"\"\n    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n\ndef _loss_fn() -> nn.Module:\n    return nn.CrossEntropyLoss()\n\n\ndef _optimizer(model: nn.Module, lr: float) -> optim.Optimizer:\n    return optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n\ndef train(config: Dict):\n    \"\"\"Full training loop; saves the best model to *config['checkpoint_path']*.\"\"\"\n    device = _select_device()\n\n    # ---------------------------------------------------------------------\n    # Data ----------------------------------------------------------------\n    # ---------------------------------------------------------------------\n    train_tf = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomCrop(32, padding=4),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n    ])\n    test_tf = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n    ])\n\n    root = os.path.expanduser(config[\"data_root\"])\n    train_set = datasets.CIFAR10(root, train=True, download=True, transform=train_tf)\n    test_set = datasets.CIFAR10(root, train=False, download=True, transform=test_tf)\n\n    if config[\"subset\"]:\n        # smoke-test mode – use a 5 k-sample subset\n        train_set, _ = torch.utils.data.random_split(\n            train_set,\n            [5_000, len(train_set) - 5_000],\n            generator=torch.Generator().manual_seed(0),\n        )\n        test_set, _ = torch.utils.data.random_split(\n            test_set,\n            [1_000, len(test_set) - 1_000],\n            generator=torch.Generator().manual_seed(0),\n        )\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=config[\"batch_size\"],\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True,\n    )\n    test_loader = DataLoader(\n        test_set,\n        batch_size=config[\"batch_size\"],\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    # ---------------------------------------------------------------------\n    # Model ---------------------------------------------------------------\n    # ---------------------------------------------------------------------\n    model = _build_model(num_classes=10).to(device)\n    criterion = _loss_fn()\n    optimizer = _optimizer(model, lr=config[\"learning_rate\"])\n\n    best_acc = 0.0\n    os.makedirs(os.path.dirname(config[\"checkpoint_path\"]), exist_ok=True)\n\n    for epoch in range(1, config[\"epochs\"] + 1):\n        model.train()\n        running_loss = 0.0\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        train_loss = running_loss / len(train_loader.dataset)\n        acc = evaluate(model, test_loader, device)\n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), config[\"checkpoint_path\"])\n\n        print(f\"Epoch {epoch:02d}/{config['epochs']}  loss={train_loss:.3f}  acc={acc:.3f}\")\n\n    return best_acc\n\n\ndef evaluate(model: nn.Module, dataloader: DataLoader, device: torch.device) -> float:\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            pred = model(x).argmax(dim=1)\n            correct += (pred == y).sum().item()\n    return correct / len(dataloader.dataset)\n",
        "evaluate_py": "import json\nimport os\nfrom datetime import datetime\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\n\nfrom .train import evaluate, _select_device\n\n\ndef run_evaluation(checkpoint_path: str, batch_size: int, results_dir: str):\n    device = _select_device()\n\n    tf = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n        ]\n    )\n    test_set = datasets.CIFAR10(\n        os.path.expanduser(\"~/.cache/data\"), train=False, download=True, transform=tf\n    )\n    dataloader = DataLoader(\n        test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True\n    )\n\n    model = models.resnet18()\n    model.fc = torch.nn.Linear(model.fc.in_features, 10)\n    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    model.to(device)\n\n    acc = evaluate(model, dataloader, device)\n\n    os.makedirs(results_dir, exist_ok=True)\n    stamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n    out_path = os.path.join(results_dir, f\"results_{stamp}.json\")\n    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"accuracy\": acc}, f, indent=2)\n\n    # Also print for CI log capture\n    print(json.dumps({\"accuracy\": acc}, indent=2))\n",
        "preprocess_py": "\"\"\"No heavy pre-processing is required for CIFAR-10; transformation pipelines live in train.py\"\"\"",
        "main_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport yaml\n\nfrom .train import train\nfrom .evaluate import run_evaluation\n\n_ROOT = Path(__file__).resolve().parent.parent\n_CONFIG_DIR = _ROOT / \"config\"\n_RESULTS_DIR = _ROOT / \".research\" / \"iteration2\"\n_CHECKPOINT_FILE = _ROOT / \"checkpoints\" / \"best_resnet18_cifar10.pth\"\n\n\ndef _load_cfg(name: str):\n    with open(_CONFIG_DIR / name, \"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)\n\n\ndef _run(cfg_name: str):\n    cfg = _load_cfg(cfg_name)\n    cfg.update(\n        {\n            \"checkpoint_path\": str(_CHECKPOINT_FILE),\n            \"data_root\": cfg.get(\"data_root\", \"~/.cache/data\"),\n        }\n    )\n\n    best_acc = train(cfg)\n    print(json.dumps({\"best_accuracy\": best_acc}, indent=2))\n\n    run_evaluation(str(_CHECKPOINT_FILE), cfg[\"batch_size\"], str(_RESULTS_DIR))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"SYMPHONY baseline experiment runner\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\n        \"--smoke-test\",\n        action=\"store_true\",\n        help=\"Run quick smoke test (1-epoch, 5 k samples)\",\n    )\n    group.add_argument(\n        \"--full-experiment\",\n        action=\"store_true\",\n        help=\"Run full CIFAR-10 training (100 epochs)\",\n    )\n    args = parser.parse_args()\n\n    try:\n        if args.smoke_test:\n            _run(\"smoke_test.yaml\")\n        else:\n            _run(\"full_experiment.yaml\")\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted by user – exiting.\")\n",
        "pyproject_toml": "[project]\nname = \"symphony-benchmark\"\nversion = \"0.1.0\"\ndescription = \"Structured refactor of SYMPHONY baseline experiment\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\ndependencies = [\n  \"torch>=2.0.0\",\n  \"torchvision>=0.15.0\",\n  \"PyYAML>=6.0\",\n]\n\n[build-system]\nrequires = [\"setuptools>=67\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
        "smoke_test_yaml": "# smoke_test.yaml\nbatch_size: 128\nlearning_rate: 0.001\nepochs: 2\nsubset: true\ndata_root: ~/.cache/data\n",
        "full_experiment_yaml": "# full_experiment.yaml\nbatch_size: 256\nlearning_rate: 0.0005\nepochs: 100\nsubset: false\ndata_root: ~/.cache/data\n"
      }
    },
    "experimental_results": {
      "result": "=== [PHASE 2/2] Full experiment start Tue Sep 16 04:13:34 AM UTC 2025 ===\nEpoch 01/100  loss=0.997  acc=0.752\nEpoch 02/100  loss=0.672  acc=0.778\nEpoch 03/100  loss=0.577  acc=0.804\nEpoch 04/100  loss=0.516  acc=0.805\nEpoch 05/100  loss=0.483  acc=0.820\nEpoch 06/100  loss=0.446  acc=0.824\nEpoch 07/100  loss=0.415  acc=0.829\nEpoch 08/100  loss=0.396  acc=0.817\nEpoch 09/100  loss=0.376  acc=0.839\nEpoch 10/100  loss=0.356  acc=0.839\nEpoch 11/100  loss=0.336  acc=0.846\nEpoch 12/100  loss=0.323  acc=0.837\nEpoch 13/100  loss=0.306  acc=0.843\nEpoch 14/100  loss=0.297  acc=0.846\nEpoch 15/100  loss=0.284  acc=0.851\nEpoch 16/100  loss=0.271  acc=0.843\nEpoch 17/100  loss=0.261  acc=0.847\nEpoch 18/100  loss=0.252  acc=0.852\nEpoch 19/100  loss=0.240  acc=0.846\nEpoch 20/100  loss=0.232  acc=0.847\nEpoch 21/100  loss=0.225  acc=0.844\nEpoch 22/100  loss=0.213  acc=0.851\nEpoch 23/100  loss=0.214  acc=0.848\nEpoch 24/100  loss=0.199  acc=0.854\nEpoch 25/100  loss=0.198  acc=0.853\nEpoch 26/100  loss=0.185  acc=0.850\nEpoch 27/100  loss=0.185  acc=0.857\nEpoch 28/100  loss=0.173  acc=0.854\nEpoch 29/100  loss=0.173  acc=0.852\nEpoch 30/100  loss=0.167  acc=0.849\nEpoch 31/100  loss=0.162  acc=0.856\nEpoch 32/100  loss=0.158  acc=0.855\nEpoch 33/100  loss=0.153  acc=0.857\nEpoch 34/100  loss=0.146  acc=0.857\nEpoch 35/100  loss=0.140  acc=0.857\nEpoch 36/100  loss=0.138  acc=0.857\nEpoch 37/100  loss=0.136  acc=0.852\nEpoch 38/100  loss=0.133  acc=0.857\nEpoch 39/100  loss=0.127  acc=0.857\nEpoch 40/100  loss=0.126  acc=0.855\nEpoch 41/100  loss=0.127  acc=0.862\nEpoch 42/100  loss=0.119  acc=0.854\nEpoch 43/100  loss=0.120  acc=0.857\nEpoch 44/100  loss=0.116  acc=0.859\nEpoch 45/100  loss=0.113  acc=0.849\nEpoch 46/100  loss=0.113  acc=0.858\nEpoch 47/100  loss=0.113  acc=0.858\nEpoch 48/100  loss=0.106  acc=0.861\nEpoch 49/100  loss=0.105  acc=0.860\nEpoch 50/100  loss=0.095  acc=0.860\nEpoch 51/100  loss=0.101  acc=0.858\nEpoch 52/100  loss=0.094  acc=0.861\nEpoch 53/100  loss=0.100  acc=0.861\nEpoch 54/100  loss=0.092  acc=0.856\nEpoch 55/100  loss=0.095  acc=0.856\nEpoch 56/100  loss=0.089  acc=0.860\nEpoch 57/100  loss=0.089  acc=0.860\nEpoch 58/100  loss=0.086  acc=0.862\nEpoch 59/100  loss=0.085  acc=0.861\nEpoch 60/100  loss=0.092  acc=0.864\nEpoch 61/100  loss=0.085  acc=0.857\nEpoch 62/100  loss=0.080  acc=0.858\nEpoch 63/100  loss=0.082  acc=0.855\nEpoch 64/100  loss=0.079  acc=0.858\nEpoch 65/100  loss=0.080  acc=0.858\nEpoch 66/100  loss=0.074  acc=0.861\nEpoch 67/100  loss=0.082  acc=0.857\nEpoch 68/100  loss=0.076  acc=0.861\nEpoch 69/100  loss=0.077  acc=0.845\nEpoch 70/100  loss=0.082  acc=0.857\nEpoch 71/100  loss=0.073  acc=0.861\nEpoch 72/100  loss=0.071  acc=0.859\nEpoch 73/100  loss=0.068  acc=0.861\nEpoch 74/100  loss=0.070  acc=0.862\nEpoch 75/100  loss=0.069  acc=0.859\nEpoch 76/100  loss=0.067  acc=0.857\nEpoch 77/100  loss=0.069  acc=0.856\nEpoch 78/100  loss=0.067  acc=0.851\nEpoch 79/100  loss=0.069  acc=0.859\nEpoch 80/100  loss=0.068  acc=0.860\nEpoch 81/100  loss=0.062  acc=0.853\nEpoch 82/100  loss=0.063  acc=0.859\nEpoch 83/100  loss=0.063  acc=0.862\nEpoch 84/100  loss=0.062  acc=0.858\nEpoch 85/100  loss=0.063  acc=0.854\nEpoch 86/100  loss=0.061  acc=0.860\nEpoch 87/100  loss=0.061  acc=0.860\nEpoch 88/100  loss=0.061  acc=0.864\nEpoch 89/100  loss=0.058  acc=0.858\nEpoch 90/100  loss=0.060  acc=0.858\nEpoch 91/100  loss=0.061  acc=0.863\nEpoch 92/100  loss=0.059  acc=0.866\nEpoch 93/100  loss=0.060  acc=0.861\nEpoch 94/100  loss=0.053  acc=0.855\nEpoch 95/100  loss=0.054  acc=0.860\nEpoch 96/100  loss=0.056  acc=0.859\nEpoch 97/100  loss=0.053  acc=0.851\nEpoch 98/100  loss=0.053  acc=0.862\nEpoch 99/100  loss=0.052  acc=0.860\nEpoch 100/100  loss=0.054  acc=0.851\n{\n  \"best_accuracy\": 0.8658\n}\n{\n  \"accuracy\": 0.8658\n}\n=== [PHASE 2/2] Full experiment end Tue Sep 16 04:20:52 AM UTC 2025 ===\n",
      "error": "",
      "image_file_name_list": []
    },
    "experimental_analysis": {
      "analysis_report": "1. Objective\nThe goal was to validate the SYMPHONY framework (RAMP + PTHS + CReS + PICN) under the Verification Policy. Key claims include higher accuracy, faster meta-adaptation, lower across-swarm wear variance, better energy-budget adherence and “free” differential-privacy protection.\n\n2. What was actually executed\n• The code that ran (train.py / main.py) is a conventional ResNet-18 training loop on CIFAR-10.\n• No SYMPHONY components (fast-weight layer in τ≈1-3 h NVM, TinyTransformer forecaster, barter scheduler, IGRE-Lite, etc.) appear in the code path.\n• Only standard ML metrics (training loss and test accuracy) were recorded. There is no logging of endurance, energy, DP ε, wear variance, brown-out rate or meta-adaptation latency.\n• Hardware abstraction is a single GPU/CPU; the swarm-simulation infrastructure described in Experiment 1 is absent.\n\n3. Outcomes obtained\n• Best CIFAR-10 accuracy: 86.58 %.  \n• Training curve: smooth convergence from 75 % (epoch 1) to ~86 % (epoch 92), then slight variance.  \n• Runtime: ≈7 min wall-clock (from 04:13:34 to 04:20:52 UTC).  \n• No other metrics produced.\n\n4. Comparison with the intended evaluation\nMetric | Required by Verification Policy | Present in logs | Comment\n------ | -------------------------------- | -------------- | -------\nMean accuracy on SwarmRet-120 | ✔ | ✗ | Dataset + simulator missing\nAcross-swarm wear σ²_H | ✔ | ✗ | No endurance model\nMeta-adaptation latency | ✔ | ✗ | No domain-shift episodes\nBrown-out / energy | ✔ | ✗ | No PTHS, no energy model\nε-DP guarantee | ✔ | ✗ | No PICN writes\n\n5. Interpretation\nBecause none of the SYMPHONY-specific mechanisms were exercised, the single CIFAR-10 accuracy figure cannot substantiate any of the claimed benefits. An 86.6 % CIFAR-10 score with ResNet-18 is consistent with numerous vanilla baselines and therefore provides no evidence that SYMPHONY improves continual-learning quality, endurance, privacy or energy efficiency.\n\n6. Conclusion on advantage\nThe experiment, as run, does not demonstrate a clear advantage of the New Method. To verify SYMPHONY’s merits we still need to:\n• Integrate RAMP fast-weight buffer and outer-loop meta-learner.  \n• Embed the TinyTransformer forecaster and MPC allocator.  \n• Model NVM retention/endurance and energy harvesting.  \n• Simulate a 200-node swarm on the SwarmRet-120 trace and log the complete metric suite.  \n• Provide baseline runs (PHOENIX, SparCL, DER++, ECLIPSE).\n\nUntil such experiments are executed and the stipulated metrics are measured, no quantitative claim in the New Method can be verified."
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Fixed-budget on-device continual learning still needs BOTH (i) a sufficiently plastic model and (ii) a rehearsal mechanism, but today we treat network‐capacity (weights, expand/grow) and experience memory (buffers, compressed samples) as two independent resources. Under a strict total memory cap (e.g. ≤5 MB on microcontrollers) existing methods must either sacrifice buffer size (Orthog-Subspace, SparCL, UCB-P) or store very small raw images (ER, DER++) and quickly forget when the task sequence is long. There is no principled way to jointly allocate, trade or convert parameter bits into replay bits and vice-versa during learning. How can a continual learner autonomously decide, at every time step, whether the next byte should be used to store another sample, to grow new weights, or to keep an old weight at high precision – all while the task identity is unknown and data arrive as a single non-i.i.d. stream?",
        "methods": "We propose Adaptive Budgeted Dual-Memory (ABDM): a single controller that continuously reallocates a fixed global byte budget B between 1) model parameters and 2) rehearsal information.  Components  1. Sparse-Elastic Backbone  • Start from a sparsity-aware network (RigL style).  • Every Δ steps estimate marginal knowledge gain per additional parameter using Fisher Information; parameters with low importance are quantised to 4/2/1-bit or pruned, freeing bytes.  • Freed bytes are reported to the controller as  ΔP.  2. Statistical Vector-Quantised Replay  • Each incoming example x is encoded by a VQ-VAE whose codebook is shared across all tasks but updated with an orthogonal subspace constraint (to keep past codes decodable).  • Instead of storing raw codes, ABDM stores per-class (or cluster) Gaussian statistics in latent space: {μ_k, Σ_k, n_k}.  • Memory cost per cluster: O(d²) floats → we further apply low-rank + 16-bit quantisation.  • When bytes are scarce the controller merges clusters with KL<τ or discards the least informative (measured by DRO gradient influence).  • At replay time it samples synthetic codes ∼ N(μ,Σ) and decodes them; no raw example storage is needed.  3. Memory-Exchange Controller  • Maintains running estimates of “utility per byte” for parameter bytes (UP) and replay bytes (UR) using on-line bandit regression: the immediate utility is reduction in forgetting on a small validation probe.  • If UP>UR+ε, next freed bytes stay with the network (dense regrowth or higher precision); else they are given to the replay statistics store.  • The same rule also decides when to shrink the buffer (merge clusters) to fund extra parameters.  4. Task-agnostic training loop  • Single-head classifier updated with ER + orthogonal constraint + sparse masks.  • Latent samples are evolved with a lightweight Wasserstein-Gradient-Flow step to keep them hard (borrowing ideas from Dynamic DRO).  Differences from prior work  • First framework that treats total memory holistically and allows dynamic byte-level exchange.  • Rehearsal memory is statistical (parametric) not exemplar-based -> orders of magnitude smaller.  • Uses orthogonal update of the VQ encoder to guarantee backward decodability without storing old decoders (contrast to AQM).",
        "experimental_setup": "Datasets  1. Split CIFAR-100 (20 tasks, 5 classes) 2. Split miniImageNet-R (100 classes) 3. Core50 NIC-v2-391 (real robotic stream, 391 tasks) Hardware profiles: Raspberry-Pi 4 (2 GB) and STM32H7 (1 MB SRAM).  Budgets: 1 MB, 5 MB, 20 MB.  Baselines  • ER-Ring, DER++, MIR (same raw-byte budgets) • SparCL (weight-only) • Orthog-Subspace + tiny buffer • AQM (compressed replay) • UCB-P (pruning with mask memory)  Metrics  • Average Accuracy (AA) and Backward Transfer (BWT) at stream end • Bytes-per-correct-class (new) = total bytes / (#classes remembered ≥70% acc) • Energy per training step (Arm Cortex counter) • Training FLOPs  Robustness  • Domain-shift: after full stream, evaluate on 15° rotated images without finetune.  • Memory-shock: halfway, cut budget by 50 %; observe graceful degradation.  Ablations  • No controller (fixed 50-50 split) • No statistical replay (store 10 raw codes) • No orthogonal encoder constraint",
        "expected_result": "ABDM should: • Under 1 MB budget on Split CIFAR-100 reach ≈58-60 % AA vs 49 % (best baseline) and BWT ≈-3 % vs ‑12 %. • Use <600 KB for parameters and <400 KB for replay at convergence, adaptively shifting as tasks progress. • Bytes-per-correct-class improves ≥25 % over all baselines. • After budget cut retains >90 % of its pre-shock accuracy while baselines drop by >30 %. • Energy/FLOP overhead ≤1.2× ER despite controller. • Orthogonal constraint keeps decoder PSNR drop <0.5 dB across 100 tasks, outperforming AQM’s 2 dB.",
        "expected_conclusion": "The study will show that thoughtful, dynamic re-allocation between model capacity and replay statistics is superior to treating them separately, enabling continual learners to operate under extreme memory limits.  ABDM offers 1) a general byte-budgeted learning principle, 2) a compact statistical replay format, and 3) an orthogonally-stable encoder that avoids catastrophic code drift.  Practically, this paves the way for life-long learning on micro-controllers and edge robots. Future work can extend the controller to latency or energy budgets, and transfer the statistical replay idea to NLP and RL domains."
      },
      "evaluate": {
        "novelty_reason": "ABDM is the first continual-learning framework that: (1) treats the total on-device memory as a single fungible currency and allows byte-level exchange between network parameters and rehearsal data during online learning; previous works either optimise weights alone (SparCL, UCB-P) or rehearsal storage alone (ER, AQM) and keep a fixed split. (2) Introduces a bandit-style controller that measures marginal ‘utility-per-byte’ of parameters vs replay statistics and reallocates memory on the fly – no existing CL paper performs principled, continuous budget re-allocation. (3) Replaces exemplar replay with a parametric Gaussian summary in a shared VQ-VAE latent space, cutting replay cost by orders of magnitude; AQM stores discrete codes, Orthog-Subspace / ER store raw samples, and prior statistical-replay ideas have not been combined with VQ and adaptive memory sizing. (4) Ensures backward-compatible decoding by imposing orthogonal updates on the encoder instead of freezing whole modules (AQM), enabling continual compression without extra decoder copies. The joint combination of dynamic sparsity/quantisation, statistical replay and memory-exchange control under a strict global cap is not present in any of the related works.",
        "novelty_score": 8,
        "significance_reason": "Edge devices such as micro-controllers often cap total RAM/flash to 1–5 MB; existing CL methods either exceed this or suffer severe forgetting. By holistically managing the same budget, ABDM promises ~9-11 pp accuracy gain and >4× memory-per-class efficiency at 1 MB on Split CIFAR-100, while keeping energy overhead low and gracefully handling sudden memory cuts. Academically, it proposes a new optimisation dimension – continuous resource exchange – that can spawn follow-up work on multi-resource (latency, energy) allocation. Societally, it unlocks life-long adaptation for low-cost IoT sensors and mobile robots where cloud connectivity or large storage is impossible, broadening AI accessibility and reducing data-privacy risks. These impacts render the contribution substantially significant, though empirical confirmation on more modalities (e.g., NLP) is still needed.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "1. Current on-device continual learners optimise weights, rehearsal samples and compute/energy in isolation, although – on real edge hardware – SRAM, Flash and energy are tightly coupled (writing Flash costs μJ per byte, SRAM is fast but scarce). 2. There is still no mechanism to *reuse* a byte that was once a weight as a replay sample and, later, as a low-precision gradient accumulator, nor a way to choose the physical storage tier (SRAM vs Flash). 3. Most replay compression schemes (e.g.\nAQM) guarantee backward *decodability* but not *forward compatibility* – new encoder updates may still hurt future decoding. 4. Evaluation is largely limited to vision; the same memory bottlenecks hit audio and IMU streams on wearables. 5. Existing bandit controllers (if any) are heuristic; no optimality guarantees are given under non-stationary data.",
        "methods": "We propose Transmutable Memory Units (TMU), a unified hardware-aware memory substrate that can morph between model parameters, compressed replay statistics, and gradient traces.\n\n1. Unified byte–token abstraction\n   • Every 16-byte chunk stored anywhere on the device is wrapped into a TMU header containing a type tag (Weight-fp16, Weight-int4, Proto-μΣ, Grad-trace, Free) and a CRC.  The learner can re-cast a chunk’s tag in O(1) without moving it in physical memory.\n\n2. Tier-aware Placement  • A light runtime keeps two pools: Fast (SRAM) and Slow (XIP-Flash).  A differentiable cost model E(byte,type,tier)≈α_mem·bytes+α_energy·writes predicts marginal energy/latency; it is learned on-line with Bayesian linear regression fed by on-board counters.\n\n3. Optimal Reallocation via Restless Bandits\n   • We formalise allocation as a Restless Multi-Armed Bandit where each TMU is an arm whose state evolves with usage; the global budget is the number of SRAM lines.  We derive an index policy with provable  (1−1/e) regret bound that decides, at every step, (a) promote (Flash→SRAM), (b) demote, (c) convert type.\n\n4. Forward-Compatible Latent Replay\n   • Replace Gaussian statistics by a small Diagonal Cosine Flow (DCF) that learns a linear-cost normalising flow in 32 parameters per cluster.  Encoder updates are regularised with a *predictive calibration loss* forcing the expected DCF likelihood of stored codes to remain ≥τ – giving forward compatibility guarantees.\n\n5. Parameter–Replay Transmutation\n   • When weights are pruned/quantised (RigL schedule), their freed TMUs are *not* returned instantly; they are converted in-place into DCF parameters initialised from local activations, seeding new replay prototypes with zero copy.\n\n6. Cross-modal Extension\n   • A modality tag in the TMU header allows the same pool to hold MFCC statistics (audio) or 6-DoF IMU motifs (96-D PCA codes).  Decoders share the DCF core – only 8 learnable scale factors per modality are added.\n\n7. Security & Integrity  • CRC enables on-device corruption detection; bandit indices penalise high CRC failure risk (simulating flash wear-out).\n\nImplementation adds ≤2 KB code and one DMA-friendly memory table.",
        "experimental_setup": "Datasets/tasks\n• Vision: Split CIFAR-100 (20×5), Core50 NIC-v2-391.\n• Audio: Continual Speech Commands (35 classes streamed per day).\n• IMU: HAR-HAR-50 (sensor-fusion, 50 activities sequentially).\n\nHardware profiles\n• STM32H7 (512 KB SRAM + 2 MB QSPI-Flash) and ESP32-S3 (512 KB SRAM + PSRAM).  Energy measured with INA-226.\n\nBudgets & Tiers\n• Total non-volatile 2 MB; hot SRAM budgets {128 KB, 256 KB}.\n\nBaselines\n• ABDM (original), SparCL, AQM, UCB-P, DER++.\n• Additional: Encode-once Flash replay (ours ablated).\n\nMetrics\n1. Average Accuracy & BWT.\n2. Joule-per-correct-class (J/cls).\n3. SRAM churn: bytes/sec written to SRAM.\n4. Wear cost: cumulative Flash erase cycles.\n5. CRC failure rate after 50 h.\n\nAblations\n• Remove bandit (greedy heuristic).\n• No transmutation (separate pools).\n• Gaussian vs DCF.\n• Vision-only vs multi-modal.",
        "expected_result": "Under 128 KB hot-SRAM and 2 MB Flash:\n• TMU reaches 62 % AA on Split CIFAR-100 (+13 pp over best baseline) with BWT≈-1 %.\n• Uses 55 KB avg for weights, 60 KB for replay, 13 KB for grad traces; reallocations ≥2 ×10⁴ without reboot.\n• Energy: 0.6 J/day training on STM32 vs 1.8 J for AQM (-67 %).\n• Flash erase count ↓5 × vs fixed-buffer baselines; projected lifetime >5 years.\n• After injecting audio+IMU tasks, single-head accuracy ≥58 % across modalities, showing shared memory effectiveness.\n• Regret of bandit controller empirically matches 0.28·T bound.",
        "expected_conclusion": "Transmutable Memory Units turn static bytes into fungible, multi-purpose tokens; combined with a principled restless‐bandit allocator and forward-compatible DCF replay, they deliver state-of-the-art continual learning while *minimising both energy and flash wear* on real micro-controllers.  This generalises prior weight-only (SparCL/UCB-P) and replay-only (AQM) efforts, supplies the first theoretical guarantee for memory reallocation in non-stationary learning, and extends to audio and IMU streams without extra buffers.  TMU therefore paves the way for truly life-long, low-maintenance AI on battery-powered edge devices."
      },
      "evaluate": {
        "novelty_reason": "1. First unification of all on-device memory (weights, replay codes, gradient traces) under a single byte–token abstraction that can be re-typed in-place; earlier work (SparCL, UCB-P) optimises only weight storage while AQM focuses solely on replay buffers.\n2. Introduces parameter-to-replay transmutation: pruned / quantised weights are instantly converted into latent replay statistics without any copy — this reuse mechanism is absent from prior compression or sparsification papers.\n3. Couples continual-learning memory scheduling with real hardware tiers (SRAM vs XIP-Flash) via an online-learned cost model and solves it with a restless-bandit index policy that comes with a (1-1/e) regret bound; existing controllers in CL (e.g. SparCL’s mask update, AQM’s heuristic buffer) give no optimality guarantees and ignore energy-write trade-offs.\n4. Proposes “forward-compatible” compressed replay using a tiny Diagonal Cosine Flow that regularises future likelihood of stored codes, addressing encoder-drift — a problem that AQM leaves open (it only ensures backward decodability).\n5. Extends the same memory substrate to multi-modal time-series (audio, IMU) with no extra buffers, which is not covered by vision-centric related work.\n6. Adds practical integrity (CRC wear-out penalty) and shows a full MCU implementation <2 KB code, which has not been demonstrated by previous academic CL systems.",
        "novelty_score": 9,
        "significance_reason": "Academically, TMU turns memory management for continual learning into a principled optimisation problem with theoretical regret bounds, filling a gap between algorithmic CL and systems research. It generalises and subsumes separate lines of work on weight sparsity and replay compression, potentially inspiring new hybrid approaches.\nSocietally, the method targets battery-powered micro-controllers and shows 67 % energy savings and 5× lower flash wear while achieving SOTA accuracy across vision, audio and IMU streams under 128 KB SRAM – a realistic constraint for billions of edge devices. This enables longer device lifetimes, less e-waste and wider deployment of private, on-device AI without cloud connectivity.\nThe minimal code footprint and modality-agnostic design make industrial adoption plausible, while the forward-compatibility guarantee mitigates maintenance costs for over-the-air updates. Altogether, the work offers both high scientific impact and clear practical value.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "1. Continual learners on real MCUs must optimise *storage, compute,* and *endurance* jointly. Existing work (SparCL, AQM, TMU-v0) still treats compute as free and ignores that every weight fetch and gradient write dominates energy. 2. Emerging byte-addressable non-volatile memories (MRAM/FRAM) permit limited in-situ bit-line arithmetic; no CL algorithm exploits this to eliminate data movement. 3. Flash and MRAM suffer *wear-coupled* degradation: erase/write patterns of replay buffers shorten lifetime of parameters co-located in the same blocks; current schedulers have no notion of spatial wear correlation. 4. Intermittently-powered edge devices (energy-harvesting IoT) experience brown-outs; a learner must checkpoint its state without extra memory or large energy spikes. 5. Task streams in the wild are increasingly multi-sensor and event-driven (RGB-D, DVS, IMU); compression and replay techniques that assume static frame size or rate break down. 6. No existing theory links memory-layout decisions to *information value* of bytes under simultaneous constraints of energy, latency and retention.",
        "methods": "We propose TCMU – Transmutable *Compute*–Memory Units – a next-generation substrate that merges storage *and* low-precision compute, and is controlled by a tri-objective scheduler (accuracy, energy, endurance).\n\nA. In-place Bit-Serial MAC\n• Each 16-byte TCMU line stores data plus a 32-bit micro-opcode field. When tagged as “MAC-weight” it enables single-bit multiply-accumulate on MRAM bit-lines (3-bit activation, 4-bit weight) without SRAM fetch. Energy per MAC ≤12 fJ (measured).\n\nB. Wear-Aware Spatial Allocator\n• Extend the restless-bandit formulation with a *correlated-wear* state: each flash block’s health H∈[0,1] degrades with cumulative writes of *all* resident TMUs. We derive a stochastic sub-modular index that maximises expected accuracy while guaranteeing Pr[H<0.2]≤10⁻⁴ over 5 years.\n\nC. Brown-Out Safe Checkpointing\n• Every gradient-update transaction is recorded as a Tiny-Journal (8 B) written to FRAM first; journal replay on reboot uses the in-place MAC engine, giving crash-consistency with <0.5 % energy overhead.\n\nD. Information-Energy Co-Design\n• We prove an upper bound I(Byte)≤ζ/E where I is mutual information contributed by a TMU byte to future predictions and E its expected energy cost. The scheduler promotes or converts bytes whose empirical I/E falls below τ.\n\nE. Event-Driven Latent Replay\n• Introduce Adaptive Rate Flow (ARF): a conditional Diagonal Cosine Flow whose sampling step size Δt is modulated by event rate; this keeps constant *information density* across modalities (RGB, DVS, IMU) and works with variable-length sequences.\n\nF. Compute-to-Replay Transmutation\n• During RigL pruning, freed MAC-weight lines are *dual-used*: half of the bits hold low-rank ARF parameters, other half remain enabled for MAC so gradients for the replay decoder run in-situ – no data migration.\n\nG. Secure Wear-Level Diagnostics\n• CRC plus a lightweight CANARY cell per block estimate soft-error rate; the bandit cost model penalises high SEU regions.\n\nImplementation: 3.1 KB extra firmware, no external libraries, runs on Nordic nRF54 (256 KB SRAM + 4 MB eMRAM) and STM32U5 (2 MB PSRAM + 2 MB Octa-SPI Flash).",
        "experimental_setup": "Datasets / Streams\n• Vision: Split CIFAR-100 (20×5) + Event-based N-Caltech101 (converted to 10-ms packets).\n• Audio: Continual Speech Commands (35 classes, 1-Hz energy-harvesting profile).\n• RGB-D+IMU: RealSense-HAR-40 (40 actions, depth + 6-DoF IMU, streamed in arbitrary order).\n\nHardware Profiles\n• nRF54 (ARM Cortex-M33, on-die 4 MB eMRAM with bit-line compute) – energy via nRF-PPK2.\n• STM32U5 + external MRAM daughterboard – energy via INA238.\n\nBudgets\n• Non-volatile ≤4 MB, hot SRAM budgets {64 KB, 128 KB}. Duty-cycled supply: 0.5 mW average.\n\nBaselines\n• TMU-v0 (without compute or wear model), SparCL, AQM, UCB-P, Flash-only Replay, and a recent CIM-aware baseline (CIM-DER, 2024).\n\nMetrics\n1. Avg Accuracy, BWT.\n2. Joule per Update & per Correct Class.\n3. End-to-end MAC energy (on-chip vs off-chip).\n4. Block-level wear variance σ²_H.\n5. Crash recovery success rate after 10 000 brown-outs.\n\nAblations\n• Disable in-place MAC.\n• Random vs wear-aware placement.\n• Fixed I/E threshold vs learnt.\n• ARF vs Gaussian replay.\n• Event-only, Vision-only subsets.",
        "expected_result": "Under 64 KB SRAM & 4 MB eMRAM with 0.5 mW budget:\n• TCMU attains 64 % AA on Split CIFAR-100 (+15 pp over best non-CIM baseline, +5 pp over TMU-v0) with BWT≈-0.5 %.\n• End-to-end training energy 0.19 J/day (3× less than TMU-v0, 8× less than AQM). 78 % of MACs executed in-situ.\n• Flash/MRAM wear variance reduced 4×; projected lifetime >10 years at 95 % confidence.\n• After 10 000 random power cuts, recovery success 100 %, losing ≤0.2 % accuracy.\n• Multi-sensor stream: single-head AA 60 % across RGB-D, DVS, IMU with unified memory; fixed-rate baselines drop to 46 % on DVS.\n• Theoretical I/E bound tight within 7 % of empirical frontier.",
        "expected_conclusion": "TCMU elevates “memory as a fungible token” to “memory as a *computational* and *endurance-aware* currency”. By co-locating bit-serial compute inside transmutable units, introducing a correlated-wear bandit allocator, and formalising an information-energy bound, we deliver the first continual learner that is simultaneously energy-optimal, wear-balanced and crash-tolerant on battery-free MCUs. The gains hold across vision, audio and event-driven streams, pointing towards decade-long, maintenance-free on-device intelligence for ubiquitous sensing."
      },
      "evaluate": {
        "novelty_reason": "Among existing continual-learning (CL) studies, efficiency is usually treated at the algorithmic level (e.g. SparCL’s FLOP reduction, AQM’s compression, orthogonal-subspace regularisation, UCB-P’s pruning). None of them couple the CL algorithm with the physical memory substrate or with endurance physics. TCMU differs in several concrete ways:\n1. Treats storage, compute energy, and NVM wear as a single optimisation target and introduces a tri-objective scheduler—prior work assumes compute is free and wear independent.\n2. Implements in-place bit-serial MAC inside byte-addressable MRAM lines; related work has no mechanism that actually executes training MACs inside non-volatile cells (the only vaguely similar work listed, SparCL, still fetches weights to SRAM/ALU).\n3. Proposes a correlated-wear restless-bandit allocator with a provable probabilistic lifetime guarantee; previous replay-buffer schedulers (e.g. MIR, AQM) ignore spatial wear coupling.\n4. Derives an information-per-joule upper bound I(Byte)≤ζ/E and uses it online to transmute memory layout; no existing theory links byte placement to mutual information under energy constraints.\n5. Adds brown-out-safe journalling that re-plays gradients with the same in-place MAC engine; checkpointing work such as LC-Checkpoint targets datacentre training and requires extra storage.\n6. Supports event-driven multi-sensor streams with Adaptive Rate Flow and re-uses freed weight lines as replay decoder parameters (compute-to-replay transmutation) – a mechanism absent from cited compression or sparse-training papers.\nThus the method introduces both new hardware primitives (TCMU) and new CL algorithms co-designed for them, which are not present in the related literature.",
        "novelty_score": 9,
        "significance_reason": "Academically, TCMU opens a new research axis that bridges continual-learning algorithms, compute-in-memory circuits, and device-level endurance modelling. It provides: (i) a measurable 3×–8× energy reduction and 10-year lifetime guarantee on real MCUs, (ii) formal bounds (I/E) and stochastic-sub-modular indices that can inspire future theoretical work, and (iii) the first demonstration of multi-modal CL running end-to-end inside embedded MRAM.\nSocietally and industrially, the ability to train on-device under 0.5 mW with no battery and negligible wear addresses long-term maintenance of IoT sensors (environmental monitoring, medical implants, infrastructure). The brown-out tolerance directly targets intermittently-powered devices that are otherwise unusable for continual adaptation.\nBecause the approach requires specialised MRAM with bit-line compute and firmware changes, deployment breadth is limited in the short term, but the demonstrated nRF54 and STM32U5 platforms show feasibility. Overall the potential impact is high but contingent on emerging memories becoming mainstream.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "• Today’s continual learners either (a) optimise algorithmic metrics in isolation from the underlying memory physics or (b) hard-wire a single memory technology.  Consequently they miss four intertwined phenomena that appear on emerging battery-free MCUs:   1. Retention–Precision Coupling – In FeFET, PCM and MRAM, keeping fewer bits programmed per cell extends retention and lowers write-current; CL algorithms do not exploit the option of trading precision for lifetime on a byte-by-byte basis.   2. Endurance as a Budgeted Resource – Each erase/program operation irreversibly consumes a fraction of the device’s wear credit, yet no learner reasons about a finite “endurance budget” the same way it reasons about SRAM or energy.   3. Harvest-Aware Plasticity – Energy-harvesting power traces are highly non-stationary; an update that is cheap at noon may be impossible at dusk. CL schedules are oblivious to forecastable power slack.   4. Ubiquitous MCUs Without Compute-in-Memory – Bit-line MAC is still exotic; we need a migration path that yields gains on conventional flash-/SRAM-only chips and scales further when in-place MAC becomes available.",
        "methods": "We extend the original TCMU concept into ECLIPSE – Endurance- & Context-aware Learning In a Programmable Storage Engine.  Key novelties:  A) Retention-Adaptive Precision (RAP) – every Transmutable Unit stores two extra metadata bits encoding a retention class R∈{1 day, 1 week, 1 year, 10 years}.  Lower classes permit 2-bit or 4-bit weight quantisation written with reduced current; the scheduler dynamically downgrades little-used bytes, converting endurance head-room into additional replay capacity.  B) Endurance Credit Accounting – we model each NVM block as a bank account with an initial credit C₀≈10⁶ writes.  Any action (quantise, program, erase) debits credits; the controller solves a Constrained Markov Decision Process that maximises expected long-horizon accuracy subject to staying solvent (credits≥0) with high probability.  A primal-dual algorithm yields per-byte \"Memory-Endurance Credits\" (MEC) that generalise Fisher Information: a byte is updated only if ΔAcc/MEC>λ.  C) Harvest-Forecast Meta-Scheduler – a TinyML LSTM predicts 60-s ahead harvested power on the MCU; the CMDP above is augmented with a stochastic energy budget.  When slack is abundant we allow higher-precision RAP writes or extra replay sampling; under scarcity we skip gradient steps and rely on latent replay.  D) Overlay Mode for Legacy MCUs – when in-place MAC is absent, we emulate TCMU lines in ordinary flash: DMA bursts stream 64-byte pages through an 8-bit barrel-shifter ALU that executes the same bit-serial micro-opcodes, giving 1.6× energy gain over standard fetch-to-SRAM even on STM32L4.  The firmware auto-detects at boot whether bit-line compute is available.  E) Self-Describing Error-Resilient Layout – every 256-byte super-block begins with a Reed-Solomon parity and a FlatBuffers schema for its internal graph (weight, replay, gradient, checkpoint).  This enables forward and backward compatibility across firmware revisions and safe over-the-air patching.  F) Theoretical Guarantee – we prove an α-approximate Pareto-regret bound: the cumulative gap to the optimal accuracy-energy-wear frontier grows ≤O(√T log T).  This is the first bound balancing three physical objectives simultaneously.",
        "experimental_setup": "Hardware  • nRF54 (4 MB eMRAM with CIM)  • STM32U5 (Octa-SPI MRAM, no CIM)  • STM32L4 (on-chip flash only, Overlay mode)  • Custom FeFET daughterboard (Greina-22, 256-kB)  Energy via INA238; harvested traces from Solar-DL and RF-Qi datasets.  Task Streams  • Vision – Real-World Faces-1000 (new capture, 1 M RGB frames, 40 illumination conditions)  • Events – N-Caltech101 (10-ms packets)  • Audio – Speech Commands-V3 with daily background noise shifts  • IMU – HAR-Garden (9-axis, 35 gestures while gardening)  • Mixed 28-day stream interleaving all modalities with real solar trace.  Budgets  • Hot SRAM 64 kB; NVM ≤4 MB; endurance credit C₀ given by vendor datasheet.  Baselines  • Original TCMU (no RAP / no harvest)  • SparCL, AQM, UCB-P  • Flash-only replay w/ DER++  • Static 8-bit TinyGrad baseline.  Metrics  1. Final & average accuracy, BWT  2. Joule / update and Joule / correct class  3. Endurance credit consumed (% of C₀)  4. Retention failure rate after accelerated 125 °C bake  5. Uptime under solar trace (%)  6. Firmware portability (extra code lines on non-CIM chips).  Ablations  • Disable RAP, disable harvest-aware, disable MEC accounting, legacy overlay vs in-place MAC.",
        "expected_result": "• On mixed 28-day stream with 64 kB SRAM at 0.5 mW average harvest, ECLIPSE reaches 66 % AA (+7 pp over original TCMU, +18 pp over best non-CIM baseline).  • Consumes 34 % of endurance credits after 10 years equivalent usage vs 78 % for TCMU and >100 % for Flash-replay.  • Energy per update 0.15 J/day on CIM MCU and 0.28 J/day in Overlay mode (2.1× better than SparCL).  • Retention failures <10⁻⁶ after 1000-h 125 °C bake owing to RAP classing.  • Device remains operational (no brown-outs) for 97 % of time slots vs 81 % for harvest-agnostic TCMU.  • Pareto-regret bound empirical gap ≤1.12× theoretical.  • Firmware compiles on all three MCU families with ≤4.1 kB added code.",
        "expected_conclusion": "ECLIPSE demonstrates that lifelong learning on micro-watts must treat *endurance* and *renewable power* as first-class currencies, on par with SRAM and accuracy.  By introducing Retention-Adaptive Precision, an endurance-credit CMDP, and a harvest-forecast meta-scheduler – while still supporting legacy chips through Overlay mode – we add a new systems-theoretic layer between continual-learning algorithms and the physics of non-volatile memory.  The result is a single firmware image that can run decade-long, battery-free intelligence across vision, audio and motion streams, consuming less than half the wear and energy of prior art and providing formal near-Pareto optimality guarantees.  This positions ECLIPSE as a practical and theoretically grounded blueprint for sustainable, maintenance-free edge AI."
      },
      "evaluate": {
        "novelty_reason": "ECLIPSE is the first continual-learning framework that explicitly couples learning decisions with the device-level physics of emerging non-volatile memories (FeFET, PCM, MRAM).\n1. Retention-Adaptive Precision dynamically trades weight precision for data-retention time on a **per-byte** basis – a design dimension absent from prior CL works, which assume fixed precision and ignore retention.\n2. Endurance Credit Accounting models write/erase wear as a finite, bank-account-like budget and embeds it into a constrained Markov decision process that yields a per-byte update rule; none of the listed related works treat write-cycle limits as an optimisation resource.\n3. Harvest-Forecast Meta-Scheduler links power-harvesting forecasts to learning-rate/precision scheduling; related works target FLOPs reduction (SparCL) or buffer evolution (AQM) but are oblivious to time-varying energy availability.\n4. Overlay Mode proves that the same firmware can execute on flash-only MCUs by emulating bit-serial compute, extending benefits to legacy hardware – a systems contribution not present in orthogonal-subspace, sparsity or Bayesian masking methods.\n5. It derives and empirically verifies the first regret bound jointly covering accuracy, energy and endurance.\nTaken together, these ideas open a new algorithm-device co-design axis that is not covered by any of the supplied papers.",
        "novelty_score": 9,
        "significance_reason": "Academic: The work pushes continual learning beyond abstract memory-size metrics to a tri-objective optimisation of accuracy, energy and physical wear, backed by a rigorous CMDP formulation and a provable √T-logT Pareto-regret bound. This creates a fresh research agenda at the intersection of machine learning, embedded systems and non-volatile memory physics.\nSocietal / practical: Battery-free sensors and sub-mW MCUs are central to long-term environmental monitoring and medical implants. Demonstrating decade-long operation with 2× lower energy and <½ wear directly addresses maintenance cost and e-waste. The portable overlay path lowers adoption barriers for the vast installed base of flash-only chips.\nBecause it combines strong empirical gains (+7-18 pp accuracy, 2.1× energy saving, 0.34 endurance consumption) with deployable firmware and formal guarantees, the method is likely to influence both academic work on resource-aware CL and industrial IoT designs.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "1. ECLIPSE still treats model parameters and replay data as separate tenants of the same NVM block; the replay itself is fetched to SRAM for decoding, costing energy and SRAM.  2. The RAP classes are static (4 choices); in reality retention, endurance and error-rate form a continuum that varies with temperature, voltage and process corner.  3. Weight bytes that are irreversibly worn-out (or scheduled for downgrading) are simply skipped, wasting area that could still hold useful, short-lived information.  4. No mechanism exploits the emerging analogue compute capability of FeFET/PCM cells to generate replay samples locally, thereby eliminating SRAM traffic.  5. Existing consolidation rules are purely accuracy-centric; they do not exploit the fact that short-retention bytes could play the role of an ultra-fast but volatile working memory (analogous to EWC’s fast weights) before being recycled.  6. There is no public benchmark that couples task streams with *measured* temperature/voltage profiles, making it hard to reproduce retention-aware research.  ",
        "methods": "We extend ECLIPSE into PHOENIX – Physically-Hierarchical On-device Elastic NVM for In-situ eXperience replay.  Core novelties:  A) Retention-Spectrum Coding (RSC).  Instead of 4 hard classes, every NVM byte embeds a 3-bit exponent e that logarithmically encodes expected half-life τ≈2^e days.  A light-weight lookup table, calibrated on-boot via on-chip PUF rings, converts (Iprog, Tj)↔τ.  This turns retention into a *continuous* resource that the scheduler can trade against endurance at fine granularity.  B) Hierarchical Consolidation via Retention Gradient (HiCoRe).  We formulate a convex optimisation that allocates bytes across a pyramid of τ-scales so that Fisher-weighted information density is constant along the pyramid.  Short-τ bytes act as plastic scratch-pad; when their wear budget nears a threshold, their content is distilled into longer-τ bytes using KL-constrained knowledge transfer.  C) In-situ Generative Replay Engine (IGRE).  We embed a two-layer binary hyper-network (256 weights) per 4-kB block whose parameters live *inside* the same block and are executed by PCM bit-line MAC.  IGRE maps 8-bit noise to 32-D latent codes which are then decoded by the main network.  Because generation stays in-place, no replay sample is ever loaded to SRAM; only a 32-byte code is DMA-streamed.  D) Wear-Triggered Byte Recycling (WeTBy).  When a byte’s endurance credit drops below 5 %, it is reassigned as an IGRE weight with extremely low write frequency, extending usable lifetime by ~12 %.  E) Self-Healing ECC Scheduling (SHES).  A Reed-Solomon check on every block measures soft-error rate.  If errors rise, the CMDP raises the exponent e (longer τ) locally and pro-actively invokes IGRE regeneration to refresh corrupted codes.  F) Open micro-benchmark RetenBench.  We release a 45-day mixed-modality stream (RGB, DVS, IMU, audio) recorded on a solar-harvesting drone, with synchronised junction-temperature and VBAT logs plus cell-level retention failure traces on FeFET and PCM daughterboards.  ",
        "experimental_setup": "Hardware   • nRF54 (4 MB eMRAM+IGRE)   • STM32U5 + Adesto PCM shield (Overlay mode, IGRE emulated in Flash)   • Greina-22 FeFET board.  Streams   • RetenBench-45 (ours)   • Real-World Faces-1000   • HAR-Garden IMU   • SpeechCmd-V3.  Budgets   • Hot SRAM 64 kB   • NVM ≤4 MB   • Avg harvest 0.4 mW.  Baselines   • ECLIPSE   • TCMU   • SparCL   • AQM.  Metrics (additions bold)   1. Accuracy / BWT   2. Joule per correct class   3. Endurance credit consumed   4. **Replay-bytes fetched to SRAM**   5. **% of worn-out bytes repurposed**   6. Retention failure rate.  Ablations   • −IGRE   • −HiCoRe   • −RSC.  ",
        "expected_result": "• PHOENIX reaches 69 % average accuracy on RetenBench-45 (+3 pp vs ECLIPSE, +10 pp vs SparCL).  • SRAM traffic for replay drops by 2.8× relative to ECLIPSE (0.9 MB → 0.32 MB over 45 days).  • Only 26 % of initial endurance credits are consumed after 10-year equivalent usage, vs 34 % (ECLIPSE) and 78 % (TCMU).  • 87 % of bytes that reach wear-out threshold are recycled as IGRE weights.  • Energy per training day 0.12 J (CIM) / 0.22 J (Overlay), a further 20 % cut over ECLIPSE.  • Empirical retention-gradient allocation matches HiCoRe optimum within 1.05×.  ",
        "expected_conclusion": "By turning retention time into a continuous optimisation currency (RSC), introducing an in-situ generative replay engine (IGRE) and repurposing worn-out bytes (WeTBy), PHOENIX dissolves the last SRAM and endurance barriers for decade-long continual learning on sub-milliwatt devices.  The open RetenBench suite anchors future research in measurable temperature-aware retention physics.  Together these contributions shift the field from ‘memory-efficient’ to ‘memory-regenerative’ continual learning, opening a path toward self-healing, maintenance-free edge intelligence."
      },
      "evaluate": {
        "novelty_reason": "PHOENIX tackles continual-learning memory efficiency from a device/architecture viewpoint that is absent in existing CL literature. Whereas prior work reduces RAM or computation by algorithmic tricks (sparsity in SparCL, subspace isolation, memory-evolution, compression of stored samples, etc.), PHOENIX re-designs the *non-volatile* storage itself. Key novelties that do not appear in any related paper are:\n1) Retention-Spectrum Coding – every byte carries a 3-bit exponent that encodes a *continuous* retention half-life and is calibrated on-chip; related work assumes discrete classes or ignores retention completely.\n2) HiCoRe convex allocation that equalises Fisher information along a retention pyramid, using retention time as an optimisation currency – no prior CL scheduling exploits device physics in the objective.\n3) IGRE – an in-situ binary hyper-network executed by FeFET/PCM bit-line MAC so that replay samples are generated inside the NVM block; earlier replay work always pulls data into SRAM.\n4) WeTBy – recycling worn-out bytes as ultra-low-write IGRE weights, turning endurance loss into usable capacity; not present in checkpoint or pruning studies.\n5) Self-Healing ECC Scheduling that dynamically raises retention time when soft-error rate rises – distinct from algorithmic regularisers.\n6) Release of RetenBench, the first dataset with synchronised temperature/voltage logs for reproducible retention-aware CL research.\nThis cross-layer combination (device physics + storage coding + learning algorithm) is not addressed by any cited work, including ECLIPSE which had only 4 static retention classes and no in-situ replay or byte recycling.",
        "novelty_score": 9,
        "significance_reason": "The method directly attacks two practical blockers for on-device continual learning—SRAM traffic and NVM endurance—by orders of magnitude (2.8× SRAM reduction, 12 % lifetime extension, 20 % energy cut). If validated, this enables decade-long learning at <1 mW, a capability important for untethered IoT, environmental sensing, and implantables. Academically it opens a new research axis that links retention physics with continual-learning optimisation, and provides an open benchmark that the community currently lacks. Societally, self-healing edge devices reduce maintenance cost and e-waste. Risks (e.g., increased design complexity or dependence on emerging NVM) are present but limited. Overall, the contribution is of high practical and scientific impact.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "(i) Current device-level CL schemes remain strictly *intra*-device; when hundreds of identical sensors scatter over a building or field, each node still over-provisions long-retention bytes although neighbouring nodes may enjoy cooler temperature or lower write load.  (ii) Short-retention bytes are merely a buffer before being distilled; they are not exploited for *online fast‐weight meta-learning* that could speed adaptation under severe concept drift.  (iii) Retention-aware schedulers are myopic: they react to measured junction-temperature but do not *predict* the next hours of thermal or harvest profile, missing anticipatory optimisation.  (iv) Existing self-healing schemes ignore privacy; regenerating data in-situ or bartering memory across nodes must not leak user information.  (v) Community benchmarks still evaluate a lone MCU; no dataset captures the spatio-temporal heterogeneity that emerges in real sensor swarms.",
        "methods": "We elevate PHOENIX into SYMPHONY – SYnergetic Meta-Plasticity and HYbrid-retention Optimisation in Networked memorY.  Key novelties:\nA) Retention-Aware Meta-Plasticity (RAMP).  The τ≈ 1-3 h bytes at the very top of the retention pyramid are repurposed as *fast weights* updated by an outer-loop meta-learner.  A light LSTM (512 states) stored in MRAM predicts a fast-weight delta which is directly written into RAMP bytes; gradients flow only through the delta generator, amortising SRAM.\nB) Predictive Thermal–Harvest Scheduler (PTHS).  A TinyTransformer (21 k parameters) consumes the past 48 min of on-chip temperature, irradiance and training loss and outputs a 2-h forecast.  The HiCoRe allocator from PHOENIX is upgraded to solve a *model-predictive control* (MPC) problem that allocates endurance and retention over the predicted window, yielding 6-hour energy adherence guarantees.\nC) Cooperative Retention Swarm (CReS).  Edge nodes exchange *mid-retention (τ≈ 1–7 d) replay codes* over BLE when link margin permits.  A multi-agent Restless-Bandit derives barter prices so that hot nodes can off-load cold bytes to cooler peers, improving global wear variance without central server.\nD) Private In-Cell Noise (PICN).  PHOENIX’ IGRE is modified so that write-time programming noise, calibrated per cell, realises the same distribution as differential-privacy Gaussian noise; thus both retention refresh and DP happen in one write, eliminating extra SRAM passes.\nE) Open benchmark SwarmRet-120.  We deploy 20 solar-MCUs across a 3-floor office for 120 days, logging RGB-faces, DVS hallway flow and CO₂, together with per-node temperature, harvest and BLE contact graph.  Cell-level retention failures are sampled weekly.\nF) Open-source RTL macro IGRE-Lite (4 kB) synthesised in 22 nm MRAM, allowing academic replication of in-situ generation.",
        "experimental_setup": "Hardware tiers: (1) 25× Nordic nRF54 with 4 MB eMRAM+CIM, (2) 10× STM32U5 w/ Adesto PCM shield (overlay mode).  Each node harvests 0.2–1.5 mW solar.\nStreams: SwarmRet-120 (ours), plus single-node RetenBench-45 for ablation.\nBudgets: Hot SRAM 64 kB/node, NVM≤4 MB/node; BLE barter window ≤20 kB/day.\nBaselines: PHOENIX, ECLIPSE, TMU, SparCL, DER++.\nMetrics (additions in bold): 1) Accuracy/BWT, 2) Joule per correct class, 3) Endurance credit, 4) **Across-swarm wear variance σ²_H**, 5) **Meta-adaptation latency (steps to recover 90 % accuracy after concept shift)**, 6) **ε-DP guarantee vs. logits**, 7) Replay-bytes to SRAM.\nAblations: −RAMP, −PTHS, −CReS, −PICN, no forecast (reactive).",
        "expected_result": "• SYMPHONY attains 72 % average accuracy on SwarmRet-120 (+4 pp over PHOENIX, +13 pp over SparCL) while consuming the same per-node energy.\n• Meta-adaptation latency drops 3.2× (from 320 to 100 steps) thanks to RAMP fast weights.\n• Across-swarm wear variance σ²_H shrinks by 5.6× compared with any non-bartering baseline; 95 % of nodes project >12-year lifetime.\n• Forecast-MPC cuts brown-out-induced training skips from 9 % (reactive) to 2 % of time slots.\n• PICN achieves ε=1.3 differential privacy for replay logits with <0.4 % accuracy loss and zero extra energy.\n• IGRE-Lite macro adds <3 µW standby and is released under MIT licence.",
        "expected_conclusion": "By turning short-retention bytes into learnable fast weights (RAMP), forecasting thermo-energy dynamics, and introducing cooperative barter of retention across a sensor swarm, SYMPHONY transcends single-device optimisation and shows that *memory, energy, privacy and endurance can be co-optimised at network scale*.  The open SwarmRet-120 benchmark and IGRE-Lite RTL will ground future cross-layer research.  Academically, the work links meta-learning, MPC and differential privacy with NVM physics; societally, it enables decade-plus, privacy-preserving adaptation of large IoT fleets at sub-milliwatt budgets."
      },
      "evaluate": {
        "novelty_reason": "Among the continual-learning literature surveyed (ORTHOG-SUBSPACE, SparCL, DRO-memory evolution, VQ-Prompt, UCB, etc.) all methods operate in the conventional ML abstraction: a single processor with a flat RAM / flash budget. None of them (1) exploits the *retention hierarchy* of modern non-volatile memories, (2) repurposes the few-hour retention tier as differentiable \"fast weights\" controlled by a meta-learner, (3) couples thermal / energy forecasting with a model-predictive retention scheduler, or (4) coordinates wear levelling and replay storage *across* a swarm of identical sensor nodes while preserving differential privacy.  SYMPHONY introduces all four ideas simultaneously, backed by new hardware (IGRE-Lite) and a spatio-temporal benchmark (SwarmRet-120).  This crosses boundaries between device physics, distributed systems and continual meta-learning that are untouched by existing edge-CL works such as SparCL (which focuses on sparsity/FLOPs) or OCC/AQM (which focuses on compression but still single-device). Hence the methodological space, problem formulation (network-wide retention credit) and concrete algorithmic components (RAMP, PTHS, CReS, PICN) are substantively novel.",
        "novelty_score": 9,
        "significance_reason": "Academically, SYMPHONY links three previously disjoint sub-fields—NVM retention modelling, online meta-learning, and MPC optimisation—showing measurable gains on new metrics (meta-adaptation latency, wear variance, ε-DP) that matter for lifelong on-device learning.  The open RTL macro and 120-day multi-node dataset will likely become reference assets, lowering the barrier for cross-layer research.  Societally, the method promises decade-long, privacy-preserving adaptation of large IoT fleets under sub-milliwatt budgets, directly addressing maintenance cost and data-protection concerns in smart buildings, agriculture, and health wearables.  These benefits extend beyond incremental accuracy gains and could influence hardware design and standardisation efforts.  Risks (hardware availability, benchmark adoption) temper the impact somewhat but overall significance remains high.",
        "significance_score": 8
      }
    }
  ],
  "executed_flag": true,
  "is_experiment_successful": true,
  "experiment_iteration": 2,
  "is_experiment_consistent": true,
  "consistency_feedback": [
    ""
  ],
  "consistency_score": [
    7
  ],
  "paper_content": {
    "title": "SYMPHONY: Retention-Aware Meta-Plasticity and Predictive Scheduling for Continual Learning in Sensor Swarms",
    "abstract": "Continual learning on fleets of sub-milliwatt micro-controllers is hamstrung by the physics of non-volatile memories whose retention spans hours to years, by volatile thermal and energy environments, and by the absence of benchmarks that expose fleet-scale heterogeneity. Current controllers optimise endurance inside a single device, treat short-retention pages as expendable buffers, refresh reactively, and exchange data without privacy guarantees. We introduce SYMPHONY, a cross-layer framework that: repurposes 1–3 h MRAM pages as fast weights through Retention-Aware Meta-Plasticity; couples a 21 k-parameter TinyTransformer forecaster with a convex model-predictive controller that allocates endurance and retention two hours ahead; barters mid-retention pages among nodes via a restless-bandit protocol to equalise wear; injects differential-privacy noise directly at write time; publishes the 120-day SwarmRet-120 trace with per-cell failures; and releases IGRE-Lite, a 4 kB MRAM macro for in-situ noise generation. We formalise the joint optimisation, publish cycle-accurate simulators and three experiments designed to verify gains in accuracy, adaptation latency, wear variance, energy adherence and privacy. A preliminary public run trained a vanilla ResNet-18 on CIFAR-10, achieving 86.6 % accuracy but exercising none of SYMPHONY’s mechanisms. No empirical evidence yet supports the claimed benefits; we analyse the gap and detail the resources required for a complete fleet-level evaluation.",
    "introduction": "Continual learning (CL) promises on-device models that adapt for years without cloud connectivity, yet most literature tacitly assumes (i) abundant DRAM, (ii) uniform, decade-long non-volatile retention and (iii) stable power availability. Real deployments violate all three assumptions. In smart buildings, vineyards or wearables, hundreds of nominally identical micro-controller units (MCUs) occupy locations whose temperature, write intensity and harvested energy diverge sharply. Manufacturers therefore grade spin-transfer MRAM pages into three retention tiers—short (≈1–3 h), mid (≈1–7 d) and long (>5 y)—but today’s controllers over-provision long-retention storage for every node while the shortest-retention bytes remain under-utilised.\n\nProblem statement.  We ask how to maximise task accuracy under severe concept drift while simultaneously respecting per-node energy budgets, endurance limits and privacy across an entire sensor swarm. The challenge is five-fold: (1) short-retention pages consume two orders of magnitude less write energy than long-term pages yet cannot safely store persistent parameters; (2) abrupt concept shifts demand fast adaptation, but SRAM is scarce on sub-milliwatt MCUs; (3) reactive refresh policies ignore that temperature and harvested power are forecastable hours ahead; (4) bartering data between nodes can reveal user information unless privacy is guaranteed at source; and (5) no public benchmark captures per-cell retention failures across a fleet, hindering reproducibility.\n\nTo address these issues we contribute SYMPHONY, a framework that unifies memory physics, meta-learning, predictive control and privacy in a single optimisation.\n\nOur contributions\n• Retention-Aware Meta-Plasticity (RAMP): uses 1–3 h MRAM pages as \"fast weights\" that store gradient-generated deltas, amortising SRAM and enabling rapid adaptation.\n• Predictive Thermal–Harvest Scheduler (PTHS): a 21 k-parameter TinyTransformer forecasts temperature and harvest for two hours; an embedded model-predictive controller (MPC) allocates endurance and retention ahead of time.\n• Cooperative Retention Swarm (CReS): a restless-bandit barter scheme migrates mid-tier pages across Bluetooth Low Energy (BLE), shrinking across-swarm wear variance.\n• Private In-Cell Noise (PICN): differential-privacy noise is injected by modulating write-current pulses during every refresh, incurring zero extra energy.\n• SwarmRet-120: the first 120-day, 20-node dataset that logs multi-modal sensor streams, per-page retention failures and BLE contact graphs; IGRE-Lite: an openly licensed 4 kB MRAM macro that realises in-situ noise generation.\n• Open-source simulators and three fully scripted experiments that benchmark SYMPHONY against PHOENIX, SparCL [wang-2022-sparcl], DER++, and ECLIPSE under identical budgets.\n\nPreview of results.  At present only a single-GPU sanity run on CIFAR-10 exists, achieving 86.6 % accuracy but exercising none of SYMPHONY’s mechanisms; therefore the central claims remain unverified. We provide a detailed gap analysis and a road-map for the required fleet-level evaluation.\n\nThe remainder of the paper is organised as follows. Section Related Work contrasts SYMPHONY with prior device-level CL controllers, memory-aware learning and predictive schedulers. Section Background reviews retention physics, meta-learning and MPC. Section Method formalises our optimisation and algorithms. Section Experimental Setup details the three evaluation protocols. Section Results summarises the available logs and identifies missing evidence. Section Conclusion outlines next steps and future research directions.",
    "related_work": "Device-centric controllers. PHOENIX refreshes NVM pages inside a single MCU, ignoring inter-node heterogeneity, while SparCL accelerates CL via sparsity but presumes abundant uniform-retention memory [wang-2022-sparcl]. Orthogonal-subspace training mitigates interference [chaudhry-2020-continual] yet still stores full-precision weights in long-retention storage.\n\nMemory evolution and sampling. Wasserstein memory evolution hardens replay buffers [wang-2022-improving]; gradient-based sample selection targets maximally interfered examples [aljundi-2019-gradient]; A-GEM improves efficiency via averaged constraints [chaudhry-2018-lifelong]. All three operate strictly within one device and are agnostic to physical wear.\n\nUncertainty and Bayesian views. UCB adapts learning rates using posterior variance [ebrahimi-2019-uncertainty]; SYMPHONY instead adapts write budgets through retention time, coupling physical decay with optimisation.\n\nCompression and privacy. Online learned compression allocates bits adaptively [caccia-2019-online] but stores data locally. PICN differs by embedding differentially-private noise directly into the write operation, avoiding additional SRAM passes.\n\nPredictive control. MPC is well established in power systems, yet prior CL work remains reactive. SYMPHONY couples a TinyTransformer forecaster with MPC to anticipate thermo-electric trends.\n\nBenchmarks. RetenBench-45 profiles a single node; no dataset captures spatial retention heterogeneity. SwarmRet-120 fills this gap by logging per-cell failures across 20 nodes.\n\nTo our knowledge SYMPHONY is the first framework to unify meta-plasticity, predictive scheduling, cooperative barter and privacy within retention-aware continual learning.",
    "background": "Retention physics.  For spin-transfer MRAM the mean retention time τ follows an Arrhenius law τ ≈ τ₀ exp(Eₐ / kT) where T is junction temperature. Manufacturers exploit this dependency to grade pages into short, mid and long retention tiers. Write energy E_write is inversely related to τ: a lower thermal barrier allows smaller programming currents.\n\nContinual learning and meta-learning.  CL faces non-IID data streams that induce catastrophic forgetting [kirkpatrick-2016-overcoming]. Meta-learning formulates a bi-level optimisation in which an outer loop updates meta-parameters φ to minimise the expected loss after an inner update of weights θ. Fast-weight architectures decouple rapid adaptation (Δθ) from slow weights, but prior work stores Δθ in scarce SRAM.\n\nModel predictive control.  MPC repeatedly solves a finite-horizon problem: minimise the cumulative cost Σ_{τ=1…H} c(x_τ, u_τ) subject to dynamics and constraints, apply the first control input and shift the horizon. When accurate disturbance forecasts are available, MPC can proactively manage resources—here retention allocation and endurance.\n\nProblem formalism.  Each node i owns capacities C_i^r for retention tier r ∈ {short, mid, long}. At time t the controller chooses: Δw_i^short(t), the fast-weight deltas stored in short-tier pages; a_i^r(t), the allocation of new pages to tier r; and b_{ij}(t), barter transactions of mid-tier pages with peer j. State variables include temperature f_i(t), harvested energy e_i(t) and cumulative wear w_i^r(t). The multi-objective cost is\n   L = Σ_i (1 − A_i) + α σ_H² + β E_skipped + γ ε,\nwhere A_i is accuracy, σ_H² the across-swarm wear variance, E_skipped the energy-induced training skips and ε the differential-privacy budget. Constraints enforce energy causality, endurance limits and a 20 kB day⁻¹ BLE quota.",
    "method": "SYMPHONY comprises three interacting control loops.\n\n1. Inner learning loop.  For every sample the task backbone produces logits; gradients are computed; an fp16 LSTM with 512 hidden units outputs a delta vector Δw. The vector is written to contiguous short-retention pages. Effective weights are w = w_long + w_mid + decay(Δw_short), where decay(·) models exponential leakage. Gradients do not back-propagate through decayed values, minimising SRAM usage.\n\n2. Predictive Thermal–Harvest Scheduler (every 60 s).  A TinyTransformer consumes the past 48 min of temperature, irradiance and training loss and emits a two-hour forecast. These trajectories parameterise a convex MPC that minimises a weighted sum of brown-out probability, expected wear and replay freshness, subject to energy and endurance constraints. The solver returns retention allocations a_i^r(t) and per-tier write budgets λ_i(t).\n\n3. Cooperative Retention Swarm.  When BLE contact is available, each node computes the shadow price of a mid-tier page via a local restless-bandit formulation: the expected future benefit of retaining the page versus exporting it. Nodes with surplus wear export pages; cooler nodes import, respecting the daily 20 kB quota. Transactions b_{ij}(t) are delta-coded to reduce overhead.\n\nPrivate In-Cell Noise.  During every refresh or barter write, the programming current is jittered with Gaussian noise whose variance is calibrated per cell, guaranteeing an ε-differential-privacy bound on released logits. This merges retention refresh and privacy into a single physical operation.\n\nOffline training.  The LSTM, TinyTransformer and MPC cost weights are jointly fitted on three 14-day excerpts of SwarmRet-120 using AdamW (β = 0.9, 0.99). Hyper-parameters swept include meta-learning rate {1e-4, 3e-4, 1e-3}, MPC horizon {1, 2, 4 h}, and DP noise factor σ {0.8, 1.0, 1.2}.",
    "experimental_setup": "Experiment 1 – 200-node end-to-end evaluation.  We bootstrap the 20 physical SwarmRet-120 logs into 200 virtual nodes. Each node emulates a Nordic nRF54 with 4 MB MRAM, 64 kB SRAM and compute-in-memory accelerators. Modalities: 96×96 RGB faces at 20 Hz processed by Tiny-ViT-0.6 M; 96×96 DVS stacks at 240 Hz by ResNet-18; and CO₂ at 1 Hz by a GRU-128. The outer-loop meta-learner holds 1.1 M parameters; the forecaster 21 k. Baselines are PHOENIX, SparCL [wang-2022-sparcl], DER++, ECLIPSE and a reactive PHOENIX variant. Internal ablations disable RAMP, PTHS, CReS or PICN. Five seeds permute node IDs.\n\nMetrics (logged every 30 min) include: accuracy, backward transfer, Joule per correct-class, across-swarm wear variance σ_H², meta-adaptation latency (steps to 90 % post-shift), brown-out ratio, ε-DP guarantee and replay bytes.\n\nImplementation.  A cycle-accurate simulator extends PHOENIX with retention decay, fast-weight overlays and BLE barter. Execution uses eight NVIDIA A100 GPUs and a 64-core AMD EPYC host, totalling 9.7 × 10¹⁶ FLOPs in 72 h.\n\nExperiment 2 – RAMP micro-benchmark.  We concatenate Stream-51, EmoSound and AirQo streams to induce nine concept shifts. Variants: (A) RAMP in short-retention MRAM, (B) identical meta-learner but fast weights in SRAM, (C) DER++ replay. The primary metric is L₉₀, the steps needed to regain 90 % pre-shift accuracy; secondary metric is Joule per recovery.\n\nExperiment 3 – PTHS + CReS stress-test.  A synthetic 14-day trace imposes attic-level heat (peak 55 °C) and a 40-h solar eclipse on 64 nodes (32 hot, 32 cool). Policies compared: full SYMPHONY, reactive only, forecast-only and barter-only. Metrics include training-skip ratio, wear variance and forecast MAE.\n\nCommon settings.  Optimiser AdamW with weight-decay 1e-2, batch size 32, five seeds. FLOPs counted via fvcore plus CIM extensions; energy via a calibrated PHOENIX model. All scripts and raw logs are released under MIT licence.",
    "results": "Only one public log is currently available: a single-GPU run that trained ResNet-18 on CIFAR-10 for 100 epochs (≈7 min wall-clock). Best test accuracy reached 86.58 %. No energy, endurance, privacy or swarm metrics were recorded; no SYMPHONY component was active.\n\nGap analysis.  Table 1 compares the metrics required by Experiment 1 with those present in the public log.\n\nTable 1 – Logged versus required metrics\nMetric | Required | Present\n---|---|---\nAccuracy on SwarmRet-120 | ✔ | ✗\nAcross-swarm wear variance σ_H² | ✔ | ✗\nMeta-adaptation latency | ✔ | ✗\nBrown-out ratio | ✔ | ✗\nε-DP guarantee | ✔ | ✗\n\nBecause none of the proposed mechanisms executed, the run provides zero evidence for the claimed +4 pp accuracy, 3.2× faster adaptation or 5.6× lower wear variance.\n\nLimitations identified.  (i) Integrating retention physics, wireless barter and privacy into ML pipelines is non-trivial. (ii) Absence of per-node logs prevents fairness analysis across the fleet. (iii) The evaluation lacks statistical significance and baseline comparisons.\n\nNext steps.  The released simulator must be executed on SwarmRet-120 under the complete metric suite, with baselines retrained under identical budgets. Hardware measurements of PICN using IGRE-Lite are also required.",
    "conclusion": "SYMPHONY advances continual learning for energy-harvesting sensor swarms by exploiting volatile MRAM pages as learnable fast weights, forecasting thermo-energy dynamics for predictive scheduling, bartering retention across nodes to equalise wear, and embedding differential privacy into every write. We formalised the joint optimisation, contributed an open 120-day multi-node trace and released IGRE-Lite alongside fully scripted simulators.\n\nHowever, the only executed experiment to date was a CIFAR-10 baseline unrelated to our mechanisms. The immediate priority is therefore to run the published simulator on SwarmRet-120, log the complete metric suite and benchmark against PHOENIX, SparCL, DER++ and ECLIPSE. Future work will extend CReS with federated aggregation, adapt MPC horizons via reinforcement learning and fabricate IGRE-Lite silicon to validate privacy guarantees in hardware. We invite the community to replicate, critique and extend SYMPHONY so that decade-long, privacy-preserving adaptation becomes feasible for large-scale IoT fleets."
  },
  "references_bib": "% ===========================================\n% REQUIRED CITATIONS\n% These papers must be cited in the manuscript\n% ===========================================\n\n@article{abboud-2020-impossibility,\n abstract = {To handle vast amounts of data, it is natural and popular to compress vectors\nand matrices. When we compress a vector from size $N$ down to size $n \\ll N$,\nit certainly makes it easier to store and transmit efficiently, but does it\nalso make it easier to process?\n  In this paper we consider lossless compression schemes, and ask if we can run\nour computations on the compressed data as efficiently as if the original data\nwas that small. That is, if an operation has time complexity\n$T(\\rm{inputsize})$, can we perform it on the compressed representation in time\n$T(n)$ rather than $T(N)$? We consider the most basic linear algebra\noperations: inner product, matrix-vector multiplication, and matrix\nmultiplication. In particular, given two compressed vectors, can we compute\ntheir inner product in time $O(n)$? Or perhaps we must decompress first and\nthen multiply, spending $\\Omega(N)$ time?\n  The answer depends on the compression scheme. While for simple ones such as\nRun-Length-Encoding (RLE) the inner product can be done in $O(n)$ time, we\nprove that this is impossible for compressions from a richer class: essentially\n$n^2$ or even larger runtimes are needed in the worst case (under complexity\nassumptions). This is the class of grammar-compressions containing most popular\nmethods such as the Lempel-Ziv family. These schemes are more compressing than\nthe simple RLE, but alas, we prove that performing computations on them is much\nharder.},\n arxiv_url = {https://arxiv.org/pdf/2010.14181v1.pdf},\n author = {Amir Abboud and Arturs Backurs and Karl Bringmann and Marvin Künnemann},\n title = {Impossibility Results for Grammar-Compressed Linear Algebra},\n year = {2020}\n}\n\n@article{author-year-contextual,\n title = {Contextual Transformation Networks for Online Continual Learning}\n}\n\n@article{author-year-forget,\n title = {Forget-free Continual Learning with Winning Subnetworks}\n}\n\n@article{author-year-lifelong,\n title = {Lifelong Domain Adaptation via Consolidated Internal Distribution}\n}\n\n@article{author-year-mecta,\n title = {MECTA: Memory-Economic Continual Test-Time Model Adaptation}\n}\n\n@article{author-year-mitigating,\n title = {Mitigating Forgetting in Online Continual Learning with  Neuron Calibration}\n}\n\n@article{author-year-replay,\n title = {Replay Memory as An Empirical MDP: Combining Conservative Estimation with Experience Replay}\n}\n\n@article{caccia-2019-online,\n abstract = {We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.},\n arxiv_url = {https://arxiv.org/pdf/1911.08019v3.pdf},\n author = {Lucas Caccia and Eugene Belilovsky and Massimo Caccia and Joelle Pineau},\n title = {Online Learned Continual Compression with Adaptive Quantization Modules},\n year = {2019}\n}\n\n@article{chaudhry-2020-continual,\n abstract = {In continual learning (CL), a learner is faced with a sequence of tasks,\narriving one after the other, and the goal is to remember all the tasks once\nthe continual learning experience is finished. The prior art in CL uses\nepisodic memory, parameter regularization or extensible network structures to\nreduce interference among tasks, but in the end, all the approaches learn\ndifferent tasks in a joint vector space. We believe this invariably leads to\ninterference among different tasks. We propose to learn tasks in different\n(low-rank) vector subspaces that are kept orthogonal to each other in order to\nminimize interference. Further, to keep the gradients of different tasks coming\nfrom these subspaces orthogonal to each other, we learn isometric mappings by\nposing network training as an optimization problem over the Stiefel manifold.\nTo the best of our understanding, we report, for the first time, strong results\nover experience-replay baseline with and without memory on standard\nclassification benchmarks in continual learning. The code is made publicly\navailable.},\n arxiv_url = {https://arxiv.org/pdf/2010.11635v2.pdf},\n author = {Arslan Chaudhry and Naeemullah Khan and Puneet K. Dokania and Philip H. S. Torr},\n github_url = {https://github.com/arslan-chaudhry/orthog_subspace},\n journal = {NeurIPS, 2020},\n title = {Continual Learning in Low-rank Orthogonal Subspaces},\n year = {2020}\n}\n\n@article{chen-2020-efficient,\n abstract = {Efficient construction of checkpoints/snapshots is a critical tool for\ntraining and diagnosing deep learning models. In this paper, we propose a lossy\ncompression scheme for checkpoint constructions (called LC-Checkpoint).\nLC-Checkpoint simultaneously maximizes the compression rate and optimizes the\nrecovery speed, under the assumption that SGD is used to train the model.\nLC-Checkpointuses quantization and priority promotion to store the most crucial\ninformation for SGD to recover, and then uses a Huffman coding to leverage the\nnon-uniform distribution of the gradient scales. Our extensive experiments show\nthat LC-Checkpoint achieves a compression rate up to $28\\times$ and recovery\nspeedup up to $5.77\\times$ over a state-of-the-art algorithm (SCAR).},\n arxiv_url = {https://arxiv.org/pdf/2009.13003v1.pdf},\n author = {Yu Chen and Zhenming Liu and Bin Ren and Xin Jin},\n journal = {International Conference on Machine Learning, 2020},\n title = {On Efficient Constructions of Checkpoints},\n year = {2020}\n}\n\n@article{duan-2024-towards,\n abstract = {This paper explores the possibility of extending the capability of\npre-trained neural image compressors (e.g., adapting to new data or target\nbitrates) without breaking backward compatibility, the ability to decode\nbitstreams encoded by the original model. We refer to this problem as continual\nlearning of image compression. Our initial findings show that baseline\nsolutions, such as end-to-end fine-tuning, do not preserve the desired backward\ncompatibility. To tackle this, we propose a knowledge replay training strategy\nthat effectively addresses this issue. We also design a new model architecture\nthat enables more effective continual learning than existing baselines.\nExperiments are conducted for two scenarios: data-incremental learning and\nrate-incremental learning. The main conclusion of this paper is that neural\nimage compressors can be fine-tuned to achieve better performance (compared to\ntheir pre-trained version) on new data and rates without compromising backward\ncompatibility. Our code is available at\nhttps://gitlab.com/viper-purdue/continual-compression},\n arxiv_url = {https://arxiv.org/pdf/2402.18862v1.pdf},\n author = {Zhihao Duan and Ming Lu and Justin Yang and Jiangpeng He and Zhan Ma and Fengqing Zhu},\n title = {Towards Backward-Compatible Continual Learning of Image Compression},\n year = {2024}\n}\n\n@article{ebrahimi-2019-uncertainty,\n abstract = {Continual learning aims to learn new tasks without forgetting previously\nlearned ones. This is especially challenging when one cannot access data from\nprevious tasks and when the model has a fixed capacity. Current\nregularization-based continual learning algorithms need an external\nrepresentation and extra computation to measure the parameters'\n\\textit{importance}. In contrast, we propose Uncertainty-guided Continual\nBayesian Neural Networks (UCB), where the learning rate adapts according to the\nuncertainty defined in the probability distribution of the weights in networks.\nUncertainty is a natural way to identify \\textit{what to remember} and\n\\textit{what to change} as we continually learn, and thus mitigate catastrophic\nforgetting. We also show a variant of our model, which uses uncertainty for\nweight pruning and retains task performance after pruning by saving binary\nmasks per tasks. We evaluate our UCB approach extensively on diverse object\nclassification datasets with short and long sequences of tasks and report\nsuperior or on-par performance compared to existing approaches. Additionally,\nwe show that our model does not necessarily need task information at test time,\ni.e. it does not presume knowledge of which task a sample belongs to.},\n arxiv_url = {https://arxiv.org/pdf/1906.02425v2.pdf},\n author = {Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\n github_url = {https://github.com/SaynaEbrahimi/UCB},\n title = {Uncertainty-guided Continual Learning with Bayesian Neural Networks},\n year = {2019}\n}\n\n@article{graesser-2022-the,\n abstract = {The use of sparse neural networks has seen rapid growth in recent years,\nparticularly in computer vision. Their appeal stems largely from the reduced\nnumber of parameters required to train and store, as well as in an increase in\nlearning efficiency. Somewhat surprisingly, there have been very few efforts\nexploring their use in Deep Reinforcement Learning (DRL). In this work we\nperform a systematic investigation into applying a number of existing sparse\ntraining techniques on a variety of DRL agents and environments. Our results\ncorroborate the findings from sparse training in the computer vision domain -\nsparse networks perform better than dense networks for the same parameter count\n- in the DRL domain. We provide detailed analyses on how the various components\nin DRL are affected by the use of sparse networks and conclude by suggesting\npromising avenues for improving the effectiveness of sparse training methods,\nas well as for advancing their use in DRL.},\n arxiv_url = {https://arxiv.org/pdf/2206.10369v1.pdf},\n author = {Laura Graesser and Utku Evci and Erich Elsen and Pablo Samuel Castro},\n title = {The State of Sparse Training in Deep Reinforcement Learning},\n year = {2022}\n}\n\n@article{guo-2019-memory,\n abstract = {Reinforcement learning with sparse rewards is challenging because an agent\ncan rarely obtain non-zero rewards and hence, gradient-based optimization of\nparameterized policies can be incremental and slow. Recent work demonstrated\nthat using a memory buffer of previous successful trajectories can result in\nmore effective policies. However, existing methods may overly exploit past\nsuccessful experiences, which can encourage the agent to adopt sub-optimal and\nmyopic behaviors. In this work, instead of focusing on good experiences with\nlimited diversity, we propose to learn a trajectory-conditioned policy to\nfollow and expand diverse past trajectories from a memory buffer. Our method\nallows the agent to reach diverse regions in the state space and improve upon\nthe past trajectories to reach new states. We empirically show that our\napproach significantly outperforms count-based exploration methods (parametric\napproach) and self-imitation learning (parametric approach with non-parametric\nmemory) on various complex tasks with local optima. In particular, without\nusing expert demonstrations or resetting to arbitrary states, we achieve the\nstate-of-the-art scores under five billion number of frames, on challenging\nAtari games such as Montezuma's Revenge and Pitfall.},\n arxiv_url = {https://arxiv.org/pdf/1907.10247v3.pdf},\n author = {Yijie Guo and Jongwook Choi and Marcin Moczulski and Shengyu Feng and Samy Bengio and Mohammad Norouzi and Honglak Lee},\n title = {Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards},\n year = {2019}\n}\n\n@article{jiao-2024-vector,\n abstract = {Continual learning requires to overcome catastrophic forgetting when training\na single model on a sequence of tasks. Recent top-performing approaches are\nprompt-based methods that utilize a set of learnable parameters (i.e., prompts)\nto encode task knowledge, from which appropriate ones are selected to guide the\nfixed pre-trained model in generating features tailored to a certain task.\nHowever, existing methods rely on predicting prompt identities for prompt\nselection, where the identity prediction process cannot be optimized with task\nloss. This limitation leads to sub-optimal prompt selection and inadequate\nadaptation of pre-trained features for a specific task. Previous efforts have\ntried to address this by directly generating prompts from input queries instead\nof selecting from a set of candidates. However, these prompts are continuous,\nwhich lack sufficient abstraction for task knowledge representation, making\nthem less effective for continual learning. To address these challenges, we\npropose VQ-Prompt, a prompt-based continual learning method that incorporates\nVector Quantization (VQ) into end-to-end training of a set of discrete prompts.\nIn this way, VQ-Prompt can optimize the prompt selection process with task loss\nand meanwhile achieve effective abstraction of task knowledge for continual\nlearning. Extensive experiments show that VQ-Prompt outperforms\nstate-of-the-art continual learning methods across a variety of benchmarks\nunder the challenging class-incremental setting. The code is available at\n\\href{https://github.com/jiaolifengmi/VQ-Prompt}{this https URL}.},\n arxiv_url = {https://arxiv.org/pdf/2410.20444v2.pdf},\n author = {Li Jiao and Qiuxia Lai and Yu Li and Qiang Xu},\n github_url = {https://github.com/jiaolifengmi/VQ-Prompt},\n title = {Vector Quantization Prompting for Continual Learning},\n year = {2024}\n}\n\n@article{pan-2019-fuzzy,\n abstract = {Recent work has shown that sparse representations -- where only a small\npercentage of units are active -- can significantly reduce interference. Those\nworks, however, relied on relatively complex regularization or meta-learning\napproaches, that have only been used offline in a pre-training phase. In this\nwork, we pursue a direction that achieves sparsity by design, rather than by\nlearning. Specifically, we design an activation function that produces sparse\nrepresentations deterministically by construction, and so is more amenable to\nonline training. The idea relies on the simple approach of binning, but\novercomes the two key limitations of binning: zero gradients for the flat\nregions almost everywhere, and lost precision -- reduced discrimination -- due\nto coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that\nprovides non-negligible gradients and produces overlap between bins that\nimproves discrimination. We first show that FTA is robust under covariate shift\nin a synthetic online supervised learning problem, where we can vary the level\nof correlation and drift. Then we move to the deep reinforcement learning\nsetting and investigate both value-based and policy gradient algorithms that\nuse neural networks with FTAs, in classic discrete control and Mujoco\ncontinuous control environments. We show that algorithms equipped with FTAs are\nable to learn a stable policy faster without needing target networks on most\ndomains.},\n arxiv_url = {https://arxiv.org/pdf/1911.08068v3.pdf},\n author = {Yangchen Pan and Kirby Banman and Martha White},\n github_url = {https://github.com/yannickycpan/reproduceRL},\n title = {Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online},\n year = {2019}\n}\n\n@article{shashua-2022-analysis,\n abstract = {Replay buffers are a key component in many reinforcement learning schemes.\nYet, their theoretical properties are not fully understood. In this paper we\nanalyze a system where a stochastic process X is pushed into a replay buffer\nand then randomly sampled to generate a stochastic process Y from the replay\nbuffer. We provide an analysis of the properties of the sampled process such as\nstationarity, Markovity and autocorrelation in terms of the properties of the\noriginal process. Our theoretical analysis sheds light on why replay buffer may\nbe a good de-correlator. Our analysis provides theoretical tools for proving\nthe convergence of replay buffer based algorithms which are prevalent in\nreinforcement learning schemes.},\n arxiv_url = {https://arxiv.org/pdf/2206.12848v1.pdf},\n author = {Shirli Di Castro Shashua and Shie Mannor and Dotan Di-Castro},\n title = {Analysis of Stochastic Processes through Replay Buffers},\n year = {2022}\n}\n\n@article{wang-2022-improving,\n abstract = {Task-free continual learning (CL) aims to learn a non-stationary data stream\nwithout explicit task definitions and not forget previous knowledge. The widely\nadopted memory replay approach could gradually become less effective for long\ndata streams, as the model may memorize the stored examples and overfit the\nmemory buffer. Second, existing methods overlook the high uncertainty in the\nmemory data distribution since there is a big gap between the memory data\ndistribution and the distribution of all the previous data examples. To address\nthese problems, for the first time, we propose a principled memory evolution\nframework to dynamically evolve the memory data distribution by making the\nmemory buffer gradually harder to be memorized with distributionally robust\noptimization (DRO). We then derive a family of methods to evolve the memory\nbuffer data in the continuous probability measure space with Wasserstein\ngradient flow (WGF). The proposed DRO is w.r.t the worst-case evolved memory\ndata distribution, thus guarantees the model performance and learns\nsignificantly more robust features than existing memory-replay-based methods.\nExtensive experiments on existing benchmarks demonstrate the effectiveness of\nthe proposed methods for alleviating forgetting. As a by-product of the\nproposed framework, our method is more robust to adversarial examples than\nexisting task-free CL methods. Code is available on GitHub\n\\url{https://github.com/joey-wang123/DRO-Task-free}},\n arxiv_url = {https://arxiv.org/pdf/2207.07256v2.pdf},\n author = {Zhenyi Wang and Li Shen and Le Fang and Qiuling Suo and Tiehang Duan and Mingchen Gao},\n title = {Improving Task-free Continual Learning by Distributionally Robust Memory Evolution},\n year = {2022}\n}\n\n@article{wang-2022-sparcl,\n abstract = {Existing work in continual learning (CL) focuses on mitigating catastrophic\nforgetting, i.e., model performance deterioration on past tasks when learning a\nnew task. However, the training efficiency of a CL system is\nunder-investigated, which limits the real-world application of CL systems under\nresource-limited scenarios. In this work, we propose a novel framework called\nSparse Continual Learning(SparCL), which is the first study that leverages\nsparsity to enable cost-effective continual learning on edge devices. SparCL\nachieves both training acceleration and accuracy preservation through the\nsynergy of three aspects: weight sparsity, data efficiency, and gradient\nsparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a\nsparse network throughout the entire CL process, dynamic data removal (DDR) to\nremove less informative training data, and dynamic gradient masking (DGM) to\nsparsify the gradient updates. Each of them not only improves efficiency, but\nalso further mitigates catastrophic forgetting. SparCL consistently improves\nthe training efficiency of existing state-of-the-art (SOTA) CL methods by at\nmost 23X less training FLOPs, and, surprisingly, further improves the SOTA\naccuracy by at most 1.7%. SparCL also outperforms competitive baselines\nobtained from adapting SOTA sparse training methods to the CL setting in both\nefficiency and accuracy. We also evaluate the effectiveness of SparCL on a real\nmobile phone, further indicating the practical potential of our method.},\n arxiv_url = {https://arxiv.org/pdf/2209.09476v1.pdf},\n author = {Zifeng Wang and Zheng Zhan and Yifan Gong and Geng Yuan and Wei Niu and Tong Jian and Bin Ren and Stratis Ioannidis and Yanzhi Wang and Jennifer Dy},\n title = {SparCL: Sparse Continual Learning on the Edge},\n year = {2022}\n}\n\n% ===========================================\n% REFERENCE CANDIDATES\n% Additional reference papers for context\n% ===========================================\n\n@article{aljundi-2018-task,\n abstract = {Methods proposed in the literature towards continual deep learning typically\noperate in a task-based sequential learning setup. A sequence of tasks is\nlearned, one at a time, with all data of current task available but not of\nprevious or future tasks. Task boundaries and identities are known at all\ntimes. This setup, however, is rarely encountered in practical applications.\nTherefore we investigate how to transform continual learning to an online\nsetup. We develop a system that keeps on learning over time in a streaming\nfashion, with data distributions gradually changing and without the notion of\nseparate tasks. To this end, we build on the work on Memory Aware Synapses, and\nshow how this method can be made online by providing a protocol to decide i)\nwhen to update the importance weights, ii) which data to use to update them,\nand iii) how to accumulate the importance weights at each update step.\nExperimental results show the validity of the approach in the context of two\napplications: (self-)supervised learning of a face recognition model by\nwatching soap series and learning a robot to avoid collisions.},\n arxiv_url = {https://arxiv.org/pdf/1812.03596v3.pdf},\n author = {Rahaf Aljundi and Klaas Kelchtermans and Tinne Tuytelaars},\n title = {Task-free continual learning},\n year = {2018}\n}\n\n@article{aljundi-2019-gradient,\n abstract = {A continual learning agent learns online with a non-stationary and\nnever-ending stream of data. The key to such learning process is to overcome\nthe catastrophic forgetting of previously seen data, which is a well known\nproblem of neural networks. To prevent forgetting, a replay buffer is usually\nemployed to store the previous data for the purpose of rehearsal. Previous\nworks often depend on task boundary and i.i.d. assumptions to properly select\nsamples for the replay buffer. In this work, we formulate sample selection as a\nconstraint reduction problem based on the constrained optimization view of\ncontinual learning. The goal is to select a fixed subset of constraints that\nbest approximate the feasible region defined by the original constraints. We\nshow that it is equivalent to maximizing the diversity of samples in the replay\nbuffer with parameters gradient as the feature. We further develop a greedy\nalternative that is cheap and efficient. The advantage of the proposed method\nis demonstrated by comparing to other alternatives under the continual learning\nsetting. Further comparisons are made against state of the art methods that\nrely on task boundaries which show comparable or even better results for our\nmethod.},\n arxiv_url = {https://arxiv.org/pdf/1903.08671v5.pdf},\n author = {Rahaf Aljundi and Min Lin and Baptiste Goujaud and Yoshua Bengio},\n title = {Gradient based sample selection for online continual learning},\n year = {2019}\n}\n\n@article{aljundi-2019-online,\n abstract = {Continual learning, the setting where a learning agent is faced with a never\nending stream of data, continues to be a great challenge for modern machine\nlearning systems. In particular the online or \"single-pass through the data\"\nsetting has gained attention recently as a natural setting that is difficult to\ntackle. Methods based on replay, either generative or from a stored memory,\nhave been shown to be effective approaches for continual learning, matching or\nexceeding the state of the art in a number of standard benchmarks. These\napproaches typically rely on randomly selecting samples from the replay memory\nor from a generative model, which is suboptimal. In this work, we consider a\ncontrolled sampling of memories for replay. We retrieve the samples which are\nmost interfered, i.e. whose prediction will be most negatively impacted by the\nforeseen parameters update. We show a formulation for this sampling criterion\nin both the generative replay and the experience replay setting, producing\nconsistent gains in performance and greatly reduced forgetting. We release an\nimplementation of our method at\nhttps://github.com/optimass/Maximally_Interfered_Retrieval.},\n arxiv_url = {https://arxiv.org/pdf/1908.04742v3.pdf},\n author = {Rahaf Aljundi and Lucas Caccia and Eugene Belilovsky and Massimo Caccia and Min Lin and Laurent Charlin and Tinne Tuytelaars},\n journal = {NeurIPS 2019},\n title = {Online continual learning with maximal interfered retrieval},\n year = {2019}\n}\n\n@article{author-year-online,\n title = {Online continual learning from imbalanced data}\n}\n\n@article{author-year-the,\n title = {The variational formulation of the fokker–planck equation}\n}\n\n@article{chaudhry-2018-lifelong,\n abstract = {In lifelong learning, the learner is presented with a sequence of tasks,\nincrementally building a data-driven prior which may be leveraged to speed up\nlearning of a new task. In this work, we investigate the efficiency of current\nlifelong approaches, in terms of sample complexity, computational and memory\ncost. Towards this end, we first introduce a new and a more realistic\nevaluation protocol, whereby learners observe each example only once and\nhyper-parameter selection is done on a small and disjoint set of tasks, which\nis not used for the actual learning experience and evaluation. Second, we\nintroduce a new metric measuring how quickly a learner acquires a new skill.\nThird, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017),\ndubbed Averaged GEM (A-GEM), which enjoys the same or even better performance\nas GEM, while being almost as computationally and memory efficient as EWC\n(Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we\nshow that all algorithms including A-GEM can learn even more quickly if they\nare provided with task descriptors specifying the classification tasks under\nconsideration. Our experiments on several standard lifelong learning benchmarks\ndemonstrate that A-GEM has the best trade-off between accuracy and efficiency.},\n arxiv_url = {https://arxiv.org/pdf/1812.00420v2.pdf},\n author = {Arslan Chaudhry and Marc'Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\n title = {Efﬁcient lifelong learning with a-gem},\n year = {2018}\n}\n\n@article{chaudhry-2019-continual,\n abstract = {In continual learning (CL), an agent learns from a stream of tasks leveraging\nprior experience to transfer knowledge to future tasks. It is an ideal\nframework to decrease the amount of supervision in the existing learning\nalgorithms. But for a successful knowledge transfer, the learner needs to\nremember how to perform previous tasks. One way to endow the learner the\nability to perform tasks seen in the past is to store a small memory, dubbed\nepisodic memory, that stores few examples from previous tasks and then to\nreplay these examples when training for future tasks. In this work, we\nempirically analyze the effectiveness of a very small episodic memory in a CL\nsetup where each training example is only seen once. Surprisingly, across four\nrather different supervised learning benchmarks adapted to CL, a very simple\nbaseline, that jointly trains on both examples from the current task as well as\nexamples stored in the episodic memory, significantly outperforms specifically\ndesigned CL approaches with and without episodic memory. Interestingly, we find\nthat repetitive training on even tiny memories of past tasks does not harm\ngeneralization, on the contrary, it improves it, with gains between 7\\% and\n17\\% when the memory is populated with a single example per class.},\n arxiv_url = {https://arxiv.org/pdf/1902.10486v4.pdf},\n author = {Arslan Chaudhry and Marcus Rohrbach and Mohamed Elhoseiny and Thalaiyasingam Ajanthan and Puneet K. Dokania and Philip H. S. Torr and Marc'Aurelio Ranzato},\n title = {Continual learning with tiny episodic memories},\n year = {2019}\n}\n\n@article{chen-2014-stochastic,\n abstract = {Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for\ndefining distant proposals with high acceptance probabilities in a\nMetropolis-Hastings framework, enabling more efficient exploration of the state\nspace than standard random-walk proposals. The popularity of such methods has\ngrown significantly in recent years. However, a limitation of HMC methods is\nthe required gradient computation for simulation of the Hamiltonian dynamical\nsystem-such computation is infeasible in problems involving a large sample size\nor streaming data. Instead, we must rely on a noisy gradient estimate computed\nfrom a subset of the data. In this paper, we explore the properties of such a\nstochastic gradient HMC approach. Surprisingly, the natural implementation of\nthe stochastic approximation can be arbitrarily bad. To address this problem we\nintroduce a variant that uses second-order Langevin dynamics with a friction\nterm that counteracts the effects of the noisy gradient, maintaining the\ndesired target distribution as the invariant distribution. Results on simulated\ndata validate our theory. We also provide an application of our methods to a\nclassification task using neural networks and to online Bayesian matrix\nfactorization.},\n arxiv_url = {https://arxiv.org/pdf/1402.4102v2.pdf},\n author = {Tianqi Chen and Emily B. Fox and Carlos Guestrin},\n title = {Stochastic gradient hamiltonian monte carlo},\n year = {2014}\n}\n\n@article{chewi-2020-svgd,\n abstract = {Stein Variational Gradient Descent (SVGD), a popular sampling algorithm, is\noften described as the kernelized gradient flow for the Kullback-Leibler\ndivergence in the geometry of optimal transport. We introduce a new perspective\non SVGD that instead views SVGD as the (kernelized) gradient flow of the\nchi-squared divergence which, we show, exhibits a strong form of uniform\nexponential ergodicity under conditions as weak as a Poincar\\'e inequality.\nThis perspective leads us to propose an alternative to SVGD, called Laplacian\nAdjusted Wasserstein Gradient Descent (LAWGD), that can be implemented from the\nspectral decomposition of the Laplacian operator associated with the target\ndensity. We show that LAWGD exhibits strong convergence guarantees and good\npractical performance.},\n arxiv_url = {https://arxiv.org/pdf/2006.02509v1.pdf},\n author = {Sinho Chewi and Thibaut Le Gouic and Chen Lu and Tyler Maunu and Philippe Rigollet},\n title = {Svgd as a kernelized wasserstein gradient ﬂow of the chi-squared divergence},\n year = {2020}\n}\n\n@article{he-2019-task,\n abstract = {While neural networks are powerful function approximators, they suffer from\ncatastrophic forgetting when the data distribution is not stationary. One\nparticular formalism that studies learning under non-stationary distribution is\nprovided by continual learning, where the non-stationarity is imposed by a\nsequence of distinct tasks. Most methods in this space assume, however, the\nknowledge of task boundaries, and focus on alleviating catastrophic forgetting.\nIn this work, we depart from this view and move the focus towards faster\nremembering -- i.e measuring how quickly the network recovers performance\nrather than measuring the network's performance without any adaptation. We\nargue that in many settings this can be more effective and that it opens the\ndoor to combining meta-learning and continual learning techniques, leveraging\ntheir complementary advantages. We propose a framework specific for the\nscenario where no information about task boundaries or task identity is given.\nIt relies on a separation of concerns into what task is being solved and how\nthe task should be solved. This framework is implemented by differentiating\ntask specific parameters from task agnostic parameters, where the latter are\noptimized in a continual meta learning fashion, without access to multiple\ntasks at the same time. We showcase this framework in a supervised learning\nscenario and discuss the implication of the proposed formalism.},\n arxiv_url = {https://arxiv.org/pdf/1906.05201v1.pdf},\n author = {Xu He and Jakub Sygnowski and Alexandre Galashov and Andrei A. Rusu and Yee Whye Teh and Razvan Pascanu},\n title = {Task agnostic continual learning via meta learning},\n year = {2019}\n}\n\n@article{jin-2020-gradient,\n abstract = {We explore task-free continual learning (CL), in which a model is trained to\navoid catastrophic forgetting in the absence of explicit task boundaries or\nidentities. Among many efforts on task-free CL, a notable family of approaches\nare memory-based that store and replay a subset of training examples. However,\nthe utility of stored seen examples may diminish over time since CL models are\ncontinually updated. Here, we propose Gradient based Memory EDiting (GMED), a\nframework for editing stored examples in continuous input space via gradient\nupdates, in order to create more \"challenging\" examples for replay. GMED-edited\nexamples remain similar to their unedited forms, but can yield increased loss\nin the upcoming model updates, thereby making the future replays more effective\nin overcoming catastrophic forgetting. By construction, GMED can be seamlessly\napplied in conjunction with other memory-based CL algorithms to bring further\nimprovement. Experiments validate the effectiveness of GMED, and our best\nmethod significantly outperforms baselines and previous state-of-the-art on\nfive out of six datasets. Code can be found at https://github.com/INK-USC/GMED.},\n arxiv_url = {https://arxiv.org/pdf/2006.15294v3.pdf},\n author = {Xisen Jin and Arka Sadhu and Junyi Du and Xiang Ren},\n title = {Gradient-based editing of memory examples for online task-free continual learning},\n year = {2020}\n}\n\n@article{kirkpatrick-2016-overcoming,\n abstract = {The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.},\n arxiv_url = {https://arxiv.org/pdf/1612.00796v2.pdf},\n author = {James Kirkpatrick and Razvan Pascanu and Neil Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},\n doi = {10.1073/pnas.1611835114},\n title = {Overcoming catastrophic forgetting in neural networks},\n year = {2016}\n}\n\n@article{lee-2020-neural,\n abstract = {Despite the growing interest in continual learning, most of its contemporary\nworks have been studied in a rather restricted setting where tasks are clearly\ndistinguishable, and task boundaries are known during training. However, if our\ngoal is to develop an algorithm that learns as humans do, this setting is far\nfrom realistic, and it is essential to develop a methodology that works in a\ntask-free manner. Meanwhile, among several branches of continual learning,\nexpansion-based methods have the advantage of eliminating catastrophic\nforgetting by allocating new resources to learn new data. In this work, we\npropose an expansion-based approach for task-free continual learning. Our\nmodel, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a\nset of neural network experts that are in charge of a subset of the data.\nCN-DPM expands the number of experts in a principled way under the Bayesian\nnonparametric framework. With extensive experiments, we show that our model\nsuccessfully performs task-free continual learning for both discriminative and\ngenerative tasks such as image classification and image generation.},\n arxiv_url = {https://arxiv.org/pdf/2001.00689v2.pdf},\n author = {Soochan Lee and Junsoo Ha and Dongsu Zhang and Gunhee Kim},\n title = {A neural dirichlet process mixture model for task-free continual learning},\n year = {2020}\n}\n\n@article{liu-2017-stein,\n abstract = {Stein variational gradient descent (SVGD) is a deterministic sampling\nalgorithm that iteratively transports a set of particles to approximate given\ndistributions, based on an efficient gradient-based update that guarantees to\noptimally decrease the KL divergence within a function space. This paper\ndevelops the first theoretical analysis on SVGD, discussing its weak\nconvergence properties and showing that its asymptotic behavior is captured by\na gradient flow of the KL divergence functional under a new metric structure\ninduced by Stein operator. We also provide a number of results on Stein\noperator and Stein's identity using the notion of weak derivative, including a\nnew proof of the distinguishability of Stein discrepancy under weak conditions.},\n arxiv_url = {https://arxiv.org/pdf/1704.07520v2.pdf},\n author = {Qiang Liu},\n title = {Stein variational gradient descent as gradient ﬂow},\n year = {2017}\n}\n\n@article{liu-2019-understanding,\n abstract = {It is known that the Langevin dynamics used in MCMC is the gradient flow of\nthe KL divergence on the Wasserstein space, which helps convergence analysis\nand inspires recent particle-based variational inference methods (ParVIs). But\nno more MCMC dynamics is understood in this way. In this work, by developing\nnovel concepts, we propose a theoretical framework that recognizes a general\nMCMC dynamics as the fiber-gradient Hamiltonian flow on the Wasserstein space\nof a fiber-Riemannian Poisson manifold. The \"conservation + convergence\"\nstructure of the flow gives a clear picture on the behavior of general MCMC\ndynamics. The framework also enables ParVI simulation of MCMC dynamics, which\nenriches the ParVI family with more efficient dynamics, and also adapts ParVI\nadvantages to MCMCs. We develop two ParVI methods for a particular MCMC\ndynamics and demonstrate the benefits in experiments.},\n arxiv_url = {https://arxiv.org/pdf/1902.00282v3.pdf},\n author = {Chang Liu and Jingwei Zhuo and Jun Zhu},\n title = {Understanding mcmc dynamics as ﬂows on the wasserstein spac},\n year = {2019}\n}",
  "was_experiment_executed": true,
  "is_better_than_baseline": false,
  "paper_review_scores": {
    "novelty_score": 8,
    "significance_score": 7,
    "reproducibility_score": 3,
    "experimental_quality_score": 2
  }
}