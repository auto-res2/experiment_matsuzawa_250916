{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "accelerate GAT training",
    "GAT training optimization",
    "efficient graph attention",
    "parallel GAT computation",
    "sparse GAT speedup"
  ],
  "research_study_list": [
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
      "title": "Accelerating Transformer Pre-training with 2:4 Sparsity"
    },
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost"
    },
    {
      "title": "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening"
    },
    {
      "title": "Efficient Attention via Control Variates"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "GOAT: A Global Transformer on Large-scale Graphs"
    },
    {
      "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
      "title": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness"
    },
    {
      "title": "Even Sparser Graph Transformers"
    }
  ]
}