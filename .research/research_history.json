{
  "research_topic": "KVキャッシュ量子化 × 精度保持を改善したい（2bit域）",
  "queries": [
    "KV-cache quantization",
    "2-bit transformer quantization",
    "cache quantization accuracy",
    "quantization error compensation",
    "mixed-precision KV cache"
  ],
  "research_study_list": [
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization"
    },
    {
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models"
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"
    },
    {
      "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers"
    },
    {
      "title": "BiT: Robustly Binarized Multi-distilled Transformer"
    },
    {
      "title": "BiT: Robustly Binarized Multi-distilled Transformer"
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks"
    },
    {
      "title": "Binarized Neural Machine Translation"
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization"
    },
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
    },
    {
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators"
    },
    {
      "title": "StepbaQ: Stepping backward as Correction for Quantized Diffusion Models"
    },
    {
      "title": "Soft then Hard: Rethinking the Quantization in Neural Image Compression"
    },
    {
      "title": "PD-Quant: Post-Training Quantization Based on Prediction Difference Metric"
    },
    {
      "title": "ERQ: Error Reduction for Post-Training Quantization of Vision Transformers"
    },
    {
      "title": "Outlier-aware Slicing for Post-Training Quantization in Vision Transformer"
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
    },
    {
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models"
    },
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization"
    }
  ]
}