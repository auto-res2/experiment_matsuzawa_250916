{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "GAT 学習 加速",
    "Graph Attention Network 高速化",
    "Efficient GAT training",
    "Sparse Attention GAT",
    "GAT quantization"
  ],
  "research_study_list": [
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"
    },
    {
      "title": "$\\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"
    },
    {
      "title": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers"
    },
    {
      "title": "Robust Graph Representation Learning via Neural Sparsification"
    },
    {
      "title": "Even Sparser Graph Transformers"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Degree-Quant: Quantization-Aware Training for Graph Neural Networks"
    },
    {
      "title": "$\\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks"
    },
    {
      "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing"
    },
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    }
  ]
}