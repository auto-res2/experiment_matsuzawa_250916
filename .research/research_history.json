{
  "research_topic": "KVキャッシュ量子化 × 精度保持を改善したい（2bit域）",
  "queries": [
    "transformer KV cache quantization",
    "2-bit transformer quantization",
    "post-training 2-bit quantization",
    "accuracy-preserving quantization",
    "mixed-precision KV cache quantization"
  ],
  "research_study_list": [
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
      "abstract": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
      "full_text": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Coleman Hooper1 Sehoon Kim1 Hiva Mohammadzadeh1 Michael W. Mahoney1,2,3 Yakun Sophia Shao1 Kurt Keutzer1 Amir Gholami1,2 1University of California, Berkeley 2ICSI 3LBNL {chooper, sehoonkim, hiva, mahoneymw, ysshao, keutzer, amirgh}@berkeley.edu Abstract LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facili- tates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional em- bedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quan- tization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quanti- zation, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantiza- tion on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ∼1.7× speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant. 1 Introduction Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [6], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [2, 30], as well as in academia [6, 22]. Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [ 17]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [12]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2401.18079v6  [cs.LG]  28 May 202516% 84%98% 2% Long Sequence LengthsKV Cache is the bottleneckShort sequence length Weightsare the bottleneck BaselineKVQuant int3 +Grouping 10.87 Pre-RoPEKeyQuantizationNon-UniformQuantization5.755.94 1%Outliers6.23 Per-Channel KeyQuantization7.05 Perplexity on Wikitext2fp16 Baseline Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in4.8× reduction in cached activation memory footprint. sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [ 18, 17]. However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference. It is therefore crucial to develop methods for compressing the KV cache to enable efficient long- sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1): • We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2). • We find that existing uniform and non-uniform quantization methods result in sub-optimal quanti- zation signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3). • Even with the above, we find that outlier values in cached KV activations can significantly de- grade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only 1% of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with 4.8× longer context length. • We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to ∼1.7× speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization. 22 Background 2.1 LLM Inference When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the Key-Value (KV) cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with n layers and h attention heads with dimension d that is stored using e bytes per element, the KV cache size for batch size b and sequence length l is 2 · n · h · d · e · b · l, meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process. 2.2 LLM Quantization There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21, 9, 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7, 9, 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9, 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17, 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat [8], or derived quantization signposts using a sensitivity-weighted k-means approach [17]. Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization. There has also been work on quantizing both weights and activations (including KV cache) [41, 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34, 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [34] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance [24]. One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size [26]. 2.3 KV Cache Compression There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25, 43, 11, 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [32]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference. 3Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (pre-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (post-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Layer 10 Values Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens. 3 Method 3.1 Per-Channel Key Quantization To inform our approach, we first performed a detailed analysis to understand the KV cache distribu- tions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7, 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels). Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [ 34, 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per- token quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix- vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation. Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping. 3.2 Pre-RoPE Key Quantization One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 [35]. Given Query and Key vectors Qm = Wq ∗ xm and Kn = Wk ∗ xn at positions m and n in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain ˜Qm = Rd θ,m · Qm and 4˜Kn = Rd θ,n · Kn. This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache ˜Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize Kn) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7). 3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In [17], the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offline on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range [−1, 1] prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation A, we formulate the error minimization objective as follows, where A is flattened to one dimension and where N is the number of elements from all of the samples in our calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 . (1) We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [8], demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). 3.4 Per-Vector Dense-and-Sparse Quantization As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [17], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel 5may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer. Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range [−1, 1], and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing 1% of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline. 3.5 Attention Sink-Aware Quantization Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [42]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a “sink”. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work [ 23]. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization. 3.6 Offline Calibration versus Online Computation A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance. 3.7 Kernel Implementation In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed 6Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models. Method LLaMA-7B LLaMA-13B LLaMA-30B LLaMA-65B PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) baseline 5.68 64.0 5.09 100.0 4.10 195.0 3.53 320.0 int4 5.98 16.0 5.32 25.0 4.34 48.8 3.73 80.1 nf4 5.87 16.0 5.23 25.0 4.25 48.8 3.63 80.1 ATOM-4bit 5.77 16.6 5.16 26.0 4.16 50.7 3.57 83.1 FlexGen-4bit 5.73 17.3 5.14 27.0 4.14 52.6 3.56 86.3 KVQuant-4bit 5.72 16.0 5.13 25.0 4.13 48.8 3.55 80.0 KVQuant-4bit-1% 5.69 17.3 5.10 27.0 4.11 52.7 3.54 86.5 int3 10.87 12.0 8.69 18.8 6.82 36.6 6.37 60.1 nf3 7.33 12.0 6.21 18.8 5.46 36.6 4.44 60.1 ATOM-3bit 6.17 12.6 5.47 19.7 4.44 38.4 3.78 63.0 FlexGen-3bit 5.93 13.2 5.29 20.6 4.26 40.2 3.66 65.9 KVQuant-3bit 5.87 12.0 5.25 18.8 4.25 36.6 3.63 60.0 KVQuant-3bit-1% 5.75 13.3 5.14 20.8 4.15 40.5 3.57 66.5 int2 11779 8.0 69965 12.5 1470 24.4 7272 40.1 nf2 3210 8.0 5786 12.5 2044 24.4 5367 40.1 ATOM-2bit 37.37 8.6 41.77 13.4 16.49 26.1 13.63 42.8 FlexGen-2bit 11.09 9.1 9.84 14.3 6.60 27.8 5.54 45.6 KVQuant-2bit 7.23 8.0 5.82 12.5 4.87 24.4 4.03 40.0 KVQuant-2bit-1% 6.01 9.3 5.36 14.5 4.35 28.3 3.70 46.5 vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either Compressed- Sparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R. 4 Results 4.1 Main Evaluation We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36, 37, 1, 16, 27, 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat [8] without grouping (nfX), as well as (iii) Atom [44] and FlexGen [34]. Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates. Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44, 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining 3.7 ×, 4.8×, and 6.9 × memory savings, respectively). 73.7x4.8x6.8x 3.7x4.8x6.8x Figure 3: Perplexity results for the LLaMA-2-7B-32K model [5] as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths. Table 2: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2- 70B-32K LongLoRA model [6]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual [ 26]. Average bit widths are estimated for each approach assuming 32K context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K. Model Method 2K 4K 8K 16K 32K Avg. Bit Width LLaMA-2-7B-32K fp16 1 1 1 1 1 16 KIVI-2-gs32-r128 0.76 0.72 0.72 0.68 0.7 3.05 nuq4-1% 1 1 1 1 1 4.33 nuq3-1% 0.98 1 1 1 1 3.33 nuq2-1% 1 1 0.98 1 1 2.33 Llama-2-70B-32K fp16 1 1 1 1 1 16 nuq4-1% 1 1 1 1 1 4.35 nuq3-1% 1 1 1 1 1 3.35 nuq2-1% 0.98 0.98 0.96 1 0.74 2.35 4.2 Long Context Length Evaluation Perplexity Evaluation. We evaluated long context length performance using the LLaMA-2-7B- 32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2-70B-32K LongLoRA model [6]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3 [6, 13]. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference. Passkey Retrieval Evaluation. We also evaluated the performance of our quantization method on passkey retrieval to assess the model’s ability to use its context. Passkey retrieval involves evaluating the model’s capacity to locate specific information in long texts [19], and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from [45] (which is based on the methodology from [28]) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model [26], demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window. LongBench Evaluation. Table 3 shows evaluation on LongBench [3] for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning [ 3]. The max input context length is set at 31500, and results using KIVI are also included for reference [26]. Our results demonstrate that our 3-bit 8Table 3: LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit-1%. Com- parisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming 12.2K context length, which was the average number of tokens across all tasks. Config Avg. bitNtrvQAQasperMF-enHotpot2WikiMusiqueGovRepQMSumMNewsTRECTriviQASamSumRBenchLCC PsgRetrPsgCntAvg. fp16 Baseline 16 17.96 10.51 33.43 12.55 12.53 6.19 29.65 16.99 22.15 71 87.79 43.97 59.99 62.14 23 1.50 31.96 KIVI-2-gs32-r128 3.17 19.25 10.66 24.78 12.48 11.19 6.38 27.05 16.36 23.37 71 80.80 43.93 57.74 60.61 13.58 1.50 30.04 KVQuant-3bit-1% 3.33 18.87 13.67 30.93 12.07 12.55 6.25 27.10 16.53 16.54 71 87.55 43.95 59.50 61.52 19.5 1.75 31.21 Table 4: RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning. Config Avg. bit Niah1 Niah2 Niah3 MKey1MKey2MKey3MValueMQueryVT CWE FWE QA1 QA2 Avg. fp16 Baseline 16 100 99.8 98.6 94 68.2 11 55.95 64.5 37.88 9.64 30.4 31.6 31.6 56.40 KIVI-2-gs32-r128 3.05 76 85.6 59.6 72.6 11.4 0 34.7 46.45 39.6 8.26 30.53 24.8 27.6 39.78 KVQuant-3bit-1% 3.33 99.8 98.8 95.2 92.8 61.6 6.4 47.5 54.45 41.04 8.52 29.33 31.0 31.0 53.65 KVQuant-2bit-1% 2.33 95.4 86.8 49.8 73.6 23.4 0 16.65 22.95 22.52 5.14 24.0 26.4 28.4 36.54 model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level. RULER Evaluation. Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite [15] using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves 14% better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with 1.5× smaller bit-width. 4.3 Joint Weight and KV Cache Quantization Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [ 17]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4-1% for the LLaMA- 7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods. 4.4 Performance Analysis and Memory Savings Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve 1.2-1.6× and 1.3-1.7× latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths. Appendix A highlights the benefits of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides 8× KV cache compression and enables serving the quantized LLaMA-7B model with a context length of 1M tokens on a single A100 GPU, as well as enabling serving the LLaMA-7B model with 10M context length on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference 9Table 5: KV cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [17]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [ 17], respectively. See Appendix M for experimental details. Weights KV Cache LLaMA-7B LLaMA-13B Avg. Bits (KV Cache) fp16 fp16 5.68 5.09 16 w4-s45 fp16 5.77 5.17 16 nuq4-1% 5.79 5.18 4.32-4.33 w3-s45 fp16 6.13 5.45 16 nuq3-1% 6.23 5.52 3.32-3.33 Table 6: Average latency (in microseconds) for the Key and Value nuq4-1% kernels, benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model across different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.7 and Appendix R provide additional details for our kernel implementation, Appendix R describes our benchmarking methodology, and Table 22 provides a detailed breakdown of kernel runtime on an A6000 GPU. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key nuq4-1% 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value nuq4-1% 22.1 37.9 124.5 while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. 5 Conclusion As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers) with only 0.1 perplexity degradation across different LLaMA, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings. Acknowledgements The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, Samsung SAIT, POSCO HOLDINGS, and NVIDIA. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer’s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel 10One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. References [1] AI@Meta. Llama 3 model card. 2024. [2] Anthropic. Introducing claude 2.1, Nov 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [9] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [10] Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28–September 1, 2017, Proceedings 23, pages 697–709. Springer, 2017. [11] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [12] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021. [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [14] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023. [15] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. 11[18] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023. [19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [20] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [22] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024. [23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024. [24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantiza- tion aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [28] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [29] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018. [30] OpenAI. New models and developer products announced at devday 2023, Nov 2023. [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza- tion for large language models. arXiv preprint arXiv:2308.13137, 2023. [34] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 12[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [38] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402–17414, 2022. [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. [41] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [42] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023. [43] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. [44] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. [45] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. 13A Memory Bottlenecks for Long Context Length Inference Table 7 shows the model size and KV cache memory requirements for different LLaMA models with different sequence lengths. For short sequence lengths, the model weights are the primary memory bottleneck. However, for longer sequence lengths and larger batch sizes, the KV cache memory is the main bottleneck. This is particularly pronounced when the weights are already quantized to low precision. In our work, we demonstrate that we can help address the KV cache memory bottleneck through low-precision KV cache quantization. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Table 8 shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths, with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides 3.7× KV cache compression (nuq4-1%) and enables serving the quantized LLaMA-65B model with a context length of 32K tokens on a single A100-80GB GPU (requiring 30.3GB for the model weights compressed to 4-bit, and 46.5GB for the KV cache when compressed with nuq2-1%), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with 10M context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. Table 7: Model size and activation memory size estimates for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). By compressing the KV cache to 2-bit precision, we can enable1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. BS Model Model Size (GB) KV Cache Size w/ Diff. Seq Len (GB) 16 → 2-bit 32K 128K 1M 10M (16 → 2-bit) 1 7B 12.6 → 1.6 16 64 512 4883 → 610 13B 24.1 → 3.0 25 100 800 7629 → 954 30B 60.3 → 7.5 49 195 1560 14877 → 1860 65B 121.1 → 15.1 80 320 2560 24414 → 3052 4 7B 12.6 → 1.6 64 256 2048 19531 → 2441 13B 24.1 → 3.0 100 400 3200 30518 → 3815 30B 60.3 → 7.5 195 780 6240 59509 → 7439 65B 121.1 → 15.1 320 1280 10240 97656 → 12207 B Additional Related Works B.1 Outlier-Aware LLM Quantization LLMs have been known to have distinct outliers both in weights and activations [ 7, 9, 17]. SqueezeLLM and SpQR both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [9, 17]. LLM.int8() [7] handled particular outlier channels separately in higher precision, and SmoothQuant [41] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [4, 14, 39, 38]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy benefits given the outlier structure in Keys, and that dense-and-sparse quantization can be efficiently applied for KV cache quantization. 14Table 8: Activation memory size estimates (GB) for 128K, 1M, and 10M sequence length ( l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Model Method l=128K l=1M l=10M LLaMA-7B fp16 64.0 512.0 4882.8 nuq4 16.0 128.1 1221.9 nuq4-1% 17.3 138.4 1319.6 nuq3 12.0 96.1 916.7 nuq3-1% 13.3 106.4 1014.4 nuq2 8.0 64.1 611.5 nuq2-1% 9.3 74.4 709.2 LLaMA-65B fp16 320.0 2560.0 24414 nuq4 80.0 640.3 6106.5 nuq4-1% 86.5 691.5 6595.0 nuq3 60.0 480.3 4580.6 nuq3-1% 66.5 531.5 5069.1 nuq2 40.0 320.3 3054.7 nuq2-1% 46.5 371.5 3543.3 B.2 Non-uniform LLM Quantization Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [17, 8]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [8]. SqueezeLLM [ 17] derived per-channel non-uniform quantization signposts using a sensitivity-weighted k-means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using a sensitivity-weighted k-means approach with KV cache activations. C RoPE Equation The rotation matrix for RoPE is provided in Equation 2, where c and s are cosine and sine functions, θi = 10000−2(i−1)/d, d is the attention head dimension, and n is the current position in the sequence:   c(nθ1) −s(nθ1) ··· 0 0 s(nθ1) c( nθ1) ··· 0 0 ... ... ... ... ... 0 0 ··· c(nθd/2) −s(nθd/2) 0 0 ··· s(nθd/2) c( nθd/2)   (2) The Query vectors computed at each iteration will have RoPE applied (to obtain ˜Qm = Rd θ,m ∗ Qm). When caching Key vectors, we therefore need to either cache ˜Kn = Rd θ,n ∗ Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. In order to apply Rd θ,n efficiently on-the-fly, we leverage the element-wise formulation of RoPE rather than the matrix-multiplication formulation from Equation 2. The element-wise formulation for Rd θ,nx is as follows, where ⊙ is the element- wise multiplication operator (note that the formulation that we use matches the implementation in the Transformers library for LLaMA [ 40], and it is a different but equivalent formulation to the element-wise expression in [35]): 15  x1 x2 ... xd 2 xd 2 +1 ... xd−1 xd   ⊙   c(θ1n) c(θ2n) ... c(θd 2 n) c(θ1n) ... c(θd 2 −1n) c(θd 2 n)   +   −xd 2 +1 −xd 2 +2 ... −xd x1 ... xd 2 −1 xd 2   ⊙   s(θ1n) s(θ2n) ... s(θd 2 n) s(θ1n) ... s(θd 2 −1n) s(θd 2 n)   (3) By leveraging this element-wise implementation, we can apply RoPE on-the-fly to the Key activations (after dequantizing the Key activations and before multiplying them with the corresponding elements in the Query vector). D Derivation for Sensitivity Analysis To compute the sensitivity analysis we largely follow the derivation in [29], which was originally provided to compute sensitivity of a NN based classifier but can be extended with minor modifications for quantization. In particular, the sensitivity measure is based on how much the loss output of the model is perturbed. To compute this we denote activations at a layer before quantization as A, after quantization as AQ, quantization perturbation in activation as A − AQ, and the gradient of the Loss function w.r.t. activation as J(A) = ∂L ∂A (A). By making the assumption that the quantization perturbation of different activations follow a Gaussian distribution with zero mean, we can show that the sensitivity of an activation value is proportional to Fii \u0000 A − Q(A) \u00012 as was given in Equation 1: E∆A h |L(A) − L(A + ∆A)|2 i ≈ E∆A h\u0000 J(A)T ∆A \u00012i = E∆A \"\u0010X i Ji∆Ai \u00112 # = E∆A \"X i J2 i ∆A2 i # = X i J2 i E∆A \u0002 ∆A2 i \u0003 . Here note that we first assume a first order Taylor series expansion to approximate the perturbation to the loss, and then use the zero mean assumption of the quantization perturbation to derive the second line. To approximate the E∆A \u0002 ∆A2 i \u0003 we use empirical evaluation of the expectation by sampling multiple different inputs and empirically computing the resulting quantization perturbation which results in Equation 1. E Derivation for Quantization Error In our work, before applying the sensitivity-weighted K-means to derive quantization signposts, we normalize each element Ai in the flattened activation A. This normalization for Ai involves a shift by a zero-point zi followed by rescaling the quantization signposts by a scaling factor si, where si and zi are the scaling factor and zeropoint corresponding to element Ai: Ai,norm = Ai − zi si , (4) where Ai and Ai,norm are element i from activation A before and after normalization, respectively. We then quantize Ai,norm to Q(Ai,norm) with quantization error ∆Ai,norm. After we dequantized, we rescale each element by si and add zi to get the recovered quantized activation value Q(Ai): 16Q(Ai) = si Q(Ai,norm) + zi. (5) As such, if there is quantization error ∆Ai,norm in Ai,norm, this will be scaled by si in terms of the error in Ai, i.e., ∆Ai = si∆Ai,norm. For activation A which is normalized to Anorm (with corresponding scaling factors si for each element Ai), minimizing the sensitivity-weighted quantization error as expressed in Equation 1 gives us the following expression, which we can minimize using the normalized activations across all N elements from the samples in a calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 (6) = arg min Q NX i=1 Fii \u0010 s2 i \u0000 Ai,norm − Q(Ai,norm) \u00012\u0011 (7) F Key and Value Dynamic Range Figure 4 shows the portion of the elements contained within difference percentages of the dynamic range for both Keys and Values. The majority of values (∼ 99%) are contained in a small portion of the dynamic range, and a small portion of numerical outliers skew the dynamic range that must be represented. This motivates our dense-and-sparse approach which removes numerical outliers and stores them in a separate sparse matrix, thereby restricting the range that needs to be represented in the dense component. 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Keys 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Values t100 t99.99 t99.9 t99 Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision). G Per-Channel Key Quantization Ablations As shown in Table 9, per-channel quantization for Keys and per-token quantization for Values outperforms the standard per-token quantization approach for both Keys and Values, yielding an improvement of 3.82 perplexity for the LLaMA-7B model at 3-bit precision. This demonstrates the 17benefits of per-channel Key quantization to mitigate the large outlier channels in Keys. Note that for all experiments using per-channel quantization, we use an fp16 zeropoint rather than a low-precision zeropoint that is rounded to the nearest integer value. We do this since for some of the Key channels, all of the elements are positive or all of the elements are negative, meaning that the zeropoint will fall outside of the range that is representable by a low-precision integer (and rounding it to the nearest low-precision value can degrade performance). Additionally, we observe that per-channel quantization for Values actually performs worse than per-token quantization. We hypothesize that this behavior is because per-channel Value quantization leads to greater error accumulation in particular output values (since the result of the attention scores multiplied by one channel of the Values will be localized to a single value in the output vector), which leads to greater quantization error at later model layers. Another concurrent work, KIVI [26], observes similar behavior for per-channel Value quantization, which they attribute to the fact that per-token Value quantization confines the error to each token. Assuming that the output is a weighted sum of only a few important tokens (as only a few attention scores are large), a perturbation in these tokens can lead to significant degradation. Per-token Value quantization therefore ensures that the quantization of unimportant tokens does not adversely impact the important tokens. Table 9: Ablation Study: Perplexity comparison of per-token and per-channel quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization, and PC refers to per-channel quantization. Datatype Key Dim. Value Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 int3 PT PT 10.87 12.0 int3 PC PC 223 12.0 int3 PC PT 7.05 12.0 H Pre-RoPE Key Quantization Ablations As shown in Table 10, pre-RoPE Key quantization achieves higher accuracy than post-RoPE quanti- zation, with an improvement of 0.82 perplexity for 3-bit quantization with the LLaMA-7B model. These results show that the rotary positional embeddings make Key quantization more challenging due to mixing pairs of channels with different magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization at low precision. Table 10: Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key quantization for LLaMA-7B (using per-channel Key quantization and per-token Value quantization). Pre-RoPE quantization leads to significant improvement (see Section 3.2 for more details). Datatype Scheme Perplexity KV Cache Size (GB) Seqlen 128K fp16 - 5.68 64.0 int3 post-RoPE 7.05 12.0 int3 pre-RoPE 6.23 12.0 I Sensitivity-Weighted Non-Uniform Quantization Ablations Table 11 shows perplexity evaluation results across different LLaMA, Llama-2, and Mistral models on Wikitext-2 for different datatypes, including nuq3, nuq3 without using sensitivity-weighting, as well as nuq3 with sensitivity-weighting but without accounting for the per-channel scaling factors when performing k-means. We observe particularly noticeable gains relative to uniform quantization for 3- bit and 2-bit quantization, where the benefits of non-uniform quantization are more pronounced due to the reduced precision. These results demonstrate the benefits of our sensitivity-weighted non-uniform quantization approach relative to NormalFloat quantization [8], as we achieve consistent accuracy improvements of up to 0.33 perplexity across different models. These results also demonstrate the 18necessity of our sensitivity-weighting approach in order to derive performant non-uniform datatypes using a k-means based approach. Additionally, we observe distinct benefits when also accounting for the per-channel scaling factors when performing k-means. Table 11: Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization for Keys and per-token quantization for Values (meaning that all configurations are the same as in KVQuant, except for the datatype). We compare against both uniform (int3) and non-uniform (nf3) [8] approaches, as well as with using “unweighted” k-means (i.e., not sensitivity-weighted) and “Fisher-weighted k- means” (without accounting for per-channel scaling factors) to compute the non-uniform quantization signposts. Note that there is slight variation in average bitwidth across models due to the differing hidden dimensions. Results report perplexity with 2K/4K/8K sequence length for LLaMA, Llama-2, and Mistral, respectively. Method LLaMA Llama-2 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 4.76 16 int3 6.23 5.60 5.09 5.18 5.95 5.98 3.26 5.26 3.00-3.02 nf3 6.05 5.42 4.51 3.84 5.55 5.15 3.27 5.13 3.00-3.02 nuq3 (unweighted) 6.84 6.16 5.37 4.57 8.52 7.66 3.67 5.29 3.00-3.02 nuq3 (Fisher-weighted) 6.01 5.34 4.41 3.74 5.49 4.83 3.26 5.03 3.00-3.02 nuq3 (KVQuant) 5.94 5.32 4.34 3.68 5.39 4.82 3.23 4.98 3.00-3.02 J Per-Vector Dense-and-Sparse Quantization Ablations Table 12 shows the performance improvements we observe when isolating a small portion of outliers and storing them in a sparse format. We provide results both with using a single per-matrix outlier threshold, as well as with applying separate outlier thresholds per-vector. In particular, we see greater improvements by employing outlier detection with a different threshold per-channel for Keys and per-token for Values. This provides additional benefits since some values which would be considered outliers for the entire matrix are not actually outliers within a particular channel (so they are not hard to quantize). It is therefore better to directly target the outliers that will skew the quantization range for a particular channel. By removing 1% of outliers using per-vector thresholds, we can achieve an additional 0.19 reduction in perplexity for the LLaMA-7B model at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity. Table 12: Ablation Study: Perplexity comparison of different outlier isolation methods for LLaMA- 7B on Wikitext-2. Per-vector outlier detection allows for significant accuracy improvements relative to per-tensor outlier detection. All experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE). “PV” refers to using per-vector outlier thresholds, and “PM” refers to using a single per-matrix outlier threshold. Datatype % Outliers Outlier Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 nuq3 - - 5.94 12.0 nuq3 0.1% PM 5.89 12.2 nuq3 0.1% PV 5.82 12.2 nuq3 1% PM 5.85 13.3 nuq3 1% PV 5.75 13.3 K Attention Sink-Aware Quantization Ablations Table 13 provides perplexity with and without Attention Sink-Aware quantization across different LLaMA, Llama-2, and Llama-3 models. For all bit widths (4, 3, and 2-bit) and in both dense- only and dense-and-sparse quantization settings, Attention Sink-Aware quantization consistently yields perplexity improvement. Notably, the perplexity gain is more pronounced at lower bit widths 19and without sparsity. We observe particularly significant improvements for Llama-3 models, with perplexity improvements of 0.25 and 0.13 PPL for nuq2-1% on Llama-3-8B and Llama-3-70B, respectively. Table 13: Ablation Study: Perplexity with and without Attention Sink-Aware quantization on Wikitext-2 with various models. Attention Sink-Aware quantization consistently improves perplexity across different models and bit widths, particularly with lower bit widths and without sparsity. Datatype Attention Sink-Aware LLaMA-7B LLaMA-13B Llama-2-7B Llama-2-13B Llama-3-8B Llama-3-70B fp16 - 5.68 5.09 5.12 4.57 5.54 2.59 nuq4 X 5.72 5.14 5.17 4.62 5.64 2.65 nuq4 O 5.72 5.13 5.16 4.60 5.60 2.62 nuq4-1% X 5.69 5.10 5.13 4.59 5.57 2.60 nuq4-1% O 5.69 5.10 5.13 4.58 5.56 2.60 nuq3 X 5.94 5.32 5.39 4.82 6.10 2.99 nuq3 O 5.87 5.25 5.34 4.71 5.84 2.72 nuq3-1% X 5.75 5.15 5.18 4.62 5.67 2.66 nuq3-1% O 5.75 5.14 5.17 4.61 5.64 2.63 nuq2 X 8.47 7.29 11.20 23.34 16.63 6.79 nuq2 O 7.23 5.82 7.03 9.59 7.04 3.49 nuq2-1% X 6.05 5.39 5.47 4.85 6.29 2.95 nuq2-1% O 6.01 5.36 5.41 4.78 6.04 2.82 L Calibration Ablations Figure 5 outlines the accuracy and efficiency challenges which our work addresses in order to enable accurate and efficient KV cache quantization. We demonstrate that we can circumvent the challenges related to efficiently performing online per-channel scaling factor computation by instead calibrating offline for the scaling factor without hurting accuracy. Additionally, we show that we can perform per-token scaling factor and outlier threshold computation efficiently online during inference, thereby enabling accurate per-token quantization without compromising on efficiency. Per-channel Quantization  dim sequence length KeysNew Key Per-token Quantization  dim sequence length ValuesNew ValueChallenge: need to potentially recompute the scaling factor every time a new key is added if we computeitonline Challenge: Need to compute the scaling factor for each incoming token per-channel scaling factor per-token scaling factor Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix L, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation. Table 14 shows accuracy results when using offline calibration for computing the scaling factors for the Keys. For 3-bit quantization, we observe minor accuracy degradation when not employing outlier extraction methods. However, if we remove a small percentage of outliers, then the accuracy with offline calibration is the same as computing the scaling factors online per-channel during evaluation. This demonstrates that when incorporating outlier extraction methods, we are better able to perform offline calibration due to reduced sensitivity to outliers (either to outliers during calibration that exaggerate the quantization range, or to outliers during evaluation that cannot be represented accurately if there weren’t large outliers observed during calibration). 20Table 15 shows the runtime for the topk operation for the LLaMA-7B model (which is required for computing outlier thresholds online). It compares the runtime of the topk operation with the runtime for the QKV projections, finding that the topk runtime is 45% of the matrix-vector operation runtime for a single projection layer. The topk operation can also be performed efficiently on the CPU, so we can actually run this operation in parallel with the subsequent linear layer matrix-vector operations on the GPU (which is possible by computing the Value projection before the Key and Query projections). This allows us to compress the activations dynamically without added runtime overhead, thereby enabling online scaling factor computation for the Value tensors. Table 16 shows how both Fisher information computation and calibration (including k-means) per- layer take only a few minutes for the LLaMA-65B model on a typical server machine. Even if we perform calibration sequentially for each layer, the entire calibration process would take a maximum of 6 hours for the LLaMA-65B model at 4-bit precision. Table 14: Ablation Study: Model accuracy when using offline calibration for Keys with LLaMA-7B. When incorporating outlier detection, offline calibration for Keys is able to perform comparably with online calibration. All nuq3 experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection. Datatype % Outliers Perplexity Perplexity (Online for K) (Offline for K) fp16 - 5.68 5.68 nuq3 - 5.91 5.94 nuq3 1% 5.75 5.75 Table 15: topk runtime on a vector of length 4096 for computing outlier thresholds when using 1% sparsity (compared with the runtime for the QKV matrix multiplications, which are 4096×4096 by 4096 matrix-vector multiplications for the LLaMA-7B model). The runtime is reported on a system with an A6000 GPU and an Intel Xeon Gold 6126 CPU. We find that the runtime for the topk operation is only 45% of the runtime of each matvec operation. Additionally, the topk operation can be performed efficiently on the CPU; we can therefore run this operation in parallel with subsequent linear layer operations on the GPU to compress the activations dynamically without added overhead. Operation Device Outlier % Runtime (ms) QKV Projection GPU - 0.172 topk CPU 1% 0.026 topk GPU 1% 0.088 QKV Projection / topk (Fused) GPU / CPU 1% 0.173 Table 16: Runtime for computing Fisher information as well as for calibration (including k-means) with 16 samples for LLaMA-65B quantization. Runtime for computing Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration (including k-means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown for a single layer. Note that calibration is independent for each layer, so it can be easily parallelized. Operation Runtime (minutes) Computing Fisher Information 2.8 4-bit Calibration Per-Layer (including k-means) 4.5 3-bit Calibration Per-Layer (including k-means) 2.7 2-bit Calibration Per-Layer (including k-means) 1.9 M Additional Experimental Details For our empirical evaluation, we use 16 calibration samples of sequence length 2K from the Wikitext- 2 training set (as well as the corresponding gradients) to derive the per-channel scaling factors and zero-points, and to derive the non-uniform datatypes for both Keys and Values. While we use KVQuant models that are calibrated on the Wikitext-2 dataset for all experiments, in Appendix L, we 21include an additional experiment that demonstrates the robustness of the calibration process to the choice of data. We measured perplexity on both Wikitext-2 and on C4 using a sequence length equal to the maximum context length of the model (2K for LLaMA, 4K for Llama-2, and 8K for Llama-3 and Mistral- 7B). For generative tasks, when processing the input prompt, the Key/Value matrix multiplications are computed using the fp16 Keys and Values, and then they are separately compressed into low precision. For baseline experiments, we use post-RoPE quantization, both since this is required from an efficiency perspective without a dedicated kernel implementation, and because it provides better accuracy when quantizing Keys per-token as shown in Appendix P. We make several assumptions in order to estimate average bit widths and KV cache sizes for different approaches. We compute these estimates assuming a sequence length of 128K (unless otherwise specified). For integer quantization, we assume a low-precision integer offset and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers are assumed for the per-token indices (since we need to support long sequence lengths), and the elements and per-element indices are assumed to be 16-bit. This means that for CSR, the rows are assumed to be 32-bit and the columns and values are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit and the rows and values are assumed to be 16-bit. N Comparison Between Different Datatypes and Sparsity Levels Table 17 shows perplexity across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 using different datatypes. The results highlight that the NUQ datatype generally outperforms both uniformly quantized INT datatype and non-uniformly quantized NF datatype even after they are incorporated with grouping at the expense of increased bit-widths. This performance can be further improved by extracting 0.1% to 1.0% of outliers. Note that NUQ with a sparsity level 1.0% has a similar memory requirement as INT with a group size of 64, but achieves significant perplexity improvement across all models and bit widths. O Full Perplexity Evaluation Tables 18 and 19 show perplexity evaluation across all LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4, respectively. These results demonstrate the benefits of our approach for KV cache compression across different models and bit widths. 22Table 17: Comparison of our NUQ datatype with and without sparsity against other data types on different models using the perplexity (PPL) measured on Wikitext-2. Non-uniform quantization (“nuq”) results are using pre-RoPE per-channel quantization for Keys. “gs64/128” refers to baseline experiments using grouping with group size 64/128. Note that there is a slight variation in average bitwidth across models due to the differing hidden dimensions. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 int4-gs128 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 int4-gs64 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 nf4-gs128 5.77 5.17 4.17 3.58 5.30 4.71 3.16 5.84 2.75 4.83 4.25 nuq4 5.72 5.14 4.14 3.56 5.17 4.62 3.14 5.64 2.65 4.81 4.00-4.02 + 0.1% outliers 5.70 5.12 4.12 3.55 5.15 4.60 3.13 5.63 2.66 4.79 4.04-4.06 + 0.5% outliers 5.70 5.11 4.12 3.54 5.14 4.59 3.13 5.59 2.61 4.78 4.16-4.19 + 1.0% outliers 5.69 5.10 4.11 3.54 5.13 4.59 3.13 5.57 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 int3-gs128 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 int3-gs64 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 nf3-gs128 6.26 5.52 4.54 3.83 6.21 5.43 3.38 7.79 4.74 5.23 3.25 nuq3 5.94 5.32 4.34 3.68 5.39 4.82 3.23 6.10 2.99 4.98 3.00-3.02 + 0.1% outliers 5.82 5.22 4.21 3.62 5.27 4.69 3.19 5.96 2.96 4.91 3.04-3.06 + 0.5% outliers 5.76 5.16 4.16 3.59 5.20 4.65 3.16 5.74 2.69 4.84 3.16-3.19 + 1.0% outliers 5.75 5.15 4.15 3.57 5.18 4.62 3.15 5.67 2.66 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 int2-gs128 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 int2-gs64 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 nf2 3210 5785 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 nf2-gs128 351 141 60.97 31.69 635 642 71.21 1024 3091 253 2.25 nuq2 8.47 7.29 6.08 9.19 11.20 23.34 4.18 16.63 6.79 6.87 2.00-2.02 + 0.1% outliers 6.82 5.72 4.83 4.00 6.38 5.33 3.54 7.97 4.77 5.83 2.04-2.06 + 0.5% outliers 6.24 5.49 4.45 3.80 5.59 4.95 3.33 6.53 3.18 5.28 2.16-2.19 + 1.0% outliers 6.05 5.39 4.41 3.72 5.47 4.85 3.28 6.29 2.95 5.14 2.32-2.35 Table 18: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 ATOM-4bit 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 Flexgen-4bit 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 KVQuant-4bit 5.72 5.13 4.13 3.55 5.16 4.60 3.13 5.60 2.62 4.80 4.00-4.02 KVQuant-4bit-1% 5.69 5.10 4.11 3.54 5.13 4.58 3.13 5.56 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 ATOM-3bit 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 FlexGen-3bit 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 KVQuant-3bit 5.87 5.25 4.25 3.63 5.34 4.71 3.18 5.84 2.72 4.98 3.00-3.02 KVQuant-3bit-1% 5.75 5.14 4.15 3.57 5.17 4.61 3.15 5.64 2.63 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 nf2 3210 5786 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 ATOM-2bit 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 FlexGen-2bit 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 KVQuant-2bit 7.23 5.82 4.87 4.03 7.03 9.59 3.45 7.04 3.49 6.84 2.00-2.02 KVQuant-2bit-1% 6.01 5.36 4.35 3.70 5.41 4.78 3.26 6.04 2.82 5.14 2.32-2.35 23Table 19: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on C4. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 7.08 6.61 5.98 5.62 6.63 6.05 4.97 7.10 5.78 5.71 16 int4 7.40 6.82 6.18 5.75 7.31 6.59 5.12 8.79 14.36 5.91 4.00-4.02 nf4 7.27 6.74 6.10 5.69 7.09 6.45 5.06 7.93 6.58 5.85 4.00-4.03 ATOM-4bit 7.16 6.67 6.02 5.65 6.87 6.20 5.00 7.32 6.03 5.76 4.16 FlexGen-4bit 7.12 6.64 6.00 5.63 6.79 6.15 4.99 7.23 5.84 5.75 4.31 KVQuant-4bit 7.11 6.64 6.00 5.63 6.68 6.08 4.99 7.18 5.80 5.75 4.00-4.02 KVQuant-4bit-1% 7.09 6.62 5.99 5.62 6.64 6.06 4.98 7.12 5.78 5.72 4.32-4.35 int3 12.97 10.95 9.13 8.27 30.14 28.57 16.00 63.75 301 8.84 3.00-3.02 nf3 8.90 7.84 7.43 6.37 14.92 13.75 5.96 68.38 148 7.27 3.00-3.03 ATOM-3bit 7.62 6.93 6.24 5.79 8.00 7.06 5.16 9.00 6.72 6.08 3.15 FlexGen-3bit 7.34 6.78 6.11 5.70 7.29 6.59 5.08 7.92 6.18 5.92 3.30 KVQuant-3bit 7.25 6.72 6.06 5.68 6.89 6.18 5.03 7.43 5.90 5.92 3.00-3.02 KVQuant-3bit-1% 7.13 6.64 6.00 5.63 6.69 6.09 4.99 7.19 5.81 5.76 3.32-3.35 int2 10892 100870 1411 7216 4708 4220 814 2113 1977 477 2.00-2.02 nf2 2850 4680 1617 5190 13081 4176 3217 78331 7616 1102 2.00-2.03 ATOM-2bit 43.49 56.25 21.07 17.05 113 97.04 23.67 135 3734 50.73 2.14 FlexGen-2bit 13.91 13.36 8.49 7.34 35.21 40.40 8.28 50.78 42.27 13.83 2.28 KVQuant-2bit 8.52 7.32 6.67 5.96 9.49 15.36 5.30 10.06 6.45 7.63 2.00-2.02 KVQuant-2bit-1% 7.33 6.78 6.11 5.70 6.96 6.25 5.08 7.60 5.96 6.05 2.32-2.35 24P Post-RoPE Per-Token Quantization Ablation Table 20 shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model with uniform quan- tization, with Keys quantized pre-RoPE and post-RoPE. These results show that post-RoPE Key quantization is superior to pre-RoPE Key quantization when quantizing Keys per-token. This is because when rotating an outlier channel with large average magnitude and another channel with smaller average magnitude together, at some positions in the sequence, part of the magnitude from the outlier channel will be shifted to the smaller channel. This partially mitigates the impact of the outlier channel on skewing the quantization range for some of the tokens in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token Key quantization to serve as a stronger baseline. Table 20: Model accuracy when using pre-RoPE and post-RoPE quantization for LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE quantization is superior when using per-token Key quantization. Therefore, we decided to use these results for baseline comparison with per-token quantization. Datatype Perplexity (Pre-RoPE) Perplexity (Post-RoPE) fp16 5.68 5.68 int4 6.02 5.98 int4-gs128 5.76 5.77 int3 14.68 10.87 int3-gs128 6.28 6.17 Q Experiments on Calibration Data Robustness To evaluate the robustness of our quantization method to the choice of calibration datasets, we measure the perplexity on Wikitext-2 and C4 using different quantized models calibrated with Wikitext-2 and C4. As shown in Table 21, the resulting perplexity numbers remain similar even when the models are calibrated with out-of-domain examples (e.g., calibrated with Wikitext-2 and evaluated on C4, and vice versa), demonstrating the robustness of the calibration process to the choice of data. Table 21: Perplexity (PPL) results on Wikitext-2 and C4 using different quantization schemes, calibrated using Wikitext-2 and C4. Datatype Wikitext-2 C4 Calib. with Wikitext-2 Calib. with C4 Calib. with Wikitext-2 Calib. with C4 4-bit, 1% sparsity 5.69 5.70 7.09 7.09 3-bit, 1% sparsity 5.75 5.75 7.13 7.13 2-bit, 1% sparsity 6.05 6.07 7.38 7.38 R Kernel Implementation Details We implemented 4-bit lookup table-based kernels for matrix-vector multiplication between the Key or Value activations (packed as a lookup table (LUT) plus indices into the LUT per-element) and a full-precision activation vector. These kernels load the compressed Key and Value activations and dequantize them only as needed in order to minimize memory bandwidth utilization. All arithmetic is performed in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform datatype for that particular layer scaled according to the range of activations that need to be represented [8]. When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse Row (CSR) format for storing the outliers for the Keys and Values, we needed to consider how easy it would be to append new vectors. When using CSC format for the Key matrix, we only need to append a single element to the column vector, as well as one new element to the row and value vectors per nonzero element in that new column. If we used CSR format, we would need to insert the new column and value elements in the middle of the existing column and value vectors, and we would 25need to recompute the elements of the row vector. When using CSR format for the Value matrix, we only need to append a single element to the row vector, as well as one new element to the column and value vectors per nonzero element in that new row. If we used CSC format, we would need to insert the new row and value elements in the middle of the existing row and value vectors, and we would need to recompute the elements of the column vector. We therefore used the CSC format for the Key matrices and the CSR format for the Value matrices. One challenge with efficiently processing the sparse matrix-dense vector operation is that the sparsity distribution may be unbalanced. This poses a challenge for efficiently processing the sparse matrix on a GPU as there can be different numbers of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense vector kernel based on [10, 17], which assigns an equal number of nonzeros per thread. This has greater synchronization overhead than assigning a single thread for an entire row or column when processing CSR/CSC matrices, but it leads to a more balanced work assignment between threads. We set the number of threads such that there were 10 nonzero values assigned to each thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations. A second potential challenge is that the quantization dimension for the Keys and Values is not aligned with the reduction dimension for the respective matrix-vector operations. If we used per-channel or per-token lookup tables for dequantization, we would need to load from a different lookup table for each element as we iterate along the reduction dimension. However, since our implementation uses a single per-layer datatype that is rescaled per-vector, we can load a single scaling factor and zero-point for each channel / token and broadcast it across all threads (while doing a table lookup from the shared per-layer datatype, which is duplicated such that each thread can access it efficiently in parallel). The use of a single per-layer datatype that is rescaled per-vector therefore allows for much more efficient GPU kernel implementations in the context of per-channel Key / per-token Value quantization. Table 22 shows a detailed breakdown of kernel runtime, including how much time is spent packing vectors into the compressed format and how much time is spent on the dense and sparse matrix- vector multiplications. For the fp16 baselines, we benchmarked runtime by averaging across 1000 iterations. When benchmarking our dense-and-sparse kernels (both for packing and matrix-vector multiplication), since the runtime of the sparse kernels can vary based on the sparsity pattern, we ran the LLaMA-2-7B-32K model and collected activations at each layer, and we then measured the average runtime across 1000 iterations for each layer in the model. We find that even with 1% sparsity, we can attain significant speedups of up to 1.7× relative to the fp16 matrix-vector multiply kernels, demonstrating how our methodology facilitates efficient inference with a low-precision quantized KV cache. Table 22: Average latency (in microseconds) for the Key and Value dense nuq4 kernels as well as for the sparse kernels (with 1% outliers), benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model. Benchmarking results are reported for different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. We find that our dense-and-sparse approach (even with 1% outliers) provides latency benefits relative to the fp16 baseline, even when accounting for the time to compress activations online. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key (nuq4-1%) Packing 4.5 4.5 4.5 Dense Matvec 13.0 24.1 87.6 Sparse Matvec 8.1 11.2 34.2 Total Latency 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value (nuq4-1%) Packing 4.1 4.1 4.1 Dense Matvec 10.0 17.8 62.0 Sparse Matvec 7.9 15.9 58.2 Total Latency 22.1 37.9 124.5 26S Limitations While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than 100K context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory. 27",
      "meta_data": {
        "arxiv_id": "2401.18079v6",
        "authors": [
          "Coleman Hooper",
          "Sehoon Kim",
          "Hiva Mohammadzadeh",
          "Michael W. Mahoney",
          "Yakun Sophia Shao",
          "Kurt Keutzer",
          "Amir Gholami"
        ],
        "published_date": "2024-01-31T18:58:14Z",
        "pdf_url": "https://arxiv.org/pdf/2401.18079v6.pdf"
      }
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification",
      "abstract": "KV cache stores key and value states from previous tokens to avoid\nre-computation, yet it demands substantial storage space, especially for long\nsequences. Adaptive KV cache compression seeks to discern the saliency of\ntokens, preserving vital information while aggressively compressing those of\nless importance. However, previous methods of this approach exhibit significant\nperformance degradation at high compression ratios due to inaccuracies in\nidentifying salient tokens. In this paper, we present ZipCache, an accurate and\nefficient KV cache quantization method for LLMs. First, we construct a strong\nbaseline for quantizing KV cache. Through the proposed channel-separable\ntokenwise quantization scheme, the memory overhead of quantization parameters\nare substantially reduced compared to fine-grained groupwise quantization. To\nenhance the compression ratio, we propose normalized attention score as an\neffective metric for identifying salient tokens by considering the lower\ntriangle characteristics of the attention matrix. Moreover, we develop an\nefficient approximation method that decouples the saliency metric from full\nattention scores, enabling compatibility with fast attention implementations\nlike FlashAttention. Extensive experiments demonstrate that ZipCache achieves\nsuperior compression ratios, fast generation speed and minimal performance\nlosses compared with previous KV cache compression methods. For instance, when\nevaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of\ncompressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in\naccuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction\nin prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a\n$19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a\ninput length of $4096$.",
      "full_text": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification Yefei He1 Luoming Zhang1 Weijia Wu2 Jing Liu3 Hong Zhou1† Bohan Zhuang1,3† 1Zhejiang University, China 2National University of Singapore, Singapore 3ZIP Lab, Monash University, Australia Abstract KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. Ad- ditionally, the compression process introduces excessive overhead, substantially increasing memory burdens and the generation latency. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for large lan- guage models (LLMs). First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced com- pared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. The quantization bit-width for each token is then adaptively assigned based on their saliency. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and min- imal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by 4.98×, with only a 0.38% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096. 1 Introduction LLMs with the next-token-prediction scheme have achieved remarkable advancements in various text-related tasks, such as language understanding [13, 34, 10], content creation [1, 5, 36], coding [3, 29, 42] and mathematics [33, 23, 35]. In this generation scheme, the forthcoming token interacts with all previous tokens via the attention mechanism [38], where the query, key and value states will be †Corresponding author. Email: zhouhong_zju@zju.edu.cn, bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14256v1  [cs.LG]  23 May 2024calculated for each token. As the past tokens will not be altered, previously computed key and value states can be stored as KV cache to prevent re-computations, significantly improving the generation speed. However, as the batch size and the input context length grows, the stored KV cache emerges as a new memory bottleneck for LLMs. For example, when serving a 175B-parameter LLM [1] with a batch size of 64 and a context length of 4096, the KV cache can occupy 1.2TB of memory space, while the model weights only require 350GB. Meanwhile, the size of KV cache will continue to increase as decoding progresses. Therefore, the compression of KV cache is crucial for the efficient deployment of LLMs. Recent compression methods for KV cache can be broadly categorized into two types. The first type of methods compresses the KV cache uniformly, without considering the significance of individual tokens. To preserve performance, these methods often rely on either high-precision quantization [21] or maintaining recent tokens in full-precision [32], which undoubtedly compromise the compression ratio. Additionally, if salient tokens are not among the most recent ones, such as in information retrieval tasks, it may result in degraded performance. The other type of methods [ 46, 43, 16] compress KV cache adaptively by identifying salient tokens and compresses them separately. This approach aligns with the observation that a minority of tokens contribute the majority of attention scores [41], potentially achieving higher compression ratios than non-adaptive methods. However, current adaptive KV cache compression methods [ 46, 43] use accumulated attention scores as a metric of token saliency, which is insufficient in two aspects. First, accumulated attention scores is inaccurate in identifying important tokens. Due to the presence of attention masks, the attention matrix is a lower triangular matrix. Earlier tokens tend to have larger softmax attention values and more attention scores to be accumulated, as illustrated in Figure 3. Under this metric, the saliency of the most recent tokens can never surpass that of the first token, thereby introducing a bias in determining token saliency. Additionally, to obtain accumulated attention scores, full attention matrices must be explicitly computed and stored, which can be inefficient for serving LLMs. Given an input context length of l, fast attention implementations such as FlashAttention [8, 7] only require O(l) memory by computing attention output in blocks without retaining complete attention matrices. By contrast, storing full attention matrices requires O(l2) memory, and the large number of memory accesses significantly slows down the inference speed, as depicted in Figure 4. Figure 1: Accuracy and efficiency compar- isons across various KV cache compression methods. Data is collected with LLaMA3- 8B model on Line Retrieval dataset. Among these methods, ZipCache achieves the high- est accuracy, generation speed and compres- sion ratio. Details can be found in the sup- plementary material. To address these challenges, we introduce ZipCache, an efficient KV cache compression method that attains ex- ceptionally high compression ratios by accurate salient token identification. Figure 1 presents an overview of latency-accuracy comparisons among ZipCache and diverse KV cache compression methods. We start by designing an efficient quantization baseline for com- pressing the KV cache. To preserve performance, prede- cessor methods [32, 21] employ fine-grained groupwise quantization, which involves independent quantization for a small channel group within each token. However, this method necessitates storing extensive quantization parameters and results in significant memory overhead. By contrast, we introduce a channel-separable quanti- zation scheme that decouples the quantization along channel and token dimensions. This method signifi- cantly reduces the quantization overhead without com- promising performance. To accurately recognize salient tokens, we introduce a new token saliency metric based on normalized attention scores, which alleviates the bias towards earlier tokens that accumulate more val- ues. All tokens, without exception, will be quantized to the target bit-width based on their estimated saliency, boosting the overall compression ratio. Moreover, to ease integration with fast attention implementa- tions, we introduce an efficient approximation of the token saliency metric. This approximation only relies on computing and storing attention scores from a few number of tokens, which we refer to as probe tokens. An effective probe token selection strategy is then introduced to minimize performance loss. As a result, the majority of tokens can benefit from fast attention implementations, significantly enhancing the generation speed. 2In summary, our contributions are as follows: • We establish an efficient channel-separable quantization scheme for KV cache, which significantly reduces the overhead of quantization parameters without compromising performance compared to fine-grained groupwise quantization approach. • We propose an accurate metric for assessing token saliency based on normalized attention scores. All tokens are adaptively quantized according to their assessed saliency, thereby improving the overall compression ratio. • We further develop an efficient approximation method for the token saliency metric that integrates seamlessly with fast attention implementations, enhancing generation speed. • By integrating these three techniques, we present ZipCache, an accurate and efficient framework for KV cache compression. Extensive experiments demonstrate that ZipCache reaches a new state-of-the-art performance for KV cache compression in terms of compression ratio, accuracy and generation efficiency. 2 Related Work 2.1 Model Quantization Quantization is a prevalent technique for compressing deep neural networks by representing model weights and activations with lower numerical bit-widths. This technique can be categorized into two primary approaches based on the necessity of fine-tuning: post-training quantization (PTQ) [26, 17, 14] and quantization-aware training (QAT) [28, 31]. For large language models (LLMs), where fine-tuning can be data- and computation-intensive, PTQ is often the preferred method [40, 11, 45, 27]. In this paper, we also quantize KV cache in a post-training manner. For both approaches, quantization can be implemented at various levels of granularity, including channelwise, tokenwise, and groupwise approach. Typically, a finer quantization granularity involves the independent quantization of smaller parameter groups, which often results in improved performance albeit at the cost of more quantization parameters and increased memory overhead. In the context of LLMs, fine-grained quantization is frequently utilized due to the presence of outliers [22, 45]. However, for KV cache compression, this will greatly reduce the overall compression ratio. Mixed precision quantization [39, 44, 12, 2] allocates varying bit-widths to distinct parts of a model or tensor, enabling a more compact compression. This approach originates from the observation that model components exhibit differing sensitivities to quantization. Consequently, components with low sensitivity can utilize reduced bit-widths without impairing performance. For LLMs, previous studies [46, 43, 30, 18] have shown significant disparities in the importance of tokens, indicating that heavy compression of non-critical tokens has minimal impact on overall performance. This insight highlights the applicability of mixed precision quantization for compressing the KV cache. 2.2 KV Cache Compression While KV cache effectively prevents re-computation and significantly enhances generation speed, its memory footprint is notably substantial with long-context input. To alleviate this, many efforts have been made to reduce the KV cache size. Based on the compression method, these methods can be categorized into two groups: token dropping [46, 16, 30] and KV cache quantization [43, 21, 32]. The former identifies and drops unimportant tokens in the KV cache. For example, H2O [46] only maintain 20% heavy-hitted tokens and 20% recent tokens while evicting the rest. However, discarding tokens permanently erases their information, which proves to be suboptimal for tasks such as retrieval [43]. Conversely, the latter category employs quantization on the cached key and value states, and mixed precision quantization can further be applied once token importance is identified [ 43]. To tackle the outliers present in the KV cache, these methods extract the outlier as full precision [21] or use finer-grained quantization scheme [32], which increases the quantization overhead. In this study, we propose an efficient channel-separable quantization scheme with reduced quantization overhead and strong performance. Additionally, both categories of methods commonly adopt accumulated attention scores as the metric for token importance [46, 43]. However, we observe that this criterion is inaccurate and can result in significant performance deterioration at low bit-widths. In contrast, we achieve superior compression performance by utilizing a more accurate metric for identifying salient tokens. 33 Preliminary 3.1 Attention Block in LLMs Given an input prompt, the generation process of LLMs can be broadly categorized into two distinct phases: the prefill phase, which computes and stores the KV cache for input tokens, and the decoding phase, where new tokens are generated through a next-token-prediction scheme. Given input data X and an attention block with its weight matrices WQ, WK and WV, the prefill phase can be formulated as: Q = XWQ, K = XWK, V = XWV, (1) A = Softmax \u0012QKT √dk \u0013 , O = AV. (2) Here, dk is the dimension of the key, and A refers to the attention scores. K and V will be stored as KV cache. For clarity, we have omitted the output projection. For the decoding phase, given x as the embedding vector of the current token, the query q becomes a vector and the KV cache matrices will be updated as follow: q = xWQ, K = Concat(K, xWK), V = Concat(V, xWV). (3) The attention output are then computed as follows: a = Softmax \u0012qKT √dk \u0013 , o = aV. (4) To ensure clarity and consistency, we introduce notation to define the hyper-parameters used in the paper. Specifically, we denote the batch size as b, the number of attention heads as h, the sequence length as l, and the head dimension as d. 3.2 Model Quantization Uniform quantization is adopted in our study and all experiments. Given a floating-point vector x, it can be uniformly quantized to k-bit as follows: ˆx = QU (x, k) = clip(⌊x s ⌉ + z, 0, 2k − 1) · s. (5) Here, ⌊·⌉ denotes the round operation, s = max(x)−min(x) 2k−1 and z = −⌊min(x) s ⌉ are quantization parameters. It should be noted that the quantization parameters are stored in full-precision, which can lead to significant overhead if the quantization is fine-grained. 4 Method 4.1 A Strong Baseline for KV Cache Quantization Tokenwise quantization, as depicted in Figure 2(b) is prevalent in quantizing large language models (LLMs) due to the distinct representations of individual tokens. However, it has been widely observed, as illustrated in Figure 2(a), that outliers emerge within the channel dimensions of key and value matrices [43, 32], posing challenges for tokenwise quantization. To address this, recent work [32] resorts to groupwise quantization, where outlier channels are processed in distinct groups, as illustrated in Figure 2(c). However, this fine-grained quantization approach introduces excessive memory overhead, thereby significantly impacting the compression ratio. For instance, considering X ∈ Rb×h×l×d as the data to be quantized and a group size of n, tokenwise quantization only results in 2bl quantization parameters, while groupwise quantization would yield 2bhld n quantization parameters. Since these parameters are usually stored in full precision, this overhead would constitute a substantial portion of the storage cost for quantized data. Motivated by depthwise separable convolution [ 19], we introduce an efficient channel-separable tokenwise quantization scheme, which disentangles the channel and token dimensions. As shown in 4𝒔,𝒛∈𝑅𝑙 (b) Tokenwise Quantization (c) Groupwise Quantization (d) Channel-separable  Tokenwise Quantization 𝒄 ∈ 𝑅ℎ𝑑 (a) Visualization of  Key and Value States 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝒔,𝒛∈𝑅𝑙∗ℎ𝑑/𝑛 𝒔,𝒛∈𝑅𝑙 Figure 2: Visualization and different quantization granularities for key and value states. Here, we omit the batch dimension for simplicity. For keys, channel outliers emerge, yet token representations exhibit minimal differences. For values, both channel outliers and distinct token representations exist. Figure 2(d), our approach initiates by normalizing each channel of data X with a scaling factor c. For the i-th channel in X, the normalization process can be formulated as: Xi = Xi ci , where ci = p max(|Xi|). (6) After normalization, each channel is scaled to a closed magnitude, mitigating the influence of outliers during tokenwise quantization. Subsequently, tokenwise quantization can be reliably applied and the scales c are multiplied back to restore the magnitude of each channel. The process of channel-separable tokenwise quantization is summarized in the supplementary material. Within this quantization scheme, the total number of quantization parameters amounts to hd + 2bl, representing a notable reduction compared to groupwise quantization, while effectively balancing the outlier channels and the representation of each token. Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset. Key Cache Quantization Granularity Value Cache Quantization Granularity Quantization Parameters Compression Ratio Acc.(%) / / 0 1× 55.88 Groupwise Groupwise 4bhld/n 3.2× 54.51 Tokenwise Tokenwise 4bl 3.99× 49.81 Channelwise Tokenwise 2hd+ 2bl 4.00× 52.77 Channelwise Channel-separable Tokenwise 3hd+ 2bl 4.00× 54.74 As referred to Figure 2(a), since the differences in token representations are small in key cache, we employ channelwise quantization for the key cache to further reduce overhead and employ channel- separable tokenwise quantization for the value cache. As depicted in Table 1, this configuration yields superior performance with reduced quantization overhead compared with groupwise quantization, thereby establishing a robust baseline for KV cache quantization. 4.2 Accurate Salient Token Identification Adaptive KV cache compression [46, 43, 16] aims to discern the saliency of each token, keeping the information of salient tokens while evicting or aggressively compressing the rest, to achieve a higher compression ratio. These salient tokens, also referred to as \"Heavy Hitters\" [46], are often identified based on accumulated attention scores. Given attention score matrix A ∈ Rl×l, the saliency of token i is estimated by: pi = lX k=1 Ak,i. (7) 5So   there  are   five   eggs 1.00 0.00 0.00 0.00 0.00 0.33 0.67 0.00 0.00 0.00 0.35 0.30 0.35 0.00 0.00 0.04 0.06 0.14 0.76 0.00 0.02 0.02 0.04 0.06 0.86 egs five   are   there   So : 40% salient tokens 1.74 1.05 0.53 0.82 0.86 0.35 0.26 0.18 0.41 0.86 Question: There are 15 trees in the  grove… Let's think step by step… Question: If there are 3 cars… Let's think step by step… Question: Leah had 32 chocolates… Let's think step by step… … Question: Olivia has $23… Let's think step by step… Question: Janet’s ducks lay 16 eggs per  day…How much in dollars does she  make every day at the farmers' market? (a) (b) (c) 𝒑𝒊 ෥𝒑𝒊 Figure 3: (a) A toy example to illustrate accumulated attention scores and normalized attention scores. Initial tokens have larger attention scores and more values to be accumulated. (b) A sample from GSM8k dataset with chain-of-thoughts (CoT) prompting. (c) The probability of each token being selected as a salient token, measured by both accumulated and normalized attention scores. Tokens correspond to the final question are identified as low saliency by accumulated attention scores. Tokens with large saliency values are then considered salient tokens. However, this approach has inherent limitations due to the lower triangular nature of the attention score matrix, as illustrated in Figure 3(a). There are two primary issues. Firstly, earlier tokens benefit from having more values accumulated since the elements above the diagonal are all zero. For instance, in a sequence of length l, the initial token accumulates l positive values, whereas the final token only accumulates one. Secondly, Softmax function converts real numbers into probabilities, so that the earlier rows of the attention matrix tending to have higher values, as fewer numbers are involved in the Softmax calculation. Consequently, the accumulated attention score of the final token will always be smaller than that of the first, which exceeds 1. To address this, previous works, such as H2O [46], always maintain recent caches in full precision. Nevertheless, this solution is suboptimal since recent tokens are not necessarily the most significant ones. To enhance the evaluation of each token’s saliency, we introduce an accurate token saliency metric based on normalized attention scores ˜pi: ˜pi = Pl k=1 Ak,i nnz(A:,i) (8) Here, nnz(A:,i) denotes the number of non-zero elements in the i-th column of A. As evidenced in Figure 3(a), normalizing the accumulated attention scores mitigates the influence of excessively large values in the initial rows of the attention score matrix, thereby delivering a more precise assessment. To validate the efficacy of our new metric, we input a sample from GSM8k dataset with chain-of- thoughts (CoT) prompting to the LLaMA3-8B model and identify saliency of each token by Eq. 7 and Eq. 8, respectively. As depicted in Figure 3(b) and (c), the salient tokens are at the end of the prompt, which correspond to the question for LLM to answer. However, these tokens are identified as low saliency by accumulated attention scores. Under the KV cache compression framework, these tokens would either be discarded or quantized to extremely low bit-width, resulting in a significant performance deterioration. In contrast, our method accurately identifies the salient tokens. Additional experimental results regarding the accuracy of our method will be detailed in Section 5.2. 4.3 Efficient Approximation of Saliency Metric As analyzed in Section 4.2, adaptive KV cache compression requires the explicit computation of full attention scores, as referred to Figure 4(b), which clashes with fast attention implementations like FlashAttention [8, 7, 9]. As shown in Figure 4(c), FlashAttention computes attention outputs in tiles without storing the intermediate attention scores. To reconcile the efficiency of FlashAttention with the substantial compression offered by adaptive KV caching, we devise an effective approximation for Eq. 8 as a measure of token saliency. Specifically, we sample a small group of tokens, designated 6𝐀 = (b) Standard Attention 𝐀 (c) FlashAttention 𝑸 𝑽 𝑶 𝑽 𝐊𝑻 𝑸 𝐊𝑻 𝑶 Memory=𝑶(𝒍𝟐) More memory access & slower Memory=𝑶(𝒍) Less memory access & faster (a) Efficient Saliency Metric with Probe Tokens Input tokens 𝐀𝑝𝑟𝑜𝑏𝑒 Probe tokens (b) Standard Attention (c) FlashAttention Regular tokens Output : Intermediate attention scores Figure 4: (a): Efficient saliency metric only requires attention scores of probe tokens through standard attention, enabling fast computation for the majority of tokens through FlashAttention. (b): In standard attention, full attention scores are computed before deriving the attention output. (c): FlashAttention avoids large attention matrix memory transfers by partitioning input matrices into blocks for incremental computation. as probe tokens, and compute their attention scores Aprobe as follows: Aprobe = Softmax \u0012QprobeKT √dk \u0013 . (9) By substituting Aprobe into Eq. 8, we can approximate the saliency of all tokens. For the remaining non-probe tokens, their attention scores do not have to be computed explicitly, enabling the integration of fast attention implementations to expedite the generation process, as illustrated in Figure 4(a). Table 2: Performance comparisons of various probe strategies. Data is col- lected from LLaMA3-8B model on GSM8k dataset. We quantize 40% salient tokens to 4-bit and the remaining 60% tokens to 2-bit. The proportion of probe tokens is 10%. Probe Strategy Acc.(%) All tokens 52.54 Random tokens 47.46 Special tokens 46.78 Recent tokens 51.10 Random+recent tokens 52.08 However, the positions of the probe tokens will un- doubtedly affects the accuracy of the approximated token saliency and the selection of probe tokens is under explored. In this study, we suggest four strategies for sampling probe tokens: • Random tokens. The probe tokens are randomly sam- pled from all positions. • Special tokens. The special tokens and punctuation tokens will be treated as probe tokens. • Recent tokens. The most recent tokens are selected as probe tokens. • Random+recent tokens. The probe tokens will be di- vided into two parts, one using recent tokens and the other randomly selecting from the remaining tokens. It should be emphasized that our study diverges from prior research [16] in that, rather than directly choosing special or recent tokens as salient tokens, we opt to sample a subset of tokens as \"probes\" to detect the salient ones. As depicted in Table 2, we present a comprehensive comparison of the performance among four distinct sampling strategies. Among the four strategies examined, a hybrid approach that combines recent tokens with randomly selected tokens emerges as the most effective. Unless otherwise specified, this hybrid strategy with 5% recent tokens and 5% random tokens will be employed in our method. 5 Experiment 5.1 Implementation Details Models and datasets. To validate the efficacy of our proposed method, we conduct experiments with three open-source LLMs: Mistral [ 20], LLaMA2 [37] and LLaMA3. These models are evaluated on three challenging benchmarks: GSM8k [6] for math problem solving, HumanEval [4] for code 7generation, and Line Retrieval [25] for data retrieval. To ensure reproducibility, the reported results are obtained using the Language Model Evaluation Harness [15] and LongEval [24]. Quantization and generation settings. We employ mixed precision quantization for KV cache where salient tokens will be quantized to 4-bit while the remaining will be quantized to 2-bit. For both subsets, we apply channelwise quantization for the key cache and channel-separable tokenwise quantization for the value cache. The proportion of salient tokens will be denoted by \"Saliency Ratio\" in the experimental results. During the decoding process, ZipCache adopts a streaming strategy [21] and repeats the compression process for the KV cache whenever 100 new tokens are generated. 5.2 Comparison with SOTA methods 5.2.1 Evaluation on GSM8k We begin our evaluation on GSM8k dataset with chain-of-thoughts (CoT) prompting, and the results are presented in Table 3. This task requires LLM to solve mathematical problems and return the final answer without multiple options. This task poses considerable challenges and previous KV cache compression methods manifest notable declines in accuracy. For instance, KIVI [32] shows an accuracy drop of 7.89% on LLaMA3-8B model, indicating the suboptimality of preserving recent tokens in full precision instead of identifying salient ones. Moreover, there is a substantial decrease in accuracy, amounting to 20.4%, for MiKV [43] under the high compression ratio. This suggests that accumulated attention scores mistakenly identify salient tokens, resulting in the loss of vital information during compression. By contrast, the proposed normalized attention scores can accurately measure token saliency, leading to a substantial enhancement in accuracy by 18.27% for LLaMA3-8B models in comparison to MiKV . In comparison to GEAR [21], which quantizes the entire KV cache to 4-bit, our approach additionally quantizes 40% tokens to 2-bit with enhanced performance on Mistral-7B model. This underscores the superiority of accurate adaptive compression of KV cache. Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 41.62 H2O [46] 16/0 40.0% 2.50 × 1.67 GEAR [21] 4/4 100% 3.00 × 39.42 KIVI [32] 16/2 15.2% 3.46 × 39.04 MiKV [43] 4/2 60.0% 4.98 × 36.32 ZipCache 4/2 60.0% 4.98× 41.24 LLaMA2-7B FP16 16/16 100% 1 × 14.18 H2O [46] 16/0 40.0% 2.50 × 13.50 GEAR [21] 4/4 100% 3.00 × 12.96 KIVI [32] 16/2 15.2% 3.46 × 13.19 MiKV [43] 4/2 60.0% 4.98 × 9.02 ZipCache 4/2 60.0% 4.98× 13.50 LLaMA2-13B FP16 16/16 100% 1 × 28.05 H2O [46] 16/0 40.0% 2.50 × 26.00 GEAR [21] 4/4 100% 3.00 × 25.40 KIVI [32] 16/2 15.2% 3.46 × 27.29 MiKV [43] 4/2 60.0% 4.98 × 23.65 ZipCache 4/2 60.0% 4.98× 27.85 LLaMA3-8B FP16 16/16 100% 1 × 55.88 H2O [46] 16/0 40.0% 2.50 × 27.82 GEAR [21] 4/4 100% 3.00 × 49.43 KIVI [32] 16/2 15.2% 3.46 × 47.99 MiKV [43] 4/2 70.0% 4.69 × 35.48 ZipCache 4/2 70.0% 4.69× 53.75 5.2.2 Evaluation on Line Retrival We further evaluate the data retrieval performance of various KV cache compression methods on Line Retrieval [25] dataset, where LLMs are required to retrieve specific content from a record 8of lines using a corresponding line index. The accuracy results under various number of lines are depicted in Figure 5. Notably, all quantization-based compression methods exhibit superior performance compared to the eviction-based approach H2O [ 46]. For eviction-based methods, information is permanently discarded upon eviction, whereas quantization introduces only minor errors while preserving the integrity of the data. Additionally, in comparison to KIVI [32], which always maintains recent caches at full precision, our approach consistently achieves better retrieval accuracy. This can be attributed to the nature of retrieval tasks, where salient tokens may appear at any position within the context, rather than being confined to the most recent caches. Moreover, when compared to MiKV [43], which employs accumulated attention scores as a saliency metric, our method yields a remarkable 42% accuracy improvement when evaluated using 200 lines on the Mistral-7b model. This substantial enhancement once more highlights the effectiveness of normalized attention scores in identifying salient tokens. Additional experimental results on HumanEval [4] can be found in the supplementary material. 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA2-13B Full Cache ZipCache KIVI-2 MiKV H2O 100 200 300 400 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA3-8B Full Cache ZipCache KIVI-2 MiKV H2O 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) Mistral-7B Full Cache ZipCache KIVI-2 MiKV H2O Figure 5: Performance comparisons of various KV cache compression methods on Line Retrieval. 5.3 Generation Efficiency In this subsection, we compare the latency and memory consumption of ZipCache and MiKV [43] under various input lengths, as depicted in Figure 6. Data is collected by serving LLaMA3-8B model on a Nvidia A100 GPU. MiKV employs accumulated attention scores to estimate token saliency, necessitating the use of standard attention for both prefill and decoding phases. Conversely, through an efficient approximate saliency metric, ZipCache requires only the calculation of the attention matrix for 10% of the tokens, while the remaining 90% tokens can be computed using either FlashAttention [7] or FlashDecoding [9]. Consequently, ZipCache achieves faster inference speed and lower memory usage, boasting a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when the input length scales to 4096. (a) Prefill phase latency  (b) Decoding phase latency  (c) GPU memory Figure 6: Comparisons of prefill-phase, decoding-phase latency and memory consumption between MiKV and ZipCache. 96 Conclusion and Future Work In this paper, we have proposed ZipCache, an accurate and efficient mixed-precision quantization framework for compressing KV cache. To commence, we introduce a channel-separable quantization scheme for KV cache, effectively reducing the overhead of storing quantization parameters compared to traditional fine-grained quantization schemes without performance degradation. Additionally, we present a novel metric for accurately assessing token saliency based on normalized attention scores. This metric enables adaptive quantization of all tokens according to their saliency, leading to improved compression ratios without sacrificing model performance. Moreover, we introduce an efficient approximation method for the token saliency metric, seamlessly integrating with fast attention implementations such as FlashAttention and FlashDecoding. This enhancement signifi- cantly boosts generation speed and reduces GPU memory requirements. Our extensive experiments have demonstrated that ZipCache achieves state-of-the-art compression performance in terms of compression ratio, accuracy and generation speed. We believe that ZipCache will pave the way for more practical and scalable deployment of LLMs in various real-world applications. Limitations and Broader Impacts. While ZipCache presents promising advancements in KV cache mixed-quantization frameworks for LLMs, the saliency ratio is manually specified before evaluation and cannot be automatically adjusted based on task datasets. Moreover, similar to other generative models, ZipCache can potentially be used to generate malicious content. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [2] A. Chauhan, U. Tiwari, et al. Post training mixed precision quantization of neural networks using first- order information. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1343–1352, 2023. [3] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023. [6] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. [8] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [9] T. Dao, D. Haziza, F. Massa, and G. Sizov. Flash-decoding for long-context inference, 2023. [10] J. C. de Winter. Can chatgpt pass high school exams on english language comprehension? International Journal of Artificial Intelligence in Education, pages 1–16, 2023. [11] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318–30332, 2022. [12] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293–302, 2019. [13] M. Du, F. He, N. Zou, D. Tao, and X. Hu. Shortcut learning of large language models in natural language understanding. Communications of the ACM, 67(1):110–120, 2023. [14] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [15] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, 10L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In The Twelfth International Conference on Learning Representations, 2024. [17] Y . He, L. Liu, J. Liu, W. Wu, H. Zhou, and B. Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2023. [18] L. Hou, R. Y . Pang, T. Zhou, Y . Wu, X. Song, X. Song, and D. Zhou. Token dropping for efficient bert pretraining. arXiv preprint arXiv:2203.13240, 2022. [19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [21] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. [22] Y . J. Kim, R. Henry, R. Fahim, and H. H. Awadalla. Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms. arXiv preprint arXiv:2308.09723, 2023. [23] C. Li, W. Wang, J. Hu, Y . Wei, N. Zheng, H. Hu, Z. Zhang, and H. Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024. [24] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, , and H. Zhang. How long can open-source llms truly promise on context length?, June 2023. [25] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, and H. Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [26] Y . Li, R. Gong, X. Tan, Y . Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2020. [27] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [28] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang. Qllm: Accurate and efficient low-bitwidth quanti- zation for large language models. In The Twelfth International Conference on Learning Representations, 2024. [29] J. Liu, C. S. Xia, Y . Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. [30] Z. Liu, A. Desai, F. Liao, W. Wang, V . Xie, Z. Xu, A. Kyrillidis, and A. Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024. [31] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Krishnamoorthi, and V . Chandra. Llm- qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [32] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [33] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.arXiv preprint arXiv:2308.09583, 2023. [34] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018. [35] M. Tan, L. Wang, L. Jiang, and J. Jiang. Investigating math word problems using pretrained multilingual language models. arXiv preprint arXiv:2105.08928, 2021. [36] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [37] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 11[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [39] K. Wang, Z. Liu, Y . Lin, J. Lin, and S. Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8612–8620, 2019. [40] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [41] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [42] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1–10, 2022. [43] J. Y . Yang, B. Kim, J. Bae, B. Kwon, G. Park, E. Yang, S. J. Kwon, and D. Lee. No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024. [44] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y . Wang, M. Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pages 11875–11886. PMLR, 2021. [45] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.Advances in Neural Information Processing Systems, 35:27168–27183, 2022. [46] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2023. 12Appendix A Calculation of Overhead for Different Quantization Schemes Assuming b = 8, hd = l = 4096, and that the KV cache is quantized to4-bit, we proceed to calculate the actual compression ratio for different quantization granularities. For groupwise quantization with a group size of n = 32, the compression ratio Rgroup is given by: Rgroup = 2 × bhld × 16 2 × bhld × 4 +4bhld n × 16 = 3.200 (A) For tokenwise quantization, the compression ratio Rtoken can be calculated as: Rtoken = 2 × bhld × 16 2 × bhld × 4 + 4× bl × 16 = 3.992 (B) For our proposed quantization baseline, the compression ratio Rbaseline is determined by: Rbaseline = 2 × bhld × 16 2 × bhld × 4 + 3× hd × 16 + 2× bl × 16 = 3.995 (C) B Implementation Details of ZipCache In this section, we provide an overview of the channel-separable tokenwise quantization scheme in Algorithm 1. Additionally, we present the process of ZipCache’s prefill phase as described in Algorithm 2, as well as its decoding phase detailed in Algorithm 3. It is worth mentioning that during both the prefill and decoding phases, rather than calculating attention outputs separately for probe tokens and regular tokens followed by merging, FlashAttention [7] is utilized to compute the attention output for all tokens simultaneously. Additionally, attention scores of probe tokens are calculated. By bypassing the substantial memory accesses associated with matrix splitting and merging, this strategy enhances generation speed. Algorithm 1: Channel-separable Tokenwise Quantization (CSTQuant) procedure CSTQuant: Input: data X ∈ Rl×hd, target bit-width k for i ← 0 to hd do ci = p max(|Xi|) Xi = Xi ci // Normalizing each channel of X ˆX =TokenQuant(X, k) // Do tokenwise quantization for i ← 0 to hd do ˆXi = ˆXi × ci // Rescale each channel of X return ˆX C Additional Experimental Results C.1 Accuracy and Efficiency Comparisons of various KV cache compression methods In this section, we present the accuracy and efficiency comparisons of various KV cache compression methods, as presented in Table A. Data is collected by evaluating LLaMA3-8B model on200-line retrieval task with a Nvidia A100 GPU. We use a batch size of 8 and an average input length of 3072. Among these methods, ZipCache achieves the highest accuracy, compression ratio and generation speed. Specifically, in comparison to MiKV [43], which identifies salient tokens through accumulated attention scores, our method achieves a notable 10.0% accuracy improvement by accurately pinpointing salient tokens and a substantial38.0% decrease in prefill latency by integrating FlashAttention [7]. 13Algorithm 2: ZipCache for Prefill Phase procedure ZipCachePrefill: Input: Query states Q, key states K, value states V, saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl // Salient Token Identification Select probe tokens and compute their attention scores Aprobe by Eq. 9 Measure the token saliency ˜p with Aprobe by Eq. 8 // Computing Attention Output with FlashAttention O = FlashAttention(Q, K, V) // Compressing KV Cache Partition key states: Ksalient, Kregular = Split(K, ˜p, r%) Partition value states: Vsalient, Vregular = Split(V, ˜p, r%) Ksalient = ChannelQuant(Ksalient, kh), Vsalient = CSTQuant(Vsalient, kh) Kregular = ChannelQuant(Kregular, kl), Vregular = CSTQuant(Vregular, kl) ˆK = Concat(Ksalient, Kregular) ˆV = Concat(Vsalient, Vregular) // Return Attention Output and Compressed KV Cache return O, ( ˆK, ˆV) Algorithm 3: ZipCache for Decoding Phase procedure ZipCacheDecoding: Input: Query vector q, key vector k, value vector v, KV cache ( ˆK, ˆV), saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl, decoding token index i, probe attention score Aprobe K = Concat(k, ˆK) // Concatenate key cache V = Concat(v, ˆV) // Concatenate value cache o = FlashAttention(q, K, V) // Compute attention output i = i + 1 if i == 100then // Re-compress every 100 tokens Extract K[: −100] and V[: −100] and adaptively compress them with Aprobe Reset i = 0, Aprobe = None else if i >95 or randint(0, 100) < 5 then // probe tokens consists of 5% recent and 5% random tokens. Compute attention scores a of current token by Eq. 4 Aprobe = Concat(a, Aprobe) // Return Attention Output, KV Cache and Attention Scores from Probe Tokens return o, (K, V), Aprobe C.2 Evaluation on HumanEval In this section, we assess the performance of code generation across various KV cache compression methods, as summarized in Table B. Remarkably, ZipCache attains a compression ratio of 4.94× without sacrificing performance when tested with the Mistral-7B model, outperforming predecessor methods. Moreover, when evaluating on LLaMA3-8B model, our approach outperforms KIVI-2 [32] by 7.32% with a significantly higher compression ratio (4.39× vs. 2.55×). It should be noted that the average input length for this task is only119, while KIVI retains the recent 32 tokens in full-precision, thereby considerably diminishing its overall compression ratio. This underscores the advantage of ZipCache over methods that consistently retain information of recent tokens. 14Table A: Accuracy and efficiency comparisons over LLaMA3-8B on the200-line retrieval task. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. 0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 3072. Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Prefill-phase Latency (ms) FP16 16/16 100% 1 × 100 2340.11 H2O [46] 16/0 40.0% 2.50 × 0 4335.01 GEAR [21] 4/4 100% 3.00 × 100 5957.76 KIVI [32] 16/2 8.33% 4.36 × 96 4010.14 MiKV [43] 4/2 80.0% 4.43 × 90 4170.61 ZipCache 4/2 80.0% 4.43× 100 2584.01 Table B: Performance comparisons on HumanEval for code generation. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively.0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 120. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 29.27 H2O [46] 16/0 40.0% 2.50 × 14.63 GEAR [21] 4/4 100% 3.00 × 28.05 KIVI [32] 16/2 26.7% 2.55 × 28.05 MiKV [43] 4/2 60.0% 4.94 × 27.44 ZipCache 4/2 60.0% 4.94× 29.27 LLaMA2-7B FP16 16/16 100% 1 × 14.02 H2O [46] 16/0 40.0% 2.50 × 11.59 GEAR [21] 4/4 100% 3.00 × 13.02 KIVI [32] 16/2 26.7% 2.55 × 11.59 MiKV [43] 4/2 80.0% 4.39 × 10.37 ZipCache 4/2 80.0% 4.39× 12.80 LLaMA3-8B FP16 16/16 100% 1 × 33.54 H2O [46] 16/0 40.0% 2.50 × 15.85 GEAR [21] 4/4 100% 3.00 × 28.66 KIVI [32] 16/2 26.7% 2.55 × 25.61 MiKV [43] 4/2 80.0% 4.39 × 29.88 ZipCache 4/2 80.0% 4.39× 32.93 15",
      "meta_data": {
        "arxiv_id": "2405.14256v1",
        "authors": [
          "Yefei He",
          "Luoming Zhang",
          "Weijia Wu",
          "Jing Liu",
          "Hong Zhou",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T07:37:16Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14256v1.pdf"
      }
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abstract": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
      "full_text": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu * 1 Jiayi Yuan* 1 Hongye Jin 2 Shaochen (Henry) Zhong 1 Zhaozhuo Xu 3 Vladimir Braverman 1 Beidi Chen 4 Xia Hu 1 Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re- computations, significantly increases memory de- mands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straight- forward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element dis- tribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group ele- ments along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we de- veloped a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory (in- cluding model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼3.47× throughput on real LLM infer- ence workload. The source code is available at https://github.com/jy-yuan/KIVI. *Equal contribution . The order of authors is determined by flipping a coin. 1Rice University 2Texas A&M University 3Stevens Institute of Technology 4Carnegie Mellon University. Correspondence to: Zirui Liu <zl105@rice.edu>, Jiayi Yuan <jy101@rice.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Taylor et al., 2022; Yuan et al., 2023; Chuang et al., 2024). However, their deployment is very costly, requiring a large number of hardware accelerators such as GPUs. Given these substantial costs, one natural way to reduce the cost per request is to combine a sufficient number of requests together for batch processing. However, in this batch infer- ence scenario, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, is becoming the new memory and speed bottleneck. This bottleneck becomes more pronounced with larger batch sizes and longer context lengths. For instance, in 540B PaLM, with a batch size of 512 and a context length of 2048, KV cache alone can take 3TB. This is 3 times the size of the model’s parameters (Pope et al., 2023). Also, the GPU SRAM has to load the whole KV cache from the GPU device memory for every token generated, during which the computational cores are idle. Thus, reducing KV cache size in LLMs while maintaining accuracy is important. Existing works towards this problem can be roughly divided into three categories. First, some work suggests reducing the number of heads in KV cache, such as multi-query attention (Shazeer, 2019) and multi-group attention (Ainslie et al., 2023). However, these methods require either training the model from scratch or fine-tuning the existing model. Second, another research line reduces KV cache size by evicting unimportant tokens (Zhang et al., 2023). Third, some other works try to solve this problem from the system perspective, e.g., offloading KV cache (Sheng et al., 2023) or extending virtual memory and paging techniques into the attention mechanism (Kwon et al., 2023). To reduce the size of KV cache, the most simple and ef- fective way is to reduce the total bytes taken by KV cache, namely, quantization. Unlike the well-studied weight quan- tization (Lin et al., 2023; Xiao et al., 2023a; Zhao et al., 2024), to the best of our knowledge, only a few studies applied the vanilla 4bit round-to-nearest quantization to KV cache (Sheng et al., 2023; Zhang et al., 2023; Zhao et al., 2024) due to the streaming nature of KV cache or other complications. There is a lack of in-depth studies that ex- 1 arXiv:2402.02750v2  [cs.CL]  25 Jul 2024KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache per-token  quantization  per-channel  quantization  Figure 1: Definition of per-token and per-channel quantiza- tion. X ∈ Rlprompt×d is key/value cache where lprompt is the number of tokens and d is the number of channels. zX is the zero-point, sX is the scaling factor. plore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we study the element distribution of KV cache. Our analysis suggests: • For key cache, there are a few fixed channels whose magnitudes are very large, which is consistent with previous finding (Lin et al., 2023; Xiao et al., 2023a). Thus, as shown in Figure 1 right, key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In this way, it can confine the error to each individual channel, without impacting the other normal channels. • For value cache, there is no obvious outlier pattern. Al- though value cache has no obvious outlier pattern, we experimentally show that it can only be quantized per- token because it is used to calculate the attention output, which is essentially a value cache mixer. As shown in Figure 1 left, the per-token quantization can confine the error inside each individual token and ensure that the quantization of one token does not adversely impact the others. Based on the above insights, we propose KIVI, a plug-and- play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel key cache quantization, the quan- tization process spans different tokens, which cannot be directly implemented in this streaming setting. Since the number of tokens in key cache can be arbitrary, our key idea is to split key cache into two parts. The first part is the grouped key cache, which contains several groups of tokens and each group has a certain number of tokens. The second part is the residual key cache, which does not have a suffi- cient number of tokens to form a complete group. Similarly, we split value cache into the grouped and residual parts to maintain the accuracy. We only apply group-wise quanti- zation to the grouped key cache and value cache, while the residual key cache and value cache are kept in full precision. The grouped and residual parts can be combined using tiled matrix multiplication when computing attention scores. Our contributions are summarized as follows: • Extensive analysis regarding the outlier patterns and quantization error of KV cache in commonly- used LLMs. Our observations suggest that key cache should be quantized per-channel and value cache should be quantized per-token. We also explain in depth why these caches require different quantization approaches. • A new plug-and-play 2bit KV cache quantization algorithm without any fine-tuning, KIVI, with hardware-friendly implementation. We conduct an extensive evaluation for KIVI with Llama, Mistral, and Falcon on popular generation tasks. KIVI can efficiently compress KV cache to 2bit and bring 2.6× peak memory usage reduction for Llama-2-7B, with little to no accuracy drop. With our efficient system im- plementation, this memory reduction, in return, enables up to 4× larger batch size and brings2.35× ∼3.47× throughput. 2. Background: Attention Inference-Time Workflow The LLM attention inference-time workflow involves two phases: i) the prefill phase, where the input prompt is used to generate KV cache for each transformer layer of LLMs; and ii) the decoding phase, where the model uses and updates KV cache to generate the next token, one at a time. Prefill Phase. Let X ∈ Rb×lprompt×d be the input tensor, where b is the batch size, lprompt is the length of the input prompt, and d is the model hidden size. For convenience, we ignore the layer index here. The key, value tensors can be computed by XK = XWK, XV = XWV , where WK, WV ∈ Rd×d are the key and value layer weight, respectively. After obtaining XK and XV , they are cached in the memory for the ease of decoding. Decoding Phase. Let t ∈ Rb×1×d be the current input token embedding. Let tK = tWK and tV = tWV be the key and value layer output, respectively. We first update KV cache: XK ← Concat(XK, tK), XV ← Concat(XV , tV ), 2KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache then calculate the attention output as: tQ = tWQ, A = Softmax(tQX⊤ K), tO = AXV , (1) where WQ is the weight matrix of the query layer. For ease of illustration, we ignore the attention output layer and the other parts of the inference workflow. Memory and Speed Analysis. The above process is re- peated until a special token indicating the sentence’s con- clusion is reached. Let lgen be the number of generated tokens. From the above analysis, the shape of KV cache is b ×(lprompt + lgen) ×d. To get a sense of the scale, consider the OPT-175B model with a batch size b 512, a prompt length lprompt 512, and an output length lgen 32. The KV cache requires 1.2TB, which is 3.8 times the model weights (Sheng et al., 2023). Besides the memory, the inference speed is also decided by the KV cache size. The GPU needs to load KV cache from GPU main memory to GPU SRAM once for every token generated during which the computa- tional core of the chip is essentially idle (Pope et al., 2023; Kwon et al., 2023). 3. Methodology In scenarios with long contexts or batched inferences, the memory and speed bottlenecks are storing and loading KV cache. The most simple and effective way to alleviate this problem is to reduce the total bytes occupied by KV cache, specifically, quantization. Following this motivation, we first evaluate the performance of the existing quantization method in Section 3.1. Our observations suggest that key and value cache should be quantized along different dimen- sions. We analyze the rationale behind this observation in Section 3.2. Then based on the analysis, we propose KIVI, a new KV cache quantization method along with its streaming data structure, detailed in Section 3.3. 3.1. Preliminary Study of KV Cache Quantization As we analyzed in Section 2, KV cache functions as a streaming data structure, where the new tensor arrives se- quentially. Thus, optimization-based methods like GPTQ (Frantar et al., 2022) are unsuitable for quantizing KV cache due to the overhead. To the best of our knowledge, the most flexible way for quantizing KV cache is the round- to-nearest quantization. The B−bit integer quantization- dequantization process can be expressed as: Q(X) = ⌊X − zX sX ⌉, X′ = Q(X) · sX + zX, where zX = min X is the zero-point, sX = (max X − min X)/(2B −1) is the scaling factor, and ⌊·⌉ is the round- ing operation. Here we ignore the batch size for ease of Table 1: The results of simulated KV cache group-wise quantization with various configurations. The group size is set as 32. C stands for per-channel quantization and T stands for per-token quantization. Please check the whole evaluation in Table 3. Llama-2-13B CoQA TruthfulQA 16bit 66.37 29.53 4bit (K - T, V - T) 66.48 29.51 2bit (K - T, V - T) 52.93 24.98 2bit (K - C, V - C) 2.88 0.74 2bit (K - T, V - C) 2.80 0.26 2bit (K - C, V - T) 63.53 28.60 understanding. As shown in Figure 1, X is quantized along either the token or channel dimension group-wisely. Considering the streaming nature of KV cache, previous studies often apply per-token quantization to both key and value cache since the newly quantized KV cache can be naively added to the existing quantized one along the token dimension (Sheng et al., 2023). While per-channel quanti- zation is non-trivial, we have designed a padding method to implement per-channel quantization to explore its effect on both key and value cache. Setting. In Table 1, we show the results of fake KV cache group-wise quantization with different configurations on the Llama-2-13B model for the CoQA and TruthfulQA tasks. We use a group size of 32 for all configurations. Here fake quantization means we simulate the quantization process by first quantizing KV cache into lower precision and then dequantizing it in the attention layer. For per-channel quan- tization, if the number of tokens is not divided evenly into groups, we add zero-padding to ensure it can be grouped perfectly. In this way, we ensure that all tokens in KV cache are quantized for a fair comparison. The detailed experi- mental setting can be found in Section 4.1. Specifically, we observe that: OB 1. When using the commonly used per-token quanti- zation to both key and value caches, INT4 precision can maintain accuracy. However, reducing it to INT2 results in a notable accuracy drop. OB 2. When value cache is quantized per-channel, the accuracy significantly worsens regardless of how key cache is quantized. OB 3. When using a lower numerical precision such as INT2, the most accurate approach is to quantize key cache per-channel and value cache per-token. 3KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5Absolute Value Llama-2-13B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0Absolute Value Llama-2-13B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5 Llama-2-13B Layer 31 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0 2.5 Llama-2-13B Layer 31 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0 2 4 6 8 10Absolute Value Falcon-7B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5Absolute Value Falcon-7B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.0 2.5 5.0 7.5 10.0 Falcon-7B Layer 20 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5 Falcon-7B Layer 20 Head 0 Value Cache Figure 2: Magnitude of key and value cache for Llama-2-13B and Falcon-7B. We observe (1) for key cache, there are a few channels whose magnitudes are very large. (2) for value cache, there is no obvious outlier pattern. 3.2. Why Key and Value Cache Should Quantize Along Different Dimensions? In Table 1, we observe that quantizing key cache per-channel and value cache per-token to 2bit results in a very small accu- racy drop. Here we analyze why this configuration delivers better accuracy. In Figure 2 we visualize the original KV cache distribution at different layers. We observe that in key cache, some fixed channels exhibit very large mag- nitudes, whereas in value cache, there is no significant pattern for outliers. Analysis of Key Cache. The above observation for key cache aligns with previous findings that certain fixed columns in activations exhibit larger outliers (Dettmers et al., 2022; Lin et al., 2023). The persistence of outliers within each channel means that per-channel quantization can con- fine the quantization error to each individual channel with- out impacting the other normal channels. Thus, Figure 2 explains why key cache should be quantized per-channel. In Table 2 we show key cache relative reconstruction error ∥XK−X′ K XK ∥F , along with the relative attention score error ∥A−A′ A ∥F where A′ = Softmax(tQX ′⊤ K ). We observe that the per-token quantization can lead to almost 5× larger at- tention score error than per-channel quantization, which is consistent with Figure 2. Table 2: The relative error statistics averaged over all layers and all heads Llama-2-13B K Per-Token K Per-Channel Avg. ∥ XK−X′ K XK ∥F 13.67 4.55 Avg. ∥ A−A′ A ∥F 47.00 9.60 Attention sparsity 84.3% V Per-Token V Per-Channel Avg. ∥ XV −X′ V XV ∥F 4.57 3.73 Avg. ∆ 3.55 49.89 Analysis of Value Cache. Unlike key cache, value cache does not show the channel-wise outlier pattern. Furthermore, Figure 2 alone cannot explain OB2, which indicates value cache should only be quantized per-token. This is because Figure 2 implies that errors should be comparable for both per-token and per-channel quantization, given the absence of a clear pattern. As shown in Equation (1), value cache is used to calculate the attention output tO. Instead of analyzing the quantization error of value cache XV , in Table 2 we analyze the relative error ∆ = ∥AXV −AX′ V AXV ∥F with different quantization configurations. Surprisingly, we observe that the per-token quantization error is almost 15× 4KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Q_MatMul Concat Quantization by Channel Quantization by Token MatMul Prefill Phase Decoding Phase* KVCache * we omit the value cache and  attention output calculation Full Precision Tensor Low Precision Tensor Figure 3: The overview of KIVI algorithm. For ease of illustration, we omit the value cache and attention output parts. The detailed pseudo-code is provided in Algorithm 1. Here “Q_Matmul” is the mix-precision matrix multiplication which fuses the dequantization with matrix multiplication at the tiling level. smaller than per-channel quantization, which explains why OB2 happens. The intuition behind this observation stems from the attention sparsity. Equation (1) can be written as: [AXV ]i∗ = lpromptX j=1 Aij[XV ]j∗, (2) where [XV ]j∗ is the j-th row of XV . From Equation (2), the attention output is the weighted summation of value cache across various tokens, with the weights being the attention scores. Since the attention score is highly sparse (Tian et al., 2023), the output is just the combination of value caches of a few important tokens. The per-token quantization can confine the error to each individual token. Thus, quantizing other tokens does not affect the accuracy of important tokens. Consequently, per-token quantization leads to a much smaller relative error ∆. 3.3. KIVI: Algorithm and System Support Algorithm. As we previously analyzed, key cache should be quantized per-channel and value cache should be quan- tized per-token. Recall that key and value cache of newly generated tokens arrive sequentially. From the implemen- tation perspective, per-token quantization aligns well with streaming settings, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel quantiza- tion, the quantization process spans across different tokens, which cannot be directly implemented in the streaming set- ting. As shown in Figure 3, our key idea to solve this prob- lem is to group key cache everyG tokens and quantize them separately. Because the number of tokens in XK can be ar- bitrary, we splitXK into two parts, namely, the grouped part XKg = XK[: l − r] and residual part XKr = XK[l − r :], where l is the number of tokens inside the current key cache XK, r is the number of residual tokens, where l − r can be divisible by G. Since XKg can be evenly divided into (l − r)/G groups, we only store Q(XKg ) with group-wise quantization, while XKr is kept in full precision. During the decoding process, each newly arrived key cache tK is added to XKr and once XKr reaches R tokens, which is a hyperparameter - residual length, we quantize and concatenate it with the previously quantized Q(XKG). Then we reset XKr to an empty tensor. We note that R should be divisible by G. With tiled matrix multiplication, the raw attention logits is then calculated as: Ag = tQQ(X⊤ Kg ), XKr = Concat([XKr , tK]), Ar = tQX⊤ Kr , A = Concat([Ag, Ar]). (3) For value cache, similar to key cache, we also split it into two parts and keep the most recent value cache in full pre- cision, namely, XVg and XVr . Specifically, we maintain a queue and each newly arrived value cache is pushed into the queue. Once the queue reaches the predefined residual length R, the most outdated value cache is poped. Then the poped value cache is quantized per-token and concatenated with the previously quantized value cache along the token dimension. As shown in Figure 3, we also emphasize that during the prefill phase, the exact key and value tensors are passed to the next layers, although only the quantized KV cache is retained in memory. The whole algorithm can be found in Appendix A Algorithm 1. Analysis. In KIVI, the grouped key cacheXKg and value cache XVg is quantized, while the residual key cache XKr and value cache XVr is kept in full precision. By design, there are at most R tokens inside XKr or XVr . In practice, we set R ≤ 128 and the sequence length lprompt + lgen is often much longer than R. Thus the memory overhead from XKr and XVr is negligible when considering the benefit from extreme low-bit quantization, especially for the long context scenarios. Also, since the newly arrived key and 5KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache value tensors are added to XKr and XVr in full precision, KIVI maintains a full precision KV cache sliding window for the local relevant tokens. This window size is expected to be R 2 for key cache, and R for value cache. Later in the experiment section, we show that this full precision sliding window is crucial for obtaining desirable perfor- mance on hard tasks, such as GSM8K. System Support. We provide a hardware-friendly imple- mentation for running KIVI on GPUs. To minimize the overhead, we have fused the dequantization process with matrix multiplication, e.g., Q_MatMul in Figure 3, using CUDA. We also implement the group-wise quantization ker- nel in Triton. Our method is fully compatible with weight- only quantization. 4. Experiments 4.1. Settings Models. We evaluate KIVI using three popular model families: Llama/Llama-2 (Touvron et al., 2023a;b), Fal- con (Penedo et al., 2023) and Mistral (Jiang et al., 2023). Llama and Mistral model is based on multi-head attention, while Falcon is based on multi-query attention (Shazeer, 2019). We use the Hugging Face Transformers codebase and implement the KIVI algorithm upon it. Following previous work (Sheng et al., 2023), the group size G in Algorithm 1 for quantization is set as 32 across all experi- ments, the residual length R for key and value cache is set to 128. Tasks. As we analyzed in Section 2, the KV cache size grows larger with a longer context. Thus, we evaluateKIVI under the normal context length and long context setting, respectively. Specifically, we adopt generation tasks from LM-Eval (Gao et al., 2021) for normal context length eval- uation and LongBench (Bai et al., 2023) for long context evaluation, respectively1. For LM-eval, we adopt CoQA (Exact match accuracy), TruthfulQA (BLEU score), and GSM8K (Exact match accuracy). For LongBench, we chose tasks from four subgroups. Specifically, Qasper (F1 score) is a Single-Document QA task; QMSum (ROUGE score) and MultiNews (ROUGE score) are Summarization tasks; TREC (classification score), TriviaQA (F1 score), and SAM- Sum (ROUGE score) are Few-shot Learning tasks; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. The maximum sequence length in LongBench was set to 8192 for the Mistral model and 4096 for other models. We also consider the needle-in-a-haystack 1The closed-end tasks such as MMLU are not ideal to evaluate KIVI since they only involve one decoding step and directly fetch the output logits, which is not suitable for studying the impact of compressed KV cache. task (NIAH) to evaluate the model’s long context retrieval ability after quantizing KV cache. Detailed NIAH setting can be found in Appendix B. 4.2. Accuracy and Efficiency Analysis 4.2.1. C OMPARISON BETWEEN DIFFERENT QUANTIZATION CONFIGURATIONS We first utilize the fake quantization to demonstrate the effec- tiveness of our asymmetric quantization, namely, quantizing key cache per-channel and value cache per-token. Here fake quantization is exactly the same as in Table 1. The results are shown in Table 3. We observe that “2bit (K per-channel, V per-token)” consistently achieves the best results com- pared to all other configurations. This is consistent with our previous analysis. We also note that for hard generation tasks such as GSM8K, the fake “2bit (K per-channel, V per-token)” quantization results are significantly worse than the full precision counterparts. However, for KIVI in Table 3, we observe that the accuracy drop is only around 2% for GSM8K across different models. As we analyzed in Section 3.3, the difference between fake “2bit (K per-channel, V per-token)” quantization and KIVI is that KIVI maintains a full precision key and value cache sliding window for the local relevant tokens. This sliding window is crucial to maintaining accuracy for hard generation tasks such as mathematical reasoning. 4.2.2. A CCURACY COMPARISON ON GENERATION TASKS LM-Eval Results. We benchmark KIVI in CoQA, Truth- fulQA and GSM8K tasks using the LM-Eval framework. All dataset parameters were set to default. We compare the standard 16bit configuration with our KIVI compression techniques in Llama-2-7B, Llama-2-13B, Falcon-7B and Mistral-7B. As shown in Table 3, we observe that for the Llama and Mistral model, KIVI only has up to 2% accu- racy drop despite the KV cache being stored in 2bit. For instance, in the Llama-2-7B model, the transition from 16bit to 2bit only slightly decreases accuracy. Similar trends are observed in other Llama-family models. Since Falcon-7B adopts multi-query attention and only has one head for KV cache, it is already highly compressed compared to Llama- based models. Thus, in Table 3, 4bit KIVI is needed to maintain the accuracy, while 2bit KIVI may have a large accuracy drop in this case. LongBench Results. The performance of KIVI over vari- ous models in the LongBench dataset is summarised in Table 4. We apply KIVI to Llama2-7B, Llama2-13B, Llama2-7B- Chat, Llama2-13B-Chat, Falcon-7B and Mistral-7B. Table 4 suggests that KIVI is an effective method for KV cache compression with minimal impact on accuracy across var- 6KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens (a) Llama-3-8B-Instruct Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (b) Llama-3-8B-Instruct + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (c) Llama-3-8B-Instruct + KIVI-4 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens (d) Mistral-7B-Instruct-v0.2 Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (e) Mistral-7B-Instruct-v0.2 + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (f) Mistral-7B-Instruct-v0.2 + KIVI-4 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: Needle-in-a-Haystack results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2. Here we count the number of words instead of tokens to better account for the tokenizer difference. The final token length is noted in the upper right corners of each figure. Detailed setting can be found in Appendix B. ious hard long context generation tasks. We present ad- ditional results using Llama3-8b, Mistral-7B-v0.2, and LongChat-7B-v1.5, which can be found in Appendix D. NIAH Results. From Figure 4, we observe that KIVI can still maintain the retrieval ability of LLMs even with 2bit KV Cache. Detailed NIAH setting can be found in Appendix B. 4.2.3. A BLATION In this section, we benchmark KIVI on GSM8K, one of the hardest generation tasks, to show the effect of hyperpa- rameters group size G and residual length R on the model performance. For full results of KIVI with a residual length of 32, please refer to Appendix C. The effect of group size. We fix the residual length at 128 and vary the group sizes to 32, 64, and 128. From Table 5, we observe that group sizes 32 and 64 yield similar results, whereas the performance significantly decreases when the group size reaches 128. Note the zero-point and the scaling factor mentioned in Section 3.1 are calculated according to this group size; where the choice of group size will greatly impact the KV cache compression effect under a long input. The effect of residual length. We fix the group size at 32 and vary the residual length across 32, 64, 96, and 128. As shown in Table 5, there is no consistent pattern between residual lengths and model accuracy. Namely, while a resid- ual length of 128 achieves good results, 32 and 96 yield similar outcomes, but a residual length of 64 results in the worst performance. We emphasize that while we observe no significance among residual lengths of {32, 96, 128}, hav- ing a reasonably large residual length is important; as it brings much performance boosts on hard tasks like GSM8K, again as shown in Table 5. 4.2.4. E FFICIENCY COMPARISON To evaluate the wall-clock time efficiency ofKIVI, follow- ing vLLM (Kwon et al., 2023), we synthesize workloads based on ShareGPT (sha, 2023), which contain input and output texts of real LLM services. On average, the data set has an input prompt length lprompt of 161 and an output length lgen of 338 (Kwon et al., 2023). We increase the batch 7KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 3: Performance comparison between 16bit, 4-bit per- token quantization, four fake 2bit KV cache quantization similar to those in Table 1, KIVI-2 (2bit) / KIVI-4 (4bit) across various models. We emphasize that unlike KIVI, which preserves a small portion of full precision key cache XKr and value cache XVr , all tokens in fake KV cache quantization are quantized for a fair comparison. C stands for per-channel quantization and T stands for per-token quantization. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 4bit (K -T, V -T) 64.82 29.85 12.28 2bit (K -C, V -T) 59.08 33.10 5.76 2bit (K -T, V -T) 39.88 18.29 0.83 2bit (K -C, V -C) 3.60 0.27 0.00 2bit (K -T, V -C) 1.30 0.49 0.08 KIVI-4 63.78 30.80 13.80 KIVI-2 63.05 33.95 12.74 Llama-2-13B 16bit 66.37 29.53 22.67 4bit (K -T, V -T) 66.73 29.14 20.92 2bit (K -C, V -T) 63.53 28.60 12.21 2bit (K -T, V -T) 52.93 24.98 4.55 2bit (K -C, V -C) 2.88 0.74 0.00 2bit (K -T, V -C) 2.80 0.26 0.08 KIVI-4 66.38 29.49 23.65 KIVI-2 66.23 29.84 20.77 Falcon-7B 16bit 59.83 23.20 4.55 4bit (K -T, V -T) 58.53 22.94 3.26 2bit (K -C, V -T) 43.93 20.82 1.29 2bit (K -T, V -T) 25.72 0.91 0.53 2bit (K -C, V -C) 41.95 17.11 1.52 2bit (K -T, V -C) 19.53 0.94 0.15 KIVI-4 59.67 22.58 4.47 KIVI-2 57.48 24.98 3.41 Mistral-7B 16bit 67.40 30.45 38.36 4bit (K -T, V -T) 67.80 29.83 36.85 2bit (K -C, V -T) 61.65 29.64 26.46 2bit (K -T, V -T) 54.55 25.86 5.00 2bit (K -C, V -C) 24.40 24.86 2.27 2bit (K -T, V -C) 10.73 19.12 0.99 KIVI-4 66.95 30.49 37.30 KIVI-2 66.35 32.17 36.01 size until out of memory and report the peak memory usage and throughput between KIVI (with residual length 32 and 128) and FP16 baseline for the Llama-2-7B model. The hardware here is a single NVIDIA A100 GPU (80GB). As shown in Figure 5, with similar maximum memory us- age, KIVI enables up to 4× larger batch size and gives 2.35× ∼3.47× larger throughput. This throughput num- ber can grow larger with longer context length and output length. We also note that this speed-up can be greatly increased if we further fuse the KV cache quantization process with previous operations. We leave it as one of future work. Figure 5: Memory usage and throughput comparison be- tween 2bit KIVI and 16bit baseline. KIVI can achieve higher throughput by enabling a larger batch size. 5. Related Work Many machine learning systems and benchmark works con- sider scaling up LLM inference process (Pope et al., 2023; Yuan et al., 2024). Among them, quantization techniques have been widely applied (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023; Xu et al., 2023). A main branch of LLM quantization is weight-only quantization, which in- volves the quantization of model weights to lower precision. For instance, AWQ (Lin et al., 2023) cleverly quantizes model weights to INT4 and INT3 using an activation-aware manner. GPTQ (Frantar et al., 2022) utilizes approximate second-order information to quantize model weights both accurately and efficiently. SqueezeLLM (Kim et al., 2023) adopts the concept of non-uniform quantization based on sensitivity along with dense-and-sparse decomposition. This line of work is orthogonal to ours, as they can be combined. SmoothQuant (Xiao et al., 2023a) is a post-training quan- tization method that is more closely related to our work. This method uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. SmoothQuant can compress KV cache to 8bit with minor performance loss. However, it faces a significant accuracy drop when scaled down to 4bit or less (Zhao et al., 2024). FlexGen (Sheng et al., 2023) adopts 4-bit group-wise quantization for both key and value cache. 8KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 4: Performance evaluation of KIVI on various models across a range of benchmarks in LongBench. We highlight the average performance of our method. More similar results on Mistral-7B-v0.2 and LongChat-7b-v1.5 can be found in Table 10 and Table 9 Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-4 9.28 21.42 3.88 66.00 87.72 41.82 66.80 59.83 44.59 KIVI-2 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-4 9.16 20.86 3.21 69.00 86.97 44.26 65.30 57.08 44.48 KIVI-2 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-4 19.62 20.70 25.49 63.00 84.13 40.87 59.27 53.56 45.83 KIVI-2 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-4 23.00 20.36 26.06 67.50 87.20 42.04 52.55 52.77 46.44 KIVI-2 23.59 20.76 25.25 67.5 87.17 41.56 49.93 48.45 45.52 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-2 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-4 7.89 20.06 20.58 67.50 89.80 41.56 66.45 58.62 46.56 KIVI-2 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 Table 5: Ablation study of KIVI by changing group size G and residual length R. Model Group Size GSM8K Llama2-13B 32 20.77 64 21.00 128 17.29 Model Residual Length GSM8K Llama2-13B 32 20.62 64 19.86 96 20.55 128 20.77 One essential recipe of KIVI is the per-channel quantiza- tion scheme designed based on the observation made in Section 3.2 and Figure 2. ATOM (Zhao et al., 2024) also indicates that key cache exhibits more outliers compared to the value cache. KIVI provides further extensive analysis and leverages this observation to implement per-channel quantization. A similar observation and approach has been independently discovered and developed in the concurrent work KVQuant (Hooper et al., 2024). vLLM (Kwon et al., 2023) and S3 (Jin et al., 2023) are system-level works, which include memory management through the use of PagedAttention or memory usage pre- diction. They can lower the memory requirements of KV cache and simultaneously increase model throughput. This research direction is orthogonal to our work, since system- level optimizations can also be applied upon our algorithm. Several other works also consider compressing KV cache by evicting tokens. H2O (Zhang et al., 2023) retains only a small portion of tokens that contribute significantly to the attention scores. Similarly, Scissorhands (Liu et al., 2024) exploit the persistence of the importance hypothesis in KV cache sparsification. StreamingLLM (Xiao et al., 2023b) is based on the observation of “attention sink” and maintains only a few initial tokens to preserve performance. Unlike these works, our KIVI retains all input tokens and compresses them into lower precision. This line of work is orthogonal to ours, as they can also be combined together. 6. Conclusion and Future Work In this paper, we systematically analyze KV cache element distribution in popular LLMs. We conclude that key cache should be quantized per-channel and value cache should be quantized per token. Based on these observations, we propose KIVI, a plug-and-play 2bit KV cache quantization algorithm without the need for any tuning. In real LLM workload, KIVI allows up to 4× larger batch sizes and 3.47× throughput. In the future, we will further optimize the implementation to reduce the overhead of quantization process during the prefill and decoding phase. Acknowledgments The authors thank the anonymous reviewers for their help- ful comments. Dr. Beidi Chen is supported by a research gift from Moffett.AI. Dr. Vladimir Braverman is partially 9KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache supported by the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technol- ogy (KIAT) through the International Cooperative R&D pro- gram, the Naval Research (ONR) grant N00014-23-1-2737, and NSF CNS 2333887 award. Dr. Xia Hu is supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References ShareGPT Team. https://sharegpt.com/, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, et al. Understand- ing different design choices in training large time series models. arXiv preprint arXiv:2406.14045, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An- thony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A frame- work for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during genera- tive inference for higher throughput. arXiv preprint arXiv:2306.06000, 2023. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and An- shumali Shrivastava. Scissorhands: Exploiting the per- sistence of importance hypothesis for llm kv cache com- pression at test time. Advances in Neural Information Processing Systems, 36, 2024. Amirkeivan Mohtashami and Martin Jaggi. Landmark at- tention: Random-access infinite context length for trans- formers. arXiv preprint arXiv:2305.16300, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outper- forming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 10KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Ja- cob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans- former inference. Proceedings of Machine Learning and Systems, 5, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit- twieser, et al. Gemini 1.5: Unlocking multimodal un- derstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar- tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and ef- ficient post-training quantization for large language mod- els. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Large language models for healthcare data augmentation: An example on patient-trial matching. In AMIA Annual Symposium Proceedings, volume 2023, page 1324. Amer- ican Medical Informatics Association, 2023. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable ap- proaches. arXiv preprint arXiv:2407.01527, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quanti- zation for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196–209, 2024. 11KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache A. Detailed Implementations In this section, we present the algorithm for KIVI as discussed in Section 3.3. Specifically, we provide the pseudocode for KIVI when calculating the attention output in the prefill and decoding phases. Algorithm 1: The KIVI Prefill & Decoding Algorithm parameter: group size G, residual length R procedure Prefill: Input: X ∈ Rlprompt×d XK = XWK, XV = XWV XVg = XV [: lprompt − R], XVr = XV [lprompt − R :] Q(XVg ) ← GroupQuant(XVg , dim=token, numGroup=d//G) Q(XKg ), XKr ← KeyQuant(XK) KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return XK, XV end procedure Decoding: Input: KV cache, t ∈ R1×d tQ = tWQ, tK = tWK, tV = tWV Q(XKg ), XKr , Q(XVg ), XVr ← KV cache XKr ← Concat([XKr , tK], dim=token) XVr ← Concat([XVr , tV ], dim=token) if len(XKr ) = R then Q(XKr ), _ ←KeyQuant(XKr ) Q(XKg ) ← Concat([Q(XKg ), Q(XKr )], dim=token) XKr ← empty tensor. end if len(XVr ) > Rthen Q(XV ′r ) ← GroupQuant(XVr [: −R], dim=token, numGroup = d//G) Q(XVg ) ← Concat([Q(XVg ), Q(XV ′r )], dim=token) XVr ← XVr [−R :] end A ← Concat([tQQ(XKg )⊤, tQX⊤ Kr ], dim=token) Ag = Softmax(A)[: −R], Ar = Softmax(A)[−R :] tO ← AgQ(XVg ) + ArXVr KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return tO end function KeyQuant(XK ∈ Rl×d): r = l%R, XKg = XK[: l − r], XKr = XK[l − r :] Q(XKg ) ← GroupQuant(XKg , dim=channel, numGroup=l//G) return Q(XKg ), XKr end 12KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache B. NIAH Setting We largely follows the passkey retrieval prompt template of Mohtashami and Jaggi (2023) but using 7-digit passkey and Paul Graham Essays2 as the background filler, as set forth in Arize-ai and Reid et al. (2024): There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. <prefix filled by Paul Graham Essays> The pass key is <7-DIGIT PASS KEY>. Remember it. <7-DIGIT PASS KEY> is the pass key. <suffix filler> What is the pass key? The pass key is C. More Ablation Results In our efficiency evaluation, we observe that with a residual length of 32, KIVI achieves a significantly higher memory compression rate, which in turn leads to increased throughput. Additionally, our ablation study reveals that changing the residual length from 128 to 32 does not result in a substantial performance gap. We demonstrate KIVI with a residual length of 32 across all benchmark datasets. As shown in Tables 6 and 7, KIVI with a residual length of 32 also delivers performance comparable to that of the 16-bit full model. Table 6: Performance comparison between 16bit, KIVI-2 (2bit) / KIVI-4 (4bit) with residual length 128 and 32 across various models. R32 stands for residual length 32. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 KIVI-2 R128 63.05 33.95 12.74 KIVI-2 R32 62.85 33.01 13.57 Llama-2-13B 16bit 66.37 29.53 22.67 KIVI-2 R128 66.23 29.84 20.77 KIVI-2 R32 66.57 29.35 20.62 Falcon-7B 16bit 59.83 23.20 4.55 KIVI-4 R128 59.67 22.58 4.47 KIVI-4 R32 59.73 22.96 3.94 KIVI-2 R128 57.48 24.98 3.41 KIVI-2 R32 57.50 25.70 2.20 Mistral-7B 16bit 67.40 30.45 38.36 KIVI-2 R128 66.35 32.17 36.01 KIVI-2 R32 65.90 31.21 34.34 D. More Experimental Results We present additional results using Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5 in LongBench, which can be found in Table 8, Table 9 and Table 10, respectively. We also show result of Needle-in-a-Haystack Test in Figure 4. The settings largely follow the format of the original passkey retrieval task (Mohtashami and Jaggi, 2023) while including some modern modifications set forward by Arize-ai and the technical report of Gemini 1.5 (Reid et al., 2024). 2https://paulgraham.com/articles.html 13KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 7: Performance evaluation of KIVI with residual length 128 and 32 on various models across a range of benchmarks in LongBench. R32 stands for residual length 32. Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-2R128 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 KIVI-2R32 9.26 20.53 0.97 66.00 87.42 42.61 66.22 59.67 44.08 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-2R128 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 KIVI-2R32 8.38 20.74 7.01 69.50 87.78 44.43 64.89 55.31 44.75 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-2R128 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 KIVI-2R32 19.10 20.08 25.33 63.00 85.04 39.80 57.91 52.38 45.33 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-2R128 23.59 20.76 25.25 67.50 87.17 41.56 49.93 48.45 45.52 KIVI-2R32 23.56 20.90 25.45 67.50 87.42 41.40 48.93 48.81 45.49 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4R128 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-4R32 1.03 2.45 11.99 13.50 5.84 2.46 23.88 9.95 8.88 KIVI-2R128 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 KIVI-2R32 2.28 3.23 6.73 10.00 6.31 2.88 22.71 10.45 8.07 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-2R128 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 KIVI-2R32 6.84 19.81 17.20 66.50 89.63 42.82 65.13 58.06 45.74 Table 8: The results of Llama-3-8B-Instruct with KIVI on LongBench. The model has 8K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.71 44.24 44.54 46.82 21.49 36.42 30.03 22.67 w./KIVI-2 21.35 43.17 44.49 46.79 20.56 37.05 29.98 22.07 w./KIVI-4 21.01 44.83 44.60 46.96 21.43 36.48 30.22 22.44 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.79 57.00 51.22 90.23 42.53 74.50 67.00 45.21 w./KIVI-2 27.77 50.84 46.65 90.54 42.26 74.50 67.50 44.37 w./KIVI-4 27.97 57.36 52.03 90.33 42.97 74.50 66.50 45.31 Table 9: The results of Mistral-7B-Instruct-v0.2 with KIVI on LongBench. The model has 32K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.02 29.41 47.13 36.53 19.13 21.76 32.59 23.99 w./KIVI-2 20.61 28.73 44.88 35.47 17.95 20.68 32.55 23.65 w./KIVI-4 20.97 29.41 46.52 36.25 19.53 21.66 32.97 24.06 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.09 53.49 51.40 86.23 43.04 71.00 89.33 43.54 w./KIVI-2 26.54 53.03 51.16 86.00 43.34 71.00 80.83 42.43 w./KIVI-4 26.89 53.33 51.41 86.23 43.34 71.00 89.42 43.53 14KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 10: The results of LongChat-7B-v1.5-32K with KIVI on LongBench. The model has 32K context length. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 20.65 29.42 43.15 33.05 14.66 24.14 30.85 22.84 w./KIVI-2 20.79 28.69 41.02 32.91 13.82 23.00 30.47 22.59 w./KIVI-4 20.49 28.90 43.24 33.07 14.66 24.86 31.40 22.84 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 26.55 54.83 58.94 83.99 40.75 66.50 30.50 38.72 w./KIVI-2 26.28 54.11 57.62 83.19 41.28 66.50 32.25 38.30 w./KIVI-4 26.52 54.06 58.77 83.88 40.62 67.00 31.50 38.79 15",
      "meta_data": {
        "arxiv_id": "2402.02750v2",
        "doi": "10.13140/RG.2.2.28167.37282",
        "authors": [
          "Zirui Liu",
          "Jiayi Yuan",
          "Hongye Jin",
          "Shaochen Zhong",
          "Zhaozhuo Xu",
          "Vladimir Braverman",
          "Beidi Chen",
          "Xia Hu"
        ],
        "published_date": "2024-02-05T06:06:47Z",
        "pdf_url": "https://arxiv.org/pdf/2402.02750v2.pdf"
      }
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
      "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit.",
      "full_text": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization Tianyi Zhang1, Jonah Yi1, Zhaozhuo Xu2, and Anshumali Shrivastava1,3 1Department of Computer Science, Rice University 2Department of Computer Science, Stevens Institute of Technology 3ThirdAI Corp. {tz21, jwy4, anshumali}@rice.edu, zxu79@stevens.edu May 8, 2024 Abstract Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit. 1 Introduction Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks, including text generation, language translation, and reasoning, without needing specific fine- tuning [25]. These impressive capabilities have empowered LLMs to find applications in numerous domains, such as law [14], education [15], and patient care [35]. However, the high computational demands and the prohibitive deployment costs of LLMs have created significant barriers, hindering their widespread adoption [14, 3]. Particularly, as LLMs move towards larger model size [9] and longer context length [34], they require faster graphics processing units (GPUs), or other special- ized processors, with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs. To accelerate LLM inference, key and value (KV) caching [20] has been proven to be an effective technique without affecting model quality. In autoregressive decoder-only LLMs, KV caching works through trading off memory for computations: the key and value activations of all previous tokens in the current batch are saved in memory to avoid their recomputation for generating the next token. However, since KV cache scales linearly with the number of tokens and batch size, it can quickly overwhelm the memory capacity of existing GPUs under long context or large batch size 1 arXiv:2405.03917v1  [cs.LG]  7 May 2024settings. In addition, since past key and value activations are not shared between sequences (except for maybe a common prompt), reading the KV cache from GPU memory becomes the primary inference bottleneck as opposed to the computation of the attention scores and value activations of the next token [10]. Thus, it is worthwhile to explore techniques for compressing KV cache for two primary benefits: 1. speeding up LLM inference through reducing the amount of memory reads for KV cache, 2. lowering the GPU memory requirements of inference for a given batch size and context length. Existing approaches typically compress KV cache through token eviction [37, 20] or activation quantization [21, 10]. While they can preserve model quality at moderate compression rates (4× compression or 4 bits per floating-point number), model quality quickly deteriorates at high compression rates (16× compression or 1 bit per floating-point number). In this work, we propose Coupled Quantization (CQ), a novel KV cache quantization method that preserves model quality up to 16× compression or 1 bit per floating-point number. Our approach is motivated by the observation that different channels within the same key/value activation embedding are highly inter-dependent. Thus, it is more information efficient to encode multiple channels of a key/value activation embedding than quantizing each channel independently. Existing KV cache quantization methods employ per-channel or per-token quantization strategies, which fail to exploit the inter-dependence between channels and suffer catastrophic model quality degradation at high compression rates. Our proposed method exploits the mutual dependency be- tween channels by jointly quantizing multiple channels and achieves better preservation of model quality than existing approaches in most cases, especially under low bit width settings. We sum- marize our contributions as follows, 1. We empirically observe the phenomenon that different channels within the same key/value activation embedding in an LLM share a high amount of dependency or mutual information, which is a key insight not leveraged by existing KV cache compression approaches. 2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that takes advantage of the reduced entropy of jointly encoding multiple channels. 3. Through extensive experiments, we demonstrate the effectiveness of CQ against the most competitive existing methods. Furthermore, we showcase the ability of CQ in preserving model quality at an extreme KV cache compression level of 1-bit. 2 Background In this section, we introduce the relevant background and context including the KV caching tech- nique, the von Neumann bottleneck of KV cache, and channel-wise quantization. 2.1 LLM Attention and KV Cache Decoder-only transformer-based LLMs employ masked self-attention [32], in which activations of the current token is only dependent on previous tokens and unaffected by future ones. This prop- erty enables training parallelism for the next-token prediction objective, and gives rise to the KV caching technique for efficient inference decoding. Consider the decoding step for thet-th token in a single head of attention in an LLM. The input embedding of thet-th token (a column vector),et, goes through three distinct transformations to become key, query, and value activation embeddings fK(et), fQ(et), fV (et), where the transformationsfK, fQ, fV are composed of linear projection and positional encoding such as RoPE [29]. The output embedding of attention for thet-th token is 2computed as attention(et) = \u0014 fV (e1) . . . fV (et) \u0015 softmax  \u0014 fK(e1) . . . fK(et) \u0015⊤ fQ(et) ! (1) Thus, computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens,fK(ei) and fV (ei) where i ∈ {1, . . . , t− 1}. These embeddings are cached in memory from previous decoding steps asKV cacheto avoid redundant computations and reduce inference latency. The size of KV cache can be calculated asb × n × l × 2 × h × c floating-point numbers, whereb is the batch size,n is the number of tokens in each sequence,l is the number of layers in the model,2 is for key and value,h is the number of key/value attention heads, and c is the number of channels in a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of KV quickly overwhelms limited GPU memory. 2.2 The von Neumann Bottleneck of KV Cache The attention computation in Equation 1 is primarily bottlenecked by GPU memory bandwidth, known as the von Neumann bottleneck, due to low compute-to-global-memory-access ratio. GPUs are able to perform computations significantly faster than reading from global memory, and previous works have shown that attention computation can be accelerated by avoiding unnecessary global memory reads/writes through kernel fusion [5]. During the decoding phase in LLM attention, com- puting the output for the current token requires fetching the cached key and value embeddings of all previous tokens from global memory, resulting in a low ratio of arithmetic operations to global memory reads [10]. Furthermore, each sequence in a batch retains its own KV cache, leading to poor utilization of the parallel processing capabilities of GPUs. Therefore, KV cache compression algorithms can mitigate the von Neumann bottleneck and improve LLM inference efficiency, even if the approach introduces additional computational overheads in decompression or dequantization. As GPU compute cores are mostly stalled by KV cache memory accesses in attention, quantization approaches can effectively reduce memory reads while introducing negligible latency from dequan- tization computations. 2.3 Channel-wise Quantization Existing KV cache quantization methods [10, 21] employ channel-wise quantization for keys and token-wise quantization for values, based on the observation that certain key channels exhibit outlier patterns in magnitude while value channels do not. Channel-wise and token-wise quantization are similar, except the direction along which the quantization centroids are learned. In non-uniform channel-wise quantization, a set of centroids is learned for each channel. SupposeA is a key or value activation matrix, andAi,∗ denotes the i-th channel ofA. Then, non-uniform b-bit channel-wise quantization aims to learn a set of centroidsC⋆ i ⊂ R for each channeli of A independently through the objective C⋆ i = arg min C⊂R |C|=2b \r\r\rAi,∗ − q(Ai,∗) \r\r\r 2 2 (2) where q quantizes each value inAi,∗ to the nearest centroid inC. 31 2 3 4 5 10 15Key Entropy (bits) Layer 1 1 2 3 4 5 10 Layer 2 1 2 3 4 5 10 Layer 3 1 2 3 4 5 10 Layer 4 1 2 3 4 Num. of Joint Channels 5 10Value Entropy (bits) 1 2 3 4 Num. of Joint Channels 5 10 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 Sum of Marginal Entropy Joint Entropy Figure 1: Growth rate of joint entropy versus sum of marginal entropies of the LLaMA-7b key/value activation embeddings on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that jointly quantizing more channels requires fewer bits than quantizing each channel independently. 3 Methodology In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for KV cache compression. 3.1 Motivations Our proposed approach is inspired by concepts in information theory [28]. We consider each channel in a key/value activation embedding as a random variableX1, X2, . . .. The amount of information (oruncertainty)inchannel X canbemeasuredby entropy, definedasH(X) =− R X p(x) log2 p(x) dx, where p(·) is the probability density function andX is the support of X. H(X) measures the theoretical number of bits needed for losslessly encoding the channelX, so it can be used to gauge how “quantizable” a channel is: ifH(X1) < H(X2), then channelX1 may be quantized to fewer bits than channelX2 while achieving the same quantization error. Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channelsX1, X2 is measured by joint entropy, defined as H(X1, X2) = − R X1 R X2 p(x1, x2) log2 p(x1, x2) dx2 dx1, wherep(·, ·) is the joint probability density function. The joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e.,H(X1, X2) =H(X1) +H(X2) − I(X1, X2), whereI(·, ·) is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have H(X1, X2) ≤ H(X1) +H(X2) (3) which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that 41 16 32 LLaMA-7b Key Correlations Layer 1  Layer 2  Layer 13  Layer 14  Layer 28  Layer 29  Layer 31  Layer 32 1 16 32 Channels 1 16 32 LLaMA-7b Value Correlations 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1.0 0.5 0.0 0.5 1.0 Figure 2: Correlation matrices of the first 32 channels of 8 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients. deep neural networks [11] and attention-based networks [7] tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency. It is hence beneficial to measure the difference between the joint entropy of multiple channels and the sum of their marginal entropies in key and value activation embeddings. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is intractable to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the “binning” trick [17] to estimate entropy. We first observe an empirical distribution of key and value channels by saving the KV cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy ofn channels X1, . . . , Xn is estimated with the Riemann sum, H(X1, . . . , Xn) ≈ X x1∈B1 ··· X xn∈Bn bp(x1, . . . , xn) log2 bp(x1, . . . , xn) (4) where Bi is the support of the binned or discretizedXi and bp(·) is the empirical probability density function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b into non- overlapping groups ofc contiguous channels, wherec ∈ {1, 2, 3, 4}, and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1 shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of three layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset, averaged over groups. We only show a maximum group size of 4, since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1, the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding decreases. This phenomenon is the foundation that motivates our proposed approach. In addition to presenting the estimated marginal and joint entropy, we also show the Pearson correlation coefficients between channels of LLaMA-7b key and value activation embeddings on WikiText-2. Correlation coefficient captures the linear dependency between two random variables. Heat maps for the correlation matrices of 8 layers are shown in Figure 2, while the correlation matrices of all layers are presented in Section 6 of the Appendix. The key and value channels 5Channel-wise Quantization Coupled Quantization q0 q1 channel 0 -0.57 0  0.21 1  -0.38 0  -0.13 1  key or value activation embedding channel 1 channel-wise centroids q -0.62 0  0.11 1  -0.38-0.10 0.30 2  -0.38 3  -0.13-0.24 coupled channels 0 & 1 key or value activation embedding multi-channel centroids Figure 3: A comparison of 1-bit channel-wise quantization and our proposed Coupled Quantization (using 2 bits per 2 channels as an example). The quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. Channel-wise quantization is ineffective at capturing the original values at low widths, while CQ leverages the dependency between channels to achieve low quantization errors. exhibit high levels of linear dependency and they are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients. 3.2 Coupled Quantization Motivated by the finding that distinct key/value channels exhibit high dependency, we propose Coupled Quantization (CQ), an information-efficient quantization approach for compressing LLM KV cache. Unlike existing KV cache quantization methods which quantizes channel-wise or token- wise, CQ performs channel-coupled quantization for keys and values. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group arecoupled, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each group of coupled channels are quantized to the nearest centroid in terms of L2 distance. We use the CQ-<c>c<b>b notation to denote the configuration of channel coupling and quantization bit width, where<c> is the number of channels in each group and<b> indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit channel-wise quantization in terms of storage overhead of quantized codes. A illustrative comparison of channel-wise quantization and CQ is shown in Figure 3. Although previous works [10, 21] opt to quantize keys channel-wise and values token-wise, we adopt channel-coupled quantization for both keys and values. Similar to existing approaches [10, 21], CQ quantizes keys before RoPE [29] is applied, which increases the quantization difficulty by introducing more outliers in key activations. 3.2.1 Centroid Learning In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ-c cb b configuration, a set of centroidsC⋆ i ⊂ Rc is learned 61c1b 2c2b 4c4b 8c8b CQ Conﬁg. 101 102 103 Perplexity on WikiText-2 2642.72 1614.02 883.60 32.12 620.08 28.39 10.47 8.095.68 LLaMA-7b 1-bit CQ Uniform CQ Fisher-guided CQ FP16 Baseline 1c2b 2c4b 4c8b CQ Conﬁg. 101 102 Perplexity on WikiText-2 204.56 24.46 6.866.37 6.08 5.97 LLaMA-7b 2-bit CQ 1c1b 2c2b 4c4b 8c8b CQ Conﬁg. 750 1000 1250 1500 1750 2000Quant. Error on WikiText-2 LLaMA-7b 1-bit CQ Uniform CQ Key Uniform CQ Value Fisher-guided CQ Key Fisher-guided CQ Value 1c2b 2c4b 4c8b CQ Conﬁg. 600 800 1000Quant. Error on WikiText-2 LLaMA-7b 2-bit CQ Figure 4: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity. independently for each channel groupi through the objective C⋆ i = arg min C∈Rc |C|=2b \r\r\rAic:(ic+c−1), ∗ − cq \u0000 Aic:(ic+c−1), ∗ \u0001\r\r\r 2 F (5) where Aic:(ic+c−1), ∗ is the sub-matrix ofA containing all coupled channels of thei-th group, and cq quantizes each column vector to the nearest centroid inC in terms of L2 distance. We use the k-means algorithm [22] with k-means++ initialization [1] to optimize the objective. LLMs are more sensitive to the quantization precision of certain weights than others [16]. To better preserve model quality, centroids of CQ should be learned to be biased towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrixF to identify the more influential key/value activations and guide the centroid learning process. This method was proposed by Li et al. [18] and used by Kim et al. [16] for channel-wise weight quantization, and we extend it to multi-channel CQ. For performing Fisher-guided centroid learning, we first save a key/value activation matrixA and its gradient g(A) = ∂ ∂A L(A) on a calibration dataset, whereL is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, which is the element-wise square of the gradient matrixdiag(F) = g(A) ⊙ g(A). We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of channel-coupled activations, and obtain the centroid setC⋆ i for thei-th channel group using the objective C⋆ i = arg min C⊂Rc |C|=2b X j g \u0000 Aic:(ic+c−1), j \u0001⊤g \u0000 Aic:(ic+c−1), j \u0001\r\r\rAic:(ic+c−1), j− cq(Aic:(ic+c−1), j) \r\r\r 2 F (6) We leverage weighted k-means to optimize the objective. The overhead of the centroid learning process and centroid storage are discussed in Section 4.3. We validate the effectiveness of our proposed channel-coupling and Fisher-guided centroid learn- ing by compressing LLaMA-7b KV cache to 1-bit and 2-bit, and present the perplexity results and quantization errors (∥A − cq(A)∥2 F averaged over layers) on WikiText-2 under different CQ con- figurations in Figure 4. The experimental setup is given in Section 4. As the number of coupled channels increases, perplexity and quantization errors improve significantly, approaching the FP16 7Table 1: Perplexity on WikiText-2 under different KV cache quantization methods at varying bit- width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. “NaN” means Not a Number, which is caused by numerical stability issues of quantization. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 5.68 5.09 5.12 4.57 4.76 INT4 4.00-4.01 5.98 5.32 5.66 5.01 4.97 INT4-gs128 4.16 5.77 5.16 5.32 4.71 4.82 NF4 4.00-4.01 5.87 5.23 5.47 4.90 4.91 NF4-gs128 4.16 5.77 5.17 5.30 4.71 4.83 KVQuant-4b 4.00-4.02 5.73 5.15 5.18 4.63 4.81 KVQuant-4b-1% 4.32-4.35 5.70 5.11 5.14 4.59 4.78 CQ-2c8b 4.00 5.70 5.11 5.14 4.59 4.79 INT2 2.00-2.01 11779 69965 4708 3942 573 INT2-gs128 2.14 37.37 41.77 117.88 93.09 51.96 NF2 2.00-2.02 3210.5 5785.6 13601 4035.6 902.51 NF2-gs128 2.14 351.23 141.19 634.59 642.44 252.85 KVQuant-2b 2.00-2.02 8.17 7.29 9.75 29.25 7.33 KVQuant-2b-1% 2.32-2.35 6.06 5.40 5.50 4.92 5.16 CQ-4c8b 2.00 5.97 5.32 5.42 4.81 5.11 KVQuant-1b 1.00-1.02 321.58 1617.40 NaN 4709.83 203.73 KVQuant-1b-1% 1.32-1.35 9.93 7.97 9.50 13.76 10.07 CQ-8c8b 1.00 8.09 7.02 7.75 6.55 7.25 CQ-8c10b 1.25 6.78 6.00 6.25 5.47 5.90 baseline performance. Although Fisher-guided centroid learning increases the quantization error, it better preserves the salient activations and achieve lower perplexity. 4 Experiments In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, metrics, datasets, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware Testbed Experiments are performed on a Linux server running Ubuntu 20.04, equipped with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs. Software ImplementationOur software implementation of CQ is based on PyTorch [24] and the HuggingFace Transformers library [33]. Evaluation Metrics and BenchmarksWe evaluate the quality of 5 popular open-source LLMs on various benchmarks under different KV cache quantization algorithms. The 5 LLMs con- sidered are 1. LLaMA-7b, 2. LLaMA-13b [30], 3. LLaMA-2-7b, 4. LLaMA-2-13b [31], 5. Mistral-7b [13]. We evaluate LLM quality using the perplexity metric on 2 datasets: WikiText-2 [23] and C4 [26], and accuracy on 3 benchmarks in zero-shot setting: WinoGrande [27], PIQA [2], and ARC Challenge [4]. Perplexity is evaluated on the test set of the datasets at the maximum context length 8Table 2: Perplexity on C4 under different KV cache quantization methods at varying bit-width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLama-2-7b LLaMA-2-13b Mistral-7b FP16 16 7.08 6.61 6.63 6.05 5.71 INT4 4.00-4.01 7.40 6.82 7.31 6.59 5.91 INT4-gs128 4.16 7.16 6.67 6.87 6.20 5.76 NF4 4.00-4.01 7.27 6.74 7.09 6.45 5.85 NF4-gs128 4.16 7.16 6.66 6.86 6.20 5.77 KVQuant-4b 4.00-4.02 7.13 6.65 6.70 6.11 5.75 KVQuant-4b-1% 4.32-4.35 7.09 6.62 6.65 6.06 5.72 CQ-2c8b 4.00 7.11 6.64 6.67 6.09 5.74 INT2 2.00-2.01 10892 100870 4708 4220 477 INT2-gs128 2.14 43.49 56.25 113.49 97.04 50.73 NF2 2.00-2.02 2850.1 4680.3 13081.2 4175.6 1102.3 NF2-gs128 2.14 248.32 118.18 420.05 499.82 191.73 KVQuant-2b 2.00-2.02 10.28 9.05 15.16 43.77 8.40 KVQuant-2b-1% 2.32-2.35 7.38 6.83 7.06 6.38 6.08 CQ-4c8b 2.00 7.52 6.96 7.23 6.52 6.17 KVQuant-1b 1.00-1.02 168.90 1316.41 362.94 4223.37 127.07 KVQuant-1b-1% 1.32-1.35 11.18 9.56 16.04 22.87 10.53 CQ-8c8b 1.00 12.13 10.53 12.49 10.53 9.89 CQ-8c10b 1.25 9.12 8.23 9.03 8.01 7.46 of the LLM (2048 for LLaMA, 4096 for LLaMA-2, and 8192 for Mistral). Baselines We compare our proposed approach with uncompressed FP16 KV cache and com- petitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization [6] (without grouping and with a group size of 128), 3. KVQuant [10] (without sparse outliers and with 1% outliers stored in sparse format). KVQuant-<b>b-1% is a dense-and-sparse method that requires an additional sparse matrix multiplication in addition to the dense matrix multiplication during inference, which introduces additional computational overhead. KVQuant and our proposed CQ both require learn- ing centroids on a calibration dataset, and we use the same calibration set of 16 sequences (each with 2048 tokens) of WikiText-2 for both methods. Calibration is performed once on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For each method, we report bits per floating-point number (FPN) to measure the compression rate, which is calculated as the number of bits in the quantized KV cache of each token divided by the number of FPN in the uncompressed KV cache of each token, excluding the constant storage overheads of centroids, scaling factors, and zero points. 4.1 Results The results of perplexity on WikiText-2 are presented in Table 1 and the results on C4 are presented in Table 2. CQ consistently outperforms non-dense-and-sparse quantization methods, especially in low bit-width regions. Despite using lower bit-width, CQ performs better or on par with the dense-and-sparse quantization method KVQuant-<b>b-1%. Dense-and-sparse quantization methods 9Table 3: Accuracy on 3 benchmarks under different KV cache quantization methods at varying bit-width. Bits Per FPN Task LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 WinoGrande 69.93 72.69 68.90 71.98 73.88 PIQA 78.67 79.16 78.07 79.16 80.58 ARC Challenge 41.72 46.42 43.43 48.29 50.34 KVQuant-4b 4.00-4.02 WinoGrande 69.53 72.61 67.96 71.59 73.88 PIQA 78.62 79.22 77.86 78.94 80.58 ARC Challenge 42.32 45.99 42.75 46.67 49.06 KVQuant-4b-1% 4.32-4.35 WinoGrande 70.72 73.40 68.67 72.30 73.72 PIQA 78.40 79.16 78.07 79.27 80.74 ARC Challenge 41.38 46.76 43.17 47.87 49.91 CQ-2c8b 4.00 WinoGrande 70.40 72.45 68.27 72.53 73.48 PIQA 78.61 79.11 77.91 78.62 80.52 ARC Challenge 41.55 45.99 43.34 47.78 49.15 KVQuant-2b 2.00-2.02 WinoGrande 53.59 59.35 51.70 51.30 63.46 PIQA 72.47 74.81 63.38 65.40 75.46 ARC Challenge 32.00 34.47 22.44 24.66 38.57 KVQuant-2b-1% 2.32-2.35 WinoGrande 68.03 71.43 67.64 70.17 70.80 PIQA 77.69 78.51 76.60 78.51 79.65 ARC Challenge 38.74 45.14 41.47 44.97 47.53 CQ-4c8b 2.00 WinoGrande 67.48 70.72 66.45 69.06 69.38 PIQA 76.11 78.29 76.12 77.42 79.49 ARC Challenge 38.48 44.03 39.93 44.11 45.65 KVQuant-1b 1.00-1.02 WinoGrande 50.51 48.46 50.91 49.41 49.80 PIQA 53.26 53.54 53.37 50.92 54.73 ARC Challenge 21.76 21.33 20.65 21.67 19.88 KVQuant-1b-1% 1.32-1.35 WinoGrande 56.67 61.01 57.77 57.30 58.17 PIQA 71.38 75.46 69.91 70.89 73.83 ARC Challenge 29.69 35.32 31.48 32.59 33.19 CQ-8c8b 1.00 WinoGrande 56.51 61.56 55.01 57.14 58.25 PIQA 71.16 73.99 71.22 73.01 75.24 ARC Challenge 30.20 33.79 30.20 34.30 33.79 CQ-8c10b 1.25 WinoGrande 60.46 65.27 59.19 62.98 63.93 PIQA 73.45 75.90 73.07 74.37 77.31 ARC Challenge 33.28 37.12 34.64 38.74 39.59 introduce additional inference overhead due to the extra sparse matrix multiplications for activation outliers which is inefficient on GPUs, while CQ does not have this limitation. The accuracy results on different benchmarks are shown in Table 3. CQ consistently outperforms the non-dense-and-sparse baseline KVQuant-<b>b at bit-width of 1 and 2, and performs better or on par with the dense-and-sparse baseline KVQuant-<b>b-1%. 4.2 Ablation Study We perform a set of ablation experiments to study the effectiveness of each component of our proposed approach. The results of the ablation experiments are shown in Table 4. We evaluate the perplexity of 2 models Mistral-7b and LLaMA-2-13b on WikiText-2 using CQ at 2 bits per FPN, withvaryingnumberofchannelscoupledandcomparinguniformcentroidlearningandFisher-guided centroid learning. Fisher-guided centroids significantly improve model quality as demonstrated by lower perplexity. With either uniform centroids or Fisher-guided centroids, perplexity improves as the number of coupled channels increases. Hence, our proposed techniques of channel coupling and 10Table 4: Ablation study: perplexity on WikiText-2 using CQ with varying number of coupled channels and fisher-guide centroids. Perplexity consistently improves as the number of coupled channels increases. Mistral-7b LLaMA-2-13b Bits Per FPN 2 2 2 2 2 2 2 2 2 2 2 2 Num. of Channels Coupled 1 2 4 1 2 4 1 2 4 1 2 4 Fisher-guided Centroids ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ Perplexity ↓ 97.76 16.29 5.42 5.34 5.20 5.11 890.42 171.96 6.62 6.06 4.91 4.81 Fisher-guided centroid learning are both effective for maintaining model quality. 4.3 Overhead of Centroid Learning and Storage In this section, we discuss the computational overhead of centroid learning and the memory overhead of centroid storage for CQ. The centroid learning process of CQ consists of many independent k- means runs, which can be time-consuming on CPUs. Hence, we leverage a GPU implementation to accelerate the learning process. In all our experiments, we use k-means++ initialization and run 100 iterations of k-means on a single GPU to obtain the centroids. The memory overhead of storing the centroids can be calculated asl × 2 × h × c × 2b 16-bit floating-point numbers, wherel is the number of layers,2 is for keys and values,h is the number of key/value attention heads,c is the number of channels in a single-head key/value activation embedding, andb is the bit width of quantized codes. The detailed learning and memory overhead for different CQ configurations and models are given in Table 5. CQ can easily scale to large model sizes with low learning and memory overheads. Table 5: Learning and memory overhead of different CQ configurations and models. The number of centroid parameters are shown in millions, and the percentage to the model weights is shown in brackets. Centroid Learning Time Parameter Count in Centroids CQ Config. 2c8b 4c8b 8c8b 2c8b 4c8b 8c8b LLaMA-7b 53 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-13b 94 mins 44 mins 22 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) LLaMA-2-7b 54 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-2-13b 83 mins 44 mins 23 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) Mistral-7b 13 mins 7 mins 4 mins 16.78M (0.232%) 16.78M (0.232%) 16.78M (0.232%) 5 Related Works The high memory requirements and computational demands of LLMs pose a great challenge to effi- cient inference. Post-training model weight quantization has been shown to be effective for reducing inference latency and memory requirements. GPTQ [8] scales approximate Hessian-based weight quantization to large-scale models, and AWQ [19] preserves the salient weights in LLMs to achieve better quantized model quality. SqueezeLLM [16] leverages sensitivity-based non-uniform clustering and dense-and-sparse quantization for preserving salient weights and outliers. In addition to weight quantization, KV cache compression approaches are also effective for improving inference efficiency. Scissorhands [20] and H2O [37] achieve KV cache compression while preserving model quality by 11evicting unimportant tokens and only storing pivotal tokens. KIVI [21] quantizes key activations channel-wise and value activations token-wise, and uses a residual to achieve better model quality. KVQuant [10] proposes sensitivity-based quantization and dense-and-sparse quantization for KV cache. FlashAttention [5] improves the inference efficiency of attention-based models on GPUs by fusing kernels to eliminate unnecessary global memory reads/writes, while NoMAD-Attention [36] accelerates attention computations on CPUs by leveraging in-register shuffles. Product Quantiza- tion [12] is an approximate nearest neighbor search method that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and jointly quantizing the dimensions within each subspace. 6 Conclusion In this work, we propose Coupled Quantization (CQ) for enabling more efficient LLM inference by compressing KV cache, which is the latency and throughput bottleneck in long context or large batch size settings. We observe that distinct channels of key/value activation embeddings exhibit high levels of dependency, which has not been leveraged by existing compression methods. Motivated by this insight, we propose channel coupling for exploiting the inter-channel dependency to achieve more information-efficient encoding of key/value activations. Furthermore, we propose Fisher-guided centroid learning to better preserve salient activations and model quality. Extensive experiments demonstrate that our method mostly outperforms existing methods in terms of model quality under the same quantization bit width. Moreover, CQ can preserve model quality reasonably well with KV cache quantized down to 1-bit. References [1] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, volume 7, pages 1027–1035, 2007. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020. [3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance.arXiv preprint arXiv:2305.05176, 2023. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35:16344–16359, 2022. [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient fine- tuning of quantized llms.Advances in Neural Information Processing Systems, 36, 2024. [7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. InInternational Conference on Machine Learning, pages 2793–2803. PMLR, 2021. 12[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post- training quantization for generative pre-trained transformers.arXiv preprint arXiv:2210.17323, 2022. [9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.15556, 2022. [10] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization.arXiv preprint arXiv:2401.18079, 2024. [11] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.arXiv preprint arXiv:2103.10427, 2021. [12] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2010. [13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap- lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. [14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [15] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education.Learning and individual differences, 103:102274, 2023. [16] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. [17] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004. [18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [20] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.Advances in Neural Information Process- ing Systems, 36, 2024. 13[21] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [22] Stuart Lloyd. Least squares quantization in pcm.IEEE transactions on information theory, 28(2):129–137, 1982. [23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An impera- tive style, high-performance deep learning library.Advances in neural information processing systems, 32, 2019. [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. [27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.Communications of the ACM, 64(9):99–106, 2021. [28] Claude Elwood Shannon. A mathematical theory of communication.The Bell system technical journal, 27(3):379–423, 1948. [29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024. [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. [33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of- the-art natural language processing. InProceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. [34] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long- context scaling of foundation models.arXiv preprint arXiv:2309.16039, 2023. 14[35] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records.NPJ digital medicine, 5(1):194, 2022. [36] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava. Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention.arXiv preprint arXiv:2403.01273, 2024. [37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for effi- cient generative inference of large language models.Advances in Neural Information Processing Systems, 36, 2024. 15Appendix A Correlation Matrices and Scatter Plots 0 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 5: Correlation matrix for the first 32 channels of pre-RoPEkey activation embeddings of all LLaMA-7b layers on WikiText-2. 160 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 6: Correlation matrix for the first 32 channels ofvalue activation embeddings of all LLaMA- 7b layers on WikiText-2. Figure 7: 2-dimensional scatter plots of pairs of channels inkey activation embeddings of 4 LLaMA- 7b layers on WikiText-2. 17Figure 8: 2-dimensional scatter plots of pairs of channels in value activation embeddings of 4 LLaMA-7b layers on WikiText-2. 18",
      "meta_data": {
        "arxiv_id": "2405.03917v1",
        "authors": [
          "Tianyi Zhang",
          "Jonah Yi",
          "Zhaozhuo Xu",
          "Anshumali Shrivastava"
        ],
        "published_date": "2024-05-07T00:25:20Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03917v1.pdf"
      }
    },
    {
      "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
      "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing\nsoftmax-based dense self-attention in linear time. Transformer-VQ's efficient\nattention is enabled by vector-quantized keys and a novel caching mechanism. In\nour large-scale experiments, Transformer-VQ is shown highly competitive in\nquality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on\nImageNet64. In addition, the optimized implementation of Transformer-VQ is over\n3x faster than a comparable quadratic-time transformer at sequence length 8k,\nis over 12x faster at 32k, and can scale to 131k with similar throughput. Code\navailable: \\url{https://github.com/transformer-vq/transformer_vq}",
      "full_text": "Published as a conference paper at ICLR 2024 TRANSFORMER -VQ: L INEAR -TIME TRANSFORMERS VIA VECTOR QUANTIZATION Lucas D. Lingle Independent Researcher lucasdaxlingle@gmail.com ABSTRACT We introduce Transformer-VQ, a decoder-only transformer computing softmax- based dense self-attention in linear time. Transformer-VQ’s efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: https://github.com/transformer-vq/transformer_vq k1 k2 k3 k4 k5 7→ VQ ≈ ˆk1 ˆk2 ˆk3 ˆk4 ˆk5 Figure 1: Schematic of the VQ-Attention approximation. The colorful and blank boxes depict the keys and attention weights, respectively. The keys on the right have been vector-quantized. Since the green keys k2, k5 map to the same code, they have the same attention weights in this attention head. 1 I NTRODUCTION Transformer (Vaswani et al., 2017) language models would ideally scale to long sequences, since their predictive abilities often improve as context length increases (Dai et al., 2019; Kaplan et al., 2020). Unfortunately, the standard transformer uses a self-attention mechanism with a quadratic time complexity with respect to sequence length. This limits the practicality of applying transformers to very long sequences, since increasing the sequence length by a factor of 10n increases the attention computations by a factor of 100n. Transformer variants that overcome this efficiency bottleneck have the potential to facilitate new long-context applications and enable new breakthroughs. Up to this point, a variety of efficient transformers (Tay et al., 2020b) have been proposed to scale to long sequences. Techniques include sparsity (Child et al., 2019; Ye et al., 2019; Beltagy et al., 2020; Kitaev et al., 2020; Qiu et al., 2020; Roy et al., 2021; Tay et al., 2020a; Sukhbaatar et al., 2021; Wu et al., 2022; Liu et al., 2023; Zhang et al., 2023), compression (Liu et al., 2018; Rae et al., 2020; Ainslie et al., 2020; Zhu et al., 2021; Ren et al., 2021; Nawrot et al., 2021; 2023), low-rank approximations (Wang et al., 2020; Vyas et al., 2020; Katharopoulos et al., 2020; Xiong et al., 2021; Tay et al., 2021; Choromanski et al., 2021), and cross-attention operations (Dai et al., 2019; Ma et al., 2021; Hutchins et al., 2022; Hawthorne et al., 2022). Other efficient sequence models have also been proposed (Gu et al., 2022; Lee-Thorp et al., 2022; Mehta et al., 2022; Smith et al., 2022; Hasani et al., 2022; Poli et al., 2023; Peng et al., 2023). In this paper, we present Transformer-VQ, a transformer decoder withdense self-attention computible in linear time with respect to sequence length. This is made possible through a combination of vector- quantized keys, localized positional biases, and compressive cache that can be attended to efficiently, while yielding the same results as an uncompressed variable-length cache. Transformer-VQ is also simple to implement sampling for. 1 arXiv:2309.16354v2  [cs.LG]  25 Feb 2024Published as a conference paper at ICLR 2024 2 P RELIMINARIES 2.1 N OTATION The real numbers are denoted by R and the extended real numbers R ∪ {−∞, ∞} by ¯R. Zero-based indices are used for all tensors. When indexing a matrix M along the first axis, we use Mi to denote a column vector and Mi,: to denote a row vector. The functions LN(·), Softmax(·), Concat(·) denote LayerNorm (Ba et al., 2016), softmax, and concatenation, each applied row-wise. The symbols ≜, ∝, ⊙, exp(·), δa,b, SG(·) denote equality by definition, proportionality, element-wise product, element-wise exponentiation, Kronecker delta function, and the stop-gradient operator. We slightly abuse notation to write inner products of vectors u, v as u⊤v, and outer products as uv⊤. We assume familiarity with transformers (Vaswani et al., 2017), and use the notationDm to denote the model width, Dk to denote the query/key vector width, and Dv to denote the value vector width. 2.2 V ECTOR QUANTIZATION Vector quantization (VQ) is a technique used extensively throughout this work. In this subsection we briefly review vector quantization, motivate its use in self-attention, and discuss the backpropagation- compatible VQ scheme introduced by van den Oord et al. (2017). 2.3 V ECTOR QUANTIZERS AND CODEBOOKS Definition 2.1. A vector quantizer is a function VQ(·; C) with domain RD and codomain RD. For an input x, its output ˆx is given by z ≜ arg min s ||x − Cs||2 (1) ˆx ≜ Cz (2) where C ∈ RS×D is known as the codebook. The row indices {0, . . . , S− 1} of C are called shortcodes, and the rows themselves are called codewords. Theorem 2.2 (Based on Guo et al. (2019)). Let q ∈ RD be a random variable withEq[qq⊤] = σ2ID for some σ >0, and let k ∈ RD be a random variable independent of q. Let φ : RD → RD be a deterministic function. Then Eq,k||q⊤k − q⊤φ(k)||2 ∝ Ek||k − φ(k)||2. (3) Corollary 2.3. Let the conditions of Theorem 2.2 hold. Given the constraint thatφ(RD) = {Cs}S−1 s=0 , the choice φ(·) = VQ(·; C) minimizes Eq,k||q⊤k − q⊤φ(k)||2. Corollary 2.4. Let the conditions of Theorem 2.2 hold. With ˆk = VQ(k; C) we have arg min C Eq,k||q⊤k − q⊤ˆk||2 = arg min C Ek||k − ˆk||2. (4) Remark 2.5. Fnding the global minimizer C∗ = arg minC Ek||k − ˆk||2 is expensive, so in practice we approximate it using the method from van den Oord et al. (2017); Razavi et al. (2019). 2.4 V ECTOR -QUANTIZED REPRESENTATION LEARNING Definition 2.6 (Based on van den Oord et al. (2017)) . A vector-quantizer with straight-through estimator is a function STVQ(·; C) with domain RD and codomain RD. For an input x, its output ˆx is given by z ≜ arg min s ||x − Cs||2 (5) ˆx ≜ x + SG(Cz − x). (6) Remark 2.7. For any x ∈ RD, STVQ(x; C) evaluates to VQ(x; C). However, for purposes of backpropagation, the Jacobian of the quantizer w.r.t. its input will now be an identity matrix everywhere, instead of a zero matrix almost everywhere. Intuitively, when usingSTVQ, gradients w.r.t. the quantizer outputs are copied ‘straight through’ to the inputs. Remark 2.8. We overload the notation STVQ(·; C) to operate row-wise on matrix-valued inputs. 2Published as a conference paper at ICLR 2024 3 T RANSFORMER -VQ We now propose Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time. Proofs for all theoretical results are given in Appendix A. 3.1 Q UADRATIC -TIME FORMULATION Definition 3.1. Vector-Quantized Self-Attention is a function VQAttn(·; C, W{Q,K,V,G,O}) with domain RT×Dm and codomain RT×Dm . For an input X, its output Y is defined via ˜X ≜ LN(X) ∈ RT×Dm (7) Q ≜ τ−0.5LN( ˜XWQ) ∈ RT×Dk (8) K ≜ τ−0.5LN( ˜XWK) ∈ RT×Dk (9) V ≜ ϕv( ˜XWV ) ∈ RT×Dv (10) G ≜ ϕg( ˜XWG) ∈ RT×Dv (11) ˆK ≜ STVQ(K; C) ∈ RT×Dk (12) W ≜ ϕw(Q ˆK⊤ + B) ∈ RT×T (13) O ≜ (WV) ⊙ G ∈ RT×Dv (14) Y ≜ X + OWO ∈ RT×Dm (15) where each W• denotes a trainable projection, B denotes positional biases and/or mask, τ is a fixed constant, and the ϕv, ϕg, ϕw are element-wise or row-wise nonlinearities. The query/key LayerNorms use unit gain and zero bias, and STVQ(·; C) denotes row-wise application of vector-quantization with a straight-through gradient estimator (van den Oord et al., 2017). Remark 3.2. Our attention mechanism is applied to a gated attention unit (GAU) design inspired by Hua et al. (2022). GAU is a single-headed gated attention mechanism and generally uses a small key width Dk = 128, and a large value width Dv = 2Dm, with two GAUs replacing a single transformer layer. This yields a similar parameter count and compute requirement as the usual transformer layer. Remark 3.3. Prior work has also applied LayerNorm or similar to the queries and keys in attention (Henry et al., 2020; Roy et al., 2021; Zhu et al., 2021; Wu et al., 2022; Hutchins et al., 2022; Dehghani et al., 2023; Elsen et al., 2023), generally finding it to improve numerical stability and convergence. 3.2 W ARMUP : L INEAR -TIME ENCODER ATTENTION To simplify the theorems for decoder-only attention and build intuition, we first discuss a setting where there is no causal mask. Theorem 3.4. Suppose Bi,j = 0 for all i, j, and ϕw is an element-wise nonlinearity. Then the attention weights in Definition 3.1 can be factored: W ≜ ϕw(Q ˆK⊤ + B) (16) = ϕw(Q ˆK⊤) (17) = ϕw(QC⊤)∆ (18) where ϕw(QC⊤) ∈ RT×S, ∆ ∈ RS×T and ∆s,t ≜ δs,zt . Here, δ·,· denotes the Kronecker delta function and zt is the VQ shortcode for timestep t. Theorem 3.5. Suppose Bi,j = 0 for all i, j, and ϕw is the row-wise softmax nonlinearity. Then the attention weights in Definition 3.1 can be factored: W ≜ ϕw(Q ˆK⊤ + B) (19) = ϕw(Q ˆK⊤) (20) = Diag(exp(QC⊤)∆1)−1 exp(QC⊤)∆ (21) where 1 ∈ RT , Diag(exp(QC⊤)∆1)−1 exp(QC⊤) ∈ RT×S, ∆ ∈ RS×T and ∆s,t ≜ δs,zt . Here, δ·,· denotes the Kronecker delta function and zt is the VQ shortcode for timestep t. 3Published as a conference paper at ICLR 2024 ϕw(Q ˆK⊤) ∈ RT×T × V ∈ RT×Dv = ϕw(QC⊤) ∈ RT×S × ∆V ∈ RS×Dv Figure 2: Schematic of the VQ-Attention factorization with element-wise ϕw. The column set of W = ϕw(Q ˆK⊤) ∈ RT×T has size ≤ S due to VQ, so the attention output O = WV can be obtained by computing the unique attention scores ϕw(QC⊤) and using them to further aggregate to the grouped-sum ∆V. Transformer-VQ uses a softmax-based extension of this idea for its cache. 3.3 L INEAR -TIME DECODER ATTENTION Theorem 3.6. Let L be a divisor of T. Suppose Bi,j = −∞ for j > i(causal masking), and Bi,j = 0 for j < i− L (no bias outside a sliding window). Define ∆ ∈ RS×T with ∆s,t ≜ δs,zt . Let ϕw be an element-wise nonlinearity with ϕw(−∞) = 0. For a tensor M, let M(...,n,...) denote the slice M...,nL:(n+1)L,..., where unsliced dimensions will be denoted by ‘:’. Then the product WV in Definition 3.1 can be computed using the following block-level recurrence: U(n) ≜ \u001aU(n − 1) + ∆(:,n)V(n,:) if n ≥ 0 0 otherwise (22) (WV)(n,:) = ϕw(Q(n,:)C⊤)U(n − 2) (23) + ϕw(Q(n,:) ˆK⊤ (n−1,:) + B(n,n−1))V(n−1,:) (24) + ϕw(Q(n,:) ˆK⊤ (n,:) + B(n,n))V(n,:) (25) where any tensor slice M(...,n,...) is defined as a zero tensor of width L in the sliced dimension if any block slice index n is less than zero (zero-padding). Theorem 3.7. Let the assumptions of Theorem 3.6 hold, but suppose ϕw is now the row-wise softmax nonlinearity. Let 1 ∈ RT . Let A ≜ exp(Q ˆK⊤ + B). Then the product WV in Definition 3.1 can be computed using the following block-level recurrence: U(n) ≜ \u001aU(n − 1) + ∆(:,n)V(n,:) if n ≥ 0 0 otherwise (26) L(n) ≜ \u001aL(n − 1) + ∆(:,n)1(n) if n ≥ 0 0 otherwise (27) (AV)(n,:) = exp(Q(n,:)C⊤)U(n − 2) (28) + exp(Q(n,:) ˆK⊤ (n−1,:) + B(n,n−1))V(n−1,:) (29) + exp(Q(n,:) ˆK⊤ (n,:) + B(n,n))V(n,:) (30) (A1)(n) = exp(Q(n,:)C⊤)L(n − 2) (31) + exp(Q(n,:) ˆK⊤ (n−1,:) + B(n,n−1))1(n−1) (32) + exp(Q(n,:) ˆK⊤ (n,:) + B(n,n))1(n) (33) (WV)(n,:) = Diag((A1)(n))−1(AV)(n,:). (34) Remark 3.8. Theorem 3.7 provides an algorithm to compute VQ-Attention from the queries, keys, values, gates, and codebook in O(L(S + 2L)(Dk + Dv)) time per query block, and therefore O(T(S + 2L)(Dk + Dv)) time per sequence. Remark 3.9. For numerical stability, we use an equivalent implementation of Theorem 3.7 that stores the running mean of the value vectors assigned to a given shortcode, instead of the sum as done by U(n − 2). The result is made equivalent by moving the logarithm of the counts L(n − 2) inside the exponentials exp(Q(n,:)C⊤) appearing in (AV)(n,:) and (A1)(n). See pseudocode in Appendix E. 4Published as a conference paper at ICLR 2024 Remark 3.10. The general strategy of computing un-normalized softmax and its denominator is also used by many prior methods, including Memory-Efficient Attention (Rabe & Staats, 2021), FlashAttention (Dao et al., 2022), and RWKV (Peng et al., 2023); however, the first two techniques do not run in linear time, and the last one couples a recurrent state size to the model width, which is contrary to the principle of transformers. 3.4 L EARNING ALGORITHM 3.4.1 T RAINING LOSS Let θ denote the set of non-codebook parameters of a transformer with N VQ-Attention layers, and let C = {C(ℓ)}N−1 ℓ=0 denote the set of the layers’ codebooks. For autoregressive modeling of a sequence X = {xt}T t=0, we define the Transformer-VQ training loss as L(X; θ, C) = LCE(X; θ, C) + βLVQ(X; θ, C) (35) where β >0 is a hyperparameter known as the commit loss coefficient, and LCE(X; θ, C) ≜ 1 T T−1X t=0 −ln p(xt+1|x≤t, θ, C) (36) LVQ(X; θ, C) ≜ 1 T T−1X t=0 N−1X ℓ=0 ||K(ℓ) t − SG(C(ℓ) zt )||2 2. (37) Thus, the training loss is the average next-token cross-entropy loss, plus the average token’s com- mitment losses (van den Oord et al., 2017), summed over layer codebooks. The non-codebook parameters θ receive a gradient from both loss terms. Following van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. 3.4.2 T RAINING UPDATES Instead of updating on the full sequence loss given above, we generally update every W/L query blocks, where W ≪ T, which resembles a strategy used in prior works (Dai et al., 2019; Wu et al., 2022; Hutchins et al., 2022). Each update is obtained by backpropagating through a window of W timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every W/L query blocks. When W/L = 1, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When W/L >1, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. 4 R ELATED WORK 4.1 H IERARCHICAL ATTENTION Combiner (Ren et al., 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu & Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al., 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al., 2021) and MegaByte (Yu et al., 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every L tokens, there are no sporadic ‘feature consolidation’ operations required during sampling. 5Published as a conference paper at ICLR 2024 4.2 K ERNELIZABLE ATTENTION Kernelizable attention (Katharopoulos et al., 2020; Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al., 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd’s iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. 4.3 C OMPRESSIVE ATTENTION Compressive Transformers (Rae et al., 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022; Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al. (2021); Hutchins et al. (2022). More generally, Transformer-VQ differs from compressive/recurrent transformers in that it has an equivalence to quadratic-time attention over vector-quantized keys. In other words, if the keys are already vector-quantized, the Transformer-VQ cache losslessly reduces the cost to linear time. Perceivers (Jaegle et al., 2021; Hawthorne et al., 2022) use cross-attention to attend to long sequences, and compute self-attention over only a narrow stack of ‘latents’. Transformer-VQ differs from Perceivers in that it computes dense self-attention in linear time, instead of just cross-attention. Thus, while Perceivers’ long-range layers incur a quadratic time complexity during sampling, Transformer- VQ generates sequences in linear time. 4.4 G ATED SEQUENCE MODELS Gated attention was introduced in FLASH (Hua et al., 2022) as a fusion of attention sublayers (Vaswani et al., 2017) and GLU-based MLP sublayers (Shazeer, 2020). Various gating mechanisms have previously been used to stabilize training of transformers (Parisotto et al., 2019) and other sequence models including S4 (Gu et al., 2022), GSS (Mehta et al., 2022), MEGA (Ma et al., 2023) and RWKV (Peng et al., 2023). Transformer-VQ uses the original gating formulation from Hua et al. (2022), and develops a new attention mechanism. 4.5 VQ, K-M EANS , AND BEYOND Ideas relating to k-means, vector quantization, and/or codebooks have also been applied in transform- ers for sparse attention (Roy et al., 2021; Wang et al., 2021; 2022), feature learning (Mao et al., 2022; Roy et al., 2022), sparsely-activated MLPs (Lample et al., 2019), and expert selection (Roller et al., 2021). These works generally feature codebooks or similar within a transformer architecture. Several works also have proposed models that feature a codebook somewhere outside a transformer, e.g., when transformers are priors for VQ-V AEs (Kaiser et al., 2018; Dhariwal et al., 2020; Ramesh et al., 2021; Lee et al., 2022; Zhou et al., 2022). Transformer-VQ uses one codebook within each layer and, in contrast to all of the aforementioned works, computes dense self-attention in linear time. Transformer-VQ is not directly related to methods which quantize the weights of a transformer e.g., Dettmers et al. (2022); Dettmers & Zettlemoyer (2023); Frantar et al. (2023). Such methods are typically applied after training to reduce the memory overhead of the model weights, while still computing in higher precision. As such, they do not affect the bitwidth of the queries, keys, or values, nor the complexity of self-attention. However, if applying our method to large models, these approaches may be complementary during inference. 6Published as a conference paper at ICLR 2024 5 E XPERIMENTS Transformer-VQ is implemented in Jax (Bradbury et al., 2018) and Flax (Heek et al., 2023). For training, we use TPU v3 pod slices (Jouppi et al., 2017). Hyperparameters follow Appendix C unless specifically mentioned. Generated samples for all models are provided in Appendix D. 5.1 P RELIMINARY STUDIES 5.1.1 C ODEBOOK SIZE ABLATIONS Larger codebook sizes may allow more flexible attention patterns and could improve the fidelity of the gradients, both of which are likely to benefit model quality at the expense of additional wall time. To investigate, we ablate the codebook sizeS using the Enwik8 dataset (described in § 5.2.1), and report the lowest validation bits-per-byte (BPB, lower is better) obtained by each model in Table 1. Table 1: Codebook size ablations. Setting Val. BPB Latency (Rel.) S = 256 1.010 0.927 S = 512 1.005 1.0 S = 1024 1.000 1.109 Table 2: Compressive cache ablation. Compressive cache Val. BPB Latency (Rel.) No 1.026 0.836 Yes 1.010 0.927 Table 1 confirms the intuition that larger codebooks improve the prediction quality (lower BPB) in return for additional wall time per training step. In particular, for this dataset and model size, increasing the codebook size by a factor of two appears to decrease the validation BPB by about a factor of 0.995. This result may suggest that the validation loss follows a power-law scaling (Kaplan et al., 2020) w.r.t. codebook size, though more experiments are needed to verify this phenomenon, and it is subject to the caveat that the validation loss must eventually level off (Henighan et al., 2020; Hoffmann et al., 2022), as the model cannot be expected to obtain zero loss at infinite codebook size. 5.1.2 C OMPRESSIVE CACHE ABLATION Since our model has several architectural differences from most prior works, the benefit of the compressive cache must be shown directly. To investigate, we train a model with the compressive cache omitted, using codebook size S = 256. We report the validation BPB for Enwik8 in Table 2. As shown in Table 2, removing the compressive cache reduces the wall time per step by a factor of about 1.1 at the evaluated model size, but leads to a significant drop in quality (higher bits-per-byte). This confirms the importance of our compressive cache mechanism. 5.1.3 L ATENCY AND THROUGHPUT We now measure the training latency (seconds per step) and compute the training throughput (tokens per second). The latter is computed as tokens per batch divided by latency, and allows a direct efficiency comparison across different sequence lengths. We benchmark on a TPU v3 with 8 cores, using a global batch size of 8 sequences. For these experiments, we scale the sequence length T by multiples of 4×, and backpropagate through the entire sequence length. We compare an unquantized quadratic-time full attention baseline (‘Full’) to our proposed linear-time VQ-Attention (‘VQ’) using Theorem 3.7. Since this theorem does not require access to the output gates, VQ-Attention can be extended to multi-head attention variants as well. For each attention type, we therefore benchmark three head types: multi-head attention (‘MHA’; Vaswani et al. (2017)), multi-query attention (‘MQA’; Shazeer (2019)), and single-head gated attention (‘SHGA’ aka GAU; Hua et al. (2022)). For VQ-attention, we use codebook size S = 512 and block length L = 512, which is the same as our later experiments. All models use roughly 190M parameters total. As shown in Table 6, our model has a 3x lower latency/3x higher throughput than the quadratic attention baseline at T = 8192 when using SHGA for both. Moreover, Transformer-VQ is over 6x faster than the quadratic-time baselines when using MQA/MHA for both models. As the sequence 7Published as a conference paper at ICLR 2024 length increases to T = 32768, Transformer-VQ is over 12x faster than the quadratic time baseline when both use SHGA. For MQA/MHA, the quadratic-time attention gives an out-of-memory error at T = 32768, while Transformer-VQ maintains comparable or better throughput than with 4x shorter sequences. Table 7 even shows that Transformer-VQ can scale to sequences of lengthT = 131072 without a substantial decrease in throughput and without running out of memory. 5.2 Q UANTITATIVE RESULTS To assess the ability of Transformer-VQ to learn long-range dependencies, we now conduct a series of large-scale experiments, benchmarking on several long-range autoregressive modeling tasks. For fair comparison, we only benchmark against models (a) trained without using any extra data or augmentation, and (b) evaluated with fixed parameters. In all cases, we use codebook size S = 512. 5.2.1 E NWIK 8 Table 3: Test bits-per-byte on Enwik8. Model BPB Dai et al. (2019) - XL 0.99 Child et al. (2019) - Sparse 0.99 Beltagy et al. (2020) - Longform. 0.99 Roy et al. (2021) - Routing 0.99 Sukhbaatar et al. (2019) - Adapt. 0.98 Nawrot et al. (2021) - Hourglass 0.98 Rae et al. (2020) - Compress. 0.97 Zhu et al. (2021) - Long-Short 0.97 Fan et al. (2020b) - Feedback 0.96 Lei (2021) - SRU++ 0.95 Sukhbaatar et al. (2021) - Expire. 0.95 Lutati et al. (2023) - Focus Attn. 0.94 Transformer-VQ 0.99 Enwik8 is a byte-level language modeling dataset con- sisting of 100 million bytes of unprocessed English- language Wikipedia articles (Mahoney, 2011), with long-term dependencies that may span tens of thou- sands of bytes. Per convention, it is split into train, validation, and test sets of 90 million, 5 million, and 5 million bytes, respectively (Child et al., 2019; Rae et al., 2020). For this dataset, we trained a Transformer-VQ with 190M parameters, smaller than the model by Dai et al. (2019). We report test bits-per-byte (BPB) in Table 3. Transformer-VQ obtains a BPB of 0.99, notably match- ing the result of the large Transformer-XL model from Dai et al. (2019), while using 33% fewer parameters and a 75% shorter cache that covers a longer context. For this dataset, we found overfitting was a significant issue, and due to the compressive cache mechanism, using i.i.d. attention dropout was not possible. Sweeping over the residual dropout rate, weight decay coefficient, and layerdrop (Fan et al., 2020a) rate, we found a setting yielding good generalization. Nonetheless Transformer-VQ does fall short of state-of-the-art here, with several works using complex recurrence or forgetting mechanisms and obtaining better Enwik8 results. 5.2.2 PG-19 PG-19 is an open-vocabulary language modeling dataset consisting of 11 gigabytes of text from over 28,000 freely-available Project Gutenberg books published prior to 1919 (Rae et al., 2020). The average number of words per book is nearly 70,000, enabling learning long-term dependencies, especially in novels (Sun et al., 2021; Hutchins et al., 2022). For this dataset, we trained a Transformer-VQ with 1.3B parameters, similar to the largest model by Hutchins et al. (2022). Since PG-19 is an open-vocabulary dataset, we first learned a SentencePiece vocabulary (Kudo & Richardson, 2018) of size 32,000 using the BPE method. Following the calculations of Rae et al. (2020), we report the test set word-level perplexity (WLP) in Table 4. Transformer-VQ obtains a WLP of 26.6, very close to the state-of-the-art by Block-Recurrent Transformers (Hutchins et al., 2022). Interestingly, since our Transformer-VQ design is equivalent to using dense self-attention with vector-quantized keys, our strong result shows that models using self- attention only (no recurrence) can also be highly competitive on PG-19. This affirms the efficacy of standalone self-attention as a method for sequence processing at scale. Furthermore, compared to the Block-Recurrent Transformer, our model can be implemented via intra-block sums and cross-block reductions, a strategy also used by FLASH (Hua et al., 2022) and shown to be faster in Appendix B. Lastly, we avoid the instabilities of FLASH (Qin et al., 2022a; Ma et al., 2023) thanks to softmax normalization and our cache normalization (§ 3.9). 8Published as a conference paper at ICLR 2024 Table 4: Test word-level perplexity on PG-19. Model WLP Yu et al. (2023) - MegaByte 36.4 Rae et al. (2020) - XL 36.3 Rae et al. (2020) - Compressive 33.6 Roy et al. (2021) - Routing 33.2 Hawthorne et al. (2022) - Perceiver AR 28.9 Hutchins et al. (2022) - Block-Recur. 26.5 Transformer-VQ 26.6 Table 5: Validation bits-per-byte on ImageNet64. Model BPB Kingma et al. (2021) - VDM 3.40 Hawthorne et al. (2022) - Perceiver AR 3.40 Yu et al. (2023) - MegaByte 3.40 Grcic et al. (2021) - DenseFlow 3.35 Lipman et al. (2023) - Flow Matching 3.31 Hazami et al. (2022) - Efficient VDV AE 3.30 Transformer-VQ (190M) 3.22 Transformer-VQ (1.2B) 3.16 5.2.3 I MAGE NET64 ImageNet64 is an image dataset consisting of over 1.2 million images downsampled to 64x64 resolution (Chrabaszcz et al., 2017; Deng et al., 2009). Flattening the images yields an autoregressive density estimation task on sequences of over 12,000 bytes each. Note since the official test set is not public for this dataset, we report results on the official validation set. For validation purposes we used a held-out set of about 80,000 examples from the training split. For this dataset, we trained Transformer-VQ models with 190M and 1.2B parameters, similar to the Enwik8 and PG-19 models, respectively. We report the bits-per-byte on the official validation set in Table 5. Several of the earlier baselines used an earlier variant of downsampled ImageNet prepared by van den Oord et al. (2016) with a different downsampling algorithm. Since that variant has been unavailable through official channels for about a year, we used the newer variant following Lipman et al. (2023). We emphasize that our results using the newer variant cannot be directly compared with baselines using the earlier variant; however, due to several reporting ambiguities, Table 5 does not symbolically distinguish variants used. Figure 3: Generated samples from our large ImageNet64 model; nucleus 1.0. Transformer-VQ with 190M parameters is roughly the same size as the Efficient VDV AE (Hazami et al., 2022), but obtains a better result of 3.22 BPB, setting a new state-of-the-art for small models. Transformer-VQ with 1.2B parameters obtains a 3.16 BPB, setting a new absolute state-of-the-art on this dataset, and generates high-fidelity samples on par with Perceiver AR while using 33% fewer steps, omitting its image-specific architectural adjustments, and generating samples in linear time. 6 C ONCLUSION Transformer-VQ is a transformer decoder computing softmax-based self-attention in linear time. Its efficient attention is enabled by vector-quantized keys, which allow our cache to be attended to in compressed form, while yielding the same result as uncompressed attention over the same keys. Large-scale experiments show Transformer-VQ is an efficient and flexible autoregressive model, with state-of-the-art results or near on PG-19 and ImageNet64. Future work directions include formal scaling laws, larger models, and porting to lower-level frameworks like Pallas, Triton, or CUDA. 9Published as a conference paper at ICLR 2024 REPRODUCIBILITY STATEMENT To facilitate reproducibility, our attention mechanism is described mathematically in Section 3, and pseudocode is provided in Appendix E. In addition, our hyperparameters and other implementation details are given in Appendix C, and our implementation is open-sourced at the link in the abstract. ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their helpful feedback. In addition, we would like to express our gratitude to the Python community, especially the Jax ecosystem contributors, for the effective libraries used in this project. This project was generously supported with Cloud TPUs from Google’s TPU Research Cloud (TRC). REFERENCES Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC: Encoding long and struc- tured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 268–284, Online, November 2020. Associ- ation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.19. URL https: //aclanthology.org/2020.emnlp-main.19. Jimmy Lei Ba, Jamie Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL https: //arxiv.org/abs/1607.06450. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=Uynr3iPhksa. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations , 2021. URL https: //openreview.net/forum?id=Ua6zuk0WRH. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev- skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311. 10Published as a conference paper at ICLR 2024 Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative to the CIFAR datasets. CoRR, abs/1707.08819, 2017. URL http://arxiv.org/ abs/1707.08819. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence, Italy, jul 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285. Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/ 2205.14135. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Paveti´c, Dustin Tran, Thomas Kipf, Mario Luˇci´c, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters, 2023. URL http://arxiv.org/abs/2302.05442. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier- archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws, 2023. URL http://arxiv.org/abs/2212.09720. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD. Prafulla Dhariwal, Heewoo Jun, Christine McLeavey Paine, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music, 2020. URL https://arxiv.org/abs/ 2005.00341. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. CoRR, abs/1702.03118, 2017. URL http: //arxiv.org/abs/1702.03118. Erich Elsen, Augustus Odena, Maxwell Nye, Sa˘gnak Ta¸ sırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, and Arushi Somani. Releasing persimmon-8b, 2023. URL https://www.adept. ai/blog/persimmon-8b. Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations , 2020a. URL https://openreview.net/forum?id=SylO2yStDr. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory, 2020b. URL https://arxiv.org/ abs/2002.09402. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS. 11Published as a conference paper at ICLR 2024 Matej Grcic, Ivan Grubisic, and Sinisa Segvic. Densely connected normalizing flows. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad- vances in Neural Information Processing Systems, volume 34, pp. 23968–23982. Curran Asso- ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/ 2021/file/c950cde9b3f83f41721788e3315a14a3-Paper.pdf. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations , 2022. URL https: //openreview.net/forum?id=uYLFoz1vlAC. Ruiqi Guo, Quan Geng, David Simcha, Felix Chern, Sanjiv Kumar, and Xiang Wu. New loss functions for fast maximum inner product search. CoRR, abs/1908.10396, 2019. URL http: //arxiv.org/abs/1908.10396. Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models, 2022. URL https://arxiv.org/abs/ 2209.12951. Curtis Hawthorne, Andrew Jaegle, C ˘at˘alina Cangea, Sebastian Borgeaud, Charlie Nash, Ma- teusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, Joao Carreira, and Jesse Engel. General- purpose, long-context autoregressive modeling with Perceiver AR. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceed- ings of the 39th International Conference on Machine Learning , volume 162 of Proceed- ings of Machine Learning Research , pp. 8535–8558. PMLR, 17–23 Jul 2022. URL https: //proceedings.mlr.press/v162/hawthorne22a.html. Louay Hazami, Rayhane Mama, and Ragavan Thurairatnam. Efficient-VDV AE: Less is more, 2022. URL http://arxiv.org/abs/2203.13751. Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.com/google/flax. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCan- dlish. Scaling laws for autoregressive generative modeling. CoRR, abs/2010.14701, 2020. URL https://arxiv.org/abs/2010.14701. Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key normalization for transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4246–4253, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.379. URL https://aclanthology.org/2020. findings-emnlp.379. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/ 2203.15556. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research , pp. 9099–9117. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/hua22a.html. 12Published as a conference paper at ICLR 2024 DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview. net/forum?id=uloenYmLCAo. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Car- reira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research , pp. 4651–4664. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/jaegle21a.html. Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. SIGARCH Comput. Archit. News, 45(2):1–12, jun 2017. ISSN 0163-5964. doi: 10.1145/3140659.3080246. URL https://doi. org/10.1145/3140659.3080246. Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam Shazeer. Fast decoding in sequence models using discrete latent variables. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2390–2399. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/kaiser18a.html. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research , pp. 5156–5165. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/katharopoulos20a.html. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad- vances in Neural Information Processing Systems, volume 34, pp. 21696–21707. Curran Asso- ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/ 2021/file/b578f2a52a0229873fefc2a4b06377fa-Paper.pdf. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=rkgNKkHtvB. Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012. Guillaume Lample, Alexandre Sablayrolles, Marc' Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, 13Published as a conference paper at ICLR 2024 volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper/2019/file/9d8df73a3cfbf3c5b47bc9b50f214aff-Paper.pdf. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR) , pp. 11523–11532, June 2022. URL https://openaccess.thecvf.com/content/CVPR2022/html/ Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_ CVPR_2022_paper.html. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4296–4313, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.naacl-main.319. URL https://aclanthology.org/2022.naacl-main.319. Tao Lei. When attention meets fast recurrence: Training language models with reduced com- pute. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7633–7648, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.602. URL https://aclanthology.org/2021.emnlp-main.602. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Repre- sentations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Confer- ence on Learning Representations, 2018. URL https://openreview.net/forum?id= Hyg0vbWC-. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time, 2023. URL http://arxiv.org/ abs/2305.17118. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive IIR filters), 2023. URL http://arxiv.org/abs/2305.14952. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettle- moyer. LUNA: Linear unified nested attention. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https://openreview.net/forum?id=GWRkOYr4jxQ. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=qNLe3iq2El. Matt Mahoney. Large text compression benchmark, 2011. URL: http://mattmahoney.net/ dc/text.html. Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl V ondrick, Rahul Sukthankar, and Irfan Essa. Discrete representations strengthen vision transformer robustness. In International Confer- ence on Learning Representations, 2022. URL https://openreview.net/forum?id= 8hWs60AZcWk. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces, 2022. URL http://arxiv.org/abs/2206.13947. 14Published as a conference paper at ICLR 2024 Piotr Nawrot, Szymon Tworkowski, Michal Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. Hierarchical transformers are more efficient language models. CoRR, abs/2110.13711, 2021. URL https://arxiv.org/abs/2110.13711. Piotr Nawrot, Jan Chorowski, Adrian Ła´ncucki, and Edoardo M. Ponti. Efficient transformers with dynamic token pooling, 2023. URL http://arxiv.org/abs/2211.09761. Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Çaglar Gülçehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning. CoRR, abs/1910.06764, 2019. URL http://arxiv.org/abs/1910.06764. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era, 2023. URL http://arxiv.org/abs/2305. 13048. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y . Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models, 2023. URL http://arxiv.org/abs/2302.10866. Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 7025–7041, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.473. URL https://aclanthology.org/ 2022.emnlp-main.473. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. CosFormer: Rethinking softmax in attention. In International Confer- ence on Learning Representations, 2022b. URL https://openreview.net/forum?id= Bl8CQrx2Up4. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self- attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 2555–2565, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.232. URL https://aclanthology. org/2020.findings-emnlp.232. Markus N. Rabe and Charles Staats. Self-attention does not need o(n 2) memory. CoRR, abs/2112.05682, 2021. URL https://arxiv.org/abs/2112.05682. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. https: //cdn.openai.com/better-language-models/language_models_are_ unsupervised_multitask_learners.pdf Last visited on 2023/09/07. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Com- pressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron 15Published as a conference paper at ICLR 2024 Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean- Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. CoRR, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446. Prajit Ramachandran, Barret Zoph, and Quoc V . Le. Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021. URL https://arxiv.org/abs/2102.12092. Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso- ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/ 2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad- vances in Neural Information Processing Systems, volume 34, pp. 22470–22482. Curran Asso- ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/ 2021/file/bd4a6d0563e0604510989eb8f9ff71f5-Paper.pdf. Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason E Weston. Hash layers for large sparse models. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.),Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=lMgDDWb1ULW. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. doi: 10.1162/tacl_a_00353. URL https://aclanthology.org/2021. tacl-1.4. Aurko Roy, Rohan Anil, Guangda Lai, Benjamin Lee, Jeffrey Zhao, Shuyuan Zhang, Shibo Wang, Ye Zhang, Shen Wu, Rigel Swavely, Yu Tao, Phuong Dao, Christopher Fifty, Zhifeng Chen, and Yonghui Wu. N-Grammer: Augmenting transformers with latent n-grams, 2022. URL https://arxiv.org/abs/2207.06366. Noam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL http://arxiv.org/abs/1911.02150. Noam Shazeer. GLU variants improve transformer, 2020. URL https://arxiv.org/abs/ 2002.05202. Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. CoRR, abs/1804.04235, 2018. URL http://arxiv.org/abs/1804.04235. Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2022. URL https://arxiv.org/abs/2208.04933. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 331–335, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1032. URL https://aclanthology.org/P19-1032. 16Published as a conference paper at ICLR 2024 Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9902–9912. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/sukhbaatar21a.html. Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 807–822, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.62. URL https://aclanthology.org/2021.emnlp-main.62. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention.CoRR, abs/2002.11296, 2020a. URL https://arxiv.org/abs/2002.11296. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. CoRR, abs/2009.06732, 2020b. URL https://arxiv.org/abs/2009.06732. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning , volume 139 of Pro- ceedings of Machine Learning Research , pp. 10183–10192. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/tay21a.html. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn- ing. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As- sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf. Aäron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural net- works. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learn- ing Research, pp. 1747–1756, New York, New York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/oord16.html. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Asso- ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Apoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems , volume 33, pp. 21665–21674. Curran As- sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ f6a8dd1c954c8506aadc764cc32b895e-Paper.pdf. Ningning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Junqiu Wei, Qun Liu, and Xin Jiang. ClusterFormer: Neural clustering attention for efficient and effective transformer. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2390–2402, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.170. URL https://aclanthology.org/2022.acl-long. 170. Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. Cluster-former: Clustering-based sparse transformer for question answering. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , pp. 3958– 3968, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-acl.346. URL https://aclanthology.org/2021.findings-acl.346. 17Published as a conference paper at ICLR 2024 Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: self-attention with linear complexity. CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006. 04768. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations , 2022. URL https: //openreview.net/forum?id=TrjbxzRcnf-. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. Pro- ceedings of the AAAI Conference on Artificial Intelligence , 35(16):14138–14148, May 2021. doi: 10.1609/aaai.v35i16.17664. URL https://ojs.aaai.org/index.php/AAAI/ article/view/17664. Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. BP-Transformer: Modelling long-range context via binary partitioning. CoRR, abs/1911.04070, 2019. URL http://arxiv. org/abs/1911.04070. Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers, 2023. URL http: //arxiv.org/abs/2305.07185. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso- ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy- hitter oracle for efficient generative inference of large language models, 2023. URL https: //arxiv.org/abs/2306.14048. Shangchen Zhou, Kelvin C.K. Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=XdDl3bFUNn5. Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad- vances in Neural Information Processing Systems , volume 34, pp. 17723–17736. Curran As- sociates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 9425be43ba92c2b4454ca7bf602efad8-Paper.pdf. Zhenhai Zhu and Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 3801–3815, Online, August 2021. Association for Computational Lin- guistics. doi: 10.18653/v1/2021.acl-long.294. URL https://aclanthology.org/2021. acl-long.294. 18Published as a conference paper at ICLR 2024 A T HEOREMS A.1 P ROOF OF THEOREM 2.2 Proof. This proof is based on Guo et al. (2019). Invoking the fact that q, k, φ(k) ∈ RD, the assumed independence between q and k, the law of iterated expectations, and the isotropy assumption on q, i.e., Eq[qq⊤] = σ2ID for σ2 > 0, we have Eq,k[q⊤k − q⊤φ(k)]2 (38) = Eq,k[q⊤(k − φ(k))]2 (39) = Eq,k[q⊤(k − φ(k))]⊤[q⊤(k − φ(k))] (40) = Eq,k(k − φ(k))⊤qq⊤(k − φ(k)) (41) = Ek(k − φ(k))⊤Eq[qq⊤](k − φ(k)) (42) ∝ Ek(k − φ(k))⊤ID(k − φ(k)) (43) = Ek||k − φ(k)||2. (44) A.2 P ROOF OF COROLLARY 2.3 Proof. By definition, VQ(k; C) ≜ arg minc∈{Cs}S−1 s=0 ||k − c||2. In other words, φ(k) = VQ(k; C) minimizes ||k − φ(k)||2 under the constraint that the outputs of φ are limited to the rows of C, i.e., φ(RD) = {Cs}S−1 s=0 . Since this choice is a pointwise minimizer under the given constraint, it is also a minimizer of the expectation Ek||k − φ(k)||2 under the same constraint. Under the assumptions of Theorem 2.2, the aforementioned expectation is equal to Eq,k||q⊤k − q⊤φ(k)||2 up to a positive proportionality constant σ2. As a result, VQ(k; C) is also a minimizer of the expectation Eq,k||q⊤k − q⊤φ(k)||2 under the same constraint on the output of φ. A.3 P ROOF OF THEOREM 3.4 Proof. When ϕw is an element-wise nonlinearity, ϕ(c) is well-defined, where c is any scalar. Then using definitions alone, we have [ϕw(QC⊤)∆]i,j = ϕw(QC⊤)i,:∆:,j (45) = S−1X s=0 ϕw(QC⊤)i,s∆s,j (46) = S−1X s=0 ϕw(Qi,:C⊤ s,:)δs,zj (47) = ϕw(Qi,:C⊤ zj,:) (48) = ϕw(Qi,: ˆK⊤ j,:) (49) = [ϕw(Q ˆK⊤)]i,j (50) A.4 P ROOF OF THEOREM 3.5 Proof. By Theorem 3.4 with ϕw(·) = exp(·), we have exp(Q ˆK⊤) = exp(QC⊤)∆. Invoking the definition of row-wise softmax and applying substitution, we have Softmax(Q ˆK⊤) = Diag(exp(Q ˆK⊤)1)−1 exp(Q ˆK⊤) (51) = Diag(exp(QC⊤)∆1)−1 exp(QC⊤)∆. (52) 19Published as a conference paper at ICLR 2024 A.5 P ROOF OF THEOREM 3.6 Proof. For n = 0, 1 the result follows by inspection. For n ≥ 2, by the definition of U(n − 2) we have U(n − 2) = (n−1)L−1X j=0 ∆:,jVj,: = ∆(:,0:n−1)V(0:n−1,:). (53) Note that in our notation, the superscripts’ block index range is non-inclusive on the ending value, so ∆(:,0:n−1)V(0:n−1,:) is equal to the sum of the matrix products for the matrix blocks from 0 to n − 2. Thus, by substitution, we have ϕw(Q(n,:)C⊤)U(n − 2) = ϕw(Q(n,:)C⊤)∆(:,0:n−1)V(0:n−1,:) (54) We invoke the same argument as in the proof of Theorem 3.4 to concludeϕw(Q(n,:)C⊤)∆(:,0:n−1) = W(n,0:n−1). Substituting this expression into the right-hand side above gives ϕw(Q(n,:)C⊤)U(n − 2) = W(n,0:n−1)V(0:n−1,:). (55) Substituting this expression into the formula for (WV)(n,:) claimed in the theorem statement, and invoking the same argument as in the proof of Theorem 3.4 on the middle term, we see the claimed formula has the form W(n,0:n−1)V(0:n−1,:) + W(n,n−1)V(n−1,:) + W(n,n)V(n,:). The diagonal block W(n,n) of W is causally masked, so the sum of the three terms indeed equals (WV)(n,:). A.6 P ROOF OF THEOREM 3.7 Proof. Recall that we defined A ≜ exp(Q ˆK⊤ + B). The proposed expression for (AV)(n,:) follows from Theorem 3.6 with ϕw(·) = exp(·). The proposed expression for (A1)(n) follows by a substitution argument using (AV)(n,:). Normalizing (AV)(n,:) by (A1)(n) and iterating over n thus yields all blocks of the product WV when the nonlinearity ϕw is row-wise softmax. 20Published as a conference paper at ICLR 2024 B T HROUGHPUT We present throughput results for three methods to compute the cache variables: serial scan, matmul, and associative scan. The first two are generalizations of the cross-block reduction methods from FLASH (Hua et al., 2022), which were a simple cumulative sum and matrix multiplication by a lower-triangular matrix of ones, respectively. We found our proposed generalizations were necessary for stable training, an issue where FLASH has known weaknesses (Qin et al., 2022a; Ma et al., 2023). Pseudocode for each of our stable reduction methods is given in Appendix E. In addition to the three cross-block reduction methods to compute the cache variables from parallel- computed per-block summaries, we also benchmark an input scanning implementation of VQ- Attention inspired by Wu et al. (2022); Hutchins et al. (2022), such that all the operations for a transformer layer are performed one input block at a time. To ground all comparisons, we benchmark the throughput against a transformer using unquantized quadratic-time attention, the same attention head type (SHGA, MQA, or MHA), and an identical non-codebook parameter count. Table 6: Training throughput comparison (tokens/sec) on Google Cloud VM with 8 TPU v3 cores, between Full Attention and VQ-Attention with serial scan reduction. Model Sequence Length 2048 8192 32768 131072 Full VQ Speedup Full VQ Speedup Full VQ Speedup Full VQ Speedup SHGA 65.5k 63.0k 0.962× 23.9k 77.2k 3.230× 6.5k 82.1k 12.631× OOM OOM – MQA 58.9k 63.0k 1.070× 10.4k 74.8k 7.192× OOM 79.7k – OOM OOM – MHA 52.0k 49.6k 0.955× 9.5k 57.8k 6.084× OOM 60.7k – OOM OOM – Table 7: Training throughput comparison (tokens/sec) on Google Cloud VM with 8 TPU v3 cores, between Full Attention and VQ-Attention with matmul reduction. Model Sequence Length 2048 8192 32768 131072 Full VQ Speedup Full VQ Speedup Full VQ Speedup Full VQ Speedup SHGA 65.5k 62.5k 0.954× 23.9k 75.2k 3.148× 6.5k 80.0k 12.250× OOM 69.5k – MQA 58.9k 62.5k 1.061× 10.4k 74.9k 7.144× OOM 80.2k – OOM 67.7k – MHA 52.0k 48.2k 0.926× 9.5k 58.2k 6.096× OOM 61.6k – OOM OOM – Table 8: Training throughput comparison (tokens/sec) on Google Cloud VM with 8 TPU v3 cores, between Full Attention and VQ-Attention with associative scan reduction. Model Sequence Length 2048 8192 32768 131072 Full VQ Speedup Full VQ Speedup Full VQ Speedup Full VQ Speedup SHGA 65.5k 58.9k 0.899× 23.9k 62.2k 2.603× 6.5k 63.4k 9.754× OOM OOM – MQA 58.9k 62.8k 1.066× 10.4k 74.2k 7.134× OOM 79.0k – OOM 67.0k – MHA 52.0k 49.1k 0.944× 9.5k 55.4k 5.831× OOM 57.8k – OOM OOM – 21Published as a conference paper at ICLR 2024 Table 9: Training throughput comparison (tokens/sec) on Google Cloud VM with 8 TPU v3 cores, between Full Attention and VQ-Attention, both with input scanning. Model Sequence Length 2048 8192 32768 131072 Full VQ Speedup Full VQ Speedup Full VQ Speedup Full VQ Speedup SHGA 32.0k 40.8k 1.275× 12.7k 47.4k 3.732× OOM 49.4k – OOM OOM – MQA 36.2k 54.8k 1.514× 14.5k 64.9k 4.476× OOM 68.3k – OOM OOM – MHA 30.0k 43.7k 1.457× 11.4k 50.7k 4.447× OOM 53.1k – OOM OOM – 22Published as a conference paper at ICLR 2024 C T RAINING DETAILS C.1 H YPERPARAMETERS Per-dataset hyperparameters are provided below. Table 10: Hyperparameters. Name Symbol Enwik8 PG-19 ImageNet64 ImageNet64 parameter count 190M 1.3B 190M 1.2B global batch size B 128 128 16 128 sequence length T 8192 8192 12288 12288 backprop length W 2048 2048 12288 2048 block length L 512 512 512 512 model dimension Dm 768 2048 768 2048 key dimension Dk 128 128 128 128 value dimension Dv 1536 4096 1536 4096 num code S 512 512 512 512 num gau N 48 48 48 48 sinusoid dropout rate pdropsin 0.2 0.1 0.1 0.1 residual dropout rate pdropres 0.5 0.1 0.0 0.0 layerdrop rate pdroplyr 0.3 0.1 0.0 0.0 weight decay 0.0002 0.0 0.0 0.0 optimizer adamw adafactor adamw adafactor total steps 125000 500000 125000 500000 Note that the 190M parameter ImageNet64 result was added after the other experiments had concluded. To avoid biasing its result, we use the exact same architectural hyperparameters as the Enwik8 model, and the exact same regularization as the larger ImageNet64 model. The smaller ImageNet model was trained in a newer version of our codebase optimized for higher throughput and faster compile times, rather than training on long sequences in constant space via input scans and truncated backprop through time. The attention in the optimized codebase was unit-tested to match the original. C.2 I MPLEMENTATION Weights and token embeddings were initialized following Chowdhery et al. (2022). For the small model, the classifier layer omits LayerNorm and is independently parameterized. For the large model, the classifier layer uses LayerNorm and its projection is tied with the token embedding table, then scaled down by a large constant. For image datasets, we add absolute sinusoidal position embeddings, scaled by a trainable scalar, to the token embeddings (Hua et al., 2022; Vaswani et al., 2017). We used a maximum angular wavelength of 105 for all sinusoidal embeddings. We used the pre-norm placement of LayerNorm (Radford et al., 2019), and always used the RMS LayerNorm variant (Zhang & Sennrich, 2019). For the activations, we used ϕw = Softmax and ϕv = ϕg = SiLU, the self-gated activation (Elfwing et al., 2017; Ramachandran et al., 2017). Several models use LayerDrop for regularization (Fan et al., 2020a), and following the Transformer-XL codebase (Dai et al., 2019) models apply dropout to the flipped sinusoidal embeddings used for (local) relative positional biases. We used float32 parameters, with bfloat16 precision for most computations (Rae et al., 2021). For the AdamW optimizer (Loshchilov & Hutter, 2019), we used gradient clip 0.1, max learning rate α = 0.0004 and hyperparameters β1 = 0.9, β2 = 0.98, ϵ= 10−9. For the Adafactor optimizer (Shazeer & Stern, 2018), we used relative stepsizes, update clip 1.0, max learning rate α = 0.01, and hyperparameters ˆβ1 = 0.0, ˆβ2,t = 1 − t−0.8. We used weight decay with a constant schedule throughout training and omit decay on any one-dimensional parameter tensors (Radford et al., 2019). The codebook commit coefficient was always β = 0.0001 and codebook EMA rate was always γ = 0.99. Learning rates were linearly warmed up for 10,000 steps, then decayed by a 10x factor using a cosine schedule. 23Published as a conference paper at ICLR 2024 D G ENERATED SAMPLES D.1 Q UALITATIVE ANALYSIS D.1.1 PG-19 No effort has been made to explain elementary methods of photography, for the reason that such explanation has been found in the publications of every leading technical journal. The endeavor has been to present what is necessary to the amateur and the professional photographer, together with suggestions of how to make apparatus for the student, and to give a chapter on lens building. The author is fully aware of the imperfections in the methods described, and would like to emphasize the necessity of studying these methods carefully before attempting to use them, if it is desired to make satisfactory photographs. The most essential point in photography is the study of light. It is impossible to have success in photography unless the operator knows what light is. The writer believes that much may be done to advance the art of photography by the use of simple apparatus. The student must not overlook the fact that some simple apparatus is necessary in order to get good results. A lens is necessary to bring the image on the sensitive plate up to the focus of the lens. This lens is very expensive and only a few can be had of the best makers. Figure 4: Sample excerpt from our PG-19 model, generated with nucleus 0.8. We generated 128 sequences using nucleus sampling (Holtzman et al., 2020). In Figure 4, we observe a sample except in which our PG-19 model synthesizes high-quality text, and maintains a consistent tone, topic, and train of thought. These observations were found to hold for the vast majority of the samples we generated. D.1.2 I MAGE NET64 Figure 5: Generated samples from our large ImageNet64 model; nucleus 0.999. Figures 3 and 5 show a subset of samples with the same indices from two batches with different nucleus settings. We see that our large ImageNet64 model synthesizes sequences of over 12,000 bytes and is capable of depicting relatively high-fidelity ocean water, shorelines, leaves, insects, trees, animals, people, mountains, and architecture. D.2 E XTENSIVE SAMPLES Samples for Enwik8, PG-19, and ImageNet64 can be viewed at the anonymized URLs in Table 11. Table 11: Generated samples’ URLs by dataset. URL https://www.dropbox.com/sh/vu0dvw2bcglerwg/AADTQ9B4imAyEIc1Oo849v3ua?dl=0 https://www.dropbox.com/sh/12civha5ulukulz/AAATnHL91RVax5kIb7QgS9ywa?dl=0 https://www.dropbox.com/sh/xqr0q2e9seoz5wn/AADFnl1LWCaddC2CYRP3QSvpa?dl=0 D.3 I MAGE NET64 - F ULL BATCH 24Published as a conference paper at ICLR 2024 25Published as a conference paper at ICLR 2024 26Published as a conference paper at ICLR 2024 E P SEUDOCODE import flax.linen as nn import jax import jax.numpy as jnp import chex class VQAttn(nn.Module): n_code: int d_k: int d_v: int @nn.compact def __call__(self, x): \"\"\"Input shape: [batch size, num blocks, block length, model width].\"\"\" B, R, C, D = x.shape S, K, V = self.n_code, self.d_k, self.d_v x_tilde = RMSLayerNorm(axis=−1)(x) q = RMSLayerNorm(axis=−1)(nn.Dense(self.d_k)(x_tilde)) k = RMSLayerNorm(axis=−1)(nn.Dense(self.d_k)(x_tilde)) v = jax.nn.silu(nn.Dense(self.d_v)(x_tilde)) g = jax.nn.silu(nn.Dense(self.d_v)(x_tilde)) quantizer = VectorQuantizer(codebook_size=self.n_code, width=self.d_k) k_hat, z, l_commit, l_codebook = quantizer(k) # quantized keys, shortcodes, etc c = quantizer.get_codebook() local_biases = XLBiasProducer(width=self.d_k, length=2∗C)(q) chex.assert_shape(local_biases, [B, R, C, 2∗C]) local_biases_prev, local_biases_present = jnp.split(local_biases, 2, axis=−1) scores_present = jnp.einsum(\"brik,brjk−>brij\", q, k_hat) scores_present += local_biases_present scores_present −= 1e30 ∗ (1 − jnp.tril(jnp.ones_like(scores_present))) k_hat_prev = jnp.pad(k_hat[:, :−1], ((0, 0), (1, 0), (0, 0), (0, 0))) v_prev = jnp.pad(v[:, :−1], ((0, 0), (1, 0), (0, 0), (0, 0))) scores_prev = jnp.einsum(\"brik,brjk−>brij\", q, k_hat_prev) scores_prev += local_biases_prev scores_prev = jnp.pad( scores_prev[:, 1:], ((0, 0), (1, 0), (0, 0), (0, 0)), constant_values=−1e30, ) scores_cache = jnp.einsum(\"brik,sk−>bris\", q, c) cache_u_div_l_by_block, cache_l_by_block = get_cache_vars(z, v, S) chex.assert_shape(cache_u_div_l_by_block, [B, R, S, V]) chex.assert_shape(cache_l_by_block, [B, R, S]) count_biases = jnp.where( jnp.greater(cache_l_by_block, jnp.zeros_like(cache_l_by_block)), jnp.log(jnp.clip(cache_l_by_block, a_min=1.0)), jnp.full_like(cache_l_by_block, fill_values=−1e30), ) scores_cache += jnp.expand_dims(count_biases, axis=−2) scores_present_max = jnp.max(scores_present, axis=−1) scores_prev_max = jnp.max(scores_present, axis=−1) scores_cache_max = jnp.max(scores_cache, axis=−1) scores_max = jnp.maximum( jnp.maximum(scores_present_max, scores_prev_max), scores_cache_max, ) scores_max = jax.lax.stop_gradient(scores_max) scores_present −= scores_max[..., None] scores_prev −= scores_max[..., None] scores_cache −= scores_max[..., None] a_present = jnp.exp(scores_present) a_prev = jnp.exp(scores_prev) a_cache = jnp.exp(scores_cache) d = jnp.sum(a_present, axis=−1) d += jnp.sum(a_prev, axis=−1) d += jnp.sum(a_cache, axis=−1) w_present = a_present / d[..., None] w_prev = a_prev / d[..., None] w_cache = a_cache / d[..., None] wv = jnp.einsum(\"brij,brjv−>briv\", w_present, v) wv += jnp.einsum(\"brij,brjv−>briv\", w_prev, v_prev) wv += jnp.einsum(\"bris,brsv−>briv\", w_cache, cache_u_div_l_by_block) o = wv ∗ g residual = nn.Dense(D)(o) return x + residual, l_commit, l_codebook Code 1: Jax/Flax pseudocode for VQ-Attention. 27Published as a conference paper at ICLR 2024 def get_cache_vars(z, v, n_code): # throughout this function, we often use clipping of elementwise denominators at 1 to avoid nans. # in the places where we do this, it does not alter the actual cache variable estimates # since the corresponding entries in the numerator will be zero when the clip is applied. delta = jax.nn.one_hot(z, num_classes=n_code, dtype=v.dtype, axis=−1) delta1_by_block = jnp.einsum(\"bris−>brs\", delta) deltav_by_block = jnp.einsum(\"bris,briv−>brsv\", delta, v) deltav_by_block_normalized = jnp.divide( deltav_by_block, jnp.clip(delta1_by_block[..., None], a_min=1.0), ) def scan_func(carry, in_dict): # computes running average of the value vectors for each shortcode (\"upper div lower\"), # and running count (\"lower\"). lower = carry[\"lower\"] lower_block = in_dict[\"delta1_by_block\"] lower_new = lower + lower_block f1 = jnp.divide(lower, jnp.clip(lower_new, a_min=1.0)) f2 = jnp.divide(lower_block, jnp.clip(lower_new, a_min=1.0)) upper_div_lower_new = jnp.add( f1[..., None] ∗ carry[\"upper_div_lower\"], f2[..., None] ∗ in_dict[\"deltav_by_block_normalized\"], ) carry_new = dict( upper_div_lower=upper_div_lower_new, lower=lower_new, ) return carry_new, carry_new # state to carry, output to save # before we scan, we have to transpose since jax only supports scans along axis 0. # this is still fast, possibly because jnp.transpose might be choosing to return a view deltav_by_block_normalized = jnp.transpose(deltav_by_block_normalized, (1, 0, 2, 3)) delta1_by_block = jnp.transpose(delta1_by_block, (1, 0, 2)) _, cache_vars = jax.lax.scan( f=scan_func, init=dict( upper_div_lower=jnp.zeros(dtype=self.dtype, shape=deltav_by_block_normalized.shape[1:]), lower=jnp.zeros(dtype=self.dtype, shape=delta1_by_block.shape[1:]), ), xs=dict( deltav_by_block_normalized=deltav_by_block_normalized, delta1_by_block=delta1_by_block, ), unroll=1, ) cache_var_upper_div_lower = jnp.pad( jnp.transpose(cache_vars[\"upper_div_lower\"][:−2], (1, 0, 2, 3)), ((0, 0), (2, 0), (0, 0), (0, 0)), ) cache_var_lower = jnp.pad( jnp.transpose(cache_vars[\"lower\"][:−2], (1, 0, 2)), ((0, 0), (2, 0), (0, 0)), ) return cache_var_upper_div_lower, cache_var_lower Code 2: Jax/Flax pseudocode to get cache variables for all blocks; serial scan version. 28Published as a conference paper at ICLR 2024 def get_cache_vars(z, v, n_code): # throughout this function, we often use clipping of elementwise denominators at 1 to avoid nans. # in the places where we do this, it does not alter the actual cache variable estimates # since the corresponding entries in the numerator will be zero when the clip is applied. delta = jax.nn.one_hot(z, num_classes=n_code, dtype=v.dtype, axis=−1) delta1_by_block = jnp.einsum(\"bris−>brs\", delta) deltav_by_block = jnp.einsum(\"bris,briv−>brsv\", delta, v) deltav_by_block_normalized = jnp.divide( deltav_by_block, jnp.clip(delta1_by_block[..., None], a_min=1.0), ) delta1_by_block_tiled = jnp.einsum( \"brs,bgs−>bsrg\", jnp.ones_like(delta1_by_block), delta1_by_block, ) delta1_by_block_tiled = jnp.tril(delta1_by_block_tiled) delta1_fracs_by_block = jnp.divide( delta1_by_block_tiled, jnp.clip(jnp.einsum(\"bsrg−>bsr\", delta1_by_block_tiled)[..., None], a_min=1.0), ) deltav_by_block_cumulative_normalized = jnp.einsum( \"bsrg,bgsv−>brsv\", delta1_fracs_by_block, deltav_by_block_normalized ) delta1_by_block_cumulative = jnp.cumsum(delta1_by_block, axis=1) cache_var_upper_div_lower = jnp.pad( deltav_by_block_cumulative_normalized[:, :−2], ((0, 0), (2, 0), (0, 0), (0, 0)), ) cache_var_lower = jnp.pad( delta1_by_block_cumulative[:, :−2], ((0, 0), (2, 0), (0, 0)) ) return cache_var_upper_div_lower, cache_var_lower Code 3: Jax/Flax pseudocode to get cache variables for all blocks; matmul version 29Published as a conference paper at ICLR 2024 def get_cache_vars(z, v, n_code): # throughout this function, we often use clipping of elementwise denominators at 1 to avoid nans. # in the places where we do this, it does not alter the actual cache variable estimates # since the corresponding entries in the numerator will be zero when the clip is applied. delta = jax.nn.one_hot(z, num_classes=n_code, dtype=v.dtype, axis=−1) delta1_by_block = jnp.einsum(\"bris−>brs\", delta) deltav_by_block = jnp.einsum(\"bris,briv−>brsv\", delta, v) deltav_by_block_normalized = jnp.divide( deltav_by_block, jnp.clip(delta1_by_block[..., None], a_min=1.0), ) def merge_func(a, b): a_upper_div_lower = a[0] b_upper_div_lower = b[0] a_lower = a[1] b_lower = b[1] lower_new = a_lower + b_lower term1 = jnp.multiply( jnp.divide(a_lower, jnp.clip(lower_new, a_min=1.0))[..., None], a_upper_div_lower, ) term2 = jnp.multiply( jnp.divide(b_lower, jnp.clip(lower_new, a_min=1.0))[..., None], b_upper_div_lower, ) upper_div_lower_new = term1 + term2 return upper_div_lower_new, lower_new assoc_scan_output = jax.lax.associative_scan( fn=merge_func, elems=(deltav_by_block_normalized, delta1_by_block), reverse=False, axis=1, ) deltav_by_block_normalized_cumulative = assoc_scan_output[0] delta1_by_block_cumulative = assoc_scan_output[1] cache_var_upper_div_lower = jnp.pad( deltav_by_block_normalized_cumulative[:, :−2], ((0, 0), (2, 0), (0, 0), (0, 0)), ) cache_var_lower = jnp.pad( delta1_by_block_cumulative[:, :−2], ((0, 0), (2, 0), (0, 0)) ) return cache_var_upper_div_lower, cache_var_lower Code 4: Jax/Flax pseudocode to get cache variables for all blocks; associative scan version 30",
      "meta_data": {
        "arxiv_id": "2309.16354v2",
        "authors": [
          "Lucas D. Lingle"
        ],
        "published_date": "2023-09-28T11:26:52Z",
        "pdf_url": "https://arxiv.org/pdf/2309.16354v2.pdf"
      }
    },
    {
      "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers",
      "abstract": "Transformers are the backbone of powerful foundation models for many Vision\nand Natural Language Processing tasks. But their compute and memory/storage\nfootprint is large, and so, serving such models is expensive often requiring\nhigh-end hardware. To mitigate this difficulty, Post-Training Quantization\nseeks to modify a pre-trained model and quantize it to eight bits or lower,\nsignificantly boosting compute/memory/latency efficiency. Such models have been\nsuccessfully quantized to four bits with some performance loss. In this work,\nwe outline a simple scheme to quantize Transformer-based models to just two\nbits (plus some overhead) with only a small drop in accuracy. Key to our\nformulation is a concept borrowed from Harmonic analysis called Fusion Frames.\nOur main finding is that the quantization must take place not in the original\nweight space, but instead in the Fusion Frame representations. If quantization\nis interpreted as the addition of noise, our casting of the problem allows\ninvoking an extensive body of known consistent recovery and noise robustness\nguarantees. Further, if desired, de-noising filters are known in closed form.\nWe show empirically, via a variety of experiments, that (almost) two-bit\nquantization for Transformer models promises sizable efficiency gains. The code\nis available at https://github.com/vsingh-group/FrameQuant",
      "full_text": "FrameQuant: Flexible Low-Bit Quantization for Transformers Harshavardhan Adepu 1 Zhanpeng Zeng 1 Li Zhang 2 Vikas Singh 1 2 Abstract Transformers are the backbone of powerful foun- dation models for many Vision and Natural Lan- guage Processing tasks. But their compute and memory/storage footprint is large, and so, serv- ing such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre- trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quan- tize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if de- sired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. The code is available at https://github.com/ vsingh-group/FrameQuant 1. Introduction Transformer-based Large Language Models (LLMs) domi- nate the landscape for Natural Language Processing tasks such as language translation and text summarization (Zhang et al., 2023; Touvron et al., 2023; Zhang et al., 2022b). Vision Transformers (VITs) adapt this idea for computer 1University of Wisconsin-Madison 2Google Research. Corre- spondence to: Harshavardhan Adepu <adepu@wisc.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). vision, and achieve state-of-the-art results on image classi- fication (Zhai et al., 2022), object detection (Zhang et al., 2022a), generation (Chang et al., 2022; Hudson & Zitnick, 2021) and segmentation (Cheng et al., 2022; Ranftl et al., 2021). There is general agreement that scale provides re- markable new capabilities. While large models offer strong performance improvements, their deployment as a module within a product creates unique challenges. For example, serving these models on ex- pensive hardware can drastically increase data center costs. Even loading these models on consumer-grade machines is difficult, and the ability to handle heterogeneous resource- constrained devices is almost infeasible. This has led to various efficiency-focused strategies for model compression including but not limited to distillation (Hinton et al., 2015; Zhu et al., 2021), pruning (Chen & Zhao, 2019), sparsity (Yu et al., 2012; Yun et al., 2020) and quantization (Han et al., 2016; Banner et al., 2019). Among these methods, Post-Training Quantization offers unique advantages in that it does not change the model architecture or training scheme. This paper presents a new Post-Training Quantization scheme, FrameQuant, that offers much more flexibility to strike a balance between reducing model size and preserving model quality. Specifically, FrameQuant offers what may be considered equivalent to using a fractional number of bits for quantization, e.g., 2.1 or 2.2 bits: this is valuable because for large Transformer-based models like GPT, model quality deteriorates fast (Frantar et al., 2023) as we reduce bit width in the low-bit quantization regime (e.g., 2-bit). Further, de- pending on the accuracy needs of the downstream task at hand or a desire to control the worst-off error, more flexi- bility offers the user more control. Towards this goal, our main idea is to compute a specific type of redundant/over- complete representation of a pre-trained weight matrix and quantize the matrix in that representation. We will see how robustness to quantization error will follow naturally from our choice of representation. The de-quantization step uses a straightforward scheme to re-construct the full-precision weights. We leverage a mature concept from Harmonic analysis, Fusion Frames, as the foundation for our proposal. Fusion Frames (Donoho et al., 1998; Christensen, 2018) serve an important role in signal processing in analog-to- digital conversion and signal transmission. Frames are guar- 1 arXiv:2403.06082v2  [cs.LG]  31 Jul 2024Flexible Low-Bit Quantization for Transformers anteed to be robust when the Frame coefficients are cor- rupted by additive noise. They are numerically stable, and if additional compute/memory overhead is acceptable, de- noising filters with good theoretical properties or provably optimal recovery schemes are known. To our knowledge, Frame theory for neural network quantization is unexplored. Our key contributions include (a) an approach that offers fractional bit quantization capabilities with theoretical guar- antees. (b) We empirically verify that Transformer-based models can be quantized to two bits (or 2.x bits), on an ex- tensive basket of 15 popular Vision Transformers and Large Language Models from the OPT (Zhang et al., 2022b) as well as Llama2 (Touvron et al., 2023) classes. We achieve consistent improvements over all existing baselines. 1.1. Related Work Given the growth in the scale of foundation models com- mon in our community, model compression is an active topic of research. Distillation (Hinton et al., 2015; Zhu et al., 2021), pruning/shrinking (Chen & Zhao, 2019) and the use of sparsity is quite common (Yu et al., 2012; Yun et al., 2020). There is growing interest (Rokh et al., 2023; Namburi et al., 2023; Gholami et al., 2022) in approaches that perform model compression via quantization either (i) during training or (ii) post-training since minimal changes to the architecture are needed. Quantization during training works well (Gholami et al., 2022; Nagel et al., 2021), but models must be re-trained. Post-training quantization (PTQ) methods (Nagel et al., 2019) simply quantize a pre-trained model on a small calibration set, and involve much less work. These methods are effective for large language models like OPT (Zhang et al., 2022b), BLOOM (Scao et al., 2023) and can reduce the bit-width with only a small degradation in performance. For example, (Nagel et al., 2020) analyzed the effect of data-dependent rounding. A layer-wise proxy loss was studied and AdaRound quantization was proposed to efficiently minimize this loss. The approach in (Frantar & Alistarh, 2022) minimizes the squared error similar to (Nagel et al., 2020), but quantizes each layer individually while adjusting the remaining unquantized weights using the Hessian of the proxy loss term following (Lecun et al., 1989; Hassibi et al., 1993). OPTQ (Frantar et al., 2023)(for- merly GPTQ) extended upon the ideas in OBQ (Frantar & Alistarh, 2022), and offered other adjustments that gives a stable scheme that can compress large language models like OPT-175B or BLOOM-176B to 3 or 4 bits per parameter without a large loss in accuracy. For Vision Transformers, PTQ4ViT (Yuan et al., 2022) quantifies the weights in two stages, and uses a Hessian-guided search for the optimal scale for the weights. In (Liu et al., 2021b), a feature map is used to search for the optimal quantization interval for maintaining similarity between the quantized and original feature maps. The method also chooses different bit widths for each layer. Other strategies proposed for PTQ include (Ding et al., 2022; Li et al., 2023). We note a recent con- current result for two-bit quantization for language models reported in (Chee et al., 2023). Our approaches are based on different starting points: our choice of Frame theory to min- imize quantization error versus the choice in (Chee et al., 2023) of using incoherence as a pre and post-processing step, which is later shown to offer desirable theoretical prop- erties. But fundamentally, both methods work well due to similar underlying principles related to basis expansions (on a space-filling basis). We discuss later how (Chee et al., 2023) can be viewed as a special version of our formulation (but with no redundancy). 2. Finite Frame Theory and Fusion Frames Frames generalize the Orthogonal basis decomposition of a Hilbert space and provide redundant representations. Finite frames find applications in robust signal transmission with quantization and erasures (Goyal et al., 1998; 2001; Casazza & Kovaˇcevi´c, 2003), Coding theory (Strohmer & Heath Jr, 2003), distributed processing (Casazza et al., 2008), Com- pressed Sensing (Boufounos et al., 2009) among others. We start with a brief review of relevant concepts. Readers familiar with these concepts may skim this section. Consider a finite-dimensional Hilbert space H of dimension d. Throughout the paper, we denote this space as Hd. Definition 2.1 (Frames). A family of k vectors ϕ = (φi)k i=1 in Hd is called a frame for Hd if there exist con- stants 0 < A≤ B <∞ such that A||x||2 ≤ kX i=1 |⟨x, φi⟩|2 ≤ B||x||2 (1) for all x ∈ Hd where ⟨·, ·⟩ is the dot-product. The constants A and B are the lower and upper frame bounds. The sandwich expression suggests that x will not be poorly distorted when we calculate its inner products with a frame. When A = B, ϕ is called a A-tight frame. When A = B = 1, we get a Parseval’s frame. Fig. 1 shows examples of Tight Frames for R2 for different k’s. The lower bound is equivalent to asking that ϕ span H. So, for a frame, we always have k ≥ d. If k = 3d, the redundancy is r = 3. Fusion Frames provide a way for fusing “smaller” frames to construct large frames, offering various efficiency and robustness properties (Eldar & Michaeli, 2008). Formally, Definition 2.2 (Fusion Frames). Let (Wi)k i=1 be a family of subspaces in Hd, and let (wi)k i=1 ⊆ R+ be a family of weights. Then, ((Wi, wi))k i=1 is a fusion frame for Hd, if 2Flexible Low-Bit Quantization for Transformers Figure 1.Examples of Tight frames of k = 4, 5, ...,11 in R2 there exists constants 0 < A≤ B <∞ such that A||x||2 ≤ kX i=1 w2 i ||Ui(x)||2 ≤ B||x||2 for all x ∈ Hd where Ui denotes the orthonormal projection onto the sub- space Wi for each i. The constants A and B still denote the lower and upper fusion frame bounds respectively. Similar to the Frames case, the Fusion Frame((Wi, wi))k i=1 is referred to as a tight fusion frame if A = B and as a Parseval fusion frame if A = B = 1. Finally, if wi = 1 for all i, we simply utilize the notation (Wi)k i=1. 2.1. Operators in Fusion Frames Fusion Frame (FF) operators can be formally defined using a Hilbert direct sum. Since we use the operators for model quantization, without loss of generality, we describe them in terms of vectors and matrices, to keep notations simple. Let ((Wi, wi))k i=1 be a Fusion Frame for Hd with orthonormal basis (Pi)k i=1 respectively. The Analysis operator TW takes a signal x ∈ Hd and com- putes its dot product with all the basis (Pi)k i=1. The results represent x w.r.t. the FF as TW : x 7→ (wiPT i (x))k i=1 (2) The Synthesis operator T ∗ W is the adjoint of the analysis operator, and takes a sequence of representation vectors (yi)k i=1 and outputs a signal in Hd: the reconstruction of the original signal from its FF representation is defined as T ∗ W : (yi)k i=1 7→ kX i=1 wiPi(yi) (3) The Fusion Frame operator SW is defined as the compo- sition of these two operators. It first computes the FF rep- resentation of a signal in Hd in different subspaces using the Analysis operator. Then, when needed, we can recon- struct the signal back from these representations using the Synthesis operator. When the Fusion Frame is tight, the reconstruction is exact (Casazza et al., 2011). Formally, SW = T ∗ WTW : x 7→ kX i=1 w2 i Ui(x) (4) Here, Ui = PiPiT is the orthogonal projection onto the subspace Wi. If the Fusion Frame is tight, we have SW = AId where Id is the d × d Identity Matrix. Throughout, we will use Parseval Fusion Frames, where the frame bounds A = B = 1. Fusion Frames offer many other properties but due to space, we will keep the presentation focused. How will Fusion Frames be used? An easy way to see Fusion Frames in practice is to work out a simple example, Example 1. Consider the Euclidean space Hd = R4. Say, an oracle gives us a Fusion Frame where we have k = 3 subspaces, and each subspace is of equal dimension ρ = 2. For notational ease, we represent these subspaces with their Synthesis operator T ∗ W =     0.57 0 .00 0.00 0 .57 0.57 0 .00 0.00 0 .57  ,   0.57 0 .00 0.00 0 .57 −0.28 0 .50 −0.50 −0.28  ,   0.57 0 .00 0.00 0 .57 −0.28 −0.50 0.50 −0.28     We want to compute the FF representation of a signal x = \u0002 −1 −0.5 0 .5 1 \u0003T . To do so, we must apply the Analysis operator TW on x. The Analysis operator is simply based on the individual transposes in T ∗ W defined above. \u0014 0.57 0 .00 0 .57 0 .00 0.00 0 .57 0 .00 0 .57 \u0015 , \u0014 0.57 0 .00 −0.28 −0.50 0.00 0 .57 0 .50 −0.28 \u0015 · · · Applying TW on x, we get the FF representations TW(x) = \u0012\u0014−0.28 0.28 \u0015 , \u0014−1.22 −0.32 \u0015 , \u0014−0.22 −0.82 \u0015\u0013 To get the actual projections ofx onto different subspaces Wi, we multiply these coefficients with the scaled orthonor- mal basis (wiPi)k i=1 of their corresponding subspaces (w2 i Ui(x))3 i=1 =     −0.1667 0.1667 −0.1667 0.1667  ,   −0.7053 −0.1890 0.1890 0.7053  ,   −0.1280 −0.4777 0.4777 0.1280     We can verify by checking the identity SW = Id or check- ing that P3 i=1 w2 i Ui(x) = x (only accurate up to rounding errors) that this Fusion Frame is a Parseval’s frame. Ap- plying the Synthesis operator T ∗ W on the projections above recovers x perfectly. Corrupting FF representations by noise.What happens when the Fusion frame representations are corrupted by noise, say due to erasure or quantization? Because of re- dundancy in the representation of a signal, we expect some immunity to corruptions in the representations due to noise. 3Flexible Low-Bit Quantization for Transformers In the current example, this is indeed the case. If we add noise to TW(x) with an SNR of 10dB and use the noisy coefficients to reconstruct x back, we observe an MSE re- duction of 33% at a redundancy factor of r = 1.5× and 50% MSE reduction r = 2×, consistent with theory (Goyal et al., 1998). Quantizing Transformer layers. Let us consider quan- tizing each layer in a Transformer model as in (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Yuan et al., 2022), e.g., by quantizing individual weights or columns, one by one. First, notice that the quantization error/noise is weight-dependent. Further, the error will also depend on how all other weights are quantized. The only way to guide a quantization scheme is the evaluation of a loss (to be described shortly) on a small calibration dataset D. In this regime, even with strong assumptions on the noise, it is difficult to say much about the quality of the de- quantization. On the other hand, far more is known (Goyal et al., 1998; Waldron, 2019; Casazza & Kutyniok, 2012) about the behavior of quantization of data given in an appro- priate Frame basis (e.g., Fusion Frames), and error bounds on the reconstruction are available. Put simply, quantiza- tion noise in the space of Frame projections incurs far less error in the reconstructions due to the robustness of Frame representations. §3 will leverage this principle. 2.2. Tight Fusion Frames and their construction We first define the type of Fusion Frames we will use and then describe how they can be constructed. Definition 2.3 (Tight Fusion Frames or TFF). For A >0 and with Id giving the d × d Identity matrix, a (k, ρ, d)- TFF is a sequence {Ui}k i=1 of d × d orthogonal projection matrices of rank ρ and scalars {wi}k i=1, wi > 0 such that kX i=1 w2 i Ui = AId. (5) A (k, ρ, d)-TFF is a sequence of k equidimensional sub- spaces of dimension ρ in a d-dimensional space, and Ui is the orthogonal projection matrix onto the ith sub-space. Constructing TFFs. The algorithm in (Casazza et al., 2011) can be used to generate TFFs if we provide the dimension d, the number k of subspaces we need, and the dimension ρ of each of these subspaces. The algorithm has two main steps. First, one generates a Tight Frame of d unit norm vectors for the complex domain Cρ. Then, this Frame is modulated with the square roots of unity to generate the k subspaces for Cd. We use a simple construction described in (Fickus et al., 2023) to extend these Fusion Frames to Rd. Since it can be used as a black-box module, we skip the details and include a brief synopsis in Appendix §J. Remarks. A few properties are useful to note. This Fusion Frame construction is sparse/block diagonal and can be gen- Figure 2.Illustration of standard calculation (on top) versus the corresponding calculations in FF space (bottom) erated one subspace at a time. To generate another Fusion Frame, we can hit it with a random rotation. Depending on the Transformer model at hand, the dimension of the acti- vations of the layer determines d. For a desired redundancy factor (k × ρ ≥ d) in our frames, given d we simply choose a k and ρ such that they are valid (i.e., a TFF exists for the triple (k, ρ, d)) according to (Casazza et al., 2011). If not, we use a slightly lower redundancy factor r knowing that we will always have a trivial solution for k = 1 and ρ = d. 3. Fusion Frames based Quantization We can now leverage the ideas described in the preceding sections for quantizing the parameters of a Transformer model. Consistent with common PTQ approaches (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Yuan et al., 2022), we perform quantization layer-by-layer, minimizing the proxy loss between the quantized and non- quantized output of the layer. What are analogous calculations in FF space? Consider a layer l in a Transformer model, with parameters Θl. Let ˘Aprev be the activation of the already quantized previous layer for the examples in the calibration set D. The (non- quantized) output Zl of layer l is Zl = Θl ˘Aprev (6) Here, Θl maps the input ˘Aprev to the output Zl. To avoid directly quantizing Θl, we want the quantization noise to instead impact the analogous terms in the Fusion Frame representation (but equivalent calculation as (6)). To this end, let us set up some notations. In general, the dimension of Zl and ˘Aprev may not be the same. So, the number of subspaces in their respective Fusion Frames will be differ- ent. Let k, kprev denote the number of subspaces for Zl and ˘Aprev respectively. In other words, Wl = ( Wl i)k i=1 and Wprev = (Wprev i )kprev i=1 . Let the sequence of orthonor- mal basis for the subspaces of Wl and Wprev be given by (Pl i )k i=1 and (Pprev i )kprev i=1 respectively. To reduce notational clutter, we absorb the scalars wi into Pi. To write down the expression in FF space, for simplicity, let us vectorize the 4Flexible Low-Bit Quantization for Transformers set of orthogonal basis above and define Pl = [Pl 1Pl 2 . . . Pl k] and Pprev = [Pprev 1 Pprev 2 . . . Pprev kprev ] Taking the FF representations of the output Zl means PT l Zl = PT l Θl ˘Aprev| {z } =Zl (7) Rearranging brackets, PT l Θl ˘Aprev = PT l Θl(PprevPT prev) ˘Aprev (8) = (PT l ΘlPprev)(PT prev ˘Aprev) (9) In the above expression, the object (PT l ΘlPprev) maps the FF representation of ˘Aprev, i.e., (PT prev ˘Aprev), to the FF representation of (PT l Zl). This operation is completely in the FF representation space as desired. A notation simplification allows us to cross-reference what our FF-space calculations are doing w.r.t. the objective function. Let Cprev = PT prev ˘Aprev and Dl = PT l ΘlPprev. Our objective is to quantize Dl to ˆDl while minimizing the proxy loss in terms of FF representations, L( ˆDl) = ||DlCprev − ˆDlCprev||2 F = tr((Dl − ˆDl)T CT prevCprev(Dl − ˆDl)) = tr((Dl − ˆDl)CprevCT prev(Dl − ˆDl)T ) The term ˜H = CprevCT prev corresponds to the Hessian prominent in most published results on PTQ strategies (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Chee et al., 2023). So, our loss is the same as other approaches, except that we are operating in the FF represen- tation space and enjoy all the associated noise robustness properties. Further, because the loss for quantizing the trans- formed weights Dl is the same as e.g., (Frantar et al., 2023), we can directly use the Hessian-based iterative quantization algorithms in (Frantar & Alistarh, 2022; Frantar et al., 2023) with minimal changes. Finally, following recent results in Post-training Quantization (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Chee et al., 2023) we primarily focus on quantizing the transformed weights (Dl) but include one experiment with a simple activation quan- tization in §F. We note that there are standalone activation quantization strategies for smaller Vision models for up to four bits, see (Ding et al., 2022; Yuan et al., 2022). Details of the quantization procedure.Other than working in the FF space, the quantization itself is almost identical to (Frantar et al., 2023). We use the iterative method from (Frantar et al., 2023) with some modifications to improve the stability of our algorithm. For example, we found that clipping the weights before calling the iterative scheme Figure 3.Inference for a FrameQuant quantized model. from GPTQ reduces the weight range during quantization. This effectively adds more quantization noise to the outlier weights that are too large. Since Fusion Frames spread out the energy uniformly among different subspaces, we observe that there are only a few outliers in the transformed Weight matrices, and hence clipping them boosts performance. We found that simply clipping the weights at 2σ (assuming a Normal distribution), where σ is the standard deviation of Dl, works well in practice. We observe that this change also helps the method in (Chee et al., 2023) (and this modified algorithm is also included in our baselines). Alg. 1 shows the sequence of steps in FrameQuant. Algorithm 1 FrameQuant Require: Weight matrix Θl, previous layer activations ˘Aprev, input and output Fusion Frames Pl, Pprev, block size B 1: Compute Cprev = PT prevAprev, Dl = PT l ΘlPprev 2: Compute σ = std(Dl), µ = mean(Dl) 3: Dl = 2σ clip(Dl, µ− 2σ, µ+ 2σ) 4: ˆDl = quantize(Dl, Cprev, B) // modified GPTQ 5: Store ˆDl // store the quantized matrix ˆDl return Pl ˆDlCprev // return quantized layer activations 3.1. Robustness of Fusion Frames We now state some technical results that apply to both Frames and Fusion Frames. (a) Redundancy related guarantees. During quantization, the Fusion Frame coefficients are corrupted. This can be modeled as an additive noise being added to these coeffi- cients. Assume that the redundancy factor is r >1. Even with classical analysis, the result in (Rozell & Johnson, 2005; Goyal et al., 1998) shows that when using Tight Frames to reconstruct the signal from noisy coefficients, for memoryless quantization, we get an MSE reduction of O(1/r). A rate of O(1/r2) for consistent reconstruction can also be achieved by solving an LP during the dequan- tization step (Goyal et al., 1998). While this may not be preferred in practice, we know that if adopted, this matches 5Flexible Low-Bit Quantization for Transformers the lower bound of (1/r2), see Ch. 2 in (Goyal et al., 1998). (b) Another benefit of Frame representations is that recon- struction can “denoise” using filters available in closed form. For example, with Tight Frames, it is known that the Wiener filter provably minimizes the MSE, see Ch. 13 in (Casazza & Kutyniok, 2012), (Kutyniok et al., 2009). In our exper- iments, we found that even a diagonal approximation of the Wiener filter helps. But our experimental results are reported without utilizing this boost. 3.2. Inference Procedure During inference, the quantized model is loaded into mem- ory. At each layer, the inputs to the layer ( ˘Aprev) are first transformed into their Fusion Frame representations using the analysis operator PT prev. The FF representations are then transformed by the quantized weights (Dl) for this layer into the FF representations of the output. Finally the synthesis operator Pl is used to compute the layer outputs. Figure 3 shows this dequantization process and the bit widths of each of these operations for a single layer in a network. 4. Experiments We performed an extensive set of experiments comparing FrameQuant with several quantization baselines for Vision models and Language models. The goal is to assess (a) per- formance metrics of different methods on benchmark tasks and (b) how close low-bit quantization can approach the full precision performance with a small degree of represen- tation redundancy. We use image classification task (Deng et al., 2009) for Vision models and Perplexity for Language models. We start with an overview of our experimental setup. We present the evaluation results of FrameQuant on 15+ Vision Transformer architectures+configurations for image clas- sification. Next, we conduct an ablation study on image classification task to better understand the behavior of dif- ferent components of FrameQuant. We then present results on Language models such as OPT (Zhang et al., 2022b) and Llama2 (Touvron et al., 2023) by comparing perplexity and accuracy in downstream tasks. The appendix includes many additional experiments. 4.1. Experimental Setup We evaluate our method on the ImageNet-1K classification task. For quantizing the model weights of the pre-trained models obtained from the Huggingface hub (Wightman, 2019), we use 128 images randomly selected images from the training dataset as calibration dataset D. We quantize the parameter matrices of the layers sequentially from shal- low layers to deep layers, similar to (Frantar et al., 2023). After quantizing each layer, we pass the inputs to the layer again and send the output with the quantized weights to the next layer. Finally, we evaluate the quantized models on the ImageNet-1K validation dataset and report the top-1 accu- racy. All our “base” experiments correspond to 2 bits. We note that one of the baselines, PTQ4ViT (Yuan et al., 2022), performs activation quantization together with weight quan- tization, but was not tested in the extreme 2 bit quantiza- tion setting. To ensure fair comparisons to that method, we switch off activation quantization in their method and also add another experiment with 3 bits. For additional ex- periments with activation quantization, Segmentation and Object Detection tasks, we refer the reader to the Appendix sections F, G respectively. 4.2. Results on ImageNet Classification Task We use model architectures (including ViT (Dosovitskiy et al., 2021), DeiT (Touvron et al., 2021), DeiT III (Touvron et al., 2022), and Swin (Liu et al., 2021a)) and model sizes (including small, medium, large, huge) that are available on the Huggingface hub (Wightman, 2019). Our main results for these experiments are shown in Tab. 1–2. Figure 4a shows the performance of the different classes of models on the ImageNet-1K dataset. We observed that clipping the weights at 2σ also helps QuIP (Chee et al., 2023), so we include it as an additional baseline. Even with a redundancy factor of r = 1, FrameQuant achieves better accuracy com- pared to most baselines under consideration. Further, with a redundancy factor of r = 1.1, FrameQuant outperforms all baselines by a good margin and is respectably close to the full precision model, underscoring the robustness of Fusion Frames in the presence of quantization noise. We observe that adding more redundancy to the Frame representations continues to improve the performance of the quantized mod- els, especially when the models are small. See §A for more details. We note that the codebase for PTQ4ViT (Yuan et al., 2022) was not compatible with the Swin-L model, so we could not report their performance for this model. 4.3. Ablation Study In this section, we dissect FrameQuant to understand the contribution of different components of our algorithm. Ta- ble 3 shows the results of this experiment. We use GPTQ (Frantar et al., 2023) as our starting point. With GPTQ (Frantar et al., 2023) alone, the performance drops in the quantized models are significant: as high as 82% for the DeiT III (Touvron et al., 2022) Base model. Simply with the FF representation added (column TFF), we see improve- ments in performance across all models, with a maximum improvement of 56% for DeiT III-H. We note that some of the smaller-sized models are yet to see all the benefits of FF representations. That is because these models have outliers in the weights (much larger than the remaining weights) which results in higher quantization errors. The FF repre- 6Flexible Low-Bit Quantization for Transformers Method #bits ViT DeiT Swin T S S/32 B T S B S B B/384 Full-Precision 32 75.46 81.39 75.99 85.10 72.16 79.85 81.98 82.88 84.67 86.02 PTQ4ViT 2 0.33 0.55 0.71 0.41 1.51 4.47 25.54 12.54 0.15 0.15 GPTQ 2 0.40 0.40 0.39 29.26 1.60 4.23 41.00 43.54 47.38 57.52 QuIP 2 1.42 21.98 19.00 77.54 12.93 51.62 75.51 71.58 74.91 79.85 QuIP (with our 2σ clip) 2 9.10 48.96 41.41 79.54 30.49 65.70 77.69 76.34 79.17 82.40 FrameQuant (r = 1.0) 2 8.92 48.10 41.16 79.53 31.73 66.35 77.62 77.91 80.16 82.44 FrameQuant (r = 1.1) 2.2 25.79 61.51 53.85 80.93 46.48 70.43 78.67 78.77 81.33 83.42 PTQ4ViT 3 18.32 36.18 22.20 21.43 51.73 69.65 75.35 73.01 69.46 70.68 Table 1.ImageNet-1k Top-1 validation accuracy of Tiny to Base sized Vision Transformer-based models when quantized to 2 (or 3) bits by different methods. FrameQuant with a redundancy factor of r = 1already performs better or on par with Quip (Chee et al., 2023). With a slightly higher redundancy factor of r = 1.1, we get the best performance of all the methods under consideration. (a) Validation accuracies for different classes of Transformer models for Vision on ImageNet-1K (b) Weights distribution in a ViT layer and the 2σ thresholds Figure 4.(a) Validation accuracies of Vision Transformers on ImageNet-1K dataset. We can see FrameQuant closing the gap between the full precision model with increasing redundancy. Each dot in the plot corresponds to a model from tables 1-2 combined. (b) shows the distribution of weights in a ViT layer and the 2σ thresholds for clipping. We see that our thresholding keeps most of the mass while removing outliers. Method #bits ViT DeiT III Swin L H L H L Full-Precision 32 85.84 87.59 86.97 87.19 85.95 PTQ4ViT 2 37.05 00.18 2.14 55.57 - GPTQ 2 63.08 42.63 68.43 28.20 71.69 QuIP 2 82.22 84.58 84.76 86.27 83.61 QuIP (our 2σ clip) 2 83.17 85.31 85.48 86.38 84.27 FrameQuant (r = 1.0) 2 83.22 85.49 85.45 86.62 84.25 FrameQuant (r = 1.1) 2.2 83.67 85.99 85.75 86.68 84.42 PTQ4ViT 3 81.26 78.92 83.63 85.39 - Table 2.ImageNet-1K Top-1 validation accuracy of Large and Huge sized Vision Transformer-based models when quantized to 2 (or 3) bits by different methods. sentation yields a nice enough distribution that we can fit a Normal distribution. So, after we clip these weights at the ±2σ level, we see improvements in performance because of the outlier removal. Clipping is most effective once the weights are nicely distributed. A direct application of clip- ping on the weights has limited efficacy and incurs errors for weight configurations that are degenerate/poorly distributed, see D.1 for more details. Finally, we add a redundancy fac- tor of r = 1.1 and the FF representations take advantage of this redundancy: we see the best performance across the board. Impact of Gaussian assumption on the weights distri- bution. Figure 4b shows a representative example of the distribution of weights in a model from the ViT family and why the 2σ clipping seems reasonable for capturing most of the mass. The weights distribution for models from DeiT and Swin Transformer are shown in Figure §13. 4.4. Results on Language Models In this experiment, we evaluate the perplexity of quantized models from the OPT (Zhang et al., 2022b) and Llama2 (Touvron et al., 2023) family on two datasets - WikiText2 (Merity et al., 2017) and C4 (Raffel et al., 2020). Figure 5 shows the perplexity of models from the OPT family as the size is increased. We see that FrameQuant at 1× redun- dancy performs better than all other quantization methods. With a redundancy factor of 1.1×, FrameQuant reduces the performance gap with the full precision models as suggested by the theory. We see similar results for models from the Llama2 family as well. We also finetuned the Llama2-7B model quantized by various methods on diverse downstream 7Flexible Low-Bit Quantization for Transformers GPTQ TFF 2σ clip Redundancy ViT DeiT III Swin r = 1.1 S B H S B H S B L ON OFF OFF OFF 0.4 29 .26 42 .63 0 .45 8 .5 28 .2 43 .54 47 .38 71 .69 ON ON OFF OFF 0.88 59 .87 68 .75 1 .48 29 .92 84 .33 61 .01 60 .21 79 .52 ON ON ON OFF 48.10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 ON ON ON ON 61.51 80.93 85.99 65.33 80.91 86.68 78.77 81.33 84.42 Full Precision 81.39 85 .1 87 .59 83 .06 85 .7 87 .19 82 .79 84 .7 85 .95 Table 3.Incremental impact of various steps in FrameQuant on ImageNet-1k accuracy for different Transformer models in Vision Method #bits acc mm-acc Full-Precision 32 84.19 84 .67 ZeroQuant 4.33 78.69 78 .07 ZeroQuant 3.66 54.91 56 .45 ZeroQuant 2.66 38.00 38 .30 FrameQuant 2.2 80.02 79.37 Table 4.Performance of the BERT model quantized with Zero- Quant and FrameQuant on the MNLI dataset. FrameQuant per- forms better than ZeroQuant even with a lower bit-width than ZeroQuant. Method bits OPT Llama2 125M 1.3B 2.7B 6.7B 7B 70B Full-Precision 16 27.65 14.62 12.47 10.86 5.68 3.32 GPTQ 2 5.7e3 8.9e3 9.1e3 3.1e3 6.4e3 140.5 QuIP 2 913.0 37.59 22.86 15.67 26.02 6.21 FrameQuant 2 345.7 30.54 20.67 15.72 14.85 5.50 FrameQuant 2.2 131.2 22.68 15.86 13.53 8.48 4.67 Table 5.Perplexity (lower is better) of Llama2 and OPT models on WikiText2 dataset when quantized to 2 (or 2.2) bits by different methods. tasks and observed a maximum accuracy boost of 41% by FrameQuant at r = 1.1× compared to vanilla GPTQ. Table 5 summarizes the perplexity of all the models on the Wiki- Text2 (Merity et al., 2017) dataset. Results on downstream tasks/additional datasets is in Appendix §H. 4.5. Comparision with Mixed-precision Quantization A redundancy factor of 1.1 is the same as an average bit- width of 2.2 per weight parameter. Mixed-precision quan- tization methods can achieve fractional bit-widths by us- ing different bit-widths for different weights in the model. We compare FrameQuant with a recent Mixed-precision method, ZeroQuant (Yao et al., 2022). We test FrameQuant with a bit-width of 2 and a redundancy factor of 1.1 relative to ZeroQuant at different fractional bit-widths. As shown in Table 4. FrameQuant performs favorably with ZeroQuant, even at low bit widths. (a) Perplexity on WikiText2  (b) Perplexity on C4 Figure 5.Perplexity of models from OPT family on WikiText2 and C4 datasets. FrameQuant performs better than all other quan- tization methods under consideration. We can also see that the performance gap between the quantized models and the unquan- tized model goes down as the size of the models increases. Llama2 7B Llama2 13B Original model 13G 25G FrameQuant 2.1G 3.6G Table 6.Size of original and quantized model with FrameQuant. 5. Other Practical Considerations 5.1. Storage requirements Weight quantization has a direct improvement on the storage needs of the models. Table 6 shows the sizes of compressed Llama2 models. FrameQuant reduces the size of the models by around 85% on average. 5.2. Inference speeds Since FrameQuant involves additional operations to com- pute and transform the weights from the low-bit Fusion Frame representations to the regular weight space, the raw inference speed is expected to be lower than GPTQ. On the other hand, at 2 bits, the accuracy/perplexity of FrameQuant is much better than GPTQ. So, there is a trade-off. Table 7 shows the inference speeds of the quantized models on a Nvidia A100 GPU. Here, we used the block diagonal struc- ture of Fusion Frames and a Hadamard transform-based fast random projection based on (Dao, 2023; Zeng et al., 2023) for the rotation matrices. This inference speed can be improved by using efficient kernels to load the weights into the GPU and perform the transformations. 8Flexible Low-Bit Quantization for Transformers Method Llama2 7B Llama2 13B GPTQ 1425.07t/s 844.03t/s FrameQuant 974.20t/s 607.01t/s Table 7.Inference speed in tokens/sec (t/s) of quantized models with GPTQ and FrameQuant. 6. Discussion We cover a few additional aspects that were not explicitly discussed thus far. (1) Can we reduce to one bit? We per- formed experiments with redundancy of 1.8× with 1 bit per weight but were unsuccessful. For one bit, once the redundancy has exceeded r = 2, it makes more sense to just use two bits. (2) Can FrameQuant run as QuIP? For each layer, if we choose a Fusion Frame with a redundancy factor r = 1 and the random orthonormal basis Pl, Pprev, we get a setup similar to QuIP (Chee et al., 2023) after removing the 2σ weight clipping. This is also why when QuIP is augmented with our 2σ clipping we see similar results to FrameQuant with 1× redundancy. (3) Additional storage needed?: Since there are efficient deterministic al- gorithms to generate Fusion Frames, during inference, only knowledge of (k, ρ, d) is needed. For rotations, we only need knowledge of the seed. Also, since many layers in a Transformer model have the same shape, these parameters can be shared across layers. Additional details on the stor- age benefits are in §K.1 (4) Why is flexibility useful? If the performance hit at the two-bit level is unacceptable for an application, the only recourse currently is to move up to three bits for existing methods ( 50% increase). However, FrameQuant allows flexibility through the choice of the re- dundancy factor r. (5) Higher bitwidths? The main focus of this work is to evaluate 2-bit quantization of the weights in Vision and Language models and to check the benefits of applying Fusion Frames in terms of flexible bit-widths. Higher bit widths such as 3 or 4-bit quantization have been studied (Frantar et al., 2023; Chee et al., 2023) and also used in practice (Gerganov, 2023). (6) Computational complexity during Inference: The core FF-related compute is similar to alternatives (Chee et al., 2023) with a small overhead related to the number of subspaces k. During inference, we need an additional compute of O(d2(kr + logd)) for transforming the weights from the Fusion Frame representation space to the regular weight space. Any quantization scheme in the low-bit regime will incur a cost of O(d2) to transform the quantized weights by scaling and shifting them. More de- tails are provided in §K.2. (7) Quantization aware training: FrameQuant can be modified to be applicable during QAT although we do not include such experiments here. One option is to use it during fine-tuning where the quantization loss is simulated, which can then be used to regularize the loss to make it more robust to quantization. Fusion Frames can meaningfully inform this bias, via an estimate of the “out of subspace error” to minimize degradation due to quan- tization. (8) Scaling laws vis- `a-vis FrameQuant? During quantization, the number of parameters does not change. Instead, each parameter has a lower degree of freedom since the number of states it can represent is reduced. We can use the (number of parameters × bit-width) as a proxy for the degree of freedom for each (quantized) model. Taking the quantization bit width into account, a line plot of test loss (on the vertical-axis) as a function of (number of parameters × bit-width) on the horizontal axis may have a different slope compared to (Kaplan et al., 2020), Fig. 1. (9) Ratio- nale for clipping: Let u be a vector in p dimensions. Let P be a projection onto a random subspace in p′ dimensions. Projecting u using P gives v as v = Pu. Assume that the entries in u have finite mean and variance and are uncorre- lated. Then each entry of v is effectively a sum of many scaled random variables. The distribution of these entries (sum of scaled variables, suitably standardized) approaches a normal distribution as the dimensionality p grows. Weak dependence or mixing can also be handled. 7. Conclusions This paper describesFrameQuant, a Frames based algorithm for flexible low-bit quantization. Quantization is motivated by the need to efficiently serve Large Language Models on heterogeneous devices and flexibility here means that while we retain the option to go as low as two bits; depend- ing on the needs of the downstream task, the user also has the flexibility to seek models with a net footprint of 2.x bits on average. Across most widely used Vision Trans- former models and Large Language models, we find that effective quantization is possible with only a small loss in performance relative to the full-precision model. Further, flexibility for a minor increase in redundancy is available and uniformly helps close the gap with full precision models. We observe, consistent with the literature, that quantization to low bit width is more favorable for larger models (in terms of a performance hit) than a similar quantization ap- plied to smaller models. While some benefits (e.g., model loading time, loading larger models) are immediate, tighter integration with the hardware can unlock far more efficiency gains. The code is publicly available. Impact Statement This paper introduces a low precision quantization method for inference. The objective is to decrease memory needs and facilitate the implementation of larger models on less powerful devices, thereby reducing costs (economic impact) and the carbon footprint (environmental impact). We have not identified any particular ethical issues that need to be emphasized. 9Flexible Low-Bit Quantization for Transformers Acknowledgments Support through the Google Cloud Platform provided the computational resources for conducting our experiments. The research was also supported in part by a Google gift award to UW-Foundation and funding from the Vilas Board of Trustees. References Banner, R., Nahshan, Y ., and Soudry, D. Post training 4-bit quantization of convolutional networks for rapid- deployment. Advances in Neural Information Processing Systems, 32, 2019. Bisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning about physical commonsense in natural language. In Pro- ceedings of the AAAI conference on artificial intelligence, 2020. Boufounos, P., Kutyniok, G., and Rauhut, H. Compressed sensing for fusion frames. Proceedings of SPIE - The International Society for Optical Engineering, 10 2009. doi: 10.1117/12.826327. Casazza, P. G. and Kovaˇcevi´c, J. Equal-norm tight frames with erasures. Advances in Computational Mathematics, 18, 2003. Casazza, P. G. and Kutyniok, G. Finite frames, theory and applications, 2012. URL https: //link.springer.com/book/10.1007/ 978-0-8176-8373-3 . Casazza, P. G., Kutyniok, G., and Li, S. Fusion frames and distributed processing. Applied and computational harmonic analysis, 25(1), 2008. Casazza, P. G., Fickus, M., Mixon, D. G., Wang, Y ., and Zhou, Z. Constructing tight fusion frames. Applied and Computational Harmonic Analysis, 30(2), 2011. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2010.05. 002. URL https://www.sciencedirect.com/ science/article/pii/S1063520310000850. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022. Chee, J., Cai, Y ., Kuleshov, V ., and Sa, C. D. QuIP: 2-bit quantization of large language models with guarantees. In Thirty-seventh Conference on Neural Information Pro- cessing Systems, 2023. URL https://openreview. net/forum?id=xrk9g5vcXR. Chen, S. and Zhao, Q. Shallowing deep networks: Layer- wise pruning based on feature representations. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 41(12):3048–3056, 2019. doi: 10.1109/TPAMI. 2018.2874634. Cheng, B., Misra, I., Schwing, A. G., Kirillov, A., and Gird- har, R. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. Christensen, O. An introduction to frames and riesz bases, 2018. URL https://link.springer. com/book/10.1007/978-3-319-25613-9 . Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Dao, T. fast-hadamard-transform, 2023. URL https://github.com/Dao-AILab/ fast-hadamard-transform. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 2009. Ding, Y ., Qin, H., Yan, Q., Chai, Z., Liu, J., Wei, X., and Liu, X. Towards accurate post-training quantization for vision transformer. In Proceedings of the 30th ACM International Conference on Multimedia, MM ’22, New York, NY , USA, 2022. ISBN 9781450392037. doi: 10. 1145/3503161.3547826. URL https://doi.org/ 10.1145/3503161.3547826. Donoho, D., Vetterli, M., DeV ore, R., and Daubechies, I. Data compression and harmonic analysis. IEEE Transactions on Information Theory, 44(6), 1998. doi: 10.1109/18.720544. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=YicbFdNTTy. Eldar, Y . C. and Michaeli, T. Beyond bandlimited sampling: Nonlinearities, smoothness and sparsity. ArXiv, abs/0812.3066, 2008. URL https://api. semanticscholar.org/CorpusID:8702589. 10Flexible Low-Bit Quantization for Transformers Fickus, M., Iverson, J. W., Jasper, J., and Mixon, D. G. Harmonic grassmannian codes. Applied and Computational Harmonic Analysis , 65, 2023. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2023.01. 009. URL https://www.sciencedirect.com/ science/article/pii/S1063520323000106. Frantar, E. and Alistarh, D. Optimal brain compression: A framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35, 2022. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Confer- ence on Learning Representations, 2023. URL https: //openreview.net/forum?id=tcbBPnfwxS. Gao, L., Tow, J., Abbasi, B., et al. A framework for few- shot language model evaluation, 2023. URL https: //zenodo.org/records/10256836. Gerganov, G. llama.cpp, 2023. URL https://github. com/ggerganov/llama.cpp. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for effi- cient neural network inference. In Low-Power Computer Vision. Chapman and Hall/CRC, 2022. Goyal, V ., Vetterli, M., and Thao, N. Quantized overcom- plete expansions in Rn: analysis, synthesis, and algo- rithms. IEEE Transactions on Information Theory, 44(1), 1998. doi: 10.1109/18.650985. Goyal, V . K., Kova ˇcevi´c, J., and Kelner, J. A. Quan- tized frame expansions with erasures. Applied and Computational Harmonic Analysis, 10(3), 2001. ISSN 1063-5203. doi: https://doi.org/10.1006/acha.2000. 0340. URL https://www.sciencedirect.com/ science/article/pii/S1063520300903403. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In Bengio, Y . and Le- Cun, Y . (eds.),4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1510.00149. Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain sur- geon and general network pruning. In IEEE international conference on neural networks. IEEE, 1993. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowl- edge in a neural network, 2015. Hudson, D. A. and Zitnick, L. Generative adversarial trans- formers. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learn- ing, volume 139 of Proceedings of Machine Learning Research. PMLR, 18–24 Jul 2021. Kaplan, J., McCandlish, S., Henighan, T., et al. Scaling laws for neural language models, 2020. Kutyniok, G., Pezeshki, A., Calderbank, R., and Liu, T. Robust dimension reduction, fusion frames, and grassmannian packings. Applied and Computational Harmonic Analysis , 26(1):64–76, 2009. ISSN 1063- 5203. doi: https://doi.org/10.1016/j.acha.2008.03. 001. URL https://www.sciencedirect.com/ science/article/pii/S1063520308000249. Le, Q., Sarl´os, T., Smola, A., et al. Fastfood-approximating kernel expansions in loglinear time. In Proceedings of the international conference on machine learning, 2013. Lecun, Y ., Denker, J., and Solla, S. Optimal brain damage. In Advances in Neural Information Processing Systems, volume 2, 01 1989. Li, Z., Xiao, J., Yang, L., and Gu, Q. Repq-vit: Scale repa- rameterization for post-training quantization of vision transformers. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, 2023. Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 2021a. Liu, Z., Wang, Y ., Han, K., Zhang, W., Ma, S., and Gao, W. Post-training quantization for vision transformer. Ad- vances in Neural Information Processing Systems , 34, 2021b. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Nagel, M., Baalen, M. v., Blankevoort, T., and Welling, M. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding for post-training quantization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Con- ference on Machine Learning, volume 119 of Proceed- ings of Machine Learning Research. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/nagel20a.html. 11Flexible Low-Bit Quantization for Transformers Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y ., van Baalen, M., and Blankevoort, T. A white paper on neural network quantization. ArXiv, abs/2106.08295, 2021. URL https://api.semanticscholar. org/CorpusID:235435934. Namburi, S. S. S., Sreedhar, M., Srinivasan, S., et al. The cost of compression: Investigating the impact of com- pression on parametric knowledge in language models. In Findings of the Association for Computational Lin- guistics: EMNLP 2023. Association for Computational Linguistics, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ranftl, R., Bochkovskiy, A., and Koltun, V . Vision trans- formers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. Rokh, B., Azarpeyvand, A., and Khanteymoori, A. A com- prehensive survey on model quantization for deep neural networks in image classification. ACM Transactions on Intelligent Systems and Technology, 14(6):1–50, Novem- ber 2023. ISSN 2157-6912. doi: 10.1145/3623402. URL http://dx.doi.org/10.1145/3623402. Rozell, C. and Johnson, D. Analysis of noise reduction in re- dundant expansions under distributed processing require- ments. In Proceedings. (ICASSP ’05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005., volume 4, 04 2005. ISBN 0-7803-8874-7. doi: 10.1109/ICASSP.2005.1415976. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021. Scao, T. L., Fan, A., et al. Bloom: A 176b-parameter open- access multilingual language model, 2023. Strohmer, T. and Heath Jr, R. W. Grassmannian frames with applications to coding and communication. Applied and computational harmonic analysis, 14(3), 2003. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transform- ers; distillation through attention. In International Con- ference on Machine Learning, volume 139, July 2021. Touvron, H., Cord, M., and J ´egou, H. Deit iii: Revenge of the vit. In European Conference on Computer Vision. Springer, 2022. Touvron, H., Martin, L., Stone, K., et al. Llama 2: Open foundation and fine-tuned chat models, 2023. Waldron, S. F. D. An introduction to finite tight frames, 2019. URL https://link.springer. com/book/10.1007/978-0-8176-4815-2 . Wightman, R. Pytorch image models. https://github. com/rwightman/pytorch-image-models, 2019. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and efficient post-training quantization for large language models. InProceedings of the 40th International Conference on Machine Learning, 2023. Yao, Z., Yazdani Aminabadi, R., Zhang, M., et al. Zero- quant: Efficient and affordable post-training quantization for large-scale transformers. In Advances in Neural Infor- mation Processing Systems, 2022. Yu, D., Seide, F., Li, G., and Deng, L. Exploiting sparse- ness in deep neural networks for large vocabulary speech recognition. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4409–4412, 2012. doi: 10.1109/ICASSP.2012.6288897. Yuan, Z., Xue, C., Chen, Y ., Wu, Q., and Sun, G. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In European Conference on Computer Vision, 2022. Yun, C., Chang, Y .-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. O (n) connections are expressive enough: Universal approximability of sparse transform- ers. Advances in Neural Information Processing Systems, 33:13783–13794, 2020. Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi, Y . Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zeng, Z., Davies, M., Pulijala, P., et al. Lookupffn: making transformers compute-lite for cpu inference. In Proceed- ings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023. Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Zhang, B., Haddow, B., and Birch, A. Prompting large language model for machine translation: A case study. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. 12Flexible Low-Bit Quantization for Transformers Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L. M., and Shum, H.-Y . Dino: Detr with improved denois- ing anchor boxes for end-to-end object detection, 2022a. Zhang, S., Roller, S., Goyal, N., et al. Opt: Open pre-trained transformer language models, 2022b. Zhu, Z., Hong, J., and Zhou, J. Data-free knowledge distil- lation for heterogeneous federated learning. In Interna- tional conference on machine learning, pp. 12878–12889. PMLR, 2021. 13Flexible Low-Bit Quantization for Transformers Appendix In this Appendix, we provide additional details related to the experiments reported in the main paper. This Appendix is organized as follows. In Section A we analyze the impact of redundancy on the performance of the model in terms of classification accuracy on the ImageNet-1K dataset. In Section B, we study this effect on the performance of Vision Transformer models, evaluated using activation maps. Next, in Section C, we study the effect of the size of the calibration data used for quantizing various Vision Models. In Section D, we analyze the choice of the 2σ threshold for clipping the weights during quantization. We provide empirical evidence for different classes of Vision models. We also show that 2σ clipping alone cannot improve quantization performance. On the contrary, it can degrade the performance for weight configurations that are poorly distributed. Section E shows the distribution of weights in the DeiT and Swin Transformer models. In Section F, we present a framework for quantizing activations and show how the FF representation of activations inherently addresses the key pain points described in previous works. We follow this with a simple experiment with activation quantization enabled. In Section G, we provide experiments on Segmentation and Object detection tasks. In Section H, we present more experiments on Language models on different datasets and downstream tasks as mentioned in the main paper. Then, in Section I, we provide an expanded synopsis of the theoretical results that apply to our setting, as briefly described in the main paper. In Section J we provide a brief synopsis of the algorithm used to generate a TFF for the curious reader. Finally in Section K we give a detailed analysis of the storage benefits of FrameQuant and the computational complexity during inference. A. Impact of redundancy in representations We consider the impact of redundancy in our Frame representation moving forward from 2 bits, incrementally increasing redundancy. Table 8 shows the performance of different models at different levels of redundancy. We observe that for large models, the original performance without any redundancy was already high, and adding redundancy did not impact their performance significantly. However, this is not the case for smaller models. Here, we see significant performance improvements (around +21% for the ViT-S model). Redundancy bits ViT DeiT III Swin S B H S B H S B L r = 1.00 (2 .0 bits) 48 .10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 r = 1.05 (2 .1 bits) 56 .19 79 .97 85 .67 58 .74 79 .59 86 .58 78 .47 80 .41 84 .26 r = 1.10 (2 .2 bits) 61 .51 80 .93 85 .99 65 .33 80 .91 86 .68 78 .77 81 .33 84 .42 r = 1.15 (2 .3 bits) 65 .17 81 .27 86 .04 69 .54 81 .69 86 .67 78 .87 81 .88 84 .51 r = 1.20 (2 .4 bits) 66 .53 81 .59 86 .11 71 .07 81 .98 86 .61 79 .56 82 .02 84 .56 r = 1.25 (2 .5 bits) 68 .57 81 .74 86 .06 73 .48 82 .51 86 .55 79 .99 82 .26 84 .51 r = 1.30 (2 .6 bits) 69 .02 81 .77 85 .99 74 .40 82 .54 86 .38 79 .92 82 .39 84 .65 Full Precision - 81.39 85 .1 87 .59 83 .06 85 .7 87 .19 82 .79 84 .7 85 .95 Table 8.Performance of various quantized models on ImageNet-1K classification task as the redundancy in FrameQuant is increased. We see that increasing the redundancy closes the gap between the performance of the quantized model and the Full precision model B. Does redundancy impact attention maps? In the main paper, we discussed how the performance of the models improves as we increase the redundancy in the Fusion Frames during quantization. In this section, we provide additional details on how redundancy affects the attention maps of Vision Transformers from different classes. We will focus mainly on the small and base models where we see significant improvement in the validation accuracy on ImageNet, as we increase the redundancy. Figures 6, 7 and 8 show the attention maps of Vit-S, DeiT III -S, and Deit III - B models respectively. These models show an improvement in the range of 4.55% to 23.27% as we increase the redundancy from r = 1 to r = 1.3. This is reflected in the attention maps as well. We see that as the redundancy is increased, the attention regions concentrate around the objects of interest systematically. This is consistent with the improvement in accuracy and can also be seen in Figure 9. 14Flexible Low-Bit Quantization for Transformers Figure 6.Effect of flexibility/redundancy on activation maps for ViT-S.Figure showing attention maps of ViT-S as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. The first column shows the image and the ground truth label, and the rest of the columns show the regions that the model is attending to in the final transformer block. We see that as the redundancy is increased, the model gets more focused, with the attention regions concentrating on the objects of interest. #images ViT DeiT III Swin S B H S B H S B L 128 48 .10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 200 51 .48 79 .84 85 .62 53 .74 78 .38 86 .61 77 .66 80 .19 84 .09 256 51 .69 79 .84 85 .74 54 .73 79 .06 86 .47 77 .96 80 .68 84 .31 Table 9.ImageNet-1K Top-1 validation accuracies of models from different classes as the number of calibration images is increased. C. Does the calibration set size matter? In the main paper, we noted that a small calibration set size was sufficient. In this section, we report on experiments varying the number of calibration images and observe the performance of different classes of models on ImageNet-1K. We use a redundancy factor of r = 1 in this experiment. Table 9 shows the validation accuracies for different classes of models as the number of calibration images is increased from 128 to 256. We can see that the performance improvement is seen only in the small-sized models from the ViT and DeiT III classes. So, we will focus on reporting results for these models. Figure 10 shows the accuracies of ViT-S and DeiT III-S models as the number of calibration images is increased from 128 to 512. We 15Flexible Low-Bit Quantization for Transformers Figure 7.Effect of flexibility/redundancy on activation maps for DeiT III-S. Figure showing attention maps of DeiT III -S as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. can see that there is a small improvement as the number of images is increased from 128 to 200, but the benefits taper off quickly as we increase it further. This shows that if access to the calibration is not limited, a small increase in the number of images used for quantization can benefit the final accuracies of the models, especially for smaller models. D. How does 2σ clipping affect performance? In the main paper, we discussed a simple clipping threshold at the 2σ level. In this section, we analyze the benefits of this choice and its effect on the performance of different classes of models on ImageNet-1K. As in the previous section, we use a redundancy factor of r = 1 for these experiments and focus on the impact of clipping the weights at different levels based on their distribution. Figure 11 shows the accuracies of different classes of models as the threshold for the weights is varied from ±σ to ±3σ. We can see that the performance of all the models peaks in the neighborhood of ±2σ. Clipping at ±σ restricts the range of the weights too aggressively, incurring errors. At ±3σ level, which is close to allowing the entire range, we are stretching the effective scale of the weights to allow all the extreme entries to be represented within the range. This, in turn, increases the width of the quantization levels, which affects the majority of the weights impacting performance. ±2σ seems to be the sweet spot. 16Flexible Low-Bit Quantization for Transformers Figure 8.Effect of flexibility/redundancy on activation maps for DeiT III-B. Figure showing attention maps of DeiT III -B as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. 1.00 1.05 1.10 1.15 1.20 1.25 1.30 Redundancy (r) 0 20 40 60 80Imagenet 1k Accuracy ViT-S DeiT-S DeiT-B Figure 9.Trend of accuracies in small size models as we increase the redundancy 17Flexible Low-Bit Quantization for Transformers 150 200 250 300 350 400 450 500 # images 0 20 40 60 80ImageNet-1K Accuracy ViT-S DeiT-S Figure 10.Trend of accuracies in small size models as we increase the number of calibration images Model Quantization method WikiText2 C4 Llama2 7B GPTQ without clipping 6.40e3 2.27e3 Llama2 7B GPTQ with clipping 9.45e3 7.40e3 Llama2 7B FrameQuant with clipping 14.85 19.62 Llama2 70B GPTQ without clipping 140.5 68.83 Llama2 70B GPTQ with clipping 2.08e3 1.12e3 Llama2 70B FrameQuant with clipping 5.5 7.85 Table 10.Table showing the impact of clipping on GPTQ. FrameQuant computes the FF representations of the weights that are nicely distributed and can take advantage of clipping to remove outliers. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-S DeiT-S Swin-S (a) Accuracies of Small models 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-B DeiT-B Swin-B (b) Accuracies of Base models 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-H DeiT-H Swin-L (c) Accuracies of Large models Figure 11.Figure showing the impact of clipping at different thresholds based on σ D.1. Does 2σ clipping alone improve performance? From our alation study 4.3, it might seem that 2σ clipping is doing the heavy lift in improving the performance. However, clipping is most effective once the weights are nicely distributed. A direct application of clipping on the weights has limited efficacy and incurs errors for weight configurations that are degenerate/poorly distributed. Projecting onto a space-filling basis makes clipping effective. To demonstrate this point quantitatively, we run GPTQ on Llama2 models with the 2σ clipping applied directly to the weights. Table 10 shows that the performance degrades when the weights are clipped instead of their Fusion Frame representations as in FrameQuant. E. Distribution of weights in the DeiT and Swin Transformer models This section presents the distribution of the weights in the DeiT and Swin Transformer models. Figure 13 shows the distribution of weights in a linear layer from the DeiT and Swin Transformer families. We can see that the distribution is well behaved and the 2σ threshold captures most of the mass well. 18Flexible Low-Bit Quantization for Transformers (a) Activations of the first block in ViT-M  (b) Activation FF representations of the first block in ViT-M Figure 12.Activations of the first Transformer Block of Vit-M model and their FF representations. We can see the outliers in the activations (shown in red) on the left, while the FF representations are well-behaved. (a) Weight distribution in DeiT  (b) Weight distribution in Swin Transformer Figure 13.Weights distribution in DeiT and Swin Transformer models. F. FrameQuant for Activation Quantization? In the main paper, we restricted the experimental setup of Vision Transformer models to weight quantization for meaningful comparisons to recent PTQ papers. This is because activation quantization in this low-bit regime has not been reported and each baseline will need modifications to report the best possible results. In this section, we provide some details regarding applying FrameQuant for activation Quantization with the caveat that a comprehensive head-to-head comparison to all reported baselines is difficult for the reasons above. Rounding activations to the nearest. For smaller Transformer models, the inference efficiency bottleneck also largely lies in activations. So, we focus on these models to consider activation quantization. We performed activation quantization on ViT-S/B models with a simple rounding to the nearest, and we found that even when the weights are quantized to 2 (or 2.2) bits using FrameQuant, the performance drops are not large. This is promising and shows that FrameQuant is robust in preserving activations even at a 2.x bit level for weights. Table 11 shows the ImageNet-1K accuracy at different bit-widths for weights and activations. Benefits of well-behaved FF representations. Since we operate in the FF representation space, we can first compute the FF representations of the previous layer activations, Cprev = PT prevAprev (10) 19Flexible Low-Bit Quantization for Transformers Method bits ViT-S ViT-B Full Precision W32/A32 81.39 85 .10 FrameQuant W2/A32 48.17 79 .53 FrameQuant W2.2/A32 61.51 80 .93 FrameQuant W2/A8 48.02 79 .51 FrameQuant W2.2/A8 60.96 80 .64 FrameQuant W2/A6 47.41 78 .59 FrameQuant W2.2/A6 58.35 80 .14 Table 11.Performance of quantized ViT-S and ViT-B models on ImageNet-1K validation set. We used FrameQuant to quantize the weights while the activations are rounded to the nearest. and quantize these directly. Also, since activation quantization happens dynamically, during inference time, we keep the activation quantization procedure simple and just use the nearest rounding method. This can be written as: ¯Cprev = ⌊Cprev ∆C ⌉, ∆C = max |Cprev| 2N−1 − 1 (11) where ¯Cprev is in INT8 form and is the quantized version of the FF representations of the activations (Cprev). ⌊·⌉ represents nearest rounding. We can substitute with ⌊·⌋ or ⌈·⌉ to get the floor or the ceil operation. As noted by (Xiao et al., 2023), we also observe that the activations have large outliers in some of the channels whose values are more than 100× larger than the activations of other channels on average and this behavior is consistent across the tokens. This is shown in Figure 12a. So, to quantize the outliers, we need a large-scale factor ∆C, which will quantize all small values to zero. The other option is to use per-channel quantization – where we have different scale factors for different channels. This would solve the outlier problem, but it is not ideal because we cannot use integer kernels for matrix multiplications in the Linear Layers. To use integer arithmetic for the matrix multiplications in the Linear layers, we can only perform per-token quantization for the activations and per-channel quantization for the weights. To solve this problem, (Xiao et al., 2023) shifts the scale from activations to weights that are well-behaved. They dynamically search for different amounts of shifts between the weights and activations using a calibration set and use that during inference. Since we operate in the FF representation space, we observe that after we compute the FF representations of the activations, they are well-behaved. Figure 12b shows the FF representation of activation of the first Transformer block in the ViT-M model. So, we do not need to perform further scaling to reduce the range. This makes FrameQuant to be amenable to activation quantization if necessary in practice. G. Quantizing Segmentation and Object Detection models We used FrameQuant to quantize the Swin backbone for Object Detection and Segmentation Models. We compare our results with RepQ-ViT (Li et al., 2023), one of the state-of-the-art publicly available quantization methods in this regime. Since our primary focus is quantizing the weights of the Transformer, for a fair comparison, we use RepQ-ViT to quantize the rest of the parameters, such as activations and norm layers. From Table 12, we can see that FrameQuant performs similarly to RepQ-ViT, and the main benefits of frameQuant kick in at very low bit widths. H. Additional Experiments on Language models H.1. Evaluation on the C4 dataset This section is a continuation of section 4.4. Here, we present the perplexity of different models from OPT and Llama2 classes on the C4 (Raffel et al., 2020) dataset. Consistent with our previous experiments, we see that FrameQuant with 1× the redundancy performs better than all the methods under consideration. With an additional redundancy of r = 1.1×, FrameQuant closes the gap between the full precision model across all the sizes from different families of Large Language Models. The results are shown in table 13. 20Flexible Low-Bit Quantization for Transformers Method Precision of Swin Backbone Precision of rest of the network MoBY Mask RCNN w. Swin-T (APbox / APmask) MoBY Cascade Mask RCNN with Swin-T (APbox / APmask) Full Precision W32/A32 W32/A32 43.6/39.6 48 .1/41.5 RepQ-ViT W6/A6 W6/A6 42.6/39.0 47 .7/41.3 FrameQuant W6/A6 W6/A6 42.7/39.0 47 .8/41.3 RepQ-ViT W4/A4 W4/A4 34.2/32.3 43 .8/38.6 FrameQuant W4/A4 W4/A4 34.5/32.5 44 .3/39.1 RepQ-ViT W3/A4 W4/A4 27.5/26.4 38 .9/34.8 FrameQuant W3/A4 W4/A4 29.3/27.9 41 .2/36.7 RepQ-ViT W3/A4 W3/A4 16.9/16.9 32 .4/29.2 FrameQuant W3/A4 W3/A4 21.7/21.5 35 .2/31.4 Table 12.Performance of quantized models with Swin-T backbone on the Object Detection and Segmentation tasks. We can see that FrameQuant performs similarly to RepQ-Vit at higher bit widths. The main benefits of Frame representations kick in at very low bit-widths. Method #bits OPT Llama2 125M 350M 1.3B 2.7B 6.7B 7B 70B Full-Precision 16 26.56 22 .58 16 .07 14 .34 12 .71 7.26 5 .71 GPTQ 2 2203.89 5325 .65 4139 .91 4058 .41 528 .41 2265.09 68 .83 QuIP 2 543.63 432 .56 28 .91 21 .49 16 .92 26.61 8 .65 FrameQuant (r = 1.0) 2 226.15 95.38 27.90 20.74 17.28 19.62 7.85 FrameQuant (r = 1.1) 2.2 91.29 47.62 22.39 17.75 15.33 11.23 6.86 Table 13.Perplexity (smaller the better) of Llama2 and OPT models on C4 dataset when quantized to 2 (or 2.2) bits by different methods. Method #bits ARC (challenge) ARC (easy) BoolQ HellaSwag PIQA WinoGrande Full-Precision 16 43.43 76 .35 77 .71 57 .16 78 .07 69 .06 GPTQ 2 22.44 24 .58 41 .19 25 .93 51 .85 50 .43 QuIP 2 22.27 42 .76 50 .31 34 .04 61 .75 52 .64 FrameQuant (r = 1.0) 2 23.98 55.39 63.52 36.76 66.65 55.80 FrameQuant (r = 1.1) 2.2 31.91 65.53 67.95 46.46 73.07 63.61 Table 14.Evaluating Llama2-7B model quantized with different methods on a range of downstream tasks. H.2. Perplexity of Quantized Llama2 7B Figure 14 shows the perplexity of Llama2-7B model quantized by different quantization schemes. We see that FrameQuant with a redundancy of 1x already performs better than all other methods. With increasing redundancy, the performance becomes closer to the Full precision model. H.3. Performance on Downstream tasks In this experiment, we finetune the Llama2-7B model on downstream tasks. We ran experiments on ARC challenge, ARC easy (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2021). We used LM-evaluation harness (Gao et al., 2023) for running our experiments on these diverse tasks. The results are presented in table 14. We can see that again in line with our previous experiments, the LLM quantized with FrameQuant with no redundancy already performs better than all the other methods on the downstream tasks. With added redundancy, this performance goes up across all the tasks under consideration. Based on our previous experiments and as observed in (Chee et al., 2023), we expect the performance gap between the full precision model and the quantized model to go down as the size of the models increases. 21Flexible Low-Bit Quantization for Transformers (a) Perplexity of Llama2-7B model on WikiText2 dataset  (b) Perplexity of Llama2-7B model on C4 dataset Figure 14.Perplexity of Llama2-7B model on WikiText2 and C4 datasets. FrameQuant performs better than all quantization methods tested. With increasing redundancy, we see that the performance of the model also improves as indicated by the theory. I. Robustness guarantees We provide additional details on two specific results (mentioned in the main paper) that apply to our construction. We encourage the interested reader to refer to (Christensen, 2018; Casazza & Kutyniok, 2012) for a more comprehensive treatment of the topic. LMMSE estimation from fusion frame measurements. For a given layer l FrameQuant quantizes the transformed weights matrix Dl which is given by Dl = PT l (ΘlPprev). We can treat ˆDl as a projection of ΘlPprev which is corrupted by noise. During inference, the activations of this layer are given by Zl = Pl ˆDlCprev. But, can we do better? Instead of directly applying the synthesis operator Pl to compute Zl from its FF representations ˆDlCprev, we can design a simple linear filter F that minimizes the MSE in Zl because we are using a quantized ˆDl. The final expression for the computation of the output of the layer will be Zl = F ˆDlCprev. This linear MSE minimizer F is known to be the Wiener Filter and has a closed-form expression with various levels of approximation. The following theorem states that the Wiener filter minimizes MSE when the Fusion Frame is tight. Theorem I.1. (Kutyniok et al., 2009) For the model described above, the MSE in linearly estimating the signal from its noisy projections is minimized when the Fusion Frame is tight Consistent Reconstruction. Assuming the same mode of representing the modified weights Dl as above, during inference, we can get a consistent estimate of the weights ( ˆΘl) from ˆDl if one were to solve a linear program for ˆX \u0014 Pl −Pl \u0015 ˆXl ≤ \u0014∆ 2 + ˆDl ∆ 2 − ˆDl \u0015 , where ∆ is the quantization level. Here, the constraints in the Linear Program make sure that ˆX belongs to the regions where valid unquantized values must lie, thereby removing the out-of-sub-space error (Goyal et al., 1998). We can get the estimated weights from ˆXl as ˆΘl = ˆXlPT prev. Using this consistent reconstruction yields estimates with an MSE which is upper bounded by O(1/r2) (Goyal et al., 1998) J. Synopsis of Construction of Tight Fusion Frames Here, we give a brief synopsis of an algorithm for generating Tight Fusion Frames for the curious reader. (Casazza et al., 2011) was the first to introduce a systematic method for constructing UNTFs (Unit Norm Tight Frames) that play a key role in constructing Tight Fusion Frames. They also characterize the (k, ρ, d) values for which a Tight Fusion Frame exists. 22Flexible Low-Bit Quantization for Transformers Whenever such a TFF exists, we can construct Tight Fusion Frames by using their algorithm. There are two main parts to the algorithm. 1. Play Spectral Tetris to generate a UNTF of d elements in Cρ 2. Modulate this UNTF with complex roots of unity to generate a (k, ρ, d) TFF for Cd So, the first step is to generate a “smaller” frame and in the next step, we modulate the smaller frame to generate a “larger” Tight Fusion Frame. After generating a TFF for Cd we can easily extend it to the Real Field by applying the entrywise map x + iy 7→ \u0014x −y y x \u0015 . We describe the algorithm with the help of an example for the simplicity of explanation. We aim to construct a (5,4,11) TFF. So, k = 5, ρ= 3, d= 11. J.1. Spectral Tetris As the name suggests UNTFs are Tight frames where each frame vector has a unit norm. We construct a 4 × 11 matrix F whose columns are the frame vectors for C4 which satisfies • Columns of unit norm • Orthogonal rows, meaning FF ∗ is diagonal • Rows of constant norm, meaning FF ∗ is a constant multiple of identity matrix with the constant being 11 4 We start with a matrix F =   1 1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?   This leaves a norm of 11 4 − 2 = 3 4 to be filled in the first row. This can easily be added using a 2 × 2 matrix T(x) where x = 3 4 . T(x) is defined as: T(x) := 1√ 2 \u0014 √x √x√2 − x −√2 − x \u0015 , T (x)T∗(x) = \u0014x 0 0 2 − x \u0015 After inserting T(x), F is now F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 ? ? ? ? ? ? ? 0 0 0 0 ? ? ? ? ? ? ? 0 0 0 0 ? ? ? ? ? ? ?   Then we continue adding ones in row two until the norm becomes less than 11 4 . F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 1 ? ? ? ? ? ? 0 0 0 0 0 ? ? ? ? ? ? 0 0 0 0 0 ? ? ? ? ? ?   Now we insert T(x) with the remaining norm. We repeat this process until all the rows are filled. The Final F is given by F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 1 √ 2√ 8 √ 2√ 8 0 0 0 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 1 √ 7√ 8 √ 7√ 8 0 0 0 0 0 0 0 0 0 √ 7√ 8 − √ 7√ 8 1   23Flexible Low-Bit Quantization for Transformers   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 1 ω √ 3√ 8 ω2 √ 3√ 8 ω3 0 0 0 0 0 0 0 1 ω2 √ 3√ 8 ω4 √ 3√ 8 ω 0 0 0 0 0 0 0 1 ω3 √ 3√ 8 ω √ 3√ 8 ω4 0 0 0 0 0 0 0 1 ω4 √ 3√ 8 ω3 √ 3√ 8 ω2 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 3√ 8 1 √ 2√ 8 √ 2√ 8 0 0 0 0 0 0 √ 5√ 8 ω2 − √ 3√ 8 ω3 ω4 √ 2√ 8 √ 2√ 8 ω 0 0 0 0 0 0 √ 5√ 8 ω4 − √ 3√ 8 ω ω 3 √ 2√ 8 √ 2√ 8 ω2 0 0 0 0 0 0 √ 5√ 8 ω − √ 3√ 8 ω4 ω2 √ 2√ 8 √ 2√ 8 ω3 0 0 0 0 0 0 √ 5√ 8 ω3 − √ 3√ 8 ω2 ω √ 2√ 8 √ 2√ 8 ω4 0 0 0 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 1 √ 7√ 8 √ 7√ 8 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω ω 2 √ 7√ 8 ω3 √ 7√ 8 ω4 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω2 ω4 √ 7√ 8 ω √ 7√ 8 ω3 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω3 ω √ 7√ 8 ω4 √ 7√ 8 ω2 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω4 ω3 √ 7√ 8 ω2 √ 7√ 8 ω1 0 0 0 0 0 0 0 0 0 √ 7√ 8 − √ 7√ 8 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω3 − √ 7√ 8 ω4 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω − √ 7√ 8 ω3 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω4 − √ 7√ 8 ω2 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω2 − √ 7√ 8 ω 1   Table 15.(5, 4, 11)-TFF for C11. Here, ω = ei2π/5. Each pair of rows belongs to the same subspace if their indices differ by a multiple of 5 J.2. Modulation In the second step, we modulate the F matrix with complex roots of unity, one subspace at a time. So, for each ki = 0, 1, 2, . . . k− 1, we construct a row vector wki = \u0014\u0010 e i2πki k \u00110 \u0010 e i2πki k \u00111 \u0010 e i2πki k \u00112 . . . \u0010 e i2πki k \u0011d−1\u0015 We multiply each row of F with wki to generate the orthogonal basis for different subspaces indexed by ki. Theorem 14 by Casazza et al. (2011) proves that the Fusion Frames generated by this algorithm are Tight. The Final Fusion Frame vectors are shown in Table 15. K. Storage benefits and Computational complexity during inference K.1. Storage benefits Consider an example where we are quantizing a weight matrix Θl of dimension 1024 × 1024 using FrameQuant with a redundancy factor of r = 1.1×. The size of the original matrix using FP32 is 4MB. After transforming the weights to map within the FF representation space, the transformed weights Dl have dimensions 1126 × 1126, which are quantized and represented using 2 bits. This quantized weight ˆDl has a size of 0.3MB. Along with the quantized weights, we need to store the bias and scale values for each row leading to an additional storage of 1024 FP32 values, which will incur an additional cost of 0.007MB. All this sums up to a storage of 0.307MB from an initial 4MB giving a savings of 13x in the storage requirements. Since we can generate the Fusion Frames on the fly, we just need to store the (k, ρ, d) values, and a seed to 24Flexible Low-Bit Quantization for Transformers generate the random rotation matrix which incurs negligible storage costs. Table 6 shows the sizes of Llama2 models when compressed with FrameQuant. K.2. Computational Complexity during Inference Consider a linear layer in a transformer model with weights Θl of dimensions d × d. Using FrameQuant these weights are transformed to Dl and the quantized weights ˆDl are stored. Let the parameters of the TFF used for quantization be (k, ρ, d). As a recap, k is the number of subspaces, ρ is the dimension of each subspace and d is the dimension of the Hilbert space we are operating in. So, the redundancy in Frame representations is r = kρ d . Let, Tl, Tprev ∈ Rd×kρ be the vectorized Orthonormal basis for the current layer, and the previous layer respectively. During inference, the quantized weights ˆDl are transformed to the weight space as ˆΘl = Pl ˆDlPT prev. Here, Pl = Rl(Tl), Pprev = Rprev(Tprev), where Rl, Rprev ∈ Rd×d denote the rotation matrices for the current and the previous layers respectively. So, the overall operation is ˆΘl = RlTl ˆDlTprevT RT prev. Let us first look at the ˆDlTprevT operation. TprevT is a block diagonal matrix constructed as defined in section 2.2. It has ρ blocks along the diagonal, each with k rows and at most ⌈d ρ ⌉ + 2 columns. The order of the computations required to generate this matrix is O(dk). The computation complexity of ˆDlTprevT is O(d ρ kρdr) = O(d2kr). So, the overall computational complexity for the computation of TprevT and multiplication with ˆDl is O(d2kr). Now, consider the left multiplication with Tl. Tl is again a block diagonal matrix similar to TprevT . But it is multiplying a quantity with dimensions kρ × d. Hence this multiplication has a computational complexity of O(d2k). The worst-case computational complexity of multiplication with the TFF orthonormal basis of current and previous layers is O(d2kr). The final Rl, RT prev are orthogonal rotation matrices which can be efficiently computed in O(d2 log d) time using random projections such as (Le et al., 2013) or any other efficient implementation. Combining all these calculations, the overall computational complexity of transforming the weights during inference is O(d2(kr + logd)). Note that since all of these are matrix operations, they run on GPU in a vectorized manner. Table 7 shows the inference speeds of the quantized models on a Nvidia A100 GPU. 25",
      "meta_data": {
        "arxiv_id": "2403.06082v2",
        "authors": [
          "Harshavardhan Adepu",
          "Zhanpeng Zeng",
          "Li Zhang",
          "Vikas Singh"
        ],
        "published_date": "2024-03-10T04:01:49Z",
        "pdf_url": "https://arxiv.org/pdf/2403.06082v2.pdf"
      }
    },
    {
      "title": "BiT: Robustly Binarized Multi-distilled Transformer",
      "abstract": "Modern pre-trained transformers have rapidly advanced the state-of-the-art in\nmachine learning, but have also grown in parameters and computational\ncomplexity, making them increasingly difficult to deploy in\nresource-constrained environments. Binarization of the weights and activations\nof the network can significantly alleviate these issues, however, is\ntechnically challenging from an optimization perspective. In this work, we\nidentify a series of improvements that enables binary transformers at a much\nhigher accuracy than what was possible previously. These include a two-set\nbinarization scheme, a novel elastic binary activation function with learned\nparameters, and a method to quantize a network to its limit by successively\ndistilling higher precision models into lower precision students. These\napproaches allow for the first time, fully binarized transformer models that\nare at a practical level of accuracy, approaching a full-precision BERT\nbaseline on the GLUE language understanding benchmark within as little as 5.9%.\nCode and models are available at: https://github.com/facebookresearch/bit.",
      "full_text": "BiT: Robustly Binarized Multi-distilled Transformer Zechun Liu∗ Reality Labs, Meta Inc. zechunliu@fb.com Barlas O˘guz∗ Meta AI barlaso@fb.com Aasish Pappu Meta AI aasish@fb.com Lin Xiao Meta AI Scott Yih Meta AI Meng Li Peking University Raghuraman Krishnamoorthi Reality Labs, Meta Inc. Yashar Mehdad Meta AI Abstract Modern pre-trained transformers have rapidly advanced the state-of-the-art in machine learning, but have also grown in parameters and computational complexity, making them increasingly difﬁcult to deploy in resource-constrained environments. Binarization of the weights and activations of the network can signiﬁcantly alleviate these issues, however, is technically challenging from an optimization perspective. In this work, we identify a series of improvements that enables binary transformers at a much higher accuracy than what was possible previously. These include a two-set binarization scheme, a novel elastic binary activation function with learned parameters, and a method to quantize a network to its limit by successively distilling higher precision models into lower precision students. These approaches allow for the ﬁrst time, fully binarized transformer models that are at a practical level of accuracy, approaching a full-precision BERT baseline on the GLUE language understanding benchmark within as little as 5.9%. Code and models are available at: https://github.com/facebookresearch/bit. 1 Introduction The past few years have witnessed tremendous advances in almost all applied ﬁelds of AI. It would hardly be a simpliﬁcation to say that the bulk of these advances was achieved by scaling the transformer architecture (Vaswani et al., 2017) to ever larger sizes with increasing computation budget (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2018, 2019; Raffel et al., 2020; Brown et al., 2020). On the other hand, mobile devices and wearables have proliferated and shrunk in size, with stringent requirements for storage, computation and energy consumption. Consumers demand more portability, while having access to all that current AI technology has to offer. As a result, the gap between what is possible in AI, and what is deployable has never been wider. While there is a variety of methods to increase inference efﬁciency in neural networks (e.g. knowledge distillation, pruning), quantization has some attractive properties and has been widely successful in practice (Gholami et al., 2021). For one, storage and latency gains from quantization are deter- ministically deﬁned for a given quantization level. For instance, reducing the precision of model parameters by a given factor immediately translates to an identical reduction in storage cost. Similarly, reducing the precision of arithmetic operations results in a corresponding reduction in computational cost. Uniform quantization is hardware friendly, making it relatively simple to realize theoretical improvements in practice. ∗ Equal contribution Preprint. Under review. arXiv:2205.13016v2  [cs.LG]  2 Oct 2022Binarization represents the extreme limit of quantization, promising a 32×reduction in storage over full-precision (32-bit) models. Moreover, binary arithmetic completely eliminates multiplications in favor of bit-wise XNOR operations (Courbariaux et al., 2016; Rastegari et al., 2016), enabling even further improvements when using special purpose hardware. Energy efﬁciency improvements between 100-1000x have been claimed to be possible with binary neural networks (BNNs), over their full-precision counterparts (Nurvitadhi et al., 2016). The obvious challenge with binarization is the difﬁculty of optimization. While all quantization is discontinuous, higher precisions allow approximating the full-precision network to a better extent, where with BNNs, this becomes much harder. Surprisingly, researchers in computer vision have been able to demonstrate BNNs with remarkable accuracy (Liu et al., 2018; Qin et al., 2020; Martinez et al., 2020). Unfortunately, while these works have mostly been developed on convolutional architectures for image tasks, they have not generalized well to transformer models. For instance, recent work (Qin et al., 2021) has shown that a BERT (Devlin et al., 2019) model with binary weights and activations lags its full-precision counterpart by as much as 20 points on average on GLUE dataset. Even for weights-only binarization, the loss landscape was shown to be too irregular, and recent work resorted to complex and specialized methods such as weight-splitting from half-width models to achieve a reasonable accuracy (Bai et al., 2021). With this background, we tackle the problem of fully binarizing transformer models to a high level of accuracy. With the expansion trend of transformers towards becoming the standard architecture choice for all ﬁelds of AI, we believe a solution to this problem could be highly impactful. Our approach follows the same paradigm as previous work, based on knowledge distillation (Hinton et al., 2015) from higher precision models using the straight-through estimator (STE) of Bengio et al. (2013). In view of the optimization difﬁculties, we take the following steps to ensure that the student and teacher models are well-matched: • In Section 3 we describe a robust binarization framework, which allows the binary student network to better match the output distribution of the teacher. This allows us to achieve SoTA results for extreme activation quantization with BERT, producing models with little loss in accuracy down to a quantization level of binary weights and 2-bit activations and improves over previous setups by large margins in the fully binary (1-bit) setting. It also leads to competitive results for weight binarization with 4-bit activations using a single knowledge distillation step. • To further improve binary models, we propose a multi-distillation approach, described in Section 4. Instead of distilling directly into a fully binary model, we ﬁrst distill an intermediate model of medium precision and acceptable accuracy. This model then becomes the teacher in the next round of distillation into increasingly quantized models. Such a method ensures that the student model doesn’t drift too far from the teacher, while also ensuring as good an initialization as possible. We call the resulting model BiT 2. In the vanilla setting without data augmentation, our approach reduces the accuracy gap to a full- precision BERT-base model by half on the GLUE (Wang et al., 2019) benchmark compared to the previous SoTA. When using data augmentation, we are able to reduce the absolute accuracy gap to only 5.9 points (from over 15 points previously). In addition to the fully binary setting, we also report SoTA results with binary weights and 2-bit activations, where our models trail the full-precision baseline by only 3.5 points. 2 Background 2.1 Transformer architecture The transformer model of Vaswani et al. (2017) is composed of an embedding layer, followed by N transformer blocks and a linear output layer. Each transformer block consists of a multi-head attention layer followed by a feed-forward network, as shown in Figure 1. The multi-head attention layer is a concatenation of Kscaled dot-product attention heads, deﬁned by: Attention(Q,K,V ) =softmax (QKT √dk ) V 2Short for Binarized Transformer. 2where dk is the dimension of each key, Q,K,V are weight matrices for the query, key and value respectively. As such, the computation in a transformer model is limited to linear matrix mul- tiplications and additions, pointwise non-linearities (most commonly Sigmoid (Han & Moraga, 1995), GeLU (Hendrycks & Gimpel, 2016) or ReLU (Nair & Hinton, 2010) ) and the Softmax operation (Bridle, 1989). 2.2 Quantization A vectorwis uniformly quantized tob-bit precision, if its entries are restricted to the set{0,1,..., 2b− 1}for asymmetric case or {−2b,−2b + 1,..., 2b −1}for symmetric case, up to a real-valued scale α. This allows vector operations to utilize lower precision arithmetic, making them more efﬁcient by a factor of B b compared to full-precision calculation using Bbits. The scaling operation is still in higher precision, but if the dimensionality of w≫B b, then the extra computation is negligible. A neural network with parameters quantized to bw bits takes up B bw times less space. However, to take advantage of lower-precision arithmetic, the input vectors (activations) to each vector/matrix operation also need to be quantized. A network which has weights quantized tobw bits and activations quantized to ba bits is denoted as WbwAba. In this work, we’re speciﬁcally interested in W1A1 transformers. Binary arithmetic is especially attractive, since multiplications reduce to XNOR operations, and can be implemented orders of magnitude more efﬁciently using specialized hardware (Nurvitadhi et al., 2016). 2.3 Knowledge distillation Knowledge distillation (KD) (Hinton et al., 2015) is a technique whereby a student network can be trained to mimic the behavior of a teacher network. This is especially useful when the student network is more efﬁcient and easier to deploy than the more complex and cumbersome teacher. The basic way of performing KD is by using the output distribution of the teacher model (p) as soft targets for training the student model. If q is the student model’s output, then we have the loss term: Llogits = KL(p,q) (1) The advantage of KD over simple supervised training of a more efﬁcient model is that the teacher model provides a richer training signal including model conﬁdence for each output class. For computer vision tasks, distilling the ﬁnal logits solely works well for binary neural networks (Liu et al., 2020). If the student and teacher architectures are compatible, one can also distill intermediate activations for faster convergence and better transfer and generalization (Aguilar et al., 2020): Lreps = ∑ i ||rs i −rt i||2, (2) where rs i and rt i are the corresponding transformer block output activations from student and teacher. 3 Robust binarization setup In this section we ﬁrst bring together some best practices and minor improvements which we have found helpful in simplifying previous work and building a strong baseline. Then we present a novel activation binarization scheme, which we will show to be critical to achieve good performance. 3.1 Two-set binarization scheme In contrast to convolutional neural networks on images where activations exhibit comparable distribu- tions, different activations in transformer blocks are performing different functionalities, and thus vary in their output distributions. In particular, these activations can be divided into two categories: the activations after Softmax/ReLU layer that contains positive values only and the remaining activations with both positive and negative values (e.g., after matrix multiplication). If we denote by XR the vector of activation values, then the two cases are Xi R ∈R+ and Xi R ∈R respectively. For the former set, mapping to the binary levels{−1,1}would result in a severe distribution mismatch. Therefore we instead map non-negative activation layers to ˆXB ∈{0,1}n and binarize activation 3BinaryWeightFCLayer BinaryWeightFCLayer BinaryEmbeddings TransformerBlockOutput BinarizeActivation BinarizeActivationBinarizeActivationBinarizeActivation BinarizeActivation BinaryWeightFCQ BinaryWeightFCK BinaryWeightFCV BinarizeActivationBinaryWeightFC BinarizeActivation(0,1)Softmax BinarizeActivation(0,1)ReLU Self-Attention … Feed-ForwardNetwork Figure 1: Overview of BiT. A transformer block contains the multi-head self-attention and feed- forward network. We binarize all the weights to {-1, 1} in the Embedding/Fully-Connected layers and binarize activations to {0, 1} for ReLU/Softmax outputs and to {-1, 1} for other layers. layers with XR ∈Rn to ˆXB ∈{−1,1}n, shown in Figure 1. A prior work BiBERT (Qin et al., 2021) also suggests binarizing attention to {0,1}, but with bool function replacing SoftMax, while we empirically ﬁnd that simply binarizing attentions after SoftMax to {0,1}works better and binarizing ReLU output to {0,1}instead of {−1,1}brings further improvements. (See Section A.3 for details). Optimal scaling factor in two sets Additionally, we apply a layer-wise scaling factor to binarized activations to reduce the binarization error, i.e., XB = αˆXB. The optimal values of αare different for the ˆXB ∈{0,1}n and ˆXB ∈{−1,1}n cases and can be calculated by minimizing the l2 error: J(α) =||XR −αˆXB||2 α∗= arg min α∈R+ J(α) (3) Following XNOR-Net (Rastegari et al., 2016), by expanding Eq. 3, we have J(α) =α2 ˆXB T ˆXB −2αXR T ˆXB + XR TXR (4) For the layers with XR ∈Rn we follow the traditional methods of binarizing activations (Rastegari et al., 2016; Liu et al., 2018) by taking the sign of real-valued activations: ˆXi B = Sign(Xi R) = { −1, if Xi R <0 +1, if Xi R ⩾ 0 (5) In that case, ˆXB T ˆXB = nXR, where nXR is number of elements in XR, and α∗can be solved as: α∗= XR T ˆXB nXR = ||XR||l1 nXR (6) For the activations in attention layers or after the ReLU non-linearity layers with XR ∈Rn +, we binarize the activations to ˆXB ∈{0,1}n by rounding the real-valued activations: ˆXi B = ⌊Clip(Xi R,0,1)⌉= { 0, if Xi R <0.5 1, if Xi R ⩾ 0.5 (7) In that case, ˆXB T ˆXB = n{XR⩾0.5}where n{XR⩾0.5}denotes the number of elements in XR that are greater than or equal to 0.5. Then α∗can be solved as: α∗= ||XR ·1{XR⩾0.5}||l1 n{XR⩾0.5} (8) 43.2 Best practices We performed thorough experimentation and discovered the following modiﬁcations to be useful. Simpliﬁed knowledge distillation Compared to previous BERT model binarization works (Bai et al., 2021; Qin et al., 2021) which also attempt to distill the attention scores, we provide analysis and experimental results (Section 5.3) showing that using only Lreps from transformer block outputs and Llogits is more effective while being simpler. We also forego the two-step distillation scheme of Bai et al. (2021) in favor of a single step, joint distillation, where our training loss is simplyLlogits + Lreps. Mean subtraction in weight binarization For weight binarization, centeralizing the real-valued weights to be zero-mean before binarization can increase the information carrying capacity of the binary weights. Thus, for weight binarization, we have: Wi B = ||WR||l1 nWR Sign(Wi R −WR). Gradient clipping Clipping gradients to 0 when Xi R /∈[−1,1] (or Xi R /∈[0,1] if ˆXB ∈{0,1}n) is a common technique for training binarized neural networks. However, we ﬁnd that clipping weight gradients is harmful for optimization. Once a weight is outside of the clip range, the gradient is ﬁxed to 0, preventing further learning. This is not so for activations, since the activation value changes for each input. As a result, we apply gradient clipping only to activations but not to weights. Non-linearity We prefer ReLU activations whenever the output range is non-negative. Combining these, we are able to build a strong baseline, which improves the accuracy by 9.6% over naively binarized transformers. Additionally, these techniques allow us to train a weight binarized transformer network in a single training step using knowledge distillation (i.e., without resorting to weight splitting as in BinaryBert (Bai et al., 2021)) (See Section 5.3 for details). 3.3 Elastic binarization function The ﬁxed scaling and threshold derived previously works reasonably well, but might not be optimal since it ignores the distribution of the variable which is being binarized. Ideally, these parameters can be learned during training to minimize the target loss. When using classical binarization methods, i.e., ˆXi B = Sign(Xi R), the binary output is independent of the scale of the real-valued input. However, in our case where ˆXi B = ⌊Clip(Xi R,0,1)⌉, this independence no longer holds. Learning the scaling and threshold parameters, and how to approximate the gradients precisely in the process becomes crucial for the ﬁnal accuracy. To handle this, inspired by the learnable threshold in ReActNet (Liu et al., 2020), we propose the elastic binarization function to learn both the scale α∈R+ and the threshold β ∈R: Xi B = αˆXi B = α⌊Clip(Xi R −β α ,0,1)⌉ (9) In the function, we initialize αwith α∗in Sec. 3.1 and βto be 0, and train it with gradients from the ﬁnal loss. To back-propagate the gradients to αthrough the discretized binarization function, we follow the practice in Choi et al. (2018); Zhou et al. (2016); Esser et al. (2019) to use straight-through estimator (STE) (Bengio et al., 2013) to bypass the incoming gradients to the round function to be the outgoing gradients: ∂Xi B ∂α = ˆXi B + α∂ˆXi B ∂α STE ≈ ˆXi B + α∂Clip(Xi R−β α ,0,1) ∂α =    0, if Xi R <β β−Xi R α , if β ⩽ Xi R <α/2 +β 1 −Xi R−β α , if α/2 +β ⩽ Xi R <α + β 1, if Xi R ⩾ α+ β (10) Then the gradients w.r.t.βcan be similarly calculates as: ∂Xi B ∂β STE ≈ α∂Clip(Xi R−β α ,0,1) ∂β = { −1, if β ⩽ Xi R <α + β 0, otherwise (11) 5Algorithm 1 BiT: Multi-distillation algorithm Require: Dtrain, Ddev ⊿ Training Data Require: h0 ⊿ Full-precision Model Require: Q = {(b1 w, b1 a), . . . ,(bk w, bk a)} ⊿ Quantization Schedule 1: hteacher ← h0 2: for bi w, bi a in Q do 3: hstudent ← Quantize(hteacher, bi w, bi a) 4: KnowledgeDistill (hstudent, hteacher, Dtrain, Ddev) 5: hteacher ← hstudent 6: end for 7: return hstudent For the layers that contain both positive and negative real-valued activations i.e., XR ∈Rn, the binarized values ˆXB ∈ {−1,1}n are indifferent to the scale inside the Sign function: Xi B = α·Sign(Xi R−β α ) =α·Sign(Xi R −β). In that case, since the effect of scaling factorαinside the Sign function can be ignored, the gradient w.r.t.αcan be simply calculated as ∂Xi B ∂α = Sign(Xi R −β). In our ablations (Section 5.3 and A.2) we show that using this simple elastic binarization function can bring a 15.7% accuracy boost over our strong baseline on the GLUE benchmark. 4 Multi-distilled binary transformer Classical knowledge distillation (KD) (Hinton et al., 2015) trains the outputs ( i.e., logits) of a student network to be close to those of a teacher, which is typically larger and more complex. This approach is quite general, and can work with any student-teacher pair which conforms to the same output space. However, in practice, knowledge transfer happens faster and more effectively if the intermediate representations are also distilled (Aguilar et al., 2020). This approach has been found useful when distilling to student models with similar architecture (Sanh et al., 2019), and in particular for quantization (Bai et al., 2021; Kim et al., 2019). Note that having a similar student-teacher pair is a requirement for distilling representations. While how similar they need to be is an open question, intuitively a teacher which is architecturally closer to the student should make transfer of internal representations easier. In the context of quantization, it is easy to see that lower precision students are progressively less similar to the full-precision teacher, which is one reason why binarization is difﬁcult. This suggests a multi-step approach, where instead of directly distilling from a full-precision teacher to the desired quantization level, we ﬁrst distill into a model with sufﬁcient precision in order to preserve quality. This model can then be used as a teacher to distill into a further quantized student. This process can be repeated multiple times, while at each step ensuring that the teacher and student models are sufﬁciently similar, and the performance loss is limited. This multi-distillation approach is sketched in Algorithm 1. The multi-step distillation follows a quantization schedule, Q = {(b1 w,b 1 a ),(b2 w,b 2 a ),..., (bk w,b k a )} with (b1 w,b 1 a ) >(b2 w,b 2 a ) >...> (bk w,b k a )3. (bk w,b k a ) is the target quantization level, which is in our case binary for both weights and activations. In practice, we ﬁnd that down to a quantization level of W1A2, we can distill models of reasonable accuracy in single shot, following the best practices outlined in Section 3.2 (See our 1-1-2 baseline results in Table 1). As a result, we follow a ﬁxed quantization schedule, W32A32 →W1A2 →W1A1. This is not necessarily optimal, and how to efﬁciently ﬁnd the best quantization schedule is an interesting open problem. We present our initial explorations towards this direction in Section 5.5. Combining the elastic binary activations with multi-distillation we obtain BiT, the robustly binarized multi-distilled transformer. Note that BiT simultaneously ensures good initialization for the eventual (in our case binary) student model. Since the binary loss landscape is highly irregular, good initial- ization is critical to aid optimization. Previous work has proposed progressive distillation (Zhuang et al., 2018; Yang et al., 2019) to tackle this problem, wherein the student network is quantized at 3(a, b) > (c, d) if a > cand b ≥d or a ≥c and b > d. 6Table 1: Comparison of BERT quantization methods on the GLUE dev set. The E-W-A notation refers to the quantization level of embeddings, weights and activations. ‡denotes distilling binary models using full-precision teacher without using multi-distill technique in Section 4. *Data augmentation is not needed for MNLI, QNLI, therefore results in the data augmentation section are identical to that without data augmentation for these datasets. Quant #Bits Size (MB) FLOPs (G) MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg. BERT 32-32-32 418 22.5 84.9/85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 Without data augmentation Q-BERT 2-8-8 43.0 6.5 76.6/77.0 – – 84.6 – – 68.3 52.7 – Q2BERT 2-8-8 43.0 6.5 47.2/47.3 67.0 61.3 80.6 0 4.4 68.4 52.7 47.7 TernaryBERT 2-2-8 28.0 6.4 83.3/83.3 90.1 – – 50.7 – 87.5 68.2 – BinaryBERT 1-1-8 16.5 3.1 84.2/84.7 91.2 91.5 92.6 53.4 88.6 85.5 72.2 82.7 BinaryBERT 1-1-4 16.5 1.5 83.9/84.2 91.2 90.9 92.3 44.4 87.2 83.3 65.3 79.9 BinaryBERT 1-1-2 16.5 0.8 62.7/63.9 79.9 52.6 82.5 14.6 6.5 68.3 52.7 53.7 BinaryBERT 1-1-1 16.5 0.4 35.6/35.3 66.2 51.5 53.2 0 6.1 68.3 52.7 41.0 BiBERT 1-1-1 13.4 0.4 66.1/67.5 84.8 72.6 88.7 25.4 33.6 72.5 57.4 63.2 BiT ‡ 1-1-4 13.4 1.5 83.6/84.4 87.8 91.3 91.5 42.0 86.3 86.8 66.4 79.5 BiT ‡ 1-1-2 13.4 0.8 82.1/82.5 87.1 89.3 90.8 32.1 82.2 78.4 58.1 75.0 BiT ‡ 1-1-1 13.4 0.4 77.1/77.5 82.9 85.7 87.7 25.1 71.1 79.7 58.8 71.0 BiT 1-1-1 13.4 0.4 79.5 /79.4 85.4 86.4 89.9 32.9 72.0 79.9 62.1 73.5 With data augmentation TernaryBERT 2-2-8 28.0 6.4 83.3/83.3* 90.1* 90.0 92.9 47.8 84.3 82.6 68.4 80.3 BinaryBERT 1-1-8 16.5 3.1 84.2/84.7* 91.2* 91.6 93.2 55.5 89.2 86.0 74.0 83.3 BinaryBERT 1-1-4 16.5 1.5 83.9/84.2* 91.2* 91.4 93.7 53.3 88.6 86.0 71.5 82.6 BinaryBERT 1-1-2 16.5 0.8 62.7/63.9* 79.9* 51.0 89.6 33.0 11.4 71.0 55.9 57.6 BinaryBERT 1-1-1 16.5 0.4 35.6/35.3* 66.2* 66.1 78.3 7.3 22.1 69.3 57.7 48.7 BiBERT 1-1-1 13.4 0.4 66.1/67.5* 84.8* 76.0 90.9 37.8 56.7 78.8 61.0 68.8 BiT ‡ 1-1-2 13.4 0.8 82.1/82.5* 87.1* 88.8 92.5 43.2 86.3 90.4 72.9 80.4 BiT ‡ 1-1-1 13.4 0.4 77.1/77.5* 82.9* 85.0 91.5 32.0 84.1 88.0 67.5 76.0 BiT 1-1-1 13.4 0.4 79.5 /79.4* 85.4* 86.5 92.3 38.2 84.2 88.0 69.7 78.0 increasing severity as the training progresses. However, this method does not prevent the student network from drifting away from the teacher, which is always the full-precision model. We compare to progressive distillation in Section A.1. 5 Main results We follow recent work (Bai et al., 2021; Qin et al., 2021) in adopting the experimental setting of Devlin et al. (2019), and use the pre-trained BERT-base as our full-precision baseline. We evaluate on GLUE (Wang et al., 2019), a varied set of language understanding tasks (see Section A.5 for a full list), as well as SQuAD (v1.1) (Rajpurkar et al., 2016), a popular machine reading comprehension dataset. 5.1 GLUE results Our main results on the GLUE benchmarks are presented in Table 1. In the setting without data augmentation, where we only use the original training samples for knowledge distillation, we are able to reduce the gap to the full precision baseline by 49.8%, i.e., from 20.7 in (Qin et al., 2021) to 10.4 points. We also see that our baseline models with elastic activation binarization already improve previous SoTA by large margins. In the binary weight setting (4-bit activations), we can match or outperform Bai et al. (2021) without the need for pre-training half-width models and subsequently splitting weights. This result should make binary weight models much easier to implement and deploy. We also set a new state of the art for binary weight 2-bit activation (W1A2) models, with only a 3.5 point degradation compared to the full-precision baseline (using data augmentation). While not as efﬁcient as binary, 2-bit arithmetic can also be performed without multiplications, making it a good efﬁcient alternative in applications where the performance cost of going to fully binary is signiﬁcant. 5.1.1 Data augmentation From Table 1, it can be observed that the datasets with small training sets still have a large gap from the full-precision baseline. As a result, we employ data augmentation heuristics (following the exact setup in Zhang et al. (2020)) on the datasets with small training sets (all except MNLI, QNLI) to take better advantage of our model’s strong representational capability. This further reduces the quantization gap, with our models eventually trailing the full-precision model by only 5.9 points on average on the GLUE benchmark. 7Table 2: Ablation study on the effects of each component on GLUE dataset without data augmentation. Quant MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg. 1 BERT base 84.9/85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 2 BiBERT Baseline 45.8/47.0 73.2 66.4 77.6 11.7 7.6 70.2 54.1 50.4 3 BiBERT 66.1/67.5 84.8 72.6 88.7 25.4 33.6 72.5 57.4 63.2 4 BinaryBERT (Our implementation) 36.2/35.9 59.6 52.4 65.6 9.3 19.8 69.9 52.7 45.7 5 + Our simplied KD 37.7/37.3 59.5 56.8 73.4 4.1 24.8 70.8 57.0 48.0 6 + Our two-set binarization (Strong Baseline) 57.4/59.1 68.3 64.7 81.0 18.2 24.7 71.8 56.7 55.3 7 + Elastic binarization ( BiT ‡) 77.1/77.5 82.9 85.7 87.7 25.1 71.1 79.7 58.8 71.0 8 + Multi-Distillation ( BiT) 79.5/79.4 85.4 86.4 89.9 32.9 72.0 79.9 62.1 73.5 5.2 SQuAD results Table 3: Comparison of BERT quantization meth- ods on SQuADv1.1 dev set. Metrics are exact match and F1 score. Quant #Bits SQuADv1.1EM/F1 BERTbase 32-32-32 82.6/89.7 BinaryBERT 1-1-4 77.9/85.8BinaryBERT 1-1-2 72.3/81.8BinaryBERT 1-1-1 1.5/8.2BiBERT 1-1-1 8.5/18.9 BiT 1-1-1 63.1/74.9 We also evaluate on the popular machine reading comprehension (MRC) dataset from Rajpurkar et al. (2016). We compare to our own implemen- tation of the MRC task on top of the BiBERT codebase, since SQuAD results are not reported in that work. We also show results using the BinaryBERT codebase, without using weight splitting. The results (Table 3) show that this task is signiﬁcantly harder than most document classiﬁcation benchmarks in GLUE, and previ- ous binarization methods fail to achieve any meaningful level of performance. BiT does much better, but still trails the 32-bit baseline by 14.8 points in F1. We conclude that despite the improvements we have demonstrated on the GLUE benchmark, binarizing transformer models accurately is far from a solved problem in general. 5.3 Ablations We start from the basic binarization implementation from Bai et al. (2021), and add each of our contributions in sequence to get a better idea how each contributes to the performance. The results are shown in Table 2. We start by removing attention distillation (Section 3.2), which results in a 2.3% improvement (row 5 vs. 4). Then switching to our two-set binarization (Section 3.1), which binarizes the attention scores differently than the feed-forward activations, which gives an additional 7.3% boost (row 6). This results in a much stronger baseline than what was used in prior works (row 2). Moving from ﬁxed to elastic binarization (Section 3.3) proves hugely important, pushing the average accuracy to 71.0% (row 7) from only 55.3% (row 6). Note that this model already outperforms the current state-of-the-art (row 3) by 7.8% points. Finally, we add multi-step distillation (Section 4), which adds another 2.5 points, reaching the ﬁnal accuracy of 73.5% on the GLUE benchmark. 5.4 Learned parameter visualization Figure 2: The optimized scaling factor in BiT We visualize the optimized αin the ﬁ- nal BiT model. As we can see from Figure 2, the values of the αparameters vary signiﬁcantly from layer to layer, and have apparent patterns according to layer characteristics. For example, the attention layers need to distribute the at- tention to different entries, thus the scal- ing factor for the attentions are learned to be small, while the scaling factors for the query and key outputs are usually larger. Note that the biggest αvalue is 200×of the smallest α, suggesting the importance of learning αdynamically. 5.5 Exploring multi-distillation paths So far we have only considered the ﬁxed quantization schedule, W32A32 →W1A2 →W1A1. This is motivated by early experiments showing that one-step distillation to W1A2 works reasonably well. 8We explored other optimal schedules, such as distilling to W1A8 resulted in a higher accuracy model, thus a better teacher to distill down to the eventual W1A1 student. This suggests a trade-off between the quality of the intermediate model, vs. the closeness to the target quantization level. 32 8 4 2 1 Activation bits 76 78 80 82 84MNLI-m (matched) accuracy W32A32->W1A8->W1A1 W32A32->W1A4->W1A1 W32A32->W1A2->W1A1 W32A32->W1A1 W32A32->W1A4->W1A2->W1A1 Figure 3: MNLI-m accuracy on various distilla- tion paths. Each curve represents the sequence of models on a particular quantization schedule. Figure 3, illustrates this trade-off. We can see that two-step distillation improves over one-step in every case. While higher precision interme- diate models are better as expected, it is better to use a lower precision teacher in the last step since it makes the learning task for the binary student model easier. The closeness to the target quantization is favored despite the lower accu- racy of teacher model. It is of course possible, though more cumber- some, to perform more than two distillation steps. We also experiment with a three-step schedule, W32A32 →W1A4 →W1A2 → W1A1, which is plotted in the same ﬁgure (dashed line). We ﬁnd that this particular 3- step schedule does not improve over the 2-step schedule W32A32 →W1A2 →W1A1. While this result does not preclude existence of other more optimal schedules, we hypothesize that this is unlikely. 6 Related work Convolutional neural network quantization Neural network quantization is a practical tool for compressing model size and reduce storage (Hubara et al., 2017). Quantization for convolutional neural networks has been studied both in the uniform quantization (Choi et al., 2018; Zhou et al., 2016; Gong et al., 2019) and non-uniform quantization (Zhang et al., 2018; Miyashita et al., 2016; Li et al., 2020) settings. The quantization level has been progressively increased, from 8-bit (Wang et al., 2018; Zhu et al., 2020) to 4-bit (Jung et al., 2019; Liu et al., 2022) and ﬁnally to the extreme 1-bit case (Courbariaux et al., 2016; Rastegari et al., 2016; Liu et al., 2018; Martinez et al., 2020). Transformer quantization Compared to the CNNs, transformers with attention layers are naturally more challenging to quantize (Bondarenko et al., 2021). Previous research mainly focused on 8-bit quantization (Zafrir et al., 2019; Fan et al., 2020) or 4-bit quantization (Shen et al., 2020; Zadeh et al., 2020). Extremely low-bit quantization for transformers has only been attempted very recently. TernaryBERT (Zhang et al., 2020) proposed to ternarize the full-precision weights of a ﬁne-tuned BERT model. As a follow-up to TernaryBERT, weight binarization was proposed in Bai et al. (2021). Here, the network is trained by ﬁrst training a ternary half-sized model, which is used as initialization. Then a weight-splitting step results in a full-sized binarized model, which is further ﬁne-tuned in a subsequent distillation step. Binarizing both weight and activations in a transformer has proved to be challenging. BiBERT (Qin et al., 2021) made the ﬁrst attempt in this direction with limited success. Their model performed 20% worse than a real-valued baseline on the GLUE benchmark (Wang et al., 2019), which even underperforms the original LSTM baselines. 7 Conclusion Large pre-trained transformers have transformed NLP and are positioned to serve as the backbone for all AI models. In this work, we presented the ﬁrst successful demonstration of a fully binary pre-trained transformer model. While our approach is general and can be applied to any transformer, we have limited our evaluation to BERT-based models on the GLUE and SQuAD benchmarks. It remains to be seen how our conclusions will hold when applied to the wide variety of pre-trained transformer models which have gained popularity in recent years, from small mobile models, to gigantic ones with hundreds of billions of parameters. It will also be interesting to see the performance of the approach on different domains (such as image and speech processing) and tasks (such as text and image generation). Demonstrating the generality of this approach in a wider setting should signiﬁcantly widen its impact, therefore we identify this as an important future direction. 9References Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, and Chenlei Guo. Knowledge distillation from internal representations. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 7350–7357, 2020. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. In ACL/IJCNLP (1), 2021. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing textual entailment challenge. In TAC, 2009. Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efﬁcient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7947–7969, 2021. John Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. Advances in neural information processing systems, 2, 1989. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017. Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs. University of Waterloo, pp. 1–7, 2018. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, et al. Pact: Parameterized clipping activation for quantized neural networks. arXiv e-prints, pp. arXiv–1805, 2018. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), 2019. Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S Modha. Learned step size quantization. In International Conference on Learning Representations, 2019. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020. Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efﬁcient neural network inference. arXiv preprint arXiv:2103.13630, 2021. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4852–4861, 2019. 10Jun Han and Claudio Moraga. The inﬂuence of the sigmoid function parameters on the speed of backpropagation learning. In International workshop on artiﬁcial neural networks, pp. 195–201. Springer, 1995. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations.The Journal of Machine Learning Research, 18(1):6869–6898, 2017. Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4350–4359, 2019. Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, and Nojun Kwak. Qkd: Quantization-aware knowledge distillation. arXiv preprint arXiv:1911.12491, 2019. Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efﬁcient non-uniform discretization for neural networks. In International Conference on Learning Representations, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In Proceedings of the European conference on computer vision (ECCV), pp. 722–737, 2018. Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions. In European Conference on Computer Vision, pp. 143–159. Springer, 2020. Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform-to- uniform quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4942–4952, 2022. Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with real-to-binary convolutions. In ICLR, 2020. Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016. Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Icml, 2010. Eriko Nurvitadhi, David Shefﬁeld, Jaewoong Sim, Asit Mishra, Ganesh Venkatesh, and Debbie Marr. Accelerating binarized neural networks: Comparison of fpga, cpu, gpu, and asic. In 2016 International Conference on Field-Programmable Technology (FPT), pp. 77–84. IEEE, 2016. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In CVPR, 2020. Haotong Qin, Yifu Ding, Mingyuan Zhang, Y AN Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xianglong Liu. Bibert: Accurate fully binarized bert. In International Conference on Learning Representations, 2021. 11Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. In European conference on computer vision, pp. 525–542. Springer, 2016. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 8815–8821, 2020. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ﬂoating point numbers. Advances in neural information processing systems, 31, 2018. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Cola: The corpus of linguistic acceptability (with added annotations). 2019. Adina Williams, Nikita Nangia, and Samuel R Bowman. The multi-genre nli corpus. 2018. Yifan Yang, Qijing Huang, Bichen Wu, Tianjun Zhang, Liang Ma, Giulio Gambardella, Michaela Blott, Luciano Lavagno, Kees Vissers, John Wawrzynek, et al. Synetgy: Algorithm-hardware co-design for convnet accelerators on embedded fpgas. In Proceedings of the 2019 ACM/SIGDA international symposium on ﬁeld-programmable gate arrays, pp. 23–32, 2019. Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing attention-based nlp models for low latency and energy efﬁcient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 811–824. IEEE, 2020. Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 36–39. IEEE, 2019. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pp. 365–382, 2018. Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit BERT. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), EMNLP, 2020. 12Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards uniﬁed int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1969–1979, 2020. Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective low- bitwidth convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7920–7928, 2018. A Appendix A.1 BiT vs. progressive distillation Table 4: BiT vs. progressive distillation on selected GLUE tasks. Methods differ in the teacher model used and the model from which the student weights are initialized. Method Teacher Initialization MNLI-m/mmQQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.BiBERT Distillation 32-32-32 32-32-32 77.0/77.2 83.1 84.1 89.7 31.3 60.1 75.5 56.7 69.7Progressive 32-32-32 1-1-2 78.9/78.9 85.0 86.4 89.6 30.5 75.1 81.1 60.6 73.4BiT 1-1-2 1-1-2 79.5/79.4 85.4 86.4 89.9 32.9 72.0 79.9 62.1 73.5 Previous work has also recognized the importance of good initialization for binary model training, and proposed to perform distillation while progressively quantizing the student model (Zhuang et al., 2018; Yang et al., 2019). Progressive distillation ensures a good initialization for the student model at each step. However, in this approach the teacher model is ﬁxed to the full precision model, which does not address the problem of teacher-student gap. In Table 4 we compare BiT to a comparable implementation of progressive distillation, using the same quantization schedule, W32A32 →W1A2 →W1A1, as ours. We keep the teacher model ﬁxed, while re-initializing the student model from the latest quantized version at each step. We see that using a quantized teacher model is helpful, especially in the high-data regime. However, our method can lag behind progressive distillation for small datasets such as STS-B and MRPC. A.2 Elastic binarization function vs. ReActNet learnable bias Table 5: Elastic binarization function vs. ReActNet (Liu et al., 2020) learnable bias on GLUE tasks. Method MNLI -m/mmQQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.Our two-set binarization (Strong Baseline) 57.4/59.1 68.3 64.7 81.0 18.2 24.7 71.8 56.7 55.3+ learnable scale 76.5/76.8 82.7 85.1 88.1 26.6 62.3 74.3 58.1 69.2+ learnable scale and bias (BiT‡) 77.1/77.5 82.9 85.7 87.7 25.1 71.1 79.7 58.8 71.0 Inspired by the learnable bias proposed in ReActNet (Liu et al., 2020), we further propose elastic binarization function to learn both learnable scaling factors and learnable bias. We ﬁnd this learnable scaling factor critical for the ﬁnal performance. As shown in table 5, the proposed learnable scaling factor brings 13.9% accuracy improvement, and further adding learnable bias boosts the accuracy by 1.8%. A.3 Two-set binarization scheme vs. Bi-Attention Table 6: Two-set binarization scheme vs. Bi-Attention (Qin et al., 2021) on GLUE tasks. Methods differ in whether using SoftMax in attention and whether binarizing the ReLU output to {0 ,1}. Method Attention ReLU output MNLI -m/mmQQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.Bi-Attention (w/o Softmax) {0, 1} {-1, 1} 48.1/50.0 60.1 60.6 78.8 14.0 22.3 68.4 58.1 51.3Binarize attention to {0, 1} (w/ Softmax) {0, 1} {-1, 1} 51.9/52.6 76.2 60.5 79.6 11.6 18.1 70.6 55.6 53.0Two-set binarization scheme {0, 1} {0, 1} 57.4/59.1 68.3 64.7 81.0 18.2 24.7 71.8 56.7 55.3 In contrast to Bi-Attention proposed in BiBERT (Qin et al., 2021) that removesSoftMax and binarizes the attention to {0, 1} withbool function, our two-set binarization scheme ﬁnds that keepingSoftMax 13in attention computation and also binarizing the positive output of ReLU layer to {0, 1} works better. We conduct meticulous experiments to compare these choices. In Table 6, we show that, compared to removing SoftMax as Bi-Attention suggested, simply binarizing the activations after SoftMax layer to {0, 1} even produces 1.7% better accuracy. Furthermore, binarizing the ReLU layer output to {0, 1} instead of {-1, 1} helps the binary network match real-valued distributions and further brings 2.3% accuracy improvement. A.4 Binary convolution implementation for two-set binarization scheme The binary convolution between the weights and activations that are both binarized to {-1, 1} (i.e. AB ∈{-1, 1}, WB ∈{-1, 1}) can be implemented by the bitwise xnor operation followed by a popcnt operation (Rastegari et al., 2016; Liu et al., 2018): AB ·WB = popcnt(xnor(AB,WB)) (12) For the case where activations are binarized to {0, 1} in two-set binarization scheme, the binary activation AB ∈{0, 1} can be represented with A′ B ∈{-1, 1} through a simple linear mapping: AB = A′ B+1 2 . Thus the matrix computation between binary weights ( WB ∈{-1, 1} ) and binary activations (AB ∈{0, 1}) can be converted to the operations between WB ∈{-1, 1} and A′ B ∈{-1, 1} as: AB ·WB = (A′ B + 1 2 ) ·WB = 1 2(popcnt(xnor(A′ B,WB)) + ∑ i WBi) (13) Here the ∑ iWBi is summing up the values in WB, which can be pre-computed and stored as bias. Thus in the two-set binarization scheme where activations are binarized to {0, 1}, the binary convolution can still be implemented with the general binary convolution in E.q. 12 at no additional complexity cost. A.5 Evaluation benchmarks A.5.1 GLUE The GLUE benchmark (Wang et al., 2019) includes the following datasets: MNLI Multi-Genre Natural Language Inference is an entailment classiﬁcation task (Williams et al., 2018). The goal is to predict whether a given sentence entails, contradicts, or is neutral with respect to another. QQP Quora Question Pairs is a paraphrase detection task. The goal is to classify whether two given questions have the same meaning. The questions were sourced from the Quora question answering website (Chen et al., 2018). QNLI Question Natural Language Inference (Wang et al., 2019) is a binary classiﬁcation task which is derived from the Stanford Question Answering Dataset (Rajpurkar et al., 2016). The task is to predict whether a sentence contains the answer to a given question. SST-2 The Stanford Sentiment Treebank is a binary sentiment classiﬁcation task, with content taken from movie reviews (Socher et al., 2013). CoLA The Corpus of Linguistic Acceptability is a corpus of English sentences, each with a binary label denoting whether the sentence is linguistically acceptable (Warstadt et al., 2019). STS-B The Semantic Textual Similarity Benchmark is a sentence pair classiﬁcation task. The goal is to predict how similar the two sentences are in meaning, with scores ranging from 1 to 5 (Cer et al., 2017). MRPC Microsoft Research Paraphrase Corpus is another sentence pair paraphrase detection task similar to QQP. The sentence pairs are sourced from online news sources (Dolan & Brockett, 2005). 14RTE Recognizing Textual Entailment is a small natural language inference dataset similar to MNLI in content (Bentivogli et al., 2009). A.5.2 SQuAD The SQuAD benchmark (Rajpurkar et al., 2016), i.e., Stanford Question Answering Dataset, is a reading comprehension dataset, consisting of questions on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding passage, or the question might be unanswerable. A.6 Technical details For each experiment, we sweep the learning rate in {1e-4, 2e-4, 5e-4} and the batch size in {8, 16} for QNLI, SST-2, CoLA, STS-B, MRPC, RTE, and {16, 32} for MNLI, QQP as well as SQuAD, and choose the settings with the highest accuracy on the validation set. We use the same number of training epochs as BiBERT (Qin et al., 2021), i.e., 50 for CoLA, 20 for MRPC, STS-B and RTE, 10 for SST-2 and QNLI, 5 for MNLI and QQP. We adopt the Adam optimizer with weight decay 0.01 and use 0.1 warmup ratio with linear learning rate decay. Our full precision checkpoints are taken from https://textattack.readthedocs.io/en/ latest/3recipes/models.html#bert-base-uncased. 15",
      "meta_data": {
        "arxiv_id": "2205.13016v2",
        "authors": [
          "Zechun Liu",
          "Barlas Oguz",
          "Aasish Pappu",
          "Lin Xiao",
          "Scott Yih",
          "Meng Li",
          "Raghuraman Krishnamoorthi",
          "Yashar Mehdad"
        ],
        "published_date": "2022-05-25T19:01:54Z",
        "pdf_url": "https://arxiv.org/pdf/2205.13016v2.pdf"
      }
    },
    {
      "title": "BiT: Robustly Binarized Multi-distilled Transformer",
      "abstract": "Modern pre-trained transformers have rapidly advanced the state-of-the-art in\nmachine learning, but have also grown in parameters and computational\ncomplexity, making them increasingly difficult to deploy in\nresource-constrained environments. Binarization of the weights and activations\nof the network can significantly alleviate these issues, however, is\ntechnically challenging from an optimization perspective. In this work, we\nidentify a series of improvements that enables binary transformers at a much\nhigher accuracy than what was possible previously. These include a two-set\nbinarization scheme, a novel elastic binary activation function with learned\nparameters, and a method to quantize a network to its limit by successively\ndistilling higher precision models into lower precision students. These\napproaches allow for the first time, fully binarized transformer models that\nare at a practical level of accuracy, approaching a full-precision BERT\nbaseline on the GLUE language understanding benchmark within as little as 5.9%.\nCode and models are available at: https://github.com/facebookresearch/bit.",
      "full_text": "BiT: Robustly Binarized Multi-distilled Transformer Zechun Liu∗ Reality Labs, Meta Inc. zechunliu@fb.com Barlas O˘guz∗ Meta AI barlaso@fb.com Aasish Pappu Meta AI aasish@fb.com Lin Xiao Meta AI Scott Yih Meta AI Meng Li Peking University Raghuraman Krishnamoorthi Reality Labs, Meta Inc. Yashar Mehdad Meta AI Abstract Modern pre-trained transformers have rapidly advanced the state-of-the-art in machine learning, but have also grown in parameters and computational complexity, making them increasingly difﬁcult to deploy in resource-constrained environments. Binarization of the weights and activations of the network can signiﬁcantly alleviate these issues, however, is technically challenging from an optimization perspective. In this work, we identify a series of improvements that enables binary transformers at a much higher accuracy than what was possible previously. These include a two-set binarization scheme, a novel elastic binary activation function with learned parameters, and a method to quantize a network to its limit by successively distilling higher precision models into lower precision students. These approaches allow for the ﬁrst time, fully binarized transformer models that are at a practical level of accuracy, approaching a full-precision BERT baseline on the GLUE language understanding benchmark within as little as 5.9%. Code and models are available at: https://github.com/facebookresearch/bit. 1 Introduction The past few years have witnessed tremendous advances in almost all applied ﬁelds of AI. It would hardly be a simpliﬁcation to say that the bulk of these advances was achieved by scaling the transformer architecture (Vaswani et al., 2017) to ever larger sizes with increasing computation budget (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2018, 2019; Raffel et al., 2020; Brown et al., 2020). On the other hand, mobile devices and wearables have proliferated and shrunk in size, with stringent requirements for storage, computation and energy consumption. Consumers demand more portability, while having access to all that current AI technology has to offer. As a result, the gap between what is possible in AI, and what is deployable has never been wider. While there is a variety of methods to increase inference efﬁciency in neural networks (e.g. knowledge distillation, pruning), quantization has some attractive properties and has been widely successful in practice (Gholami et al., 2021). For one, storage and latency gains from quantization are deter- ministically deﬁned for a given quantization level. For instance, reducing the precision of model parameters by a given factor immediately translates to an identical reduction in storage cost. Similarly, reducing the precision of arithmetic operations results in a corresponding reduction in computational cost. Uniform quantization is hardware friendly, making it relatively simple to realize theoretical improvements in practice. ∗ Equal contribution Preprint. Under review. arXiv:2205.13016v2  [cs.LG]  2 Oct 2022Binarization represents the extreme limit of quantization, promising a 32×reduction in storage over full-precision (32-bit) models. Moreover, binary arithmetic completely eliminates multiplications in favor of bit-wise XNOR operations (Courbariaux et al., 2016; Rastegari et al., 2016), enabling even further improvements when using special purpose hardware. Energy efﬁciency improvements between 100-1000x have been claimed to be possible with binary neural networks (BNNs), over their full-precision counterparts (Nurvitadhi et al., 2016). The obvious challenge with binarization is the difﬁculty of optimization. While all quantization is discontinuous, higher precisions allow approximating the full-precision network to a better extent, where with BNNs, this becomes much harder. Surprisingly, researchers in computer vision have been able to demonstrate BNNs with remarkable accuracy (Liu et al., 2018; Qin et al., 2020; Martinez et al., 2020). Unfortunately, while these works have mostly been developed on convolutional architectures for image tasks, they have not generalized well to transformer models. For instance, recent work (Qin et al., 2021) has shown that a BERT (Devlin et al., 2019) model with binary weights and activations lags its full-precision counterpart by as much as 20 points on average on GLUE dataset. Even for weights-only binarization, the loss landscape was shown to be too irregular, and recent work resorted to complex and specialized methods such as weight-splitting from half-width models to achieve a reasonable accuracy (Bai et al., 2021). With this background, we tackle the problem of fully binarizing transformer models to a high level of accuracy. With the expansion trend of transformers towards becoming the standard architecture choice for all ﬁelds of AI, we believe a solution to this problem could be highly impactful. Our approach follows the same paradigm as previous work, based on knowledge distillation (Hinton et al., 2015) from higher precision models using the straight-through estimator (STE) of Bengio et al. (2013). In view of the optimization difﬁculties, we take the following steps to ensure that the student and teacher models are well-matched: • In Section 3 we describe a robust binarization framework, which allows the binary student network to better match the output distribution of the teacher. This allows us to achieve SoTA results for extreme activation quantization with BERT, producing models with little loss in accuracy down to a quantization level of binary weights and 2-bit activations and improves over previous setups by large margins in the fully binary (1-bit) setting. It also leads to competitive results for weight binarization with 4-bit activations using a single knowledge distillation step. • To further improve binary models, we propose a multi-distillation approach, described in Section 4. Instead of distilling directly into a fully binary model, we ﬁrst distill an intermediate model of medium precision and acceptable accuracy. This model then becomes the teacher in the next round of distillation into increasingly quantized models. Such a method ensures that the student model doesn’t drift too far from the teacher, while also ensuring as good an initialization as possible. We call the resulting model BiT 2. In the vanilla setting without data augmentation, our approach reduces the accuracy gap to a full- precision BERT-base model by half on the GLUE (Wang et al., 2019) benchmark compared to the previous SoTA. When using data augmentation, we are able to reduce the absolute accuracy gap to only 5.9 points (from over 15 points previously). In addition to the fully binary setting, we also report SoTA results with binary weights and 2-bit activations, where our models trail the full-precision baseline by only 3.5 points. 2 Background 2.1 Transformer architecture The transformer model of Vaswani et al. (2017) is composed of an embedding layer, followed by N transformer blocks and a linear output layer. Each transformer block consists of a multi-head attention layer followed by a feed-forward network, as shown in Figure 1. The multi-head attention layer is a concatenation of Kscaled dot-product attention heads, deﬁned by: Attention(Q,K,V ) =softmax (QKT √dk ) V 2Short for Binarized Transformer. 2where dk is the dimension of each key, Q,K,V are weight matrices for the query, key and value respectively. As such, the computation in a transformer model is limited to linear matrix mul- tiplications and additions, pointwise non-linearities (most commonly Sigmoid (Han & Moraga, 1995), GeLU (Hendrycks & Gimpel, 2016) or ReLU (Nair & Hinton, 2010) ) and the Softmax operation (Bridle, 1989). 2.2 Quantization A vectorwis uniformly quantized tob-bit precision, if its entries are restricted to the set{0,1,..., 2b− 1}for asymmetric case or {−2b,−2b + 1,..., 2b −1}for symmetric case, up to a real-valued scale α. This allows vector operations to utilize lower precision arithmetic, making them more efﬁcient by a factor of B b compared to full-precision calculation using Bbits. The scaling operation is still in higher precision, but if the dimensionality of w≫B b, then the extra computation is negligible. A neural network with parameters quantized to bw bits takes up B bw times less space. However, to take advantage of lower-precision arithmetic, the input vectors (activations) to each vector/matrix operation also need to be quantized. A network which has weights quantized tobw bits and activations quantized to ba bits is denoted as WbwAba. In this work, we’re speciﬁcally interested in W1A1 transformers. Binary arithmetic is especially attractive, since multiplications reduce to XNOR operations, and can be implemented orders of magnitude more efﬁciently using specialized hardware (Nurvitadhi et al., 2016). 2.3 Knowledge distillation Knowledge distillation (KD) (Hinton et al., 2015) is a technique whereby a student network can be trained to mimic the behavior of a teacher network. This is especially useful when the student network is more efﬁcient and easier to deploy than the more complex and cumbersome teacher. The basic way of performing KD is by using the output distribution of the teacher model (p) as soft targets for training the student model. If q is the student model’s output, then we have the loss term: Llogits = KL(p,q) (1) The advantage of KD over simple supervised training of a more efﬁcient model is that the teacher model provides a richer training signal including model conﬁdence for each output class. For computer vision tasks, distilling the ﬁnal logits solely works well for binary neural networks (Liu et al., 2020). If the student and teacher architectures are compatible, one can also distill intermediate activations for faster convergence and better transfer and generalization (Aguilar et al., 2020): Lreps = ∑ i ||rs i −rt i||2, (2) where rs i and rt i are the corresponding transformer block output activations from student and teacher. 3 Robust binarization setup In this section we ﬁrst bring together some best practices and minor improvements which we have found helpful in simplifying previous work and building a strong baseline. Then we present a novel activation binarization scheme, which we will show to be critical to achieve good performance. 3.1 Two-set binarization scheme In contrast to convolutional neural networks on images where activations exhibit comparable distribu- tions, different activations in transformer blocks are performing different functionalities, and thus vary in their output distributions. In particular, these activations can be divided into two categories: the activations after Softmax/ReLU layer that contains positive values only and the remaining activations with both positive and negative values (e.g., after matrix multiplication). If we denote by XR the vector of activation values, then the two cases are Xi R ∈R+ and Xi R ∈R respectively. For the former set, mapping to the binary levels{−1,1}would result in a severe distribution mismatch. Therefore we instead map non-negative activation layers to ˆXB ∈{0,1}n and binarize activation 3BinaryWeightFCLayer BinaryWeightFCLayer BinaryEmbeddings TransformerBlockOutput BinarizeActivation BinarizeActivationBinarizeActivationBinarizeActivation BinarizeActivation BinaryWeightFCQ BinaryWeightFCK BinaryWeightFCV BinarizeActivationBinaryWeightFC BinarizeActivation(0,1)Softmax BinarizeActivation(0,1)ReLU Self-Attention … Feed-ForwardNetwork Figure 1: Overview of BiT. A transformer block contains the multi-head self-attention and feed- forward network. We binarize all the weights to {-1, 1} in the Embedding/Fully-Connected layers and binarize activations to {0, 1} for ReLU/Softmax outputs and to {-1, 1} for other layers. layers with XR ∈Rn to ˆXB ∈{−1,1}n, shown in Figure 1. A prior work BiBERT (Qin et al., 2021) also suggests binarizing attention to {0,1}, but with bool function replacing SoftMax, while we empirically ﬁnd that simply binarizing attentions after SoftMax to {0,1}works better and binarizing ReLU output to {0,1}instead of {−1,1}brings further improvements. (See Section A.3 for details). Optimal scaling factor in two sets Additionally, we apply a layer-wise scaling factor to binarized activations to reduce the binarization error, i.e., XB = αˆXB. The optimal values of αare different for the ˆXB ∈{0,1}n and ˆXB ∈{−1,1}n cases and can be calculated by minimizing the l2 error: J(α) =||XR −αˆXB||2 α∗= arg min α∈R+ J(α) (3) Following XNOR-Net (Rastegari et al., 2016), by expanding Eq. 3, we have J(α) =α2 ˆXB T ˆXB −2αXR T ˆXB + XR TXR (4) For the layers with XR ∈Rn we follow the traditional methods of binarizing activations (Rastegari et al., 2016; Liu et al., 2018) by taking the sign of real-valued activations: ˆXi B = Sign(Xi R) = { −1, if Xi R <0 +1, if Xi R ⩾ 0 (5) In that case, ˆXB T ˆXB = nXR, where nXR is number of elements in XR, and α∗can be solved as: α∗= XR T ˆXB nXR = ||XR||l1 nXR (6) For the activations in attention layers or after the ReLU non-linearity layers with XR ∈Rn +, we binarize the activations to ˆXB ∈{0,1}n by rounding the real-valued activations: ˆXi B = ⌊Clip(Xi R,0,1)⌉= { 0, if Xi R <0.5 1, if Xi R ⩾ 0.5 (7) In that case, ˆXB T ˆXB = n{XR⩾0.5}where n{XR⩾0.5}denotes the number of elements in XR that are greater than or equal to 0.5. Then α∗can be solved as: α∗= ||XR ·1{XR⩾0.5}||l1 n{XR⩾0.5} (8) 43.2 Best practices We performed thorough experimentation and discovered the following modiﬁcations to be useful. Simpliﬁed knowledge distillation Compared to previous BERT model binarization works (Bai et al., 2021; Qin et al., 2021) which also attempt to distill the attention scores, we provide analysis and experimental results (Section 5.3) showing that using only Lreps from transformer block outputs and Llogits is more effective while being simpler. We also forego the two-step distillation scheme of Bai et al. (2021) in favor of a single step, joint distillation, where our training loss is simplyLlogits + Lreps. Mean subtraction in weight binarization For weight binarization, centeralizing the real-valued weights to be zero-mean before binarization can increase the information carrying capacity of the binary weights. Thus, for weight binarization, we have: Wi B = ||WR||l1 nWR Sign(Wi R −WR). Gradient clipping Clipping gradients to 0 when Xi R /∈[−1,1] (or Xi R /∈[0,1] if ˆXB ∈{0,1}n) is a common technique for training binarized neural networks. However, we ﬁnd that clipping weight gradients is harmful for optimization. Once a weight is outside of the clip range, the gradient is ﬁxed to 0, preventing further learning. This is not so for activations, since the activation value changes for each input. As a result, we apply gradient clipping only to activations but not to weights. Non-linearity We prefer ReLU activations whenever the output range is non-negative. Combining these, we are able to build a strong baseline, which improves the accuracy by 9.6% over naively binarized transformers. Additionally, these techniques allow us to train a weight binarized transformer network in a single training step using knowledge distillation (i.e., without resorting to weight splitting as in BinaryBert (Bai et al., 2021)) (See Section 5.3 for details). 3.3 Elastic binarization function The ﬁxed scaling and threshold derived previously works reasonably well, but might not be optimal since it ignores the distribution of the variable which is being binarized. Ideally, these parameters can be learned during training to minimize the target loss. When using classical binarization methods, i.e., ˆXi B = Sign(Xi R), the binary output is independent of the scale of the real-valued input. However, in our case where ˆXi B = ⌊Clip(Xi R,0,1)⌉, this independence no longer holds. Learning the scaling and threshold parameters, and how to approximate the gradients precisely in the process becomes crucial for the ﬁnal accuracy. To handle this, inspired by the learnable threshold in ReActNet (Liu et al., 2020), we propose the elastic binarization function to learn both the scale α∈R+ and the threshold β ∈R: Xi B = αˆXi B = α⌊Clip(Xi R −β α ,0,1)⌉ (9) In the function, we initialize αwith α∗in Sec. 3.1 and βto be 0, and train it with gradients from the ﬁnal loss. To back-propagate the gradients to αthrough the discretized binarization function, we follow the practice in Choi et al. (2018); Zhou et al. (2016); Esser et al. (2019) to use straight-through estimator (STE) (Bengio et al., 2013) to bypass the incoming gradients to the round function to be the outgoing gradients: ∂Xi B ∂α = ˆXi B + α∂ˆXi B ∂α STE ≈ ˆXi B + α∂Clip(Xi R−β α ,0,1) ∂α =    0, if Xi R <β β−Xi R α , if β ⩽ Xi R <α/2 +β 1 −Xi R−β α , if α/2 +β ⩽ Xi R <α + β 1, if Xi R ⩾ α+ β (10) Then the gradients w.r.t.βcan be similarly calculates as: ∂Xi B ∂β STE ≈ α∂Clip(Xi R−β α ,0,1) ∂β = { −1, if β ⩽ Xi R <α + β 0, otherwise (11) 5Algorithm 1 BiT: Multi-distillation algorithm Require: Dtrain, Ddev ⊿ Training Data Require: h0 ⊿ Full-precision Model Require: Q = {(b1 w, b1 a), . . . ,(bk w, bk a)} ⊿ Quantization Schedule 1: hteacher ← h0 2: for bi w, bi a in Q do 3: hstudent ← Quantize(hteacher, bi w, bi a) 4: KnowledgeDistill (hstudent, hteacher, Dtrain, Ddev) 5: hteacher ← hstudent 6: end for 7: return hstudent For the layers that contain both positive and negative real-valued activations i.e., XR ∈Rn, the binarized values ˆXB ∈ {−1,1}n are indifferent to the scale inside the Sign function: Xi B = α·Sign(Xi R−β α ) =α·Sign(Xi R −β). In that case, since the effect of scaling factorαinside the Sign function can be ignored, the gradient w.r.t.αcan be simply calculated as ∂Xi B ∂α = Sign(Xi R −β). In our ablations (Section 5.3 and A.2) we show that using this simple elastic binarization function can bring a 15.7% accuracy boost over our strong baseline on the GLUE benchmark. 4 Multi-distilled binary transformer Classical knowledge distillation (KD) (Hinton et al., 2015) trains the outputs ( i.e., logits) of a student network to be close to those of a teacher, which is typically larger and more complex. This approach is quite general, and can work with any student-teacher pair which conforms to the same output space. However, in practice, knowledge transfer happens faster and more effectively if the intermediate representations are also distilled (Aguilar et al., 2020). This approach has been found useful when distilling to student models with similar architecture (Sanh et al., 2019), and in particular for quantization (Bai et al., 2021; Kim et al., 2019). Note that having a similar student-teacher pair is a requirement for distilling representations. While how similar they need to be is an open question, intuitively a teacher which is architecturally closer to the student should make transfer of internal representations easier. In the context of quantization, it is easy to see that lower precision students are progressively less similar to the full-precision teacher, which is one reason why binarization is difﬁcult. This suggests a multi-step approach, where instead of directly distilling from a full-precision teacher to the desired quantization level, we ﬁrst distill into a model with sufﬁcient precision in order to preserve quality. This model can then be used as a teacher to distill into a further quantized student. This process can be repeated multiple times, while at each step ensuring that the teacher and student models are sufﬁciently similar, and the performance loss is limited. This multi-distillation approach is sketched in Algorithm 1. The multi-step distillation follows a quantization schedule, Q = {(b1 w,b 1 a ),(b2 w,b 2 a ),..., (bk w,b k a )} with (b1 w,b 1 a ) >(b2 w,b 2 a ) >...> (bk w,b k a )3. (bk w,b k a ) is the target quantization level, which is in our case binary for both weights and activations. In practice, we ﬁnd that down to a quantization level of W1A2, we can distill models of reasonable accuracy in single shot, following the best practices outlined in Section 3.2 (See our 1-1-2 baseline results in Table 1). As a result, we follow a ﬁxed quantization schedule, W32A32 →W1A2 →W1A1. This is not necessarily optimal, and how to efﬁciently ﬁnd the best quantization schedule is an interesting open problem. We present our initial explorations towards this direction in Section 5.5. Combining the elastic binary activations with multi-distillation we obtain BiT, the robustly binarized multi-distilled transformer. Note that BiT simultaneously ensures good initialization for the eventual (in our case binary) student model. Since the binary loss landscape is highly irregular, good initial- ization is critical to aid optimization. Previous work has proposed progressive distillation (Zhuang et al., 2018; Yang et al., 2019) to tackle this problem, wherein the student network is quantized at 3(a, b) > (c, d) if a > cand b ≥d or a ≥c and b > d. 6Table 1: Comparison of BERT quantization methods on the GLUE dev set. The E-W-A notation refers to the quantization level of embeddings, weights and activations. ‡denotes distilling binary models using full-precision teacher without using multi-distill technique in Section 4. *Data augmentation is not needed for MNLI, QNLI, therefore results in the data augmentation section are identical to that without data augmentation for these datasets. Quant #Bits Size (MB) FLOPs (G) MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg. BERT 32-32-32 418 22.5 84.9/85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 Without data augmentation Q-BERT 2-8-8 43.0 6.5 76.6/77.0 – – 84.6 – – 68.3 52.7 – Q2BERT 2-8-8 43.0 6.5 47.2/47.3 67.0 61.3 80.6 0 4.4 68.4 52.7 47.7 TernaryBERT 2-2-8 28.0 6.4 83.3/83.3 90.1 – – 50.7 – 87.5 68.2 – BinaryBERT 1-1-8 16.5 3.1 84.2/84.7 91.2 91.5 92.6 53.4 88.6 85.5 72.2 82.7 BinaryBERT 1-1-4 16.5 1.5 83.9/84.2 91.2 90.9 92.3 44.4 87.2 83.3 65.3 79.9 BinaryBERT 1-1-2 16.5 0.8 62.7/63.9 79.9 52.6 82.5 14.6 6.5 68.3 52.7 53.7 BinaryBERT 1-1-1 16.5 0.4 35.6/35.3 66.2 51.5 53.2 0 6.1 68.3 52.7 41.0 BiBERT 1-1-1 13.4 0.4 66.1/67.5 84.8 72.6 88.7 25.4 33.6 72.5 57.4 63.2 BiT ‡ 1-1-4 13.4 1.5 83.6/84.4 87.8 91.3 91.5 42.0 86.3 86.8 66.4 79.5 BiT ‡ 1-1-2 13.4 0.8 82.1/82.5 87.1 89.3 90.8 32.1 82.2 78.4 58.1 75.0 BiT ‡ 1-1-1 13.4 0.4 77.1/77.5 82.9 85.7 87.7 25.1 71.1 79.7 58.8 71.0 BiT 1-1-1 13.4 0.4 79.5 /79.4 85.4 86.4 89.9 32.9 72.0 79.9 62.1 73.5 With data augmentation TernaryBERT 2-2-8 28.0 6.4 83.3/83.3* 90.1* 90.0 92.9 47.8 84.3 82.6 68.4 80.3 BinaryBERT 1-1-8 16.5 3.1 84.2/84.7* 91.2* 91.6 93.2 55.5 89.2 86.0 74.0 83.3 BinaryBERT 1-1-4 16.5 1.5 83.9/84.2* 91.2* 91.4 93.7 53.3 88.6 86.0 71.5 82.6 BinaryBERT 1-1-2 16.5 0.8 62.7/63.9* 79.9* 51.0 89.6 33.0 11.4 71.0 55.9 57.6 BinaryBERT 1-1-1 16.5 0.4 35.6/35.3* 66.2* 66.1 78.3 7.3 22.1 69.3 57.7 48.7 BiBERT 1-1-1 13.4 0.4 66.1/67.5* 84.8* 76.0 90.9 37.8 56.7 78.8 61.0 68.8 BiT ‡ 1-1-2 13.4 0.8 82.1/82.5* 87.1* 88.8 92.5 43.2 86.3 90.4 72.9 80.4 BiT ‡ 1-1-1 13.4 0.4 77.1/77.5* 82.9* 85.0 91.5 32.0 84.1 88.0 67.5 76.0 BiT 1-1-1 13.4 0.4 79.5 /79.4* 85.4* 86.5 92.3 38.2 84.2 88.0 69.7 78.0 increasing severity as the training progresses. However, this method does not prevent the student network from drifting away from the teacher, which is always the full-precision model. We compare to progressive distillation in Section A.1. 5 Main results We follow recent work (Bai et al., 2021; Qin et al., 2021) in adopting the experimental setting of Devlin et al. (2019), and use the pre-trained BERT-base as our full-precision baseline. We evaluate on GLUE (Wang et al., 2019), a varied set of language understanding tasks (see Section A.5 for a full list), as well as SQuAD (v1.1) (Rajpurkar et al., 2016), a popular machine reading comprehension dataset. 5.1 GLUE results Our main results on the GLUE benchmarks are presented in Table 1. In the setting without data augmentation, where we only use the original training samples for knowledge distillation, we are able to reduce the gap to the full precision baseline by 49.8%, i.e., from 20.7 in (Qin et al., 2021) to 10.4 points. We also see that our baseline models with elastic activation binarization already improve previous SoTA by large margins. In the binary weight setting (4-bit activations), we can match or outperform Bai et al. (2021) without the need for pre-training half-width models and subsequently splitting weights. This result should make binary weight models much easier to implement and deploy. We also set a new state of the art for binary weight 2-bit activation (W1A2) models, with only a 3.5 point degradation compared to the full-precision baseline (using data augmentation). While not as efﬁcient as binary, 2-bit arithmetic can also be performed without multiplications, making it a good efﬁcient alternative in applications where the performance cost of going to fully binary is signiﬁcant. 5.1.1 Data augmentation From Table 1, it can be observed that the datasets with small training sets still have a large gap from the full-precision baseline. As a result, we employ data augmentation heuristics (following the exact setup in Zhang et al. (2020)) on the datasets with small training sets (all except MNLI, QNLI) to take better advantage of our model’s strong representational capability. This further reduces the quantization gap, with our models eventually trailing the full-precision model by only 5.9 points on average on the GLUE benchmark. 7Table 2: Ablation study on the effects of each component on GLUE dataset without data augmentation. Quant MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg. 1 BERT base 84.9/85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 2 BiBERT Baseline 45.8/47.0 73.2 66.4 77.6 11.7 7.6 70.2 54.1 50.4 3 BiBERT 66.1/67.5 84.8 72.6 88.7 25.4 33.6 72.5 57.4 63.2 4 BinaryBERT (Our implementation) 36.2/35.9 59.6 52.4 65.6 9.3 19.8 69.9 52.7 45.7 5 + Our simplied KD 37.7/37.3 59.5 56.8 73.4 4.1 24.8 70.8 57.0 48.0 6 + Our two-set binarization (Strong Baseline) 57.4/59.1 68.3 64.7 81.0 18.2 24.7 71.8 56.7 55.3 7 + Elastic binarization ( BiT ‡) 77.1/77.5 82.9 85.7 87.7 25.1 71.1 79.7 58.8 71.0 8 + Multi-Distillation ( BiT) 79.5/79.4 85.4 86.4 89.9 32.9 72.0 79.9 62.1 73.5 5.2 SQuAD results Table 3: Comparison of BERT quantization meth- ods on SQuADv1.1 dev set. Metrics are exact match and F1 score. Quant #Bits SQuADv1.1EM/F1 BERTbase 32-32-32 82.6/89.7 BinaryBERT 1-1-4 77.9/85.8BinaryBERT 1-1-2 72.3/81.8BinaryBERT 1-1-1 1.5/8.2BiBERT 1-1-1 8.5/18.9 BiT 1-1-1 63.1/74.9 We also evaluate on the popular machine reading comprehension (MRC) dataset from Rajpurkar et al. (2016). We compare to our own implemen- tation of the MRC task on top of the BiBERT codebase, since SQuAD results are not reported in that work. We also show results using the BinaryBERT codebase, without using weight splitting. The results (Table 3) show that this task is signiﬁcantly harder than most document classiﬁcation benchmarks in GLUE, and previ- ous binarization methods fail to achieve any meaningful level of performance. BiT does much better, but still trails the 32-bit baseline by 14.8 points in F1. We conclude that despite the improvements we have demonstrated on the GLUE benchmark, binarizing transformer models accurately is far from a solved problem in general. 5.3 Ablations We start from the basic binarization implementation from Bai et al. (2021), and add each of our contributions in sequence to get a better idea how each contributes to the performance. The results are shown in Table 2. We start by removing attention distillation (Section 3.2), which results in a 2.3% improvement (row 5 vs. 4). Then switching to our two-set binarization (Section 3.1), which binarizes the attention scores differently than the feed-forward activations, which gives an additional 7.3% boost (row 6). This results in a much stronger baseline than what was used in prior works (row 2). Moving from ﬁxed to elastic binarization (Section 3.3) proves hugely important, pushing the average accuracy to 71.0% (row 7) from only 55.3% (row 6). Note that this model already outperforms the current state-of-the-art (row 3) by 7.8% points. Finally, we add multi-step distillation (Section 4), which adds another 2.5 points, reaching the ﬁnal accuracy of 73.5% on the GLUE benchmark. 5.4 Learned parameter visualization Figure 2: The optimized scaling factor in BiT We visualize the optimized αin the ﬁ- nal BiT model. As we can see from Figure 2, the values of the αparameters vary signiﬁcantly from layer to layer, and have apparent patterns according to layer characteristics. For example, the attention layers need to distribute the at- tention to different entries, thus the scal- ing factor for the attentions are learned to be small, while the scaling factors for the query and key outputs are usually larger. Note that the biggest αvalue is 200×of the smallest α, suggesting the importance of learning αdynamically. 5.5 Exploring multi-distillation paths So far we have only considered the ﬁxed quantization schedule, W32A32 →W1A2 →W1A1. This is motivated by early experiments showing that one-step distillation to W1A2 works reasonably well. 8We explored other optimal schedules, such as distilling to W1A8 resulted in a higher accuracy model, thus a better teacher to distill down to the eventual W1A1 student. This suggests a trade-off between the quality of the intermediate model, vs. the closeness to the target quantization level. 32 8 4 2 1 Activation bits 76 78 80 82 84MNLI-m (matched) accuracy W32A32->W1A8->W1A1 W32A32->W1A4->W1A1 W32A32->W1A2->W1A1 W32A32->W1A1 W32A32->W1A4->W1A2->W1A1 Figure 3: MNLI-m accuracy on various distilla- tion paths. Each curve represents the sequence of models on a particular quantization schedule. Figure 3, illustrates this trade-off. We can see that two-step distillation improves over one-step in every case. While higher precision interme- diate models are better as expected, it is better to use a lower precision teacher in the last step since it makes the learning task for the binary student model easier. The closeness to the target quantization is favored despite the lower accu- racy of teacher model. It is of course possible, though more cumber- some, to perform more than two distillation steps. We also experiment with a three-step schedule, W32A32 →W1A4 →W1A2 → W1A1, which is plotted in the same ﬁgure (dashed line). We ﬁnd that this particular 3- step schedule does not improve over the 2-step schedule W32A32 →W1A2 →W1A1. While this result does not preclude existence of other more optimal schedules, we hypothesize that this is unlikely. 6 Related work Convolutional neural network quantization Neural network quantization is a practical tool for compressing model size and reduce storage (Hubara et al., 2017). Quantization for convolutional neural networks has been studied both in the uniform quantization (Choi et al., 2018; Zhou et al., 2016; Gong et al., 2019) and non-uniform quantization (Zhang et al., 2018; Miyashita et al., 2016; Li et al., 2020) settings. The quantization level has been progressively increased, from 8-bit (Wang et al., 2018; Zhu et al., 2020) to 4-bit (Jung et al., 2019; Liu et al., 2022) and ﬁnally to the extreme 1-bit case (Courbariaux et al., 2016; Rastegari et al., 2016; Liu et al., 2018; Martinez et al., 2020). Transformer quantization Compared to the CNNs, transformers with attention layers are naturally more challenging to quantize (Bondarenko et al., 2021). Previous research mainly focused on 8-bit quantization (Zafrir et al., 2019; Fan et al., 2020) or 4-bit quantization (Shen et al., 2020; Zadeh et al., 2020). Extremely low-bit quantization for transformers has only been attempted very recently. TernaryBERT (Zhang et al., 2020) proposed to ternarize the full-precision weights of a ﬁne-tuned BERT model. As a follow-up to TernaryBERT, weight binarization was proposed in Bai et al. (2021). Here, the network is trained by ﬁrst training a ternary half-sized model, which is used as initialization. Then a weight-splitting step results in a full-sized binarized model, which is further ﬁne-tuned in a subsequent distillation step. Binarizing both weight and activations in a transformer has proved to be challenging. BiBERT (Qin et al., 2021) made the ﬁrst attempt in this direction with limited success. Their model performed 20% worse than a real-valued baseline on the GLUE benchmark (Wang et al., 2019), which even underperforms the original LSTM baselines. 7 Conclusion Large pre-trained transformers have transformed NLP and are positioned to serve as the backbone for all AI models. In this work, we presented the ﬁrst successful demonstration of a fully binary pre-trained transformer model. While our approach is general and can be applied to any transformer, we have limited our evaluation to BERT-based models on the GLUE and SQuAD benchmarks. It remains to be seen how our conclusions will hold when applied to the wide variety of pre-trained transformer models which have gained popularity in recent years, from small mobile models, to gigantic ones with hundreds of billions of parameters. It will also be interesting to see the performance of the approach on different domains (such as image and speech processing) and tasks (such as text and image generation). Demonstrating the generality of this approach in a wider setting should signiﬁcantly widen its impact, therefore we identify this as an important future direction. 9References Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, and Chenlei Guo. Knowledge distillation from internal representations. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 7350–7357, 2020. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. In ACL/IJCNLP (1), 2021. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing textual entailment challenge. In TAC, 2009. Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efﬁcient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7947–7969, 2021. John Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. Advances in neural information processing systems, 2, 1989. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017. Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs. University of Waterloo, pp. 1–7, 2018. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, et al. Pact: Parameterized clipping activation for quantized neural networks. arXiv e-prints, pp. arXiv–1805, 2018. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), 2019. Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S Modha. Learned step size quantization. In International Conference on Learning Representations, 2019. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020. Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efﬁcient neural network inference. arXiv preprint arXiv:2103.13630, 2021. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4852–4861, 2019. 10Jun Han and Claudio Moraga. The inﬂuence of the sigmoid function parameters on the speed of backpropagation learning. In International workshop on artiﬁcial neural networks, pp. 195–201. Springer, 1995. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations.The Journal of Machine Learning Research, 18(1):6869–6898, 2017. Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4350–4359, 2019. Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, and Nojun Kwak. Qkd: Quantization-aware knowledge distillation. arXiv preprint arXiv:1911.12491, 2019. Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efﬁcient non-uniform discretization for neural networks. In International Conference on Learning Representations, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In Proceedings of the European conference on computer vision (ECCV), pp. 722–737, 2018. Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions. In European Conference on Computer Vision, pp. 143–159. Springer, 2020. Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform-to- uniform quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4942–4952, 2022. Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with real-to-binary convolutions. In ICLR, 2020. Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016. Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Icml, 2010. Eriko Nurvitadhi, David Shefﬁeld, Jaewoong Sim, Asit Mishra, Ganesh Venkatesh, and Debbie Marr. Accelerating binarized neural networks: Comparison of fpga, cpu, gpu, and asic. In 2016 International Conference on Field-Programmable Technology (FPT), pp. 77–84. IEEE, 2016. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In CVPR, 2020. Haotong Qin, Yifu Ding, Mingyuan Zhang, Y AN Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xianglong Liu. Bibert: Accurate fully binarized bert. In International Conference on Learning Representations, 2021. 11Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. In European conference on computer vision, pp. 525–542. Springer, 2016. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 8815–8821, 2020. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ﬂoating point numbers. Advances in neural information processing systems, 31, 2018. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Cola: The corpus of linguistic acceptability (with added annotations). 2019. Adina Williams, Nikita Nangia, and Samuel R Bowman. The multi-genre nli corpus. 2018. Yifan Yang, Qijing Huang, Bichen Wu, Tianjun Zhang, Liang Ma, Giulio Gambardella, Michaela Blott, Luciano Lavagno, Kees Vissers, John Wawrzynek, et al. Synetgy: Algorithm-hardware co-design for convnet accelerators on embedded fpgas. In Proceedings of the 2019 ACM/SIGDA international symposium on ﬁeld-programmable gate arrays, pp. 23–32, 2019. Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing attention-based nlp models for low latency and energy efﬁcient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 811–824. IEEE, 2020. Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 36–39. IEEE, 2019. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pp. 365–382, 2018. Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit BERT. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), EMNLP, 2020. 12Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards uniﬁed int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1969–1979, 2020. Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective low- bitwidth convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7920–7928, 2018. A Appendix A.1 BiT vs. progressive distillation Table 4: BiT vs. progressive distillation on selected GLUE tasks. Methods differ in the teacher model used and the model from which the student weights are initialized. Method Teacher Initialization MNLI-m/mmQQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.BiBERT Distillation 32-32-32 32-32-32 77.0/77.2 83.1 84.1 89.7 31.3 60.1 75.5 56.7 69.7Progressive 32-32-32 1-1-2 78.9/78.9 85.0 86.4 89.6 30.5 75.1 81.1 60.6 73.4BiT 1-1-2 1-1-2 79.5/79.4 85.4 86.4 89.9 32.9 72.0 79.9 62.1 73.5 Previous work has also recognized the importance of good initialization for binary model training, and proposed to perform distillation while progressively quantizing the student model (Zhuang et al., 2018; Yang et al., 2019). Progressive distillation ensures a good initialization for the student model at each step. However, in this approach the teacher model is ﬁxed to the full precision model, which does not address the problem of teacher-student gap. In Table 4 we compare BiT to a comparable implementation of progressive distillation, using the same quantization schedule, W32A32 →W1A2 →W1A1, as ours. We keep the teacher model ﬁxed, while re-initializing the student model from the latest quantized version at each step. We see that using a quantized teacher model is helpful, especially in the high-data regime. However, our method can lag behind progressive distillation for small datasets such as STS-B and MRPC. A.2 Elastic binarization function vs. ReActNet learnable bias Table 5: Elastic binarization function vs. ReActNet (Liu et al., 2020) learnable bias on GLUE tasks. Method MNLI -m/mmQQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.Our two-set binarization (Strong Baseline) 57.4/59.1 68.3 64.7 81.0 18.2 24.7 71.8 56.7 55.3+ learnable scale 76.5/76.8 82.7 85.1 88.1 26.6 62.3 74.3 58.1 69.2+ learnable scale and bias (BiT‡) 77.1/77.5 82.9 85.7 87.7 25.1 71.1 79.7 58.8 71.0 Inspired by the learnable bias proposed in ReActNet (Liu et al., 2020), we further propose elastic binarization function to learn both learnable scaling factors and learnable bias. We ﬁnd this learnable scaling factor critical for the ﬁnal performance. As shown in table 5, the proposed learnable scaling factor brings 13.9% accuracy improvement, and further adding learnable bias boosts the accuracy by 1.8%. A.3 Two-set binarization scheme vs. Bi-Attention Table 6: Two-set binarization scheme vs. Bi-Attention (Qin et al., 2021) on GLUE tasks. Methods differ in whether using SoftMax in attention and whether binarizing the ReLU output to {0 ,1}. Method Attention ReLU output MNLI -m/mmQQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.Bi-Attention (w/o Softmax) {0, 1} {-1, 1} 48.1/50.0 60.1 60.6 78.8 14.0 22.3 68.4 58.1 51.3Binarize attention to {0, 1} (w/ Softmax) {0, 1} {-1, 1} 51.9/52.6 76.2 60.5 79.6 11.6 18.1 70.6 55.6 53.0Two-set binarization scheme {0, 1} {0, 1} 57.4/59.1 68.3 64.7 81.0 18.2 24.7 71.8 56.7 55.3 In contrast to Bi-Attention proposed in BiBERT (Qin et al., 2021) that removesSoftMax and binarizes the attention to {0, 1} withbool function, our two-set binarization scheme ﬁnds that keepingSoftMax 13in attention computation and also binarizing the positive output of ReLU layer to {0, 1} works better. We conduct meticulous experiments to compare these choices. In Table 6, we show that, compared to removing SoftMax as Bi-Attention suggested, simply binarizing the activations after SoftMax layer to {0, 1} even produces 1.7% better accuracy. Furthermore, binarizing the ReLU layer output to {0, 1} instead of {-1, 1} helps the binary network match real-valued distributions and further brings 2.3% accuracy improvement. A.4 Binary convolution implementation for two-set binarization scheme The binary convolution between the weights and activations that are both binarized to {-1, 1} (i.e. AB ∈{-1, 1}, WB ∈{-1, 1}) can be implemented by the bitwise xnor operation followed by a popcnt operation (Rastegari et al., 2016; Liu et al., 2018): AB ·WB = popcnt(xnor(AB,WB)) (12) For the case where activations are binarized to {0, 1} in two-set binarization scheme, the binary activation AB ∈{0, 1} can be represented with A′ B ∈{-1, 1} through a simple linear mapping: AB = A′ B+1 2 . Thus the matrix computation between binary weights ( WB ∈{-1, 1} ) and binary activations (AB ∈{0, 1}) can be converted to the operations between WB ∈{-1, 1} and A′ B ∈{-1, 1} as: AB ·WB = (A′ B + 1 2 ) ·WB = 1 2(popcnt(xnor(A′ B,WB)) + ∑ i WBi) (13) Here the ∑ iWBi is summing up the values in WB, which can be pre-computed and stored as bias. Thus in the two-set binarization scheme where activations are binarized to {0, 1}, the binary convolution can still be implemented with the general binary convolution in E.q. 12 at no additional complexity cost. A.5 Evaluation benchmarks A.5.1 GLUE The GLUE benchmark (Wang et al., 2019) includes the following datasets: MNLI Multi-Genre Natural Language Inference is an entailment classiﬁcation task (Williams et al., 2018). The goal is to predict whether a given sentence entails, contradicts, or is neutral with respect to another. QQP Quora Question Pairs is a paraphrase detection task. The goal is to classify whether two given questions have the same meaning. The questions were sourced from the Quora question answering website (Chen et al., 2018). QNLI Question Natural Language Inference (Wang et al., 2019) is a binary classiﬁcation task which is derived from the Stanford Question Answering Dataset (Rajpurkar et al., 2016). The task is to predict whether a sentence contains the answer to a given question. SST-2 The Stanford Sentiment Treebank is a binary sentiment classiﬁcation task, with content taken from movie reviews (Socher et al., 2013). CoLA The Corpus of Linguistic Acceptability is a corpus of English sentences, each with a binary label denoting whether the sentence is linguistically acceptable (Warstadt et al., 2019). STS-B The Semantic Textual Similarity Benchmark is a sentence pair classiﬁcation task. The goal is to predict how similar the two sentences are in meaning, with scores ranging from 1 to 5 (Cer et al., 2017). MRPC Microsoft Research Paraphrase Corpus is another sentence pair paraphrase detection task similar to QQP. The sentence pairs are sourced from online news sources (Dolan & Brockett, 2005). 14RTE Recognizing Textual Entailment is a small natural language inference dataset similar to MNLI in content (Bentivogli et al., 2009). A.5.2 SQuAD The SQuAD benchmark (Rajpurkar et al., 2016), i.e., Stanford Question Answering Dataset, is a reading comprehension dataset, consisting of questions on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding passage, or the question might be unanswerable. A.6 Technical details For each experiment, we sweep the learning rate in {1e-4, 2e-4, 5e-4} and the batch size in {8, 16} for QNLI, SST-2, CoLA, STS-B, MRPC, RTE, and {16, 32} for MNLI, QQP as well as SQuAD, and choose the settings with the highest accuracy on the validation set. We use the same number of training epochs as BiBERT (Qin et al., 2021), i.e., 50 for CoLA, 20 for MRPC, STS-B and RTE, 10 for SST-2 and QNLI, 5 for MNLI and QQP. We adopt the Adam optimizer with weight decay 0.01 and use 0.1 warmup ratio with linear learning rate decay. Our full precision checkpoints are taken from https://textattack.readthedocs.io/en/ latest/3recipes/models.html#bert-base-uncased. 15",
      "meta_data": {
        "arxiv_id": "2205.13016v2",
        "authors": [
          "Zechun Liu",
          "Barlas Oguz",
          "Aasish Pappu",
          "Lin Xiao",
          "Scott Yih",
          "Meng Li",
          "Raghuraman Krishnamoorthi",
          "Yashar Mehdad"
        ],
        "published_date": "2022-05-25T19:01:54Z",
        "pdf_url": "https://arxiv.org/pdf/2205.13016v2.pdf"
      }
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks",
      "abstract": "Training with larger number of parameters while keeping fast iterations is an\nincreasingly adopted strategy and trend for developing better performing Deep\nNeural Network (DNN) models. This necessitates increased memory footprint and\ncomputational requirements for training. Here we introduce a novel methodology\nfor training deep neural networks using 8-bit floating point (FP8) numbers.\nReduced bit precision allows for a larger effective memory and increased\ncomputational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We\nshow that, unlike previous 8-bit precision training methods, the proposed\nmethod works out-of-the-box for representative models: ResNet-50, Transformer\nand NCF. The method can maintain model accuracy without requiring fine-tuning\nloss scaling parameters or keeping certain layers in single precision. We\nintroduce two learnable statistics of the DNN tensors - shifted and squeezed\nfactors that are used to optimally adjust the range of the tensors in 8-bits,\nthus minimizing the loss in information due to quantization.",
      "full_text": "Published as a conference paper at ICLR 2020 SHIFTED AND SQUEEZED 8-BIT FLOATING POINT FOR - MAT FOR LOW-PRECISION TRAINING OF DEEP NEU- RAL NETWORKS L´eopold Cambier1∗†, Anahita Bhiwandiwalla2†, Ting Gong2, Mehran Nekuii2, Oguz H Elibol2 and Hanlin Tang2 1ICME, Stanford University 2Intel AI Lab lcambier@stanford.edu {anahita.bhiwandiwalla,ting.gong}@intel.com {mehran.nekuii,oguz.h.elibol,hanlin.tang}@intel.com ABSTRACT Training with larger number of parameters while keeping fast iterations is an in- creasingly adopted strategy and trend for developing better performing Deep Neu- ral Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit ﬂoating point (FP8) numbers. Re- duced bit precision allows for a larger effective memory and increased computa- tional speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out-of-the-box for representative models: ResNet-50, Transformer and NCF. The method can maintain model accuracy without requiring ﬁne-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learn- able statistics of the DNN tensors - shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization. 1 I NTRODUCTION Deep neural networks have achieved state-of-the-art performance on a wide variety of computer vision, audio, and natural language processing (NLP) tasks. This has resulted in an explosion of in- terest around techniques to reduce the memory footprint and energy consumption of neural network training and inference (Guo, 2018). Although there are a number of methods to address some of these issues for inference, the most effective method for training is using reduced precision numeri- cal formats. While 32-bit ﬂoating point (FP32) is the most common data format for neural network training, recent hardware have leveraged techniques that allow for training with 16-bit data formats (K ¨oster et al., 2017; Micikevicius et al., 2018). However, 8-bit precision training remains an open challenge (Johnson, 2018; Kalamkar et al., 2019). Current FP8 training methodologies (Wang et al., 2018; Mellempudi et al., 2019) require either specialized chunk-based accumulation, stochastic rounding techniques, loss scaling or maintaining some layers of the network in higher precision. Tuning these knobs is non-intuitive and requires signiﬁcant experimentation for each individual network. Accelerating the adoption of 8-bit data in training DNNs requires a hardware-friendly and out-of- the-box implementation of FP8. Due to the reduced number of mantissa bits, 8-bit multipliers are smaller and consume less power compared to higher bit representations. In this work we describe a novel 8-bit ﬂoating point (FP8) format - shifted and squeezed FP8 (S2FP8) - which has the following advantages compared to previously proposed 8-bit training methodologies: ∗Work performed during an internship at Intel †Equal contribution 1 arXiv:2001.05674v1  [cs.LG]  16 Jan 2020Published as a conference paper at ICLR 2020 •S2FP8 eliminates the need for loss scaling, which requires signiﬁcant tuning of the loss scale values and schedule for individual topologies •Leveraged by the forward and backward passes of model training, S2FP8 is effective in adjusting the range of gradients and also of activations and weights •S2FP8 does not require keeping the ﬁrst and last layer in FP32 precision, which is needed for other approaches (Mellempudi et al., 2019), however maintains the master weights and accumulations inside the matrix multipliers in FP32 We demonstrate across image classiﬁcation, translation, and recommendation models that S2FP8 outperforms previous 8-bit approaches, and reaches the accuracy of FP32 models without any addi- tional hyperparameter tuning. 2 R ELATED WORK The success of 32-bit ﬂoating point data type in training deep neural networks has increased interest in the feasibility of even lower precision training. The exponential demand for compute involved in training these deep neural networks has lead to multiple advancements in lower precision data types. Several studies have developed techniques such as loss scaling, stochastic rounding, and others to train effectively in 16-bit (Micikevicius et al., 2018; Das et al., 2018; Azim), along with associated hardware support (Markidis et al., 2018). Using 16-bit ﬁxed point, (Gupta et al., 2015) showed that stochastic rounding techniques were crucial for model convergence even for simple convolutional neural networks. As noted in (Kalamkar et al., 2019), Google’s bﬂoat16 format has the same number of exponent bits as FP32, leading the success of that format without commonly requiring hardware intensive requirements such as stochastic rounding or other framework level techniques such as loss scaling. Although 8-bit formats have signiﬁcant performance and memory advantages, convergence is es- pecially challenging due to loss of accuracy in the backpropogated gradient values. Wang et al. (2018) demonstrated training models with matrix multiplications and convolutions in FP8 but they use FP16 with chunk-based accumulations and stochastic rounding hardware. Mellempudi et al. (2019) also demonstrated success with FP8, accumulating in FP32 and using loss scaling techniques on ResNets, Transformer and GNMT networks. However, they too require the ﬁrst and last layers of the model to be in FP32, and similar to (Banner et al., 2018) leverage Stochastic Rounding tech- niques to maintain model accuracy. Unlike S2FP8 proposed in this work, both of these FP8 training techniques emphasize the need for efﬁcient loss scaling, rounding hardware and restriction on some layers being in higher precision. Zhou et al. (2016) quantized weights, activations and gradients of AlexNet (Krizhevsky et al., 2012) to 1, 2 and 6 bits respectively. But they also need to maintain the ﬁrst and last convolution layers in full precision and stochastically quantize the gradients. Wu et al. (2018) demonstrate using integers for training LeNet-5 (LeCun et al., 1998) and AlexNet with 8-bits for activations, error and gradi- ents and 2-bits for weights. However, these approaches also required custom tuning such as novel initialization techniques and layer wise scaling instead of Batch Normalization and Softmax. These approaches lack generalizability to other models, requiring signiﬁcant ﬁne tuning. To the best of our knowledge, there does not exist an out-of-the-box solution using FP8 in training deep learning topologies without the need for tuned loss scaling techniques, requirements of cer- tain layers being in full precision along with efﬁcient hardware rounding schemes like Stochastic Rounding. 3 S HIFTED AND SQUEEZED 8-BIT FLOATING POINT FORMAT 3.1 C HALLENGES OF 8-BIT FLOATING POINT FORMAT The FP8 format, with 2 bits of mantissa and 5 bits of exponent (Mellempudi et al., 2019) is both nar- row (i.e., its dynamic range is very limited, from 2−16 to 216) and has lower accuracy (the machine epsilon is only 2−3). Figure A1 illustrates the range and accuracy of FP8. In contrast, FP32 ranges from 2−149 to 2128 with a machine-epsilon of 2−24 (Table A1). 2Published as a conference paper at ICLR 2020 Figure 1: The distribution of tensor elements over the course of training for three tensors from the Transformer tiny model on the English-Vietnamese translation dataset. Blue bar indicates the representable range of FP8. Left: Many of the tensor elements fall outside of FP8’s representable range. Center: Few tensor elements fall outside of FP8’s representable range. Right: Initially, most elements are within FP8’s representable range, but after training, many fall outside of the representable range On the other hand, tensors involved in neural networks (weights, activations and gradients) are spread across varying scales. As illustrated in Figure 1, the tensor distributions change over the course of training, spanning different orders of magnitude. As a result, 8-bit training usually requires a combination of multiple techniques to capture the full dynamic range of values for model training. Some of these techniques include: • Loss scaling (Micikevicius et al., 2018) scales the loss L(w) by a constant λbefore back- propagation . This makes the gradients artiﬁcially larger, allowing them to ﬁt within the FP8 range. Gradients are then scaled down before being accumulated into the trainable weights as shown in Equation 6 • Stochastic rounding (Maxﬁeld, 2006) alleviate quantization errors by capturing some of the information discarded when truncating to lower precision at the output of a GEMM operation Between these two techniques, loss scaling is more critical; once the magnitude of the gradients can no longer be represented in the FP8 range, training convergence will not be possible. However, loss scaling only modiﬁes the gradients. Weights and activations can also (albeit admittedly less frequently) exceed the FP8’s representable range of[2−16,216]. In those scenarios, convergence can also be affected. The issue with loss scaling is that it requires user interaction. Models have to be modiﬁed, and, more importantly, tedious empirical tuning is required to ﬁnd the correct loss scaling schedule. While some networks can be trained with constant loss scaling, some, notably Transformers (Mellempudi et al., 2019), require dynamic “back-off” and improved loss scaling. This requires signiﬁcant trial and error to tune the scaling schedule, slowing down wide adoption of low-precision numerical formats. 3.2 S HIFTED AND SQUEEZED FP8 To alleviate these issues and make neural network training possible with no model modiﬁcations or hyperparameter tuning, we propose a new 8-bit ﬂoating point format. Consider a tensor X of size N, i.e., X = {Xi}N i=1. Instead of directly encoding each Xi in FP8, we store X using N FP8 numbers {Yi}N i=1 accompanied by two (squeeze and shift) factors αand β (the “statistics” — see Figure 2). Figure 2: The S2FP8 format. A tensor Xof N numbers is represented by α, βand N FP8 numbers Y, related to X through Equation 1. 3Published as a conference paper at ICLR 2020 -16 0 16 log2 |Y| (a) Y, the usual FP8 distribution. 0 32 log2 |X| (b) X, for α= 1and β <0 -32 0 32 log2 |X| (c) X, for α< 1 and β = 0 Figure 3: Impact of the Shifted and Squeezed transformation log2 |Y|= αlog2 |X|+ β. αlet the distribution be as wide as necessary (though, with an associated loss of precision), and βlet us shift the distribution around any value. For Xi ̸= 0, X and Y are then related through log2(|Yi|) = αlog2(|Xi|) + β ⇔Yi = ±2β|Xi|α (1) where the ±is chosen so that Xi and Yi have the same sign. This representation allows for αand βbe chosen so that together with tensor Y they capture most of the dynamic range of the tensor X. As we will see in section 4, this is all that is necessary to train networks using 8-bit ﬂoating point numbers. In order for Y to be a tensor suitable to be represented by FP8 numbers, we enforce that it has zero mean and a maximum value within the dynamic range of FP8 (e.g. 15): N′ ∑ i=1 log2(|Yi|) = 0 and max i=1,...,N′ log2(|Yi|) = 15(= log2(215)) (2) where the ′notation indicates that the sum and the max, respectively, ignore any isuch that Yi = 0. Those equations ensure that log2(|Y|) values are distributed with zero mean and each is less than 15, which is ideal for an FP8 format. By inserting Equation 2 into Equation 1, and by denoting µ= N′ ∑ i=1 log2(|Xi|) and m= max i log2(|Xi|) (3) we ﬁnd α= 15 m−µ, β = −αµ (4) This new tensor format results in the training procedure (forward pass, backward pass, weight up- date) described in Figure 4. Forward and backward MatMul use this new S2FP8 format. Master weights are kept in FP32 and updated using S2FP8 gradients. Accumulations inside the GEMM kernel are kept in full FP32 precision. Figure 3 illustrates the impact of αand β. By having those two extra degrees of freedom for each tensor, majority of the dynamic range of each tensor can now be captured, whether very small ( β >0), very large ( β <1), very narrow ( α >1)) or very wide (α< 1). 3.3 L EARNING THE TENSOR DISTRIBUTION One way to interpret αand βis to consider them as parameters of a distribution generating the ten- sor values log2(|Xi|). We can then say that, by continuously computing αand β, we are effectively learning the distribution of log2(|Xi|). Figure 5c shows the evolution of µ, m, αand βfor a partic- ular tensor of ResNet-20. We see that αand β converge to, approximately, 5 and 21, respectively. From Equation 1, we conclude that: 4Published as a conference paper at ICLR 2020 FP32àS2FP8 T T T Master weights layer ℓ (FP32) Weights  gradients layer ℓ (S2FP8) Loss gradients layer ℓ (S2FP8) Activations  layer ℓ (S2FP8 ) Activations layer ℓ+1 (S2FP8) Loss gradients layer ℓ+1 (S2FP8) ⨉Σ ⨉Σ Σ⨉ T Update FWD GEMM WG GEMM FP32àS2FP8 FP32àS2FP8 FP32àS2FP8FP32 FP32 FP32 BWD GEMM Figure 4: Low precision training with S2FP8. T represent the truncation described in Equation 5, from FP32 to S2FP8. When using S2FP8 for training, forward and backward GEMM’s only use S2FP8. The master weights are kept in FP32 and updated during the update step. • since α> 1, this means that X is expanded into Y, i.e., X is more narrow than what FP8 allows •since β >0, this means that X is right-shifted into Y, i.e., X is smaller than what FP8 allows At convergence, thoseαand βvalues represent the distribution of each converged tensor. Notice that all statistics stabilize in the last third of the training, where the learning rate is decreased, indicating the network is converging to its ﬁnal state. 4 E XPERIMENTAL RESULTS In this section, we compare S2FP8 training with baseline FP32 and FP8 training with and with- out loss scaling for: Residual Networks (He et al., 2016) of varying depths on the CIFAR-10 and ImageNet (Deng et al., 2009) datasets, Transformer (Vaswani et al., 2017) on IWSLT’15 English- Vietnamese dataset (Luong & Manning, 2015), and Neural Collaborative Filtering (NCF) (He et al., 2017) on MovieLens 1 Million dataset (Harper & Konstan, 2016). For our experiments, we use the open source Tensorﬂow Models 1 repository for ResNet and NCF, Tensor2Tensor (Vaswani et al., 2018) for Transformer with added S2FP8 data type simulation sup- port using the methodology described in subsection 4.1. For a given model, we keep the hyperpa- rameters consistent across FP32, FP8 and S2FP8 evaluations. 4.1 S IMULATION METHODOLOGY We simulated S2FP8 by inserting appropriate truncation function throughout the network, before and after every convolution and matrix-matrix product operations, during both the forward and backward passes. The rest of the network is kept in FP32, and those truncation simulate the low-precision training described in subsection 3.2. The truncation function takes as input a tensor X, computes its magnitude mean and maximum, computes the appropriate αand βand ﬁnally truncates X by computing Xtruncated = [ 2−β{ truncateFP8 ( 2β|X|α)}]1/α (5) where truncateFP8 is a usual FP8 truncation function with RNE (round-to-nearest, with ties broken by rounding to even) rounding which is easier to implement and most widely supported in hardware. 1https://github.com/tensorflow/models 5Published as a conference paper at ICLR 2020 (a) Distribution of the magnitude log2(|X|) of original tensor Xbefore scaling using αand β (b) Distribution of the magnitude log2(|Y|) of shifted and squeezed tensor Y with |Yi| = 2β|Xi|α 0 50k 100k −4.6 −4.4 −4.2 −4 −3.8 Step µ 0 50k 100k −3 −2 −1 Step m 0 50k 100k 4 6 8 Step α 0 50k 100k 20 30 40 Step β (c) The computed statistics during training for the scale (β), shift (α), as well as the mean of the log values (µ) and the maximum log value (m). Figure 5: Evolution of the average and maximum magnitude, as well asαand βfor CIFAR-10 with ResNet-20. This illustrates how the network is actually implicitly learning the tensors distribution, by repeatedly computing magnitudes αand βthrough µand m. 4.2 R ESIDUAL NETWORKS We ﬁrst present results with Residual Networks of varying depths on the CIFAR-10 image recogni- tion dataset. We trained the model on 1 GPU using standard parameters: 250 epochs, batchsize of 128, SGD with momentum of 0.9, initial learning rate of 0.1 decreased by a factor of 10 after epochs 100, 150 and 200. Table 1 and Figure A2 presents the results. We observe that S2FP8 reaches almost exactly the FP32 baseline, sometimes even improving over it. Out-of-the-box FP8 does not converge and has very poor accuracy. Finally, FP8 with constant loss scaling of 100 (FP8+LS(100)) can reach the baseline. Both S2FP8 and FP8+LS(100) have similar performances, but S2FP8 can do so without any extra hyperparameters or tuning from the user’s perspective. CIFAR-10 FP32 S2FP8 ∆ FP8 FP8+LS(100) ResNet-20 91.5 91.1 0.4 17.9 91.1 ResNet-34 92.5 92.0 0.5 13.5 92.0 ResNet-50 93.0 93.2 -0.2 11.5 92.9 Table 1: Validation accuracy (in %) for image recognition on CIFAR-10 with ResNet-20/34/50. We also evaluate S2FP8 on the 1000 class ImageNet dataset. Here, we trained the network on 4 GPUs using standard parameters: 90 epochs, batchsize of 256, SGD with momentum of 0.9, initial learning rate of 0.1 decreased by a factor of 10 after epochs 30, 60, 80 and 90. Table 2 and Figure 6 present the results. Again, we observe that S2FP8 gets very close to the FP32 baseline. Out-of-the-box FP8 quickly diverges and does not converge at all. For FP8 with loss scaling to converge, one has to not truncate the ﬁrst and last layer, as consistent with (Mellempudi et al., 2019), which we denote as Ex in Table 2 below. A loss scaling of 10,000 can then be used to reach the baseline (FP8+LS(10k)+Ex). Finally, stochastic rounding can be added and it slightly improves the precision (FP8+LS(100k)+Ex+SR). However, both those cases are not out-of-the-box, as they require loss scaling tuning and some layers 6Published as a conference paper at ICLR 2020 to be kept in full precision. S2FP8 does not suffer from that, thanks to its improved quantization: all layers can be truncated and no loss scaling is required. Imagenet1k FP32 S2FP8 ∆ FP8 FP8+LS(10k)+Ex FP8+LS(100k)+Ex+SR ResNet-18 70.3 69.6 -0.7 NaN 68.7 68.9 ResNet-50 76.2 75.2 -1.0 NaN 75.3 75.5 Table 2: Validation accuracy (in %) for image recognition on Imagenet1k with ResNet-18/50 0 250k 500k 20 40 60 80 Step Top-1 accuracy (%) FP32 S2FP8 0 250k 500k 2 4 6 8 Step Loss FP32 S2FP8 0 250k 500k 0.4 0.6 0.8 1 1.2 Step L2 Loss FP32 S2FP8 Figure 6: Comparing Top-1 accuracy and Loss of S2FP8 with FP32 for ResNet-50 on Imagenet1k 4.3 T RANSFORMER We also tested S2FP8 on a small Transformer (Transformer Tiny) on the English-Vietnamese dataset. The model has 2 hidden layers of size 128, and a ﬁlter of size 512, and is trained using Adam optimizer (Kingma & Ba, 2014). Table 3 and Figure 7 show the result, where we compare FP32, S2FP8 and FP8 with exponential loss scaling. We tried many loss scaling schedules (constant and exponential, with various initializations) and report the best result. As one can see, S2FP8 reaches the baseline with no hyperparameter tuning. FP8, on the other hand, does not, even after extensive loss scaling tuning. This shows the value of an out-of-the-box method for the user. En-Vi FP32 S2FP8 ∆ FP8 FP8+LS(exp) Transformer tiny 25.3 25.3 0.0 NaN 21.3 Table 3: BLEU Score (Papineni et al., 2002) (from 0 to 100) for translation task on the English- Vietnamese dataset with Transformer tiny. 4.4 N EURAL COLLABORATIVE FILTERING The Neural Collaborative Filtering (NCF) network comprises of embeddings for users and items from the MovieLens dataset, that are then passed to a Multi-Layer Perceptron(MLP) network to learn the user-item interaction. Matrix-multiplication operations are the building blocks of such models. We compare S2FP8 with FP32 and FP8 without loss scaling. We simulate Matrix-Multiplications and look-ups from the embeddings in S2FP8 and compare it to FP8 with RNE. We trained the model on the MovieLens 1 Million dataset with the following standard paramaters: 20 iterations, batchsize of 1024 on 4 GPUs, 8 predictive factors, learning rate of 0.0005 using the Adam optimizer. Figure 8 and Table 4 show the result, where we compare FP32, S2FP8 and FP8 without loss scaling. This again shows that S2FP8 easily reaches the baseline out-of-the-box, without tuning of any sort. FP8 gets relatively close, but cannot reach the baseline. 7Published as a conference paper at ICLR 2020 0 125k 250k 5 10 15 20 25 Step BLEU Score FP32 S2FP8 0 125k 250k 2 4 6 Step Loss FP32 S2FP8 Figure 7: Comparing BLEU score and Loss of S2FP8 and FP32 for Transformer tiny on En-Vi dataset 1 10 20 0.5 0.55 0.6 0.65 Iteration Hit Ratio FP32 S2FP8 1 10 20 0.3 0.35 0.4 Iteration NDCG FP32 S2FP8 1 10 20 0.2 0.25 0.3 Iteration Loss FP32 S2FP8 Figure 8: Comparing Hit Ratio, NDCG and Loss of S2FP8 and FP32 for NCF on MovieLens-1M 5 H ARDWARE ASPECTS S2FP8 is a new data type and requires its own circuitry to be implemented in a tensor processing en- gine. However, the added overhead is very minimal and affects neither data throughput nor compute speed. In order to convert FP32 tensors into S2FP8, two hardware (HW) components are needed. One is to calculate each tensor’s statistics (Equation 3), which bring minimal HW complexity. To make compute operations even easier these statistics could be stored in lower precision such as FP8/INT8. The other component is to adjust the exponent and mantissa of all those tensor elements by applying the squeeze ( α) and shift ( β) factors in Equation 4 before truncating them into their 8-bit placeholders. The shift could be done using simple element-wise add/subtract operations on the exponents, and element-wise squeeze could be applied to the mantissa portions. Another con- sideration is within the tensor processing engine(e.g., GEMM engine) which requires the αand β factors while doing the calculations. The FP32 result will be converted back to S2FP8 when needed (e.g., to store back in memory) as shown in Figure 4. 6 C ONCLUSION We introduce a novel 8-bit ﬂoating point data type (S2FP8), that gives competitive performance in comparison to state-of-the-art FP32 baselines over a range of representative networks. S2FP8 makes use of shifted and squeezed factors to shift and rescale the range of tensors prior to truncation. S2FP8 allows training of neural networks with an 8-bit format while eliminating the need for loss scaling tuning, hardware-complex rounding techniques. In addition, compared to existing FP8 implemen- tations we also eliminate the restriction of maintaining the ﬁrst and last layers in FP32. Decreasing Movielens 1 million FP32 S2FP8 ∆ FP8 NCF 0.666 0.663 0.003 0.633 Table 4: HR Score for NCF on the Movielens 1 million dataset. 8Published as a conference paper at ICLR 2020 the number of bits enables larger models to ﬁt on a single device and results in faster training. As part of future work, we plan to extend the use of S2FP8 to train additional DNN topologies and also simplify the squeeze and shift statistics from a hardware implementation point of view. We also plan to explore the use of reduced precision to store the statistics and the extendability of this ap- proach to efﬁciently represent a broader suite of low precision formats like 8-bit POSIT (Gustafson & Yonemoto, 2017), 4-bit ﬂoating and integer data types. ACKNOWLEDGMENTS We would like to thank Naveen Mellempudi, Pratap Prasad, Prasanna Singamsetty and Cory Stephenson for insightful discussions. REFERENCES Anwarul Azim. Low precision arithmetic operations in deep neural networks: An overview. Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems, pp. 5145–5153, 2018. Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas, et al. Mixed precision training of convolutional neural networks using integer operations. arXiv preprint arXiv:1802.00930, 2018. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi- erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Yunhui Guo. A survey on methods and theories of quantized neural networks. CoRR, abs/1808.04752, 2018. URL http://arxiv.org/abs/1808.04752. Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746, 2015. John L Gustafson and Isaac T Yonemoto. Beating ﬂoating point at its own game: Posit arithmetic. Supercomputing Frontiers and Innovations, 4(2):71–86, 2017. F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):19, 2016. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col- laborative ﬁltering. In Proceedings of the 26th international conference on world wide web, pp. 173–182. International World Wide Web Conferences Steering Committee, 2017. Jeff Johnson. Rethinking ﬂoating point for deep learning. CoRR, abs/1811.01721, 2018. URL http://arxiv.org/abs/1811.01721. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja V ooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bﬂoat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Urs K ¨oster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K Bansal, William Constable, Oguz Elibol, Scott Gray, Stewart Hall, Luke Hornof, et al. Flexpoint: An adaptive numerical format for efﬁcient training of deep neural networks. In Advances in neural information processing systems, pp. 1742–1752, 2017. 9Published as a conference paper at ICLR 2020 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012. Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Minh-Thang Luong and Christopher D. Manning. Stanford neural machine translation systems for spoken language domain. In International Workshop on Spoken Language Translation, Da Nang, Vietnam, 2015. Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, and Jeffrey S Vetter. Nvidia tensor core programmability, performance & precision. In 2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pp. 522–531. IEEE, 2018. Clive Maxﬁeld. An introduction to different rounding algorithms. Programmable Logic Design Line, pp. 1–15, 2006. Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision train- ing with 8-bit ﬂoating point. arXiv preprint arXiv:1905.12334, 2019. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pp. 311–318, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10. 3115/1073083.1073135. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Train- ing deep neural networks with 8-bit ﬂoating point numbers. In Advances in neural information processing systems, pp. 7675–7684, 2018. Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680, 2018. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 10Published as a conference paper at ICLR 2020 A A PPENDIX A.1 SUPPLEMENTARY TABLES AND FIGURES Format Bits s/e/m Min sub- normal Min nor- mal (Approx.) Max normal Machine epsilon Range IEEE-FP32 32 1/8/23 2−149 2−126 2128 2−24 2277 IEEE-FP16 16 1/5/10 2−24 2−14 216 2−11 240 BF16 16 1/8/7 2−133 2−126 2128 2−8 2261 FP8 8 1/5/2 2−16 2−14 216 2−3 232 Table A1: Comparing several ﬂoating point formats. s/e/m indicates the number of sign (s), exponent (e) and mantissa (m) bits. Models Datasets FP32 BF16 FP8 FP8+other recipes S2FP8 ResNet-20 CIFAR-10 91.5 91.7 17.9 91.1(Loss Scale=100) 91.1 ResNet-50 CIFAR-10 93.0 93.2 11.5 92.9(Loss Scale=100) 93.2 ResNet-50 ImageNet 76.2 76.5 NaN 75.3(Loss Scale=10K, FP32 for ﬁrst and last layers) 75.2 NCF MovieLens1M 0.666 0.653 0.633 - 0.663 Transformer- tiny En-Vi 25.3 25.6 NaN 21.3(Loss Scale=Exp) 25.3 Table A2: Comparing FP32, BF16, vanilla FP8, FP8 with tuning and S2FP8 on the model ResNet(Top1-accuracy), NCF(Hit Ratio),Transformer-tiny(BLEU score). −16 −8 0 8 16 1 2 3 4 log2(|X|) Numbers density Figure A1: The range and precision of FP8. Bar indicate the number density between each power of 2. Since FP8 has 2 mantissa bit, the density is 4 (except in the denormals), and the associated machine epsilon is 2−3 = 1/8. The normal representable range goes from 2−14 to (1 −2−3)216, with denormals from 2−16 to 2−14. A.2 S UPPLEMENTARY EQUATIONS ∂(λL) ∂w (w) = λ∂L ∂w(w) ⇒w(k+1) = w(k) −α1 λ ∂(λL) ∂w (w(k)). (6) 11Published as a conference paper at ICLR 2020 0 50k 100k 60 80 100 Step Top-1 accuracy (%) FP32 S2FP8 0 50k 100k 0 1 2 3 Step Loss FP32 S2FP8 0 50k 100k 0.2 0.3 Step L2 Loss FP32 S2FP8 Figure A2: Convergence of ResNet-50 with the CIFAR-10 dataset 12",
      "meta_data": {
        "arxiv_id": "2001.05674v1",
        "authors": [
          "Léopold Cambier",
          "Anahita Bhiwandiwalla",
          "Ting Gong",
          "Mehran Nekuii",
          "Oguz H Elibol",
          "Hanlin Tang"
        ],
        "published_date": "2020-01-16T06:38:27Z",
        "pdf_url": "https://arxiv.org/pdf/2001.05674v1.pdf"
      }
    },
    {
      "title": "Binarized Neural Machine Translation",
      "abstract": "The rapid scaling of language models is motivating research using\nlow-bitwidth quantization. In this work, we propose a novel binarization\ntechnique for Transformers applied to machine translation (BMT), the first of\nits kind. We identify and address the problem of inflated dot-product variance\nwhen using one-bit weights and activations. Specifically, BMT leverages\nadditional LayerNorms and residual connections to improve binarization quality.\nExperiments on the WMT dataset show that a one-bit weight-only Transformer can\nachieve the same quality as a float one, while being 16x smaller in size.\nOne-bit activations incur varying degrees of quality drop, but mitigated by the\nproposed architectural changes. We further conduct a scaling law study using\nproduction-scale translation datasets, which shows that one-bit weight\nTransformers scale and generalize well in both in-domain and out-of-domain\nsettings. Implementation in JAX/Flax will be open sourced.",
      "full_text": "Binarized Neural Machine Translation Yichi Zhang 1 Ankush Garg 2 Yuan Cao2 Łukasz Lew 2 Behrooz Ghorbani 2 Zhiru Zhang 1 Orhan Firat 2 Abstract The rapid scaling of language models is motivat- ing research using low-bitwidth quantization. In this work, we propose a novel binarization tech- nique for Transformers applied to machine trans- lation (BMT), the ﬁrst of its kind. We identify and address the problem of inﬂated dot-product vari- ance when using one-bit weights and activations. Speciﬁcally, BMT leverages additional Layer- Norms and residual connections to improve bina- rization quality. Experiments on the WMT dataset show that a one-bit weight-only Transformer can achieve the same quality as a ﬂoat one, while being 16×smaller in size. One-bit activations in- cur varying degrees of quality drop, but mitigated by the proposed architectural changes. We fur- ther conduct a scaling law study using production- scale translation datasets, which shows that one- bit weight Transformers scale and generalize well in both in-domain and out-of-domain settings. Im- plementation in JAX/Flax will be open sourced. 1. Introduction Neural language models are scaling, with the parameter count of recent models, such as the GPT family, roughly in- creased by 10×per year (Narayanan et al., 2021). A scaling law study by Kaplan et al. (2020) suggests that the contin- uous increase in model parameters is strongly correlated with performance improvement. This trend has been vali- dated by recent successes in large-scale models, such as the 540-billion parameter Pathways Language Model (PaLM), which achieves breakthrough performance on language un- derstanding and generation (Chowdhery et al., 2022). The 540-billion parameter Minerva (Lewkowycz et al., 2022) also exceeded the national average on the National Math Exam in Poland in 2021, where language models were previ- ously far from human-level. Similarly, in the ﬁeld of neural machine translation (MT), the scaling law holds, as reported 1Cornell University 2Google Research. Correspondence to: Yichi Zhang <yz2499@cornell.edu>, Ankush Garg <ankugarg@google.com>, Yuan Cao <yuancao@google.com>, Łukasz Lew <lew@google.com>, Behrooz Ghorbani <ghor- bani@google.com>, Zhiru Zhang <zhiruz@cornell.edu>, Orhan Firat <orhanf@google.com>. by Ghorbani et al. (2021), with the translation quality im- proving as the model size increases. The aggressive scaling trend resulted in unprecedented chal- lenges in model serving. In particular: The inference cost grows exponentially. The size and computational complexity of language models are increas- ing rapidly, with roughly a 10×increase in model size and a 100×increase in operation count per year (Hoffmann et al., 2022). However, the energy efﬁciency of hardware used to run these models is not keeping pace. Speciﬁcally, the energy required for FP32 operations has improved by only 2.5×over the past 11 years (2007-2018), from 45nm to 7nm process nodes. Over the same period, DRAM access energy has only improved by 6.3×(Jouppi et al., 2021). The ever-growing gap between the inﬂation of model size and inefﬁciency in hardware energy utility is causing inference energy to grow exponentially, which is becoming a major part of the cost of running language models in datacenters. The inter-chip communication overhead becomes non- negligible. Data parallelism alone is no longer sufﬁcient for models at such a large scale since one matrix multiplication cannot ﬁt on a single accelerator chip. Each weight tensor in PaLM, for example, is partitioned across 3072 TPUv4 chips in a pod (Chowdhery et al., 2022). This leads to a huge overhead on transferring the weights and intermediate activations across the datacenter networks. Latency-critical applications can now hardly beneﬁt from parameter caching. Loading model parameters from DRAM to on-chip accelerator memory often takes a lot of time during inference. In the past, parameter caching was an effective optimization for latency because it reused model weights and avoided off-chip memory transfers. However, evaluations on edge TPUs reported that this method works best for models with fewer than 30 million parameters (Se- shadri et al., 2021). For larger models, parameter caching even becomes harmful. Beneﬁts from compiler optimiza- tions are diminishing, and the serving latency becomes al- most proportional to the model parameter count. In our case, the smallest translation model has about 50 million pa- rameters. Improving latency thus boils down to increasing memory bandwidth alone. Quantization can signiﬁcantly reduce inference cost. Bi- narization is an extreme case where both the weights and arXiv:2302.04907v1  [cs.CL]  9 Feb 2023Binarized Neural Machine Translation activations of a matrix multiplication (matmul) are quan- tized to a single bit. Compared to the Brain ﬂoating-point format (bﬂoat16) (Abadi et al., 2016) 1, binarization reduces the weight size by 16×, thus signiﬁcantly lowering the mem- ory and communication overhead. Moreover, a binarized matmul can be carried out by XNOR operations followed by a population count, which is estimated to be 256×more energy-efﬁcient than the bﬂoat16 counterpart (Zhang et al., 2022). Binarization has been successful on ImageNet in terms of accuracy-efﬁciency trade-off (Zhang et al., 2022). Prior work shows that BERT can be binarized for pretraining (Bai et al., 2020; Qin et al., 2022; Liu et al., 2022); however, it is important to note that the BERT and MT models, which both use Transformer as their core (Vaswani et al., 2017), are very different. One key difference is the architecture: while an MT model has both an encoder and a decoder, BERT only has an encoder. This difference can impact the quality of encoder quantization because every cross attention layer in the decoder requires outputs from the encoder. Another difference is that MT model inference produces a sequence of text, while BERT performs a single text classiﬁcation. This is critical because each word in the output translation sequence affects the generation of the next word. The sampling distribution of a word is therefore crucial and should be preserved after binarization, but for BERT, only the peak of the logits needs to be preserved. Due to these differences, directly applying BERT binarization techniques to MT can easily result in a lower quality model. In this work, we investigate binarized Transformer for neu- ral machine translation, which, to our knowledge, is the ﬁrst study on this topic. Each Transformer block contains an attention layer and a feed-forward network (FFN). We binarize the weights and activations separately so we can study how each one affects the quality of the model. We found that binarizing weights did not signiﬁcantly affect accuracy, but that traditional methods for binarizing activa- tions led to poor performance due to activation magnitude explosion. Then, we propose a new method for activation binarization that uses a simple scaling factor and additional residual connections. To understand the scaling behavior of the proposed 1-bit Transformer in practice, we further evaluate it on our in- house production-scale translation dataset that contains three billion sentence pairs. We for the ﬁrst time demon- strate that the 1-bit weight Transformer scales and gener- alizes similarly well as the ﬂoat one, even on the out-of- domain data. We also analyze sentences sampled from both models’ outputs and ﬁnd that the 1-bit Transformer gen- erates a similar translation quality as its ﬂoat counterpart. Binarization can therefore be a potential candidate for future MT model serving. 1In the remaining paper, “ﬂoat” refers to bﬂoat16. 2. Related Work The success of Transformer has spurred an active body of work to quantize it to lower precision. In this section, we review a subset of these efforts that inspired our approach. Transformer quantization. Much of the prior effort fo- cused on 8-bit Transformer. Bhandare et al. (2019) reported a less than 0.5 BLEU drop on the WMT14 En-De translation task with 8 bits. Prato et al. (2019) showed an 8-bit Trans- former preserved the translation quality. For non-generative tasks, Zafrir et al. (2019) quantized BERT to 8-bit with marginal quality loss. When pushed down to 4 bits, though Prato et al. (2019) reported an 8 BLEU degradation for MT, Aji & Heaﬁeld (2019) reported almost no BLEU loss by using a logarithmic quantization scheme. The exploration on 1-bit Transformers centered around BERT. Usually binarization is directly applied and the focus is on improving the training recipe. Bai et al. (2020) initi- ated the attempt by splitting a ternary BERT into a binary one, then ﬁne-tuning. It achieved 41% average accuracy on the GLUE benchmarks. Qin et al. (2022) proposed to distill each intermediate layer outputs from a ﬂoating-point model. Recently, Liu et al. (2022) proposed to incrementally quantize the model, e.g., from 32-bit to 4-bit to 2-bit, ﬁnally to 1-bit, and it improved the GLUE accuracy to 73.5%. Binarized vision models. Courbariaux et al. (2016) pio- neered the investigation on binarized deep neural nets. Re- cently, PokeBNN (Zhang et al., 2022) established a pareto SOTA on the ImageNet recognition task. We inherit the binarization functions and training recipes from PokeBNN. Generalizability. Hooker et al. (2019) show that com- pressed models do not generalize well on out-of-domain (OOD) data. We are particularly interested in evaluating BMT under OOD settings and analyze its generalizability. 3. Algorithm and Model Architecture In this section, we introduce the methodology of binariz- ing a Transformer-based MT model. We ﬁrst deﬁne the binarization equations, then show that directly applying the equations to Transformer will produce an inferior model quality because of the dot-product variance inﬂation. A scaling factor is then proposed as a solution to this problem, and we discuss using LayerNorm (Ba et al., 2016) to re- place ﬁxed scaling factors. Finally, we combine and present the architectural changes that are necessary to improve the binarized model quality. 3.1. Binarization Equations We follow the approach deﬁned in PokeBNN (Zhang et al., 2022), which includes an important hyperparameter “ B”. The function of casting ﬂoating-point values into binaryBinarized Neural Machine Translation values is summarized as follows. clip (x,xmin,xmax) := min (xmax,max (xmin,x)) xb := ( ﬂoor ( clip (x B,−1 + ϵ,1 −ϵ )) + 0.5 ) ×B where xis the input tensor, ϵis a small ﬂoating-point num- ber that prevents overﬂow when taking the ﬂoor, and Bis the binarization bound. In the backward propagation, the ﬂoor function is ignored, i.e., ∂ ﬂoor(x) ∂x := 1 , known as the straight-through estimator (Courbariaux et al., 2016). The gradient of the entire binarization function is then ∂xb x = 1 x∈[−B,B], otherwise zero. The bound B there- fore serves as a hyperparameter that controls the range of the input values that will have non-zero gradients. Note that B also serves as a scaling factor for the outputs since the binarization function maps x → { −B 2 ,+B 2 } . The bound B can also generalize to a vector, depending on the gran- ularity of binarization. The ﬁnest granularity, however, is one bound value for each dot product, i.e., per contraction dimension, so that the binarized matrix multiplication can be accelerated. For a dense layer in Transformer of the form A·W, where AN×dmodel is the input activations and Wdmodel×dk is the model weights, we instead compute a binarized matmul Ab ·Wb. Throughout the experiments we apply binarization bound BW and BA for weights and activations, respectively. BW = max(abs(W),axis = dmodel) BA = max(abs(A),axis = dmodel) where axis is the dimension along which max is taken. Using one axis only means the bound is per channel and per example (Lew et al., 2022). Both BN×1 A and B1×dk W are vectors that contain maximum absolute values along the contraction dimension. Note that the weight binarization bound BW is static in inference though it is updated in every training iteration. The activation bound BA is dynamic. 3.2. Variance Inﬂation in Binarization We start by applying the binarization function to feed- forward networks (FFNs), leaving other modules as ﬂoat. We observe that directly binarizing the weights preserves the model quality, but binarizing the input activations causes the training to not converge in the context of machine trans- lation. To understand the reason of this behavior, we analyze the variance of the dot product magnitude with and with- out binarization. Our analysis reveals that binarizing both weights and activations will statistically inﬂate the mag- nitude, leading to abnormal signal propagation within the neural network (Brock et al., 2021). We present the details of this analysis as follows. Let each weight of a dense layer be randomly initialized and sampled from a zero-mean normal distribution, w ∼ N(0, σ2 w). Assume each input activation is independent of the weights and identically distributed as a ∼N (0, σ2 a). After applying the binarization function, both wb and ab are still centered at zero and have an equal probability of being either −B 2 or +B 2 , namely, they follow the probability mass function deﬁned as follows: Pr (xb) = {1 2 xb = −B 21 2 xb = +B 2 Hence the variance of a binarized multiplication is Var (ab ·wb) = E [ a2 b ] ·E [ w2 b ] −E2 [ab] ·E2 [wb] = ∑ ab a2 b ·Pr (ab) · ∑ wb w2 b ·Pr (wb) −0 = B4 16 The variance of a binarized dot product is then Var (Ab ·Wb) = D−1∑ n=0 Varn (ab ·wb) = B4 16 ·D where Dis the dimensionality of the dot product, i.e., the hidden projection dimension in an FFN, and nis the index of each entry in the vector. Following the same analysis, the variance of a ﬂoating-point dot-product is Var (A·B) = σ2 a ·σ2 w ·D Note that the commonly used Xavier initializer (Glorot & Bengio, 2010) equalizes the variance of the activations across layers. σ2 w will therefore be initialized as 1 D , so Var (A·W) = σ2 a, which is usually at the scale of 1. Meanwhile, the common binarization bound is B ∈[1,3] (Courbariaux et al., 2016; Zhang et al., 2022; Bethge et al., 2021). Our Transformer FFN employs a hidden projection dimension D = 4096 throughout the experiments. There- fore, Var (Ab ·Wb) ≫Var (A·W). Binarization heavily inﬂates the dot product variance by at least 256×, which will be reﬂected in the magnitude of the dense layer outputs. Also note that Var (Ab ·Wb) ∝D, indicating that Trans- former with a larger width will potentially suffer more from the convergence issue. 3.3. A Scaling Factor as the Solution Inspired by the scaling factor √dk in the scaled dot-product attention Attention (Q,K,V ) = softmax ( QKT √dk ) V in the original Transformer (Vaswani et al., 2017), we pro- pose a scaling factor for each binarized dense layer, i.e., Dense (Ab) = Ab ·Wb s The scaling factor s is a hyperparameter that suppresses dot-product variance inﬂation, while in the attention layerBinarized Neural Machine Translation √dk prevents the dot products from entering small-gradient regions of the softmax function. According to the analysis in Section 3.2, its value is estimated to be s∝ √ Din order to cancel the multiplicative effect from Don the variance. To verify how the magnitude of the scaling factor affects the training loss, we sweep sin Section 5. In practice, s≥64 can make the training converge. 3.4. Replacement of Scaling Factor with LayerNorm While the scaling factor senables the binarization of FFNs, it requires hyperparameter tuning, which can be challenging for billion-parameter translation models. To address this deﬁciency, we propose using layer normalization (Layer- Norm) (Ba et al., 2016) as a drop-in replacement for the scaling factor, which has the form of LN (x) = x−E[x]√ Var (x) + ϵ ·γ+ β where γ and β are learnable parameters. Besides the fact that γcan incorporate the scaling factor s, LayerNorm also has the following advantages. The scaling factor is now dynamic and adaptive during training. The binarization function employs a dynamic bound B, so Var (Ab ·Wb) varies. The learnable parameter γ in LayerNorm can better capture the changes in the dot product variance and hence properly normalize it. LayerNorm also redistributes the input activations. It en- ables the binarization of a tensor with all positive values. A directly binarized FFN has the structure of FFN (A) = max (0,AbW1b + b1)b W2b + b2 where W1, b1 and W2, b2 are the weights and biases for the ﬁrst and second dense layer, respectively. One may note that the activations max (0,AbW1b + b1) are all pos- itive. The binarization function will then map the entire tensor to a constant +B 2 , which undermines the model train- ing. With the help LayerNorm, however, the activations are redistributed and more balanced in terms of the number of positive and negative values. This enables the normal {−1,+1}(bipolar) binarization of the second dense layer. Qin et al. (2022); Liu et al. (2022) used {0,1}binarization instead in binarized BERT to overcome the issue of constant positive values. It yields a ternary matrix multiplication since A∈{0,1}N×D and W ∈{−1,+1}D×K, which in- curs nontrivial additional overhead if computed on binary hardware accelerator. The complete proposed 1-bit FFN has the structure of FFN (A) = LN (LN (max (0,AbW1b + b1))b ·W2b + b2) When proceeding to the attention binarization, we add a Lay- erNorm to the output of each linear projection layer for the same reasons. We veriﬁed in Section 5 that a dynamic and adaptive scaling factor in LayerNorm indeed outperformed a ﬁxed one. 3.5. Residual Connection in Attention Layers Figure 1.BMT Multi-Head Attention layer — Differences from the original Transformer are highlighted (in yellow). All linear projections and einsums can be binarized. In attention layers, we also add a shortcut connection to the output linear projection layer. In BNNs, gradients of a binarized layer are approximated due to the straight-through estimator. This will eventually lead the optimization into a different direction as we stack more binarized layers. Liu et al. (2018) proposed adding additional residual connec- tions in BNNs, which became a useful method for partially addressing this issue. We therefore adopt it in our model. Note that this modiﬁcation is unnecessary for QKV (query, key, value) linear projections. The shortcut around the entire attention layer in the original Transform serves the same purpose. We will also demonstrate the effectiveness of the shortcut connection in the ablation study in Section 5. The complete modiﬁed attention architecture is shown in Figure 1, where we highlight the differences from the orig- inal one. The extra layer normalization and shortcut con- nection are both elementwise. Their overhead is small, especially comparing to the beneﬁts of binarization. 4. Experiments In this section, we empirically evaluate our proposed bi- narized Transformer on MT tasks at difference scales. To investigate the impact of binarizing different layers, we ﬁrst evaluate a standard 6-layer encoder-decoder (6L6L) Trans- former on the WMT2017 En-De translation dataset (Bojar et al., 2017). We then choose the 1-bit weight model variant and study its practical scaling law on in-house translationBinarized Neural Machine Translation Table 1.BMT results on the WMT17 En-De dataset. Binarized activations or weights are labeled by checkmarks. The data type of unlabeled tensors remains bﬂoat16. BLEU evaluation employs a beam size of 4. ATTENTION 1-BIT FFN 1- BIT METRICS AQKV WQKV AOUT WOUT QK E INSUM SCORE -V E INSUM AFFN WFFN VAL LOSS BLEU 1 1.39 26.35 2 ✓ ✓ ✓ 1.38 25.93 3 ✓ ✓ 1.40 25.44 4 ✓ ✓ ✓ ✓ 1.51 24.11 5 ✓ ✓ ✓ ✓ ✓ 1.72 21.55 6 ✓ ✓ ✓ ✓ ✓ 1.60 21.06 7 ✓ ✓ ✓ ✓ ✓ ✓ 1.89 17.87 8 ✓ ✓ ✓ ✓ 1.76 18.27 9 ✓ ✓ ✓ ✓ ✓ 2.81 9.42 datasets. We also analyze the translation samples from both 1-bit and ﬂoat models to compare their qualities. 4.1. WMT Results We binarize ﬁve different matmuls in a Transformer. In an attention layer there are (1) QKV linear projections; (2) activation-activation matmul between queries and keys (QK Einsum); (3) activation-activation matmul between attention scores and values (Score-V Einsum); (4) output linear projection. In an FFN there are two dense layers of the same type. To study their individual impact, we binarize their weights and activations separately. In our experiments we use the following training details. Model. We use a 6L6L Transformer as the base model. Embedding dimension is 1024. Each multi-head attention layer has 16 heads, with a dimension of 1024 for QKV if combining all the heads. The hidden projection dimension in FFNs is 4096. Dropout layers has a dropout rate of 0.1. Optimizer. Adam optimizer (Kingma & Ba, 2014) is used with β1 = 0.9 and β2 = 0.98. No weight decay is applied. Scheduler. We adopt a three-stage training scheme, where the learning rate (LR) of each stage decreases from base to zero following a cosine decay. A quantization event starts at the beginning of each stage. We ﬁrst train the model in ﬂoat. In the second stage, all weights will be binarized. In the last stage, both weights and activations will be binarized. Loss. We apply knowledge distillation (KD) during training. KD can be implemented by replacing the ground truth label in the cross-entropy loss function with the softmaxed logits from the teacher model, so it is optional for users. Training. We use a batch size of 1024. Base learning rate is 0.001. The ﬁrst LR cycle has 50000 steps, others have 88339 steps. We train the model with a 4×8 TPU topology. Observations. The evaluation results on WMT2017 En-De translation dataset is shown in Table 1. We mainly rely on the validation loss for comparing the model quality since BLEU score has a higher variation (Ghorbani et al., 2021). From the table we have the following key observations. Weight-only binarization preserves the model loss. The ﬂoat 6L6L Transformer baseline (row 1) has a 1.39 vali- dation loss. In contrast, binarizing all dense layer weights (in both attention layers and FFNs) produces an even lower loss (1.38, row 2), though the BLEU score slightly drops by about 0.4. Both metrics indicate that the 1-bit weight model has a similar translation quality to the ﬂoat baseline. Bina- rization therefore has the potential to compress the model size by 16×while preserving the quality. FFN binarization produces promising results. Binariz- ing the entire FFN, i.e., both activations and weights, while leaving other layers ﬂoat, again yields a similar validation loss (1.4, row 3) compared with the ﬂoat baseline. With our proposed BMT, it is the ﬁrst time on machine translation tasks that binarizing FFN activations can preserve the loss. This intriguing 1-bit FFN variant can be potentially useful for mixture-of-expert (MOE) models where FFNs contribute 50 to 90% of the total model parameters (Lepikhin et al., 2021). Combing with 1-bit all dense layer weights further downgrades the loss to 1.51 (row 4) and a 2.2 lower BLEU score in contrast to the ﬂoat model. Overall, FFN binariza- tion demonstrates a promising potential. Attention activations are the key bottleneck to high bi- nary model quality. On top of the 1-bit weights and 1-bit FFN activation model variant, further binarizing input acti- vations in all dense layers in the attention layer (row 7; this includes keys, queries, values and input activations to the output projection dense layer) leads to a 1.89 loss. This is by far the largest drop in model quality. Binarizing each individual activation tensor therein leads to at least0.3 degra- dation in loss (row 5 and 6). In addition, binarizing the two activation-activation matmuls (query-key einsum operation and attention score-value einsum operation) are particularly challenging. The 1-bit weights model with both activation-Binarized Neural Machine Translation (a) In-domain {pe, pd} : {0.18, 0.31}Float, {0.16, 0.28}Binary  (b) Out-of-domain {pe, pd} : {0.13, 0.25}Float, {0.13, 0.25}Binary Figure 2.Scaling law study on both in-domain and out-of-domain data — On in-domain data, scaling law ﬁts achieve R2 values of 99.5 and 99.7 on ﬂoat and binary models respectively. Similarly, on out-of-domain data (Wikipedia), R2 values are 99.6 and 99.8 respectively. Scaling law ﬁt on all the evaluation datasets, along with slopes (pe and pd) is presented in Figure 8 and Figure 9 (Appendix A). activation matmuls binarized additionally produces only 9.4 BLEU score (last row). Attention layer activations are the current bottleneck to a fully binarized translation model. 4.2. Scaling Law Study Though the Section 4.1 show promising results, an unan- swered question is whether the performance degrades when binarized Transformers are scaled up. Neural language model loss is known to follow a power law as model size scales up (Kaplan et al., 2020), known as the “scaling law”. It is widely adopted for predicting the performance of mod- els at scale. We therefore conduct a scaling law study on both ﬂoat and binarized models on our in-house translation dataset and compare their difference. We train a set of trans- lation models and ﬁt the losses using the following equation, similar to Ghorbani et al. (2021): L(Ne,Nd) = α ( ¯Ne Ne )pe ( ¯Nd Nd )pd + L∞, where Lis the per token loss, Ne, Nd are the number of encoder and decoder parameters respectively. L∞is the irreducible loss that the model attains if it has inﬁnite ca- pacity. ¯Ne ( ¯Nd) is the number of parameters in the base- line 6L6L Transformer, which act as normalization con- stants for numerical stability in the curve ﬁtting process. For tractability purposes, we examine scaling laws for only weight-binarized models. Weight-only model compression can also be leveraged for linear improvements in latency (Seshadri et al., 2021) and 16×improvements in memory consumption (compared to blfoat16). Dataset. To investigate the scaling behavior of the binary models in a capacity limited regime, i.e., performance is not bound by training data, we use our large in-house parallel corpora for English to German (En →De) direction. The training set contains 3 billion web-crawled sentence pairs. We are also particularly interested in evaluating BMT with the out-of-domain (OOD) setting and assessing its general- izability, as previous research in the image domain demon- strated that compressed models (weight pruned or quan- tized) have a much larger quality drop on OOD data than their uncompressed counterparts, i.e., model compression ampliﬁes brittleness (Hooker et al., 2019). As such, to have a robust evaluation of BMT, we use eleven evaluation sets, one of which is in-domain (ID) and is similarly distributed as the training set, and the rest are OOD. For ID, we sample 2000 training examples and remove them from the training data. The ten OOD evaluation sets are divided into four cat- egories (i) Web Domain (ii) News Domain (iii) Wikipedia (iv) Patents. Furthermore, they are either “source-original” or “target-original”. The source-original datasets have a natural source side (English) while the target side (German) is human or machine translated. The target-original datasets have the natural target side (German), then back translated into source English sentences. We do this differentiation to investigate the impact of binarization on “style” of sentences since natural language exhibits rich diversity as opposed to simple and literal (translationese) sentences (Freitag et al., 2020) (More details are provided in Appendex A.1). Models & Training. We train two sets of Transformers, namely, encoder-scaling and decoder-scaling models. The encoder-scaling models have a ﬁxed depth of 6 layers in the decoder while scaling up the encoder depth in sizes of {6,8,10,12,14,18,22,26,30,36,42,48}layers, for a total of 12 models. Same for the decoder-scaling ones, whereby the decoder depth is scaled up in similar ways. Due to the sufﬁciency in training data, we did not use label smoothing during training. The binary models are trained without knowledge distillation. (See Appendix A.2 for more details on Hyper-parameters and training).Binarized Neural Machine Translation Figure 3.Training vs. ID Test loss. We observe similar linear rela- tionship between training and test losses of all evaluation datasets. Observations. Figure 2 compares the scaling curves of the binary and ﬂoat models on both ID and OOD datasets, more in Appendix A.3. Figure 3 compares their training vs. In-domain test loss. We make the following observations: Binary models demonstrated similar scaling behaviors as their ﬂoat counterpart for both encoder and decoder scaling. The exponent of the ﬁtted power law for binary models in Figure 2a (pe = 0.16, pd = 0.28) is only slightly below ﬂoat ones ( pe = 0 .18, pd = 0 .31), indicating the binary model loss improves fast as the parameter count increases. This trend also holds for OOD Wikipedia dataset in Figure 2b. Binary models generalize just as well on OOD data as ﬂoat models (scaling law ﬁts on all the OOD evaluation datasets is in Appendix A.3). We also note a gap between binary and ﬂoat model losses, a phenomenon not observed from WMT experiments. We hypothesize that this is because the in-house production-scale datasets are more challenging and require a higher model capacity to learn. For the same training loss, binary and ﬂoat models achieve the same generalization performance. As shown in Figure 3, binary and ﬂoat model losses align well on a straight line, and almost overlap in the 0.95 ∼1.0 region. There are no measurable differences detected in the induc- tive biases of the two model classes. Also, binary models require fewer parameter bits to achieve a certain perfor- mance level. For example, a 6L42L binary Transformer with 195M parameters (195M bits) has a 4.3×smaller size than a 6L8L ﬂoat one with 52M parameters (832M bits) while having the same loss. Such memory savings are es- pecially advantageous when the models are deployed in a resource-constrained environments (Seshadri et al., 2021). 4.3. Generation Quality We examine the MT model generation quality in Figure 4 using two decoding strategies: a) Beam Search Decoding; b) Minimum Bayes Risk (MBR) decoding (Kumar & Byrne, 2004). Beam search. Sample quality from Beam search decoding 2 is evaluated with standard de-tokenized BLEU scores (Pa- pineni et al., 2002) using sacreBLEU library (Post, 2018).3. MBR. Freitag et al. (2022) show that beam search decod- ing selects samples with high probability rather than high quality, especially for large models, as measured by human evaluations. They propose MBR-based decoding strategy deﬁned as: hMBR = arg max h∈H 1 |Hmodel| ∑ y∈Hmodel u(h,y) where hMBR is the decoding from the model given source sentence x, Hmodel is the set of hypotheses sampled from the model p(.|x) and uis a utility function that evaluates quality of a hypothesis hagainst reference y. Freitag et al. (2022) demonstrate effectiveness of the BLEURT model (Sellam et al., 2020) for the utility function. BLEURT is a regression model that relies on the concatenation of hypothesis hand reference yand generates a scalar score between [0,1], mea- suring the hypothesis quality irrespective of the sentence structure, length or word overlap with the reference. In the same way, we use MBR decoding with BLEURT as the util- ity function to decode a sequence given the source sentence. To measure the sample quality, BLEURT(h,r) is calculated between the decoded hypothesis (hMBR) and the reference (r) for a given source sentence ( x), in the evaluation set. The BLEURT scores are averaged across the evaluation set. Observations. Figure 4a shows BLEU scores of encoder- scaling models (i.e., decoder depth=6, varying encoder depth). Figure 4b plots BLEURT scores for encoder-scaling models, where the baseline is ﬂoat models using MBR de- coding with 16 samples. We observe the following: Binary models can achieve the same BLEU score as ﬂoat models with a smaller size. Figure 4a shows that the BLEU score of binary models will consistently improve as the model size increases. Although binary models are 2-3 BLEU points worse than ﬂoat ones at the same model depth, the 42L6L binary model achieves the same BLEU score as the 10L6L ﬂoat model, while being 5×smaller in size. Increasing the sample size can match the generation quality of binary models with ﬂoat models. In Figure 4b, a larger sample size consistently produces a higher genera- tion quality for the binary models. At 4×the sample size, i.e., 64 samples, the binary model quality approximately matches the ﬂoat models. Besides, the BLEURT score of binary models also improves as the model size increases. 2beam size=4, length penalty=0.6 3case.mixed + numrefs.1 + smooth.exp + tok.13aBinarized Neural Machine Translation (a) Beam Search - BLEU  (b) MBR-BLEURT Figure 4.Comparison on translation qualities between binarized and ﬂoat models for encoder-scaling. (a) Beam Search Decoding: BLEU scores on In-Domain Test set (b) MBR Decoding: BLEURT scores on In-Domain Test set 5. Ablation Study 21 22 23 24 25 26 27 28 29 210 211 212 Scaling Factor 2 3 4 5 6 7 8 9Loss 8.07 8.87 6.81 4.98 1.93 1.64 1.58 1.54 1.52 1.51 1.51 1.50 1.49 Sweep Scaling Factor Values train loss valid loss Figure 5.Models losses with different scaling factor s. Scaling factor ablation. We binarize the FFN only and sweep the scaling factor sas a power of two from 1 (equiva- lent to no scaling factor applied) to 4096. We plot the ﬁnal training and validation losses in Figure 5. The model losses drop steeply when increasing s to 64. Models with s≤8 produce almost random translation qual- ity. Large scaling factors indeed address the convergence issue. The loss begins saturated at s = 64 and is only slightly worse than the ﬂoat baseline (1.39). This exactly matches our expectation that s∝ √ D. When s> 64, the model loss keeps improving slightly. We hypothesize that this is because the bound Bis dynamic. Even a small varia- tion on Bwill change the theoretical optimal sby a large margin since Var (Ab ·Wb) ∝B4. BMT attention layer ablation. We only binarize the atten- tion output projection linear layer. We train the model for 88339 steps, with binarization events started at step 50000. We plot the the loss curves from step 40000 in Figure 6. Applying a ﬁxed scaling factor achieves an almost 0.2 loss improvement. This is consistent with previous observations 40000 50000 60000 70000 80000 90000 Training Steps 1.2 1.4 1.6 1.8 2.0 2.2Validation Loss Impact of Model Architecture Changes basline direct binarize scaling factor 8.0 layernorm layernorm + shortcut Figure 6.Losses of different structures in attention out linear layer. where a scaling factor helps with convergence. The Layer- Norm, as a drop-in replacement for the scaling factor, not only makes the model converge to a better loss, but also recovers the loss much faster after binarization. This is ex- pected because γ in the LayerNorm is learnable and can better adapt to the dynamic bound B as analyzed in Sec- tion 3.4. The loss almost saturates after binarization. Adding a shortcut around the output projection layer removes the information bottleneck. It helps the model converge to ap- proximately the same quality as the ﬂoat baseline. 6. Conclusion The proposed method enables binarization for machine translation. The simple yet effective scaling factor is the key. Binary Transformers have a similar scaling behavior or translation quality as ﬂoat models. Binarization can thus be a potential candidate for future model serving. Unanswered questions: How to better binarize attention ein- sums? Which is better for scaling up a binary Transformer, depth or width? If combining with 4- and 8-bit quantization, what will be a better mixed-precision scheme?Binarized Neural Machine Translation References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. Aji, A. F. and Heaﬁeld, K. Neural machine transla- tion with 4-bit precision and beyond. arXiv preprint arXiv:1909.06091, 2019. Akhbardeh, F., Arkhangorodsky, A., Biesialska, M., Bojar, O., Chatterjee, R., Chaudhary, V ., Costa-jussa, M. R., Espa˜na-Bonet, C., Fan, A., Federmann, C., Freitag, M., Graham, Y ., Grundkiewicz, R., Haddow, B., Harter, L., Heaﬁeld, K., Homan, C., Huck, M., Amponsah-Kaakyire, K., Kasai, J., Khashabi, D., Knight, K., Kocmi, T., Koehn, P., Lourie, N., Monz, C., Morishita, M., Na- gata, M., Nagesh, A., Nakazawa, T., Negri, M., Pal, S., Tapo, A. A., Turchi, M., Vydrin, V ., and Zampieri, M. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pp. 1–88, Online, November 2021. Association for Computational Linguistics. URLhttps: //aclanthology.org/2021.wmt-1.1. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Bai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., Liu, Q., Lyu, M., and King, I. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020. Barrault, L., Bojar, O., Costa-juss `a, M. R., Federmann, C., Fishel, M., Graham, Y ., Haddow, B., Huck, M., Koehn, P., Malmasi, S., Monz, C., M ¨uller, M., Pal, S., Post, M., and Zampieri, M. Findings of the 2019 conference on machine translation (WMT19). In Pro- ceedings of the Fourth Conference on Machine Trans- lation (Volume 2: Shared Task Papers, Day 1) . As- sociation for Computational Linguistics, 2019. URL https://aclanthology.org/W19-5301. Bethge, J., Bartz, C., Yang, H., Chen, Y ., and Meinel, C. Meliusnet: An improved network architecture for binary neural networks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1439– 1448, 2021. Bhandare, A., Sripathi, V ., Karkada, D., Menon, V ., Choi, S., Datta, K., and Saletore, V . Efﬁcient 8-bit quantization of transformer neural machine language translation model. arXiv preprint arXiv:1906.00532, 2019. Bojar, O., Graham, Y ., and Kamran, A. Results of the WMT17 metrics shared task. In Proceedings of the Sec- ond Conference on Machine Translation, 2017. Brock, A., De, S., and Smith, S. L. Characterizing signal propagation to close the performance gap in unnormal- ized resnets. arXiv preprint arXiv:2101.08692, 2021. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y . Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. Freitag, M., Grangier, D., and Caswell, I. BLEU might be guilty but references are not innocent. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, 2020. URL https: //aclanthology.org/2020.emnlp-main.5. Freitag, M., Grangier, D., Tan, Q., and Liang, B. High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics. Transactions of the Association for Computational Linguistics, 10:811– 825, 2022. doi: 10.1162/tacl a 00491. Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun, M., Garcia, X., Chelba, C., and Cherry, C. Scaling laws for neural machine translation. 2021. Glorot, X. and Bengio, Y . Understanding the difﬁculty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Hooker, S., Courville, A. C., Dauphin, Y . N., and Frome, A. Selective brain damage: Measuring the disparate impact of model pruning. CoRR, abs/1911.05248, 2019. URL http://arxiv.org/abs/1911.05248. Jouppi, N. P., Yoon, D. H., Ashcraft, M., Gottscho, M., Jablin, T. B., Kurian, G., Laudon, J., Li, S., Ma, P., Ma, X., et al. Ten lessons from three generations shaped google’s tpuv4i: Industrial product. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Ar- chitecture (ISCA), pp. 1–14. IEEE, 2021.Binarized Neural Machine Translation Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kumar, S. and Byrne, W. Minimum Bayes-risk decod- ing for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Compu- tational Linguistics: HLT-NAACL 2004 , pp. 169–176, Boston, Massachusetts, USA, May 2 - May 7 2004. As- sociation for Computational Linguistics. URL https: //aclanthology.org/N04-1022. Lepikhin, D., Lee, H., Xu, Y ., Chen, D., Firat, O., Huang, Y ., Krikun, M., Shazeer, N., and Chen, Z. {GS}hard: Scal- ing giant models with conditional computation and auto- matic sharding. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=qrwe7XHTmYb. Lew, L., Feinberg, V ., Agrawal, S., Lee, J., Malmaud, J., Wang, L., Dormiani, P., and Pope, R. Aqt: Accurate quan- tized training), 2022. URL http://github.com/ google/aqt. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V ., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. Liu, Z., Wu, B., Luo, W., Yang, X., Liu, W., and Cheng, K.- T. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In Proceedings of the European con- ference on computer vision (ECCV), pp. 722–737, 2018. Liu, Z., Shen, Z., Li, S., Helwegen, K., Huang, D., and Cheng, K.-T. How do adam and training strategies help bnns optimization. In International Conference on Ma- chine Learning, pp. 6936–6946. PMLR, 2021. Liu, Z., Oguz, B., Pappu, A., Xiao, L., Yih, S., Li, M., Krishnamoorthi, R., and Mehdad, Y . Bit: Robustly binarized multi-distilled transformer. arXiv preprint arXiv:2205.13016, 2022. Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat- wary, M., Korthikanti, V ., Vainbrand, D., and Catanzaro, B. Scaling language model training to a trillion parame- ters using megatron, 2021. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine transla- tion. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311– 318, Philadelphia, Pennsylvania, USA, July 2002. As- sociation for Computational Linguistics. doi: 10.3115/ 1073083.1073135. URL https://aclanthology. org/P02-1040. Post, M. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers , pp. 186–191, Belgium, Brussels, October 2018. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/W18-6319. Prato, G., Charlaix, E., and Rezagholizadeh, M. Fully quantized transformer for machine translation. arXiv preprint arXiv:1910.10485, 2019. Qin, H., Ding, Y ., Zhang, M., Yan, Q., Liu, A., Dang, Q., Liu, Z., and Liu, X. Bibert: Accurate fully binarized bert. arXiv preprint arXiv:2203.06390, 2022. Sellam, T., Das, D., and Parikh, A. BLEURT: Learn- ing robust metrics for text generation. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881–7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https: //aclanthology.org/2020.acl-main.704. Seshadri, K., Akin, B., Laudon, J., Narayanaswami, R., and Yazdanbakhsh, A. An evaluation of edge tpu accelera- tors for convolutional neural networks. arXiv preprint arXiv:2102.10423, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017. Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 36–39. IEEE, 2019. Zhang, Y ., Zhang, Z., and Lew, L. Pokebnn: A binary pursuit of lightweight accuracy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12475–12485, 2022.Binarized Neural Machine Translation A. Scaling Law Study Details A.1. Dataset A concise view of evaluation datasets used for scaling laws (Section 4.2) is shown in Table. The ten OOD evaluation datsets span four categories (i) Web Domain (ii) News Domain (iii) Wikipedia (iv) Patents. They are either “source-original” or “target-original”. There are two source-original and one target-original dataset in Web Domain, one source-original each in Wikipedia and Patents domain. We use publicly available WMT newstest2019 (Barrault et al., 2019) and WMT newstest2021 (Akhbardeh et al., 2021) for News Domain. Within this domain, we have ﬁve datasets: source-original, target-original, source-original-paraphrased (Freitag et al., 2020) and source-original-high-quality (Freitag et al., 2020) from WMT newstest2019 (Barrault et al., 2019), and wmt-reference-C from WMT newstest2021 (Akhbardeh et al., 2021). Dataset Name Domain Type Source Train Subset Web mixed In-house Patents Patents mixed In-house Web domain 1 Web source-original In-house Web domain 2 Web source-original In-house Web domain 3 Web target-original In-house Wikipedia Wikipedia source-original In-house wmt-high-quality News source-original WMT newstest2019 (Freitag et al., 2020) wmt-refC News source-original WMT newstest2021 Ref-C (Akhbardeh et al., 2021) wmt-paraphrased News source-original WMT newstest2019 (Freitag et al., 2020) wmt-src-orig News source-original WMT newstest2019 (Barrault et al., 2019) wmt-tgt-orig News target-original WMT newstest2019 (Barrault et al., 2019) Table 2.Evaluation datasets used in Section 4.2. A.2. Model & Training Details All the models in Section 4.2 have an embedding dimension of 512, a hidden projection dimension of 2048, and 8 attention heads. The embedding parameters are shared on the source and the target side. The same embedding matrix (transposed) is also used for the linear readout (softmax) parameters on the decoder side. All models are trained with Adam optimizer (Kingma & Ba, 2014) and use cosine learning rate schedule. Due to the sufﬁciency in training data, we did not use label smoothing during training. In our experiments, enabling label smoothing resulted in poor development set performance across all the models. Training and Learning rate proﬁles of one model (6 encoder, 8 decoder layers) are shown in Figure 7. Float models are trained for 5 epochs, and binary models are trained for 9 epochs in two stages: ﬂoat stage and a binarization stage. An independent but identical learning rate schedules are used (with warmup) in both the stages of the binary model training. We note that a signiﬁcant amount of training (i.e. loss reduction) for binary models happens in the ﬁnal 10 steps when the learning rate is extremely small. Raw values of last 15 steps of learning rates are [5.0e-7, 3.1e-7, 1.6e-7, 6.4e-7, 1.0e-8, {2.5e-15}x10]. We also tune binary models with a constant learning rate of values in {1e-8, 1e-11, 1e-15}for the last epoch (overriding the original schedule), however we observe degradation in the quality (loss plateaus). This phenomenon of signiﬁcant learning in the ﬁnal stages of binary models’ training at extremely small learning rates is also observed by Liu et al. (2021); Zhang et al. (2022). We leave further investigation of this behavior to future work. A.3. Scaling Law Fit Scaling law ﬁt on all ten OOD evaluation datasets is shown in Figure 8. The slopes pe and pd are shown in Figure 9 and Table 3. B. Generation Quality Generation quality for decoder-scaling models is shown in Figure 10. We observe similar behavior as seen for encoder- scaling models in Section 4.3. BLEU scores for binary models are 2-3 BLEU points worse than the respective ﬂoat models at the same model depth. MBR-BLEURT based decoding quality increases consistently by increasing the sample size.Binarized Neural Machine Translation (a) Loss  (b) Learning Rate Figure 7.Test loss and learning rate proﬁles of a 6L8L ﬂoat and binary model as the training progresses. Dataset Float models Binary models pe pd pe pd Train Subset 0.18 0.31 0.16 0.28 Patents 0.20 0.30 0.19 0.32 Web Domain 1 0.14 0.25 0.14 0.27 Web Domain 2 0.19 0.37 0.16 0.30 Web Domain 3 0.12 0.18 0.14 0.23 Wikipedia 0.13 0.25 0.12 0.25 wmt-high-quality 0.20 0.31 0.18 0.30 wmt-refC 0.24 0.34 0.17 0.27 wmt-paraphrased 0.14 0.36 0.12 0.31 wmt-src-orig 0.22 0.37 0.23 0.36 wmt-tgt-orig 0.15 0.22 0.12 0.20 Table 3.Tabular representation of the same data (pe & pd) as shown in Figure 9.Binarized Neural Machine Translation Figure 8.BLEU scores on evaluation datasets deﬁned in Section 4.2Binarized Neural Machine Translation Figure 9.Encoder and Decoder scaling slopes (i.e. pe & pd) as per the scaling law deﬁned in Section 4.2. Raw values are shown in Table 3. (a) Beam Search BLEU  (b) MBR-BLEURT Figure 10.Comparison on translation qualities between binarized and bﬂoat16 models for decoder-scaling.",
      "meta_data": {
        "arxiv_id": "2302.04907v1",
        "authors": [
          "Yichi Zhang",
          "Ankush Garg",
          "Yuan Cao",
          "Łukasz Lew",
          "Behrooz Ghorbani",
          "Zhiru Zhang",
          "Orhan Firat"
        ],
        "published_date": "2023-02-09T19:27:34Z",
        "venue": "Published at NeurIPS 2023",
        "pdf_url": "https://arxiv.org/pdf/2302.04907v1.pdf"
      }
    },
    {
      "title": "Towards Accurate Post-training Network Quantization via Bit-Split and Stitching"
    },
    {
      "title": "Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization"
    },
    {
      "title": "Accurate Post Training Quantization With Small Calibration Sets"
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks",
      "abstract": "Training with larger number of parameters while keeping fast iterations is an\nincreasingly adopted strategy and trend for developing better performing Deep\nNeural Network (DNN) models. This necessitates increased memory footprint and\ncomputational requirements for training. Here we introduce a novel methodology\nfor training deep neural networks using 8-bit floating point (FP8) numbers.\nReduced bit precision allows for a larger effective memory and increased\ncomputational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We\nshow that, unlike previous 8-bit precision training methods, the proposed\nmethod works out-of-the-box for representative models: ResNet-50, Transformer\nand NCF. The method can maintain model accuracy without requiring fine-tuning\nloss scaling parameters or keeping certain layers in single precision. We\nintroduce two learnable statistics of the DNN tensors - shifted and squeezed\nfactors that are used to optimally adjust the range of the tensors in 8-bits,\nthus minimizing the loss in information due to quantization.",
      "full_text": "Published as a conference paper at ICLR 2020 SHIFTED AND SQUEEZED 8-BIT FLOATING POINT FOR - MAT FOR LOW-PRECISION TRAINING OF DEEP NEU- RAL NETWORKS L´eopold Cambier1∗†, Anahita Bhiwandiwalla2†, Ting Gong2, Mehran Nekuii2, Oguz H Elibol2 and Hanlin Tang2 1ICME, Stanford University 2Intel AI Lab lcambier@stanford.edu {anahita.bhiwandiwalla,ting.gong}@intel.com {mehran.nekuii,oguz.h.elibol,hanlin.tang}@intel.com ABSTRACT Training with larger number of parameters while keeping fast iterations is an in- creasingly adopted strategy and trend for developing better performing Deep Neu- ral Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit ﬂoating point (FP8) numbers. Re- duced bit precision allows for a larger effective memory and increased computa- tional speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out-of-the-box for representative models: ResNet-50, Transformer and NCF. The method can maintain model accuracy without requiring ﬁne-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learn- able statistics of the DNN tensors - shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization. 1 I NTRODUCTION Deep neural networks have achieved state-of-the-art performance on a wide variety of computer vision, audio, and natural language processing (NLP) tasks. This has resulted in an explosion of in- terest around techniques to reduce the memory footprint and energy consumption of neural network training and inference (Guo, 2018). Although there are a number of methods to address some of these issues for inference, the most effective method for training is using reduced precision numeri- cal formats. While 32-bit ﬂoating point (FP32) is the most common data format for neural network training, recent hardware have leveraged techniques that allow for training with 16-bit data formats (K ¨oster et al., 2017; Micikevicius et al., 2018). However, 8-bit precision training remains an open challenge (Johnson, 2018; Kalamkar et al., 2019). Current FP8 training methodologies (Wang et al., 2018; Mellempudi et al., 2019) require either specialized chunk-based accumulation, stochastic rounding techniques, loss scaling or maintaining some layers of the network in higher precision. Tuning these knobs is non-intuitive and requires signiﬁcant experimentation for each individual network. Accelerating the adoption of 8-bit data in training DNNs requires a hardware-friendly and out-of- the-box implementation of FP8. Due to the reduced number of mantissa bits, 8-bit multipliers are smaller and consume less power compared to higher bit representations. In this work we describe a novel 8-bit ﬂoating point (FP8) format - shifted and squeezed FP8 (S2FP8) - which has the following advantages compared to previously proposed 8-bit training methodologies: ∗Work performed during an internship at Intel †Equal contribution 1 arXiv:2001.05674v1  [cs.LG]  16 Jan 2020Published as a conference paper at ICLR 2020 •S2FP8 eliminates the need for loss scaling, which requires signiﬁcant tuning of the loss scale values and schedule for individual topologies •Leveraged by the forward and backward passes of model training, S2FP8 is effective in adjusting the range of gradients and also of activations and weights •S2FP8 does not require keeping the ﬁrst and last layer in FP32 precision, which is needed for other approaches (Mellempudi et al., 2019), however maintains the master weights and accumulations inside the matrix multipliers in FP32 We demonstrate across image classiﬁcation, translation, and recommendation models that S2FP8 outperforms previous 8-bit approaches, and reaches the accuracy of FP32 models without any addi- tional hyperparameter tuning. 2 R ELATED WORK The success of 32-bit ﬂoating point data type in training deep neural networks has increased interest in the feasibility of even lower precision training. The exponential demand for compute involved in training these deep neural networks has lead to multiple advancements in lower precision data types. Several studies have developed techniques such as loss scaling, stochastic rounding, and others to train effectively in 16-bit (Micikevicius et al., 2018; Das et al., 2018; Azim), along with associated hardware support (Markidis et al., 2018). Using 16-bit ﬁxed point, (Gupta et al., 2015) showed that stochastic rounding techniques were crucial for model convergence even for simple convolutional neural networks. As noted in (Kalamkar et al., 2019), Google’s bﬂoat16 format has the same number of exponent bits as FP32, leading the success of that format without commonly requiring hardware intensive requirements such as stochastic rounding or other framework level techniques such as loss scaling. Although 8-bit formats have signiﬁcant performance and memory advantages, convergence is es- pecially challenging due to loss of accuracy in the backpropogated gradient values. Wang et al. (2018) demonstrated training models with matrix multiplications and convolutions in FP8 but they use FP16 with chunk-based accumulations and stochastic rounding hardware. Mellempudi et al. (2019) also demonstrated success with FP8, accumulating in FP32 and using loss scaling techniques on ResNets, Transformer and GNMT networks. However, they too require the ﬁrst and last layers of the model to be in FP32, and similar to (Banner et al., 2018) leverage Stochastic Rounding tech- niques to maintain model accuracy. Unlike S2FP8 proposed in this work, both of these FP8 training techniques emphasize the need for efﬁcient loss scaling, rounding hardware and restriction on some layers being in higher precision. Zhou et al. (2016) quantized weights, activations and gradients of AlexNet (Krizhevsky et al., 2012) to 1, 2 and 6 bits respectively. But they also need to maintain the ﬁrst and last convolution layers in full precision and stochastically quantize the gradients. Wu et al. (2018) demonstrate using integers for training LeNet-5 (LeCun et al., 1998) and AlexNet with 8-bits for activations, error and gradi- ents and 2-bits for weights. However, these approaches also required custom tuning such as novel initialization techniques and layer wise scaling instead of Batch Normalization and Softmax. These approaches lack generalizability to other models, requiring signiﬁcant ﬁne tuning. To the best of our knowledge, there does not exist an out-of-the-box solution using FP8 in training deep learning topologies without the need for tuned loss scaling techniques, requirements of cer- tain layers being in full precision along with efﬁcient hardware rounding schemes like Stochastic Rounding. 3 S HIFTED AND SQUEEZED 8-BIT FLOATING POINT FORMAT 3.1 C HALLENGES OF 8-BIT FLOATING POINT FORMAT The FP8 format, with 2 bits of mantissa and 5 bits of exponent (Mellempudi et al., 2019) is both nar- row (i.e., its dynamic range is very limited, from 2−16 to 216) and has lower accuracy (the machine epsilon is only 2−3). Figure A1 illustrates the range and accuracy of FP8. In contrast, FP32 ranges from 2−149 to 2128 with a machine-epsilon of 2−24 (Table A1). 2Published as a conference paper at ICLR 2020 Figure 1: The distribution of tensor elements over the course of training for three tensors from the Transformer tiny model on the English-Vietnamese translation dataset. Blue bar indicates the representable range of FP8. Left: Many of the tensor elements fall outside of FP8’s representable range. Center: Few tensor elements fall outside of FP8’s representable range. Right: Initially, most elements are within FP8’s representable range, but after training, many fall outside of the representable range On the other hand, tensors involved in neural networks (weights, activations and gradients) are spread across varying scales. As illustrated in Figure 1, the tensor distributions change over the course of training, spanning different orders of magnitude. As a result, 8-bit training usually requires a combination of multiple techniques to capture the full dynamic range of values for model training. Some of these techniques include: • Loss scaling (Micikevicius et al., 2018) scales the loss L(w) by a constant λbefore back- propagation . This makes the gradients artiﬁcially larger, allowing them to ﬁt within the FP8 range. Gradients are then scaled down before being accumulated into the trainable weights as shown in Equation 6 • Stochastic rounding (Maxﬁeld, 2006) alleviate quantization errors by capturing some of the information discarded when truncating to lower precision at the output of a GEMM operation Between these two techniques, loss scaling is more critical; once the magnitude of the gradients can no longer be represented in the FP8 range, training convergence will not be possible. However, loss scaling only modiﬁes the gradients. Weights and activations can also (albeit admittedly less frequently) exceed the FP8’s representable range of[2−16,216]. In those scenarios, convergence can also be affected. The issue with loss scaling is that it requires user interaction. Models have to be modiﬁed, and, more importantly, tedious empirical tuning is required to ﬁnd the correct loss scaling schedule. While some networks can be trained with constant loss scaling, some, notably Transformers (Mellempudi et al., 2019), require dynamic “back-off” and improved loss scaling. This requires signiﬁcant trial and error to tune the scaling schedule, slowing down wide adoption of low-precision numerical formats. 3.2 S HIFTED AND SQUEEZED FP8 To alleviate these issues and make neural network training possible with no model modiﬁcations or hyperparameter tuning, we propose a new 8-bit ﬂoating point format. Consider a tensor X of size N, i.e., X = {Xi}N i=1. Instead of directly encoding each Xi in FP8, we store X using N FP8 numbers {Yi}N i=1 accompanied by two (squeeze and shift) factors αand β (the “statistics” — see Figure 2). Figure 2: The S2FP8 format. A tensor Xof N numbers is represented by α, βand N FP8 numbers Y, related to X through Equation 1. 3Published as a conference paper at ICLR 2020 -16 0 16 log2 |Y| (a) Y, the usual FP8 distribution. 0 32 log2 |X| (b) X, for α= 1and β <0 -32 0 32 log2 |X| (c) X, for α< 1 and β = 0 Figure 3: Impact of the Shifted and Squeezed transformation log2 |Y|= αlog2 |X|+ β. αlet the distribution be as wide as necessary (though, with an associated loss of precision), and βlet us shift the distribution around any value. For Xi ̸= 0, X and Y are then related through log2(|Yi|) = αlog2(|Xi|) + β ⇔Yi = ±2β|Xi|α (1) where the ±is chosen so that Xi and Yi have the same sign. This representation allows for αand βbe chosen so that together with tensor Y they capture most of the dynamic range of the tensor X. As we will see in section 4, this is all that is necessary to train networks using 8-bit ﬂoating point numbers. In order for Y to be a tensor suitable to be represented by FP8 numbers, we enforce that it has zero mean and a maximum value within the dynamic range of FP8 (e.g. 15): N′ ∑ i=1 log2(|Yi|) = 0 and max i=1,...,N′ log2(|Yi|) = 15(= log2(215)) (2) where the ′notation indicates that the sum and the max, respectively, ignore any isuch that Yi = 0. Those equations ensure that log2(|Y|) values are distributed with zero mean and each is less than 15, which is ideal for an FP8 format. By inserting Equation 2 into Equation 1, and by denoting µ= N′ ∑ i=1 log2(|Xi|) and m= max i log2(|Xi|) (3) we ﬁnd α= 15 m−µ, β = −αµ (4) This new tensor format results in the training procedure (forward pass, backward pass, weight up- date) described in Figure 4. Forward and backward MatMul use this new S2FP8 format. Master weights are kept in FP32 and updated using S2FP8 gradients. Accumulations inside the GEMM kernel are kept in full FP32 precision. Figure 3 illustrates the impact of αand β. By having those two extra degrees of freedom for each tensor, majority of the dynamic range of each tensor can now be captured, whether very small ( β >0), very large ( β <1), very narrow ( α >1)) or very wide (α< 1). 3.3 L EARNING THE TENSOR DISTRIBUTION One way to interpret αand βis to consider them as parameters of a distribution generating the ten- sor values log2(|Xi|). We can then say that, by continuously computing αand β, we are effectively learning the distribution of log2(|Xi|). Figure 5c shows the evolution of µ, m, αand βfor a partic- ular tensor of ResNet-20. We see that αand β converge to, approximately, 5 and 21, respectively. From Equation 1, we conclude that: 4Published as a conference paper at ICLR 2020 FP32àS2FP8 T T T Master weights layer ℓ (FP32) Weights  gradients layer ℓ (S2FP8) Loss gradients layer ℓ (S2FP8) Activations  layer ℓ (S2FP8 ) Activations layer ℓ+1 (S2FP8) Loss gradients layer ℓ+1 (S2FP8) ⨉Σ ⨉Σ Σ⨉ T Update FWD GEMM WG GEMM FP32àS2FP8 FP32àS2FP8 FP32àS2FP8FP32 FP32 FP32 BWD GEMM Figure 4: Low precision training with S2FP8. T represent the truncation described in Equation 5, from FP32 to S2FP8. When using S2FP8 for training, forward and backward GEMM’s only use S2FP8. The master weights are kept in FP32 and updated during the update step. • since α> 1, this means that X is expanded into Y, i.e., X is more narrow than what FP8 allows •since β >0, this means that X is right-shifted into Y, i.e., X is smaller than what FP8 allows At convergence, thoseαand βvalues represent the distribution of each converged tensor. Notice that all statistics stabilize in the last third of the training, where the learning rate is decreased, indicating the network is converging to its ﬁnal state. 4 E XPERIMENTAL RESULTS In this section, we compare S2FP8 training with baseline FP32 and FP8 training with and with- out loss scaling for: Residual Networks (He et al., 2016) of varying depths on the CIFAR-10 and ImageNet (Deng et al., 2009) datasets, Transformer (Vaswani et al., 2017) on IWSLT’15 English- Vietnamese dataset (Luong & Manning, 2015), and Neural Collaborative Filtering (NCF) (He et al., 2017) on MovieLens 1 Million dataset (Harper & Konstan, 2016). For our experiments, we use the open source Tensorﬂow Models 1 repository for ResNet and NCF, Tensor2Tensor (Vaswani et al., 2018) for Transformer with added S2FP8 data type simulation sup- port using the methodology described in subsection 4.1. For a given model, we keep the hyperpa- rameters consistent across FP32, FP8 and S2FP8 evaluations. 4.1 S IMULATION METHODOLOGY We simulated S2FP8 by inserting appropriate truncation function throughout the network, before and after every convolution and matrix-matrix product operations, during both the forward and backward passes. The rest of the network is kept in FP32, and those truncation simulate the low-precision training described in subsection 3.2. The truncation function takes as input a tensor X, computes its magnitude mean and maximum, computes the appropriate αand βand ﬁnally truncates X by computing Xtruncated = [ 2−β{ truncateFP8 ( 2β|X|α)}]1/α (5) where truncateFP8 is a usual FP8 truncation function with RNE (round-to-nearest, with ties broken by rounding to even) rounding which is easier to implement and most widely supported in hardware. 1https://github.com/tensorflow/models 5Published as a conference paper at ICLR 2020 (a) Distribution of the magnitude log2(|X|) of original tensor Xbefore scaling using αand β (b) Distribution of the magnitude log2(|Y|) of shifted and squeezed tensor Y with |Yi| = 2β|Xi|α 0 50k 100k −4.6 −4.4 −4.2 −4 −3.8 Step µ 0 50k 100k −3 −2 −1 Step m 0 50k 100k 4 6 8 Step α 0 50k 100k 20 30 40 Step β (c) The computed statistics during training for the scale (β), shift (α), as well as the mean of the log values (µ) and the maximum log value (m). Figure 5: Evolution of the average and maximum magnitude, as well asαand βfor CIFAR-10 with ResNet-20. This illustrates how the network is actually implicitly learning the tensors distribution, by repeatedly computing magnitudes αand βthrough µand m. 4.2 R ESIDUAL NETWORKS We ﬁrst present results with Residual Networks of varying depths on the CIFAR-10 image recogni- tion dataset. We trained the model on 1 GPU using standard parameters: 250 epochs, batchsize of 128, SGD with momentum of 0.9, initial learning rate of 0.1 decreased by a factor of 10 after epochs 100, 150 and 200. Table 1 and Figure A2 presents the results. We observe that S2FP8 reaches almost exactly the FP32 baseline, sometimes even improving over it. Out-of-the-box FP8 does not converge and has very poor accuracy. Finally, FP8 with constant loss scaling of 100 (FP8+LS(100)) can reach the baseline. Both S2FP8 and FP8+LS(100) have similar performances, but S2FP8 can do so without any extra hyperparameters or tuning from the user’s perspective. CIFAR-10 FP32 S2FP8 ∆ FP8 FP8+LS(100) ResNet-20 91.5 91.1 0.4 17.9 91.1 ResNet-34 92.5 92.0 0.5 13.5 92.0 ResNet-50 93.0 93.2 -0.2 11.5 92.9 Table 1: Validation accuracy (in %) for image recognition on CIFAR-10 with ResNet-20/34/50. We also evaluate S2FP8 on the 1000 class ImageNet dataset. Here, we trained the network on 4 GPUs using standard parameters: 90 epochs, batchsize of 256, SGD with momentum of 0.9, initial learning rate of 0.1 decreased by a factor of 10 after epochs 30, 60, 80 and 90. Table 2 and Figure 6 present the results. Again, we observe that S2FP8 gets very close to the FP32 baseline. Out-of-the-box FP8 quickly diverges and does not converge at all. For FP8 with loss scaling to converge, one has to not truncate the ﬁrst and last layer, as consistent with (Mellempudi et al., 2019), which we denote as Ex in Table 2 below. A loss scaling of 10,000 can then be used to reach the baseline (FP8+LS(10k)+Ex). Finally, stochastic rounding can be added and it slightly improves the precision (FP8+LS(100k)+Ex+SR). However, both those cases are not out-of-the-box, as they require loss scaling tuning and some layers 6Published as a conference paper at ICLR 2020 to be kept in full precision. S2FP8 does not suffer from that, thanks to its improved quantization: all layers can be truncated and no loss scaling is required. Imagenet1k FP32 S2FP8 ∆ FP8 FP8+LS(10k)+Ex FP8+LS(100k)+Ex+SR ResNet-18 70.3 69.6 -0.7 NaN 68.7 68.9 ResNet-50 76.2 75.2 -1.0 NaN 75.3 75.5 Table 2: Validation accuracy (in %) for image recognition on Imagenet1k with ResNet-18/50 0 250k 500k 20 40 60 80 Step Top-1 accuracy (%) FP32 S2FP8 0 250k 500k 2 4 6 8 Step Loss FP32 S2FP8 0 250k 500k 0.4 0.6 0.8 1 1.2 Step L2 Loss FP32 S2FP8 Figure 6: Comparing Top-1 accuracy and Loss of S2FP8 with FP32 for ResNet-50 on Imagenet1k 4.3 T RANSFORMER We also tested S2FP8 on a small Transformer (Transformer Tiny) on the English-Vietnamese dataset. The model has 2 hidden layers of size 128, and a ﬁlter of size 512, and is trained using Adam optimizer (Kingma & Ba, 2014). Table 3 and Figure 7 show the result, where we compare FP32, S2FP8 and FP8 with exponential loss scaling. We tried many loss scaling schedules (constant and exponential, with various initializations) and report the best result. As one can see, S2FP8 reaches the baseline with no hyperparameter tuning. FP8, on the other hand, does not, even after extensive loss scaling tuning. This shows the value of an out-of-the-box method for the user. En-Vi FP32 S2FP8 ∆ FP8 FP8+LS(exp) Transformer tiny 25.3 25.3 0.0 NaN 21.3 Table 3: BLEU Score (Papineni et al., 2002) (from 0 to 100) for translation task on the English- Vietnamese dataset with Transformer tiny. 4.4 N EURAL COLLABORATIVE FILTERING The Neural Collaborative Filtering (NCF) network comprises of embeddings for users and items from the MovieLens dataset, that are then passed to a Multi-Layer Perceptron(MLP) network to learn the user-item interaction. Matrix-multiplication operations are the building blocks of such models. We compare S2FP8 with FP32 and FP8 without loss scaling. We simulate Matrix-Multiplications and look-ups from the embeddings in S2FP8 and compare it to FP8 with RNE. We trained the model on the MovieLens 1 Million dataset with the following standard paramaters: 20 iterations, batchsize of 1024 on 4 GPUs, 8 predictive factors, learning rate of 0.0005 using the Adam optimizer. Figure 8 and Table 4 show the result, where we compare FP32, S2FP8 and FP8 without loss scaling. This again shows that S2FP8 easily reaches the baseline out-of-the-box, without tuning of any sort. FP8 gets relatively close, but cannot reach the baseline. 7Published as a conference paper at ICLR 2020 0 125k 250k 5 10 15 20 25 Step BLEU Score FP32 S2FP8 0 125k 250k 2 4 6 Step Loss FP32 S2FP8 Figure 7: Comparing BLEU score and Loss of S2FP8 and FP32 for Transformer tiny on En-Vi dataset 1 10 20 0.5 0.55 0.6 0.65 Iteration Hit Ratio FP32 S2FP8 1 10 20 0.3 0.35 0.4 Iteration NDCG FP32 S2FP8 1 10 20 0.2 0.25 0.3 Iteration Loss FP32 S2FP8 Figure 8: Comparing Hit Ratio, NDCG and Loss of S2FP8 and FP32 for NCF on MovieLens-1M 5 H ARDWARE ASPECTS S2FP8 is a new data type and requires its own circuitry to be implemented in a tensor processing en- gine. However, the added overhead is very minimal and affects neither data throughput nor compute speed. In order to convert FP32 tensors into S2FP8, two hardware (HW) components are needed. One is to calculate each tensor’s statistics (Equation 3), which bring minimal HW complexity. To make compute operations even easier these statistics could be stored in lower precision such as FP8/INT8. The other component is to adjust the exponent and mantissa of all those tensor elements by applying the squeeze ( α) and shift ( β) factors in Equation 4 before truncating them into their 8-bit placeholders. The shift could be done using simple element-wise add/subtract operations on the exponents, and element-wise squeeze could be applied to the mantissa portions. Another con- sideration is within the tensor processing engine(e.g., GEMM engine) which requires the αand β factors while doing the calculations. The FP32 result will be converted back to S2FP8 when needed (e.g., to store back in memory) as shown in Figure 4. 6 C ONCLUSION We introduce a novel 8-bit ﬂoating point data type (S2FP8), that gives competitive performance in comparison to state-of-the-art FP32 baselines over a range of representative networks. S2FP8 makes use of shifted and squeezed factors to shift and rescale the range of tensors prior to truncation. S2FP8 allows training of neural networks with an 8-bit format while eliminating the need for loss scaling tuning, hardware-complex rounding techniques. In addition, compared to existing FP8 implemen- tations we also eliminate the restriction of maintaining the ﬁrst and last layers in FP32. Decreasing Movielens 1 million FP32 S2FP8 ∆ FP8 NCF 0.666 0.663 0.003 0.633 Table 4: HR Score for NCF on the Movielens 1 million dataset. 8Published as a conference paper at ICLR 2020 the number of bits enables larger models to ﬁt on a single device and results in faster training. As part of future work, we plan to extend the use of S2FP8 to train additional DNN topologies and also simplify the squeeze and shift statistics from a hardware implementation point of view. We also plan to explore the use of reduced precision to store the statistics and the extendability of this ap- proach to efﬁciently represent a broader suite of low precision formats like 8-bit POSIT (Gustafson & Yonemoto, 2017), 4-bit ﬂoating and integer data types. ACKNOWLEDGMENTS We would like to thank Naveen Mellempudi, Pratap Prasad, Prasanna Singamsetty and Cory Stephenson for insightful discussions. REFERENCES Anwarul Azim. Low precision arithmetic operations in deep neural networks: An overview. Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems, pp. 5145–5153, 2018. Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas, et al. Mixed precision training of convolutional neural networks using integer operations. arXiv preprint arXiv:1802.00930, 2018. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi- erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Yunhui Guo. A survey on methods and theories of quantized neural networks. CoRR, abs/1808.04752, 2018. URL http://arxiv.org/abs/1808.04752. Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746, 2015. John L Gustafson and Isaac T Yonemoto. Beating ﬂoating point at its own game: Posit arithmetic. Supercomputing Frontiers and Innovations, 4(2):71–86, 2017. F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):19, 2016. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col- laborative ﬁltering. In Proceedings of the 26th international conference on world wide web, pp. 173–182. International World Wide Web Conferences Steering Committee, 2017. Jeff Johnson. Rethinking ﬂoating point for deep learning. CoRR, abs/1811.01721, 2018. URL http://arxiv.org/abs/1811.01721. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja V ooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bﬂoat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Urs K ¨oster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K Bansal, William Constable, Oguz Elibol, Scott Gray, Stewart Hall, Luke Hornof, et al. Flexpoint: An adaptive numerical format for efﬁcient training of deep neural networks. In Advances in neural information processing systems, pp. 1742–1752, 2017. 9Published as a conference paper at ICLR 2020 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012. Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Minh-Thang Luong and Christopher D. Manning. Stanford neural machine translation systems for spoken language domain. In International Workshop on Spoken Language Translation, Da Nang, Vietnam, 2015. Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, and Jeffrey S Vetter. Nvidia tensor core programmability, performance & precision. In 2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pp. 522–531. IEEE, 2018. Clive Maxﬁeld. An introduction to different rounding algorithms. Programmable Logic Design Line, pp. 1–15, 2006. Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision train- ing with 8-bit ﬂoating point. arXiv preprint arXiv:1905.12334, 2019. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pp. 311–318, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10. 3115/1073083.1073135. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Train- ing deep neural networks with 8-bit ﬂoating point numbers. In Advances in neural information processing systems, pp. 7675–7684, 2018. Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680, 2018. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 10Published as a conference paper at ICLR 2020 A A PPENDIX A.1 SUPPLEMENTARY TABLES AND FIGURES Format Bits s/e/m Min sub- normal Min nor- mal (Approx.) Max normal Machine epsilon Range IEEE-FP32 32 1/8/23 2−149 2−126 2128 2−24 2277 IEEE-FP16 16 1/5/10 2−24 2−14 216 2−11 240 BF16 16 1/8/7 2−133 2−126 2128 2−8 2261 FP8 8 1/5/2 2−16 2−14 216 2−3 232 Table A1: Comparing several ﬂoating point formats. s/e/m indicates the number of sign (s), exponent (e) and mantissa (m) bits. Models Datasets FP32 BF16 FP8 FP8+other recipes S2FP8 ResNet-20 CIFAR-10 91.5 91.7 17.9 91.1(Loss Scale=100) 91.1 ResNet-50 CIFAR-10 93.0 93.2 11.5 92.9(Loss Scale=100) 93.2 ResNet-50 ImageNet 76.2 76.5 NaN 75.3(Loss Scale=10K, FP32 for ﬁrst and last layers) 75.2 NCF MovieLens1M 0.666 0.653 0.633 - 0.663 Transformer- tiny En-Vi 25.3 25.6 NaN 21.3(Loss Scale=Exp) 25.3 Table A2: Comparing FP32, BF16, vanilla FP8, FP8 with tuning and S2FP8 on the model ResNet(Top1-accuracy), NCF(Hit Ratio),Transformer-tiny(BLEU score). −16 −8 0 8 16 1 2 3 4 log2(|X|) Numbers density Figure A1: The range and precision of FP8. Bar indicate the number density between each power of 2. Since FP8 has 2 mantissa bit, the density is 4 (except in the denormals), and the associated machine epsilon is 2−3 = 1/8. The normal representable range goes from 2−14 to (1 −2−3)216, with denormals from 2−16 to 2−14. A.2 S UPPLEMENTARY EQUATIONS ∂(λL) ∂w (w) = λ∂L ∂w(w) ⇒w(k+1) = w(k) −α1 λ ∂(λL) ∂w (w(k)). (6) 11Published as a conference paper at ICLR 2020 0 50k 100k 60 80 100 Step Top-1 accuracy (%) FP32 S2FP8 0 50k 100k 0 1 2 3 Step Loss FP32 S2FP8 0 50k 100k 0.2 0.3 Step L2 Loss FP32 S2FP8 Figure A2: Convergence of ResNet-50 with the CIFAR-10 dataset 12",
      "meta_data": {
        "arxiv_id": "2001.05674v1",
        "authors": [
          "Léopold Cambier",
          "Anahita Bhiwandiwalla",
          "Ting Gong",
          "Mehran Nekuii",
          "Oguz H Elibol",
          "Hanlin Tang"
        ],
        "published_date": "2020-01-16T06:38:27Z",
        "pdf_url": "https://arxiv.org/pdf/2001.05674v1.pdf"
      }
    },
    {
      "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction",
      "abstract": "We study the challenging task of neural network quantization without\nend-to-end retraining, called Post-training Quantization (PTQ). PTQ usually\nrequires a small subset of training data but produces less powerful quantized\nmodels than Quantization-Aware Training (QAT). In this work, we propose a novel\nPTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to\nINT2 for the first time. BRECQ leverages the basic building blocks in neural\nnetworks and reconstructs them one-by-one. In a comprehensive theoretical study\nof the second-order error, we show that BRECQ achieves a good balance between\ncross-layer dependency and generalization error. To further employ the power of\nquantization, the mixed precision technique is incorporated in our framework by\napproximating the inter-layer and intra-layer sensitivity. Extensive\nexperiments on various handcrafted and searched neural architectures are\nconducted for both image classification and object detection tasks. And for the\nfirst time we prove that, without bells and whistles, PTQ can attain 4-bit\nResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster\nproduction of quantized models. Codes are available at\nhttps://github.com/yhhhli/BRECQ.",
      "full_text": "Published as a conference paper at ICLR 2021 BRECQ : PUSHING THE LIMIT OF POST -TRAINING QUANTIZATION BY BLOCK RECONSTRUCTION Yuhang Li12∗, Ruihao Gong2∗, Xu Tan2, Yang Yang2, Peng Hu2, Qi Zhang2, Fengwei Yu2, Wei Wang, Shi Gu1\u0000 1University of Electronic Science and Technology of China, 2SenseTime Research liyuhang699@gmail.com, gongruihao@sensetime.com, gus@uestc.edu.cn ABSTRACT We study the challenging task of neural network quantization without end-to- end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ frame- work, dubbed BRECQ , which pushes the limits of bitwidth in PTQ down to INT2 for the ﬁrst time. B RECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross- layer dependency and generalization error. To further employ the power of quan- tization, the mixed precision technique is incorporated in our framework by ap- proximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both im- age classiﬁcation and object detection tasks. And for the ﬁrst time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 com- parable with QAT and enjoy 240×faster production of quantized models. Codes are available at https://github.com/yhhhli/BRECQ. 1 I NTRODUCTION The past decade has witnessed the rapid development of deep learning in many tasks, such as com- puter vision, autonomous driving, etc. However, the issue of huge computation cost and memory footprint requirements in deep learning has received considerable attention. Some works such as neural architecture search (Zoph & Le, 2016) try to design and search a tiny network, while oth- ers, like quantization (Hubara et al., 2017), and network pruning (Han et al., 2015) are designed to compress and accelerate off-the-shelf well-trained redundant networks. Many popular quantization and network pruning methods follow a simple pipeline: training the original model and then ﬁnetune the quantized/pruned model. However, this pipeline requires a full training dataset and many computation resources to perform end-to-end backpropagation, which will greatly delay the production cycle of compressed models. Besides, not all training data are always ready-to-use considering the privacy problem. Therefore, there is more demand in industry for quan- tizing the neural networks without retraining, which is called Post-training Quantization. Although PTQ is fast and light, it suffers from severe accuracy degeneration when the quantization precision is low. For example, DFQ (Nagel et al., 2019) can quantize ResNet-18 to 8-bit without accuracy loss (69.7% top-1 accuracy) but in 4-bit quantization, it can only achieve 39% top-1 accuracy. The primary reason is the approximation in the parameter space is not equivalent to the approximation in model space thus we cannot assure the optimal minimization on the ﬁnal task loss. Recent works like (Nagel et al., 2020) recognized the problem and analyzed the loss degradation by Taylor series expansion. Analysis of the second-order error term indicates we can reconstruct each layer output to approximate the task loss degeneration. However, their work cannot further quantize the weights into INT2 because the cross-layer dependency in the Hessian matrix cannot be ignored when the perturbation on weight is not small enough. In this work, we analyze the second-order ∗Equal contribution, \u0000 Corresponding author. 1 arXiv:2102.05426v2  [cs.LG]  25 Jul 2021Published as a conference paper at ICLR 2021 error based on the Gauss-Newton matrix. We show that the second-order error can be transformed into network ﬁnal outputs but suffer from bad generalization. To achieve the best tradeoff, we adopt an intermediate choice, block reconstruction. In addition, our contributions are threefold: 1. Based on the second-order analysis, we deﬁne a set of reconstruction units and show that block reconstruction is the best choice with the support from theoretical and empirical evidence. We also use Fisher Information Matrix to assign each pre-activation with an importance measure during reconstruction. 2. We incorporate genetic algorithm and the well-deﬁned intra-block sensitivity measure to generate latency and size guaranteed mixed precision quantized neural networks, which fulﬁlls a general improvement on both specialized hardware (FPGA) and general hardware (ARM CPU). 3. We conduct extensive experiments to verify our proposed methods. We ﬁnd that our method is applicable to a large variety of tasks and models. Moreover, we show that post-training quanti- zation can quantize weights to INT2 without signiﬁcant accuracy loss for the ﬁrst time. 2 P RELIMINARIES Notations Vectors are denoted by small bold letters and matrices (or tensors) are denoted by capital bold letters. For instance, W and w represent the weight tensor and its ﬂattened version. Bar accent denotes the expectation over data points, e.g.¯a. Bracketed superscript w(ℓ) indicates the layer index. For a convolutional or a fully-connected layer, we mark its input and output vectors byx and z. Thus given a feedforward neural network with nlayers, we can denote the forward process by x(ℓ+1) = h(z(ℓ)) = h(W(ℓ)x(ℓ) + b(ℓ)), 1 ≤ℓ≤n, (1) where h(·) indicates the activation function (ReLU in this paper). For simplicity, we omit the anal- ysis of bias b(ℓ) as it can be merged into activation. ||·||F denotes the Frobenius norm. Quantization Background Uniform symmetric quantization maps the ﬂoating-point numbers to several ﬁxed-points. These points (or grids) have the same interval and are symmetrically dis- tributed. We denote the set that contains these grids asQu,sym b = s×{−2b−1,..., 0,..., 2b−1 −1}. Here, sis the step size between two grids and bis the bit-width. Quantization function, denoted by q(·) : R→Q u,sym b , is generally designed to minimize the quantization error: min ||ˆw −w||2 F.s.t. ˆw ∈Qu,sym b (2) Solving this minimization problem, one can easily get theq(·) by leveraging the rounding-to-nearest operation ⌊·⌉. Rounding-to-nearest is a prevalent method to perform quantization, e.g. PACT (Choi et al., 2018). However, recently some empirical or theoretical evidence supports that simply mini- mizing the quantization error in parameter space does not bring optimal task performances. Specif- ically, Esser et al. (2020) propose to learn the step size sby gradient descent in quantization-aware training (QAT). LAPQ (Nahshan et al., 2019) ﬁnds the optimal step size when the loss function is minimized without re-training the weights. Their motivations are all towards minimizing a ﬁnal objective, which is the task loss, i.e., min E[L( ˆw)], s.t. ˆw ∈Qu,sym b . (3) While this optimization objective is simple and can be well-optimized in QAT scenarios, it is not easy to learn the quantized weight without end-to-end ﬁnetuning as well as sufﬁcient training data and computing resources. In post-training quantization settings, we only have full precision weights that w⋆ = arg minw E[L(w)] where w ∈R and a small subset of training data to do calibration. Taylor ExpansionIt turns out that the quantization imposed on weights could be viewed as a special case of weight perturbation. To quantitatively analyze the loss degradation caused by quantization, Nagel et al. (2020) use Taylor series expansions and approximates the loss degradation by E[L(w + ∆w)] −E[L(w)] ≈∆wT¯g(w) + 1 2∆wT ¯H(w)∆w, (4) where ¯g(w) = E[∇wL] and ¯H(w) = E[∇2 wL] are the gradients and the Hessian matrix and ∆w is the weight perturbation. Given the pre-trained model is converged to a minimum, the gradients can be safely thought to be close to0. However, optimizing with the large-scale full Hessian is memory- infeasible on many devices as the full Hessian requires terabytes of memory space. To tackle this problem, they make two assumptions: 2Published as a conference paper at ICLR 2021 1. Layers are mutual-independent, thus the Hessian is in the form of layer-diagonal1 and Kronecker- factored, i.e., ¯H(w(ℓ)) = E[x(ℓ)x(ℓ),T ⊗H(z(ℓ))], where ⊗is the Kronecker product. 2. The second-order derivatives of pre-activations are constant diagonal matrix (H(z(ℓ)) = c×I) which is independent of input data points. At last, the objective is transformed into a practical proxy signal, the change in feature-maps (z = Wx), and the quantized model can be obtained by a layer-by-layer feature map reconstruc- tion algorithm (with few calibration images). Recent works, like Bit-Split (Wang et al., 2020) and AdaQuant (Hubara et al., 2020), also take this layer-wise objective to improve the post-training quantization. However, they failed to quantize weights to INT2. We think the inherent reason is that when ∆w grows higher, the former assumptions do not hold and an accurate signal is required. 3 P ROPOSED METHOD 3.1 C ROSS -LAYER DEPENDENCY Denote the neural network output z(n) = f(θ), the loss function can be represented by L(f(θ)) where θ = vec[w(1),T,..., w(n),T]T is the stacked vector of weights in all nlayers. The Hessian matrix can be computed by ∂2L ∂θi∂θj = ∂ ∂θj (m∑ k=1 ∂L ∂z(n) k ∂z(n) k ∂θi ) = m∑ k=1 ∂L ∂z(n) k ∂2z(n) k ∂θi∂θj + m∑ k,l=1 ∂z(n) k ∂θi ∂2L ∂z(n) k ∂z(n) l ∂z(n) l ∂θj , (5) where z(n) ∈ Rm. Since the pretrained full precision model is converged to a local minimum, we can assume the Hessian is positive-semideﬁnite (PSD). Speciﬁcally, the converged model has ∇z(n) Lclose to 0 so the ﬁrst term in Eq. (5) is neglected and Hessian becomes the Gauss-Newton (GN) matrix G(θ). GN matrix can be written in matrix form (Botev et al., 2017) as H(θ) ≈G(θ) = Jz(n) (θ)TH(z(n))Jz(n) (θ), (6) where Jz(n) (θ) is the Jacobian matrix of the network output with respect to the network parameters. However, in practice, we cannot explicitly compute and store the Jacobian for each input data point in such a raw form. To reduce the computation and memory budget, we will transform the second- order error into the network output, as shown in the following theorem. Theorem 3.1. Consider an n-layer feedforward neural network with ReLU activation function. Assuming all weights are quantized, the second-order error optimization can be transformed by: arg min ˆθ ∆θT ¯H(θ)∆θ≈arg min ˆθ E [ ∆z(n),TH(z(n))∆z(n) ] . (7) Remark 3.1. The same transformation is also applicable for activation quantization. The quadratic loss is deﬁned as E[∆γTH(γ)∆γ] where ∆γ = vec[∆x(1),T,..., ∆x(n),T]T. We prove the theorem using the quadratic form, details can be found in Appendix A.1. Here we provide a sketch of the proof by matrix form. The product of perturbation and Jacobian can be thought as the ﬁrst-order Taylor approximation of the change in network output ∆z(n): ∆z(n) = ˆz(n) −z(n) ≈Jz(n) (θ)∆θ. (8) Therefore, combining Eq. (8) and Eq. (6) we can transform the large-scale second-order error into the change in network outputs characterized by the output Hessian H(z(n)). The theorem indicates a simple observation, suppose a well-trained teacher model and an initialized student model, we can minimize their discrepancy by reconstructing the network’s ﬁnal output z(n), which coincides with and generalizes the distillation (Hinton et al., 2015; Polino et al., 2018). LAPQ (Nahshan et al., 2019) also considers the dependency but their optimization does not rely on second-order information. However, we should emphasize that distillation requires the same computation and data resources as in normal training procedure, which is impractical for PTQ with limited data. 1To prevent ambiguity, we hereby use layer-diagonal Hessian to replace the common name “block-diagonal Hessian” because the block in this paper means a building block in the CNNs. 3Published as a conference paper at ICLR 2021 stem Network Structure                             Body Structure                             Stage Structure                     Block Structure (w, h, 3) body (w/2, h/2, c0) head (w/32, h/32, c4) (1, 1, #classes) stage 1 (w/2, h/2, c0) stage 2 (w/4, h/4, c1) stage 4 ... ... (w/32, h/32, c4) block 1 (w, h, ci) block 2 (w/2, h/2, ci+ 1) block n ... ... (w/2, h/2, ci+ 1) layer 1 (w, h, ci) layer 2 (w, h, ci*e) layer 3 (w, h, ci*e) (w, h, ci) + (a) A typical structure of CNN (taken from Radosavovic et al. (2020)). Network is composed of a stem layer (ﬁrst convolu- tion on input images), a body and a head layer (average pool- ing with a fully connected layer). A body contains several stages, and a stage contains several blocks. A representative block is the bottleneck block with residual path. layer-diagonal  block-diagonal  Full matrix (b) An example illustration of Hessian (or Fisher) matrix. Blue sub-block means the layer-diagonal and each layer are mutual- independent; orange sub-block consider the dependency inside a building block and green parts measure all dependencies. Figure 1: We deﬁne 4 kinds of reconstruction granularity, namely net-wise, stage-wise, block-wise and layer- wise optimization, each of them corresponds an essential component of CNN. 3.2 B LOCK RECONSTRUCTION Although the network output reconstruction has an accurate estimation of the second-order error, we ﬁnd in practice it is worse than the layer-by-layer reconstruction in PTQ. The primary reason for this is optimizing the whole networks over 1024 calibration data samples leads to over-ﬁtting easily. As Jakubovitz et al. (2019) explained, the networks can have perfect expressivity when the number of parameters exceeds the number of data samples during training, but lower training error does not ensure lower test error. We ﬁnd layer-wise reconstruction acts like a regularizer which reduces the generalization error by matching each layer’s output distribution. In other words, both layer-wise and network-wise output reconstruction has their own drawbacks. And there should be a better bias-variance trade-off choice to conduct reconstruction at an intermediate granularity. The layer-wise optimization corresponds to layer-diagonal Hessian (Fig. 1b blue parts) and the network-wise optimization corresponds to full Hessian (Fig. 1b green parts). Similarly, we can de- ﬁne an intermediate block-diagonal Hessian. Formally, if layer kto layer ℓ(where 1 ≤k <ℓ≤n) form a block, the weight vector is deﬁned as ˜θ = vec[w(k),T,..., w(ℓ),T]T and the Hessian can be also transformed by ∆˜θT ¯H(˜θ)∆˜θ= E[∆z(ℓ),TH(z(ℓ))∆z(ℓ)]. Such block-diagonal Hessian ignores the inter-block dependency and considers the intra-block dependency but it produces less general- ization error. Then we can block-by-block reconstruct the intermediate output. To this end, we deﬁne 2 extra kinds of intermediate reconstruction granularity: Stage-wise recon- struction and Block-wise reconstruction. These 4 reconstruction granularities are described below: 1. Layer-wise Reconstruction: Assume the Hessian matrix is layer-diagonal and optimize the layer output one-by-one. It does not consider cross-layer dependency and resemble existing methods (Nagel et al., 2020; Hubara et al., 2020; Wang et al., 2020). 2. Block-wise Reconstruction: A block is the core component in modern CNN, such as the Residual Bottleneck Block as shown in Fig. 1a. This method assumes the Hessian matrix is block- diagonal and block-by-block perform reconstruction, which ignores inter-block dependencies. 3. Stage-wise Reconstruction: A stage is where the featuremaps will be downsampled and gener- ate more channels, which is believed to produce higher-level features. Typical CNN in ImageNet dataset contains 4 or 5 different stages. This method simultaneously optimizes all layers within a stage and thus considers more dependencies than the block-wise method. 4. Network-wise Reconstruction: Optimize the whole quantized network by reconstructing the output of the ﬁnal layers. This method resembles distillation but does not result in good perfor- mances with few images because of high generalization error. The relationship between network, stage, block, and layer is illustrated in Fig. 1a. We test these 4 kinds of reconstruction granularity and ﬁnd that block-wise optimization outperforms others . We think this is because the main off-diagonal loss in the Hessian is concentrated in each block, as Fig. 1b orange part illustrated, while the inter-block loss is small and can be ignored in the opti- 4Published as a conference paper at ICLR 2021 Algorithm 1:BRECQ optimization Input: Pretrained FP model; Calibration dataset, iteration T for all i= 1,2,...,N -th block in the FP model do Collect input data to the block x(i), the FP output z(i) and its gradient g(z(i)) ; for all j = 1,2,...,T -iteration do Get quantized output ˆz(i) and compute ∆z(i) = z(i) −ˆz(i); Descend Eq. (10) and update the rounding of all the weights in this block (Eq. (16)); if Activation Quantization is triggered then Update the activation quantization step size (Eq. (18)). After optimization, compute the sensitivity for each layer and between layers (2-bit only); return Quantized model, Sensitivities for mixed precision; mization. The shortcut connections, which is proposed in (He et al., 2016), may also increase the dependencies within a block. Also, the stage-wise or net-wise optimization suffer from the bad generalization on the validation set and degenerate the ﬁnal performances. We report the quanti- tative comparison in Sec. 4.1. We name our algorithm B RECQ , because we choose block as our base reconstruction unit. It is necessary to point out that our analysis does not give the optimal conﬁguration of the reconstruction granularity. The choice of block-wise optimization comes from our experiments and we ﬁnd this choice has two merits. (1) No hyper-parameters included and (2) applicable for all models and all tasks we tested. 3.3 A PPROXIMATING PRE-ACTIVATION HESSIAN With block-diagonal approximated Hessian matrix, we can measure the cross-layer dependency inside each block and transform any block’s second-order error to the output of this block E[∆z(ℓ),TH(z(ℓ))∆z(ℓ)]. This objective requires the further computation of the knowledge in the rest of the network, i.e., pre-activation Hessian H(z(ℓ)). One way is to follow Nagel et al. (2020) and assume H(z(ℓ)) = c×I. Therefore the quadratic loss becomes ||∆z(ℓ)||2. This method might be easy to implement but lose too much information. We use the diagonal Fisher Information Matrix (FIM) to replace the pre-activation Hessian. For- mally, given a probabilistic model p(x|θ), the FIM is deﬁned as: ¯F(θ) = E [ ∇θlog pθ(y|x)∇θlog pθ(y|x)T] = −E [ ∇2 θlog pθ(y|x) ] = −¯H(θ) log p(x|θ). (9) The FIM is equal to the negative expected Hessian of the log-likelihood function, therefore, a simple corollary is that the Hessian of task loss will become FIM if the model distribution matches the true data distribution (LeCun et al., 2012). Although matching true data distribution seems impossible, this is the best we can do since the pretrained model is converged. The diagonal of the pre-activation FIM is equal to the squared gradients of each elements, which is successfully used in Adam (Kingma & Ba, 2014) for the second momentum. The optimization objective becomes min ˆw E [ ∆z(ℓ),TH(z(ℓ))∆z(ℓ) ] = min ˆw E [ ∆z(ℓ),Tdiag ( ( ∂L ∂z(ℓ) 1 )2,..., ( ∂L ∂z(ℓ) a )2 ) ∆z(ℓ) ] . (10) Compared with the MSE minimization, the above minimization incorporates the squared gradient information. If the output has higher absolute gradients, it will receive more attention when being reconstructed. A similar method for pruning the pre-activation has been proposed in Theis et al. (2018). Note that BRECQ is compatible with any optimization method, like STE (Hubara et al., 2017). Here we adopt adaptive rounding (Nagel et al., 2020) for weights and learned step size (Esser et al., 2020) for activation step size because we observe they generally perform better in PTQ, see details in Appendix B.4.1. We formulate the overall calibration algorithm for a uniﬁed precision model in algorithm 1. We should emphasize that we only need a small subset (1024 in our experiments) of the 5Published as a conference paper at ICLR 2021 whole training dataset to calibrate the quantized model. And we can obtain a quantized ResNet-18 within 20 minutes on a single GTX 1080TI GPU. 3.4 M IXED PRECISION To further push the limit of post-training quantization, we employ mixed precision techniques, which can be formulated by min c L( ˆw,c), s.t. H(c) ≤δ, c ∈{2,4,8}n. (11) Here, c is the bit-width vector with the shape of number of layers. H(·) is a hardware performance measurement function, which is used to ensure the mixed precision model has the same or lower hardware performance (e.g., memory and speed) than a predeﬁned threshold δ. We choose 2, 4, 8-bit for mixed precision because they are most common in practical deployment. Regarding the training lossL, we ﬁnd that nearly all existing literature (Cai et al., 2020; Hubara et al., 2020; Dong et al., 2019) uses layer-wise measurement. They all assume the sensitivity within a layer is independent and can be summed together. Therefore, the mixed precision problem becomes an integer programming problem. However, we argue that the loss measurement should contain two parts: diagonal loss and off-diagonal loss, the ﬁrst is the same with previous works and measure the sensitivity of each layer independently, while the off-diagonal loss is used to measure the cross-layer sensitivity. Theoretically, we should examine all permutations, which results in 3n possibilities and prohibits the search algorithm. Our ﬁrst attempt is to reduce the off-diagonal loss into the block- level as we mentioned that the Hessian can be approximated to a block-diagonal matrix. Granted, we still ﬁnd the search space is large, for example, if a block has four layers, then we have to consider the 34 = 81 permutations for a single block. Based on our preliminary experiments, we ﬁnd that 4-bit and 8-bit quantization nearly do not drop the ﬁnal accuracy. Hence we only take 2-bit permutations into consideration and drastically reduce the search space. We use genetic algorithm (Guo et al., 2020) to search the optimal bitwidth conﬁguration with hardware performance threshold, the algorithm is located in algorithm 2. Due to space limits, we put related works in Appendix 5. Readers can refer to related works for a brief discussion on quantization and second- order analysis. 4 E XPERIMENTS In this section, we report experimental results for the ImageNet classiﬁcation task and MS COCO object detection task. The detailed implementation of the experiments can be found in the Ap- pendix B.4.4. The rest of this section will contain ablation study on reconstruction granularity, classiﬁcation and detection results, mixed precision results and comparison with quantization-aware training. In Appendix B, we conduct more experiments, including the impact of the ﬁrst and the last layer, the impact of calibration dataset size and data source. 4.1 A BLATION STUDY We test four kinds of reconstruction granularity: Net-wise, Stage-wise, Block-wise, and Layer-wise Table 1: Ablation study. Model Layer Block Stage Net ResNet-18 65.19 66.39 66.01 54.15 MobileNetV2 52.13 59.67 54.23 40.76 reconstruction. We conduct ImageNet experi- ments using MobileNetV2 and ResNet-18 with 2- bit weight quantization for all layers except for the ﬁrst and the last layer. It can be seen from Table 1 that Block-wise optimization outperforms other methods. This result implies that the general- ization error in net-wise and stage-wise optimiza- tion outweighs their off-diagonal loss. In ResNet- 18, we ﬁnd the difference is not signiﬁcant, this can be potentially attributed to that ResNet-18 only has 19 layers in the body and the block size, as well as the stage size, is small, therefore leading to indistinct results. 4.2 I MAGE NET We conduct experiments on a variety of modern deep learning architectures, including ResNet (He et al., 2016) with normal convolution, MobileNetV2 (Sandler et al., 2018) with depthwise separa- 6Published as a conference paper at ICLR 2021 Table 2: Accuracy comparison on weight-only quantized post-training models. Activations here are un- quantized and kept full precision. We also conduct variance study for our experiments. Bold values indicates best results. * indicates our implementation based on open-source codes. Methods Bits (W/A) ResNet-18 ResNet-50 MobileNetV2 RegNet-600MF RegNet-3.2GF MnasNet-2.0 Full Prec. 32/32 71.08 77.00 72.49 73.71 78.36 76.68 Bias Correction* 4/32 50.43 64.64 62.82 67.09 71.73 72.31 OMSE (Choukroun et al., 2019) 4/32 67.12 74.67 - - - - AdaRound (Nagel et al., 2020) 4/32 68.71 75.23 69.78 71.97* 77.12* 74.87* AdaQuant (Hubara et al., 2020) 4/32 68.82 75.22 44.78 - - - Bit-Split (Wang et al., 2020) 4/32 69.11 75.58 - - - - BRECQ(Ours) 4/32 70.70±0.07 76.29±0.04 71.66±0.04 73.02±0.09 78.04±0.04 76.00±0.02 Bias Correction* 3/32 12.85 7.97 10.89 28.82 17.95 40.72 AdaRound (Nagel et al., 2020)* 3/32 68.07 73.42 64.33 67.71 72.31 69.33 AdaQuant (Hubara et al., 2020)* 3/32 58.12 67.61 12.56 - - - Bit-Split (Wang et al., 2020) 3/32 66.75 73.24 - - - BRECQ(Ours) 3/32 69.81±0.05 75.61±0.09 69.50±0.12 71.48±0.07 77.22±0.04 74.58±0.08 Bias Correction* 2/32 0.13 0.12 0.14 0.18 0.11 0.11 AdaRound (Nagel et al., 2020)* 2/32 55.96 47.95 32.54 25.66 24.70 30.60 AdaQuant (Hubara et al., 2020)* 2/32 0.30 0.49 0.11 - - - BRECQ(Ours) 2/32 66.30±0.12 72.40±0.12 59.67±0.13 65.83±0.13 73.88±0.14 67.13±0.13 Table 3: Accuracy comparison on fully quantized post-training models. Activations here are quantized to 4-bit. Notations follows the upper table. Methods Bits (W/A) ResNet-18 ResNet-50 MobileNetV2 RegNet-600MF RegNet-3.2GF MNasNet-2.0 Full Prec. 32/32 71.08 77.00 72.49 73.71 78.36 76.68 ACIQ-Mix (Banner et al., 2019) 4/4 67.0 73.8 - - - - ZeroQ (Cai et al., 2020)* 4/4 21.71 2.94 26.24 28.54 12.24 3.89 LAPQ (Nahshan et al., 2019) 4/4 60.3 70.0 49.7 57.71* 55.89* 65.32* AdaQuant (Hubara et al., 2020) 4/4 67.5 73.7 34.95* - - - Bit-Split (Wang et al., 2020) 4/4 67.56 73.71 - - - BRECQ(Ours) 4/4 69.60±0.04 75.05±0.09 66.57±0.67 68.33±0.28 74.21±0.19 73.56±0.24 ZeroQ (Cai et al., 2020)* 2/4 0.08 0.08 0.10 0.10 0.05 0.12 LAPQ (Nahshan et al., 2019)* 2/4 0.18 0.14 0.13 0.17 0.12 0.18 AdaQuant (Hubara et al., 2020)* 2/4 0.21 0.12 0.10 - - BRECQ(Ours) 2/4 64.80±0.08 70.29±0.23 53.34±0.15 59.31±0.49 67.15±0.11 63.01±0.35 ble convolution and RegNet (Radosavovic et al., 2020) with group convolution. Last but not least important, we also investigate the neural architecture searched (NAS) models, MNasNet (Tan et al., 2019). In Table 2, we only quantize weights into low-bit integers and keep activations full precision. We compare with strong baselines including Bias Correction, optimal MSE, AdaRound, AdaQuant, and Bit-split. Note that the ﬁrst and the last layer are kept with 8-bit. While most of the existing methods have good performances in 4-bit quantization, they cannot successfully quantize the model into 2-bit. Our method consistently achieves the lowest accuracy degradation for ResNets (within 5%) and other compact models. We further quantize activations into 4-bit to make the quantized model run on integer-arithmetic hardware platforms. We ﬁnd that 4-bit activation quantization can have a huge impact on RegNet and MobileNet. Nonetheless, our methods produce higher perfor- mance than other state-of-the-arts. To be noted, B RECQ is the ﬁrst to promote the 2W4A accuracy of PTQ to a usable level while all other existing methods crashed. 4.3 C OMPARISON WITH QUANTIZATION -AWARE TRAINING Table 4: Performance as well as training cost comparison with quantization-aware training (QAT). Models Methods Precision Accuracy Model Size Training Data GPU hours ResNet-18 FP: 71.08 ZEROQ (CAI ET AL., 2020)) 4/4 21.20 5.81 MB 0 0.008 BRECQ(OURS) 4/4 69.60 5.81 MB 1024 0.4 BRECQ(W/ DISTILLEDDATA) 4/4 69.32 5.81 MB 0 0.4 PACT (CHOI ET AL., 2018) 4/4 69.2 5.81 MB 1.2 M 100 DSQ (GONG ET AL., 2019) 4/4 69.56 5.81 MB 1.2 M 100 LSQ (ESSER ET AL., 2020) 4/4 71.1 5.81 MB 1.2 M 100 MobileNetV2 FP: 72.49 BRECQ(OURS) 4/4 66.57 2.26 MB 1024 0.8 PACT (CHOI ET AL., 2018) 4/4 61.40 2.26 MB 1.2 M 192 DSQ (GONG ET AL., 2019) 4/4 64.80 2.26 MB 1.2 M 192 BRECQ(OURS) Mixed/8 70.74 1.38 MB 1024 3.2 HAQ (WANG ET AL., 2019) Mixed/8 70.90 1.38 MB 1.2 M 384 7Published as a conference paper at ICLR 2021 Table 5: Objection detection task (MS COCO) comparison onfully quantized post-training models. Activations here are quantized to 8-bit. We report the bounding box mean Average Precision (mAP) metric. Models Backbone Full Prec. Bias Correction* AdaRound* ZeroQ B RECQ(Ours) 32/32 8/8 4/8 4/8 2/8 4MP/8 8/8 4/8 2/8 Faster RCNN (Ren et al., 2015) ResNet-18 34.55 34.30 0.84 33.96 23.01 - 34.53 34.34 31.82 ResNet-50 38.55 38.25 0.25 37.58 19.63 - 38.54 38.29 34.23 MobileNetV2 33.44 33.24 18.39 32.77 16.35 - 33.40 33.18 27.54 RetinaNet (Lin et al., 2017) ResNet-18 33.20 33.00 0.04 32.59 19.93 - 33.14 33.01 31.42 ResNet-50 36.82 36.68 0.07 36.00 19.97 33.7 36.73 36.65 34.75 MobileNetV2 32.63 32.60 18.47 31.89 14.10 - 32.57 32.31 27.59 3.0 3.5 4.0 4.5 5.0 5.5 Model Size (MB) 64 65 66 67 68 69 70 71Test Accuracy66.09 67.99 68.82 69.53 70.13 70.53 ResNet-18 Mixed Unified 2-bit 4-bit ZeroQ 0.8 1.0 1.2 1.4 1.6 Model Size (MB) 56 58 60 62 64 66 68 70 72Test Accuracy 63.73 67.4 68.99 70.28 71.39 MobileNet-V2 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Model Size (MB) 64 66 68 70 72Test Accuracy65.58 68.36 69.62 71.39 71.88 72.68 RegNetX_600MF 30 35 40 45 50 Latency (ms) 64 65 66 67 68 69 70 71Test Accuracy 66.49 68.9 70.25 70.37 70.5 28.5 29.0 29.5 30.0 30.5 31.0 31.5 Latency (ms) 56 58 60 62 64 66 68 70 72Test Accuracy 66.35 67.47 68.63 70.47 71.4 71.54 22 24 26 28 30 Latency (ms) 64 66 68 70 72Test Accuracy 66.61 68.73 69.43 71.3 72.07 72.53 Figure 2: Mixed Precision results. In this section, we compare our algorithm (post-training quantization) with some quantization-aware training methods, including PACT (Choi et al., 2018), DSQ (Gong et al., 2019), LSQ (Esser et al., 2020), and a mixed precision technique HAQ (Wang et al., 2019). Table 4 shows that although BRECQ is a PTQ method with limited available data, it can achieve comparable accuracy results with existing quantization-aware training models. In addition, our method can surpass them in 4- bit MobileNetV2 while using less than one training GPU hours. Our method also has comparable accuracy with HAQ, which is a training-based mixed precision method. Note that our GPU hours include 3 uniﬁed precision training (2-, 4-, 8-bit respectively) and further mixed-precision training only needs to check the lookup table. Instead, HAQ would end-to-end search for each hardware performance threshold from scratch. 4.4 MS COCO To validate the effectiveness of B RECQ on other tasks, we conduct object detection on the two- stage Faster-RCNN (Ren et al., 2015) and the one-stage RetinaNet (Lin et al., 2017). ResNet-18, 50 as well as MobileNetV2 are adopted as backbones for the detection model. Results in Table 5 demonstrate our method nearly does not drop the performance in 4-bit weight quantization and 8- bit activation. In particular, B RECQ only decreases 0.21% mAP performance on 4-bit ResNet-18 backboned Faster RCNN. On 4-bit ResNet-50 backboned RetinaNet, our method is outperforms the mixed precision based ZeroQ model by 3% mAP. Even when the weight bit decreases to 2, the model still achieves near-to-original mAP. 8Published as a conference paper at ICLR 2021 4.5 M IXED PRECISION In this section, we test (1) model-size guaranteed mixed precision and (2) FPGA latency guar- anteed mixed precision 2 to unleash the potential of mixed precision and further push the limit of PTQ. We choose ResNet-18, MobileNetV2, and RegNetX-600MF to validate the efﬁcacy of our algorithm. Note that in this section, we keep activation in 8-bit because we only compare the dis- crepancy between the uniﬁed and mixed precision in weights. We omit 3-bit weight quantization in uniﬁed precision because it is usually unfriendly to the hardware. Latency settings can be found in Appendix B.4.3. From Fig. 2 we ﬁnd that (1) mixed precision consistently outperforms uniﬁed precision, especially when using extremely low-bit, e.g., up to 10% accuracy increase with the same latency as the 2-bit model. (2) mixed precision can produce many bit conﬁgurations that can adapt to plenty of hardware requirements while uniﬁed precision can only have 2 ﬁxed models. 5 R ELATED WORKS Quantization Model quantization can be classiﬁed into two categories: Quantization-aware Train- ing (QAT) and Post-training Quantization (PTQ). Rounding ﬂoating-point numbers to ﬁxed-points numbers will produce 0 gradients almost everywhere. Therefore, most QAT methods employ the Straight-Through Estimator (STE) for gradients approximation. Gong et al. (2019) uses a differen- tiable tanh function to gradually approach the step function. Choi et al. (2018); Esser et al. (2020) introduces parameterized clipping thresholds to learn it by STE. Apart from uniform quantization, some works like Li et al. (2019) argue that non-uniform quantization has better performance than uniform quantization while keeping its efﬁciency. Despite the promising results given by QAT meth- ods, they usually need more than 100 GPU hours to get it. In that case, PTQ plays an important role which is what we focus on in this paper. Generally, most deep learning models can be safely quan- tized to 8-bit without re-training. Data-Free Quantization Nagel et al. (2019) even do layer-wise 8-bit PTQ without any data. However, in 4-bit quantization, most parameter space-based methods cannot obtain good performances. Recently, Nagel et al. (2020) propose to do layer-wise calibration and made huge progress in 4-bit quantization. Our work continues its analysis on Taylor expansion and considers the off-diagonal loss. Another perspective of quantiﬁcation is the precision allocation scheme. Hardware-aware Quantization (HAQ Wang et al. (2019)) leverages reinforcement learning to search the optimal bitwidth conﬁguration. Hessian-aware Weight Quantization (HAWQ) (Dong et al., 2019) utilizes the second-order information to decide the bitwidth. Mixed precision also appears in PTQ, such as the Pareto frontier method in ZeroQ (Cai et al., 2020) and the Integer Programming method in AdaQuant (Hubara et al., 2020). Second-order Analysis and OptimizationThe history of second-order information in perturbation analysis can be traced to the 1990s like Optimal Brain Surgeon (Hassibi & Stork, 1993; Dong et al., 2017). The Hessian matrix is essential for pruning and quantization. As aforementioned, HAWQ uses the largest eigenvalue of Hessian to determine the sensitivity. Hessian matrix is also impor- tant for second-order optimization like Newton’s method as it consists of the curvature informa- tion. However, calculating the real full Hessian is prohibitive on today’s deep learning architectures. Therefore, approximations are made to simplify the calculation and make the storage more ﬂexi- ble, e.g., Gauss-Newton optimization with Kronecker-factored recursive approximation Botev et al. (2017). Hessian-Free optimization (Martens, 2010) avoids the explicit computation of the Hessian matrix by solving the linear system g= Hv. Second-order optimization with FIM is called Natural Gradient Descent (Amari, 1998). K-FAC (Martens & Grosse, 2015) utilizes the layer-diagonal FIM and the approximation of the expected Kronecker product to compute the curvature information. 6 C ONCLUSION In this paper, we propose BRECQ , a post-training quantization framework by analyzing the second- order error. We show that the reconstruction of quantization at the block granularity arrives at a good balance of cross-layer dependency and ﬁrst order approximation, especially in 2-bit weight quantization where no prior works succeed to quantize. B RECQ is compatible with mixed precision and can reduce the search cost. To our best knowledge, B RECQ reaches the highest performance in post-training quantization and is the ﬁrst to be on a par with quantization-aware training using 4-bit. 2We also test mobile CPU latency guaranteed mixed precision, located in Appendix B.3. 9Published as a conference paper at ICLR 2021 ACKNOWLEDGMENT We thank Markus Nagel and anonymous reviewers for their kind help of this work. This project is primarily supported by NSFC 61876032. REFERENCES Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):251– 276, 1998. Haoli Bai, Jiaxiang Wu, Irwin King, and Michael Lyu. Few shot network compression via cross distillation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pp. 3203–3210, 2020. Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. In Advances in Neural Information Processing Systems, 2019. Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep learning. arXiv preprint arXiv:1706.03662, 2017. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13169–13178, 2020. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini- vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net- works for efﬁcient inference. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp. 3009–3018. IEEE, 2019. Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In Advances in Neural Information Processing Systems, 2017. Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hes- sian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE International Conference on Computer Vision, pp. 293–302, 2019. Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S. Modha. Learned step size quantization. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkgO66VKDS. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. arXiv preprint arXiv:1908.05033, 2019. Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision, pp. 544–560. Springer, 2020. Qingchang Han, Yongmin Hu, Fengwei Yu, Hailong Yang, Bing Liu, Peng Hu, Ruihao Gong, Yanfei Wang, Rui Wang, Zhongzhi Luan, et al. Extremely low-bit convolution optimization for quantized neural network on modern computer architectures. In 49th International Conference on Parallel Processing-ICPP, pp. 1–12, 2020. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in neural information processing systems, pp. 164–171, 1993. 10Published as a conference paper at ICLR 2021 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Jour- nal of Machine Learning Research, 18(1):6869–6898, 2017. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020. Daniel Jakubovitz, Raja Giryes, and Miguel RD Rodrigues. Generalization error in deep learning. In Compressed Sensing and Its Applications, pp. 153–193. Springer, 2019. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Yann A LeCun, L ´eon Bottou, Genevieve B Orr, and Klaus-Robert M ¨uller. Efﬁcient backprop. In Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012. Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efﬁcient non- uniform discretization for neural networks. In International Conference on Learning Representa- tions, 2019. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision , pp. 2980–2988, 2017. James Martens. Deep learning via hessian-free optimization. 2010. James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408–2417, 2015. Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE International Con- ference on Computer Vision, pp. 1325–1334, 2019. Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. arXiv preprint arXiv:2004.10568, 2020. Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and Avi Mendelson. Loss aware post-training quantization. arXiv preprint arXiv:1911.07190, 2019. Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quanti- zation. arXiv preprint arXiv:1802.05668, 2018. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10428–10436, 2020. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91–99, 2015. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo- bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510–4520, 2018. 11Published as a conference paper at ICLR 2021 Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, and Hadi Esmaeilzadeh. Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Archi- tecture (ISCA), pp. 764–775. IEEE, 2018. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019. Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz ´ar. Faster gaze prediction with dense networks and ﬁsher pruning. arXiv preprint arXiv:1801.05787, 2018. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quan- tization with mixed precision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8612–8620, 2019. Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In Proc. 37nd Int. Conf. Mach. Learn.(ICML), 2020. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. A M AIN PROOFS A.1 P ROOF OF THEOREM 3.1 Proof. We will prove the theorem using quadratic form. Denote the weight vector shape asθ∈Rd, and the network output vector shape as z(n) ∈Rm. The quadratic form of the ∆θTH(θ)∆θcan be represented by: ∆θTH(θ)∆θ= d∑ i=1 ∆θ2 i (∂2L ∂θ2 i ) + 2 d∑ i<j ∆θi∆θj ∂L ∂θiθj = d∑ i=1 d∑ j=1 ( ∆θi∆θj ∂L ∂θiθj ) , (12) where Lis the cross-entropy loss. Based on Eq. (5), we have ∂2L ∂θiθj = m∑ k,l ∂z(n) k ∂θl ∂2L ∂z(n) k z(n) l ∂z(n) l ∂θj (13) Substituting above equation in Eq. (12), we have ∆θTH(θ)∆θ= d∑ i=1 d∑ j=1 ∆θi∆θj (m∑ k=1 m∑ l=1 ∂z(n) k ∂θi ∂2L ∂z(n) k z(n) l ∂z(n) l ∂θj ) (14a) = d∑ i=1 d∑ j=1 m∑ k=1 m∑ l=1 ( ∆θi∆θj ∂z(n) k ∂θi ∂2L ∂z(n) k z(n) l ∂z(n) l ∂θj ) (14b) = m∑ k=1 m∑ l=1 ( ∂2L ∂z(n) k z(n) l )( d∑ i=1 ∆θi ∂z(n) k ∂θi )  d∑ j=1 ∆θj ∂z(n) k ∂θj   (14c) = (∆θJ [z(n) θ ] )TH(z(n))(∆θJ [z(n) θ ] ), (14d) where we deﬁne the J[x y] is the Jacobian matrix of xw.r.t. y. To this end, we use the ﬁrst-order Taylor expansion as we did in Eq. (8) to approximate the change in network output, i.e., ∆z(n) ≈∆θJ [z(n) θ ] (15) Therefore, the ﬁnal objective is transformed to ∆z(n),TH(z(n))∆z(n). 12Published as a conference paper at ICLR 2021 B E XPERIMENTS B.1 E FFECT OF THE FIRST AND THE LAST LAYER Many papers claim that the ﬁrst and the last layer can have a huge impact on the ﬁnal accuracy. In this section, we investigate this phenomenon as well as the impact of the ﬁrst and the last layer on hardware performances. We test ResNet-18, MobileNetV2 as well as RegNet-600MF. Our observa- tions include: 1. In terms of accuracy, the 4-bit quantization is essentially good, both of these two layers won’t drop too much accuracy (with 0.2%). But in 2-bit quantization, the last fully connected layer is far more important than the ﬁrst layer. We also observe that the ﬁrst layer in MobileNetV2 and RegNet (3×3 kernels, 32 channels) is slightly more sensitive than that in ResNet-18 (7×7 kernel, 64 channels). 2. In terms of model size, the ﬁrst layer merely has a minor impact because the input images only have 3 channels, while the last layer contains many weight parameters and greatly affects the memory footprint. We should point out that just the model size in the ﬁrst layer is low doesn’t mean the memory burden is low, because the input image will cost huge memory space. 3. In terms of latency, the situation depends on the architecture. For example, in ResNet-18 the ﬁrst layer has a huge impact on the latency, while in MobileNetV2 and RegNet-600MF the last layer is more important than the ﬁrst layer. This is because the latency is affected by multiple factors, such as the input size of the featuremap, the FLOPs, and the weight memory size. The arithmetic intensity (OPs/byte) greatly affects the latency. We ﬁnd that the operations with high arithmetic intensity, i.e., shallow layers in the network, generate a less latency gap between different bit- widths. In conclusion, we ﬁnd that keeping the ﬁrst and the last layer 8-bit is unnecessary. Especially in ResNet-18, we ﬁnd that setting all layers to 4-bit results in 53.3 ms latency and is faster than the 59.8 ms in 2-bit quantization (with ﬁrst and last layer 8-bit), but the accuracy is even 4% higher. Such phenomenon indicates the potential power of the mixed precision. B.2 E FFECT OF DATA We evaluated the inﬂuence of the size of calibration dataset and the source of the data on ResNet-18. We test different numbers of input data point and ﬁnd that the improvement in 4-bit quantization is trivial. Yet in 2-bit quantization we can see that the accuracy increases 5% when #data points increase. We also test the distilled data introduced in ZeroQ (Cai et al., 2020). Distilled data is learned from pretrained models’ BN statistics, i.e.,x1 distilled = arg minx∈R ∑n i=1((µi−ˆµi)2 +(ςi− ˆςi)) where µi and ςi is the original mean and variance in BN statistics of the (i)-th layer. We ﬁnd that distilled data performs good in 4-bit quantization but still has a large margin with the original ImageNet dataset in 2-bit quantization. We also ﬁnd the ﬁnal accuracy does not beneﬁt much from the increase of number of distilled data, this might because the distilled data are minimized by a same objective and has low diversity. Table 6: Impact of the ﬁrst and the last layer Models No Quantization Precision 4/8 Precision 2/8 First Last Accuracy Model Size Latency Accuracy Model Size Latency ResNet-18 FP: 71.08 \u0013 \u0013 70.76 5.81 MB 70.72 ms 66.30 3.15 MB 59.84 ms \u0017 \u0013 70.66 5.81 MB 53.76 ms 65.95 3.15 MB 31.20 ms \u0013 \u0017 70.64 5.57 MB 70.08 ms 64.87 2.79 MB 58.72 ms \u0017 \u0017 70.58 5.56 MB 53.28 ms 64.53 2.78 MB 30.88 ms MobileNetV2 FP: 72.49 \u0013 \u0013 71.80 2.26 MB 32.80 ms 59.59 1.74 MB 30.40 ms \u0017 \u0013 71.69 2.26 MB 32.64 ms 59.13 1.74 MB 30.24 ms \u0013 \u0017 71.42 1.65 MB 31.52 ms 56.29 0.83 MB 28.48 ms \u0017 \u0017 71.42 1.65 MB 31.36 ms 55.58 0.82 MB 28.32 ms RegNet-600MF FP: 73.71 \u0013 \u0013 72.98 3.19 MB 31.84 ms 65.66 1.84 MB 23.20 ms \u0017 \u0013 72.89 3.19 MB 31.68 ms 65.83 1.85 MB 22.88 ms \u0013 \u0017 72.69 2.94 MB 31.20 ms 62.93 1.47 MB 22.40 ms \u0017 \u0017 72.73 2.94 MB 31.04 ms 63.08 1.47 MB 22.08 ms 13Published as a conference paper at ICLR 2021 64 128 256 512 1024 # data points 70.0 70.1 70.2 70.3 70.4 70.5 70.6 70.7Accuracy 70.4 70.49 70.55 70.63 70.7 70.1 70.32 70.28 70.34 70.33 ResNet-18 4bit ImageNet data Distilled data 64 128 256 512 1024 # data points 58 60 62 64 66Accuracy 61.11 63.15 64.72 65.65 66.39 58.22 59.17 60.03 60.67 60.94 ResNet-18 2bit ImageNet data Distilled data Figure 3: Effect of #data points and data source. B.3 M OBILE CPU L ATENCY GUARANTEED MIXED PRECISION 550 560 570 580 590 600 Latency (ms) 64 65 66 67 68 69 70 71Test Accuracy 66.65 67.89 69.36 70.04 70.43 ResNet-18 Mixed Unified 2-bit 4-bit 1150 1175 1200 1225 1250 1275 Latency (ms) 71 72 73 74 75 76Test Accuracy 71.92 73.57 74.8 75.71 76.32 ResNet-50 Mixed Unified 2-bit 4-bit Figure 4: Mixed precision results on ResNet-18 and 50. In this section, we test the mobile CPU latency guaranteed mixed precision. The latency lookup table is tested using the technique in Gong et al. (2019). We only validate it on ResNet-18 and ResNet-50 because the current low-bit General Matrix Multiply (GEMM) implementation only supports normal convolution. The results concur with Fig. 2. Below 4-bit, the mixed precision can achieve better task performance than the uniﬁed precision models. For ResNet-50, the improvement is lower than that for ResNet-18 and any other mixed precision models. We think this is because the sensitivity in ResNet-50 is not distinct and therefore the improvement brought by mixed precision is trivial. B.4 I MPLEMENTATION B.4.1 L EARNING STRATEGIES In this work, we mainly focus on developing optimization objective rather than optimization strate- gies. We observe adaptive rounding performs well in post-training quantization. A brief introduction on AdaRound is given below, the detailed algorithm can be found in Nagel et al. (2020). Traditional quantization function is performed by rounding-to-nearest operation: ˆw = s × clip(⌊w/s⌉,n,p ). AdaRound optimizes the rounding policy in post-training quantization. Specif- ically, all weights are initially rounded by ﬂoor operation, and a learnable variable v determines the ﬁnal rounding result to be ﬂooring or ceiling. A sigmoid-like function σ(·) keeps the learnable variable v moving between 0 and 1 and a regularization term assures the σ(v) can converged to either 0 or 1. The formulation is given by ˆw = s×clip ( ⌊w s⌋+ σ(v),n,p ) (16) The minimization problem together with the regularization is given by arg min v E [ ∆z(ℓ),Tdiag(( ∂L ∂z(ℓ) 1 )2,..., ( ∂L ∂z(ℓ) a )2)∆z(ℓ) ] + λ ∑ i ( 1 −|2σ(vi) −1|β) , (17) 14Published as a conference paper at ICLR 2021 where progressively decreasing β in the calibration ensures the σ(v) converged to binary values. The activations cannot be quantized using adaptive rounding because they vary with different input data points. Thus, we can only adjust its quantization step size Esser et al. (2020). Denoting the quadratic loss in above equation as Lq, the gradients of step size is given by ∂Lq ∂s =    ∂Lq ∂ˆx if x >n ∂Lq ∂ˆx (ˆx s −x s ) if 0 ≤x <α 0 if x ≤0 , (18) where all step size in the block will be optimized. B.4.2 G ENETIC ALGORITHM FOR MIXED PRECISION Algorithm 2:Genetic algorithm Input: Random initialized population P0 with population size S; Iteration T, mutation probability p; Hardware performance threshold δ; Hardware measurement function H(·) TopK = ∅ ; for all t= 1,2,...,T -th iteration do Evaluate ﬁtness value (Eq. (11)) for each individual ; Update and sort TopK based on ﬁtness function; repeat New bitwidth conﬁguration by crossover ccross = Crossover(TopK); Pcrossover := Pcrossover + ccross if H(ccross) <δ; until Size of Pcrossover equal to S/2; repeat New bitwidth conﬁguration by mutation cmutate = Mutate(TopK,probability = p); Pmutate := Pmutate + cmutate if H(cmutate) <δ; until Size of Pmutate equal to S/2; Pt = Pcrossover ∪Pmutate; Pmutate = ∅, Pcrossover = ∅; Get the best ﬁtted entry and then do the overall block reconstruction (cf. algorithm 1); return mixed precision model B.4.3 L ATENCY ACQUISITION We test the latency of quantized neural networks on a self-developed simulator of a precision- variable accelerator for NN. The basic architecture of this accelerator is inspired by typical systolic- matrix multiplication. The accelerator supports the per-channel quantization parameter. The preci- sion of each layer of a NN is highly conﬁgurable in this accelerator, supporting 9 types of precision combination: activation: 2-, 4-, 8-bit × weight: 2-, 4-, 8-bit, see Fig. 5a. With the support of scal- able function-unit (Sharma et al., 2018), the peak performance of the accelerator is able to achieve corresponding linear improvement as the precision decreases. For example, the peak performance of this accelerator is 256 GMAC/s in 8-bit ×8-bit precision, and it scales to 512 GMAC/s in 8- bit ×4-bit precision and 4 TMAC/s in 2-bit ×2-bit precision. Although this accelerator provides considerable computation resources especially in low precision, the parallelism of the speciﬁc layer (like depthwise convolution) and the bandwidth of on-chip buffer is limited. Consequently, actual performance may not scale accurately along with the peak performance, and the ﬁnal performance differs according to the size and type of layers. The simulator performs cycle-accurate simulation and evaluation for a given NN executed on the accelerator, so we can get an equivalent evaluation by using this simulator. The simulator is available in the provided source codes. For the acquisition of mobile ARM CPU latency, we adopt the redesigned low-bit GEMM imple- mentation in Han et al. (2020). Fig. 5b shows a brief overview of the low-bit GEMM implementa- tion. Since there is no instruction supporting the bit-width below 8 on ARM architecture, we can 15Published as a conference paper at ICLR 2021 PE Array Input Buffer Weight Buffer Requant/quant 1-D computation Unit Output Buffer Precision -variablePE (a) Accelerator design. A11 A21 A 31 Multiply Replicate Element - wise Multiply Produce Accu - mulate Accumulate A11 A12 A13 A21 A22 A23 A31 A32 A33 A 11 A11 A11 B11 B12 B13 A21 A21 A21 B11 B12 B13 A31 A31 A31 B11 B12 B13 B 11 B 12 B 13 B 21 B 22 B 23 B 31 B 32 B 33 B 11 B 12 B 13 B 11 B 12 B 13 B 11 B 12 B 13 C11 C12 C13 C21 C22 C23 C31 C32 C33 C11 C12 C13 C21 C22 C23 C31 C32 C33 BufferA BufferB BufferC Results MatrixC (b) Low-bit GEMM implementation on ARM CPU. Figure 5: FPGA-based and mobile CPU-based latency acquisition. not get a higher computation efﬁciency for extremely low-bit such as 2-bit and 4-bit. But we can acquire a better memory access efﬁciency. The primary speedup comes from the reduction of data movement. Speciﬁcally, we can conduct more times of addition in the same 8-bit register before we have to move it to a 16-bit register to avoid overﬂow. The lower bit-width is used, the less movement is needed. Together with the optimized data packing and data padding, we can run mixed precision quantization on Raspberry Pi 3B, which has a 1.2 GHz 64-bit quad-core ARM Cortex-A53. Note that this implementation is not optimized for depthwise separable or group convolution, therefore we only verify the latency on ResNets. B.4.4 I MPLEMENTATION DETAILS The ImageNet dataset consists of 1.2M training images and 50K test images. We follows standard pre-process (He et al., 2016) to get 1024 224 ×224 input images as the calibration dataset. We fold the batch normalization layer into convolution and freeze the BN statistics before post-training quan- tization. We use Adam optimizer (Kingma & Ba, 2014) to learn the weight rounding and activation range to reconstruct the block output. Note that some layers are not a component of any block, such as the ﬁrst convolutional layer and the last fully connected layer and the last convolutional layer in the MobileNetV2. These layers use naive layer reconstruction. The batch size of learning is set to 32 and each block will be optimized for 2 ×104 iterations. The learning rate is set to 10−3 during the whole learning process. Other hyper-parameters such as the temperature β are kept the same with Nagel et al. (2020). For activation step size, we also use Adam optimizer and set the learning rate to 4e-5. Note that we do not implement the gradient scale as introduced in the original paper (Esser et al., 2020). After reconstruction, we will store the sensitivity measured on the calibration dataset. Note that we will store intra-block sensitivity in 2-bit quantization. The sensitivity, as well as hard- ware performances for each layer, will be stored in a lookup table. When calculating the ﬁtness value and determining the hardware performances in a genetic algorithm, we will check the lookup table. For genetic algorithm, we set the population size to 50 and evolve 100 iterations to obtain the best individual. The ﬁrst population is initialized by Gaussian distribution and we round the samples to integers in [0, 1, 2], corresponding to bit-width [2, 4, 8]. The mutation probability is set to 0.1. The genetic algorithm usually completes the evolution in only about 3 seconds. For object detection tasks, we use 256 training images taken from the MS COCO dataset for cal- ibration. The image resolution is set to 800 (max size 1333) for ResNet-18 and ResNet-50, while the image resolution for MobileNetV2 is set to 600 (max size 1000). Note that we only apply block reconstruction in the backbone because other parts of the architecture, such as Feature Pyramid Net, do not have the block structure. Therefore a naive layer reconstruction is applied to the rest of the network. Learning hyper-parameters are kept the same with ImageNet experiments. 16",
      "meta_data": {
        "arxiv_id": "2102.05426v2",
        "authors": [
          "Yuhang Li",
          "Ruihao Gong",
          "Xu Tan",
          "Yang Yang",
          "Peng Hu",
          "Qi Zhang",
          "Fengwei Yu",
          "Wei Wang",
          "Shi Gu"
        ],
        "published_date": "2021-02-10T13:46:16Z",
        "pdf_url": "https://arxiv.org/pdf/2102.05426v2.pdf"
      }
    },
    {
      "title": "PowerQuant: Automorphism Search for Non-Uniform Quantization",
      "abstract": "Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as\ncomputer vision. However, due to their high latency, the deployment of DNNs\nhinges on the development of compression techniques such as quantization which\nconsists in lowering the number of bits used to encode the weights and\nactivations. Growing concerns for privacy and security have motivated the\ndevelopment of data-free techniques, at the expanse of accuracy. In this paper,\nwe identity the uniformity of the quantization operator as a limitation of\nexisting approaches, and propose a data-free non-uniform method. More\nspecifically, we argue that to be readily usable without dedicated hardware and\nimplementation, non-uniform quantization shall not change the nature of the\nmathematical operations performed by the DNN. This leads to search among the\ncontinuous automorphisms of $(\\mathbb{R}_+^*,\\times)$, which boils down to the\npower functions defined by their exponent. To find this parameter, we propose\nto optimize the reconstruction error of each layer: in particular, we show that\nthis procedure is locally convex and admits a unique solution. At inference\ntime, we show that our approach, dubbed PowerQuant, only require simple\nmodifications in the quantized DNN activation functions. As such, with only\nnegligible overhead, it significantly outperforms existing methods in a variety\nof configurations.",
      "full_text": "Arxiv version POWER QUANT : A UTOMORPHISM SEARCH FOR NON- UNIFORM QUANTIZATION Edouard Yvinec1,2 , Arnaud Dapogny2 , Matthieu Cord1 , Kevin Bailly1,2 Sorbonne Universit´e1, CNRS, ISIR, f-75005, 4 Place Jussieu 75005 Paris, France Datakalab2, 114 boulevard Malesherbes, 75017 Paris, France ey@datakalab.com ABSTRACT Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and acti- vations. Growing concerns for privacy and security have motivated the devel- opment of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing ap- proaches, and propose a data-free non-uniform method. More speciﬁcally, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical oper- ations performed by the DNN. This leads to search among the continuous auto- morphisms of (R∗ +,×), which boils down to the power functions deﬁned by their exponent. To ﬁnd this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modiﬁcations in the quantized DNN activation functions. As such, with only negligible overhead, it signiﬁcantly outperforms existing methods in a variety of conﬁgurations. 1 I NTRODUCTION Deep neural networks (DNNs) tremendously improved algorithmic solutions for a wide range of tasks. In particular, in computer vision, these achievements come at a consequent price, as DNNs deployment bares a great energetic price. Consequently, the generalization of their usage hinges on the development of compression strategies. Quantization is one of the most promising such technique, that consists in reducing the number of bits needed to encode the DNN weights and/or activations, thus limiting the cost of data processing on a computing device. Existing DNN quantization techniques, for computer vision tasks, are numerous and can be distin- guished by their constraints. One such constraint is data usage, as introduced in Nagel et al. (2019), and is based upon the importance of data privacy and security concerns. Data-free approaches such as Banner et al. (2019); Cai et al. (2020); Choukroun et al. (2019); Fang et al. (2020); Garg et al. (2021); Zhao et al. (2019); Nagel et al. (2019), exploit heuristics and weight properties in order to perform the most efﬁcient weight quantization without having access to the training data. As com- pared to data-driven methods, the aforementioned techniques are more convenient to use but usually come with higher accuracy loss at equivalent compression rates. Data-driven methods performance offer an upper bound on what can be expected from data-free approaches and in this work, we aim at further narrowing the gap between these methods. To achieve this goal, we propose to leverage a second aspect of quantization: uniformity. For sim- plicity reasons, most quantization techniques such as Nagel et al. (2019); Zhao et al. (2019); Cong et al. (2022) perform uniform quantization, i.e. they consist in mapping ﬂoating point values to an evenly spread, discrete space. However, non-uniform quantization can theoretically provide a closer ﬁt to the network weight distributions, thus better preserving the network accuracy. Previous work on non-uniform quantization either focused on the search of binary codes (Banner et al., 2019; 1 arXiv:2301.09858v1  [cs.CV]  24 Jan 2023Arxiv version W8/A8 W6/A6 W4/A4 PowerQuant SPIQ SQuant OCS DFQ 80 70 60 50 40 ImageNet top1 Accuracy Figure 1: Comparison of the proposed method to other data-free quantization schemes on DenseNet 121 pre- trained on ImageNet. The proposed method (right bin in blue) drastically improves upon the existing data-free methods especially in the challenging W4/A4 quantization. Hubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018) or leverage logarithmic distribution (Miyashita et al., 2016; Zhou et al., 2017). However, these approaches map ﬂoating point multiplications operations to other operations that are hard to leverage on current hardware (e.g. bit-shift) as opposed to uniform quantization which maps ﬂoating point multiplications to in- teger multiplications (Gholami et al., 2021; Zhou et al., 2016). To circumvent this limitation and reach a tighter ﬁt between the quantized and original weight distributions, in this work, we propose to search for the best possible quantization operator that preserves the nature of the mathematical operations. We show that this search boils down to the space deﬁned by the continuous automor- phisms of (R∗ +,×), which is limited to power functions deﬁned by their exponent. We optimize the value of this parameter by minimizing the error introduced by quantization. This allows us to reach superior accuracy, as illustrated in Fig 1. To sum it up, our contributions are: • We search for the best quantization operator that do not change the nature of the mathemat- ical operations performed by the DNN, i.e. the automorphisms of (R∗ +,×). We show that this search can be narrowed down to ﬁnding the best exponent for power functions. • We ﬁnd the optimal exponent parameter to more closely ﬁt the original weight distribution compared with existing (e.g. uniform and logarithmic) baselines. To do so, we propose to optimize the quantization reconstruction error. We show that this problem is locally convex and admits a unique solution. • In practice, we show that the proposed approach, dubbed PowerQuant, only requires simple modiﬁcations in the quantized DNN activation functions. Furthermore, we demonstrate through extensive experimentation that our method achieves outstanding results on various and challenging benchmarks with only negligible computational overhead. 2 R ELATED WORK 2.1 Q UANTIZATION In this section, we provide a background on the current state of DNNs quantization. Notice that while certain approaches are geared towards memory footprint reduction (e.g. without quantizing inputs and activations) (Chen et al., 2015; Gong et al., 2014; Han et al., 2016; Zhou et al., 2017), in what follows, we essentially focus on methods that aim at reducing the inference time. In particular, motivated by the growing concerns for privacy and security, data-free quantization methods (Banner et al., 2019; Cai et al., 2020; Choukroun et al., 2019; Fang et al., 2020; Garg et al., 2021; Zhao et al., 2019; Nagel et al., 2019; Cong et al., 2022) are emerging and have signiﬁcantly improved over the recent years. The ﬁrst breakthrough in data-free quantization (Nagel et al., 2019) was based on two mathematical ingenuities. First, they exploited the mathematical properties of piece-wise afﬁne activation func- 2Arxiv version tions (such as e.g. ReLU based DNNs) in order to balance the per-channel weight distributions by iteratively applying scaling factors to consecutive layers. Second, they proposed a bias correction scheme that consists in updating the bias terms of the layers with the difference between the ex- pected quantized prediction and the original predictions. They achieved near full-precision accuracy in int8 quantization. Since this seminal work, two main categories of data-free quantization methods have emerged. First, data-generation based methods, such as Cai et al. (2020); Garg et al. (2021), that used samples generated by Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) as samples to ﬁne-tune the quantized model through knowledge distillation (Hinton et al., 2014). Nevertheless, these methods are time-consuming and require signiﬁcantly more computational re- sources. Other methods, such as Banner et al. (2019); Choukroun et al. (2019); Fang et al. (2020); Zhao et al. (2019); Nagel et al. (2019); Cong et al. (2022), focus on improving the quantization operator but usually achieve lower accuracy. One limitation of these approaches is that they are essentially restricted to uniform quantization, while considering non-uniform mappings between the ﬂoating point and low-bit representation might be key to superior performance. 2.2 N ON-UNIFORM QUANTIZATION Indeed, in uniform settings, continuous variables are mapped to an equally-spaced grid in the orig- inal, ﬂoating point space. Such mapping introduces an error: however, applying such uniform mapping to an a priori non-uniform weight distribution is likely to be suboptimal in the general case. To circumvent this limitation, non-uniform quantization has been introduced (Banner et al., 2019; Hubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018; Miyashita et al., 2016; Zhou et al., 2017) and (Zhang et al., 2021a; Li et al., 2019). We distinguish two categories of non-uniform quantization approaches. First, methods that introduce a code-base and require very sophisticated implementations for actual inference beneﬁts such as Banner et al. (2019); Hubara et al. (2016); Jeon et al. (2020); Wu et al. (2016); Zhang et al. (2018). Second, methods that simply modify the quantization operator such as Miyashita et al. (2016); Zhou et al. (2017). In particular, (Zhang et al., 2021a) propose a log-quantization technique. Similarly, Li et al. (Li et al., 2019) use log quantization with basis 2. In both cases, in practice, such logarithmic quantization scheme changes the nature of the mathematical operations involved, with multiplications being replaced by bit shifts. Nevertheless, one limitation of this approach is that because the very nature of the mathematical operations is intrinsically altered, in practice, it is hard to leverage without dedicated hardware and implementation. Instead of transforming ﬂoating point multiplications in integer mul- tiplications, they change ﬂoating point multiplications into bit-shifts or even look up tables (LUTs). Some of these operations are very speciﬁc to some hardware (e.g. LUTs are thought for FPGAs) and may not be well supported on most hardware. Conversely, in this work, we propose a non-uniform quantization scheme that preserves the nature of the mathematical operations by mapping ﬂoating point multiplications to standard integer multiplications. As a result, our approach boils down to simple modiﬁcations of the computations in the quantized DNN, hence allowing higher accuracies than uniform quantization methods while leading to straightforward, ready-to-use inference speed gains. Below we describe the methodology behind the proposed approach. 3 M ETHODOLOGY Let F be a trained feed forward neural network with L layers, each comprising a weight tensor Wl. Let Qbe a quantization operator such that the quantized weights Q(Wl) are represented on b bits. The most popular such operator is the uniform one. We argue that, despite its simplicity, the choice of such a uniform operator is responsible for a signiﬁcant part of the quantization error. In fact, the weights Wl most often follow a bell-shaped distribution for which uniform quantization is ill-suited: intuitively, in such a case, we would want to quantize more precisely the small weights on the peak of the distribution. For this reason, the most popular non-uniform quantization scheme is logarithmic quantization, outputting superior performance. Practically speaking, however, it consists in replacing the quantized multiplications by bit-shift operations. As a result, these methods have limited adaptability as the increment speed is hardware dependent. To adress this problem, we look for the non-uniform quantization operators that preserve the nature of matrix multiplications. Formally, taking aside the rounding operation in quantization, we want to 3Arxiv version square root distribution uniform distribution square distribution a=0.5 a=1 a=2 Figure 2: Inﬂuence of the power parameter a on the quantized distribution for weights distributed following a Gaussian prior. In such a case, the reconstruction error is typically minimized for a <1. deﬁne the space Qof functions Qsuch that ∀Q∈Q,∃Q−1 ∈Q s.t. ∀x,y Q −1(Q(x) ∗Q(y)) =x×y (1) where ∗is the intern composition law of the quantized space and ×is the standard multiplication, and Q, Q−1 are the quantization and de-quantization operators, respectively. In the case of uniform quantization and our work ∗will be the multiplication while in other non-uniform works it often corresponds to other operations that are harder to leverage, e.g. bit-shift. Recall that, for now, we omit the rounding operator. The proposed PowerQuant method consists in the search for the best suited operator Qfor a given trained neural network and input statistics. 3.1 A UTOMORPHISMS OF (R∗ +,×) Let Qbe a transformation from R+ to R+. In this case, ∗, the intern composition law in quantized space in (1), simply denote the scalar multiplication operator ×and (1) becomes Q(x) ×Q(y) = Q(x×y) ∀x,y ∈R2 +. In order to deﬁne a de-quantization operation, we need Q−1 to be deﬁned i.e. Qis bijective. Thus, by deﬁnition, Qis a group automorphism of (R∗ +,×). Thus, quantization operators that preserve the nature of multiplications are restricted to automorphisms of (R∗ +,×). The following lemma further restricts the search to power functions. Lemma 1.The set of continuous automorphisms of(R∗ +,×) is deﬁned by the set of power functions Q= {Q: x↦→xa|a∈R}. A proof of this result can be found in Appendix A. For the sake of clarity, we will now include the rounding operation in the quantization operators. Q= { Qa : W ↦→ ⌊ (2b−1 −1)sign(W) ×|W|a max |W|a ⌋⏐⏐⏐a∈R } (2) where W is a tensor and all the operations are performed element-wise. As functions of W, the quantization operators deﬁned in equation 2 are (signed) power functions. Fig 2 illustrates the effect of the power parameter a on quantization (vertical bars). Uniform quantization and a = 1 are equivalent and correspond to a quantization invariant to the weight distribution. For a <1, the quantization is more ﬁne-grained on weight values with low absolute value and coarser on high absolute values. Conversely, fora> 1, the quantization becomes more ﬁne-grained on high absolute values. We now deﬁne the search protocol in the proposed search space Q. 3.2 A UTOMORPHISM SEARCH AS A MINIMIZATION PROBLEM We propose to use the error introduced by quantization on the weights as a proxy on the distance between the quantized and the original model. Reconstruction Error Minimization: The operator Qa is not a bijection. Thus, quantization introduces a reconstruction error summed over all the layers of the network, and deﬁned as follows: ϵ(F,a) = L∑ l=1 Wl −Q−1 a (Qa(Wl))  p (3) 4Arxiv version where ∥·∥p denotes the Lp vector norm (in practice p= 2, see appendix B) and the de-quantization operator Q−1 a is deﬁned as: Q−1 a (W) =sign(W) × ⏐⏐⏐⏐W ×max |W| 2b−1 −1 ⏐⏐⏐⏐ 1 a (4) In practice, the problem of ﬁnding the best exponent a∗= argminaϵ(F,a) in (3) is a locally convex optimization problem (Appendix C.1) which has a unique minimum (see Appendix C.2). We ﬁnd the optimal value for ausing the Nelder–Mead method (Nelder & Mead, 1965) which solves problems for which derivatives may not be known or, in our case, are almost-surely zero (due to the rounding operation). In practice, more recent solvers are not required in order to reach the optimal solution (see Appendix D). Lastly, we discuss the limitations of the proposed metric in Appendix H. 3.3 F USED DE-QUANTIZATION AND ACTIVATION FUNCTION Based on equation 2, the quantization process of the weights necessitates the storage and multipli- cation of W along with a signs tensor, which is memory and computationally intensive. For the weights, however, this can be computed once during the quantization process, inducing no overhead during inference. As for activations, we do not have to store the sign of ReLU activations as they are always positive. In this case, the power function has to be computed at inference time (see al- gorithm 2). However, it can be efﬁciently computed Kim et al. (2021), using Newton’s method to approximate continuous functions in integer-only arithmetic. This method is very efﬁcient in prac- tice as it converges in 2 steps for low bit representations (four steps for int32). Thus, PowerQuant leads to signiﬁcant accuracy gains with limited computational overhead. Conversely, for non-ReLU feed forward networks such as EfﬁcientNets (SiLU) or Image Transformers (GeLU), activations are signed. This can be tackled using asymmetric quantization which consists in the use of a zero-point. In general, asymmetric quantization allows one to have a better coverage of the quantized values support. In our case, we use asymmetric quantization to work with positive values only. Formally, for both SiLU and GeLU, the activations are analytically bounded below by CSiLU = 0.27846 and CGeLU = 0.169971 respectively. Consequently, assuming a layer with SiLU activation with input x and weights W, we have: Q−1 a (Qa(x+ CSiLU)Qa(W)) ≈((x+ CSiLU)aWa) 1 a = xW + CSiLUW (5) The bias term CSiLUW induces a very slight computation overhead which is standard in asymmetric quantization. We provide a detailed empirical evaluation of this cost in Appendix G. Using the adequate value for the bias corrector, we can generalize equation 5 to any activation functionσ. The quantization process and inference with the quantized DNN are summarized in Algorithm 1 and 2. The proposed representation is fully compatible with integer multiplication as deﬁned in Jacob et al. (2018), thus it is fully compatible with integer only inference (see appendix F for more details). Algorithm 1Weight Quantization Algorithm Require: trained neural network F with Llayers to quantize, number of bits b a←solver(min{error(F,a)}) ⊿in practice we use the Nelder–Mead method for l∈{1,...,L }do Wsign ←sign(Wl) ⊿save the sign of the scalar values in W Wl ←Wsign ×|Wl|a ⊿power transformation s←max |Wl| 2b−1−1 ⊿get quantization scale Q: Wl ↦→ ⌊Wl s ⌉ and Q−1 : W ↦→Wsign ×|W ×s| 1 a ⊿qdeﬁne Qand Q−1 end for 4 E XPERIMENTS In this section, we empirically validate our method. First, we discuss the optimization of the expo- nent parameter aof PowerQuant using the reconstruction error, showing its interest as a proxy for the quantized model accuracy from an experimental standpoint. We show that the proposed approach preserves this reconstruction error signiﬁcantly better, allowing a closer ﬁt to the original weight dis- tribution through non-uniform quantization. Second, we show through a variety of benchmarks that 5Arxiv version Algorithm 2Simulated Inference Algorithm Require: trained neural network F quantized with Llayers, input X and exponent a∗ for l∈{1,...,L }do X ←Xa∗ ⊿X is assumed positive (see equation (5)) XQ ←⌊XsX⌉ ⊿where sX is a scale in the input range O←Fl(XQ) ⊿O contains the quantized output of the layer X ← ( σ(O) sXsW )1 a∗ ⊿where σis the activation function and sW the weight scale end for return X the proposed approach signiﬁcantly outperforms state-of-the-art data-free methods, thanks to more efﬁcient power function quantization with optimized exponent. Third, we show that the proposed approach comes at a negligible cost in term of inference speed. 4.1 D ATASETS AND IMPLEMENTATION DETAILS We validate the proposed PowerQuant method on ImageNet classiﬁcation (Deng et al., 2009) (≈ 1.2M images train/50k test). In our experiments we used pre-trained MobileNets (Sandler et al., 2018), ResNets (He et al., 2016), EfﬁcientNets (Tan & Le, 2019) and DenseNets (Huang et al., 2017). We used Tensorﬂow implementations of the baseline models from ofﬁcial repositories, achieving standard baseline accuracies. The quantization process was done using Numpy library. Activations are quantized as unsigned integers and weights are quantized using a symmetric repre- sentation. We fold batch-normalization layers as in Yvinec et al. (2022a). We performed ablation study using the uniform quantization operator over weight values from Kr- ishnamoorthi (2018) and logarithmic quantization from Miyashita et al. (2016). For our compari- son with state-of-the-art approaches in data-free quantization, we implemented the more complex quantization operator from SQuant (Cong et al., 2022). To compare with strong baselines, we also implement bias correction (Nagel et al., 2019) (which measures the expected difference between the outputs of the original and quantized models and updates the biases terms to compensate for this difference) as well as input weight quantization (Nagel et al., 2019). 4.2 E XPONENT PARAMETER FITTING Fig 3 illustrates the evolution of both the accuracy of the whole DNN and the reconstruction error summed over all the layers of the network, as functions of the exponent parameter a. Our target is the highest accuracy with respect to the value of a: however, in a data-free context, we only have access to the reconstruction error. Nevertheless, as shown on Fig 3, these metrics are strongly anti- correlated. Furthermore, while the reconstruction curve is not convex it behaves well for simplex based optimization method such as the Nelder-Mead method. This is due to two properties: locally convex (Appendix C.1) and has a unique minimum (Appendix C.2). Empirically, optimal values a∗ for the exponent parameter are centered on 0.55, which approxi- mately corresponds to the ﬁrst distribution in Fig 2. Still, as shown on Table 1 we observe some variations on the best value for awhich motivates the optimization of afor each network and bit- width. Furthermore, our results provide a novel insight on the difference between pruning and quantization. In the pruning literature (Han et al., 2015; Frankle & Carbin, 2018; Molchanov et al., 2019), the baseline method consists in setting the smallest scalar weight values to zero and keeping unchanged the highest non-zero values, assuming that small weights contribute less to the network prediction. In a similar vein, logarithmic or power quantization with a > 1 roughly quantizes (almost zeroing it out) small scalar values to better preserve the precision on larger values. In prac- tice, in our case, lower reconstruction errors, and better accuracies, are achieved by setting a <1: this suggests that the assumption behind pruning can’t be straightforwardly applied to quantization, where in fact we argue that ﬁnely quantizing smaller weights is paramount to preserve the patterns learned at each layer, and the representation power of the whole network. 6Arxiv version Figure 3: Accuracy/reconstruction error relationship for ResNet and DenseNet quantized in W4/A4. Table 1: Comparison between logarithmic, uniform and the proposed quantization scheme on ResNet 50 trained for ImageNet classiﬁcation task. We report for different quantization conﬁguration (weights noted W and activations noted A) both the top1 accuracy and the reconstruction error (equation 3). Architecture Method W-bit A-bit a∗ Accuracy Reconstruction Error ResNet 50 Baseline 32 32 - 76.15 - uniform 8 8 1 76.15 1.1 ×10−4 logarithmic 8 8 - 76.12 2.0 ×10−4 PowerQuant 8 8 0.55 76.15 1.0 ×10−4 uniform 4 4 1 54.68 3.5 ×10−3 logarithmic 4 4 - 57.07 2.1 ×10−3 PowerQuant 4 4 0.55 70.29 1.9 ×10−3 Another approach that puts more emphasis on the nuances between low valued weights is logarith- mic based non-uniform quantization. In Table 1 and Appendix E, we compare the proposed power method to both uniform and logarithmic approaches. By deﬁnition, the proposed power method nec- essarily outperforms the uniform method in every scenario as uniform quantization is included in the search space. For instance, in int4, the proposed method improves the accuracy by 13.22 points on ResNet 50. This improvement can also be attributed to a better input quantization of each layer, especially on ResNet 50 where the gap in the reconstruction error (over the weights) is smaller. 4.3 C OMPARISON WITH DATA-FREE QUANTIZATION METHODS In table 2, we report the performance of several data-free quantization approaches on ResNet 50. Although no real training data is involved in these methods, some approaches such as ZeroQ (Cai et al., 2020), DSG (Zhang et al., 2021b) or GDFQ (Xu et al., 2020) rely on data generation (DG) in order to calibrate parameters of the method or to apply ﬁne-tuning to preserve the accuracy through quantization. As shown in table 2, in the W8/A8 setup, the proposed PowerQuant method outper- forms other data-free solutions, fully preserving the accuracy of the ﬂoating point model. The gap is even wider on the more challenging low bit quantization W4/A4 setup, where the PowerQuant im- proves the accuracy by1.93 points over SQuant (Cong et al., 2022) and by14.88 points over GDFQ. This shows the effectiveness of the method on ResNet 50. We provide more results on DenseNet (Huang et al., 2017), MobileNet (Sandler et al., 2018), Efﬁcient Net (Tan & Le, 2019) in Appendix J. These results demonstrate the versatility of the method on both large and very compact convnets. In summary, the proposed PowerQuant vastly outperforms other data-free quantization schemes. Last but not least, when compared to recent QAT methods such as OCTA V Sakr et al. (2022), PowerQuant achieves competitive results on both ResNets and MobileNets using either both static or dynamic quantization. This is remarkable since PowerQuant does not involve any ﬁne-tuning of the network. We provide more details on this benchamrk in Appendix I. In what follows, we evaluate PowerQuant on recent transformer architectures for both image and language applications. 7Arxiv version Table 2: Comparison between state-of-the-art post training quantization techniques on ResNet 50 on ImageNet. We distinguish methods relying on data (synthetic or real) or not. In addition to being fully data-free, our approach signiﬁcantly outperforms existing methods. Architecture Method Data W-bit A-bit Accuracy gap ResNet 50 Baseline - 32 32 76.15 - DFQ Nagel et al. (2019) No 8 8 75.45 -0.70 ZeroQ Cai et al. (2020) Synthetic 8 8 75.89 -0.26 DSG Zhang et al. (2021b) Synthetic 8 8 75.87 -0.28 GDFQ Xu et al. (2020) Synthetic 8 8 75.71 -0.44 SQuant Cong et al. (2022) No 8 8 76.04 -0.11 PowerQuant No 8 8 76.15 0.00 DFQ Nagel et al. (2019) No 4 4 0.10 -76.05 ZeroQ Cai et al. (2020) Synthetic 4 4 7.75 -68.40 DSG Zhang et al. (2021b) Synthetic 4 4 23.10 -53.05 GDFQ Xu et al. (2020) Synthetic 4 4 55.65 -20.50 SQuant Cong et al. (2022) No 4 4 68.60 -7.55 PowerQuant No 4 4 70.53 -5.62 Table 3: Comparison of data-free quantization methods on ViT and DeiT trained on ImageNet. model method W / A accuracy ViT baseline -/- 78.05% DFQ (ICCV 2019) 8/8 70.33% SQuant (ICLR 2022) 8/8 68.85% PSAQ (arxiv 2022) 8/8 37.36% PowerQuant 8/8 77.46% DFQ (ICCV 2019) 4/8 66.63% SQuant (ICLR 2022) 4/8 64.62% PSAQ (arxiv 2022) 4/8 25.34% PowerQuant 4/8 75.24% (a) Evaluation for ViT Base model method W / A accuracy DeiT T baseline -/- 72.21% DFQ (ICCV 2019) 8/8 71.32% SQuant (ICLR 2022) 8/8 71.11% PSAQ (arxiv 2022) 8/8 71.56% PowerQuant 8/8 72.23% DFQ (ICCV 2019) 4/8 67.71% SQuant (ICLR 2022) 4/8 67.58% PSAQ (arxiv 2022) 4/8 65.57% PowerQuant 4/8 69.77% (b) Evaluation for DeiT Tiny model method W / A accuracy DeiT S baseline -/- 79.85% DFQ (ICCV 2019) 8/8 78.76% SQuant (ICLR 2022) 8/8 78.94% PSAQ (arxiv 2022) 8/8 76.92% PowerQuant 8/8 79.33% DFQ (ICCV 2019) 4/8 76.75% SQuant (ICLR 2022) 4/8 76.61% PSAQ (arxiv 2022) 4/8 73.23% PowerQuant 4/8 78.16% (c) Evaluation for DeiT Small model method W / A accuracy DeiT B baseline -/- 81.85% DFQ (ICCV 2019) 8/8 80.72% SQuant (ICLR 2022) 8/8 80.60% PSAQ (arxiv 2022) 8/8 79.10% PowerQuant 8/8 81.26% DFQ (ICCV 2019) 4/8 79.41% SQuant (ICLR 2022) 4/8 79.21% PSAQ (arxiv 2022) 4/8 77.05% PowerQuant 4/8 80.67% (d) Evaluation for DeiT Base 4.4 E VALUATION ON TRANSFORMER ARCHITECTURES In Table 3, we quantized the weight tensors of a ViT Dosovitskiy et al. (2021) with85M parameters and baseline accuracy≈78 as well as DeiT T,S and B Touvron et al. (2021) with baseline accuracies 72.2, 79.9 and 81.8 and ≈5M, ≈22M, ≈87M parameters respectively. Similarly to ConvNets, the image transformer is better quantized using PowerQuant rather than standard uniform quantization schemes such as DFQ. Furthermore, more complex and recent data-free quantization schemes such as SQuant, tend to under-perform on the novel Transformer architectures as compared to ConvNets. This is not the case for PowerQuant which maintains its very high performance even in low bit representations. This is best illustrated on ViT where PowerQuant W4/A8 out performs both DFQ and SQuant even when they are allowed 8 bits for the weights (W8/A8) by a whopping 4.91 points. The proposed PowerQuant even outperforms methods dedicated to transformer quantization such as PSAQ Li et al. (2022) on every image transformer tested. 8Arxiv version Table 4: Complementary Benchmarks on the GLUE task quantized in W4/A8. We consider the BERT trans- former architecture. We provide the original performance (from the article) of BERT on GLUE as well as our reproduced results (baseline). task original baseline uniform log SQuant PowerQuant CoLA 49.23 47.90 45.60 45.67 46.88 47.11 SST-2 91.97 92.32 91.81 91.53 91.09 92.23 MRPC 89.47/85.29 89.32/85.41 88.24/84.49 86.54/82.69 88.78/85.24 89.26/85.34 STS-B 83.95/83.70 84.01/83.87 83.89/83.85 84.01/83.81 83.80/83.65 84.01/83.87 QQP 88.40/84.31 90.77/84.65 89.56/83.65 90.30/84.04 90.34/84.32 90.61/84.45 MNLI 80.61/81.08 80.54/80.71 78.96/79.13 78.96/79.71 78.35/79.56 79.02/80.28 QNLI 87.46 91.47 89.36 89.52 90.08 90.23 RTE 61.73 61.82 60.96 60.46 60.21 61.45 WNLI 45.07 43.76 39.06 42.19 42.56 42.72 Table 5: ACE cost of the overhead computations introduced by PowerQuant. Architecture overhead cost accuracy in W6/A6 ResNet 50 0.63% 75.07 DenseNet 121 0.97% 72.71 MobileNet V2 0.57% 52.20 EfﬁcientNet B0 0.80% 58.24 We further compare the proposed power quantization, in W4/A8, on natural language processing (NLP) tasks and report results in Table 4. We evaluate a BERT model (Devlin et al., 2018) on GLUE (Wang et al., 2018) and report both the original (reference) and our reproduced (baseline) results. We compare the three quantization processes: uniform, logarithmic and PowerQuant. Similarly to com- puter vision tasks, the power quantization outperforms the other methods in every instances which further conﬁrms its ability to generalize well to transformers and NLP tasks. In what follows, we show experimentally that our approach induces very negligible overhead at inference time, making this accuracy enhancement virtually free from a computational standpoint. 4.5 I NFERENCE COST AND PROCESSING TIME The ACE metrics was recently introduced in Zhang et al. (2022) to provide a hardware-agnostic measurement of the overhead computation cost in quantized neural networks. In Table 5, we evaluate the cost in the inference graph due to the change in the activation function. We observe very similar results to Table 17. The proposed changes are negligible in terms of computational cost on all tested networks. Furthermore, DenseNet has the highest cost due to its very dense connectivity. On the other hand, using this metric it seems that the overhead cost due to the zero-point technique from section 3.3 for EfﬁcientNet has no signiﬁcant impact as compared to MobileNet and ResNet. In addition, we provide a more detailed discussion on the inference and processing cost of PowerQuant on speciﬁc hardware using dedicated tools in Appendix K. 5 C ONCLUSION In this paper, we pinpointed the uniformity of the quantization as a limitation of existing data- free methods. To address this limitation, we proposed a novel data-free method for non-uniform quantization of trained neural networks for computer vision tasks, with an emphasis on not chang- ing the nature of the mathematical operations involved (e.g. matrix multiplication). This led us to search among the continuous automorphisms of(R∗ +,×), which are restricted to the power functions x→xa. We proposed an optimization of this exponent parameter based upon the reconstruction er- ror between the original ﬂoating point weights and the quantized ones. We show that this procedure is locally convex and admits a unique solution. At inference time, the proposed approach, dubbed PowerQuant, involves only very simple modiﬁcations in the quantized DNN activation functions. We empirically demonstrate that PowerQuant allows a closer ﬁt to the original weight distributions compared with uniform or logarithmic baselines, and signiﬁcantly outperforms existing methods in 9Arxiv version a variety of benchmarks with only negligible computational overhead at inference time. In addi- tion, we also discussed and addressed some of the limitations in terms of optimization (per-layer or global) and generalization (non-ReLU networks). Future work involves the search of a better proxy error as compared with the proposed weight re- construction error as well as the extension of the search space to other internal composition law of R+ that are suited for efﬁcient calculus and inference. ACKNOWLEDGMENTS This work has been supported by the french National Association for Research and Technology (ANRT), the company Datakalab (CIFRE convention C20/1396) and by the French National Agency (ANR) (FacIL, project ANR-17-CE33-0002). This work was granted access to the HPC resources of IDRIS under the allocation 2022-AD011013384 made by GENCI. REFERENCES Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. NeurIPS, pp. 7950–7958, 2019. Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. CVPR Workshops, pp. 696–697, 2020. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. CVPR, pp. 13169–13178, 2020. Wenlin Chen, James Wilson, et al. Compressing neural networks with the hashing trick. ICML, pp. 2285–2294, 2015. Vladimir Chikin and Vladimir Kryzhanovskiy. Channel balancing for accurate quantization of wino- grad convolutions. In CVPR, pp. 12507–12516, 2022. Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net- works for efﬁcient inference. ICCV Workshops, pp. 3009–3018, 2019. Guo Cong, Qiu Yuxian, Leng Jingwen, Gao Xiaotian, Zhang Chen, Liu Yunxin, Yang Fan, Zhu Yuhao, and Guo Minyi. Squant: On-the-ﬂy data-free quantization via diagonal hessian approxi- mation. ICLR, 2022. Andrew R Conn, Katya Scheinberg, and Ph L Toint. On the convergence of derivative-free methods for unconstrained optimization. Approximation theory and optimization: tributes to MJD Powell, pp. 83–108, 1997. J. Deng, W. Dong, et al. ImageNet: A Large-Scale Hierarchical Image Database. CVPR, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. Jun Fang, Ali Shaﬁee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H Hassoun. Post-training piecewise linear quantization for deep neural networks. ECCV, pp. 69– 86, 2020. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2018. Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell Nahmias. Confounding tradeoffs for neural net- work quantization. arXiv preprint arXiv:2102.06366, 2021. 10Arxiv version Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efﬁcient neural network inference. arXiv preprint arXiv:2103.13630, 2021. Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional net- works using vector quantization. arXiv preprint arXiv:1412.6115, 2014. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014. Zbigniew Hajduk. High accuracy fpga activation function implementation for neural networks. Neurocomputing, 247:59–61, 2017. Kun Han, Yuxuan Wang, DeLiang Wang, William S Woods, Ivo Merks, and Tao Zhang. Learning spectral mapping for speech dereverberation and denoising. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(6):982–992, 2015. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016. Kaiming He, Xiangyu Zhang, et al. Deep residual learning for image recognition. CVPR, pp. 770–778, 2016. Horst Herrlich. Axiom of choice, volume 1876. Springer, 2006. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NeurIPS, 2014. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. CVPR, pp. 4700–4708, 2017. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. NeurIPS, 29, 2016. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁcient integer-arithmetic-only inference. CVPR, pp. 2704–2713, 2018. Yongkweon Jeon, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Jeongin Yun, and Dongsoo Lee. Biqgemm: matrix multiplication with lookup table for binary-coding-based quantized dnns. SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–14, 2020. Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro. Mr. biq: Post-training non-uniform quantization based on minimizing the reconstruction error. In CVPR, pp. 12329–12338, 2022. Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer- only bert quantization. In International conference on machine learning, pp. 5506–5518. PMLR, 2021. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. Maximilian Lam, Michael Mitzenmacher, Vijay Janapa Reddi, Gu-Yeon Wei, and David Brooks. Tabula: Efﬁciently computing nonlinear activation functions for secure neural network inference. arXiv preprint arXiv:2203.02833, 2022. Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efﬁcient non- uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi Gu. Patch similarity aware data-free quantization for vision transformers. arXiv preprint arXiv:2203.02250, 2022. Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016. 11Arxiv version Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. CVPR, pp. 11264–11272, 2019. Markus Nagel, Mart van Baalen, et al. Data-free quantization through weight equalization and bias correction. ICCV, pp. 1325–1334, 2019. John A Nelder and Roger Mead. A simplex method for function minimization. The computer journal, 7(4):308–313, 1965. Michael JD Powell. An efﬁcient method for ﬁnding the minimum of a function of several variables without calculating derivatives. The computer journal, 7(2):155–162, 1964. Charbel Sakr, Steve Dai, Rangha Venkatesan, Brian Zimmer, William Dally, and Brucek Khailany. Optimal clipping and magnitude-aware differentiation for improved quantization-aware training. In ICML, pp. 19123–19138. PMLR, 2022. Mark Sandler, Andrew Howard, et al. Mobilenetv2: Inverted residuals and linear bottlenecks.CVPR, pp. 4510–4520, 2018. Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. ICML, pp. 6105–6114, 2019. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021. Mart van Baalen, Brian Kahne, Eric Mahurin, Andrey Kuzmin, Andrii Skliar, Markus Nagel, and Tijmen Blankevoort. Simulated quantization, real power savings. InCVPR, pp. 2757–2761, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceed- ings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. CVPR, pp. 4820–4828, 2016. Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan. Generative low-bitwidth data free quantization. ECCV, pp. 1–17, 2020. Edouard Yvinec, Arnaud Dapogny, and Kevin Bailly. To fold or not to fold: a necessary and sufﬁcient condition on batch-normalization layers folding. IJCAI, 2022a. Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static input quantization. arXiv preprint arXiv:2203.14642, 2022b. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. ECCV, pp. 365–382, 2018. Sai Qian Zhang, Bradley McDanel, HT Kung, and Xin Dong. Training for multi-resolution inference using reusable quantization terms. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 845–860, 2021a. Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantiza- tion. CVPR, pp. 15658–15667, 2021b. Yichi Zhang, Zhiru Zhang, and Lukasz Lew. Pokebnn: A binary pursuit of lightweight accuracy. In CVPR, pp. 12475–12485, 2022. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. ICML, pp. 7543–7552, 2019. 12Arxiv version Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza- tion: Towards lossless cnns with low-precision weights. ICLR, 2017. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 13Arxiv version A P ROOF OF LEMMA 1 In this section, we provide a simple proof for lemma 1 as well as a discussion on the continuity hypothesis. Proof. We have that ∀x ∈R+,Q(x) ×Q(0) =Q(0) and ∀x ∈R+,Q(x) ×Q(1) =Q(x) which induces that Qis either the constant 1 or Q(0) = 0and Q(1) = 1. Because Qis an automorphism we can eliminate the ﬁrst option. Now, we will demonstrate that Qis necessarily a power function. Let nbe an integer, then Q(xn) =Q(x) ×Q(xn−1) =Q(x)2 ×Q(xn−2) =··· = Q(x)n. (6) Similarly, for fractions, we get Q(x 1 n ) ×···× Q(x 1 n ) =Q(x) ⇔Q(x 1 n ) =Q(x) 1 n . Assuming Q is continuous, we deduce that for any rational a∈R, we have Q(xa) =Q(x)a (7) In order to verify that the solution is limited to power functions, we use a reductio ad absurdum. Assume Q is not a power function. Therefore, there exists (x,y) ∈ R2 + and a ∈ R such that Q(x) ̸= xa and Q(y) =ya. By deﬁnition of the logarithm, there exists bsuch that xb = y. We get the following contradiction, from (7), { Q(xba ) =Q(ya) =ya Q(xba ) =Q(xab) =Q(xa)b ̸= ( xab = ya) (8) Consequently, the suited functions Qare limited to power functions i.e. Q= {Q : x ↦→xa|a ∈ R}. We would also like to put the emphasis on the fact that there are other Automorphisms of (R,×). However, the construction of such automorphisms require the axiom of choice Herrlich (2006). Such automorphisms are not applicable in our case which is why the key constraint is being an automorphism rather than the continuous property. B N ORM SELECTION In the minimization objective, we need to select a norm to apply. In this section, we provide the- oretical arguments in favor of the l2 vector norm. Let F be a feed forward neural network with L layers to quantize, each deﬁned by a set of weights Wl = (wl)i,j ∈Rnl×ml and bias bl ∈Rnl . We note (λ(i) l )i the eigenvalues associated with Wl. We want to study the distance d(F,Fa) between the predictive function F and its quantized version Fa deﬁned as d(F,Fa) = max x∈D ∥F(x) −Fa(x)∥p (9) where Dis the domain of F. We prove that minimizing the reconstruction error with respect to ais equivalent to minimizing d(F,Fa) with respect to a. Assume L = 1for the sake of simplicity and we drop the notation l. With the proposed PowerQuant method, we minimize the vector norm ∥W −Q−1 a (Qa(W))∥p p = ∑ i<=n max j<=m |wi,j −Q−1 a (Qa(wi,j))|p (10) For p= 2, the euclidean norm is equal to the spectral norm, thus minimizing∥W−Q−1 a (Qa(W))∥2 is equivalent to minimizing d(F,Fa) for L = 1. However, we know that minimizing for another value of pmay result in a different optimal solution and therefore not necessarily minimized(F,Fa). In the context of data-free quantization, we want to avoid uncontrollable changes on F, which is why we recommend the use of p= 2. 14Arxiv version C M ATHEMATICAL PROPERTIES C.1 L OCAL CONVEXITY We prove that the minimization problem deﬁned in equation 3 is locally convex around the solution a∗. Formally we prove that x↦→ x−Q−1 a (Qa(x))  p (11) is locally convex around a∗deﬁned as arg mina x−Q−1 a (Qa(x))  p. Lemma 2. The minimization problem deﬁned as arg min a {x−Q−1 a (Qa(x))  p } (12) is locally convex around any solution a∗. Proof. We recall that ∂xa ∂a = xalog(x). The function x−Q−1 a (Qa(x)) is differentiable. We assume x∈R, then we can simplify the sign functions (assumexpositive without loss of generality) and note y= max|x|, then ∂Q−1 a (Qa(x)) ∂a = ∂ ⏐⏐⏐ ⌊ (2b−1 −1)xa ya ⌋ ya 2b−1−1 ⏐⏐⏐ 1 a ∂a . (13) This simpliﬁes to ∂Q−1 a (Qa(x)) ∂a = y ∂ ( ⌊B(x y ) a ⌋ B )1 a ∂a , (14) with B = 2b−1 −1. By using the standard differentiation rules, we know that the rounding operator has a zero derivative a.e.. Consequently we get, ∂Q−1 a (Qa(x)) ∂a = −a2y   ⌊ B ( x y )a⌋ B   1 a log   ⌊ B ( x y )a⌋ B  . (15) Now we can compute the second derivative of Q−1 a (Qa(x)), ∂2Q−1 a (Qa(x)) ∂a2 = a4y   ⌊ B ( x y )a⌋ B   1 a log2   ⌊ B ( x y )a⌋ B  . (16) From this expression, we derive the second derivative, using the property(f◦g)′′= f′′◦g×g′2 + f′◦g×g′′and the derivatives |·| 1 p ′ = x|x| 1 p −2 p and |·| 1 p ′′ = 1−p p2 |x| 1 p x2 , then for any xi ∈x ∂2 ⏐⏐xi −Q−1 a (Qa(xi)) ⏐⏐ ∂a2 = 1 −p p2 |xi −Q−1 a (Qa(xi)| 1 p (xi −Q−1a (Qa(xi))2 (∂Q−1 a (Qa(x)) ∂a )2 + (xi −Q−1 a (Qa(xi))|xi −Q−1 a (Qa(xi)| 1 p −2 p ∂2Q−1 a (Qa(x)) ∂a2 (17) We now note the ﬁrst term in the previous additionT1 = 1−p p2 |xi−Q−1 a (Qa(xi)| 1 p (xi−Q−1 a (Qa(xi))2 ( ∂Q−1 a (Qa(x)) ∂a )2 and the second term as a product ofT2 = (xi−Q−1 a (Qa(xi))|xi−Q−1 a (Qa(xi)| 1 p −2 p times T3 = ∂2Q−1 a (Qa(x)) ∂a2 . We know that T1 > 0 and T3 > 0, consequently, and T2 is continuous in a. At a∗ the terms with |xi −Q−1 a (Qa(xi)) |are negligible in comparison with ∂2Q−1 a (Qa(x)) ∂a2 and ( ∂Q−1 a (Qa(x)) ∂a )2 . Consequently, there exists an open set around a∗where T1 >|T2|T3, and ∂2|xi−Q−1 a (Qa(xi))| ∂a2 >0. This concludes the proof. 15Arxiv version Table 6: Minimization of the reconstruction error on a MobileNet V2 for W6/A6 quantization with different solvers. Solver a∗ reconstruction error accuracy Nelder-Mead 0.750 1.12 64.248 Powell (Powell, 1964) 0.744 1.10 64.104 COBYLA (Conn et al., 1997) 0.752 1.11 64.364 C.2 U NIQUENESS OF THE SOLUTION In this section we provide the elements of proof on the uniqueness of the solution of the minimization of the quantization reconstruction error. Lemma 3. The minimization problem over x∈RN deﬁned as arg min a {x−Q−1 a (Qa(x))  p } (18) has almost surely a unique global minimum a∗. Proof. We assume that x can not be exactly quantized, i.e. mina {x−Q−1 a (Qa(x))  p } > 0 which is true almost everywhere. We use a reductio ad absurdum and assume that there ex- ist two optimal solutions a1 and a2 to the optimization problem. We expand the expressionx−Q−1 a (Qa(x))  p and get x−Q−1 a (Qa(x))  p = x− ⏐⏐⏐⏐ ⌊ (2b−1 −1)sign(x) ×|x|a max |x|a ⌋max |x|a 2b−1 −1 ⏐⏐⏐⏐ 1 a sign(x) .p (19) We note the rounding term Ra and get x−Q−1 a (Qa(x))  p = x− ⏐⏐⏐⏐Ra max |x|a 2b−1 −1 ⏐⏐⏐⏐ 1 a sign(x)  p . (20) Assume Ra1 = Ra2 = R, the minimization problem arg mina x− ⏐⏐⏐Rmax |x|a 2b−1−1 ⏐⏐⏐ 1 a sign(x)  p is convex and has a unique solution, thus a1 = a2. Now assume Ra1 ̸= Ra2 . Let’s denoteD(R) the domain of power valuesaover which we have ⌊ (2b−1 −1)sign(x)×|x|a max |x|a ⌋ = R. If there is a value aoutside of D(Ra1 ) ∪D(Ra2 ) such that R′has each of its coordinate strictly between the coordinates of Ra1 and Ra2 , then, without loss of generality, assume that at least half of the coordinates of Ra1 are further away from the corresponding coordinates of xthan one quan- tization step. This implies that there exists a value a′in D(R′) such that x−Q−1 a′ (Qa′(x))  p <x−Q−1 a1 (Qa1 (x))  p. which goes against our hypothesis. Thus, there are up to N possible values for Rthat minimize the problem which happens iff xsatisﬁes at least one coordinate can be either ceiled or ﬂoored by the rounding. The set deﬁned by this condition has a zero measure. D S OLVER FOR MINIMIZATION In the main article we state that we can use Nelder-Mead (Nelder & Mead, 1965) solver to ﬁnd the optimal a∗. We tested several other solvers and report the results in Table 6. The empirical results show that basically any popular solver can be used, and that the Nelder-Mead solver is sufﬁcient for the minimization problem. E C OMPARISON BETWEEN LOG, NAIVE AND POWER QUANTIZATION COMPLEMENTARY RESULTS To complement the results provided in the main paper on ResNet 50, we list in Table 7 more quan- tization setups on ResNet 50 as well as DenseNet 121. To put it in a nutshell, The proposed power 16Arxiv version Table 7: Comparison between logarithmic, uniform and the proposed quantization scheme on ResNet 50 and DenseNet 121 trained for ImageNet classiﬁcation task. We report for different quantization conﬁguration (weights noted W and activations noted A) both the top1 accuracy and the reconstruction error (equation 3). Architecture Method W-bit A-bit a∗ Accuracy Reconstruction Error ResNet 50 Baseline 32 32 - 76.15 - uniform 8 8 1 76.15 1.1 ×10−4 logarithmic 8 8 - 76.12 2.0 ×10−4 PowerQuant 8 8 0.55 76.15 1.0 ×10−4 uniform 6 6 1 75.07 8.0 ×10−4 logarithmic 6 6 - 75.37 4.6 ×10−4 power (ours) 6 6 0.50 75.95 4.3 ×10−4 uniform 4 4 1 54.68 3.5 ×10−3 logarithmic 4 4 - 57.07 2.1 ×10−3 PowerQuant 4 4 0.55 70.29 1.9 ×10−3 DenseNet 121 Baseline 32 32 - 75.00 - uniform 8 8 1 75.00 2.8 ×10−4 logarithmic 8 8 - 74.91 2.5 ×10−4 PowerQuant 8 8 0.60 75.00 2.2 ×10−4 uniform 6 6 1 74.47 1.1 ×10−3 logarithmic 6 6 - 72.71 1.0 ×10−3 power (ours) 6 6 0.50 74.84 0.7 ×10−3 uniform 4 4 1 54.83 4.7 ×10−3 logarithmic 4 4 - 5.28 4.8 ×10−3 PowerQuant 4 4 0.55 68.04 3.1 ×10−3 quantization systematically achieves signiﬁcantly higher accuracy and lower reconstruction error than the logarithmic and uniform quantization schemes. On a side note, the poor performance of the logarithmic approach on DenseNet 121 can be attributed to the skewness of the weight distributions. Formally, ResNet 50 and DenseNet 121 weight values show similar average standard deviations across layers (0.0246 and 0.0264 respectively) as well as similar kurtosis (6.905 and 6.870 respec- tively). However their skewness are signiﬁcantly different: 0.238 for ResNet 50 and more than twice as much for DenseNet 121, with 0.489. The logarithmic quantization, that focuses on very small value is very sensible to asymmetry which explains the poor performance on DenseNet 121. In contrast, the proposed method offers a robust performance in all situations. F H OW TO PERFORM MATRIX MULTIPLICATION WITH POWER QUANT The proposed PowerQuant method preserves the multiplication operations, i.e. a multiplication in the ﬂoating point space remains a multiplication in the quantized space (integers). This allows one to leverage current implementations of uniform quantization available on most hardware Gholami et al. (2021); Zhou et al. (2016). However, while PowerQuant preserves multiplications it doesn’t preserve additions which are signiﬁcantly less costly than multiplications. Consequently, in order to infer under the PowerQuant transformation, instead of accumulating the quantized products, as done in standard quantization Jacob et al. (2018), one need to accumulate the powers of said products. Formally, let’s consider two quantized weightsw1,w2 and their respective quantized inputs x1,x2. The standard accumulation would be performed as followsw1x1+w2x2. In the case of PowerQuant, this would be done as(w1x1) 1 a +(w2x2) 1 a . Previous studies on quantization have demonstrated that such power functions can be computed with very high ﬁdelity at almost no latency cost Kim et al. (2021). G O VERHEAD COST OF ZERO -POINTS IN ACTIVATION QUANTIZATION The overhead cost introduced in equation 5 is well known in general in quantization as it arises from asymmetric quantization. Nonetheless, we share here (as well as in the article) some empirical values. 17Arxiv version Table 8: Overhead induced by asymmetric quantization Architecture parameters overhead run-time overhead (CPU intel-m3) ResNet50 0.25% 4.35% EfﬁcientNet 0.20% 3.38% ViT b16 0.73% 5.14% Table 9: Comparison between the per-layer and global method of power parameter a ﬁtting on a ResNet 5 `a trained for ImageNet classiﬁcation task. Architecture Method W-bit A-bit Accuracy Reconstruction Error ResNet 50 Baseline 32 32 76.15 - per-layer 8 8 76.14 0.8 ×10−4 global 8 8 76.15 1.0 ×10−4 per-layer 4 4 64,19 1.7 ×10−3 global 4 4 70.29 1.9 ×10−3 These are empirical results from our own implementation. We include ResNet50 as it can also be quantized using asymmetric quantization although in our research, we only applied asymmetric quantization to SilU and GeLU based architectures. We included these results in the appendix of the revised article. It is worth noting that according to LSQ+ Bhalgat et al. (2020), asymmetric quantization can be achieved at virtually not run-time cost. H L IMITATIONS OF THE RECONSTRUCTION ERROR METRIC In the proposed PowerQuant method, we ﬁt the parameter abased on the reconstruction error over all the weights, i.e. over all layers in the whole network. Then, we perform per-channel quantization layer by layer independently. However, if the ﬁnal objective is to minimize the reconstruction error from equation (3), a more efﬁcient approach would consist in ﬁtting the parameter aseparately for each layer. We note a∗ l such that for every layer lwe have a∗ l = arg min a {Wl −Q−1 a (Qa(Wl))  p } (21) Then the network (F,(al)∗) quantized with a per-layer ﬁt of the power parameter will satisfy L∑ l=1 Wl −Q−1 al (Qal (Wl))  p < L∑ l=1 Wl −Q−1 a (Qa(Wl))  p (22) if and only if their exists at least one lsuch that al ̸= a. Consequently, if the reconstruction error was a perfect estimate of the resulting accuracy, the per-layer strategy would offer an even higher accuracy than the proposed PowerQuant method. Unfortunately, the empirical evidence, in table 9, shows that the proposed PowerQuant method achieves better results in every benchmark. This observation demonstrates the limits of the measure of the reconstruction error. We explain this phe- nomenon by the importance of inputs and activations quantization. This can be seen as some form of overﬁtting the parameters al on the weights which leads to poor performance on the activation quantization and prediction. In the general sens, this highlights the limitations of the reconstruction error as a proxy for maximizing the accuracy. Previous results can be interpreted in a similar way. For instance, in SQuant Cong et al. (2022) the author claim that it is better to minimize the abso- lute sum of errors rather than the sum of absolute errors and achieve good performance in data-free quantization. I I MPROVEMENT WITH RESPECT TO QAT In the introduction, we argued that data-driven quantization schemes performance deﬁne an upper- bound on data-free performance. Our goal was to narrow the resulting gap between these methods. In Table 10, we report the evolution in the gap between data-free and data-driven quantization tech- niques. These empirical results validate the signiﬁcant improvement of the proposed method at narrowing the gap between data-free and data-driven quantization methods by 26.66% to 29.74%. 18Arxiv version Table 10: Performance Gap as compared to Data-driven techniques on ResNet 50 quantization in W4/A4. The relative gap improvement to the state-of-the-art SQuant [6], is measured as gs−gp gs with gs = ∗−SQuant ∗ and gp = ∗−PowerQuant ∗ where ∗ is the performance of a data-driven method data-driven method SQuant PowerQuant relative gap OCTA V Sakr et al. (2022) (ICML) 8,72% 6,15% +29,47% SQ van Baalen et al. (2022) (CVPR) 8,64% 6,07% +29,74% WinogradQ Chikin & Kryzhanovskiy (2022) (CVPR) 9,55% 7,00% +26,66% Mr BiQ Jeon et al. (2022) (CVPR) 8,74% 6,17% +29,38% Table 11: Performance gap between data-free PowerQuant and short-retraining OCTA V Sakr et al. (2022). method architecture quantization accuracy PowerQuant ResNet 50 W4/A4 70.53 OCTA V ResNet 50 W4/A4 75.84 PowerQuant MobileNet V2 W4/A4 45.84 OCTA V MobileNet V2 W4/A4 0.66 Table 12: Performance gap between PowerQuant and OCTA V Sakr et al. (2022) (using an additional short retraining), both using dynamic range estimation. method architecture quantization accuracy PowerQuant ResNet 50 W4/A4 76.02 OCTA V ResNet 50 W4/A4 76.46 PowerQuant MobileNet V2 W4/A4 71.65 OCTA V MobileNet V2 W4/A4 71.23 In order to complete our comparison to QAT methods, we considered the short-re-training (30 epochs) regime from OCTA V in Table 11. We can draw two observations from this comparison. First, on ResNet 50, OCTA V achieves remarkable results by reach near full-precision accuracy. Still the proposed method does not fall too far back with only 5.31 points lower accuracy while being data-free. Second, on very small models such as MobileNet V2, using a strong quantization oper- ator rather than a short re-training leads to a huge accuracy improvement as PowerQuant achieves 45.18 points higher accuracy. This is also the ﬁnding of the author in OCTA V , as they conclude that models such as MobileNet tend to be very challenging to quantize using static quantization and short re-training. In Table 12, we draw a comparison between the proposed PowerQuant and the QAT method OCTA V Sakr et al. (2022), both using dynamic quantization (i.e. estimating the ranges of the activations on- the-ﬂy depending on the input). As expected, the use of dynamic ranges has a considerable inﬂuence on the performance of both quantization methods. As can be observed the QAT method OCTA V achieved very impressive results and even outperforming the full-precision model on ResNet 50. Nevertheless, it is on MobileNet that the inﬂuence of dynamic ranges is the most impressive. For OCTA V , we observe a boost of almost 71 points going from almost random predictions to near exact full-precision accuracy. It is to be noted that PowerQuant does not fall shy in front of these perfor- mances, as using static quantization we still manage to preserve some of the predictive capability of the model. Furthermore, using dynamic quantization, Powerquant achieves similar accuracies than OCTA V while not involving any ﬁne-tuning, contrary to OCTA V . All in all, we can conclude that the proposed data-free method manages to hold close results to a state-of-the-art QAT method in some context. An interesting future work could be the extension of PowerQuant as a QAT method and possibly learning the power parameter athat we use in our quantization operator. J C OMPARISON TO STATE-OF-THE-ART DATA-FREE QUANTIZATION ON OTHER CONV NETS In addition to our evaluation on ResNet, we propose some complementary results on DenseNet in Table 13 as well as the challenging and compact architectures MobileNet and EfﬁcientNet in Table 19Arxiv version Table 13: Comparison between state-of-the-art post-training quantization techniques on DenseNet 121 on Im- ageNet. We distinguish methods relying on data (synthetic or real) or not. In addition to being fully data-free. our approach signiﬁcantly outperforms existing methods. Architecture Method Data W-bit A-bit Accuracy gap DenseNet 121 Baseline - 32 32 75.00 - DFQ Nagel et al. (2019) No 8 8 74.75 -0.25 SQuant Cong et al. (2022) No 8 8 74.70 -0.30 OMSE Choukroun et al. (2019) Real 8 8 74.97 -0.03 SPIQ Yvinec et al. (2022b) No 8 8 75.00 -0.00 PowerQuant No 8 8 75.00 -0.00 DFQ Nagel et al. (2019) No 4 4 0.10 -74.90 SQuant Cong et al. (2022) No 4 4 47.14 -27.86 SPIQ Yvinec et al. (2022b) No 4 4 51.83 -23.17 OMSE Choukroun et al. (2019) Real 4 4 57.07 -17.93 PowerQuant No 4 4 69.37 -5.63 Table 14: Complementary Benchmarks on ImageNet Architecture Method Data W-bit A-bit Accuracy gap MobileNet V2 Baseline - 32 32 71.80 - DFQ (ICCV 2019) No 8 8 70.92 -0.88 SQuant (ICLR 2022) No 8 8 71.68 -0.12 SPIQ (W ACV 2023) No 8 8 71.79 -0.01 PowerQuant No 8 8 71.81 +0.01 DFQ (ICCV 2019) No 4 4 27.1 -44.70 SQuant (ICLR 2022) No 4 4 28.21 -43.59 SPIQ (W ACV 2023) No 4 4 31.28 -40.52 PowerQuant No 4 4 45.84 25.96 EfﬁcientNet B0 Baseline - 32 32 77.10 - DFQ (ICCV 2019) No 8 8 76.89 -0.21 SQuant (ICLR 2022) No 8 8 76.93 -0.17 SPIQ (W ACV 2023) No 8 8 77.02 -0.08 PowerQuant No 8 8 77.05 -0.05 DFQ (ICCV 2019) No 6 6 43.08 -34.02 SQuant (ICLR 2022) No 6 6 54.51 -32.59 SPIQ (W ACV 2023) No 6 6 74.67 -2.43 PowerQuant No 6 6 75.13 -1.97 14 as well as weights only for Bert in Table 16. In table 13, we report the performance of other data-free quantization processes on DenseNet 121. The OMSE method (Choukroun et al., 2019) is a post-training quantization method that leverages validation examples during quantization, thus cannot be labelled as data-free. Yet, we include this work in our comparison as they show strong performance in terms of accuracy at a very low usage of real data. As showcased in table 13, the proposed PowerQuant method almost preserves the ﬂoating point accuracy in W8/A8 quantization. Additionally, on the challenging W4/A4 setup, our approach improves the accuracy by a remarkable 12.30 points over OMSE and 17.54 points over SQuant. This is due to the overall better efﬁciency of non-uniform quantization, that allows a theoretically closer ﬁt to the weight distributions of each DNN layer. The results on MobileNet and EfﬁcientNet from Table 14 conﬁrm our previous ﬁndings. We observe a signiﬁcant boost in performance from PowerQuant as compared to the other very competitive data-free solutions. K O VERHEAD COST DISCUSSION In this section, we provide more empirical results on the inference cost of the proposed method. Table 17 shows the inference time of DNNs quantized with our approach (which only implies modi- ﬁcations of the activation function and a bias correction-see Section 3.3). For DenseNet, ResNet and MobileNet V2, the baseline activation function is the ReLU, which is particularly fast to compute. 20Arxiv version Table 15: Complementary Benchmarks on Vision Transformers for ImageNet Architecture Method Data W-bit A-bit Accuracy gap CaiT xxs24 Baseline - 32 32 78.524 - DFQ (ICCV 2019) No 8 8 77.612 -0.912 SQuant (ICLR 2022) No 8 8 77.638 -0.886 PowerQuant No 8 8 77.718 -0.806 DFQ (ICCV 2019) No 4 8 74.192 -4.332 SQuant (ICLR 2022) No 4 8 74.224 -4.300 PowerQuant No 4 8 75.104 -3.420 CaiT xxs36 Baseline - 32 32 79.760 - DFQ (ICCV 2019) No 8 8 79.000 -0.760 SQuant (ICLR 2022) No 8 8 78.914 -0.846 PowerQuant No 8 8 79.150 -0.610 DFQ (ICCV 2019) No 4 8 76.906 -2.854 SQuant (ICLR 2022) No 4 8 76.896 -2.864 PowerQuant No 4 8 77.702 -2.058 CaiT s24 Baseline - 32 32 83.368 - DFQ (ICCV 2019) No 8 8 82.802 -0.566 SQuant (ICLR 2022) No 8 8 82.784 -0.584 PowerQuant No 8 8 82.766 -0.602 DFQ (ICCV 2019) No 4 8 81.474 -1.894 SQuant (ICLR 2022) No 4 8 81.486 -1.882 PowerQuant No 4 8 81.612 -1.756 Table 16: Complementary Benchmarks on the GLUE task. We consider the BERT transformer architecture. We provide the reference performance of BERT on GLUE as well as our reproduced results (baseline). task (reference) baseline uniform log power CoLA 49.23 47.90 46.24 46.98 47.77 SST-2 91.97 92.32 91.28 91.85 92.32 MRPC 89.47/85.29 89.32/85.41 86.49/81.37 86.65/82.86 89.32/85.41 STS-B 83.95/83.70 84.01/83.87 83.25/83.14 84.01/83.81 84.01/83.87 QQP 88.40/84.31 90.77/84.65 90.23/84.61 90.76/84.65 90.77/84.65 MNLI 80.61/81.08 80.54/80.71 79.72/79.13 79.22/79.71 80.54/80.71 QNLI 87.46 91.47 90.32 91.43 91.47 RTE 61.73 61.82 59.23 61.27 61.68 WNLI 45.07 43.76 40.85 42.80 42.85 Nevertheless, our results show that our approach leads to only increasing by1% the whole inference time on most networks. More precisely, in the case of ResNet 50, the change in activation function induces a slowdown of0.15%. The largest runtime increase is obtained on DenseNet with a 3.4% in- crease. Lastly, note that our approach is also particularly fast and efﬁcient on EfﬁcientNet B0, which uses SiLU activation, thanks to the bias correction technique introduced in Section 3.3. Overall, the proposed approach can be easily implemented and induces negligible overhead in inference on GPU. To furthermore justify the practicality of the proposed quantization process, we recall that the only practicality concern that may arise is on the activation function as the other operations are strictly identical to standard uniform quantization. According to Kim et al. (2021) efﬁcient power functions can be implemented for generic hardware as long as they support standard integer arithmetic, i.e. as long as they support uniform quantization. When it comes to Field-Programmable Gate Array (FPGA), activation functions are implemented using look-up tables (LUT) as detailed in Hajduk (2017). More precisely, they are pre-computed using Pad ´e approximation which are quotients of polynomial functions. Consequently the proposed approach would simply change the polynomial values but not the inference time as it would still rely on the same number of LUTs. In general, activation functions that are non-linear can be very effectively implemented in quantiza- tion runtime Lam et al. (2022). However these considerations are hardware agnostic. In order to circumvent this limitation and address any concerns to our best, we conducted a small study using 21Arxiv version Table 17: Inference time, in seconds, over ImageNet using batches of size 16 of several networks on a 2070 RTX GPU. We also report the accuracy for W6/A6 quantization setup. Architecture Method inference time (gap) accuracy ResNet 50 Uniform 164 75.07 Power Function 164 (+0.2) 75.95 DenseNet 121 Uniform 162 72.71 Power Function 167 (+4.8) 74.84 MobileNet V2 Uniform 85 52.20 Power Function 86 (+0.7) 64.09 EfﬁcientNet B0 Uniform 125 58.24 Power Function 127 (+2.2) 66.38 Table 18: Inference cost each component of a convolutional layer and percentage of total in terms of number of cycles on a wide range of simulated hardware using nntool from GreenWaves. operation number of cycles number of ops % of total cycles % of total ops convolution 22950 442368 85.482% 99.310% bias 2033 1024 7.573% 0.229% relu 924 1024 3.442% 0.229% power function 940 1024 3.502% 0.229% Table 19: We report the processing time in seconds (on an Intel(R) Core(TM) i9-9900K CPU) required to quantize a trained neural network such as ResNet, MobileNet, DenseNet or EfﬁcientNet. Architecture GDFQ SQuant Uniform Power MobileNet V2 7.103 134 <1 <1 ResNet 50 11.103 320 <1 1.3 the simulation tool nntool from GreenWaves, a risc-v chips manufacturer that enables to simulate inference cost of quantized neural networks on their gap unit. We tested a single convolutional layer with bias and relu activation plus our power quantization operation and reported the number of cycles and operations. These results demonstrate that even without any optimization the proposed method has a marginal computational cost on MCU inference which corroborates our previous em- pirical results. We would like to put the emphasis on the fact that this cost could be further reduced via optimizing the computation of the power function using existing methods such as Kim et al. (2021). Similarly, we measure the empirical time required to perform the proposed quantization method on several neural networks and report the results in table 19. These results show that the proposed PowerQuant method offers outstanding trade-offs in terms of compression and accuracy at virtually no cost over the processing and inference time as compared to other data-free quantiza- tion methods. For instance, SQuant is a sophisticated method that requires heavy lifting in order to efﬁciently process a neural network. On a CPU, it requires at least 100 times more time to reach a lower accuracy than the proposed method as we will showcase in our comparison to state-of-the-art quantization schemes. 22",
      "meta_data": {
        "arxiv_id": "2301.09858v1",
        "authors": [
          "Edouard Yvinec",
          "Arnaud Dapogny",
          "Matthieu Cord",
          "Kevin Bailly"
        ],
        "published_date": "2023-01-24T08:30:14Z",
        "pdf_url": "https://arxiv.org/pdf/2301.09858v1.pdf"
      }
    },
    {
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
      "abstract": "The majority of the research on the quantization of Deep Neural Networks\n(DNNs) is focused on reducing the precision of tensors visible by high-level\nframeworks (e.g., weights, activations, and gradients). However, current\nhardware still relies on high-accuracy core operations. Most significant is the\noperation of accumulating products. This high-precision accumulation operation\nis gradually becoming the main computational bottleneck. This is because, so\nfar, the usage of low-precision accumulators led to a significant degradation\nin performance. In this work, we present a simple method to train and fine-tune\nhigh-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\naccumulators, with no significant degradation in accuracy. Lastly, we show that\nas we decrease the accumulation precision further, using fine-grained gradient\napproximations can improve the DNN accuracy.",
      "full_text": "TOWARDS CHEAPER INFERENCE IN DEEP NETWORKS WITH LOWER BIT-WIDTH ACCUMULATORS Yaniv Blumenfeld Technion, Israel yanivblm6@gmail.com Itay Hubara Technion, Israel itayhubara@gmail.com Daniel Soudry Technion, Israel daniel.soudry@gmail.com ABSTRACT The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, 12-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy. 1 I NTRODUCTION Deep Neural Networks (DNNs) quantization (Hubara et al., 2017; Sun et al., 2020; Banner et al., 2018; Nagel et al., 2022; Chmiel et al., 2021) have been generally successful at improving the efficiency of neural networks’ computation without harming the accuracy of the network Liang et al. (2021). The suggested methods aim to reduce the cost of the Multiply-And-Accumulate (MAC) operations for both training and inference. To this end, they quantize the weights, activations, and gradients. For applications utilizing such quantization methods, the cost of multiplications, commonly considered to be the computational bottleneck, can be substantially reduced. However, the accumulation of computed products is still performed with high-precision data types. Consequently, the cost of the accumulation, as a component of MAC operations, becomes increasingly dominant in performance breakdowns (Sakr et al., 2019; Ni et al., 2020; Chmiel et al., 2021). For example, when the weights and activations are in the common FP8 format, van Baalen et al. (2023) showed the accumulation becomes a computational bottleneck. For example, they conducted experiments to estimate the raw gate count for various FP8 implementations (a first-order approx- imation for power and area) and observed a 2× reduction in gate count when employing FP16 accumulators instead of FP32. Similarly, Ni et al. (2020) reported analogous findings for INT8, demonstrating that an 8-bit×8-bit multiplier consumes a comparable amount of power and silicon area to a 32-bit accumulator. In this study, we focus on reducing the numerical precision of the accumulation operation in DNNs. Building our solution on top of the emerging FP8 format, which has gained prominence for both training and inference on the most prevalent hardware Andersch et al. (2022), we aim to optimize such DNNs, to enable inference on hardware with Low Bit-width Accumulators (LBAs). Our main contributions are: • We propose a simple scheme for fine-tuning models with 12-bit accumulators for a variety of tasks, and show this method can already achieve strong performance. For example, we show for the first time that 12-bits accumulators can be used in ResNets on ImageNet, with no significant degradation in accuracy. • We examine more fine-grained approaches, in which, for the first time, we backpropagate through the entire accumulation-computation graph. Though much more expensive during training, such fine-grained backpropagation can be used to significantly improve the accuracy of DNNs with LBAs at lower bit-widths. 1 arXiv:2401.14110v1  [cs.LG]  25 Jan 20242 P RELIMINARIES : Q UANTIZED NEURAL NETWORKS 2.1 Q UANTIZED WEIGHTS AND ACTIVATIONS The quantization of neural networks is, by now, a standard practice for achieving efficient neural networks. Unlike traditional scientific computation, that often (Bailey, 2005) requires high-precision floating point arithmetic (e.g., FP64) to achieve accurate results, it was observed (Gupta et al., 2015) that deep neural networks can maintain high accuracy when the weights and activations in the network are represented in low bit representation. As a result, training deep neural networks (DNNs) using half-precision (FP16) arithmetic became the default setup for modern Deep Learning applications (Brown et al., 2020). Lower precision representation (INT8, FP8, INT4, FP4, and Binary) (Sun et al., 2019; 2020; Courbariaux et al., 2016) is also used for a variety of deep learning applications, for either training or inference, albeit using them is more experimental and may result in lower model performance, depending on the specific application. Quantization of Weights and Activations (W/A) has two main benefits. • Lower memory footprint: By reducing the number of bits used for representation of each numerical value, W/A quantization can significantly reduce the memory required for storing and using a neural network. Consequently, W/A quantization enables storing larger models (with more parameters and activations) on DL accelerators with finite storage and improves the computation efficiency of smaller models by mitigating memory bottlenecks. • Reduced complexity of multiplication operation: Neural networks commonly compute mul- tiplications of weight and activation pairs. When both weight and activation are represented at a lower precision, it is possible to perform the multiplication operation with cheaper hardware (smaller area, less energy). This allows us to do more multiplication operations per second, provided that the hardware was designed to support these lower-precision operations. Numerical values are typically represented using either fixed, or floating point format. Methods for quantization of DNNs can be divided accordingly. 2.2 F IXED POINT QUANTIZATION Given a full-precision value x, a fixed number of bits B, and an integer b (exponent-bias), we define the fixed-point quantization of x as: Rmin ≡ −2B−b−1 Rmax ≡ 2−b \u0000 2B−1 − 1 \u0001 QFIXED B,b (x) ≡    Rmin x ≤ Rmin Rmax x ≥ Rmax 2−bRound \u0000 x · 2b\u0001 else (1) As we can see from Eq. (1), the process of fixed-point quantization involves two explicit changes to the value of x. First, we round x · 2b to an integer value. The rounding operation can be done using a variety of operations (such as Floor, Ceil, Nearest-Neighbour, or Stochastic Rounding (Wang et al., 2018)), but will result in a loss of information either way, with a rounding error that decreases as we increase the parameter b: ∆ ∼ 2−b. If the value of x is sufficiently small |x| < 2−b, the quantization noise will exceed the represented value (∆ > |x|) and we have no way to accurately represent the value of x. We will refer to this event as underflow. Second, we have a limited range for representation, that increases with the number of bits B and decreases with b. We refer to the event when x is outside the range (Rmin, Rmax) as overflow, noting that the quantization error in this case is unbounded. Integer quantization is a specific case of fixed-point quantization, where the exponent biasb is set to 0. While we defined the exponent-bias b to be an integer, it is important to note that non-integer values could have worked mathematically just as well to define valid quantization operations. The main benefit of choosing b to be an integer is the efficiency of computing power-of-two multiplications in hardware. The main advantage of fixed point quantization comes from its relative simplicity. Integer multiplica- tion (and addition) are generally considered to be cheaper on hardware when compared with floating point operations. 22.3 F LOATING POINT QUANTIZATION Given a full-precision scalar value x, number of mantissa bits M, number of exponent bits E, and an integer b (exponent-bias), we define the floating point (Dekker, 1971) quantization M/E: s ≡ 1 2 (1 − sign(x)) , e ≡ ⌊log2(|x|)⌋ m = 2−M Round \u0000 2M (|x|2−e − 1) \u0001 ROF ≡ 22E−b−1 \u0000 2 − 2−M \u0001 , R UF = 2−b QFLOAT M,E,b (x) ≡ (−1)s    ROF |x| ≥ROF 0 |x| < RUF 2e (m + 1) else (2) Note that 1 ≤ |x|2−e < 2, due to the definition of e, which helps make sense of the quantization operation in Eq. (2). The total number of bits used for this representation is B = M + E + 1: 1 sign bit (s), M mantissa bits (m) and E exponent bits (e). As we can see, floating point representation can cover a larger range of values when compared with a fixed point representation that uses the same amount of bits and exponent bias, (ROF/UF depends on 22±E while Rmax, Rmin depends on 2B), reducing the occurrence of overflow and underflow events. Unlike fixed-point representation, which had a fixed bound for quantization error (∆ ∼ 2−b) within the represented range, the quantization error for floating point representation varies, depending on the magnitude of x: ∆ ∼ 2e−M . As a direct result, floating point’s arithmetic also adds additional complexity, in the form of swamping Higham (1993). When performing an addition over two floating points values ¯z = z1 +(FP) z2 ≡ QFLOAT M,E,b (z1 + z2), it is possible that the precision of ¯z will not be sufficient for full-representation of its summands, causing the least significant bits to be swamped out — resulting in a ‘noisy‘ addition operation. In the extreme case, denoted as Full-Swamping, if |z1| > 2M+1|z2|, z2 is swamped out entirely, so ¯z = z1 despite z2 being non-zero. In contrast, fixed-point addition will always be exact, as long as the sum remains within the representation range (no overflow). 2.4 L OW BIT-WIDTH ACCUMULATORS When performing a general matrix multiplication (GEMM) operation, (e.g. matrix-multiplication, or convolution), each individual scalar computed during the operation can be expressed as the sum of product pairs y = N−1X i=0 xiwi. (3) Here, y is a scalar component of the output tensor of the GEMM operation, N is the accumulations size (i.e., the number of summands used per scalar output), while {xi}N−1 i=0 and {wi}N−1 i=0 are two series of scalar inputs used for the calculation of y. The values in both series originate from the input tensors, but the exact mapping, from tensors to series, will depend on the performed operation (see Appendix A for more details). Due to the common structure of the multiply-accumulate operation, hardware implementations of GEMM operation often rely on the fundamental Fused Multiply-Add (FMA) operation, defined as FMA(x, w, s) ≡ x · w + s, with x, w, sbeing scalars. Our goal in this work will be to decrease the cost of the FMA component. Previous discussed methods, such as W/A quantization, have been helpful in reducing the cost of the multiplication of FMA. In contrast, the accumulation component of FMA has been studied to a much lesser extent. In (Wang et al., 2018), the authors show that training a neural network with FP16 accumulators can result in noisy training, with a modest loss of accuracy. To mitigate this, the paper recommends chunk-based accumulation and floating-point stochastic rounding. Chunk-based accumulation changes the order of accumulation, while stochastic rounding is a method where a small, random noise is added to the result of high-precision summation, before the result is cast to a low-precision representation. While successful at closing the gap (e.g., for ResNet18 on ImageNet), both methods may prove difficult to implement on modern hardware. Specifically, the order of accumulation on DL accelerators will usually depend on their block architecture and is not easily configured. Moreover, stochastic rounding requires an implicit addition operation, which is projected to increase the cost of hardware addition, negating the benefit of using LBAs. Sakr et al. (2019) examined the effect of low precision accumulators on training through the accu- mulation variance statistic, which they theoretically derive, given several statistical assumptions on the distribution of the summands. In Ni et al. (2020), the authors propose WrapNet, where the 3additions are performed with 8 and 12 integer accumulators with wrap-around. WrapNet is shown to perform complex inference tasks (e.g. ImageNet classification) with extreme quantization (e.g., 7 bits activations, 2 bit weights, and 12 bits accumulators), but it does suffer a noticeable accuracy degradation in this setup, for tasks such as ImageNet classification. Although mostly experimental, FP16 accumulation was integrated in the design of several commercial products (Agrawal et al., 2021), including the tensor cores in the Hopper architecture (NVIDIA) Andersch et al. (2022). 3 F INE -TUNING NEURAL NETWORKS WITH LOW-BIT ACCUMULATORS One key difference between W/A quantization and quantization of the accumulators is that accumula- tion is an internal FMA operation, which is not generally visible to the software user. To simulate the effect of quantized FMA component, we implement the GEMM operations (convolution/ matrix multiply) in CUDA, where the FMA operation is replaced with our custom FMAq operation: FMAq(x, w, s) ≡ Qacc (Qprod (x · w) + s) , (4) as illustrated in Fig. 1. In all experiments, we use a constant chunk size of 16, based on the sizes exposed to the user of NVIDIA’s tensor cores. It is important to highlight that the product and W i X i  Multiply  Q W Q A  S i - 1  Q p r o d  Add S i Q a c c  FMA  w 0 x 0  FMA  w 1 x 1  FMA  w n - 1 x n - 1  FMA  w n x n  FMA  w n + 1 x n + 1  FMA FMA  w 2 n - 1 x 2 n - 1  Add  w N - n x N - n  FMA FMA FMA  w N - 1 x N - 1 w N - n + 1 x N - n + 1  Q acc  Add  Q acc  Figure 1: Left: an illustration of quantized FMA component, as simulated in our work. Unlike the W/A quantization operations (QW (w), QA(x)) that can be efficiently performed in software, Qprod and Qacc are explicitly internal hardware operations, intended to simulate the logic of a cheaper hardware component. Right: Illustration of chunk-based accumulation, with chunk base of n. Chunk- based accumulation is useful for reducing error caused by swamping, but the chunk size is not easily configured and will usually depend on the architecture design of the systolic array. accumulator quantization functions (Qprod and Qacc) are intended to simulate the hardware, rather than suggest an implementation for it. Breaking down the FMA to components in hardware would, in practice, undermine its efficiency — as it will no longer be ‘fused’. Taking this into account, Qprod and Qacc must remain simple and computationally efficient. For example, ‘round to nearest’ and stochastic rounding methods, which are taken for granted for W/A quantization, will not be available to us during inference, as their hardware implementation would still perform addition internally with a higher number of bits. Our quantization will instead rely on the simple ‘floor’ operation, implemented in software via bit-mask. For Hardware analysis of our implementation, see Appendix E. As discussed in section 2.3, floating point quantization can be broken down into 3-distinct events: underflow, overflow and swamping. Eventually, our low-precision model will have to handle all three events. We will, however, start by examining their individual properties, as displayed in Tab. 1. Our main insight from Tab. 1, is that underflow events are expected to have the least significant effect over the network output (They have the lowest absolute error, since the default value for the exponent bias b, as used by the common FP32/FP16 definitions, is b = 2E−1.). In Fig. 2, we evaluate the correctness of this claim, and show that the wide-scope loss-landscape of an LBA ResNet is barely affected when we ignore UF events. And yet, the large relative error induced during underflow 4Event Condition Key Parameters Absolute Error (bound): ∆ = |Q(x) − x| Relative Error: ∆ |x| Overflow (OF) |x| ≳ 22E−b E, −b ∞ (0%, ∞) Underflow (UF) |x| < 2−b E, b 2−b 100% Swamping No OF/UF M 2⌊log2(|x|)⌋−M \u0002 2−M−1, 2−M \u0003 Table 1: Properties of each type of floating-point quantization event. (small elements are effectively replaced with zero), will cause significant optimization errors for gradient-based methods: During fine-tuning, we can expect the magnitude of the weight updates to be proportional to the magnitude of the corresponding weights, causing the underflow region to be particular hard region to ‘escape’ from. Values that are stuck at underflow are effectively excluded from the training since the forced value of zero prevents them from learning meaningful correlations. (see Appendix F for more details.) 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 2 4 6 8 (a) Baseline LBA 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 2 4 6 8 (b) Excluding UF 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 12 2 4 6 8 10 (c) Excluding Swamping Figure 2: Wide scope loss landscapes Li et al. (2018) of an LBA resnet50, using pre-trained ResNet50 weights (CIFAR10, FP32). Here, we compare the qualitative effect of different components in floating points quantization over the network output: In (a), we use a complete implementation of FP quantization during convolution accumulation, with 7 Mantissa and 4 Exponent bits. In (b), we repeat the previous experiment but ignore underflow events during quantization. For comparison, in (c), we repeat the original experiment, but add 16 additional bits to the mantissa, greatly diminishing the effect of swamping, without affecting the role of underflow. All landscapes appear similar, but while the effect of excluding swamping events (c) is visible, the loss landscapes of networks with (a) and without (b) underflow are hardly distinguishable. Therefore, we propose the following method: Starting off with the weights of a pre-trained net- work (trained in full-precision), we will design a network that utilizes quantized FMA for forward- propagation, excluding underflow events, and perform a standard gradient-based optimization (i.e. Stochastic gradient decent, while keeping the backward implementation of each operation as it was with full-precision FMAs). Once we converge to some accuracy value, we will enable the underflow implementation and proceed with further fine-tuning. As seen in Tab. 1, the exponent bias (b) can be configured to control underflow and overflow events, with a clear trade-off between the former and the latter. Previous works Kuzmin et al. (2022) have made the insight, that the default value b = 2E−1 is not always suitable for neural networks. For our purposes, we note that the different quantization functions Qprod and Qacc as seen in Fig. 1, are likely to require different ranges for representation: Assuming the product terms ui = wixi are i.i.d, the accumulator’s value will follow the central limit theorem, and is therefore more likely to reach overflow, resulting unbounded quantization noise. To try and avoid this scenario, our setup will give a smaller exponent bias to the accumulator. In our experiments, we use a relative factor based on the chunk-size, so that bacc = bprod − 1 2 log2 (Chunk-Size). Following the same reasoning, one may suggest that the exponent bias should depend on the sequence number in which the FMA is applied within every GEMM operation. Nevertheless, for the context of this work, we will treat all FMA units as homogeneous, with the same exponent bias. 53.1 E XPERIMENTS : I MAGE CLASSIFICATION For our first set of experiments, we aim to check the effect low-bit accumulators have on residual neural networks He et al. (2016). For each experiment, we use the standard ResNet architecture and replace each GEMM operation used during forward-propagation (convolution and matrix multiplica- tion) with our custom implementation, as described in section 3. With no adjustments, the effect of these changes on the network accuracy (zero-shot), can be severe, as we show in Appendix B. For Qprod, Qacc, we used the same amount of mantissa and exponent bits, M = 7, E= 4, a setup we will denote as M7E4. For overflow, we used the exponent biases: bacc = 10, bprod = 12, but disabled underflow events for the first part of the experiment. After loading the networks with pre-trained weights, we proceed to train the network for 5 epochs, using Adam optimizer with a learning rate of η0 = 10 −6, and a cosine scheduler, so that η5 = 10 −8). Then, we enable underflow events and run a fine-tuning again for a single epoch, using a reduced learning rate of ηUF = 10−7. To evaluate the benefit of the two-staged fine-tuning, we also ran the same experiment with a single stage, where underflow is enabled for 10 epochs. The baseline numbers were obtained by repeating the fine-tuning process in a non-LBA setup, which resulted in an improvement of up to 0.65% over the zero-shot accuracy. Our full setup and implementation are detailed in Appendix C. The results of this experiment are presented in Tab. 2. Model Baseline 1-stage no UF* no UF → with UF ResNet18 70.23% 69.94% 70.01% 70.06% ResNet34 73.87% 73.64% 73.61% 73.45% ResNet50 76.80% 74.70% 76.60% 76.40% Table 2: Top-1 Accuracy results: Fine-tuning ResNets with low-bit accumulators for ImageNet classification. *Intermediate Stage: Both training and evaluation are done without underflow. For LBA ResNets with full-precision W/A, our results indicate that the models we suggest can train surprisingly well even without a dedicated fine-tuning regime. The dual-stage approach (Training without UF first and enabling it later) only shows clear benefit, so far, in the case of the larger, ResNet50 model. That being said, scaling the method for larger models is important, and tasks will only become more difficult from now on. In order for a model with low-bit accumulators to be commercially viable, it is vital to show that quantized accumulation still works when the weights and activations are quantized. Therefore, our next set of experiments will test the feasibility of LBA ResNets in this setting. For weights and activations, we will use 8-bit floating point representation (Wang et al., 2018). Following the results presented in Kuzmin et al. (2022), we use M4E3 representation with flex-bias for both weights and activations, implemented using the qtorch library Zhang et al. (2019). For our flex-bias implementation, we evaluate the maximal exponent for each tensor during forward propagation, and use the maximal integer exponent bias that is sufficient to prevent overflows (single value per tensor). The results of fine-tuning LBA ResNets in this setup can be seen in Tab. 3, as well as a comparison of our results with previous works that also used lower-bit accumulators. We note that a direct comparison between the methods based on final accuracy alone will not be valid: the method presented in Wang et al. (2018) is intended for quantized training, and includes several more quantized components, as well as several methods that are projected to reduce hardware efficiency. Meanwhile, Ni et al. (2020) proposes the cheapest implementation (Fewer bits for Weights and activations, Integer quantization), sacrificing model accuracy for hardware efficiency. Nevertheless, when aiming for cheaper inference, our LBA models were the only models to achieve accuracy on par with non-LBA models, while providing a cheaper alternative compared to models with standard accumulation. 3.2 E XPERIMENTS : L ANGUAGE MODELS To assess the capability of LBA language models, our next set of experiments will focus on the common Bert (Devlin et al., 2018) architecture, and the SQUAD (Question-Answering) task. In 6Model Data Type Weights Activations Accumulator Top-1 Accuracy ResNet18 Baseline FP 32 32 32 70.23% Baseline (FP8) FP 8 8 32 69.90% Wang et al. (2018) FP 8 8 16 66.95% Ni et al. (2020) INT 7 2 12 63.84% Ours (1-stage) FP 8 8 12 69.54% Ours (dual-stage) FP 8 8 12 69.70% ResNet34 Baseline FP 32 32 32 73.87% Baseline (FP8) FP 8 8 32 73.49% Ours (1-stage) FP 8 8 12 73.18% Ours (dual-stage) FP 8 8 12 73.42% ResNet50 Baseline FP 32 32 32 76.80% Baseline (FP8) FP 8 8 32 76.25% Wang et al. (2018) FP 8 8 16 71.72% Ours (1-stage) FP 8 8 12 74.15% Ours (dual-stage) FP 8 8 12 76.22% Table 3: Top-1 Accuracy results: Fine-tuning ResNets with low-bit accumulators and FP8 weights and activations for ImageNet classification. Results are compared with similar models utilizing LBAs in the literature. this case, fine-tuning a pre-trained model is already the standard. In contrast to our experience with residual networks, breaking down the fine-tuning process into separate phases was not, in general, beneficial for the accuracy of the larger models. The exponent biases we used for the different LBA models also had to be changed, to avoid overflow events. In table4, we compare the results of fine-tuning LBA Bert models with the results of fine-tuning non-LBA models, as described in C.2. While LBA Bert-small has a small (∆f1 = 0.37%) performance degradation compared with the non-LBA model, the gap is closed completely for the Bert ( ∆f1 = −0.09%) and Bert-Large (∆f1 = −0.26%). Baseline LBA (M7E4) bacc,bprod=7,9 LBA (M7E4) bacc,bprod=8,10 Model Exact (%) f1 (%) Exact (%) f1 (%) Exact (%) f1 (%) Bert-Small 71.32 80.96 70.88 80.24 71.35 80.59 Bert-Base 79.84 87.53 79.60 87.62 79.80 87.52 Bert-Large 83.22 90.40 82.97 89.97 83.25 90.66 Table 4: SQUAD v1 fine-tuning for LBA-Bert models. Inspired by our LBA-Bert model results (which were favorable toward larger models), we tested our LBA-aware fine-tuning method on the LLama-v2-7B model (Touvron et al., 2023). We used the same settings and scripts as QLoRA paper (Dettmers et al., 2023), which uses frozen 4-bit weights with an additional trainable low-rank matrix in BF16. To measure performance on a range of language understanding tasks, we used the MMLU (Massively Multitask Language Understanding) benchmark Hendrycks et al. (2020), a multiple-choice benchmark covering 57 tasks. The fine-tuning was done over the Open Assistant (OASSA1) dataset Köpf et al. (2023) using official training scripts found in the QLoRA code (i.e., llama2_guanaco_7b). We report 5-shot test accuracy in tabel 5. Model Baseline M10E5 M6E5 M7E4* LLamma v2 (OASSA1) 45.3 45.4 44.3 45.1 Table 5: MMLU 5-shot test accuracy with and without LBA, for QLORA+ LLama v2 (7B parameters). * For runs with 4 exponent bits, we used dynamic (per-layer) exponent-bias. 74 BELOW 12 BITS : F INE -GRAINED GRADIENT FOR LOW BIT ACCUMULATORS Thus far, we have shown that 12 bits are sufficient for inference in a variety of deep neural networks. However, the simple methods described in section 3 are not sufficient for training neural networks with lower amounts of accumulation bits. For example, a shallow fully-connected DNN trained over MNIST, will fail when using a M4E3 accumulator, even when excluding underflow events. The cause of the failure is known as it is similar to quantization failures in other areas of deep neural network: Quantization changes the output of the network during forward pass, and when the change is significant enough, it is no longer feasible to rely on the gradients of non-quantized operations for optimization. Of course, we cannot use the “real” gradients with respect to quantized operation, since they are zero almost everywhere. The common solution to this problem, with relation to the quantization of weights and activations, is to replace the derivative of the quantized function with a Straight-Through-Estimator (STE). In our case, we would like to use the STE with respect to the derivatives of the quantizers Qacc and Qprod inside the FMAq operation from Eq. (4). So far in this work, we used the naive “identity STE” (Bengio et al., 2013) which makes the replacement “ d dx Q(x)” = 1 (we will use the quotation marks to denote an STE replacement of a derivative). However, the more common STE for quantization zeros out gradients outside of the representation range (Hubara et al., 2017). For the quantizers in Eq. (1) and Eq. (2), we get: “ d dxQFIXED B,b (x)” = 1(Rmin < x < Rmax) ; “ d dxQFLOAT M,E,b (x)” = 1(|x| < ROF), (5) where we defined 1(·) as the indicator function which is equal 1 if its input is ‘true’ and zero otherwise. Many alternative forms of STEs exist and have been studied in the context of W/A quantization. The implementation of STEs for LBA networks, on the other hand, has several additional difficulties. The first, most immediate problem, is that the values of the inputs of the quantization functions within the FMAq (Qacc and Qprod) are not exposed to the software or stored in memory during forward propagation. Saving these internal values is generally not feasible, since the quantization operation occurs in each FMAq, and the number of FMAqs in DNNs typically exceeds the size of weights and activations by many orders of magnitude. However, if the hardware operation is deterministic and well-known, we found we are still able to use software for re-computation of the GEMM operation, to retrieve the required values during backpropagation (1 bit per operation). Such a re-computation operation is expensive (training time is doubled, at the very least), and so far feasible only in fully connected and attention layers (not convolutional layers). To the best our knowledge, this is the first time backpropagation is used on the full computational graph of the summation operation. Another possible problem for using standard STEs for the accumulation process stems from the recursive nature of the summation operation. The STE in equation Eq. (5) sets the corresponding gradient of any overflowing value to zero. As explained in Appendix D, if this STE is used for the accumulator’s quantization function, each overflow event will eliminate the gradients of all previously accumulated product pairs. Lastly, another possible problem is that, for floating point summation, other events besides overflow can potentially be important when estimating the gradient. Motivated by the last two potential problems, in appendix D, we propose, describe, and justify the practicality of several alternative methods for estimating the gradients of FMAq(x, w, s). The different methods use different types of STE: OF passes zero on overflow of Qacc (using Eq. (5), while DIFF passes zero on overflow, underflow, and full-swamping events of the FMAq. We also distinguish between a method where we apply identity STE with respect to the partial sum s, and the non-identity STE over the product-pair (x, w) (a.k.a Immediate), to the standard method, where the STE is applied with respect to all inputs (x, w, s) (a.k.a Recursive). For example, defining z ≡ FMAq(x, w, s) = Qacc (Qprod (x · w) + s) and ϵ1, ϵ2 as some small constants, we get: Immediate / DIFF: “dz ds” = 1 ; 1 w “ dz dx” = 1 x“ dz dw ” = 1 \u0012 |z − s| |xw| + ϵ1 > ϵ2 \u0013 , (6) Recursive / OF: “dz ds” = 1 w “ dz dx” = 1 x“ dz dw ” = 1(|Qprod (xw) + s| < ROF) ) (7) In Tab. 6, we compare the accuracy achieved using the proposed STE variants over the MNIST dataset. We see that such fine-grained gradient methods can indeed enable high accuracy in models with only 8-bit accumulators. 8STE Underflow Accuracy (Top-1, %) STE Underflow Accuracy (Top-1, %) Baseline - 98.65 Immediate / OF Yes 98.47 Identity Yes 18.28 Immediate / DIFF Yes 11.35 Identity No 18.28 Immediate / DIFF No 97.67 +Identity* Yes 42.28 Recursive / OF Yes 98.47 Table 6: Training a fully-connected NN with 8-bit (M4E3) accumulators for MNIST classification. The reported accuracy matches the final accuracy of the experiment. The model’s loss does not converge when using naive (Identity) STE for accumulation. Full details in Appendix C.3. *The mantissa for the accumulator was extended by 2 additional bits in this run. As we saw in the case of residual neural networks (Tab. 2 and 3) with 1-stage training, successful implementation of LBA is not guaranteed to scale to larger models. To evaluate the quality of our estimated gradients, we would like to compare the optimization of the different approaches. To that end, we train a small LBA transformer from scratch for masked language modeling, over a modest-sized dataset (200K rows), for 15 epochs. In Tab. 7, we compare different STE variants for a variety of very-low precision accumulators. Accumulator Identity (%) Recursive / OF (%) Immediate / OF (%) Immediate / DIFF (%) FP32 51.31 - - - M3E3 20.86 19.20 14.80 24.60 M4E3 13.88 39.57 37.23 41.94 M5E3 9.47 45.28 44.76 50.12 M6E3 14.71 46.17 46.13 50.03 M3E4 15.2 15.15 15.43 25.53 M4E4 42.93 42.81 42.81 41.50 M5E4 47.87 48.76 48.76 47.93 Table 7: Accuracy of LBA transformer for the task of Masked Language Modelling (200K rows), when using different STEs for the accumulator operation. Full details of the experiments are available in Appendix C.4. Based on our results for training masked language models, using fine-grained STEs becomes crucial when the number of accumulation bits is decreased below M = 4 or E = 4 (hence, this includes all possible FP8 formats). While successful at improving the optimization, none of the STEs we have tried were successful at closing the gap with the baseline completely, when extreme accumulator quantization was applied. Out of the three proposed STEs, we recommend Immediate/ DIFF STE, which generally achieved better accuracy in the areas where naive, identity STE was insufficient, despite its higher cost. The Immediate/ DIFF STE may also prove more suitable in cases where the exact behavior of the FMAq is unknown (i.e., ’black-box’) since its definition is agnostic to the FMAq internals. 5 D ISCUSSION The quantization of the accumulator in deep neural networks is a hard but necessary task in the effort to improve neural networks’ efficiency, reduce cost, and cut down carbon footprint. Despite the many difficulties involving the training, the implementation, and the theoretical analysis of networks with low-bit-accumulators, our results show that LBA networks are surprisingly easy to fine-tune. By applying simple optimization methods over pre-trained networks, we show it is possible to adjust the models for inference with cheaper hardware, that utilizes12 bits accumulators. When the accumulators bit width is further reduced we alleviate the accuracy degradation by using fine-grained approaches for estimating the gradient. 9ACKNOWLEDGMENTS The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. REFERENCES Ankur Agrawal, Sae Kyu Lee, Joel Silberman, Matthew Ziegler, Mingu Kang, Swagath Venkatara- mani, Nianzheng Cao, Bruce Fleischer, Michael Guillorn, Matthew Cohen, et al. 9.1 a 7nm 4-core ai chip with 25.6 tflops hybrid fp8 training, 102.4 tops int4 inference and workload-aware throttling. In 2021 IEEE International Solid-State Circuits Conference (ISSCC), volume 64, pp. 144–146. IEEE, 2021. (Cited on 4) Michael Andersch, Greg Palmer, Ronny Krashinsky, Nick Stam, Vishal Mehta, Gonzalo Brito, and Sridhar Ramaswamy. Nvidia hopper architecture in-depth, Apr 2022. URL https: //developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ . (Cited on 1, 4) David H Bailey. High-precision floating-point arithmetic in scientific computation. Computing in science & engineering, 7(3):54–61, 2005. (Cited on 2) Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems, pp. 5145–5153, 2018. (Cited on 1, 17) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. (Cited on 8) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. (Cited on 2) Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, and Daniel Soudry. Logarithmic unbiased quantization: Practical 4-bit training in deep learning. arXiv preprint arXiv:2112.10769, 2021. (Cited on 1) Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. Advances in Neural Information Processing Systems, 2016. (Cited on 2) Theodorus Jozef Dekker. A floating-point technique for extending the available precision.Numerische Mathematik, 18(3):224–242, 1971. (Cited on 3) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. (Cited on 7) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. (Cited on 6) Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746, 2015. (Cited on 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. (Cited on 6) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. (Cited on 7) 10Nicholas J Higham. The accuracy of floating point summation.SIAM Journal on Scientific Computing, 14(4):783–799, 1993. (Cited on 3) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations.The Journal of Machine Learning Research, 18(1):6869–6898, 2017. (Cited on 1, 8) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023. (Cited on 7) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. Fp8 quantization: The power of the exponent. Advances in Neural Information Processing Systems, 35:14651–14662, 2022. (Cited on 5, 6) Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Neural Information Processing Systems, 2018. (Cited on 5) Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for deep neural network acceleration: A survey. Neurocomputing, 461:370–403, 2021. (Cited on 1) Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, and Tijmen Blankevoort. Overcoming oscillations in quantization-aware training. arXiv preprint arXiv:2203.11086, 2022. (Cited on 1) Renkun Ni, Hong-min Chu, Oscar Castañeda, Ping-yeh Chiang, Christoph Studer, and Tom Gold- stein. Wrapnet: Neural net inference with ultra-low-resolution arithmetic. arXiv preprint arXiv:2007.13242, 2020. (Cited on 1, 3, 6, 7) Charbel Sakr, Naigang Wang, Chia-Yu Chen, Jungwook Choi, Ankur Agrawal, Naresh Shanbhag, and Kailash Gopalakrishnan. Accumulation bit-width scaling for ultra-low precision training of deep networks. arXiv preprint arXiv:1901.06588, 2019. (Cited on 1, 3) Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Xiaodong Cui, Wei Zhang, Kailash Gopalakrishnan, et al. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. 2019. (Cited on 2, 17) Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33:1796–1807, 2020. (Cited on 1, 2) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. (Cited on 7) Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, et al. Fp8 versus int8 for efficient deep learning inference. arXiv preprint arXiv:2303.17951, 2023. (Cited on 1, 16) Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In Advances in neural information processing systems, pp. 7675–7684, 2018. (Cited on 2, 3, 6, 7) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. (Cited on 13, 14) Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. Qpytorch: A low-precision arithmetic simulation framework, 2019. (Cited on 6, 13) 11A G ENERAL MATRIX MULTIPLICATION : E XAMPLE In section 2.4, we defined the FMA operation, and presented Eq. (3) as a general formula for all GEMM operation. It is worth taking a moment to illustrate the connection between the known tensor operations and the formula. For example, let us look at the simple case of matrix-multiplication (Y = XW T , X∈ Rd0×d1 , W∈ Rd2×d1 ). Here, if we wish to calculate the scalar y = Ykl, we can use the mapping: xi = Xki, wi = Wli. In this case, all values of X and W were used exactly once in the calculation of Y . This is not always the case, however. In batch matrix multiplication, values ofW will be used multiple times, paired with values of X of different batch dimension. In convolution, the same values of W will be used to calculate every scalar in the same output channel, and the neuron in the input channel may be used to calculate a multiple values in multiple output channel. This level of repetition will be, in part, what prevents us from using fine-grained STE methods on convolutional neural networks, in Sec. 4. B E FFECT OF QUANTIZED FMA ON ZERO -SHOT ACCURACY To give the reader a sense of the effect of low bit accumulators on deep neural networks, we include Tab. 8, where we measure the zero-shot accuracy of different ResNet architectures, pretrained with full-precision, after replacing all FMA components with FMAq (as described in C). Mantissa Effect Model Baseline M10E5 M9E5 M8E5 M7E5 M6E5 ResNet18 69.75 69.50 68.95 66.70 57.09 20.49 ResNet34 73.31 73.17 72.68 70.46 60.07 17.19 ResNet50 76.12 75.95 75.57 73.70 64.94 19.48 Exponent Bias Effect (M7E4) Model b = 8 b = 9 b = 10 b = 11 b = 12 bacc, bprod = 10, 12 ResNet18 55.68 60.64 60.00 58.84 56.96 60.14 ResNet34 50.80 63.30 63.88 62.46 59.90 63.65 ResNet50 26.41 64.25 68.69 67.57 66.12 68.49 Table 8: Zeroshot Accuracies for LBA-ResNets, with weights of pre-trained, full precision ResNets [%] The accuracies presented in Tab. 8 illustrates well why M7E4 quantization was chosen: Increasing the mantissa below M = 7 bits would result a much lower zero-shot accuracy, too far for proper fine-tuning. Likewise, reducing the number of bits to E = 4 already resulted lower accuracy due to overflow and underflow events, as indicated by the effect of the exponent bias. For example, the default exponent bias for E = 4 is b = 8, and using it for the accumulator in Resnet50 results in a significant degradation in accuracy. A small increase to b = 9, increases both underflow and overflow thresholds by a factor of 2 and is sufficient for increasing the accuracy by almost 40%. C E XPERIMENTS IMPLEMENTATION DETAILS C.1 I MAGE NET Each of the ImageNet experiments were performed on a single server, containing 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000). We used a total mini-batch size of 256, equally divided across the 8 workers. For the training datasets, we used the standard RandomResizedCrop and RandomHorizon- talFlip augmentations only. With no quantization, our model architecture was identical to the standard torchvision architecture, for all ImageNet models, while our custom GEMM kernels were used to override all forward GEMM operations (convolutions and matrix multiplications). For optimization, we used the Adam optimizer, with the hyperparameters β = (0.9, 0.999), ϵ= 10−8, λ= 10−4. Dropout was not used. As mentioned in the main text, we used cosine scheduling, the parameters of which depend on the phase in which it was used. We used 10 epochs in the 1-stage compared 12with 5 epochs for the dual-stage to support our claims that the gaps between the methods (where they exist) are not simply a result of better hyperparameters. The epoch count was initially chosen due to time-constraints, and was kept since the benefit of running more epochs was small. For W/A quantization, we used the qtorch (Zhang et al., 2019) library, which provides reliable quantization functions. Weights quantization was applied during every optimization step, while the activations were quantized using dedicated modules, preceding all convolutions, except the first one, and the downsample convolutions. The input of the final fully-connected layer was not quantized as well, in accordance with prior works. The quantization function we applied used stochastic-rounding (which is not considered expensive in this case, as we are not implementing the FMAq internals and the number of quantized components is significantly lower). No other components (e.g. Gradients or Momentum) were quantized since our solution is only aimed at inference. In addition to hyperparameters used in this experiment, we have also ran a set of experiments using fixed-learning rates (no cosine annealing). In the other set, we tested a few initial learning rate values (1E-7, 3E-8, 1E-8) for few epochs, and used enough epochs to reach convergence (for training accuracy/loss). The results in this regime were slightly better than the results published in the paper: For 8bit quantized ResNets with 4ME3, we achieved 69.6% for Resnet18, 73.48% for ResNet34 and 76.35% for ResNet50. However, this required more epochs and finer-tuned hyperparameters (different models used different learning rates). In the paper, we used the regime with cosine annealing since it was more robust to hyperparameter changes. C.2 SQUAD For the SQUAD fine-tuning experiment, we use 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000). We used the SQUAD training script of the transformers library (Wolf et al., 2019), while using our custom, LBA-model. In our LBA model, all fully connected layers and matrix multiplication operations were modified to use LBA GEMM operations during forward propagation, with the exception of the final fully connected layer (qa-outputs). For pre-trained models, we used either bert-base-uncased for Bert or prajjwal1/bert-small for Bert-small. For optimization, the Adam optimizer, with 1000 warmup steps to a learning rate of 3 · 10−5, from which we applied a cosine annealing scheduler. The batch size was configured to be 8. Our run was set for 20 epochs, but we applied early stopping once the model performance reached its peak (usually after 3 − 5 epochs). C.3 MNIST For each experiment with the MNIST setting, we used a single RTX 2080 Ti GPU with a mini- batch size of 16. Our neural network consisted of 4 fully connected layers (with LBA), and ReLU activations, with all hidden layers being1024 neurons wide. Outside of the accumulator, all data types were with full precision. Dropout wasn’t used (although it was shown to benefit the results slightly), and no data augmentation wasn’t used during training. For optimization, we used Adam optimizer, with an initial learning rate of10−3, with the hyper-parameters: β = (0.9, 0.999), ϵ= 10−8, λ= 0.0, and StepLR scheduler (γ = 0.95). We used 100 epochs per experiment, which was usually much more than needed for convergence or divergence. To test the STE, we replaced the default linear operations with our custom implementation, this time also implementing a custom backward operation. During backpropagation, we used a new, cuda kernel that imitated the (deterministic) operation of the original GEMM operation (using the available weights and activations), but outputted a binary tensor, that indicated the value of all STEs involved in the operation (the type of STE was configurable). For recursive implementation, we modified the tensor ad-hoc to account for the recursive nature of the STE (although less efficient than the optimal implementation). After running the kernel, we used the output to adjust the computation of the weights/ neural gradients as described in section D. In this experiment, we used a fixed exponent bias of 5, which was shown to perform the best among all values in its vicinity. C.4 M ASKED LANGUAGE MODELLING Each of the Masked Language Modelling (MLM) experiments was performed on a single server, containing 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000, or A100). Our tests were run over theoscar: unshuffled-original-af dataset, with a single tokenizer we trained over the same dataset (vocabulary 13size of 1000). The dataset was chosen due to its moderate size (200K rows), being difficult enough to show gaps in convergence while allowing us to perform meaningful optimizations with simulation kernels in moderate time. For the transformer, we used the Bert architecture, with the hidden size of 512, 2 hidden layers, 4 attention heads, and maximum position embedding of 1024 (All other parameters were according to transformers library defaults). We used the available Huggingface infrastructure (Wolf et al., 2019) to train/ evaluate the model, with Adam optimizer, an initial learning rate for 10−3, a drop-on-plateau scheduler (evaluating every 250 step, γ = 0.1), and a global mini- batch size of 64. In practice, the drop-on-plateau scheduling was only applied to ‘failed’ runs, to give them another shot for optimization, with no success (They did not converge, even well passed the 15 specified epochs). When the number of exponent bits was set to E = 3, we used a fixed exponent bias of b = 6 for the product and accumulator. D G RADIENT ESTIMATION FOR LBAS Following the general equation for GEMM operation (Eq. (3)), the operation can be expressed, using the recursive expression: S0 = 0; Si+1 = FMA (xi, wi, Si) ; y = SN−1 (8) In this example, we add the values of the product to the intermediate accumulator sum, ( S), in a sequential manner. Different orderings of the FMA operation are possible and can have an effect on the output (i.e., floating point addition is not commutative as ‘ideal’ addition, due to swamping). Let us write the recursive expression in Eq. (8) explicitly Sq i ≡ FMAq (xi−1, wi−1, FMAq (xi−2, wi−2, FMAq (...FMAq(x0, w0, 0)))) ; yq = Sq N−1. (9) Our goal in this section is to find a good estimate for the derivative for ∂yq ∂xi and ∂yq ∂wi . D.1 R ECURSIVE STE S The first, and most obvious method to estimate the derivative is by using the common STE (Eq. (5)). Per our definition, FMAq contains two quantization functions. Our main concern, however, is for the post-accumulator quantization, Qacc, and our first attempt will be to quantize it directly using a general STE function (STE : R3 → R). By doing so, we get the gradients: “ dyq dSq i ” = 1 wi “dyq dxi ” = 1 xi “ dyq dwi ” = dyq dSq i+1 STE (xi, wi, Sq i ) , (10) which, when expanded upon, will give us: “ dyq dSq i ” = NY k=i+1 STE (xk, wk, Sq k) . (11) Eq. (11) reveals an additional, possible issue for using standard STEs for the accumulation process. Usually, when applied over an overflowed activation neuron, the STE in equation Eq. (5) will set the corresponding neural gradient to zero. If the same STE is used for the accumulator’s quantization function, as per Eq. (11), each overflow event will eliminate the neural gradients of all previously accumulated product-pairs. Still, this approach may help us calculate a more accurate gradient, provided that conditions in which the STE returns 0 are not commonly met. We will denote the approach in Eq. (11) as Recursive. To perform the recursive correction to the gradient, all we really need to know is the last index (if any), in which the accumulator was overflown. While this may be more complex in cases where the values are added in non-sequential ordering, this is still a feasible calculation to perform, with modest-sized output. Computationally, calculating the underflow indexes is no more difficult as a task than computing the original GEMM operation, and this calculation (as well the calculation that will be presented in the following section), can be done during backpropagation, to avoid using essential memory for a long term. 14D.2 I MMEDIATE STE S To avoid setting too many gradients to zero, we suggest an alternative approach. First, we will re-write Eq. (9) as: Sq i = α0x0w0 + α1x1w1 + α2x2jw2 + ... + αi−1xi−1wi−1 , (12) where αi ≡ FMAq (xi, wi, Sq i ) − Sq i xiwi . (13) If xi = 0 or wi = 0 , we will define αi = 0 for simplicity. Recall Sq i here is the value of the “quantized\" accumulator in step i (Eq. (9)). From its definition, αi is the correction we make to the product xi · wi, to account for the FMA quantization error. Our choice to express the correction this way is based on the assumption that for most steps, |Sq i | ≫ |xi · wi|. This is true, because Sq i is, approximately, the accumulated sum of many such products. During a floating point addition, the bits of the lesser value will be the first to be swamped out, and thus we entangle the quantization error with this component. Moving on to the gradients, we are interested in the gradients of the operation inputs, dyq dxi and dyq dwi . We can use the chain rule to get: dyq dxi = wiαi + N−1X k=i+1 dαk dxi xkwk. (14) The exact expression we got for the gradient remains complex, sincedαk dxi ̸= 0, for k > i. Nevertheless, moving forward, we will take the approximation that ∀k, dαk dxi = 0. i.e., we neglect the cumulative effect that any individual scalar value has on the quantization correction we make in the following steps of the GEMM operation. We then get: dyq dxi = wiαi, dyq dwi = xiαi (15) Eq. (15) suggests a correction we can make to the standard backpropagation operation, which we will denote as an Immediate STE. However, to make any use of this correction, we must first have the values αi. In terms of computation, calculating α is quite similar to performing the original GEMM operation. In terms of memory, however, αi scales with the number of overall FMA operations. This is feasible in the case of fully connected operations, but not for GEMM operations that include a large amount of shared weights. To make sure the evaluation of αi by itself does not overburden the memory, it is possible to quantize the values of αi. By doing so, we get the equation: 1 wi “dyq dxi ” = 1 xi “ dyq dwi ” = QFIXED B,0 \u0012FMAq (xi, wi, Sq i ) − Sq i xiwi + ϵ1 \u0013 . (16) where ϵ1 is a small constant, added with flexible sign to prevent invalid denominator. In our experi- ments, we have observed that the quantization of αi does not harm the quality of the optimization process, and proceeded to binarize the value. The result is that we ended up suggesting an alternative STE to the one presented in Eq. (5), which is designed to address overflow only. We denote the new STE as DIFF: STEDIFF (xi, wi, Sq i ) = ( 1 |FMAq(xi,wi,Sq i )−Sq i | |xiwi|+ϵ1 > ϵ2 0 Else STEOF (xi, wi, Sq i ) = \u001a1 |Qprod(xiwi) + Sq i | < ROF 0 Else (17) The DIFF STE is similar to the common Overflow STE, but is tuned to detect cases of full-swamping and cases of product underflow in addition to cases of overflow. In our experiments, we tested the immediate approach with both STEs. One unique advantage of the DIFF STE, is that is agnostic to the specific implementation of the FMAq component. Therefore, the DIFF STE remains relevant in 15the general cases where the FMAq operation has an unspecified, black-box behavior, as common for hardware modules. Our derivation in this section was done in respect to the sequential ordering of FMAq operations, which is not commonplace in hardware accelerators that try to achieve large degree of parallelism. A more typical case, presumably common for systolic-array-like accelerators, is the chunk-based accumulation, where the accumulation is performed in two hierarchies, as seen in Fig. 1. In our experiments, all simulated GEMM operations and gradient estimation used a chunk size of 16, which means that an operation with an accumulation width of N is initiated with N 16 parallel operations (i.e., the first hierarchy), before aggregating the results (i.e., the second hierarchy). For example, in the case of recursive STE, every detection of OF or DIFF during re-computation will result in a ‘0’ in all preceding operations, just as we saw for sequential accumulation. The only difference for parallel accumulation is that the hierarchy tree can expand in two directions (Like 1 (right), with all the arrows reversed). E H ARDWARE ANALYSIS In this section, we try to give an estimate for the effect of incorporation of LBA models on hardware cost (area/ power), by estimating the number of gates needed to implement qLBA with different levels of quantization. Following an existing design of FMAq component (van Baalen et al. (2023),figure 2b), we adjusted the design for the case of FMAq with m/e quantization of weights and activations and M/E quantization of intermediate values (product, accumulator), and suggested the following gate counts, as seen in table 9. The gate counts are all based on the gate count assumptions listed in [van Baalen et al. (2023), appendix B], and common block designs. FMA Components breakdown Gate Count Exponent Adder (e − 1) · CFA + CHA Exponent Differ (min(E, e+ 1) − 1) · CFA + CHA · (1 + |e + 1 − E|) Exponent Max E · CMUX Mantissa MUL (m + 3)2 · CAND + (m + 2)2 · CFA + (m + 2) · CHA Sort Exponent (M + 1) · CMUX 1st Shift (M + 1 >> k→ F) (F − 1) · log2(kmax) · CMUX Mantissa Adder (F, F→ F) (M) · CFA + CHA Leading Zero Detector F(CAND + COR) + log2(kmax)2COR 2nd Shift (F >> k→ M + 1) (M + 1) · log2(kmax) · CMUX − kmax · (CFA − CAND) Exponent Rebase (E − 1) · CFA + CHA Final Incrementor (M + 1)CHA Table 9: FMA components gate-count breakdown. For the gate count, we used CAND = COR = 1 for the standard gates AND2/OR2, CMUX = 3 for MUX2, and CHA = 3, CFA = 7 for half and full adder. We do not include Flip-Flops in our gate count. For the value of F (Canvas bits, after shifting to fixed-point representation), we used 2M +1, the maximum bit width in which two 2’s complementary values with M + 1 bits can interact during addition. For kmax (the maximum shift distance), we used min(log2(F), E), as the magnitude of the shift is bounded by both the number of exponent bits and the size of the canvas F. Weights/Activations bits FMAq Bits Canvas Gates m e M E F log2(kmax) Count Ratio [%] 4 3 23 8 47 6 2208 100 4 3 10 5 21 5 1082 49 4 3 7 4 15 4 808 37 Table 10: Gate estimation for Quantized FMA We summarize our numerical results in table 10. Our results show that for 8bit activations and weights (at FP format M4E3), as we used in the paper, any half-precision FMAq that follows our quantization 16scheme is expected to reduce the number of gates by about 50% from the gate count of full-precision accumulators. Reducing the accumulator to M7E4, as was done in the paper, will cut the number of gates by an additional 25%, compared to half precision. We conclude our 12bit accumulators will reduce the gate count by 63% compared to 32 bit FP accumulation. We note that the 16-bit accumulation gate count in our analysis is not directly applicable to previous works that used16-bits accumulators– This is because in Sun et al. (2019), only the output of the accumulator was quantized with no explicit quantization of the internals. Presumably, the majority of the gain there was achieved by the reduction of communication bandwidth between FMAq components, which does not affect the gate count in this analysis. F W HY IS IT HARD TO TRAIN WITH UNDERFLOW ? Our claim that underflow cause unique problems during SGD optimization is based, first and foremost, on empirical observations (see: Tab. 2, Tab. 3). In addition, we suggest several explanations to why this problem may occur when underflow is introduced during training, despite it having a small effect on the loss landscape. Consider a neural network, where a specific scalar weight w is connected to its scalar activation x as input, and their product is z = xw. Suppose w is small enough so that z consistently results in product underflow, i.e. Qprod(z) = 0. In this case, during forward propagation, the value of x has little to no direct effect on the output neuron to which w is connected. Therefore, it is reasonable to assume that the computed neural gradient g ≡ dL dz (where L is the loss) will be uncorrelated with x. Consequently, the gradient update of the weight w will be ∆w ∝ gx, with the expected value E[∆w] ∝ E[gx] = E[g]E[x]. Based on previous quantization literature Banner et al. (2018), we have approximately E[g] = 0, and so E[∆w] = 0. Therefore, any sufficiently small weight w will become “stuck\", so that its z cannot escape underflow for a long time. The issue is excavated by the ratio between updates magnitude, and the magnitude a weight has to be updated to surpass the underflow threshold. In a fully-trained model, the gradients are expected to be dL dW ≃ 0. When transitioning to an LBA-model, we make sure to avoid significant changes to the loss landscape (as indicated by the zero-shot accuracy). As a result, we can expect the relative change in gradient to remain small, |∆w| = |η dL dw | ∼ |w|. (Otherwise, the loss landscape would change rapidly during SGD, and we can no longer consider the process as fine-tuning). When dealing with quantized values, it is always possible that a gradient step will be too small to change the value. (This is the main motivation behind stochastic rounding, which is not suitable for our case). For example, for floating point quantization without underflow/overflow, the gradient step must be approximately |∆w| = |η dL dw | ≥2−M |w| for the quantized value of w to ‘jump‘ quantization level. In this case, |∆w| ∼ |w| means that the ability of all weights to change during fine-tuning only depends on M, and the learning rate. In the case of underflow, however, values must surpass an absolute threshold (2−b), for the gradient step to have any effect. Consequently, under previous assumptions, any small enough value subjected to floating point quantization is expected to receive updates which are too small to result a state-change. This is what we referred to when mentioning values being ’stuck’ and ’escaping’. In LBA networks, the quantization is performed over intermediate values (products and partial accumulation values). These value do not get the explicit updates, but they will still get implicit updates by passing their respective neural gradient backwards. 17",
      "meta_data": {
        "arxiv_id": "2401.14110v1",
        "authors": [
          "Yaniv Blumenfeld",
          "Itay Hubara",
          "Daniel Soudry"
        ],
        "published_date": "2024-01-25T11:46:01Z",
        "pdf_url": "https://arxiv.org/pdf/2401.14110v1.pdf"
      }
    },
    {
      "title": "Post-Training Sparsity-Aware Quantization",
      "abstract": "Quantization is a technique used in deep neural networks (DNNs) to increase\nexecution performance and hardware efficiency. Uniform post-training\nquantization (PTQ) methods are common, since they can be implemented\nefficiently in hardware and do not require extensive hardware resources or a\ntraining set. Mapping FP32 models to INT8 using uniform PTQ yields models with\nnegligible accuracy degradation; however, reducing precision below 8 bits with\nPTQ is challenging, as accuracy degradation becomes noticeable, due to the\nincrease in quantization noise. In this paper, we propose a sparsity-aware\nquantization (SPARQ) method, in which the unstructured and dynamic activation\nsparsity is leveraged in different representation granularities. 4-bit\nquantization, for example, is employed by dynamically examining the bits of\n8-bit values and choosing a window of 4 bits, while first skipping zero-value\nbits. Moreover, instead of quantizing activation-by-activation to 4 bits, we\nfocus on pairs of 8-bit activations and examine whether one of the two is equal\nto zero. If one is equal to zero, the second can opportunistically use the\nother's 4-bit budget; if both do not equal zero, then each is dynamically\nquantized to 4 bits, as described. SPARQ achieves minor accuracy degradation\nand a practical hardware implementation. The code is available at\nhttps://github.com/gilshm/sparq.",
      "full_text": "Post-Training Sparsity-Aware Quantization Gil Shomron† Freddy Gabbay§ Samer Kurzum† Uri Weiser† †Technion — Israel Institute of Technology, Haifa, Israel §Ruppin Academic Center, Emek Hefer, Israel {gilsho@campus, ssamer15@campus, uri.weiser@ee}.technion.ac.il freddyg@ruppin.ac.il Abstract Quantization is a technique used in deep neural networks (DNNs) to increase exe- cution performance and hardware efﬁciency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efﬁciently in hard- ware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in differ- ent representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while ﬁrst skipping zero-value bits. Moreover, instead of quantizing activation-by- activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can oppor- tunistically use the other’s 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation and a practical hardware implementation. The code is available at https://github.com/gilshm/sparq. 1 Introduction Deep neural networks (DNNs) are at the heart of numerous applications, such as image classiﬁcation and object detection [8], image synthesis [30], and recommendation systems [7]. DNNs, however, require abundant computations, as, for example, billions of multiply-and-accumulate (MAC) op- erations are required to assign a 224 ×224 colored image from the ImageNet dataset to one of its thousand possible classes. Limited computational resources, such as those in edge devices, latency constraints, and higher input resolutions, are all catalysts for development of methods that increase the ratio between DNN execution performance to hardware area, with as minimal impact on model accuracy as possible. One common method of doing so is quantization. Quantization is commonly used to map the 32-bit ﬂoating-point (FP32) activations and weights in convolutional neural networks (CNNs) to 8-bit integers (INT8), which is known to result in minor or no degradation in model accuracy while easing hardware implementation [ 14]. Going below 8 bits, however, is not trivial, as quantization noise leads to a noticeable decrease in model accuracy. Quantization-aware training (QAT) methods employ training for quantization, to decrease quantization noise and recoup model accuracy [3, 25, 42]. Nevertheless, it is not always possible to employ training, for reasons such as lack of hardware resources, time, power, energy, dataset availability, or skilled manpower. Post-training quantization (PTQ) methods circumvent these issues [1, 5, 6]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2105.11010v2  [cs.LG]  28 Oct 2021PTQ methods, basically, search for the optimal tensor clipping values to minimize quantization noise [1, 5]. They usually employ uniform quantization, since computing a dot product (DP) of evenly-spaced integer values can be implemented efﬁciently in hardware. DNN tensor distributions, however, are known to follow a bell-shaped distribution, such as Gaussian or Laplacian, i.e., the uniform quantization that is, on one hand, hardware-friendly, may not be, on the other hand, the best choice for minimizing the noise induced by the quantization process. To solve this mismatch, to some extent, PTQ methods that break tensor distributions into different quantization regions were proposed [6, 12, 24]. Computing a DP comprising values from different quantizations is not trivial though, since each activation-weight multiplication result may correspond to a different scaling factor, i.e., it will induce a multiplication by a different FP value per quantization region. In this paper, we propose sparsity-aware quantization (SPARQ), which leverages the inherent and dynamic activation sparsity from granularities of entire integer 8-bitvalues (vSPARQ), down to INT8 representation zero-value bits (bSPARQ). With bSPARQ, instead of quantizing every activation to, for example, 4 bits according to a predetermined scaling factor, activations are ﬁrst quantized to 8 bits and then dynamically quantized to 4 bits by choosing the most signiﬁcant consecutive 4 bits while skipping leading zero bits (Figure 1). bSPARQ effectively achieves a number of quantization ranges while still enabling a practical hardware implementation. Moreover, inspired by [32], we also leverage the entire 8-bit activation sparsity with vSPARQ, for additional mitigation of quantization noise. Instead of quantizing activation-by-activation to 4 bits, activations are quantized to 4 bits in pairs. If one activation is zero, then the other can span its bits across the ﬁrst, and thereby still be represented by 8 bits to avoid additional quantization noise. If, however, both activations are non-zero, both are quantized to 4 bits by bSPARQ. We experiment with vSPARQ and bSPARQ in conﬁgurations of 4, 3, and 2 data bits. This paper makes the following contributions: • Sparsity-aware quantization (SPARQ).We present a sparsity-aware quantization method, in which n-bit quantization takes place by picking the most signiﬁcant n bits from the 8-bit value representation, while skipping leading zero-value bits. Moreover, since many activations are zero-value, we consider pairs of activations in the quantization process. If one activation is zero, the other can use the entire 2n-bit budget. We experiment with a number of bit-group selection options and activation bit-widths that demonstrates the trade-off between model accuracy and hardware overhead. • Practical hardware implementation.We implement SPARQ on top of a systolic array (SA), inspired by Google TPUs, and on top of a Tensor Core (TC) DP unit, inspired by NVIDIA GPUs, and show that SPARQ is practical in terms of area overheads. In addition, we also discuss SPARQ implementation on top of NVIDIA Sparse TCs (STCs), thus leveraging activation sparsity on top of weight sparsity. • Comprehensive evaluation.We evaluate our method on a variety of image classiﬁcation models, with numerous conﬁgurations and activation bit-widths, and compare it with previous PTQ works. 2 Related Work PTQ methods are the most relevant works that are related to this work. ACIQ [ 1] analytically extracts the optimal quantization clipping values from the tensors’ distributions and uses per-channel bit-allocation and per-channel quantization of activations. LBQ [ 5] formulates a minimum MSE optimization problem that is then solved numerically per layer, and employs additional low-precision tensors to sensitive layers. AdaQuant [10] and AdaRound [21] optimize the common round-to-nearest rounding scheme to reduce quantization noise. BRECQ [ 16] analyzes the second-order error and optimizes the quantization at block granularity. Conceptually, both vSPARQ and bSPARQ can be employed on top of any of the above quantizations (for simplicity’s sake, we use a simple 8b-8b min-max symmetric quantization, as we also describe in Section 5). Other works, such as OLAccel [24], PWLQ [6], and BiScaled-DNN [12], divide the tensor distribution into two regions. OLAccel divides the tensor distribution into a low-precision region that contains the majority of data, and a high-precision region that contains a small portion of the data (e.g., 3%), which they deﬁne as outliers. PWLQ and BiScaled-DNN, on the other hand, divide the tensor distribution 2into two regions with the same bit-width. BiScaled-DNN uses different scale factors on overlapping regions and implements a ratio heuristic to set the breakpoint between the regions, whereas PWLQ picks the appropriate breakpoint via minimization of the quantization error. Interestingly, PWLQ is capable of breaking the distribution into more than two regions; however, the authors state that from a hardware perspective, this may not be feasible. Following OLAccel, OverQ [41] leverages activation sparsity to avoid the dedicated outlier datapath used in OLAccel. In this work, we employ a simple rounding mechanism and bit-level sparsity to mitigate noise in the occasion a zero-value does not exist, and we propose a parallel implementation rather than a serial one. SySMT [32] leverages sparsity in quantization of both activations and weights to 4 bits. Their method incurs relatively high area overheads, since the quantization logic has to be scaled with the number of processing units. Moreover, SySMT incurs relatively high degradation in accuracy, since quantization to 4 bits is implemented by trimming either the 4-bit most signiﬁcant bits (MSBs) or the 4-bit least signiﬁcant bits (LSBs). These two options are not optimal, since we ﬁnd that, for example, with ResNet-18 and ILSVRC-2012, 67% of the non-zero-value activation values have at least one of the 4-bit MSBs toggled (i.e., equal to one), even though 90% of the time, the two MSBs are not toggled. That is, the two MSBs are most likely not toggled when the 4-bit MSBs are chosen. 3 The Basic Principle of SPARQ SPARQ comprises two orthogonal techniques: bSPARQ and vSPARQ. The former leverages zero- value bits to trim an 8-bit value to an n-bit value; and the latter leverages zero-value activations. Below, we describe both in detail. Throughout this work, we focus on quantizing the activations and leveraging only their sparsity, i.e., no correlation is made with the weight values, unless otherwise stated. 3.1 bSPARQ: Leveraging Bit Sparsity Consider an already quantized 8-bit activation, x, and quantization to 4 bits (i.e., n = 4). bSPARQ trims the activation from 8 bits to 4 bits by inspecting the activation bits and choosing the most signiﬁcant consecutive 4 bits within it, which, in practice, is achieved by searching for the ﬁrst most signiﬁcant toggled bit. The motivation behind bSPARQ is twofold: ﬁrst, activations usually follow a bell-shaped distribution, meaning that the MSBs are usually equal to zero and, therefore, can be skipped; and second, if the MSBs are toggled, the LSBs’ contribution to the entire value is insigniﬁcant. For example, given the value 000110112 (2710), the 4-bit window will be positioned at bits [4:1] (000110112), thus achieving the approximated value 2610. Notice that since there are ﬁve window position options, the 4-bit window is accompanied by a 3-bit identiﬁer that corresponds to the window position—that is, how much shift-left is required on top of the four trimmed bits. In addition, to further reduce the dynamic quantization noise, we round the value within the chosen window according to the residual LSBs. bSPARQ is visually demonstrated in Figure 1. Supporting ﬁve window options requires additional circuitry compared with, for example, three window options, since additional placement options require additional hardware support by the shift-left unit. The trade-off is, however, improved accuracy, since additional placement options introduce less quantization noise. We experiment with ﬁve, three, and two placement options, denoted as 5opt, 3opt, and 2opt, respectively. With the 3opt conﬁguration, [7:4], [5:2], or [3:0] are chosen, and with the 2opt conﬁguration, either [7:4] or [3:0] are chosen (we leave the analysis of asymmetrical conﬁgurations for future work). For example, given the previous value, 000110112, 3opt will choose bits [5:2] (000110112), whereas 2opt will choose bits [7:4] (000110112). Relation to piecewise linear quantization.To mitigate quantization errors, previous works suggest dividing the tensor distributions into different quantization regions, each with a scaling factor of its own [6, 12, 24]. In a sense, bSPARQ is somewhat similar to those. First, each activation is assigned to a quantization range according to its value; however, we break the distributions into hardware-oriented regions of power of two. For example, for the 5opt case, the regions are [0, 21 −1], [21, 22 −1], and so on. As a result, values are mapped to their appropriate range by simply counting the leading zero bits. In addition, we avoid any need for preprocessing that searches for the distribution breakpoints to minimize the quantization noise. Second, each region has an individual scaling factor; however, each 3[7:4] [6:3] [5:2] [4:1] [3:0] Example +Round 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 Options (a) 5opt [7:4] [5:2] [3:0] Example +Round 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 Options (b) 3opt [7:4] [3:0] Example +Round 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 Options (c) 2opt Figure 1: Demonstration of SPARQ 8b-to-4b quantization. More window placement options (e.g., 5opt) decrease the quantization noise; however, additional hardware is needed to support many placement options. region scaling factor is a product of a base scaling factor with the corresponding power of two. For example, in the 5opt conﬁguration, the scaling factor of the decimal number 3310 = 001000012 is the original scaling factor times 22. This enables a relatively simple implementation with up to ﬁve regions when considering 4-bit activations, and even six and seven regions when considering 3- and 2-bit activations, respectively—as opposed to the two quantization regions used by previous works. 3.2 vSPARQ: Leveraging Sparsity with Pairs of Activations Consider an 8-bit unsigned activation vector, X = (x1, ··· , xL), and an 8-bit signed weight vector, W = (w1, ··· , wL), both of length L. Also, consider a single MAC unit that computes a single activation-weight multiplication per cycle. vSPARQ, similar to [32, 34, 41], groups activations in pairs, to leverage the dynamic and unstructured activation sparsity. That is, the DP calculations can be formulated as: X ·W = L∑ i even xiwi + xi+1wi+1 = y , (1) where y is the DP scalar result, and in our context, an output activation. For some i, if xi = 0, then xi+1 can be used with 8-bit representation, and vice versa. If, however, both xi ̸= 0and xi+1 ̸= 0, and given that, for example, bSPARQ is employed, then the precision of both xi and xi+1 is reduced to 4 bits. For a certain i, the vSPARQ operation can also be formulated as: xiwi + xi+1wi+1 =    xiwi, if xi+1 = 0 xi+1wi+1, if xi = 0 bSPARQ(xi)wi + bSPARQ(xi+1)wi+1, otherwise . (2) Notice that the two ﬁrst case statements correspond to an 8b-8b computation, whereas the last case statement corresponds to two 4b-8b computations. The latter case is possible, since two 4b-8b multiplications are logically equivalent to a single 8b-8b multiplication, as we describe next. 8b-8b = 2x4b-8b.Given an 8-bit unsigned activation,x, and an 8-bit signed weight, w, the activation- weight multiplication can be formulated as x[7:0] ·w[7:0] = 7∑ i=0 2ixi ·w[7:0] = ( 3∑ i=0 2i+4xi+4 + 3∑ i=0 2ixi ) ·w[7:0] = 24x[7:4] ·w[7:0] + x[3:0] ·w[7:0] , (3) where the [b : a] notation represents the b-to-a range in bits, the two activation-weight multiplications are 4b-8b wide, and the 24 is equivalent to a 4-bit shift-left operation. By considering an additional weight input as well as dynamic shift-left operations, we can reuse the multipliers and achieve a multiplier capable of either one 8b-8b multiplication or two independent 44b-8b  Multiplier 4b-8b  Multiplier << << 4 4 8 ShiftCtrl 8 x1 x2w1 w2 16 16 17 MuxCtrl Figure 2: Equation (4) hardware implementation. PE PE PE PE PE PE PE PE Weight Buffer Activation  Buffer PE 8b-8b  Multiplier Partial Sum 8 8 16 32 32 Figure 3: Illustration of a conventional 8b-8b output stationary systolic array. 4b-8b multiplications with a dynamic range: 2opt1 xin1,4b ·win1,8b + 2opt2 xin2,4b ·win2,8b , (4) where the activation and weight inputs are 4 bits and 8 bits long, respectively. Equation (4) resembles a FP representation; however, the “opt” conﬁgurations are not necessarily continuous, as in 3opt and 2opt. Figure 2 illustrates how Equation (4) is mapped to hardware. The two 4b-8b multipliers correspond to xin1 ·win1 and xin2 ·win2, and the two shift-left units ( ≪) correspond to 2opt1 and 2opt2 . The adder corresponds to the addition of the two groups, and the multiplexers, which are not explicitly formulated in Equation (4), are used to choose dynamically between win1, win2, or select both, during execution. We use this multiplier instead of the conventional one used in well-known hardware structures. 4 Case Studies In this section, we examine SPARQ on top of two well-known matrix multiplication accelerator implementations: systolic arrays (SAs) and Tensor Cores (TCs). These accelerators are commonly used for CNNs, since it is a standard practice to map the convolution operation to matrix multiplication [2, 18, 39]. Our focus here is on the processing engines (PEs) comprising each of these structures and that are responsible for single DPs. Both implementations are fully equivalent from a mathematical point of view. Systolic arrays. SAs consist of a large monolithic network of PEs designed for fast and efﬁcient processing of systematic algorithms that execute the same computations with different data at different time instances [15]. The topology of SAs, illustrated in Figure 3, consists of a homogeneous network of tightly coupled PEs, each performing a MAC operation. PEs work in tandem: each PE in the SA receives data from its upstream neighbors, performs a MAC operation, and forwards the data downstream. In our PE design, also known as output-stationary SA, each PE will eventually hold the result of a DP; and the entire SA will comprise a tile from a result matrix. Google’s TPUv2 and TPUv3, for example, consist of 128×128 SA arrays [22]. To deploy SPARQ, the conventional multiplier in each PE is replaced with the one presented in Figure 2, the weight bandwidth is doubled, and the activation bandwidth does not change. Tensor cores.TCs were ﬁrst introduced in NVIDIA’s V olta architecture to accelerate matrix oper- ations [4, 13, 19]. TCs multiply two 4×4 matrices and add an additional one to the multiplication result. The speciﬁc implementation details of TCs are not publicly disclosed; however, a proposed architecture that ﬁts the original TC performance is suggested in [27]. In the proposed TC architecture, there are a number of DP units. Each DP unit performs four parallel activation-weight multiplications, accumulating them in an adder tree together with an additional third value. In this work, we focus on the architecture of a single DP, as presented in Figure 4. To enable SPARQ, the multipliers are replaced and the weight bandwidth is doubled, similar to the SA. NVIDIA also recently introduced weight sparsity acceleration in its Ampere microarchitecture [20, 23]. The Sparse TC (STC) hardware achieves 2×speedup over the original TC by essentially 58b-8b  Multiplier 8b-8b  Multiplier 8b-8b  Multiplier 8b-8b  Multiplier 8 8 16 17 18 32 32 Figure 4: Illustration of a conventional 8b-8b DP unit comprising a TC [27]. MUX 4:2 MUX 4:2 DP Unit Activations Weights 4x8 32 4x8 2x3 8 Idx Figure 5: Conventional STC microarchitec- ture [23]. skipping 50% of the computations (Figure 5). STC requires 50% weight structured pruning at a granularity of four elements, i.e., every four adjacent weights must have two zero-value weights. Only the non-zero-value weights are stored with additional coordinates. In Figure 5, the two leftmost weights and two rightmost weights correspond to the four leftmost activations and rightmost activations, respectively. The stored coordinates indicate which activations are picked, since they are to be multiplied by non-zero-value weights. After ﬁltering the activations, they are passed with the weights to the DP unit for further processing. Notice, however, that activation sparsity may still exist even after the selection process. 5 Experiments We evaluate the impact on model accuracy using PyTorch [26], the ILSVRC-2012 dataset [28], and various CNN models [8, 9, 11, 37, 37, 38] (see Table 1). All models are quantized using a simple uniform min-max quantization, employing symmetric unsigned per-layer quantization for activations and symmetric signed per-kernel quantization for weights. The min-max statistics are gathered during a quick preprocessing stage on 2K randomly picked images from the training set. In addition, during preprocessing, we recalibrate the BatchNorm layers’ running mean and running variance statistics [29, 33, 35, 36]. In all models, the ﬁrst convolution layer is left intact, since its input activations, which correspond to the image pixels, do not include many zero values, if any. Quantization is, therefore, performed on all convolution layers, with the exception of the ﬁrst layer. We present the quantization results in Table 1 . Throughout this section, we use SPARQ on top of the 8-bit models (A8W8) and report the accuracy degradation relative to the corresponding FP32 model. A4W8 and A8W4 are presented in Table 1 as references to the worse-case accuracy. Table 1: ILSVRC-2012 CNN top-1 accuracies, given different quantization precisions. Throughout this work, SPARQ is used on top of the A8W8 representation. Model FP32 A8W8 A4W8 A8W4 ResNet-18 69.76% 69.80% 67.70% 67.49% ResNet-34 73.31% 73.39% 71.47% 72.01% ResNet-50 76.13% 76.22% 72.79% 75.03% ResNet-101 77.37% 77.38% 73.74% 76.41% GoogLeNet 69.78% 69.67% 65.38% 65.81% Inception-v3 77.49% 77.50% 73.91% 74.22% DenseNet-121 74.69% 74.68% 72.57% 72.89% SqueezeNet 58.09% 57.81% 28.12% 34.14% In Section 5.3, we experiment with a 2:4 structured pruning [23]. To achieve the sparse model with the baseline accuracy, we prune the network based on its pretrained weights and retrain the model from scratch for 90 epochs with a learning rate starting from 0.1 and divided by 10 at epochs 30 and 60. Weight decay and momentum are set to 0.0001 and 0.9, respectively. 6The different designs are implemented using SystemVerilog and synthesized using Synopsys® Design Compiler® and Virage (now Synopsys) 65nm standard cell library. We use a frequency of 500MHz at slow and fast corners for setup and hold timing closure, respectively. Area estimates were extracted after place-and-route using Cadence® Innovus™ . We assume that the overall hardware overhead related to activation trimming and rounding is relatively negligible with respect to the SA and TC, since (1) the trimming and rounding unit involves a simple hardware scheme; and (2) it is performed at a signiﬁcantly lower processing rate. We validated our multiplier against our PyTorch CUDA implementation with cycle-accurate testbenches to verify calculation integrity. 5.1 Accuracy Results In Table 2, we present our method’s results for the 5opt, 3opt, and 2opt conﬁgurations, with and without rounding (±R), as described in Section 3.1, and without vSPARQ (-vS). As expected, we observe that (1) better accuracy is achieved with the increase of window placement options; (2) overall, rounding further reduces quantization noise, which leads to smaller accuracy degradation; and (3) vSPARQ contribution is noticeable mainly in conﬁgurations with relatively high quantization noise. In addition, we observe a large impact on accuracy in the transition from 2opt to 3opt, since there is a high probability that at least one of the 4-bit MSBs will be toggled. For example, given the non-zero-valued activations in ResNet-18 with the ILSVRC-2012 dataset, we measure that bits 7, 6, 5, and 4 are toggled in 0.5%, 9.2%, 33.8%, and 44.8% of the time, respectively. Assuming the bit values are statistically independent, the probability of at least one toggled bit is 67%. Notice that there is a clear redundancy in the 2opt conﬁguration that picks the 4-bit MSBs, even though 10% of the time the two MSBs are toggled. Table 2: SPARQ accuracy results using the ILSVRC-2012 dataset, without rounding (-R), with rounding (+R), and with rounding but without vSPARQ (+R-vS). 5opt 3opt 2opt Model Trim +R +R-bS Trim +R +R-bS Trim +R +R-bS ResNet-18 -0.11% -0.07% -0.11% -0.22% -0.14% -0.48% -2.87% -1.37% -2.02% ResNet-34 -0.00% +0.04% -0.05% -0.25% -0.14% -0.25% -2.38% -1.10% -1.75% ResNet-50 -0.03% -0.05% -0.02% -0.41% -0.18% -0.31% -4.18% -2.18% -2.83% ResNet-110 -0.22% -0.25% -0.19% -0.67% -0.59% -0.60% -3.31% -1.64% -2.82% GoogLeNet -0.83% -0.68% -0.77% -1.59% -0.75% -0.99% -5.14% -2.55% -4.31% Inception-v3 -0.73% -0.62% -0.95% -1.51% -1.21% -1.68% -3.98% -1.86% -3.30% DenseNet-121 +0.10% +0.09% +0.05% -0.16% +0.05% -0.02% -2.39% -0.57% -1.10% SqueezeNet -1.63% -0.80% -0.90% -3.73% -1.05% -1.26% -54.5% -8.24% -11.6% Computationally, SPARQ may be considered as a dynamic 4b-8b PTQ, in which quantization to 4 bits from 8 bits is conducted occasionally in the event of two adjacent non-zero-value activations. The upside of conventional PTQ methods, however, is the reduction in memory footprint, where the dynamic method falls short, due to the additional metadata. For example, the 3opt conﬁguration requires additional 3-bit metadata per 4-bit activation data (2-bit ShiftCtrl and 1-bit MuxCtrl). Still, the memory footprint may be reduced by grouping the metadata for several activations, which we leave for future exploration. In Table 3, we present our results compared with previous related works [1, 5, 6, 31]. We would like to point out that SySMT is similar to the 2opt conﬁguration. The slightly different results are due to the different BatchNorm calibrations and the slightly different 8-bit quantized models. Regarding ResNet-50, SySMT quantizes its weights, whereas SPARQ focuses on quantizing activations. Reducing the bit width: 3 bits and 2 bits.To further challenge SPARQ efﬁciency, we experiment with 3-bit and 2-bit conﬁgurations. The lower budget leads to increased quantization noise even when one of the activations within the activation pair has a zero value, since the total window sizes are 6 and 4 bits for the 3-bit and 2-bit conﬁgurations, respectively. In Table 4, we present SPARQ accuracy results compared with other methods that reported sub-4b quantization results. As opposed to Table 2, we observe that vSPARQ impact is more signiﬁcant in lower bit-widths. 7Table 3: Relative top-1 accuracy degradation (relative to FP32) of SPARQ versus different quantiza- tion methods used for 4b-8b quantization (the best out of 4-bit activations or weights). SPARQ Model 5opt 3opt 2opt SySMT PWLQ ACIQ LBQ KURE ResNet-18 -0.07% -0.14% -1.37% -1.29% - -2.01% -1.20% -2.84% ResNet-34 +0.04% -0.14% -1.10% - - - - - ResNet-50 -0.03% -0.18% -2.18% -0.43% -0.67% -1.05% -1.36% -0.92% ResNet-101 -0.22% -0.59% -1.64% - - -0.52% -1.18% - GoogLeNet -0.68% -0.75% -2.55% -2.85% - - - - Inception-v3 -0.62% -1.21% -1.86% - -1.34% -2.72% -1.88% - DenseNet-121 +0.10% +0.05% -0.57% -0.39% - - -1.17% - SqueezeNet -0.80% -1.05% -8.24% - - - -2.96% - Table 4: Relative top-1 accuracy degradation (relative to FP32) for 3-bit and 2-bit SPARQ (with 8-bit weights) in 6opt and 7opt conﬁgurations, respectively, also with and without vSPARQ (-vS). SPARQ KURE ACIQ Model 3b 2b 3b (-vS) 2b (-vS) 3b 2b 3b ResNet-18 -0.21% -1.64% -0.51% -2.57% -10.9% -42.8% -17.1% ResNet-34 -0.18% -1.19% -0.37% -1.66% - - - ResNet-50 -0.59% -2.34% -0.73% -3.53% -3.53% -15.9% -11.4% ResNet-101 -0.66% -2.64% -1.06% -3.73% - - -6.08% GoogLeNet -1.32% -6.47% -1.91% -9.16% - - - Inception-v3 -1.70% -5.60% -2.45% -9.29% - - -26.4% DenseNet-121 -0.07% -0.86% -0.25% -1.73% - - - SqueezeNet -1.63% -10.4% -2.32% -15.0% - - - 5.2 Hardware Evaluation Table 5 summarizes the area overhead normalized to the MAC throughput of SPARQ for both SA and TC use cases. The SA and TC baselines are conventional 8b-8b SA and TC PEs, respectively. Memory, such as SRAMs, are not considered in the analysis (which could decrease the area overhead percentages). The 2×4b-8b design is presented as a reference implementation in the case of 4b-8b quantized values with equivalent throughput to the design in Figure 2. For the sake of fair comparison, there is a single psum in the 2×4b-8b design. With respect to the SA, the 2×4b-8b PE requires approximately half the area per MAC operation than the 8b-8b PE. On the one hand, the total multipliers’ area of the 2×4b-8b PE is signiﬁcantly smaller; however, the 2×4b-8b PE employs a 3-input adder. The shift-left logic is the main contributor to the increasing area overhead of opt2 through opt5. As the number of shift-left options increases, the shift logic becomes more complex and utilizes a bigger logic area. Regarding 6opt (3 bits) and 7opt (2 bits) conﬁgurations, even though they require additional window placement options, the overall area decreases, since the area of the multipliers, registers, and multiplexers within the shift-left units is reduced. Also, our 2opt scheme introduces a signiﬁcantly smaller area overhead compared with SySMT, due to the fact that SySMT required the trimming and rounding hardware to operate at the same high throughput rate as the SA. Regarding TC, the 2×4b-8b implementation requires half the area (normalized) of the TC 8b-8b baseline PE. Similar to the SA use case, the 2 ×4b-8b PE multipliers are smaller; however, this time the 2×4b-8b PE adder tree grows. Interestingly, the relative area of 5opt no-vSPARQ (-vS) is only slightly higher than the “full” 3opt SPARQ implementation. Given the accuracy differences between the two conﬁgurations (Table 2), the 3opt SPARQ operating point presented in this work may not be a good trade-off between accuracy and area. 8Table 5: Relative hardware area (normalized to MAC operation throughput) of different SA and TC implementations. SPARQ SPARQ (-vS) Type 8b-8b 2 ×4b-8b 7opt 6opt 5opt 3opt 2opt 5opt 3opt SySMT Systolic Array PE 1.00 0.50 0.59 0.66 0.72 0.61 0.57 0.62 0.59 0.72 Tensor Core PE 1.00 0.50 0.58 0.63 0.72 0.66 0.61 0.67 0.61 - Table 6: Accuracy results of SPARQ simulated on top of an STC with 2:4 structured pruned models. 4-bit 3-bit 2-bit Model FP32 A8W8 5opt 3opt 2opt 6opt 7opt ResNet-18 69.77% 69.79% -0.13% -0.34% -1.59% -0.41% -1.92% ResNet-50 76.16% 76.10% -0.24% -0.57% -2.59% -0.85% -3.18% ResNet-101 77.38% 77.34% -0.28% -0.39% -2.06% -0.79% -2.94% 5.3 Leveraging Activation Sparsity on Top of Sparse Tensor Cores We simulate SPARQ on top of an STC with models pruned with 2:4 structured pruning. As presented in Figure 5, activations are ﬁrst ﬁltered through the multiplexers according to the non-zero-value weight coordinates. Then, vSPARQ comes into play, inspecting pairs of activations, as described in Section 3. Since in STC the trimming and rounding logic should be replicated for each DP unit, we implemented and synthesized the trimming and rounding unit to estimate its area overhead. The unit area, relative to the conventional TC (Figure 4), is 17%, 12%, and 9% for the 5opt, 3opt, and 2opt conﬁgurations, respectively. The relative area may be even smaller if we consider the entire STC design (Figure 5). SPARQ is, therefore, beneﬁcial in terms of performance-to-area when attached to an STC. In Table 6, we report the pruned models’ FP32 and A8W8 quantized accuracies, and repeat all experiments described thus far. Interestingly, the relative accuracy degradation of the pruned models is slightly higher than that of the unpruned models in Table 3 [17, 40]. Nevertheless, SPARQ still achieves less than 1% relative degradation in accuracy with 4-bit 5opt and 3opt, and 3-bit 6opt. 6 Limitations and Societal Impacts SPARQ has two main limitations: (1) It does not achieve the memory footprint decrease as native 4b-8b quantization methods do, because of the additional metadata that accompanies each value, as discussed in Section 5.1. The memory footprint may be decreased by giving up vSPARQ or sharing ShiftCtrl for a number of activations. We leave these research directions for future work. (2) From a hardware perspective, SPARQ requires hardware support, i.e., it cannot run on today’s commodity hardware. In addition, compared with native 4b-8b quantizations, our hardware implementation incurs some overhead, as described in Section 5.2. As for the societal impacts, quantization methods, in general, increase the effective amount of available computing resources, since the execution requirements of quantized models are lower. The effective increase in computing power may be targeted towards negative use, such as surveillance and fake proﬁle generation. 7 Conclusion We present SPARQ, a sparsity-aware quantization method that dynamically leverages sparsity in different granularities—from the entire 8-bit value to the individual bits. Thanks to the inherent activation sparsity, quantization to n bits occurs only occasionally. When quantization to n bits does occur, bit-level sparsity is leveraged by trimming leading zero bits and picking the most signiﬁcant consecutive n bits. SPARQ induces minor accuracy degradation and is hardware-friendly. 9Acknowledgements We thank the anonymous reviewers for their comments and suggestions. We also thank Moran Shkolnik, Mario Shalabi, and Michael Behar for their valuable feedback. References [1] R. Banner, Y . Nahshan, and D. Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. In Advances in Neural Information Processing Systems (NIPS), pages 7948–7956, 2019. [2] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, and E. Shelhamer. cuDNN: Efﬁcient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014. [3] J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V . Srinivasan, and K. Gopalakrishnan. PACT: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. [4] J. Choquette, O. Giroux, and D. Foley. V olta: Performance and programmability. IEEE Micro, 38(2): 42–52, 2018. [5] Y . Choukroun, E. Kravchik, F. Yang, and P. Kisilev. Low-bit quantization of neural networks for efﬁcient inference. In International Conference on Computer Vision (ICCV) Workshops, pages 3009–3018, 2019. [6] J. Fang, A. Shaﬁee, H. Abdel-Aziz, D. Thorsley, G. Georgiadis, and J. H. Hassoun. Post-training piecewise linear quantization for deep neural networks. In European Conference on Computer Vision (ECCV), pages 69–86, 2020. [7] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia, et al. The architectural implications of facebook’s DNN-based personalized recommendation. In International Symposium on High Performance Computer Architecture (HPCA), pages 488–501, 2020. [8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. [9] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 4700–4708, 2017. [10] I. Hubara, Y . Nahshan, Y . Hanani, R. Banner, and D. Soudry. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning (ICML), pages 4466–4475. PMLR, 2021. [11] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016. [12] S. Jain, S. Venkataramani, V . Srinivasan, J. Choi, K. Gopalakrishnan, and L. Chang. BiScaled-DNN: Quantizing long-tailed datastructures with two scale factors for deep neural networks. InDesign Automation Conference (DAC), pages 1–6. IEEE, 2019. [13] Z. Jia, M. Maggioni, B. Staiger, and D. P. Scarpazza. Dissecting the NVIDIA V olta GPU architecture via microbenchmarking. arXiv preprint arXiv:1804.06826, 2018. [14] R. Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. [15] H. Kung and C. E. Leiserson. Systolic arrays (for VLSI). In Sparse Matrix Proceedings 1978, pages 256–282. Society for Industrial and Applied Mathematics, 1979. [16] Y . Li, R. Gong, X. Tan, Y . Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. BRECQ: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations (ICLR), 2021. [17] L. Liebenwein, C. Baykal, B. Carter, D. Gifford, and D. Rus. Lost in pruning: The effects of pruning neural networks beyond test accuracy. In Conference on Machine Learning and Systems (MLSys), 2021. [18] Z.-G. Liu, P. N. Whatmough, and M. Mattina. Sparse systolic tensor array for efﬁcient CNN hardware acceleration. arXiv preprint arXiv:2009.02381, 2020. [19] S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, and J. S. Vetter. NVIDIA tensor core programmability, performance & precision. In International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pages 522–531. IEEE, 2018. 10[20] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu, and P. Micikevicius. Accelerat- ing sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021. [21] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), pages 7197–7206. PMLR, 2020. [22] T. Norrie, N. Patil, D. H. Yoon, G. Kurian, S. Li, J. Laudon, C. Young, N. Jouppi, and D. Patterson. The design process for Google’s training chips: TPUv2 and TPUv3. IEEE Micro, 41(2):56–63, 2021. [23] NVIDIA. NVIDIA A100 tensor core GPU architecture. https://www.nvidia.com/content/dam/en- zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf. [24] E. Park, D. Kim, and S. Yoo. Energy-efﬁcient neural network accelerator based on outlier-aware low- precision computation. In International Symposium on Computer Architecture (ISCA), pages 688–698, 2018. [25] E. Park, S. Yoo, and P. Vajda. Value-aware quantization for training and inference of neural networks. In European Conference on Computer Vision (ECCV), pages 580–595, 2018. [26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NIPS), pages 8024–8035. 2019. [27] M. A. Raihan, N. Goli, and T. M. Aamodt. Modeling deep learning accelerator enabled GPUs. In International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 79–92, 2019. [28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. [29] S. Schneider, E. Rusak, L. Eck, O. Bringmann, W. Brendel, and M. Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems (NIPS), pages 11539–11551, 2020. [30] T. R. Shaham, T. Dekel, and T. Michaeli. SinGAN: Learning a generative model from a single natural image. In International Conference on Computer Vision (ICCV), pages 4570–4580, 2019. [31] M. Shkolnik, B. Chmiel, R. Banner, G. Shomron, Y . Nahshan, A. Bronstein, and U. Weiser. Robust quantization: One model to rule them all. In Advances in Neural Information Processing Systems (NIPS), volume 33, pages 5308–5317, 2020. [32] G. Shomron and U. Weiser. Non-blocking simultaneous multithreading: Embracing the resiliency of deep neural networks. In International Symposium on Microarchitecture (MICRO), pages 256–269, 2020. [33] G. Shomron and U. Weiser. Post-training BatchNorm recalibration. arXiv preprint arXiv:2010.05625, 2020. [34] G. Shomron, T. Horowitz, and U. Weiser. SMT-SA: Simultaneous multithreading in systolic arrays. Computer Architecture Letters (CAL), 18(2):99–102, 2019. [35] G. Shomron, R. Banner, M. Shkolnik, and U. Weiser. Thanks for nothing: Predicting zero-valued activations with lightweight convolutional neural networks. In European Conference on Computer Vision (ECCV), 2020. [36] X. Sun, J. Choi, C.-Y . Chen, N. Wang, S. Venkataramani, V . V . Srinivasan, X. Cui, W. Zhang, and K. Gopalakrishnan. Hybrid 8-bit ﬂoating point (HFP8) training and inference for deep neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 4900–4909, 2019. [37] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–9, 2015. [38] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, 2016. 11[39] A. Vasudevan, A. Anderson, and D. Gregg. Parallel multi channel convolution using general matrix multiplication. In International Conference on Application-Speciﬁc Systems, Architectures and Processors (ASAP), pages 19–24, 2017. [40] R. Yazdani, M. Riera, J.-M. Arnau, and A. González. The dark side of DNN pruning. In International Symposium on Computer Architecture (ISCA), pages 790–801. IEEE, 2018. [41] R. Zhao, J. Dotzel, Z. Hu, P. Ivanov, C. D. Sa, and Z. Zhang. OverQ: Opportunistic outlier quantization for neural network accelerators. arXiv preprint arXiv:1910.06909, 2021. [42] S. Zhou, Y . Wu, Z. Ni, X. Zhou, H. Wen, and Y . Zou. DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 12",
      "meta_data": {
        "arxiv_id": "2105.11010v2",
        "authors": [
          "Gil Shomron",
          "Freddy Gabbay",
          "Samer Kurzum",
          "Uri Weiser"
        ],
        "published_date": "2021-05-23T20:12:35Z",
        "pdf_url": "https://arxiv.org/pdf/2105.11010v2.pdf"
      }
    },
    {
      "title": "Extreme Compression of Large Language Models via Additive Quantization",
      "abstract": "The emergence of accurate open large language models (LLMs) has led to a race\ntowards performant quantization techniques which can enable their execution on\nend-user devices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression-defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter-from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our algorithm, called AQLM, generalizes the classic\nAdditive Quantization (AQ) approach for information retrieval to advance the\nstate-of-the-art in LLM compression, via two innovations: 1) learned additive\nquantization of weight matrices in input-adaptive fashion, and 2) joint\noptimization of codebook parameters across each transformer blocks. Broadly,\nAQLM is the first scheme that is Pareto optimal in terms of\naccuracy-vs-model-size when compressing to less than 3 bits per parameter, and\nsignificantly improves upon all known schemes in the extreme compression (2bit)\nregime. In addition, AQLM is practical: we provide fast GPU and CPU\nimplementations of AQLM for token generation, which enable us to match or\noutperform optimized FP16 implementations for speed, while executing in a much\nsmaller memory footprint.",
      "full_text": "Extreme Compression of Large Language Models via Additive Quantization Vage Egiazarian* 1 2 Andrei Panferov* 1 2 Denis Kuznedelev 2 3 Elias Frantar 4 Artem Babenko 2 Dan Alistarh 4 5 Abstract The emergence of accurate open large language models (LLMs) has led to a race towards perfor- mant quantization techniques which can enable their execution on end-user devices. In this pa- per, we revisit the problem of “extreme” LLM compression—defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter—from the point of view of classic methods in Multi- Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information re- trieval to advance the state-of-the-art in LLM com- pression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook pa- rameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when com- pressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addi- tion, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token gen- eration, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint. 1. Introduction The rapid advancement of generative large language models (LLMs) has led to massive industrial and popular interest, driven in part by the availability of accurate open LLMs, such as LLAMA 1 and 2 (Touvron et al., 2023), Falcon (TII UAE, 2023), BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), or NeoX/Pythia (Biderman et al., 2023). A key advantage of open models is that they can be inferenced or fine-tuned locally by end-users, assuming that their compu- tational and memory costs can be reduced to be manageable on commodity hardware. This has led to several methods for *Equal contribution 1HSE University 2Yandex Research 3Skoltech 4IST Austria 5NeuralMagic. Correspondence to: <dan.alistarh@ist.ac.at>. 7 13 70 #Params (×109) 3 4 5 6 7 8Perplexity on WikiText-2 QuIP# (2bit) AQLM (2bit) Baseline (FP16) Figure 1: Comparison of AQLM (2-bit) relative to the state- of-the-art QuIP# (2-bit) and the original 16-bit weights on LLAMA 2 7, 13, and 70B models. inference and fine-tuning on compressed LLMs (Dettmers et al., 2022; Frantar et al., 2022a; Dettmers & Zettlemoyer, 2022; Lin et al., 2023; Dettmers et al., 2023a). Currently, the primary approach for accurate post-training compression of LLMs is quantization, which reduces the bit-width at which model weights (and possibly activations) are stored, leading to improvements in model footprint and memory transfer. By and large, LLM weights are compressed via “direct” quantization, in the sense that a suitable quantization grid and normalization are first chosen for each matrix sub- component, and then weights are each mapped onto the grid either by direct rounding, e.g. (Dettmers & Zettlemoyer, 2022), or via more complex allocations, e.g. (Frantar et al., 2022a). Quantization induces a natural compression-vs- accuracy trade-off, usually measured in terms of model size vs model perplexity (PPL). Existing approaches can achieve arguably low accuracy loss at 3-4 bits per element (Dettmers et al., 2023b; Chee et al., 2023; Kim et al., 2023), and can even stably compress models to 2 or even less bits per ele- ment, in particular, for extremely large models (Frantar & Alistarh, 2023). Yet, in most cases, low bit counts come at the cost of significant drops in accuracy, higher implementa- tion complexity and runtime overheads. Specifically, from the practical perspective, “extreme” quantization in the 2-bit range using current techniques is inferior to simply using a smaller base model and quantizing it to higher bitwidths, such as 3-4 bits per parameter, as the latter yields higher accuracy given the same model size in bytes (Dettmers & Zettlemoyer, 2022; Chee et al., 2023). 1 arXiv:2401.06118v4  [cs.LG]  11 Sep 2024Extreme LLM Compression via Additive Quantization Contribution. In this work, we improve the state-of-the-art in LLM compression by showing for the first time thatMulti- Codebook Quantization (MCQ) techniques can be extended to LLM weight compression. Broadly, MCQ is a family of information retrieval methods (Chen et al., 2010; Jegou et al., 2010; Ge et al., 2013; Zhang et al., 2014; Babenko & Lempitsky, 2014; Martinez et al., 2016; 2018), consisting of specialized quantization algorithms to compress databases of vectors, allowing for efficient search. Unlike direct quan- tization, MCQ compresses multiple values jointly, by lever- aging the mutual information of quantized values. More precisely, we extend Additive Quantization (AQ) (Babenko & Lempitsky, 2014; Martinez et al., 2016), a popular MCQ algorithm, to the task of compressing LLM weights such that the output of each layer and Transformer block are approximately preserved. Our extension reformulates the classic AQ optimization problem to reduce the error in LLM layer outputs under the input token distribution and as well as to jointly optimize codes over layer blocks, rather than only preserving the weights themselves as in standard AQ. We refer to the resulting procedure as Additive Quantization of Language Models (AQLM). Unlike some extreme LLM quantization approaches that require hybrid sparse-quantized formats which separate outlier quantization (Kim et al., 2023; Dettmers et al., 2023b), AQLM quantizes models in a simple homogeneous format, which is easy to support in practice. Our main contributions are as follows: 1. We propose the AQLM algorithm, which extends AQ to post-training compression of LLM weights, via two innovations: (1) adapting the MAP-MRF 1 optimiza- tion problem behind AQ to be instance-aware, taking layer calibration input & output activations into ac- count; (2) complementing the layer-wise optimization with an efficient intra-block tuning technique, which optimizes quantization parameters jointly over several layers, using only the calibration data. 2. We evaluate the effectiveness of this algorithm on the task of compressing accurate open LLMs from the LLAMA 2 (Touvron et al., 2023) family with com- pression rates of 2-4 bits per parameter. We find that AQLM outperforms the previous state-of-the-art across the standard 2-4 bit compression range, with the most significant improvements for extreme 2-bit quantiza- tion (see Figure 1). We provide detailed ablations for the impact of various algorithm parameters, such as code width and number of codebooks, and extend our analysis to the recent Mixtral model (Jiang et al., 2024). We also evaluate AQLM with improved fine- tuning algorithms from subsequent works, which leads to further increase in accuracy for 2- and 3-bit models. 1Maximum a Posteriori inference in Markov Random Fields 3. We show that AQLM is practical, by providing efficient GPU and CPU kernels implementations for specific encodings, as well as end-to-end generation2. Results show that our approach can match or even outperform the floating point baseline in terms of speed, while re- ducing the memory footprint by up to 8x. Specifically, AQLM can be executed with layer-wise speedups of ∼ 30% for GPUs, and of up to 4x for CPU inference. 2. Background & Related Work 2.1. LLM Quantization Early efforts towards post-training quantization (PTQ) methods (Nagel et al., 2020; Gholami et al., 2021) that scale to LLMs such as ZeroQuant (Yao et al., 2022), LLM.int8() (Dettmers et al., 2022), and nuQmm (Park et al., 2022) employed direct round-to-nearest (RTN) projections, and adjusted quantization granularity to balance memory efficiency and accuracy. GPTQ (Frantar et al., 2022a) pro- posed a more accurate data-aware approach via an approxi- mate large-scale solver for minimizing layer-wise ℓ2 errors. Dettmers & Zettlemoyer (2022) examined the accuracy- compression trade-offs of these early methods, suggesting that 4-bit quantization may be optimal for RTN quantization, and observing that data-aware methods like GPTQ allow for higher compression, i.e. strictly below 4 bits/weight, maintaining Pareto optimality. Our work brings this Pareto frontier below 3 bits/weight, for the first time. Parallel work quantizing both weights and activations to 8-bits, by Dettmers et al. (2022), Xiao et al. (2022), and Yao et al. (2022) noted that the “outlier features” in large LLMs cause substantial errors, prompting various mitigation strategies. Recently, several improved techniques have focused on the difficulty of quantizing weight outliers, which have high impact on the output error. SpQR (Dettmers et al., 2023b) addresses this by saving outliers as a highly-sparse higher- precision matrix. AWQ (Lin et al., 2023) reduces the error of quantizing channels with the highest activation magnitudes by employing per-channel scaling to reduce the error on important weights. SqueezeLLM (Kim et al., 2023) uses the diagonal Fisher as a proxy for the Hessian and implements non-uniform quantization through K-means clustering. The published state-of-the-art method is QuIP (Chee et al., 2023). Concurrent to our work, an improved variant called QuIP# (Tseng et al., 2024) was introduced. Roughly, they work by first “smoothening” weights by multiplying with a rotation matrix, and then mapping them onto a lattice. At a high level, QuIP and QuIP# aim to minimize the “worst- case” error for each layer, given initial weights and calibra- tion data. For instance, in QuIP#, the distribution of the 2https://github.com/Vahe1994/AQLM 2Extreme LLM Compression via Additive Quantization rotated weights approximates a Gaussian, while the encod- ing lattice (E8P) is chosen to minimize “rounding” error. By contrast, our approach uses a different weight encoding (codebooks are additive), and learned codebooks instead of a fixed codebook. Thus, our insight is that we should be able to obtain higher accuracy by direct optimization of the codebooks over the calibration set, removing the rotation. Further, we show that codebooks for different layers can co-train via joint fine-tuning over the calibration data. 2.2. Quantization for Nearest Neighbor Search Our work builds on approximate nearest neighbor search (ANN) algorithms. Unlike PTQ, ANN quantization aims to compress a database of vectors to allow a user to efficiently compute similarities and find nearest neighbors relative to a set of query points. For high compression, modern ANN search algorithms employ vector quantization (VQ)—which quantizes multiple vector dimensions jointly (Burton et al., 1983; Gray, 1984). It achieves this by learning “codebooks”: i.e. a set of learnable candidate vectors that can be used to encode the data. To encode a given database vector, VQ splits it into sub-groups of entries, then encodes every group by choosing a vector from the learned codebook. The algorithm efficiently computes distances or dot-products for similarity search by leveraging the linearity of dot products. Quantization methods for ANN search generalize vector quantization and are referred to as multi-codebook quan- tization (MCQ). MCQ methods typically do not involve information loss on the query side, which makes them the leading approach for memory-efficient ANN (Ozan et al., 2016; Martinez et al., 2018). We briefly review MCQ below. Product quantization (PQ) (Jegou et al., 2010) is an early version of MCQ, which encodes each vector x ∈ RD as a concatenation of M codewords from M D M -dimensional codebooks C1, . . . , CM , each containing K codewords. PQ decomposes a vector intoM separate subvectors and applies vector quantization (VQ) to each subvector, while using a separate codebook. Thus, each vector x is encoded by a tuple of codeword indices[i1, . . . , iM ] and approximated by x ≈ [c1i1 , . . . , cMiM ]. Fast Euclidean distance computation becomes possible using lookup tables: ∥q − x∥2 ≈ ∥q − [c1i1 , . . . , cMiM ]∥2 = MX m=1 ∥qm − cmim∥2, where qm is the mth subvector of a query q. This sum can be calculated using M additions and lookups if the dis- tances from query subvectors to codewords are precomputed. Since product-based approximations work better if the D M - dimensional components independent distributions, subse- quent work has looked into finding better transformations (Ge et al., 2013; Norouzi & Fleet, 2013). As for the other similarity functions, (Guo et al., 2016) proposes a quantiza- tion procedure for maximum inner product search (MIPS). They minimize quantization error in the inner products be- tween database and query vectors by solving a constrained optimization problem. Similarly to the formula above, this procedure allows for efficient inner product search by pre- computing dot products between the query q an all codes in the learned codebooks, then adding these partial dot prod- ucts to recover the full similarity score. Non-orthogonal quantizations. Follow-up work (Chen et al., 2010; Babenko & Lempitsky, 2014; Martinez et al., 2016; Zhang et al., 2014; Ozan et al., 2016; Martinez et al., 2018) generalized the idea of Product Quantization by ap- proximating each vector by a sum of M codewords instead of concatenation. The resulting procedure is still efficient while the approximation accuracy is increased. For this, Residual Vector Quantization (Chen et al., 2010), quantizes original vectors, and then iteratively quantizes the approximation residuals from the previous iteration. Addi- tive Quantization (AQ) (Babenko & Lempitsky, 2014) is more general, as it does not impose constraints on the code- words from the different codebooks. Usually, AQ provides the smallest compression errors, but is more complex to train for large M. We discuss this in detail in Section 3. Finally, several recent works (Martinez et al., 2016; 2018; Zhang et al., 2014) elaborate the idea of Additive Quantiza- tion, proposing the more effective procedure for codebooks learning. Composite Quantization (CQ) (Zhang et al., 2014) learns codebooks with a fixed value of inner product be- tween the codewords from different codebooks. Currently, the state-of-the-art compression accuracy is achieved by the LSQ method (Martinez et al., 2018). Vector quantization for model compression. There has been significant work on exploiting vector quantization in the context of machine learning. For instance, Zhou et al. (2017); Li et al. (2017); Chen et al. (2019) use multi-codebook quantization to compress word embeddings within deep learning models. Another line of work (Blalock & Guttag, 2021; McCarter & Dronen, 2022; Fernández- Marqués et al., 2023) explores vector quantization for linear models, or linear layers within deep models. Similarly to PQ above, these techniques pre-compute inner products be- tween inputs and all codes, then compute linear layer via look-up, which speeds up inference. However, these algo- rithms introduce significant prediction error that does not allow them to compress deep models. Thus, we believe we are the first to successfully adapt and scale MCQ to LLMs. 3Extreme LLM Compression via Additive Quantization 3. AQLM: Additive Quantization for LLMs 3.1. Overview We start from the observation that additive quantization (AQ) solves a related problem to post-training quantization (PTQ) (Nagel et al., 2020; Frantar et al., 2022b): both set- tings assume the existence of a set of “input” vectors, i.e. input data for AQ, and the weight matrix rows for PTQ. The goal is to compress these inputs while preserving dot prod- uct similarity, against query vectors (for AQ), and against layer input embeddings (for PTQ). The difference between the two is that AQ assumes that the distribution of queries is unknown, whereas PTQ methods, e.g. (Frantar et al., 2022b), show that it is sufficient to optimize for sample input embeddings from a set of calibration data. At a high level, we start by solving the following problem: for a linear layer with din input and dout output features given its weights W ∈ Rdout×din and a set of calibration inputs X ∈ Rdin×n, one seeks for a configuration of quan- tized weights cW that optimizes squared error between the output of the original and compressed layer: arg min cW ||WX − cWX||2 2. (1) In the following, we will assume that cW is quantized using AQ, and adopt standard notation (Martinez et al., 2016). AQ splits weight rows into groups of g consecutive elements, and represents each group of weights as a sum of M vec- tors chosen from multiple learned codebooks C1, ..., CM , each containing 2B vectors (for B-bit codes). A weight is encoded by choosing a single code from each codebook and summing them up. We denote this choice as a one-hot vector bm, which results in the following representation for a group: PM m=1 Cmbijm. This is similar to PTQ algo- rithms (Frantar et al., 2022a), except for using much more complex coding per group. To represent the full weights, we simply concatenate: cWi= MX m=1 Cmbi,1,m ⊕ ... ⊕ MX m=1 Cmbi,din/g,m, (2) where ⊕ denotes concatenation and bijm ∈ R2B represents a one-hot code for the i-th output unit, j-th group of input dimensions and m-th codebook. Our algorithm will learn codebooks Cm ∈ Rg×2B and the discrete codes represented by one-hot b ∈ Rdout×din/g×M×2B . The resulting scheme encodes each group of g weights using M · B bits and further requires g · 2B · 16 bits for FP16 codebooks. The error becomes: arg min C,b ||WX −   Concati,j MX m=1 Cmbi,j,m ! X||2 2. (3) m m Σ i j m i j m i j m i j Codes Codes Codebooks Expanded Codes Unscaled Rows Weight Matrix Scales CodesCodebooksWeight Matrix Figure 2: Groups of weights are represented by a sum of codes selected from codebooks by corresponding indices. To learn this weight representation, we initialize codebooks C and codes b by running residual K-means as in Chen et al. (2010). Specifically, the initialization algorithm proceeds as follows: first, it runs K-means clustering of weight groups and saves the resulting cluster indices. Next, it computes the quantization errors by subtracting the nearest cluster from every weight. Finally, the algorithm runs another round of K-means clustering, but this time on quantization errors instead of weights. Thus, each subsequent codebook is ini- tialized to compensate the quantization error from previous codebooks. After initialization, we alter between updating codes bi,j,m and codebooks Cm until the loss function (3) stops improving up to the specified tolerance. Since codes are discrete and codebooks are continuous, and we are op- timizing over multiple interacting layers, our approach has three phases, described in Algorithm 1 and detailed below. 3.2. Phase 1: Beam search for codes First, AQLM updates the codes bi,j,m to minimize the MSE objective (3). Similarly to Babenko & Lempitsky (2014); Martinez et al. (2016; 2018), we reformulate the objective in terms of a fully-connected discrete Markov Random Field (MRF) to take advantage of MRF solvers. To simplify the derivation, let us first consider a special case of a single output unit (dout=1) and a single quantization group (i.e. g=din), to get rid of the concatenation operator: ||WX − PM m=1 CmbmX||2 2. We rewrite this objective by expanding the squared difference: ||WX − MX m=1 CmbmX||2 2 = ||WX||2 2− − 2 * WX , MX m=1 CmbmX + F + || MX m=1 CmbmX||2 2 (4) Above, ⟨·, ·⟩F denotes a Frobenius inner product of two matrices. Next, let us consider the three components of Eqn. (4) in isolation. First, note that ||WX||2 2 is constant in b and can be ignored. The third component can be expanded further into pairwise dot products: 4Extreme LLM Compression via Additive Quantization || MX m=1 CmbmX||2 2 = MX i=1 MX j=1 ⟨CibiX, CjbjX⟩F . (5) Note that both the second and third components rely on Frobenius products of CmbmX-like matrices. These matri- ces can be inconvenient in practice: since X ∈ Rdin×n, the size of each matrix will scale with the size of calibration dataset n. To circumvent this, we rewrite the products as: ⟨CibiX, CjbjX⟩F =  CibiXXT , Cjbj \u000b F . (6) Thus one can pre-compute XXT ∈ Rdin×din. We will de- note this type of product as ⟨A, B⟩XXT def =  AXXT , B \u000b F in future derivations. Then, Eqn. (4) becomes: ||WX − MX m=1 CmbmX||2 2 = ||WX||2 2− −2 MX m=1 ⟨W, Cmbm⟩XXT + MX i=1 MX j=1 ⟨Cibi, Cjbj⟩XXT . (7) Finally, we generalize this equation to multiple output units (dout > 1) and quantization groups (g̸=din). For dout > 1, note that the original objective (3) is additive with respect to output units: thus, we can apply (7) independently to each output dimension and sum up results. To support multiple input groups (g̸=din), we can treat each group as a separate codebook where only the codes for the active group are nonzero. Thus, we need to repeat each codebook din/g times and pad it with zeros according to the active group. It is now evident that minimizing (4) is equivalent to MAP inference in a Markov Random Field with⟨W, Cmbm⟩XXT as unary potentials and ⟨Cibi, Cjbj⟩XXT as pairwise poten- tials. While finding the exact optimum is infeasible, prior work has shown that this type of MRF can be solved approx- imately via beam search or ICM (Besag, 1986). To solve this problem, we chose to adapt a beam search al- gorithm from Babenko & Lempitsky (2014). This algorithm maintains a beam of k (beam size) best configurations for the codes, starting from the previous solution. On each step, the algorithm attempts to replace one code by trying all2Bk alternatives and selecting the k best based on MSE (7). Since the loss function is additive, changing one code only affects a small subset of loss components. Thus, we can compute the loss function efficiently by starting with a pre- vious loss function (before code replacement), then adding and subtracting the components that changed during this iteration. These few loss components can be computed ef- ficiently by multiplying with XXT ahead of beam search. The beam search runs over all dout output units in paral- lel. This is possible because encoding one output unit does not affect the objective (7) of other units. Note that beam search is not necessarily the best solution to this problem. AQ variants for retrieval (Martinez et al., 2016; 2018) use randomized ICM to find solutions faster. In this study, we chose beam search because it was easier to implement in ML frameworks like PyTorch/JAX. 3.3. Phase 2: Codebook update In the second phase, we find the optimal codebook vec- tors C1, ..., CM that minimize the same squared error as the beam search. If we treat the codes b as constants, min- imizing (3) becomes a least squares problem for Cm. The original AQ algorithm solves this problem in closed form, relying on the fact that each vector dimension can be opti- mized independently. Our problem is complicated due to the presence of XXT : the optimal value of one codebook coor- dinate depends on the values of all others. In principle, we could optimize Cm in closed form, but it would require in- verting a large matrix, or using iterative least squares solvers (e.g. conjugate gradients) specialized to this problem. For simplicity, our current implementation defaults to using Adam (Kingma & Ba, 2015) for approximately solving this minimization problem. In practice, this codebook tuning phase takes up a small fraction of the total compute time. We compute the objective as follows: ||WX − cWX||2 2 = ||(W − cW)X||2 2 = = D (W − cW)XXT , (W − cW) E F , (8) where cW is the quantized weight matrix from 2, and the XXT matrix is pre-computed. We optimize this objective by iterating (non-stochastic) full-batch gradient descent. For each update phase, our implementation runs 100 Adam steps with learning rate 10−4. However, we found that the fi- nal result is not sensitive to either of these parameters: train- ing with smaller number of steps or learning rate achieves the same loss, but takes longer to converge. In future work, these hyperparameters could be eliminated by switching to dedicated least squares solver for codebooks. Similarly to other algorithms, we also learn per-unit scales s ∈ Rdout that are initialized as si := ||Wi||2 and updated alongside codebooks via the same optimizer (line 10 in Algorithm 1). 3.4. Phase 3: Fine-tuning for intra-layer cohesion So far, our algorithm compresses each weight matrix inde- pendently of the rest of the model. However, in practice, quantization errors interact differently between matrices. This issue is especially relevant in the case of extreme (2- bit) compression, where quantization errors are larger. 5Extreme LLM Compression via Additive Quantization m m Σ i j m i j i j i j Codes Codes Codebooks Expanded Codes Unscaled Rows Weight Matrix Scales Codes Codebooks Weight Matrix Figure 3: AQLM compressed weight format. Horizontal and vertical axes are input features and output units, respectively. Depth represents the codebook index. Reconstruction procedure, from left to right: i) compressed weight codes ii) zoom-in one weight group, each code is an index in its respective codebook iii) select codes from each codebook iv) add up codes as in (2) v) multiply by scales (one scale per output dimension). Algorithm 1 AQLM: Additive Quantization for LLMs Require: model, data 1: Xblock := model.input_embeddings(data) 2: for i = 1, . . . ,model.num_layers do 3: block := model.get_block(i) 4: Yblock := block(Xblock) 5: for layer ∈ linear_layers(block) do 6: W := layer.weight 7: X := layer_inputs(layer, Xblock) 8: C, b, s:= initialize(W) // k-means 9: while loss improves by at least τ do 10: C, s:= train_Cs_adam(XXT , W, C, b, s) 11: b := beam_search(XXT , W, C, b, s) 12: end while 13: /* save for fine-tuning */ 14: layer.weight := AQLMFormat(C, b, s) 15: end for 16: θ := trainable_parameters(block) 17: while loss improves by at least τ do 18: L := ||block(Xblock) − Yblock||2 2 19: θ := adam(θ, ∂L ∂θ ) 20: end while 21: Xblock := block(Xblock) 22: end for Prior work addresses this issue via quantization-aware train- ing (QAT), e.g. (Gholami et al., 2021). Instead of compress- ing the entire model in a single pass, they quantize model parameters gradually and train the remaining parameters to compensate for the quantization error. Unfortunately, run- ning QAT in our setting is infeasible, since most modern LLMs are extremely expensive to train or even fine-tune. Thus, most PTQ algorithms for LLMs only adjust model pa- rameters within the same linear layer (Frantar et al., 2022a; Lin et al., 2023; Dettmers et al., 2023b). Here, we opt for a middle ground by performing optimiza- tion at the level of individual transformer blocks, i.e. groups of 4-8 linear layers3 that constitute a single multi-head self- attention, followed by a single MLP layer. Having quantized all linear layers within a single transformer block, we fine- tune its remaining parameters to better approximate the orig- inal outputs of that transformer block by backpropagating through the weight representation (2). 3This number depends on factors including the use of gated GLU activations, group query attention and QKV weight merging. Concretely, we use the PyTorch autograd engine to differ- entiate the ||block(Xblock) − Yblock||2 , where Xblock are the inputs activations for that transformer block and Yblock are output activations of block(Xblock) recorded prior to quantization. We train the codebooks Cm, scale vectors s and all non-quantized parameters (RMSNorm scales and biases), while keeping the codes bi,j,m frozen. Similarly to Section 3.3, we train these parameters using Adam to minimize the MSE against the original block outputs (prior to quantization). This phase uses the same calibration data as for the individual layer quantization. The full procedure is summarized in Alg. 1. While fine-tuning blocks is more expensive than individual linear layers, it is still possible to quantize billion-parameter models on a single GPU in reasonable time. Also, since the algorithm only modifies a few trainable parameters, it uses little VRAM for optimizer states. This fine-tuning converges after a few iterations, as it starts from a good initial guess. In practice, fine-tuning transformer layers takes a minority (10-30% or less) of the total calibration time. 4. Experiments We evaluate the AQLM algorithm in typical scenarios for post-training quantization of modern LLMs. Our evalua- tion is focused on the LLAMA 2 model family since it is a popular backbone for fine-tuned models or general LLM ap- plications, e.g. (Dettmers et al., 2023a), and we also present results on Mistral-family models (Jiang et al., 2024). In Section 4.1, we evaluate the full AQ procedure for various LLAMA 2 models and quantization bit-widths; Section 4.3 presents an ablation analysis for individual AQ components and implementation details. 4.1. Compression quality for modern LLMs We report perplexity on WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2020) validation sets. We also measure zero-shot accuracy on WinoGrande (Sakaguchi et al., 2021), PiQA (Tata & Patel, 2003), HellaSwag (Zellers et al., 2019), ARC-easy and ARC-challenge (Clark et al., 2018) via the LM Eval Harness (Gao et al., 2021). We follow the eval- uation setup of GPTQ (Frantar et al., 2022a) and provide configurations for AQLM and baselines in Appendix C. 6Extreme LLM Compression via Additive Quantization Table 1: Evaluation of quantized LLAMA 2 models for 2-2.8 bits per parameter, with an extra section for higher bitwidth. We report perplexity on WikiText-2 (Merity et al., 2016) & C4 (Raffel et al., 2020) and accuracy for zero-shot tasks. The Average accuracy is the mean of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 7B – 16 5.12 6.63 67.25 78.45 56.69 69.32 40.02 62.35 AQLM 2.02 6.59 8.54 65.67 74.76 49.55 63.68 32.76 57.28 QuIP# 2.02 8.22 11.01 62.43 71.38 42.94 55.56 28.84 52.23 AQLM 2.29 6.29 8.11 65.67 74.92 50.88 66.50 34.90 58.57 13B – 16 4.57 6.05 69.61 78.73 59.72 73.27 45.56 65.38 AQLM 1.97 5.60 7.49 68.82 75.90 53.80 69.28 38.82 61.32 QuIP 2.00 13.48 16.16 52.80 62.02 35.80 45.24 23.46 43.86 QuIP# 2.01 6.06 8.07 63.38 74.76 51.58 64.06 33.96 57.55 AQLM 2.19 5.37 7.16 67.64 77.37 55.03 70.29 38.65 61.80 AQLM 2.53 5.13 6.82 69.77 76.99 56.15 70.33 39.16 62.48 AQLM 2.76 4.94 6.54 68.98 77.58 57.71 72.90 43.60 64.15 70B – 16 3.12 4.97 76.95 81.07 63.99 77.74 51.11 70.17 AQLM 2.07 3.94 5.72 75.93 80.43 61.79 77.68 47.93 68.75 QuIP 2.01 5.90 8.17 67.48 74.76 50.45 62.16 33.96 57.76 QuIP# 2.01 4.16 6.01 74.11 79.76 60.01 76.85 47.61 67.67 Table 2: Evaluation of quantized LLAMA 2 models for 3-3.1 bits per parameter, with the same metrics as in Table 1. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 7B – 16 5.12 6.63 67.25 78.45 56.69 69.32 40.02 62.35 AQLM 3.04 5.46 7.08 66.93 76.88 54.12 68.06 38.40 60.88 GPTQ 3.00 8.06 10.61 59.19 71.49 45.21 58.46 31.06 53.08 SpQR 2.98 6.20 8.20 63.54 74.81 51.85 67.42 37.71 59.07 13B – 16 4.57 6.05 69.61 78.73 59.72 73.27 45.56 65.38 AQLM 3.03 4.82 6.37 68.43 77.26 58.30 70.88 42.58 64.49 GPTQ 3.00 5.85 7.86 63.93 76.50 53.47 65.66 38.48 59.61 SpQR 2.98 5.28 7.06 67.48 77.20 56.34 69.78 39.16 61.99 QuIP 3.00 5.12 6.79 69.93 76.88 57.07 70.41 41.47 63.15 70B – 16 3.12 4.97 76.95 81.07 63.99 77.74 51.11 70.17 AQLM 3.01 3.36 5.17 77.19 81.28 63.23 77.61 50.00 69.86 GPTQ 3.00 4.40 6.26 71.82 78.40 60.00 72.73 44.11 65.41 SpQR 2.98 3.85 5.63 74.66 80.52 61.95 75.93 48.04 68.22 QuIP 3.01 3.87 5.67 74.59 79.98 60.73 73.19 46.33 66.96 We consider three main targets in terms of compression ranges: 2-2.8 bits, 3-3.1 bits, and 4-4.1 bits per model pa- rameter. In the results below average bits per parameter takes into account only quantized weights, we do not include parameters kept in floating precision similarly to the related work. The details on the model size estimate are provided in Appendix H. We compare AQ against GPTQ for 3&4 bits (Frantar et al., 2022a), SpQR for 3&4 bits (Dettmers et al., 2023b), QuIP in 2,3 & 4 bits (Chee et al., 2023) and QuIP# for 2&4 bits (Tseng et al., 2024). While GPTQ and SpQR technically support 2-bit quantization, they perform poorly in the 2-3 bit range. For QuIP, our adapted4 imple- 4The official QuIP (non-#) code does not support LLAMA 2. mentation shows acceptable performance for LLAMA 2 13B & 70B but performs poorly for the 7B model. We calibrate each algorithm using the subset of RedPajama dataset (Com- puter, 2023), with a sequence length of 4096. The exact bit-widths for each method are dictated by param- eters such as the number of codebooks and code width. We report results for the 2−2.8 and 3−3.1 bitwidth ranges in Tables 1 and 2, respectively. Additional results for4 − 4.1 bits are deferred to Appendix F.2. The results show that AQLM outperforms the previous best PTQ algorithms across all settings, often by wide margins, especially at high compression. This holds both in terms of PPL across standard validation sets (Wiki-Text2 and C4), 7Extreme LLM Compression via Additive Quantization Table 3: Evaluation of quantized Mixtral (Jiang et al., 2024) models for 2 bits. The table reports perplexity on WikiText- 2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy column is the mean of 5 zero-shot task accuracies. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 8x7B – 16 3.46 5.02 75.45 82.37 64.65 83.38 55.80 72.33 AQLM 1.98 4.61 5.75 73.64 79.27 57.91 78.96 48.63 67.68 QuIP# 2.01 4.75 5.89 71.11 79.05 58.23 77.57 45.73 66.34 and accuracy across zero-shot tasks. Specifically, we ob- serve the highest accuracy gains in the “extreme” 2-2.1 bits per parameter range, where the deviation from the uncom- pressed model becomes large for all methods. Mixtral quantization. Table 3 presents results on the Mixtral MoE, comparing against QuIP# at 2-bits. (See Appendix F.1 for full results.) AQLM outperforms QuIP# in this case as well. Although the margins are lower compared to LLAMA 2 models, they are still significant for “harder” tasks, such as Arc Challenge (+3 points). Pareto optimality of AQLM.The significant error improve- ments raise the question of choosing the “optimal” model variant to maximize accuracy within a certain memory bud- get. For this, we follow Dettmers & Zettlemoyer (2022): a quantized model is said to be Pareto-optimal if it maximizes accuracy at the same or lower total size (bytes). Despite rapid progress, prior art methods are not Pareto-optimal at 2-bits: for instance, the previous best 2-bit LLAMA 2 13B (QuIP#, Table 1) achieves Wiki2 PPL of 6.06, but one can get much lower 5.21 PPL by using a 7B model with 4-bit quantization, which is smaller (see Appendix Table 10). AQLM compression to strictly 2 bits for the same model is also below Pareto-optimality, as it is outperformed by 4-bit AQLM compression for LLAMA 2 7B (5.21 vs 5.59). To find the Pareto-optimal quantization bitwidth, we run experiments between 2-3 bits per parameter and report them in Table 1, below horizontal bars. Thus, the Pareto-optimal bitwidth for AQLM appears to be around 2.5 bits per param- eter (Table 1), at which point we are comparable to 5-bit AQLM for LLAMA 2 7B (Appendix Table 10). In turn, the 2.76-bit AQLM on 13B outperforms the uncompressed 7B model. As such, AQLM is the first algorithm to achieve Pareto-optimality at less than 3 bits per parameter. 4.2. End-to-end fine-tuning experiments Subsequent work in QuIP# (Tseng et al., 2024) improves upon our block-wise protocol (Section 3.4) by fine-tuning the entire model to mimimize KL divergence. Here, we an- alyze how this end-to-end fine-tuning translates to AQLM. We follow the setup from QuIP# (Tseng et al., 2024) and run end-to-end fine-tuning with default parameters (see Ap- pendix A). Table 4 reports our results for 2-bit quantization using AQLM and QuIP# with end-to-end fine-tuning. We report additional results in this setup in Tables 6, 13 and 15 in supplementary materials. To differentiate between two versions, we mark quantized models with end-to-end fine- tuning with ⋆. Overall, end-to-end fine-tuning improves both QuIP# and AQLM, reaching comparable accuracy for both methods. Additionally, we notice that the boost from end-to-end fine-tuning is more profound on 2-bit quantized models with diminishing returns for 3 bits and above. Fi- nally, we can see that 2.19-bit AQLM with end-to end fine- tuning on 13B is comparable with an uncompressed 7B model achieving Pareto optimality on zero-shot tasks. 4.3. Ablation analysis In Appendix E, we examine key design choices regarding initialization, alternating optimization, the impact of the fine- tuning, and sensitivity to hyperparameters. In brief, we first find that the residual K-means initialization is critical for fast algorithm convergence: when compared with random initialization, it needs significantly fewer training iterations. We also compare different hyperparameter configurations for the same bitwidth, varying the number of codebooks and group size. Second, to validate our calibration fine- tuning procedure, we compare it against 1) no fine-tuning, 2) fine-tuning only of non-linear layers (e.g. RMSNorm) but not of codebook parameters, and 3) fine-tuning only the codebooks (but not other layers). The results, presented in full in Appendix E, show that fine-tuning the codebook parameters has the highest impact on accuracy, by far, while fine-tuning the RMSNorm only has minor impact. This validates our choice of leveraging the calibration set for learned codebooks. Further, we observe that, increasing the number of sam- ple sequences in the range 128 to 4096 leads to a gradual PPL improvement, but with diminishing returns. This is true for both initial AQLM calibraton and fine-tuning. In this respect, AQLM benefits more from larger calibration sets (similarly to QuIP#), as opposed to direct methods like GPTQ which saturate accuracy at around 256 input sequences. Finally, we investigate various options for in- vesting a given bit budget, comparing e.g. longer codes (e.g. 1x15) vs multiple codebooks with shorter codes (e.g. 2x8). 8Extreme LLM Compression via Additive Quantization Table 4: Evaluation of quantized LLAMA 2 with end-to-end fine-tuning, with the same metrics as in Table 1. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 7B – 16 5.12 6.63 67.25 78.45 56.69 69.32 40.02 62.35 AQLM⋆ 2.02 6.14 8.09 65.67 76.01 51.83 63.43 34.39 58.27 QuIP#⋆ 2.02 6.19 8.16 64.96 75.41 51.91 64.96 35.15 58.48 AQLM⋆ 2.29 5.92 7.86 63.77 76.93 52.82 66.16 36.95 59.33 13B – 16 4.57 6.05 69.61 78.73 59.72 73.27 45.56 65.38 AQLM⋆ 1.97 5.33 7.19 68.67 76.82 56.31 69.99 40.36 62.43 QuIP#⋆ 2.01 5.35 7.20 67.64 77.26 56.04 69.02 39.85 61.96 AQLM⋆ 2.19 5.22 6.98 68.27 77.53 57.09 69.78 40.70 62.67 70B – 16 3.12 4.97 76.95 81.07 63.99 77.74 51.11 70.17 AQLM⋆ 2.07 3.83 5.62 74.35 80.90 62.17 74.58 48.98 68.20 QuIP#⋆ 2.01 3.91 5.71 74.66 79.54 62.52 77.06 47.61 68.28 4.4. Inference Speed Although our primary objective is to maximize accuracy for a given model size, AQLM can also be practical in terms of inference latency. To demonstrate this, we implemented efficient GPU and CPU kernels for a few hardware-friendly configurations of AQLM. The results can be found in Ta- ble 5. For GPU inference, we targeted quantized LLAMA 2 models with 16-bit codebooks, corresponding to 2.07 bits for LLAMA 2 70B, 2.19 bits for 13B, and 2.29 bits for 7B models (see Table 1, 4), as well as a 2x8-bit codebook model with perplexity 6.57 on Wiki2(see Table 12). For each model we benchmark the matrix-vector multiplication subroutine performance on a standard layer. The results show that AQLM can execute at speeds comparable to or better than FP16. End-to-end generative numbers with HuggingFace integration can be found in Appendix I: for instance, we can achieve ≈14 tokens/s on LLAMA 2 70B in this setting. We observe that multiple smaller codebooks allow efficient GPU cache utilization, leading to greater speedup, at the price of slightly lower accuracy. Table 5: Speed of the FP16 gate_proj layer matrix-vector multiplication in PyTorch, and relative AQLM speedups. Llama 2 7B 13B 70B 2 bit speedup over FP16 on Nvidia RTX 3090 GPU Original (float16) 129 µs 190 µs 578 µs AQLM (Table 1) x1.31 x1.20 x1.20 AQLM (2×8-bit) x1.57 x1.82 x3.05 2 bit speedup over FP32 on Intel i9 CPU, 8 cores Original (float32) 1.83 ms 3.12 ms 11.31 ms AQLM (2×8-bit) x2.75 x3.54 x3.69 AQLM (4×8-bit) x2.55 x3.02 x4.07 AQLM (8×8-bit) x2.29 x2.68 x4.03 Next, we explore how to leverage AQLM to accelerate CPU inference. As discussed in Section 2.2, additive quantization can compute dot products efficiently if the codebook size is small. One way to achieve it for AQLM is to replace each 16-bit codebook with a number of smaller 8-bit ones. This leads to higher quantization error, but still outperforms the baselines in terms of accuracy (see Appendix Table 9). The results in Table 5 show that this also allows for up to 4x faster inference relative to FP32 on CPU. 5. Conclusion and Future Work We presented AQLM, a new form of additive quantization (AQ) targeting LLM compression, which significantly im- proved the state-of-the-art results for LLM quantization in the regime of 2 and 3 bits per weight. In terms of limita- tions, AQLM is more computationally-expensive than direct post-training quantization methods, such as RTN or GPTQ, specifically because of the use of a more complex coding representation. Yet, despite the more sophisticated encoding and decoding, we have shown AQLM lends itself to effi- cient implementation on both CPU and GPU. Overall, we find it remarkable that, using AQLM, massive LLMs can be executed accurately and efficiently using little memory. While AQLM already achieves substantial improvements in low-bit quantization, there are several promising directions for further improvement that we did not explore in this work. One such direction is better fine-tuning strategies. In Sec- tion 4.2 we found that better fine-tuning algorithms (Tseng et al., 2024; Malinovskii et al., 2024) can significantly im- prove quantized model accuracy. We believe that AQLM can benefit from a more systematic exploration of fine-tuning algorithms in future work. Another promising direction is generalizing AQLM to other quantization scenarios. While our work is focused around LLM quantization, the underly- ing algorithm can potentially be adapted to other problems, e.g. quantizing computer vision models, compressing LLM attention caches for long sequences, and others. 9Extreme LLM Compression via Additive Quantization Acknowledgements Authors would like to thank Ruslan Svirschevski for his help in solving technical issues with AQLM and baselines. We also thank Tim Dettmers for helpful discussions on the structure of weights in modern LLMs and size-accuracy trade-offs. The authors would also like to thank Daniil Pavlov for his assistance with CPU benchmarking. The au- thors would also like to thank contributors and community from Github repository5 for helping to improve the code and the text of the paper. Finally, authors would like to thank the communities of ML enthusiasts known as LocalLLaMA 6 and Petals community on discord 7 for the crowd wisdom about running LLMs on consumer devices. Egiazarian Vage and Denis Kuznedelev and Andrei Panferov were supported by the grant for research centers in the field of AI provided by the Analytical Center for the Government of the Rus- sian Federation (ACRF) in accordance with the agreement on the provision of subsidies (identifier of the agreement 000000D730321P5Q0002) and the agreement with HSE University No. 70-2021-00139 Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Babenko, A. and Lempitsky, V . Additive quantization for extreme vector compression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 931–938, 2014. Besag, J. On the statistical analysis of dirty pictures. Jour- nal of the Royal Statistical Society Series B: Statistical Methodology, 48(3):259–279, 1986. Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for ana- lyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023. Blalock, D. and Guttag, J. Multiplying matrices without multiplying. In International Conference on Machine Learning, pp. 992–1004. PMLR, 2021. Burton, D., Shore, J., and Buck, J. A generalization of isolated word recognition using vector quantization. In 5https://github.com/Vahe1994/AQLM/ 6https://www.reddit.com/r/LocalLLaMA/ 7https://github.com/bigscience-workshop/ petals/ ICASSP ’83. IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 8, pp. 1021–1024, 1983. doi: 10 .1109/ICASSP.1983.1171915. Chee, J., Cai, Y ., Kuleshov, V ., and Sa, C. D. Quip: 2-bit quantization of large language models with guarantees, 2023. Chen, S., Wang, W., and Pan, S. J. Deep neural network quantization via layer-wise optimization using limited training data. Proceedings of the AAAI Conference on Artificial Intelligence , 33(01):3329–3336, Jul. 2019. doi: 10 .1609/aaai.v33i01.33013329. URL https://ojs.aaai.org/index.php/AAAI/ article/view/4206. Chen, Y ., Guan, T., and Wang, C. Approximate nearest neighbor search by residual vector quantization. Sensors, 10(12):11259–11273, 2010. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Computer, T. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/ RedPajama-Data. Dettmers, T. and Zettlemoyer, L. The case for 4-bit pre- cision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022. Dettmers, T., Lewis, M., Belkada, Y ., and Zettlemoyer, L. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023a. Dettmers, T., Svirschevski, R., Egiazarian, V ., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023b. Fernández-Marqués, J., AbouElhamayed, A. F., Lane, N. D., and Abdelfattah, M. S. Are we there yet? 10Extreme LLM Compression via Additive Quantization product quantization and its hardware accelera- tion. ArXiv, abs/2305.18334, 2023. URL https: //api.semanticscholar.org/CorpusID: 258967539. Frantar, E. and Alistarh, D. Qmoe: Practical sub-1-bit compression of trillion-parameter models. arXiv preprint arXiv:2310.16795, 2023. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre- trained transformers. arXiv preprint arXiv:2210.17323, 2022a. Frantar, E., Singh, S. P., and Alistarh, D. Optimal Brain Compression: A framework for accurate post- training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022b. Accepted to NeurIPS 2022, to appear. Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muen- nighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few- shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628. Ge, T., He, K., Ke, Q., and Sun, J. Optimized product quantization. IEEE transactions on pattern analysis and machine intelligence, 36(4):744–755, 2013. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021. Gray, R. Vector quantization. IEEE ASSP Magazine, 1(2): 4–29, 1984. doi: 10 .1109/MASSP.1984.1162229. Guo, R., Kumar, S., Choromanski, K., and Simcha, D. Quan- tization based fast inner product search. In Artificial intelligence and statistics, pp. 482–490. PMLR, 2016. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. CoRR, abs/2009.03300, 2020. URL https://arxiv.org/abs/2009.03300. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowl- edge in a neural network, 2015. Jegou, H., Douze, M., and Schmid, C. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2010. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Kingma, D. P. and Ba, J. Adam: A method for stochas- tic optimization. International Conference on Learning Representations (ICLR), 2015. Li, Z., Ni, B., Zhang, W., Yang, X., and Gao, W. Perfor- mance guaranteed network acceleration via high-order residual quantization, 2017. Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Malinovskii, V ., Mazur, D., Ilin, I., Kuznedelev, D., Burlachenko, K., Yi, K., Alistarh, D., and Richtarik, P. Pv-tuning: Beyond straight-through estimation for ex- treme llm compression. arXiv preprint arXiv:2405.14852, 2024. Martinez, J., Clement, J., Hoos, H. H., and Little, J. J. Revisiting additive quantization. In Computer Vision– ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 137–153. Springer, 2016. Martinez, J., Zakhmi, S., Hoos, H. H., and Little, J. J. Lsq++: Lower running time and higher recall in multi-codebook quantization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 491–506, 2018. McCarter, C. and Dronen, N. Look-ups are not (yet) all you need for deep learning inference. ArXiv, abs/2207.05808, 2022. URL https: //api.semanticscholar.org/CorpusID: 250491319. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020. Norouzi, M. and Fleet, D. J. Cartesian k-means. In Pro- ceedings of the IEEE Conference on computer Vision and Pattern Recognition, pp. 3017–3024, 2013. 11Extreme LLM Compression via Additive Quantization Ozan, E. C., Kiranyaz, S., and Gabbouj, M. Com- petitive quantization for approximate nearest neighbor search. IEEE Transactions on Knowledge and Data Engineering, 28(11):2884–2894, 2016. doi: 10 .1109/ TKDE.2016.2597834. Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y ., and Lee, D. nuQmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. InConference on Neural Information Processing Systems (NeurIPS). 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21 (140):1–67, 2020. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . Winogrande: an adversarial winograd schema chal- lenge at scale. Commun. ACM , 64(9):99–106, 2021. doi: 10 .1145/3474381. URL https://doi.org/ 10.1145/3474381. Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. Shazeer, N. Glu variants improve transformer, 2020. Tata, S. and Patel, J. M. PiQA: An algebra for querying pro- tein data sets. In International Conference on Scientific and Statistical Database Management, 2003. TII UAE. The Falcon family of large language models. https://huggingface.co/tiiuae/ falcon-40b, May 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Tseng, A., Chee, J., Sun, Q., Kuleshov, V ., and Sa, C. D. Quip#: Even better llm quantization with hadamard inco- herence and lattice codebooks, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022. Yao, Z., Aminabadi, R. Y ., Zhang, M., Wu, X., Li, C., and He, Y . Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022. Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi, Y . Hellaswag: Can a machine really finish your sentence? In Korhonen, A., Traum, D. R., and Màrquez, L. (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pp. 4791–4800. Association for Computational Linguistics, 2019. doi: 10 .18653/v1/p19-1472. URL https:// doi.org/10.18653/v1/p19-1472. Zhang, B. and Sennrich, R. Root mean square layer normalization. CoRR, abs/1910.07467, 2019. URL http://arxiv.org/abs/1910.07467. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Zhang, T., Du, C., and Wang, J. Composite quantization for approximate nearest neighbor search. In International Conference on Machine Learning, pp. 838–846. PMLR, 2014. Zhou, S.-C., Wang, Y .-Z., Wen, H., He, Q.-Y ., and Zou, Y .-H. Balanced quantization: An effective and efficient approach to quantized neural networks. Journal of Com- puter Science and Technology, 32(4):667–682, Jul 2017. ISSN 1860-4749. doi: 10 .1007/s11390-017-1750-y. URL https://doi.org/10.1007/s11390-017- 1750-y. 12Extreme LLM Compression via Additive Quantization A. End-to-end fine-tuning The block-wise finetuning procedure, introduced in 3.4, considerably improves performance of compressed models. However, block-wise finetuning optimizes the loss only at the level of a current transformer block and is agnostic of the actual task of interest. To minimize the target loss, one can run backpropagation through the whole model and directly optimize all trainable parameters to minimize a model-level objective function. This allows to search for globally optimal parameters, as opposed to sequentially selected ones, during block-wise finetuning. One can minimize the error between the quantized model and the floating-point model on some calibration set. The parameters being optimized (namely the codebooks, scales and the non-quantized parameters) typically constitute a small fraction of the total number of parameters in the original model. Therefore, the proposed distillation method resembles parameter-efficient finetuning (PEFT) in both optimization and memory footprint. To transfer the knowledge from the original model to the quantized one, we adopt Knowledge Distillation (Hinton et al., 2015) where the student model is taught to mimic the output of a teacher given the same input. We follow the setup from QuIP# (Tseng et al., 2024) that uses KL divergence between the outputs of teacher and student models: L = 1 N N−1X i=0 DKL(ps(xi), pt(xi)) (9) Above DKL is the Kullback–Leibler divergence and ps, pt are the student and teacher probabilities given input sequence xi. Despite its simplicity, this fine-tuning procedure often significantly improves performance of the compressed model. We fine-tune all models on 4−16M training tokens: 1−4k sequences of length 4k for LLAMA 2 models (Touvron et al., 2023) and 512 sequences of length 8k for Mixtral (Jiang et al., 2024). We fine-tune on the same data as during initial calibration (i.e. samples from RedPajama (Computer, 2023)) and use Adam (Kingma & Ba, 2015) optimizer with constant learning rate 10−5 without weight decay. Batch size is set to 8−16 sequences. A single epoch of fine-tuning turns out to be sufficient, and longer training leads to marginal improvements. B. Code reproducibility We share the code for our method in the GitHub repository https://github.com/Vahe1994/AQLM/tree/ AQLM_camera_ready. The hyperparameters for our experimental setup are discussed in Appendix C. C. Experimental Configurations Hardware. In all of our experiments, we used either Nvidia A100 or H100. The number of GPUs varied from 1 to 8. We used activation offloading to lower pick memory usage. To evaluate inference speed on GPU we used consumer-grade GPU Nvidia 3090 and for CPU setup we used Intel core i9 13900k. Calibration set. All methods were calibrated on a slice of RedPajama-v1 dataset (Computer, 2023) for both LLAMA and Mistral/Mixtral family models. We used the same context length as models were trained on, for LLAMA 2 4096 and for Mistral/Mixtral 8192. For LLAMA 2 experiments, we used 8M tokens as a calibration set for SpQR, GPTQ, and AQLM. Quip, however, was calibrated on 4M tokens due to OOM errors when trying to use more samples. Taking into account the fact that after 2M tokens improvement of methods results is fairly small we chose to report these numbers as is. For Quip#, we used LLAMA 2 and Mistral’s quantized models provided by authors in their GitHub repository. To the best of our knowledge, they used 6k samples for calibration with a context length of 4096/8192. For Mixtral we calibrated both our method and QUIP# on 8M tokens with context length 8192. Hyperparameters. For GPTQ for both 3 and 4 bits we used a standard set of parameters without grouping and with permutation order act_order. SpQR method was evaluated with base 2 and 3 bit-width with group size of 16 and 3 bits for zeros and scales. Outliers rate was chosen such that average bit will be close to 3 and 4 bits respectively. 13Extreme LLM Compression via Additive Quantization Table 6: Evaluation of quantized LLAMA 2 end-to-end fine-tuned models for 3-3.1 bits per parameter, with the same metrics as in Table 1. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 7B – 16 5.12 6.63 67.25 78.45 56.69 69.32 40.02 62.35 AQLM⋆ 3.04 5.38 7.01 65.35 77.31 55.49 66.79 38.48 60.68 QuIP#⋆ 3.04 5.41 7.04 66.85 77.31 55.32 68.43 38.99 61.38 13B – 16 4.57 6.05 69.61 78.73 59.72 73.27 45.56 65.38 AQLM⋆ 3.03 4.78 6.33 68.75 78.45 58.54 72.94 42.75 64.29 QuIP#⋆ 3.01 4.78 6.35 68.03 77.86 57.56 72.18 41.38 63.40 70B – 16 3.12 4.97 76.95 81.07 63.99 77.74 51.11 70.17 AQLM⋆ 3.01 3.36 5.17 75.30 80.69 63.48 77.99 50.26 69.54 QuIP#⋆ 3.00 3.35 5.15 76.40 81.45 63.53 77.53 50.77 69.94 Quip was adapted to work on the LLAMA family and was calibrated with 1024 samples and 4096 context length. Quip# For LLAMA 2 and Mistral models we used the officially published quantized models. For Mixtral we adapted the code to work with the model’s architecture and quantized it with the recommended set of parameters. For both AQLM and QuIP#, we don’t quantize gate linear layer in Mixtral, because it contains relatively small amount of paramters and have severe impact on performance. AQLM For to get 2, 3, 4 bits: we used 1 codebook size of 215 or 216, with groups of 8 for 2 bits. For 3 bits we used 2 codebooks size of 212 with groups of 8. Finally for 4 bits we used 2 codebooks size of 215 or 216 with groups of 8. Both for finetuning 3.4 and codebooks update 3.3 we used Adam optimizer (Kingma & Ba, 2015) with learning rate of 10−4, β1 = 0.90 and β2 = 0.95. We used early stopping both for the finetuning phase and for the codebook optimization phase, by stopping when the least square error not decreasing more than some threshold. In our experiments the threshold varies between 10−2 and 10−3. Hyperparameters for end-end fine-tuning discussed at the end of Appendix A. D. Quantization time AQLM quantization takes considerably longer to calibrate than simpler quantization methods such as RTN or GPTQ. This only impacts quantization time, not inference time. Quantizing a 7B model with default configuration takes about 1 day on a single A100 gpu. Similarly, quantizing a 70B model on a single GPU would take 10-14 days. However, the procedure can be parallelized across multiple GPU: 7B quantization takes 14h on 2 GPUs, and 70B quantization takes 3-4 days on 8 GPUs. Full model fine-tuning with default configuration for 7B model would take 3-6 hours on four A100 , for 13B 10-16 hours on four A100, and for 70B 1-2 days on 8 A100. Finally, the quantization time is dependent on the quantization configuration and its hyperparameters. Tweaking these parameters, e.g. by reducing the number of beams, can achieve notable speedups of 2-4x during quantization, but at the cost of lower model accuracy. E. Ablation analysis The AQLM algorithm makes several design choices that need to be validated separately: initialization, alternating optimiza- tion, the fine-tuning protocol, and the choice of hyperparameters. Here, we study how each of these components affect results. Initialization. As discussed in Section 3, we initialize AQLM with residual K-means to obtain a good initial guess for both codes and codebooks. That is, we run K-means for the weight matrix, then subtract the nearest cluster from each weight, and run K-means again M times. A simple baseline would be to initialize all codes uniformly at random. We compare the two initialization strategies for the problem of quantizing a single linear layer within LLAMA 2 70B model to 3 bits per parameter. We quantize groups of 8 consecutive weights using 2 codebooks, 12 bit each. Each codebook contains 212 learnable values. As we can see in Figure 4, AQLM with K-means initialization needs significantly fewer training iterations 14Extreme LLM Compression via Additive Quantization to achieve the desired loss. The difference is so drastic that we expect that running AQLM with a random initialization would require extremely high runtimes to accurately quantize the largest models. 0 250 500 750 1000 1250 1500 1750 2000 Steps 10 3 10 2 10 1 100 MSE Random K-Means Figure 4: MSE loss learning curves of AQLM trained on the self attention q_proj linear layer of 10-th block in the LLAMA 2 70B model. Fine-tuning. Next, we validate the fine-tuning procedure. We compare the full block fine-tuning (default) against three alternatives: i) no fine-tuning at all, ii) fine-tuning only non-linear layers (i.e. RMSNorm), but not the AQ parameters, and iii) fine-tuning only the AQ parameters, but not the non-linear layers. Table 7 summarizes our results: fine-tuning the entire model or only AQ parameters achieves competitive performance, while training only RMSNorm scales is comparable to not fine-tuning at all. We attribute these observations to the fact that over 99% of quantized layer parameters are contained in AQ codebooks Cm, whereas the remaining parameters are small 1-dimensional tensors. This validates the use of the AQ approach, as many competing algorithms do not have learnable per-layer codebooks. Notably, QuIP# uses a shared fixed lattice instead. We also note that, even without fine-tuning, AQLM is competitive to previous state-of-the-art results. Table 7: Ablation analysis of AQLM with different fine-tuning restrictions on Llama-2 7B model at 2.02 bit width. Name Wiki2 ↓ C4↓ w/o 8.18 10.59 RMSnorm 8.31 10.46 AQ params 6.92 8.85 Full 6.93 8.84 Number of samples. We verify our choice of calibration hyperparameters. Traditionally, most PTQ algorithms use several hundred calibration sequences (e.g. Frantar et al. (2022a) has 128). In our experiments, we evaluate both AQLM and baselines with additional calibration data. Our original motivation for that was to avoid potential overfitting when fine-tuning entire transformer blocks. To test this assumption, we run our algorithm with different calibration set sizes, varying from 128 to 4096 sequences. For each size, we report the average perplexity on WikiText-2 over 3 runs, along with standard deviations. The results in Table 8 demonstrate that increasing the number of samples leads to gradual reduction in perplexity with seemingly diminishing returns. Since the perplexity is still monotonically improving from 128 to 4096 samples, it is possible that larger sample sizes would yield further improvements. Number of codebooks vs groups. Finally, we conducted an additional set of experiments on LLAMA 2 7B models to see perplexity dependence on simultaneous change on WikiText-2 of both codebooks and groups keeping compression rate fixed to 2bits. We present both AQLM with and without end-to-end fine-tuning in Table 9. 15Extreme LLM Compression via Additive Quantization Table 10: Evaluation of quantized LLAMA 2 models for 4+ bits per parameter. The table reports perplexity on WikiText- 2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy column is the mean of 5 zero-shot task accuracies. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 7B – 16 5.12 6.63 67.25 78.45 56.69 69.32 40.02 62.35 AQLM 4.04 5.21 6.75 67.32 78.24 55.99 70.16 41.04 62.55 GPTQ 4.00 5.49 7.20 68.19 76.61 55.44 66.20 36.77 60.64 SpQR 3.98 5.28 6.87 66.93 78.35 56.10 69.11 39.68 62.17 QuIP# 4.02 5.29 6.86 66.85 77.91 55.78 68.06 39.68 61.66 AQLM 5.02 5.16 6.68 67.40 78.29 56.53 68.94 39.93 62.22 13B – 16 4.57 6.05 69.61 78.73 59.72 73.27 45.56 65.38 AQLM 3.94 4.65 6.14 69.85 78.35 59.27 73.32 44.80 65.12 GPTQ 4 4.78 6.34 70.01 77.75 58.67 70.45 42.49 63.87 SpQR 3.98 4.69 6.20 69.69 78.45 59.25 71.21 44.52 64.42 QuIP 4.00 4.76 6.29 69.69 79.00 58.91 73.27 44.88 65.15 QuIP# 4.01 4.68 6.20 69.38 77.91 58.86 73.74 44.63 64.90 70B – 16 3.12 4.97 76.95 81.07 63.99 77.74 51.11 70.17 AQLM 4.14 3.19 5.03 76.48 81.50 63.69 77.31 50.68 69.93 GPTQ 4.00 3.35 5.15 75.61 81.23 63.47 76.81 49.15 69.25 SpQR 3.97 3.25 5.07 76.01 81.28 63.71 77.36 49.15 69.50 QuIP 4.00 3.58 5.38 76.01 80.25 61.97 74.28 47.01 67.90 QuIP# 4.01 3.22 5.05 76.80 81.45 63.51 78.37 50.85 70.20 AQLM 3.82 3.21 5.03 76.32 80.90 63.69 77.61 50.34 69.77 Table 8: WikiText-2 PPL as a function of calibration set size for Llama 2 (7B) quantized to 2.3 bits with AQLM, averaged over 3 runs. SD stands for adjusted standard deviation. # of samples Average PPL SD 128 6.994 0.127 256 6.584 0.031 512 6.455 0.005 1024 6.353 0.008 2048 6.297 0.018 4096 6.267 0.005 Table 9: WikiText-2 PPL as a function of from groups and number of codebook for Llama 2 (7B) quantized with approximately 2 bits quantization. Method Setup Average PPL AQLM 2x8gs8 7.6107 4x8gs16 8.1394 8x8gs32 7.3755 15x8gs64 7.8459 AQLM⋆ 2x8gs8 6.5746 8x8gs32 6.6126 15x8gs64 6.6602 F. Additional experiments In this section we report additional experimental results for Mixtral(Jiang et al., 2024), Mistral7B(Jiang et al., 2023) and LLAMA 2 model. F.1. Mixtral We report the results for Mixtral(Jiang et al., 2024) MoE-type model for 3 and 4 bits in Table 11. In the 4 bit case, performance of QuIP# and AQLM are very similar across all metrics and close to uncompressed FP16 model. 16Extreme LLM Compression via Additive Quantization Table 11: Evaluation of quantized Mixtral (Jiang et al., 2024) models for 3 and 4 bits per parameter. The table reports perplexity on WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy column is the mean of 5 zero-shot task accuracies. The primary metrics are Wiki2 (PPL, lower is better), C4 (PPL, lower is better) and Average accuracy (percentage, higher is better). Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 3-bit – 16.00 3.46 5.02 75.45 82.37 64.65 83.38 55.80 72.33 AQLM 3.02 3.79 5.17 75.45 81.61 63.25 81.90 53.92 71.23 4-bit – 16.00 3.46 5.02 75.45 82.37 64.65 83.38 55.80 72.33 AQLM 3.915 3.57 5.07 74.82 81.99 64.23 83.12 54.61 71.75 QuIP# 4.000 3.60 5.08 76.56 81.99 63.92 82.62 54.78 71.97 Table 12: Evaluation of quantized LLAMA 2 for 2x8groupsize8 codebooks models. We report perplexity on WikiText- 2 (Merity et al., 2016) & C4 (Raffel et al., 2020) and accuracy for zero-shot tasks. The Average accuracy is the mean of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 7B – 16 5.12 6.63 67.25 78.40 56.67 69.36 39.51 62.24 AQLM 2 7.61 9.68 62.27 71.87 46.41 61.03 30.03 54.32 AQLM⋆ 2 6.57 8.60 63.22 74.54 50.08 61.28 31.83 56.19 13B – 16 4.57 6.05 69.61 78.73 59.72 73.27 45.56 65.38 AQLM 2 6.54 8.77 55.96 71.06 48.29 62.50 31.40 53.84 AQLM⋆ 2 5.63 7.55 6385 77.04 54.19 67.85 37.20 60.03 70B – 16 3.12 4.97 76.95 81.07 63.99 77.74 51.11 70.17 AQLM⋆ 2 4.21 5.99 73.48 79.54 61.29 74.49 46.84 67.13 Table 13: Evaluation of quantized Mistral7B (Jiang et al., 2023) models for 2, 3 and 4 bits per parameter: perplexity on WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy column is the mean of 5 zero-shot task accuracies. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy. Size Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑ 2-bit – 16.00 4.77 5.71 73.64 80.47 61.15 78.87 49.23 68.67 AQLM 2.01 6.32 6.93 68.75 76.01 52.13 73.65 40.44 62.17 QuIP# 2.01 6.02 6.84 69.30 76.71 52.95 72.14 39.76 62.20 AQLM⋆ 2.01 5.76 6.60 68.67 77.64 56.44 73.32 42.66 63.75 3-bit – 16.00 4.77 5.71 73.64 80.47 61.15 78.87 49.23 68.67 AQLM 3.04 5.02 5.93 73.24 79.22 59.31 78.28 46.76 67.36 AQLM⋆ 3.04 5.12 6.09 72.85 79.05 59.92 77.57 48.12 67.50 4-bit – 16.00 4.77 5.71 73.64 80.47 61.15 78.87 49.23 68.67 AQLM 4.02 4.89 5.81 73.80 79.71 60.27 77.86 48.21 67.97 QuIP# 4.01 4.85 5.79 73.95 80.41 60.62 78.96 49.40 68.67 F.2. LLAMA 2 We show results for 4 bit quantization of the LLAMA 2 models in Table 10. We can see that AQLM outperforms other methods in terms of perplexity and has the best or close to the best results. We also report results of perplexity for our quantized 2x8 codebooks models in Table 12. 17Extreme LLM Compression via Additive Quantization 4 8 16 32 Model size (GiB) 3 4 5 6 7 8Perplexity on WikiText2 70B 13B 7B AQLM QuIP# Figure 5: Comparison of AQLM relative to QuIP# on LLAMA 2 7B, 13B, and 70B models. 4 8 16 32 Model size (GiB) 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5Perplexity on WikiText2 70B 13B 7B Llama-2 7B Llama-2 13B Llama-2 70B Figure 6: Model optimality for AQLM on LLAMA 2 7, 13, and 70B models. F.3. Mistral Finally, we evaluate AQLM and QuIP# quantization on Mistral 7b (Jiang et al., 2023) model for 3 and 4 bits in Table 13. In 2 bits, QuIP# slightly outperform AQLM on most benchmarks. And for 4 bits setup results are very close across the board. G. Pareto optimality We visualize WikiText-2 perplexity of Llama-2 7B, 13B, 70B models quantized with AQLM and QuIP# as plotted against quantized weight size in bytes and report it in Figure 5. Our method outperforms QuIP# in terms of perplexity in WikiText-2 across all model sizes. Additionally, in Figure 6, we show perplexity on WikiText-2 for AQLM method against size of quantized parameters. We can notice that starting around 3.7 GiB of quantized weights, which correspond to 2.5 bits compression on LLAMA 2 13B model, it is more advantageous to compress 13B model rather 7B model at the same model size in bytes. H. Estimating model size In this section, we describe how to estimate the size of the quantized model for a given codebook configuration. The total cost of storing quantized weight comprises the codebooks, codes and per-unit scales. Specifically for a weight with input dimension din, output dimension dout, group size g, M codebooks corresponding to B-bit codes, the total amount of memory required is (assuming that codebooks and scales are stored in half precision): • codebooks: g · M · 2B · 16 • codes: dout · (din/g) · B • scales: dout · 16 Therefore, the average bits per parameter can be computed as follows: ¯b = size in bits number of parameters = 16 g M2B + dout (din/g) B M+ 16 dout doutdin (10) For example, for mlp.gate_proj layer of LLAMA 2 70B model with din = 8192, dout = 28672, quantization with group size 8, two 8-bit codebooks the formula above yields 2.002 bits per parameter. Typically, storage cost is dominated by the codes, whereas codebooks and scales induce small memory overhead. 18Extreme LLM Compression via Additive Quantization 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of codes 0.0 0.2 0.4 0.6 0.8 1.0 Cumulative probability 0.06  0.04  0.02  0.00 0.02 0.04 0.06 PCA dim 1 0.06 0.04 0.02 0.00 0.02 0.04 0.06 PCA dim 2 Figure 7: Visualization of learned codes and codebooks in layers.5.self_attn.q_proj linear projection. (Left) Codes distribution. (Right) Two leading principal components of codebook. I. End-to-End Inference Speed Table 14: Text generation speed benchmark. Llama 2 7B 13B 70B Inference on Nvidia RTX 3090 GPU, tok/s Original (float16) 54.2 29.5 5.8 AQLM (1×16-bit) 65.3 34.1 6.7 AQLM (2×8-bit) 114.1 68.1 14.3 Inference on Intel i9 CPU, 8 cores, tok/s Original (float32) 3.106 1.596 0.297 AQLM (2×8-bit) 6.961 4.180 0.966 AQLM (4×8-bit) 6.837 4.004 0.948 AQLM (8×8-bit) 5.319 3.193 0.775 For quantized LLAMA 2 models, setup described in Sec- tion 4.4, we measure the time it takes to generate 128 tokens from scratch, performed on compiled computa- tional graphs, with batch size 1, and report the average number of generated tokens per second on a single 24GB RTX 3090 GPU, as well as Intel i9 CPU, in Table 14. Perplexity on WikiText-2 on these configurations pre- sented at the Table 9 J. Codebook and codes distribution The proposed AQLM quantization method allows for large freedom in the choice of quantization lattice and ability to represent different weight distribution. To understand how do the learned codes and codebooks look like, we visualize the distribution of codes (how frequently given codebook vector is chosen) and the learned codebooks. Below on Figure 7 we provide a cumulative probability plot of leaned codes and two leading principal codebook components for a specific layer. One can observe that codes distribution is close to uniform. Its entropy equals 15.91 bits per code, which is close to the maximum possible entropy of 16 bits (for a 16-bit codebook) for the uniform distribution. Codebook vectors are concentrated in some ball. This pattern is pertinent to all linear projections inside transformer blocks. K. Evaluation on MMLU and GSM8k While measurement of perplexity on WikiText-2 and C4 together with zero-shot accuracy on subset of simple 0-shot tasks from LM Eval Harness (Gao et al., 2021) is an established benchmark for evaluation of performance of compressed models, it may be not exhaustive enough for many real-world cases. While the complete and exhaustive evaluation of LLM abilities is still an open question, we evaluate our AQLM models and QuIP# on MMLU (Hendrycks et al., 2020) benchmark that involves problems from 57 different domains, such as humanities, social sciences, physics, e.t.c, and GSM8k (Cobbe et al., 2021) to assess the performance of quantized models on more complex and challenging tasks, requiring reasoning to get the correct answer. Below we consider AQLM and QuIP# after end-to-end finetuning, i.e. the best performing quantized models. We observed that relative decrease on performance on these tasks is higher compared to the standard evaluation. Fine-tuned AQML and QuIP# yield very similar performance on these benchmarks. 19Extreme LLM Compression via Additive Quantization Table 15: Evaluation of quantized LLAMA 2 models for 2-2.1 bits per parameter on MMLU and GSM8k. ⋆ corresponds to end-to-end finetuning Size Method Avg bits MMLU (5-shot) GSM8k (8-shot) 7B – 16 45.9 14.6 QuIP#⋆ 2.02 36.8 6.2 AQLM⋆ 2.02 38.5 5.3 13B – 16 55.2 24.3 QuIP#⋆ 2.01 50.0 14.0 AQLM⋆ 1.97 48.8 13.8 70B – 16 68.8 56.3 QuIP#⋆ 2.01 65.3 46.4 AQLM⋆ 2.07 65.3 47.9 L. Block-wise tuning for scalar quantization The block-wise procedure introduced in our work is quite general and can be applied to scalar quantization as well. Specifically, operations with quantized weights are differentiable with respect to quantization scales kept in original precision. Therefore, scales can be tuned in the same way as AQLM codebooks. We observed that tuning significantly improves the quality of GPTQ at low bit widths. However, the resulting quality is still far below AQLM at similar bit-widths. Table 16: Evaluation of AQLM and GPTQ quantization after block tuning for LLAMA 2 models with 2-2.1 bits per parameter. Size Method Avg bits Wiki2↓ C4↓ 7B – 16 5.12 6.63 GPTQ 2.14 16.77 17.53 AQLM 2.02 6.64 8.56 20",
      "meta_data": {
        "arxiv_id": "2401.06118v4",
        "authors": [
          "Vage Egiazarian",
          "Andrei Panferov",
          "Denis Kuznedelev",
          "Elias Frantar",
          "Artem Babenko",
          "Dan Alistarh"
        ],
        "published_date": "2024-01-11T18:54:44Z",
        "pdf_url": "https://arxiv.org/pdf/2401.06118v4.pdf"
      }
    },
    {
      "title": "Redistribution of Weights and Activations for AdderNet Quantization",
      "abstract": "Adder Neural Network (AdderNet) provides a new way for developing\nenergy-efficient neural networks by replacing the expensive multiplications in\nconvolution with cheaper additions (i.e.l1-norm). To achieve higher hardware\nefficiency, it is necessary to further study the low-bit quantization of\nAdderNet. Due to the limitation that the commutative law in multiplication does\nnot hold in l1-norm, the well-established quantization methods on convolutional\nnetworks cannot be applied on AdderNets. Thus, the existing AdderNet\nquantization techniques propose to use only one shared scale to quantize both\nthe weights and activations simultaneously. Admittedly, such an approach can\nkeep the commutative law in the l1-norm quantization process, while the\naccuracy drop after low-bit quantization cannot be ignored. To this end, we\nfirst thoroughly analyze the difference on distributions of weights and\nactivations in AdderNet and then propose a new quantization algorithm by\nredistributing the weights and the activations. Specifically, the pre-trained\nfull-precision weights in different kernels are clustered into different\ngroups, then the intra-group sharing and inter-group independent scales can be\nadopted. To further compensate the accuracy drop caused by the distribution\ndifference, we then develop a lossless range clamp scheme for weights and a\nsimple yet effective outliers clamp strategy for activations. Thus, the\nfunctionality of full-precision weights and the representation ability of\nfull-precision activations can be fully preserved. The effectiveness of the\nproposed quantization method for AdderNet is well verified on several\nbenchmarks, e.g., our 4-bit post-training quantized adder ResNet-18 achieves an\n66.5% top-1 accuracy on the ImageNet with comparable energy efficiency, which\nis about 8.5% higher than that of the previous AdderNet quantization methods.",
      "full_text": "Redistribution of Weights and Activations for AdderNet Quantization Ying Nie1 Kai Han1,2 Haikang Diao3 Chuanjian Liu1 Enhua Wu2,4 Yunhe Wang1∗ 1Huawei Noah’s Ark Lab 2State Key Lab of Computer Science, ISCAS & UCAS 3School of Integrated Circuits, Peking University 4University of Macau {ying.nie, kai.han, yunhe.wang}@huawei.com Abstract Adder Neural Network (AdderNet) provides a new way for developing energy- efﬁcient neural networks by replacing the expensive multiplications in convolution with cheaper additions (i.e., ℓ1-norm). To achieve higher hardware efﬁciency, it is necessary to further study the low-bit quantization of AdderNet. Due to the limitation that the commutative law in multiplication does not hold in ℓ1-norm, the well-established quantization methods on convolutional networks cannot be applied on AdderNets. Thus, the existing AdderNet quantization techniques propose to use only one shared scale to quantize both the weights and activations simultane- ously. Admittedly, such an approach can keep the commutative law in the ℓ1-norm quantization process, while the accuracy drop after low-bit quantization cannot be ignored. To this end, we ﬁrst thoroughly analyze the difference on distributions of weights and activations in AdderNet and then propose a new quantization algo- rithm by redistributing the weights and the activations. Speciﬁcally, the pre-trained full-precision weights in different kernels are clustered into different groups, then the intra-group sharing and inter-group independent scales can be adopted. To further compensate the accuracy drop caused by the distribution difference, we then develop a lossless range clamp scheme for weights and a simple yet effective outliers clamp strategy for activations. Thus, the functionality of full-precision weights and the representation ability of full-precision activations can be fully preserved. The effectiveness of the proposed quantization method for AdderNet is well veriﬁed on several benchmarks, e.g. , our 4-bit post-training quantized adder ResNet-18 achieves an 66.5% top-1 accuracy on the ImageNet with comparable energy efﬁciency, which is about 8.5% higher than that of the previous AdderNet quantization methods. Code is available at https://gitee.com/mindspore/ models/tree/master/research/cv/AdderQuant. 1 Introduction Convolutional neural networks (CNNs) have achieved extraordinary performance on many vision tasks. However, the huge energy consumption of massive multiplications makes it difﬁcult to deploy CNNs on portable devices like mobile phones and embedded devices. As a result, substantial research efforts have been devoted to reducing the energy consumption of CNNs [12, 16, 8, 7, 32, 39, 29]. Beyond pioneering model compression approaches, Chen et al. [ 4] advocate the use of ℓ1-norm instead of cross-correlation for similarity measure between weights and activations in conventional convolution operation, thus produces a series of adder neural networks (AdderNets) without massive ∗Corresponding author 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2212.10200v1  [cs.CV]  20 Dec 2022×Sx X W3 W2 W1 ×Sx ×Sx Quant ×Sw ×Sw ×Sw De-Quant or  Quant ×Sx Sw1 Activations (INT4) Weights (INT4) ×Sx Sw2 ×Sx Sw3 De-Quant  Scales (FP) Activations (FP) Weights (FP) Vanilla Conv Activations (INT4) Weights (INT4) Scales (FP) Adder Conv :Eq. 1 :Eq. 2   (a) The input full-precision activations and weights (b) Quantization for vanilla conv (c) Quantization for adder conv -8 7 -7 7 -7 7 -8 7 -8 7 -7 7 -7 7 -8 7 -8 7 -7 7 -7 7 -8 7 W1 W2 W3 X W1 W2 W2 W1 W3 W3  Bits waste Over clamp X X ... Figure 1: Comparisons of quantization methods of vanilla convolution and adder convolution (symmetric 4-bit as an example). The independent scales adopted by vanilla convolution quantization can properly handle the challenge of large differences in weights and activations. One shared scale adopted in existing adder convolution quantization methods cannot handle this challenge. number of multiplications. Compared with multiplication, addition is more energy efﬁciency, e.g. full-precision (FP) multiplication takes about 4×energy as full-precision addition [15, 41, 34, 23]. AdderNets have shown impressive performance in large scale image classiﬁcation [ 4, 40], object detection [5], etc. which shed light into the design of low-power hardware for AI applications. The operation of low-bit addition has much lower energy consumption than the full-precision addition [15, 41, 34, 23]. To further improve the efﬁciency of AdderNet, low-bit quantization is a crucial option. Quantization algorithms have been studied in conventional convolution networks for a long time and achieved remarkable performance [20, 1, 19, 11, 28, 24, 18, 10, 38, 30]. As the law of commutation holds in the quantization process of multiplication, the scales of weights and activations for quantization can be independent of each other (Eq. 6). In practice, weights tend to be quantized with multiple scales based on the output channels, while activations are quantized with one scale based on the whole layer (Figure 1 (b)). However, the law of commutation does not hold in the ℓ1-norm quantization, which restricts the weights and activations in AdderNet to be quantized with shared scale (Eq. 7, 10). Therefore, existing AdderNet quantization techniques [36, 5] adopt only one shared scale to quantize both the weights and activations simultaneously. In practice, the shared scale is calculated based on the activations of the whole layer (Top of Figure 1 (c)) or the weights of the whole layer (Bottom of Figure 1 (c)). However, given a pre-trained full-precision AdderNet model, the ranges of weights usually vary widely between output channels, and the ranges of activations and weights also vary widely, both of which pose a huge challenge for quantization. If the range of activations is adopted to quantize weights, a large proportion of weights will be clamped, i.e., over clamp. If the range of weights is adopted to quantize activations, a large proportion of precious bits will be never used, i.e., bits waste. In both cases, a poor quantized accuracy will be incurred. In contrast, multiple independent scales adopted in the vanilla convolution can avoid this challenge. To enhance the accuracy of quantized low-bit AdderNets, we ﬁrst thoroughly analyze the problems of the one shared scale paradigm for quantization in existing AdderNet quantization methods. Then, we develop a novel quantization algorithm for AdderNet. Speciﬁcally, we ﬁrst propose a scheme of group-shared scales with a negligible increase in computational workload to address the challenge that the ranges of weights vary widely between output channels. The pre-trained full-precision weights are divided into multiple groups by a quantization-oriented clustering process. Since the weights within a group are similar, the intra-group sharing and inter-group independent scales are thus used in our method. To further improve the performance of quantized adder neural network, we introduce a lossless range clamp scheme for weights to further reduce the difference on distribution of activations and weights in AdderNet. In practice, for groups where the range of the weights exceeds the range of the activations, we clamp the weights to the range of activations and then incorporate 2the clamped values into the following biasterm. Next, for those activations that contain outliers, a simple yet effective outliers clamp strategy for activations is presented to eliminate the negative impact of outliers. The extensive experiments conducted on several benchmarks demonstrate the effectiveness of the proposed quantization method. 2 Preliminaries and Motivation In this section, we brieﬂy revisit the necessary fundamental of adder neural network and existing quantization process for conventional CNN and AdderNet, then we describe the motivation. 2.1 Preliminaries Adder Neural Network (AdderNet). Conventional convolution uses massive multiplications for computation, while adder convolution utilize addition to replace multiplication for reducing computa- tional cost. Given the input activations X ∈RH×W×cin and the weights W ∈Rd×d×cin×cout , the traditional convolutional operation is deﬁned as: Y(m,n,c ) =X⊗W = d∑ i=1 d∑ j=1 cin∑ k=1 X(m+ i,n + j,k) ×W(i,j,k,c ), (1) where H and W are the height and width of activations, cis the index of output channels, d×d denotes the kernel size, cin and cout are the number of input and output channels, respectively. To avoid massive ﬂoating number multiplications, Chenet al. [ 4] advocate the use ofℓ1-norm instead of cross-correlation for similarity measure between activations and weights in CNN: Y(m,n,c ) =X⊕W = − d∑ i=1 d∑ j=1 cin∑ k=1 |X(m+ i,n + j,k) −W(i,j,k,c )|, (2) where |·|is the absolute value function. Since Eq. 2 only contains addition operation, the energy costs of the adder neural network can be effectively reduced [15, 34, 41]. With modiﬁed back-propagation approach and adaptive learning rate strategy, AdderNet achieves satisfactory performance on image classiﬁcation [4, 40, 36], object detection [5], etc. Quantization for CNN. The common uniform quantization for CNN linearly maps full-precision real numbers into low-bit integer representations, which is preferred by hardware efﬁciency. We take the symmetric mode without zero point as an example to describe the procedure of quantization. Given a full-precision input value vand bit-width b, the quantized value ¯vcan be deﬁned as: ¯v= clamp(⌊v/s⌉,qn,qp), (3) where the scale s= 2 ∗max(|v|)/(2b −1), qn = −2b−1 and qp = 2b−1 −1. ⌊·⌉denotes the round function and clamp(z,r1,r2) indicates that value zis clamped between r1 and r2. Correspondingly, the de-quantized data can be computed by: ˆv= ¯v×s. (4) And the quantization loss can be deﬁned as: Lquant = |ˆv−v|. (5) Since the commutative law holds in multiplication, the multiplication operation of weights and activations in the conventional convolution can be converted as follows: X⊗W ≈ ˆX⊗ ˆW = (sx ¯X) ⊗(sw ¯W) = (sxsw) ×( ¯X⊗ ¯W), (6) where ⊗denotes the vanilla convolution operation in Eq. 1. In common settings [24, 19, 20, 1, 6], the weights adopt independent sw for each output channel, and all activations in a layer share one sx. From Eq. 6, the data format of vanilla convolution operation is changed from full-precision to low-bit integer, which can reduce the energy consumption effectively. Quantization for AdderNet. Due to the fact that the commutative law does not hold in adder convolution, the independent scales sw and sx cannot be adopted when quantifying weights and activations in adder convolution as we do in conventional convolution, that is: X⊕W ≈ ˆX⊕ ˆW = (sx ¯X) ⊕(sw ¯W) ̸= (swsx) ×( ¯X⊕ ¯W), (7) 3where ⊕denotes the adder convolution operation in Eq. 2. Therefore, the existing works on AdderNet quantization [36, 5] all adopt one shared scale sfor quantifying weights and activations. Thus, the quantized and de-quantized weights are deﬁned as: ¯W = clamp(⌊W/s⌉,qwn,qwp), ˆW = s¯W, (8) similarly, the quantized and de-quantized activations are deﬁned as: ¯X = clamp(⌊X/s⌉,qxn,qxp), ˆX = s¯X, (9) where s= 2 ∗max(|X|)/(2b −1) or s= 2 ∗max(|W|)/(2b −1). Therefore, the quantized adder convolution can be implemented: X⊕W = ˆX⊕ ˆW = (s¯X) ⊕(s¯W) = s×( ¯X⊕ ¯W). (10) 2.2 Motivation [0, 1) 75.3% [1, 2) 19.9% [2, 5] 4.8% [0, 10) 79.2% [10, 20) 15.3% [20, 40) 4.8% [40, 90] 0.7% (a) Activations (b) Weights Figure 2: The statistics of the ab- solute ranges of activations and weights in the pre-trained full- precision AdderNet. Empirically, the values of weights and activations tend to vary widely in pre-trained full-precision AdderNet. We take the layer1.1.conv2 in pre-trained adder ResNet-20 [14] as an ex- ample to visualize the statistics of the absolute ranges of activa- tions and weights in Figure 2. Obviously, the absolute ranges of weights vary widely between output channels, and the ab- solute ranges of activations and weights also vary widely, both of which pose a huge challenge for the quantization with one shared scale. After thorough analysis, we conclude that when weights and activations in AdderNet are quantized with one shared scale, no matter whether the scale is calculated based on weights or activations, a large quantization loss (Eq. 5) will be incurred, resulting in a poor accuracy, especially in the case of low bits. For example, the Top-1 accuracy loss of 5-bit quantized adder ResNet-18 on the ImageNet is 3.3%, while the accuracy loss of 4-bit quantized adder ResNet-18 increases to 9.9% [36]. Proposition 1. Denote rx and rw as the ranges of activations and weights in AdderNet, respectively. If rx is adopted to quantize weights, a large proportion of weights will be clamped. Ifrw is adopted to quantize activations, a large proportion of precious bits will be never used. In both cases, large quantization loss is incurred, resulting in a poor accuracy. Proof. For the ﬁrst case where rx is adopted to quantize weights, according to Eq. 8, the detailed quantization of weights can be formulated as: ¯W = clamp(⌊W(2b −1)/2rx⌉,−2b−1,2b−1 −1). (11) Without losing generality, suppose W = 50rx, then ⌊W(2b −1)/2rx⌉= 25(2b −1), if 25(2b −1) is clamped between −2b−1 and 2b−1, then the proportion of the clamped value is (25 ∗(2b −1) − 2b−1)/(25 ∗(2b −1)) = (49 ∗2b−1 −25)/(50 ∗2b−1 −25) ≈0.98. Besides, the quantization loss is |ˆW −W|≈| rx −50rx|= 49rx, which will result in a signiﬁcant accuracy degradation. For the second case where rw is adopted to quantize activations, according to Eq. 9, the detailed quantization of activations can be formulated as: ¯X = clamp(⌊X(2b −1)/2rw⌉,−2b−1,2b−1 −1). (12) Similarly, suppose X = 0.02rw, then ⌊X(2b −1)/2rw⌉= ⌊0.01(2b −1)⌉, if we clamp ⌊0.01(2b −1)⌉ between −2b−1 and 2b−1, the remaining 2b−1 −⌊0.01(2b −1)⌉≈ 0.98 ∗2b−1 bits will be never used, which is a huge waste for precious bits. 3 Approach In this section, we will describe the proposed quantization algorithm for AdderNet in detail. Firstly, we introduce the method of group-shared scales based on clustering. Then, the clamp for weights and activations are proposed, respectively. 4-1.4 1.2 1.1 -4.5 0.0 0.6 -0.4 0.8 1.6 0.5 1.2 -0.7 0.8 -0.7 0.6 -0.5 0.37 -1.0 X W3 W2 -1.2 0.5 -0.8 0.52 1.2 1.0 7.3 -5.1 1.3 W1 16.8 12.3 -15.4 11.3 21.0 -19.2 -10.0 18.4 16.2 -1.4 1.2 1.1 -1.6 0.0 0.6 -0.4 0.8 1.6 0.5 1.2 -0.7 0.8 -0.7 0.6 -0.5 0.37 -1.0 -1.2 0.5 -0.8 0.52 1.2 1.0 7.3 -5.1 1.3 1.6 1.6 -1.6 1.6 1.6 -1.6 -1.6 1.6 1.6 + 0 0 -126.2 Bias (a) Clustering-based Weights Grouping 3 7 -4 3 -5 4 -6 -7 3 -5 .. .. 6 7 7 7 -8 7 7 -8 -8 7 7 ×S1 ×S1 ×S2 De-Quant  + -8 7 6 -8 0 4 -2 5 7 -7 6 5 -8 0 3 -2 4 7 0 0 -126.2 (c) Range Clamp for Weights  X W3 W2 W1 X W3 W2 W1X  Bias Quant (b) Outliers Clamp for Acts  -1.4 1.2 1.1 -1.6 0.0 0.6 -0.4 0.8 1.6 0.5 1.2 -0.7 0.8 -0.7 0.6 -0.5 0.37 -1.0 X W3 W2 -1.2 0.5 -0.8 0.52 1.2 1.0 7.3 -5.1 1.3 W1 16.8 12.3 -15.4 11.3 21.0 -19.2 -10.0 18.4 16.2 ≈ =(d) Quantization Figure 3: An illustration of the proposed quantization method for AdderNet (symmetric 4-bit as an example). The pre-trained full-precision weights are clustered into different groups. Then the clamp scheme for weights and activations are explored respectively to make efﬁcient use of the precious bits and eliminate the negative impact of outliers. 3.1 Clustering-based Weights Grouping To address the challenge that the ranges of weights in pre-trained full-precision AdderNet vary widely between output channels, we propose a novel quantization method with group-shared scales as illustrated in Figure 3 (a). Formally, X⊕W =concat(o1,...,o g), where oj =sj ×(¯X⊕ ¯W[:,:,:,Ij]), (13) where j = 1,...,g is the index of groups, g is the number of pre-deﬁned groups. Ij indicates a set of indexes of weights belonging to the j-th group, and sj indicates the j-th scale. The set I is achieved by clustering the weights in the pre-trained full-precision model. Since the scale s= 2∗max(|v|)/(2b −1), which indicates that sand the maximum of |v|are positively correlated. Thus, we cluster vbased on the maximum of |v|, not all the elements of v. In practice, the absolute maximum in each output channels of weights are taken as the clustering feature. Formally, the generated feature vector of weights is denoted as [f1,...,f cout ]: fc = max(|W[:,:,:,c]|), (14) where c = 1,...,c out is the index of output channels. Next, the feature vector is divided into pre- deﬁned gclusters I ={I1,I2,..., Ig}. Any pair of points within each cluster should be as close to each other as possible: min I g∑ j=1 ∑ c∈Ij |fc −µj|2, (15) where µj is the mean of points in cluster Ij. The method like k-means [13] can be adopted for clustering weights with the objective function in Eq. 15. Finally, the intra-group sharing and inter- group independent scales can be adopted for quantization. In practice, for groups where the range of the weights is within the range of the activations, the difference between the range of the weights and the activations is usually small, so the scale calculated based on the range of weights can be adopted for quantifying both the weights and activations. Conversely, for groups where the range of weights exceeds the range of activations, the range of weights and activations tends to differ signiﬁcantly, making it difﬁcult to quantize both weights and activations with one shared scale, which will be addressed in the next subsection. Analysis on Complexities. Compared to quantifying weights and activations with only one scale, the proposed quantization scheme with group-shared scales can improve the performance of quantized AdderNet, but it will result in a small increase in Floating Point Operations (FLOPs). After thorough analysis, we conclude that the increased FLOPs is negligible. 5Before Clamp After Clamp Adder activations Adder weights Adder activations Adder weights Adder bias Figure 4: The distributions of activations and weights in pre-trained full-precision AdderNet. Proposition 2. Without losing generality, supposecout = cin = c, then the additional FLOPs ratio introduced by the group-shared scales is approximately equal to1/(6c+ 1), which is negligible. The detailed proof is provided in the supplementary material under the assumption that g = 4 , considering that the magnitude of cis generally in the tens or hundreds for common networks, thus the additional FLOPs ratio is very small. 3.2 Clamp for Weights and Activations To address another challenge that the ranges of activations and weights in pre-trained full-precision AdderNet vary widely, more speciﬁcally, the range of weights far exceeds the range of activations (Left of Figure 4), we present a lossless range clamp scheme for weights (Figure 3 (c)). Besides, a simple yet effective outliers clamp strategy for activations is also introduced to eliminate the negative impact of outliers (Figure 3 (b)). Range Clamp for Weights. A lossless range clamp scheme for weights is ﬁrst proposed. Formally, denote Wc as any one output channel weights, i.e., Wc = W[:,:,:,c],c ∈[1,...,c out], rx and rw as the ranges of activations and weights, respectively. In practice, we clamp the full-precision values of Wc to the range of [−rx,rx], then add the clamped values to the following bias b. Theorem 1. The proposed range clamp scheme for weights has no any effect on the accuracy, i.e., lossless. Formally, X⊕Wc = X⊕clamp(Wc,−rx,rx) +b, where b= − d∑ i=1 d∑ j=1 cin∑ k=1 max(|Wc[i,j,k ]|−rx,0). (16) Proof. X⊕Wc = − d∑ i=1 d∑ j=1 cin∑ k=1 |X(m+ i,n + j,k) −Wc(i,j,k )|≜ − d∑ i=1 d∑ j=1 cin∑ k=1 f(X,Wc(i,j,k )), (17) where f(X,Wc(i,j,k )) can be divided into three cases: • if Wc(i,j,k ) >rx , then f(X,Wc(i,j,k )) = (rx −X(m+ i,n + j,k)) + (Wc(i,j,k ) −rx). • if −rx ≤Wc(i,j,k ) ≤rx , then f(X,Wc(i,j,k )) =|X(m+ i,n + j,k) −Wc(i,j,k )|. • if Wc(i,j,k ) <−rx , then f(X,Wc(i,j,k )) = (X(m+i,n+j,k)+ rx)+( −Wc(i,j,k )−rx). The above three cases can be uniﬁed into: X⊕Wc = X⊕clamp(Wc,−rx,rx) − d∑ i=1 d∑ j=1 cin∑ k=1 max(|Wc[i,j,k ]|−rx,0). (18) After the lossless range clamp process for weights, rw is equal to rx. Thus, we can directly adopt rx to quantize the weights and activations. Outliers Clamp for Activations. As shown in the left of Figure 4, some activations clearly have outliers. To avoid the negative impact of these outliers during quantization, we propose a simple yet effective outliers clamp strategy for activations. Speciﬁcally, the absolute activations are denoted as 6X = {|x0|,..., |xn−1|}, and the sorted X in ascending order is denoted as ˜X, where nis the number of the values. We select the valuerx = ˜X[⌊α∗(n−1)⌉] as the range of activations for the calculation of scale, where α∈(0,1] is a hyper-parameter controlling the ratio of discarded activations. Similarly, there is another option, i.e., rx = ˜X[n−1] ∗α. However, this method can only reduce the negative impact of outliers, whereas ours can eliminate the negative impact of outliers. 4 Experiments In this section, we conduct extensive experiments. Thorough ablation studies are also provided. 4.1 Experiments on CIFAR Re-train FP Networks. Following the pioneering work of AdderNet [4], we ﬁrst conduct exper- iments on CIFAR-10 and CIFAR-100 [ 21]. The CIFAR-10/100 dataset consists of 60,000 color images in 10/100 classes with 32 ×32 size, including 50,000 training and 10,000 validation images. The training images are padded by 4 pixels and then randomly cropped and ﬂipped. We re-train the three relevant full-precision networks, including VGG-Small [3], ResNet-20 [14] and ResNet-32 [14], according to the experimental settings in AdderNet [4]. Speciﬁcally, the ﬁrst and last layers of the network are kept as conventional convolution, and the other layers are replaced with adder convolu- tion. We employ learning rate starting at 0.1 and decay the learning rate with cosine learning rate schedule. SGD with momentum of 0.9 is adopted as our optimization algorithm. The weight decay is set to 5 ×10−4. We train the models for 400 epochs with a batch size of 256. The ﬁnal results are provided in in the supplementary material, and are basically consistent with the results in [ 4]. The baseline results of CNN and binary neural network (BNN) [43] are cited from [4]. Low-bit Quantization. Following the general setting in quantization [31, 42, 22, 27, 25], we do not quantize the ﬁrst and the last layers. The proposed quantization method can be employed in either post-training quantization (PTQ) or quantization-aware training (QAT). In practice, when the bits is 4, we begin to perform QAT procedure. Unless otherwise speciﬁed, the number of groups we use is 4. The hyper-parameter αcontrolling the ratio of discarded outliers in activations is empirically set to 0.999 in CIFAR experiments. In the QAT procedure, we keep the same settings as the FP training, except that the learning rate is changed to 1 ×10−4, and only 50 epochs are used for saving the training time. The straight-through estimator [2] is adopted for avoiding the zero-gradient problem. Table 1: Quantization results of AdderNets on CIFAR-10 and CIFAR-100 datasets. Model Bits CIFAR-10 (%) CIFAR-100 (%) PTQ QAT PTQ QAT VGG-Small 8 93.42 (-0.02) - 73.56 (-0.04) - 6 93.48 (+0.04) - 73.62 (+0.02) - 5 93.41 (-0.03) - 73.50 (-0.10) - 4 93.20 (-0.24) 93.46 (+0.02) 73.15 (-0.45) 73.58 (-0.02) ResNet-20 8 91.40 (-0.02) - 67.58 (-0.01) - 6 91.32 (-0.10) - 67.63 (+0.04) - 5 91.36 (-0.06) - 67.46 (-0.13) - 4 90.72 (-0.70) 91.21 (-0.21) 65.76 (-1.83) 67.35 (-0.24) ResNet-32 8 92.76 (+0.04) - 70.46 (+0.29) - 6 92.62 (-0.10) - 70.08 (-0.09) - 5 92.59 (-0.13) - 70.09 (-0.08) - 4 91.73 (-0.99) 92.35 (-0.37) 68.24 (-1.93) 69.38 (-0.79) The quantization results of AdderNets are reported in Table 1. For all networks, there is almost no accuracy loss in 8-bit quantization compared to the full-precision counterparts. For example, the accuracy of 8-bit quantized ResNet-32 is even 0.29% higher. For the 6-bit and 5-bit quantization, the accuracy loss exists, but is still within an acceptable range. For the more challenging 4-bit quantization, the accuracy loss of PTQ increases signiﬁcantly. Therefore, QAT procedure is necessary in this case. After 50 epochs training, the accuracy loss was greatly reduced. For example, in the case of 4-bit ResNet-20 on CIFAR-100, the accuracy loss is reduced from 1.83% to 0.24%. If equipped with more reﬁned QAT techniques, such as EWGS [22], KD [27] or simply a longer training epoch, the accuracy loss can be further reduced, but it is not the focus of this paper. 74.2 Experiments on ImageNet Re-train FP Networks. We further conduct evaluation on ImageNet dataset [9], which contains over 1.2M training images and 50K validation images belonging to 1,000 classes. The pre-processing and data augmentation follow the same protocols as in [ 14]. We re-train two full-precision networks including ResNet-18 and ResNet-50 [14] according to the experimental settings in [ 4]. The entire training takes 150 epochs with a weight decay of 1 ×10−4 and the other hyper-parameters is the same as that in CIFAR experiments. The ﬁnal results are provided in the supplementary material and the baseline results of CNN and BNN are also cited from [4]. Low-bit Quantization. We adopt the same quantization settings discussed in CIFAR experiments except that the hyper-parameter αis set to 0.9992. Besides, in the QAT procedure, we also keep the same settings as the full-training training, except that the learning rate is changed to 1 ×10−3, and only 20 epochs are used for saving training time. Table 2: Quantization results of AdderNets on ImageNet. Model Bits Top-1 Acc (%) Top-5 Acc (%) PTQ QAT PTQ QAT ResNet-18 8 67.7 (-0.2) - 87.7 (-0.1) - 6 67.7 (-0.2) - 87.6 (-0.2) - 5 67.4 (-0.5) - 87.3 (-0.5) - 4 66.5 (-1.4) 67.4 (-0.5) 86.6 (-1.2) 87.4 (-0.4) ResNet-50 8 75.0 (-0.0) - 91.9 (-0.0) - 6 75.0 (-0.0) - 91.8 (-0.1) - 5 74.7 (-0.3) - 91.5 (-0.4) - 4 73.2 (-1.8) 73.7 (-1.3) 90.7 (-1.2) 90.9 (-1.0) The quantization results are reported in Table 8. For both networks, there is almost no loss in 8-bit and 6-bit quantization compared to the full-precision counterparts. For the 5-bit quantization, the accuracy loss is still within an acceptable range, i.e., a maximum loss of 0.5%. Similar to CIFAR experiments, QAT is adopted in 4-bit quantization. After 20 epochs training, the accuracy loss was greatly reduced. For example, the accuracy loss of 4-bit ResNet-18 is reduced from 1.4% to 0.5%. 4.3 Comparisons with State-of-the-arts Image Classiﬁcation. We compare the proposed method with the existing AdderNet quantization technique QSSF [36] on image classiﬁcation task. We ﬁrst compare the PTQ results on ImageNet with it. The detailed Top-1 accuracy loss comparison with full-precision Adder ResNet-18 are reported in Figure 5 (a). QSSF [36] simply adopt only one shared scale to quantize both the weights and activations simultaneously, which ignores the properties of the distribution of the weights and activations of AdderNet, leading to the problems of \"Over clamp\" and \"Bits waste\", further leading to a poor accuracy. In contrast, we propose a quantization method consisting of three parts: clustering- based weights grouping, lossless range clamp for weights and outliers clamp for activations. The problems of \"Over clamp\" and \"Bits waste\" can be effectively resolved with our quantization method, further leading to a high quantized accuracy. As the number of bits decreases, the advantages of our method become more pronounced. For example, in the case of 4-bit, our method achieves 8.5% higher accuracy than QSSF [36]. (a) Comparison of Adder ResNet-18 on ImageNet (b) Comparison of Adder ResNet-20 on CIFAR-100 Figure 5: Comparisons with QSSF [36] on image classiﬁcation task. 8In addition, we also compare the QAT results on CIFAR-100 with it. In Figure 5 (b), we visualize the training loss, training accuracy and testing accuracy of the 4-bit Adder ResNet-20. For the accuracy curve, the solid lines and dash lines represent the testing accuracy and training accuracy, respectively. Our quantization method achieves a higher accuracy on both training and testing. Speciﬁcally, under the same epoch, our method achieves 67.35% test accuracy, which is higher than 64.61% achieved by QSSF [36]. Object Detection. AdderDet [5] presents an empirical study of AdderNet on object detection task. Besides, the PTQ mAP with one shared scale of 8-bit Adder FCOS [35] is also reported in this work, i.e., 36.2 on COCO val2017 [26], which is 0.8 lower than the full-precision counterpart. To verify the generality of our quantization method on detection task, we conduct the PTQ experiment of the 8-bit Adder FCOS following the settings in AdderDet [5]. Speciﬁcally, the adder convolutions in the backbone and neck are quantized with 4-group shared scales, and the hyper-parameter αis set to 0.9992. The ﬁnal 8-bit quantized mAP with our method is 36.5, which is 0.3 higher. Figure 6: Comparisons of accuracy and en- ergy cost of ResNet-20 on CIFAR-100. Energy Cost. In Figure 6, we compare the accu- racy and energy cost of different network architectures after quantization, including AdderNet, CNN and Shif- tAddNet [41]. The experiments are based on ResNet-20 network and CIFAR-100 dataset. We follow the related works [41, 36, 23] to measure the practical energy cost of ResNet-20 with various bits and basic operations on a FPGA platform. Besides, the quantized results of CNN is achieved by the BRECQ [24] PTQ method. The quantized accuracy of ShiftAddNet is cited from the original paper [41] and the energy of ShiftAddNet is calculated from the relative ratio of the energy of ShiftAddNet and AdderNet in the original paper [41], i.e., -25.2%. From Figure 6, under similar energy cost, the quantized Adder ResNet-20 with our method achieves a higher accuracy. 4.4 Ablation Studies In this subsection, we conduct thorough ablation studies based on the 4-bit quantized Adder ResNet-20 network on CIFAR-100 dataset. Analysis on Sub-Methods. We ﬁrst conduct analysis on three sub-methods as reported in Table 10, including group-shared scales, lossless range clamp for weights and outliers clamp for activations. The complete method achieves the highest accuracy of 67.35%, which is higher than the initial accuracy of 57.88%. Besides, the introduction of any sub-method contributes to the improvement of accuracy. Table 3: Analysis on sub-methods. Method Acc (%)Group-Shared Scales Weights Clamp Activations Clamp 57.88 ✓ 60.42 ✓ 62.30 ✓ 61.55 ✓ ✓ 65.17 ✓ ✓ ✓ 67.35 Analysis on Group Number. The results with various group number are reported in Table 4, and the relative FLOPs are calculated under the assumption that the number of channels of input and output is 32. From Table 4, when the group number is 4, the quantized model achieves the highest accuracy with a negligible increase in FLOPs. More group does not lead to a higher accuracy. Analysis on Group Methods. We also conduct analysis on the group methods. The group number we used is 4, and the results are reported in Table 5. Uniform indicates that the weights are evenly grouped based on the output channel without clustering. Compared with other group methods, the clustering method based on max(|W|) achieves the highest accuracy. 9Table 4: Analysis on the group number. Group Relative FLOPs Acc (%) 1 1 65.91 2 1.002 66.38 4 1.005 67.35 8 1.012 67.11 Table 5: Analysis on the group methods. Group Method Acc (%) Uniform 65.72 All 66.61 Mean 66.86 Max 67.35 5 Conclusion To further improve the efﬁciency of AdderNet, this paper studies the quantization algorithm for AdderNet. Considering that the weights of different channels may differ greatly, and the ranges of weights and activations are different, we propose a scheme of group-shared scales and a clamp strategy for weights and activations to improve the performance of quantized AdderNet. The effectiveness of the proposed quantization method for AdderNet is well-veriﬁed on several benchmarks. We achieve state-of-the-art results on various quantized network architectures and datasets. We believe that the advantages of low-bit quantized AdderNet can shed light into the design of low-power hardware for AI applications. Acknowledgement We gratefully acknowledge the support of MindSpore, CANN(Compute Architecture for Neural Networks) and Ascend AI Processor used for this research. This research is supported by NSFC (62072449, 61972271); National Key R&D Program of China (2020YFC2004100); University of Macau Grant (MYRG2019-00006-FST). References [1] Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. Advances in Neural Information Processing Systems, 32, 2019. [2] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [3] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5918–5926, 2017. [4] Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. Addernet: Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1468–1477, 2020. [5] Xinghao Chen, Chang Xu, Minjing Dong, Chunjing Xu, and Yunhe Wang. An empirical study of adder neural networks for object detection. Advances in Neural Information Processing Systems, 34, 2021. [6] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. [7] Kanghyun Choi, Deokki Hong, Noseong Park, Youngsok Kim, and Jinho Lee. Qimera: Data-free quantization with synthetic boundary supporting samples. Advances in Neural Information Processing Systems, 34, 2021. [8] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123–3131, 2015. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [10] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq- v2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems, 33:18518–18529, 2020. [11] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. 10[12] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016. [13] John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal of the royal statistical society. series c (applied statistics), 28(1):100–108, 1979. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. [15] Mark Horowitz. 1.1 computing’s energy problem (and what we can do about it). In2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10–14. IEEE, 2014. [16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [17] Huawei. Mindspore. https://www.mindspore.cn/, 2020. [18] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. Advances in neural information processing systems, 29, 2016. [19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁcient integer- arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2704–2713, 2018. [20] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. [21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [22] Junghyup Lee, Dohyung Kim, and Bumsub Ham. Network quantization with element-wise gradient scaling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6448–6457, 2021. [23] Wenshuo Li, Hanting Chen, Mingqiang Huang, Xinghao Chen, Chunjing Xu, and Yunhe Wang. Winograd algorithm for addernet. In Proceedings of the International Conference on Machine Learning, pages 6307–6315, 2021. [24] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [25] Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu, Feiyue Huang, and Chia-Wen Lin. Rotated binary neural network. Advances in neural information processing systems, 33:7474–7485, 2020. [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. [27] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions. In European Conference on Computer Vision, pages 143–159. Springer, 2020. [28] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197–7206. PMLR, 2020. [29] Ying Nie, Kai Han, Zhenhua Liu, An Xiao, Yiping Deng, Chunjing Xu, and Yunhe Wang. Ghostsr: Learning ghost features for efﬁcient image super-resolution. arXiv preprint arXiv:2101.08525, 2021. [30] Ying Nie, Kai Han, and Yunhe Wang. Multi-bit adaptive distillation for binary neural networks. [31] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. In European Conference on Computer Vision, pages 525–542. Springer, 2016. [32] Gil Shomron, Freddy Gabbay, Samer Kurzum, and Uri Weiser. Post-training sparsity-aware quantization. Advances in Neural Information Processing Systems, 34, 2021. [33] Han Shu, Jiahao Wang, Hanting Chen, Lin Li, Yujiu Yang, and Yunhe Wang. Adder attention for vision transformer. Advances in Neural Information Processing Systems, 34:19899–19909, 2021. [34] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efﬁcient processing of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329, 2017. [35] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627–9636, 2019. 11[36] Yunhe Wang, Mingqiang Huang, Kai Han, Hanting Chen, Wei Zhang, Chunjing Xu, and Dacheng Tao. Addernet and its minimalist hardware design for energy-efﬁcient artiﬁcial intelligence. arXiv preprint arXiv:2101.10015, 2021. [37] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. arXiv preprint arXiv:2203.05740, 2022. [38] Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Learning frequency domain approximation for binary neural networks. Advances in Neural Information Processing Systems, 34:25553–25565, 2021. [39] Yixing Xu, Yunhe Wang, Hanting Chen, Kai Han, Chunjing Xu, Dacheng Tao, and Chang Xu. Positive- unlabeled compression on the cloud. Advances in Neural Information Processing Systems, 32, 2019. [40] Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, and Yunhe Wang. Kernel based progressive distillation for adder neural networks. In Proceedings of the Neural Information Processing Systems, 2020. [41] Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu, Zhangyang Wang, and Yingyan Lin. Shiftaddnet: A hardware-inspired deep network. Advances in Neural Information Processing Systems, 33:2771–2783, 2020. [42] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 365–382, 2018. [43] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 12Table 6: Full-precision results on CIFAR-10 and CIFAR-100 datasets. Model Method # Mul. # Add. # XNOR. CIFAR-10 (%) CIFAR-100 (%) VGG-Small CNN 0.65G 0.65G 0 93.80 72.73 BNN 0.05G 0.65G 0.60G 89.80 67.24 AddNN 0.05G 1.25G 0 93.44 73.60 ResNet-20 CNN 41.17M 41.17M 0 92.25 68.14 BNN 0.45M 41.17M 40.72M 84.87 54.14 AddNN 0.45M 81.89M 0 91.42 67.59 ResNet-32 CNN 69.12M 69.12M 0 93.29 69.74 BNN 0.45M 69.12M 68.67M 86.74 56.21 AddNN 0.45M 137.79M 0 92.72 70.17 A Appendix A.1 Proof of Proposition 2 Proof. For a weight W ∈Rd×d×cin×cout and an input activation X ∈Rh×w×cin , the FLOPs of ¯X⊕ ¯W is 2hw(cind2 + 1)cout. In addition, the FLOPs required to calculate ¯wand ¯xare cind2cout and ghwcin, respectively. Besides, the size of the output activation can be calculated by: hout = ⌊(h−d+ 2 ∗padding)/stride+ 1⌋,wout = ⌊(w−d+ 2 ∗padding)/stride+ 1⌋. (19) Without loss of generality, assume that the values ofpaddingand strideare both 1, therefore, the FLOPs required to de-quantize the output activation is houtwoutcout = (h−d+ 3)(w−d+ 3)cout. Finally, all FLOPs required for a layer quantization is: FLOPsall = 2hw(cind2 + 1)cout + cind2cout + ghwcin + (h−d+ 3)(w−d+ 3)cout. (20) Without loss of generality, assume that d= 3, cin = cout = c, and h= w= k, then FLOPsall = 18k2c2 + gk2c+ 9c2 + 3k2c. (21) Compared with only one scale (g = 1), FLOPs are increased by rwhen adopting multiple scales (g≥2): r= (g−1)k2c 18k2c2 + k2c+ 9c2 + 3k2c = (g−1)k2 (18k2 + 9)c+ 4k2 ≈ g−1 18c+ 4, (22) As we discussed in the section of experiments, the value of gthat we adopt is 4. In this case, Eq. 22 can be further simpliﬁed to r≈ g−1 18c+ 4 = 3 18c+ 4 ≈ 1 6c+ 1. (23) Considering that the magnitude of cis generally in the tens or hundreds of common neural networks, thus the value of r is very small. Therefore, the increase in FLOPs brought by the scheme of group-shared scales is negligible. A.2 Full-precision Results In the section of experiments, we re-trained multiple full-precision adder networks on various datasets. The full-precision results on CIFAR-10 and CIFAR-100 are reported in Table 6, and the full-precision results on ImageNet are reported in Table 7, both denoted by AddNN. The results of AddNN are basically consistent with the results in [ 4]. The baseline results of convolutional neural network (CNN) and binary neural network (BNN) are cited from [4]. A.3 Analysis on the Ratio of Discarded Outliers As we discussed in the subsection of outliers clamp for activations, the valuerx = ˜X[⌊α∗(n−1)⌉] is selected as the range of activations for the calculation of scale, where α∈(0,1] is a hyper-parameter controlling the ratio of discarded outliers in activations. We supplement the ablation study of this ratio with 4-bit quantized adder ResNet-20 network on CIFAR-100 dataset. As shown in Table 8 , α= 1 means that the scheme of outliers clamp for activations is not adopted, resulting in a signiﬁcantly degraded quantized accuracy. The quantized accuracy can be improved with an appropriate α. 13Table 7: Full-precision results on ImageNet. Model Method # Mul. # Add. # XNOR. Top-1 Acc (%) Top-5 Acc (%) ResNet-18 CNN 1.8G 1.8G 0 69.8 89.1 BNN 0.1G 1.8G 1.7G 51.2 73.2 AddNN 0.1G 3.5G 0 67.9 87.8 ResNet-50 CNN 3.9G 3.9G 0 76.2 92.9 BNN 0.1G 3.9G 3.8G 55.8 78.4 AddNN 0.1G 7.6G 0 75.0 91.9 Table 8: Analysis on the ratio of discarded outliers in activations. α 0.9985 0.9990 0.9995 1.0 Acc (%) 67.29 67.35 67.11 65.17 A.4 Quantization Results on Adder Vision Transformers We also try the proposed quantization method on adder vision transformers [ 33]. We re-train the full-precision adder DeiT-T for 400 epochs from scratch on ImageNet dataset following [33], and the ﬁnal top-1 accuracy of the full-precision adder DeiT-T is 68.3%. For the next quantization step, the number of groups we use is 4, the hyper-parameter αcontrolling the ratio of discarded outliers in activations is set to 0.9992. The accuracy drops after post-training quantization are reported in Table 9. The advantage of the proposed method over QSSFF [36] is signiﬁcant. For example, at the case of W4A4, the accuracy drop of our method is 8.7%, which is much lower than the 16.3% of QSSF [36]. Table 9: Accuracy drops under various bits. W8A8(%) W6A6 (%) W4A4 (%) Ours -0.5 -4.1 -8.7 QSSF [36] -1.7 -6.5 -16.3 A.5 Quantization Results on Lower-bit We supplement the 3-bit PTQ quantization experiment of adder ResNet-20 on CIFAR-100 dataset. Besides, the comparisons with more CNN quantization methods are also supplemented. The detailed accuracy drops are reported in Table 10. A.6 Distribution of the Weights and Activations In Figure 7, we visualize the histogram of the weights and activations in AdderNet. The input full- precision (FP) activations and weights in pre-trained AdderNet show a signiﬁcant difference, which pose a huge challenge for AdderNet quantization. Other AdderNet quantization methods [ 36, 5] fail to deal with this challenge, leading to the phenomenon of over clamp and bits waste, further resulting in a poor quantized accuracy. In contrast, our quantization method can effectively address this challenge by the redistribution of full-precision weights and activations, resulting in a good quantized accuracy. One-shared scale is adopted here for the simpliﬁcation of visualization, and symmetric 4-bit quantization is taken as an example. A.7 Limitations and Societal Impacts Our AdderNet quantization method has one major limitation: as the number of bits decreases, the accuracy loss of the quantization model will increase. Therefore, quantization-aware training is necessary for the low bits, which is time consuming and computationally consuming. As for the societal impacts, the proposed quantization method can further reduce the energy con- sumption of AdderNet with a lower quantized accuracy loss. The low power devices equipped with quantized AdderNet can be deployed to surveillance scenario. If used improperly, there may be a risk of information leakage. 14Table 10: Comparisons with more CNN quantization methods. W4A4 (%) W3A3 (%) AddNN -1.83 -6.02 CNN AdaRound [28] -2.01 -6.77 CNN BRECQ [24] -1.74 -5.95 CNN QDROP [37] -1.70 -5.86 -8                                            0                                        7 -8                                            0                                        7 -8                                            0                                        7-8                                            0                                        7-8                                            0                                        7 or -8                                            0                                        7 … FP Activations FP Weights FP Activations FP Weights  Other Adder-Quantization Methods : Over clamp : Bits waste or INT4 Activations  INT4 Activations  INT4 Weights  INT4 Weights  INT4 Weights  INT4 Weights INT4 Activations  INT4 Activations  … … … -8                                          0                                     7 -8                                         0                                    7 -8                                          0                                     7 -8                                         0                                    7 Our Adder-Quantization Method FP Activations  After Clamp FP Activations  After ClampFP Weights  After Clamp FP Weights  After Clamp INT4 Activations INT4 Activations INT4 Weights INT4 Weights  Input FP Activations & Weights  Figure 7: Distribution of the weights and activations in AdderNet. 15",
      "meta_data": {
        "arxiv_id": "2212.10200v1",
        "authors": [
          "Ying Nie",
          "Kai Han",
          "Haikang Diao",
          "Chuanjian Liu",
          "Enhua Wu",
          "Yunhe Wang"
        ],
        "published_date": "2022-12-20T12:24:48Z",
        "pdf_url": "https://arxiv.org/pdf/2212.10200v1.pdf"
      }
    },
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
      "abstract": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
      "full_text": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Coleman Hooper1 Sehoon Kim1 Hiva Mohammadzadeh1 Michael W. Mahoney1,2,3 Yakun Sophia Shao1 Kurt Keutzer1 Amir Gholami1,2 1University of California, Berkeley 2ICSI 3LBNL {chooper, sehoonkim, hiva, mahoneymw, ysshao, keutzer, amirgh}@berkeley.edu Abstract LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facili- tates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional em- bedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quan- tization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quanti- zation, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantiza- tion on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ∼1.7× speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant. 1 Introduction Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [6], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [2, 30], as well as in academia [6, 22]. Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [ 17]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [12]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2401.18079v6  [cs.LG]  28 May 202516% 84%98% 2% Long Sequence LengthsKV Cache is the bottleneckShort sequence length Weightsare the bottleneck BaselineKVQuant int3 +Grouping 10.87 Pre-RoPEKeyQuantizationNon-UniformQuantization5.755.94 1%Outliers6.23 Per-Channel KeyQuantization7.05 Perplexity on Wikitext2fp16 Baseline Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in4.8× reduction in cached activation memory footprint. sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [ 18, 17]. However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference. It is therefore crucial to develop methods for compressing the KV cache to enable efficient long- sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1): • We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2). • We find that existing uniform and non-uniform quantization methods result in sub-optimal quanti- zation signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3). • Even with the above, we find that outlier values in cached KV activations can significantly de- grade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only 1% of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with 4.8× longer context length. • We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to ∼1.7× speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization. 22 Background 2.1 LLM Inference When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the Key-Value (KV) cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with n layers and h attention heads with dimension d that is stored using e bytes per element, the KV cache size for batch size b and sequence length l is 2 · n · h · d · e · b · l, meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process. 2.2 LLM Quantization There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21, 9, 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7, 9, 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9, 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17, 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat [8], or derived quantization signposts using a sensitivity-weighted k-means approach [17]. Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization. There has also been work on quantizing both weights and activations (including KV cache) [41, 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34, 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [34] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance [24]. One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size [26]. 2.3 KV Cache Compression There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25, 43, 11, 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [32]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference. 3Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (pre-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (post-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Layer 10 Values Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens. 3 Method 3.1 Per-Channel Key Quantization To inform our approach, we first performed a detailed analysis to understand the KV cache distribu- tions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7, 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels). Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [ 34, 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per- token quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix- vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation. Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping. 3.2 Pre-RoPE Key Quantization One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 [35]. Given Query and Key vectors Qm = Wq ∗ xm and Kn = Wk ∗ xn at positions m and n in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain ˜Qm = Rd θ,m · Qm and 4˜Kn = Rd θ,n · Kn. This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache ˜Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize Kn) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7). 3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In [17], the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offline on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range [−1, 1] prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation A, we formulate the error minimization objective as follows, where A is flattened to one dimension and where N is the number of elements from all of the samples in our calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 . (1) We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [8], demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). 3.4 Per-Vector Dense-and-Sparse Quantization As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [17], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel 5may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer. Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range [−1, 1], and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing 1% of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline. 3.5 Attention Sink-Aware Quantization Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [42]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a “sink”. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work [ 23]. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization. 3.6 Offline Calibration versus Online Computation A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance. 3.7 Kernel Implementation In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed 6Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models. Method LLaMA-7B LLaMA-13B LLaMA-30B LLaMA-65B PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) baseline 5.68 64.0 5.09 100.0 4.10 195.0 3.53 320.0 int4 5.98 16.0 5.32 25.0 4.34 48.8 3.73 80.1 nf4 5.87 16.0 5.23 25.0 4.25 48.8 3.63 80.1 ATOM-4bit 5.77 16.6 5.16 26.0 4.16 50.7 3.57 83.1 FlexGen-4bit 5.73 17.3 5.14 27.0 4.14 52.6 3.56 86.3 KVQuant-4bit 5.72 16.0 5.13 25.0 4.13 48.8 3.55 80.0 KVQuant-4bit-1% 5.69 17.3 5.10 27.0 4.11 52.7 3.54 86.5 int3 10.87 12.0 8.69 18.8 6.82 36.6 6.37 60.1 nf3 7.33 12.0 6.21 18.8 5.46 36.6 4.44 60.1 ATOM-3bit 6.17 12.6 5.47 19.7 4.44 38.4 3.78 63.0 FlexGen-3bit 5.93 13.2 5.29 20.6 4.26 40.2 3.66 65.9 KVQuant-3bit 5.87 12.0 5.25 18.8 4.25 36.6 3.63 60.0 KVQuant-3bit-1% 5.75 13.3 5.14 20.8 4.15 40.5 3.57 66.5 int2 11779 8.0 69965 12.5 1470 24.4 7272 40.1 nf2 3210 8.0 5786 12.5 2044 24.4 5367 40.1 ATOM-2bit 37.37 8.6 41.77 13.4 16.49 26.1 13.63 42.8 FlexGen-2bit 11.09 9.1 9.84 14.3 6.60 27.8 5.54 45.6 KVQuant-2bit 7.23 8.0 5.82 12.5 4.87 24.4 4.03 40.0 KVQuant-2bit-1% 6.01 9.3 5.36 14.5 4.35 28.3 3.70 46.5 vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either Compressed- Sparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R. 4 Results 4.1 Main Evaluation We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36, 37, 1, 16, 27, 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat [8] without grouping (nfX), as well as (iii) Atom [44] and FlexGen [34]. Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates. Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44, 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining 3.7 ×, 4.8×, and 6.9 × memory savings, respectively). 73.7x4.8x6.8x 3.7x4.8x6.8x Figure 3: Perplexity results for the LLaMA-2-7B-32K model [5] as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths. Table 2: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2- 70B-32K LongLoRA model [6]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual [ 26]. Average bit widths are estimated for each approach assuming 32K context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K. Model Method 2K 4K 8K 16K 32K Avg. Bit Width LLaMA-2-7B-32K fp16 1 1 1 1 1 16 KIVI-2-gs32-r128 0.76 0.72 0.72 0.68 0.7 3.05 nuq4-1% 1 1 1 1 1 4.33 nuq3-1% 0.98 1 1 1 1 3.33 nuq2-1% 1 1 0.98 1 1 2.33 Llama-2-70B-32K fp16 1 1 1 1 1 16 nuq4-1% 1 1 1 1 1 4.35 nuq3-1% 1 1 1 1 1 3.35 nuq2-1% 0.98 0.98 0.96 1 0.74 2.35 4.2 Long Context Length Evaluation Perplexity Evaluation. We evaluated long context length performance using the LLaMA-2-7B- 32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2-70B-32K LongLoRA model [6]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3 [6, 13]. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference. Passkey Retrieval Evaluation. We also evaluated the performance of our quantization method on passkey retrieval to assess the model’s ability to use its context. Passkey retrieval involves evaluating the model’s capacity to locate specific information in long texts [19], and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from [45] (which is based on the methodology from [28]) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model [26], demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window. LongBench Evaluation. Table 3 shows evaluation on LongBench [3] for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning [ 3]. The max input context length is set at 31500, and results using KIVI are also included for reference [26]. Our results demonstrate that our 3-bit 8Table 3: LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit-1%. Com- parisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming 12.2K context length, which was the average number of tokens across all tasks. Config Avg. bitNtrvQAQasperMF-enHotpot2WikiMusiqueGovRepQMSumMNewsTRECTriviQASamSumRBenchLCC PsgRetrPsgCntAvg. fp16 Baseline 16 17.96 10.51 33.43 12.55 12.53 6.19 29.65 16.99 22.15 71 87.79 43.97 59.99 62.14 23 1.50 31.96 KIVI-2-gs32-r128 3.17 19.25 10.66 24.78 12.48 11.19 6.38 27.05 16.36 23.37 71 80.80 43.93 57.74 60.61 13.58 1.50 30.04 KVQuant-3bit-1% 3.33 18.87 13.67 30.93 12.07 12.55 6.25 27.10 16.53 16.54 71 87.55 43.95 59.50 61.52 19.5 1.75 31.21 Table 4: RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning. Config Avg. bit Niah1 Niah2 Niah3 MKey1MKey2MKey3MValueMQueryVT CWE FWE QA1 QA2 Avg. fp16 Baseline 16 100 99.8 98.6 94 68.2 11 55.95 64.5 37.88 9.64 30.4 31.6 31.6 56.40 KIVI-2-gs32-r128 3.05 76 85.6 59.6 72.6 11.4 0 34.7 46.45 39.6 8.26 30.53 24.8 27.6 39.78 KVQuant-3bit-1% 3.33 99.8 98.8 95.2 92.8 61.6 6.4 47.5 54.45 41.04 8.52 29.33 31.0 31.0 53.65 KVQuant-2bit-1% 2.33 95.4 86.8 49.8 73.6 23.4 0 16.65 22.95 22.52 5.14 24.0 26.4 28.4 36.54 model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level. RULER Evaluation. Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite [15] using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves 14% better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with 1.5× smaller bit-width. 4.3 Joint Weight and KV Cache Quantization Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [ 17]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4-1% for the LLaMA- 7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods. 4.4 Performance Analysis and Memory Savings Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve 1.2-1.6× and 1.3-1.7× latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths. Appendix A highlights the benefits of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides 8× KV cache compression and enables serving the quantized LLaMA-7B model with a context length of 1M tokens on a single A100 GPU, as well as enabling serving the LLaMA-7B model with 10M context length on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference 9Table 5: KV cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [17]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [ 17], respectively. See Appendix M for experimental details. Weights KV Cache LLaMA-7B LLaMA-13B Avg. Bits (KV Cache) fp16 fp16 5.68 5.09 16 w4-s45 fp16 5.77 5.17 16 nuq4-1% 5.79 5.18 4.32-4.33 w3-s45 fp16 6.13 5.45 16 nuq3-1% 6.23 5.52 3.32-3.33 Table 6: Average latency (in microseconds) for the Key and Value nuq4-1% kernels, benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model across different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.7 and Appendix R provide additional details for our kernel implementation, Appendix R describes our benchmarking methodology, and Table 22 provides a detailed breakdown of kernel runtime on an A6000 GPU. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key nuq4-1% 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value nuq4-1% 22.1 37.9 124.5 while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. 5 Conclusion As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers) with only 0.1 perplexity degradation across different LLaMA, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings. Acknowledgements The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, Samsung SAIT, POSCO HOLDINGS, and NVIDIA. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer’s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel 10One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. References [1] AI@Meta. Llama 3 model card. 2024. [2] Anthropic. Introducing claude 2.1, Nov 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [9] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [10] Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28–September 1, 2017, Proceedings 23, pages 697–709. Springer, 2017. [11] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [12] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021. [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [14] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023. [15] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. 11[18] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023. [19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [20] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [22] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024. [23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024. [24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantiza- tion aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [28] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [29] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018. [30] OpenAI. New models and developer products announced at devday 2023, Nov 2023. [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza- tion for large language models. arXiv preprint arXiv:2308.13137, 2023. [34] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 12[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [38] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402–17414, 2022. [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. [41] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [42] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023. [43] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. [44] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. [45] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. 13A Memory Bottlenecks for Long Context Length Inference Table 7 shows the model size and KV cache memory requirements for different LLaMA models with different sequence lengths. For short sequence lengths, the model weights are the primary memory bottleneck. However, for longer sequence lengths and larger batch sizes, the KV cache memory is the main bottleneck. This is particularly pronounced when the weights are already quantized to low precision. In our work, we demonstrate that we can help address the KV cache memory bottleneck through low-precision KV cache quantization. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Table 8 shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths, with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides 3.7× KV cache compression (nuq4-1%) and enables serving the quantized LLaMA-65B model with a context length of 32K tokens on a single A100-80GB GPU (requiring 30.3GB for the model weights compressed to 4-bit, and 46.5GB for the KV cache when compressed with nuq2-1%), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with 10M context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. Table 7: Model size and activation memory size estimates for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). By compressing the KV cache to 2-bit precision, we can enable1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. BS Model Model Size (GB) KV Cache Size w/ Diff. Seq Len (GB) 16 → 2-bit 32K 128K 1M 10M (16 → 2-bit) 1 7B 12.6 → 1.6 16 64 512 4883 → 610 13B 24.1 → 3.0 25 100 800 7629 → 954 30B 60.3 → 7.5 49 195 1560 14877 → 1860 65B 121.1 → 15.1 80 320 2560 24414 → 3052 4 7B 12.6 → 1.6 64 256 2048 19531 → 2441 13B 24.1 → 3.0 100 400 3200 30518 → 3815 30B 60.3 → 7.5 195 780 6240 59509 → 7439 65B 121.1 → 15.1 320 1280 10240 97656 → 12207 B Additional Related Works B.1 Outlier-Aware LLM Quantization LLMs have been known to have distinct outliers both in weights and activations [ 7, 9, 17]. SqueezeLLM and SpQR both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [9, 17]. LLM.int8() [7] handled particular outlier channels separately in higher precision, and SmoothQuant [41] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [4, 14, 39, 38]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy benefits given the outlier structure in Keys, and that dense-and-sparse quantization can be efficiently applied for KV cache quantization. 14Table 8: Activation memory size estimates (GB) for 128K, 1M, and 10M sequence length ( l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Model Method l=128K l=1M l=10M LLaMA-7B fp16 64.0 512.0 4882.8 nuq4 16.0 128.1 1221.9 nuq4-1% 17.3 138.4 1319.6 nuq3 12.0 96.1 916.7 nuq3-1% 13.3 106.4 1014.4 nuq2 8.0 64.1 611.5 nuq2-1% 9.3 74.4 709.2 LLaMA-65B fp16 320.0 2560.0 24414 nuq4 80.0 640.3 6106.5 nuq4-1% 86.5 691.5 6595.0 nuq3 60.0 480.3 4580.6 nuq3-1% 66.5 531.5 5069.1 nuq2 40.0 320.3 3054.7 nuq2-1% 46.5 371.5 3543.3 B.2 Non-uniform LLM Quantization Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [17, 8]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [8]. SqueezeLLM [ 17] derived per-channel non-uniform quantization signposts using a sensitivity-weighted k-means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using a sensitivity-weighted k-means approach with KV cache activations. C RoPE Equation The rotation matrix for RoPE is provided in Equation 2, where c and s are cosine and sine functions, θi = 10000−2(i−1)/d, d is the attention head dimension, and n is the current position in the sequence:   c(nθ1) −s(nθ1) ··· 0 0 s(nθ1) c( nθ1) ··· 0 0 ... ... ... ... ... 0 0 ··· c(nθd/2) −s(nθd/2) 0 0 ··· s(nθd/2) c( nθd/2)   (2) The Query vectors computed at each iteration will have RoPE applied (to obtain ˜Qm = Rd θ,m ∗ Qm). When caching Key vectors, we therefore need to either cache ˜Kn = Rd θ,n ∗ Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. In order to apply Rd θ,n efficiently on-the-fly, we leverage the element-wise formulation of RoPE rather than the matrix-multiplication formulation from Equation 2. The element-wise formulation for Rd θ,nx is as follows, where ⊙ is the element- wise multiplication operator (note that the formulation that we use matches the implementation in the Transformers library for LLaMA [ 40], and it is a different but equivalent formulation to the element-wise expression in [35]): 15  x1 x2 ... xd 2 xd 2 +1 ... xd−1 xd   ⊙   c(θ1n) c(θ2n) ... c(θd 2 n) c(θ1n) ... c(θd 2 −1n) c(θd 2 n)   +   −xd 2 +1 −xd 2 +2 ... −xd x1 ... xd 2 −1 xd 2   ⊙   s(θ1n) s(θ2n) ... s(θd 2 n) s(θ1n) ... s(θd 2 −1n) s(θd 2 n)   (3) By leveraging this element-wise implementation, we can apply RoPE on-the-fly to the Key activations (after dequantizing the Key activations and before multiplying them with the corresponding elements in the Query vector). D Derivation for Sensitivity Analysis To compute the sensitivity analysis we largely follow the derivation in [29], which was originally provided to compute sensitivity of a NN based classifier but can be extended with minor modifications for quantization. In particular, the sensitivity measure is based on how much the loss output of the model is perturbed. To compute this we denote activations at a layer before quantization as A, after quantization as AQ, quantization perturbation in activation as A − AQ, and the gradient of the Loss function w.r.t. activation as J(A) = ∂L ∂A (A). By making the assumption that the quantization perturbation of different activations follow a Gaussian distribution with zero mean, we can show that the sensitivity of an activation value is proportional to Fii \u0000 A − Q(A) \u00012 as was given in Equation 1: E∆A h |L(A) − L(A + ∆A)|2 i ≈ E∆A h\u0000 J(A)T ∆A \u00012i = E∆A \"\u0010X i Ji∆Ai \u00112 # = E∆A \"X i J2 i ∆A2 i # = X i J2 i E∆A \u0002 ∆A2 i \u0003 . Here note that we first assume a first order Taylor series expansion to approximate the perturbation to the loss, and then use the zero mean assumption of the quantization perturbation to derive the second line. To approximate the E∆A \u0002 ∆A2 i \u0003 we use empirical evaluation of the expectation by sampling multiple different inputs and empirically computing the resulting quantization perturbation which results in Equation 1. E Derivation for Quantization Error In our work, before applying the sensitivity-weighted K-means to derive quantization signposts, we normalize each element Ai in the flattened activation A. This normalization for Ai involves a shift by a zero-point zi followed by rescaling the quantization signposts by a scaling factor si, where si and zi are the scaling factor and zeropoint corresponding to element Ai: Ai,norm = Ai − zi si , (4) where Ai and Ai,norm are element i from activation A before and after normalization, respectively. We then quantize Ai,norm to Q(Ai,norm) with quantization error ∆Ai,norm. After we dequantized, we rescale each element by si and add zi to get the recovered quantized activation value Q(Ai): 16Q(Ai) = si Q(Ai,norm) + zi. (5) As such, if there is quantization error ∆Ai,norm in Ai,norm, this will be scaled by si in terms of the error in Ai, i.e., ∆Ai = si∆Ai,norm. For activation A which is normalized to Anorm (with corresponding scaling factors si for each element Ai), minimizing the sensitivity-weighted quantization error as expressed in Equation 1 gives us the following expression, which we can minimize using the normalized activations across all N elements from the samples in a calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 (6) = arg min Q NX i=1 Fii \u0010 s2 i \u0000 Ai,norm − Q(Ai,norm) \u00012\u0011 (7) F Key and Value Dynamic Range Figure 4 shows the portion of the elements contained within difference percentages of the dynamic range for both Keys and Values. The majority of values (∼ 99%) are contained in a small portion of the dynamic range, and a small portion of numerical outliers skew the dynamic range that must be represented. This motivates our dense-and-sparse approach which removes numerical outliers and stores them in a separate sparse matrix, thereby restricting the range that needs to be represented in the dense component. 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Keys 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Values t100 t99.99 t99.9 t99 Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision). G Per-Channel Key Quantization Ablations As shown in Table 9, per-channel quantization for Keys and per-token quantization for Values outperforms the standard per-token quantization approach for both Keys and Values, yielding an improvement of 3.82 perplexity for the LLaMA-7B model at 3-bit precision. This demonstrates the 17benefits of per-channel Key quantization to mitigate the large outlier channels in Keys. Note that for all experiments using per-channel quantization, we use an fp16 zeropoint rather than a low-precision zeropoint that is rounded to the nearest integer value. We do this since for some of the Key channels, all of the elements are positive or all of the elements are negative, meaning that the zeropoint will fall outside of the range that is representable by a low-precision integer (and rounding it to the nearest low-precision value can degrade performance). Additionally, we observe that per-channel quantization for Values actually performs worse than per-token quantization. We hypothesize that this behavior is because per-channel Value quantization leads to greater error accumulation in particular output values (since the result of the attention scores multiplied by one channel of the Values will be localized to a single value in the output vector), which leads to greater quantization error at later model layers. Another concurrent work, KIVI [26], observes similar behavior for per-channel Value quantization, which they attribute to the fact that per-token Value quantization confines the error to each token. Assuming that the output is a weighted sum of only a few important tokens (as only a few attention scores are large), a perturbation in these tokens can lead to significant degradation. Per-token Value quantization therefore ensures that the quantization of unimportant tokens does not adversely impact the important tokens. Table 9: Ablation Study: Perplexity comparison of per-token and per-channel quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization, and PC refers to per-channel quantization. Datatype Key Dim. Value Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 int3 PT PT 10.87 12.0 int3 PC PC 223 12.0 int3 PC PT 7.05 12.0 H Pre-RoPE Key Quantization Ablations As shown in Table 10, pre-RoPE Key quantization achieves higher accuracy than post-RoPE quanti- zation, with an improvement of 0.82 perplexity for 3-bit quantization with the LLaMA-7B model. These results show that the rotary positional embeddings make Key quantization more challenging due to mixing pairs of channels with different magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization at low precision. Table 10: Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key quantization for LLaMA-7B (using per-channel Key quantization and per-token Value quantization). Pre-RoPE quantization leads to significant improvement (see Section 3.2 for more details). Datatype Scheme Perplexity KV Cache Size (GB) Seqlen 128K fp16 - 5.68 64.0 int3 post-RoPE 7.05 12.0 int3 pre-RoPE 6.23 12.0 I Sensitivity-Weighted Non-Uniform Quantization Ablations Table 11 shows perplexity evaluation results across different LLaMA, Llama-2, and Mistral models on Wikitext-2 for different datatypes, including nuq3, nuq3 without using sensitivity-weighting, as well as nuq3 with sensitivity-weighting but without accounting for the per-channel scaling factors when performing k-means. We observe particularly noticeable gains relative to uniform quantization for 3- bit and 2-bit quantization, where the benefits of non-uniform quantization are more pronounced due to the reduced precision. These results demonstrate the benefits of our sensitivity-weighted non-uniform quantization approach relative to NormalFloat quantization [8], as we achieve consistent accuracy improvements of up to 0.33 perplexity across different models. These results also demonstrate the 18necessity of our sensitivity-weighting approach in order to derive performant non-uniform datatypes using a k-means based approach. Additionally, we observe distinct benefits when also accounting for the per-channel scaling factors when performing k-means. Table 11: Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization for Keys and per-token quantization for Values (meaning that all configurations are the same as in KVQuant, except for the datatype). We compare against both uniform (int3) and non-uniform (nf3) [8] approaches, as well as with using “unweighted” k-means (i.e., not sensitivity-weighted) and “Fisher-weighted k- means” (without accounting for per-channel scaling factors) to compute the non-uniform quantization signposts. Note that there is slight variation in average bitwidth across models due to the differing hidden dimensions. Results report perplexity with 2K/4K/8K sequence length for LLaMA, Llama-2, and Mistral, respectively. Method LLaMA Llama-2 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 4.76 16 int3 6.23 5.60 5.09 5.18 5.95 5.98 3.26 5.26 3.00-3.02 nf3 6.05 5.42 4.51 3.84 5.55 5.15 3.27 5.13 3.00-3.02 nuq3 (unweighted) 6.84 6.16 5.37 4.57 8.52 7.66 3.67 5.29 3.00-3.02 nuq3 (Fisher-weighted) 6.01 5.34 4.41 3.74 5.49 4.83 3.26 5.03 3.00-3.02 nuq3 (KVQuant) 5.94 5.32 4.34 3.68 5.39 4.82 3.23 4.98 3.00-3.02 J Per-Vector Dense-and-Sparse Quantization Ablations Table 12 shows the performance improvements we observe when isolating a small portion of outliers and storing them in a sparse format. We provide results both with using a single per-matrix outlier threshold, as well as with applying separate outlier thresholds per-vector. In particular, we see greater improvements by employing outlier detection with a different threshold per-channel for Keys and per-token for Values. This provides additional benefits since some values which would be considered outliers for the entire matrix are not actually outliers within a particular channel (so they are not hard to quantize). It is therefore better to directly target the outliers that will skew the quantization range for a particular channel. By removing 1% of outliers using per-vector thresholds, we can achieve an additional 0.19 reduction in perplexity for the LLaMA-7B model at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity. Table 12: Ablation Study: Perplexity comparison of different outlier isolation methods for LLaMA- 7B on Wikitext-2. Per-vector outlier detection allows for significant accuracy improvements relative to per-tensor outlier detection. All experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE). “PV” refers to using per-vector outlier thresholds, and “PM” refers to using a single per-matrix outlier threshold. Datatype % Outliers Outlier Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 nuq3 - - 5.94 12.0 nuq3 0.1% PM 5.89 12.2 nuq3 0.1% PV 5.82 12.2 nuq3 1% PM 5.85 13.3 nuq3 1% PV 5.75 13.3 K Attention Sink-Aware Quantization Ablations Table 13 provides perplexity with and without Attention Sink-Aware quantization across different LLaMA, Llama-2, and Llama-3 models. For all bit widths (4, 3, and 2-bit) and in both dense- only and dense-and-sparse quantization settings, Attention Sink-Aware quantization consistently yields perplexity improvement. Notably, the perplexity gain is more pronounced at lower bit widths 19and without sparsity. We observe particularly significant improvements for Llama-3 models, with perplexity improvements of 0.25 and 0.13 PPL for nuq2-1% on Llama-3-8B and Llama-3-70B, respectively. Table 13: Ablation Study: Perplexity with and without Attention Sink-Aware quantization on Wikitext-2 with various models. Attention Sink-Aware quantization consistently improves perplexity across different models and bit widths, particularly with lower bit widths and without sparsity. Datatype Attention Sink-Aware LLaMA-7B LLaMA-13B Llama-2-7B Llama-2-13B Llama-3-8B Llama-3-70B fp16 - 5.68 5.09 5.12 4.57 5.54 2.59 nuq4 X 5.72 5.14 5.17 4.62 5.64 2.65 nuq4 O 5.72 5.13 5.16 4.60 5.60 2.62 nuq4-1% X 5.69 5.10 5.13 4.59 5.57 2.60 nuq4-1% O 5.69 5.10 5.13 4.58 5.56 2.60 nuq3 X 5.94 5.32 5.39 4.82 6.10 2.99 nuq3 O 5.87 5.25 5.34 4.71 5.84 2.72 nuq3-1% X 5.75 5.15 5.18 4.62 5.67 2.66 nuq3-1% O 5.75 5.14 5.17 4.61 5.64 2.63 nuq2 X 8.47 7.29 11.20 23.34 16.63 6.79 nuq2 O 7.23 5.82 7.03 9.59 7.04 3.49 nuq2-1% X 6.05 5.39 5.47 4.85 6.29 2.95 nuq2-1% O 6.01 5.36 5.41 4.78 6.04 2.82 L Calibration Ablations Figure 5 outlines the accuracy and efficiency challenges which our work addresses in order to enable accurate and efficient KV cache quantization. We demonstrate that we can circumvent the challenges related to efficiently performing online per-channel scaling factor computation by instead calibrating offline for the scaling factor without hurting accuracy. Additionally, we show that we can perform per-token scaling factor and outlier threshold computation efficiently online during inference, thereby enabling accurate per-token quantization without compromising on efficiency. Per-channel Quantization  dim sequence length KeysNew Key Per-token Quantization  dim sequence length ValuesNew ValueChallenge: need to potentially recompute the scaling factor every time a new key is added if we computeitonline Challenge: Need to compute the scaling factor for each incoming token per-channel scaling factor per-token scaling factor Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix L, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation. Table 14 shows accuracy results when using offline calibration for computing the scaling factors for the Keys. For 3-bit quantization, we observe minor accuracy degradation when not employing outlier extraction methods. However, if we remove a small percentage of outliers, then the accuracy with offline calibration is the same as computing the scaling factors online per-channel during evaluation. This demonstrates that when incorporating outlier extraction methods, we are better able to perform offline calibration due to reduced sensitivity to outliers (either to outliers during calibration that exaggerate the quantization range, or to outliers during evaluation that cannot be represented accurately if there weren’t large outliers observed during calibration). 20Table 15 shows the runtime for the topk operation for the LLaMA-7B model (which is required for computing outlier thresholds online). It compares the runtime of the topk operation with the runtime for the QKV projections, finding that the topk runtime is 45% of the matrix-vector operation runtime for a single projection layer. The topk operation can also be performed efficiently on the CPU, so we can actually run this operation in parallel with the subsequent linear layer matrix-vector operations on the GPU (which is possible by computing the Value projection before the Key and Query projections). This allows us to compress the activations dynamically without added runtime overhead, thereby enabling online scaling factor computation for the Value tensors. Table 16 shows how both Fisher information computation and calibration (including k-means) per- layer take only a few minutes for the LLaMA-65B model on a typical server machine. Even if we perform calibration sequentially for each layer, the entire calibration process would take a maximum of 6 hours for the LLaMA-65B model at 4-bit precision. Table 14: Ablation Study: Model accuracy when using offline calibration for Keys with LLaMA-7B. When incorporating outlier detection, offline calibration for Keys is able to perform comparably with online calibration. All nuq3 experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection. Datatype % Outliers Perplexity Perplexity (Online for K) (Offline for K) fp16 - 5.68 5.68 nuq3 - 5.91 5.94 nuq3 1% 5.75 5.75 Table 15: topk runtime on a vector of length 4096 for computing outlier thresholds when using 1% sparsity (compared with the runtime for the QKV matrix multiplications, which are 4096×4096 by 4096 matrix-vector multiplications for the LLaMA-7B model). The runtime is reported on a system with an A6000 GPU and an Intel Xeon Gold 6126 CPU. We find that the runtime for the topk operation is only 45% of the runtime of each matvec operation. Additionally, the topk operation can be performed efficiently on the CPU; we can therefore run this operation in parallel with subsequent linear layer operations on the GPU to compress the activations dynamically without added overhead. Operation Device Outlier % Runtime (ms) QKV Projection GPU - 0.172 topk CPU 1% 0.026 topk GPU 1% 0.088 QKV Projection / topk (Fused) GPU / CPU 1% 0.173 Table 16: Runtime for computing Fisher information as well as for calibration (including k-means) with 16 samples for LLaMA-65B quantization. Runtime for computing Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration (including k-means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown for a single layer. Note that calibration is independent for each layer, so it can be easily parallelized. Operation Runtime (minutes) Computing Fisher Information 2.8 4-bit Calibration Per-Layer (including k-means) 4.5 3-bit Calibration Per-Layer (including k-means) 2.7 2-bit Calibration Per-Layer (including k-means) 1.9 M Additional Experimental Details For our empirical evaluation, we use 16 calibration samples of sequence length 2K from the Wikitext- 2 training set (as well as the corresponding gradients) to derive the per-channel scaling factors and zero-points, and to derive the non-uniform datatypes for both Keys and Values. While we use KVQuant models that are calibrated on the Wikitext-2 dataset for all experiments, in Appendix L, we 21include an additional experiment that demonstrates the robustness of the calibration process to the choice of data. We measured perplexity on both Wikitext-2 and on C4 using a sequence length equal to the maximum context length of the model (2K for LLaMA, 4K for Llama-2, and 8K for Llama-3 and Mistral- 7B). For generative tasks, when processing the input prompt, the Key/Value matrix multiplications are computed using the fp16 Keys and Values, and then they are separately compressed into low precision. For baseline experiments, we use post-RoPE quantization, both since this is required from an efficiency perspective without a dedicated kernel implementation, and because it provides better accuracy when quantizing Keys per-token as shown in Appendix P. We make several assumptions in order to estimate average bit widths and KV cache sizes for different approaches. We compute these estimates assuming a sequence length of 128K (unless otherwise specified). For integer quantization, we assume a low-precision integer offset and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers are assumed for the per-token indices (since we need to support long sequence lengths), and the elements and per-element indices are assumed to be 16-bit. This means that for CSR, the rows are assumed to be 32-bit and the columns and values are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit and the rows and values are assumed to be 16-bit. N Comparison Between Different Datatypes and Sparsity Levels Table 17 shows perplexity across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 using different datatypes. The results highlight that the NUQ datatype generally outperforms both uniformly quantized INT datatype and non-uniformly quantized NF datatype even after they are incorporated with grouping at the expense of increased bit-widths. This performance can be further improved by extracting 0.1% to 1.0% of outliers. Note that NUQ with a sparsity level 1.0% has a similar memory requirement as INT with a group size of 64, but achieves significant perplexity improvement across all models and bit widths. O Full Perplexity Evaluation Tables 18 and 19 show perplexity evaluation across all LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4, respectively. These results demonstrate the benefits of our approach for KV cache compression across different models and bit widths. 22Table 17: Comparison of our NUQ datatype with and without sparsity against other data types on different models using the perplexity (PPL) measured on Wikitext-2. Non-uniform quantization (“nuq”) results are using pre-RoPE per-channel quantization for Keys. “gs64/128” refers to baseline experiments using grouping with group size 64/128. Note that there is a slight variation in average bitwidth across models due to the differing hidden dimensions. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 int4-gs128 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 int4-gs64 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 nf4-gs128 5.77 5.17 4.17 3.58 5.30 4.71 3.16 5.84 2.75 4.83 4.25 nuq4 5.72 5.14 4.14 3.56 5.17 4.62 3.14 5.64 2.65 4.81 4.00-4.02 + 0.1% outliers 5.70 5.12 4.12 3.55 5.15 4.60 3.13 5.63 2.66 4.79 4.04-4.06 + 0.5% outliers 5.70 5.11 4.12 3.54 5.14 4.59 3.13 5.59 2.61 4.78 4.16-4.19 + 1.0% outliers 5.69 5.10 4.11 3.54 5.13 4.59 3.13 5.57 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 int3-gs128 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 int3-gs64 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 nf3-gs128 6.26 5.52 4.54 3.83 6.21 5.43 3.38 7.79 4.74 5.23 3.25 nuq3 5.94 5.32 4.34 3.68 5.39 4.82 3.23 6.10 2.99 4.98 3.00-3.02 + 0.1% outliers 5.82 5.22 4.21 3.62 5.27 4.69 3.19 5.96 2.96 4.91 3.04-3.06 + 0.5% outliers 5.76 5.16 4.16 3.59 5.20 4.65 3.16 5.74 2.69 4.84 3.16-3.19 + 1.0% outliers 5.75 5.15 4.15 3.57 5.18 4.62 3.15 5.67 2.66 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 int2-gs128 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 int2-gs64 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 nf2 3210 5785 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 nf2-gs128 351 141 60.97 31.69 635 642 71.21 1024 3091 253 2.25 nuq2 8.47 7.29 6.08 9.19 11.20 23.34 4.18 16.63 6.79 6.87 2.00-2.02 + 0.1% outliers 6.82 5.72 4.83 4.00 6.38 5.33 3.54 7.97 4.77 5.83 2.04-2.06 + 0.5% outliers 6.24 5.49 4.45 3.80 5.59 4.95 3.33 6.53 3.18 5.28 2.16-2.19 + 1.0% outliers 6.05 5.39 4.41 3.72 5.47 4.85 3.28 6.29 2.95 5.14 2.32-2.35 Table 18: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 ATOM-4bit 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 Flexgen-4bit 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 KVQuant-4bit 5.72 5.13 4.13 3.55 5.16 4.60 3.13 5.60 2.62 4.80 4.00-4.02 KVQuant-4bit-1% 5.69 5.10 4.11 3.54 5.13 4.58 3.13 5.56 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 ATOM-3bit 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 FlexGen-3bit 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 KVQuant-3bit 5.87 5.25 4.25 3.63 5.34 4.71 3.18 5.84 2.72 4.98 3.00-3.02 KVQuant-3bit-1% 5.75 5.14 4.15 3.57 5.17 4.61 3.15 5.64 2.63 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 nf2 3210 5786 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 ATOM-2bit 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 FlexGen-2bit 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 KVQuant-2bit 7.23 5.82 4.87 4.03 7.03 9.59 3.45 7.04 3.49 6.84 2.00-2.02 KVQuant-2bit-1% 6.01 5.36 4.35 3.70 5.41 4.78 3.26 6.04 2.82 5.14 2.32-2.35 23Table 19: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on C4. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 7.08 6.61 5.98 5.62 6.63 6.05 4.97 7.10 5.78 5.71 16 int4 7.40 6.82 6.18 5.75 7.31 6.59 5.12 8.79 14.36 5.91 4.00-4.02 nf4 7.27 6.74 6.10 5.69 7.09 6.45 5.06 7.93 6.58 5.85 4.00-4.03 ATOM-4bit 7.16 6.67 6.02 5.65 6.87 6.20 5.00 7.32 6.03 5.76 4.16 FlexGen-4bit 7.12 6.64 6.00 5.63 6.79 6.15 4.99 7.23 5.84 5.75 4.31 KVQuant-4bit 7.11 6.64 6.00 5.63 6.68 6.08 4.99 7.18 5.80 5.75 4.00-4.02 KVQuant-4bit-1% 7.09 6.62 5.99 5.62 6.64 6.06 4.98 7.12 5.78 5.72 4.32-4.35 int3 12.97 10.95 9.13 8.27 30.14 28.57 16.00 63.75 301 8.84 3.00-3.02 nf3 8.90 7.84 7.43 6.37 14.92 13.75 5.96 68.38 148 7.27 3.00-3.03 ATOM-3bit 7.62 6.93 6.24 5.79 8.00 7.06 5.16 9.00 6.72 6.08 3.15 FlexGen-3bit 7.34 6.78 6.11 5.70 7.29 6.59 5.08 7.92 6.18 5.92 3.30 KVQuant-3bit 7.25 6.72 6.06 5.68 6.89 6.18 5.03 7.43 5.90 5.92 3.00-3.02 KVQuant-3bit-1% 7.13 6.64 6.00 5.63 6.69 6.09 4.99 7.19 5.81 5.76 3.32-3.35 int2 10892 100870 1411 7216 4708 4220 814 2113 1977 477 2.00-2.02 nf2 2850 4680 1617 5190 13081 4176 3217 78331 7616 1102 2.00-2.03 ATOM-2bit 43.49 56.25 21.07 17.05 113 97.04 23.67 135 3734 50.73 2.14 FlexGen-2bit 13.91 13.36 8.49 7.34 35.21 40.40 8.28 50.78 42.27 13.83 2.28 KVQuant-2bit 8.52 7.32 6.67 5.96 9.49 15.36 5.30 10.06 6.45 7.63 2.00-2.02 KVQuant-2bit-1% 7.33 6.78 6.11 5.70 6.96 6.25 5.08 7.60 5.96 6.05 2.32-2.35 24P Post-RoPE Per-Token Quantization Ablation Table 20 shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model with uniform quan- tization, with Keys quantized pre-RoPE and post-RoPE. These results show that post-RoPE Key quantization is superior to pre-RoPE Key quantization when quantizing Keys per-token. This is because when rotating an outlier channel with large average magnitude and another channel with smaller average magnitude together, at some positions in the sequence, part of the magnitude from the outlier channel will be shifted to the smaller channel. This partially mitigates the impact of the outlier channel on skewing the quantization range for some of the tokens in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token Key quantization to serve as a stronger baseline. Table 20: Model accuracy when using pre-RoPE and post-RoPE quantization for LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE quantization is superior when using per-token Key quantization. Therefore, we decided to use these results for baseline comparison with per-token quantization. Datatype Perplexity (Pre-RoPE) Perplexity (Post-RoPE) fp16 5.68 5.68 int4 6.02 5.98 int4-gs128 5.76 5.77 int3 14.68 10.87 int3-gs128 6.28 6.17 Q Experiments on Calibration Data Robustness To evaluate the robustness of our quantization method to the choice of calibration datasets, we measure the perplexity on Wikitext-2 and C4 using different quantized models calibrated with Wikitext-2 and C4. As shown in Table 21, the resulting perplexity numbers remain similar even when the models are calibrated with out-of-domain examples (e.g., calibrated with Wikitext-2 and evaluated on C4, and vice versa), demonstrating the robustness of the calibration process to the choice of data. Table 21: Perplexity (PPL) results on Wikitext-2 and C4 using different quantization schemes, calibrated using Wikitext-2 and C4. Datatype Wikitext-2 C4 Calib. with Wikitext-2 Calib. with C4 Calib. with Wikitext-2 Calib. with C4 4-bit, 1% sparsity 5.69 5.70 7.09 7.09 3-bit, 1% sparsity 5.75 5.75 7.13 7.13 2-bit, 1% sparsity 6.05 6.07 7.38 7.38 R Kernel Implementation Details We implemented 4-bit lookup table-based kernels for matrix-vector multiplication between the Key or Value activations (packed as a lookup table (LUT) plus indices into the LUT per-element) and a full-precision activation vector. These kernels load the compressed Key and Value activations and dequantize them only as needed in order to minimize memory bandwidth utilization. All arithmetic is performed in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform datatype for that particular layer scaled according to the range of activations that need to be represented [8]. When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse Row (CSR) format for storing the outliers for the Keys and Values, we needed to consider how easy it would be to append new vectors. When using CSC format for the Key matrix, we only need to append a single element to the column vector, as well as one new element to the row and value vectors per nonzero element in that new column. If we used CSR format, we would need to insert the new column and value elements in the middle of the existing column and value vectors, and we would 25need to recompute the elements of the row vector. When using CSR format for the Value matrix, we only need to append a single element to the row vector, as well as one new element to the column and value vectors per nonzero element in that new row. If we used CSC format, we would need to insert the new row and value elements in the middle of the existing row and value vectors, and we would need to recompute the elements of the column vector. We therefore used the CSC format for the Key matrices and the CSR format for the Value matrices. One challenge with efficiently processing the sparse matrix-dense vector operation is that the sparsity distribution may be unbalanced. This poses a challenge for efficiently processing the sparse matrix on a GPU as there can be different numbers of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense vector kernel based on [10, 17], which assigns an equal number of nonzeros per thread. This has greater synchronization overhead than assigning a single thread for an entire row or column when processing CSR/CSC matrices, but it leads to a more balanced work assignment between threads. We set the number of threads such that there were 10 nonzero values assigned to each thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations. A second potential challenge is that the quantization dimension for the Keys and Values is not aligned with the reduction dimension for the respective matrix-vector operations. If we used per-channel or per-token lookup tables for dequantization, we would need to load from a different lookup table for each element as we iterate along the reduction dimension. However, since our implementation uses a single per-layer datatype that is rescaled per-vector, we can load a single scaling factor and zero-point for each channel / token and broadcast it across all threads (while doing a table lookup from the shared per-layer datatype, which is duplicated such that each thread can access it efficiently in parallel). The use of a single per-layer datatype that is rescaled per-vector therefore allows for much more efficient GPU kernel implementations in the context of per-channel Key / per-token Value quantization. Table 22 shows a detailed breakdown of kernel runtime, including how much time is spent packing vectors into the compressed format and how much time is spent on the dense and sparse matrix- vector multiplications. For the fp16 baselines, we benchmarked runtime by averaging across 1000 iterations. When benchmarking our dense-and-sparse kernels (both for packing and matrix-vector multiplication), since the runtime of the sparse kernels can vary based on the sparsity pattern, we ran the LLaMA-2-7B-32K model and collected activations at each layer, and we then measured the average runtime across 1000 iterations for each layer in the model. We find that even with 1% sparsity, we can attain significant speedups of up to 1.7× relative to the fp16 matrix-vector multiply kernels, demonstrating how our methodology facilitates efficient inference with a low-precision quantized KV cache. Table 22: Average latency (in microseconds) for the Key and Value dense nuq4 kernels as well as for the sparse kernels (with 1% outliers), benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model. Benchmarking results are reported for different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. We find that our dense-and-sparse approach (even with 1% outliers) provides latency benefits relative to the fp16 baseline, even when accounting for the time to compress activations online. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key (nuq4-1%) Packing 4.5 4.5 4.5 Dense Matvec 13.0 24.1 87.6 Sparse Matvec 8.1 11.2 34.2 Total Latency 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value (nuq4-1%) Packing 4.1 4.1 4.1 Dense Matvec 10.0 17.8 62.0 Sparse Matvec 7.9 15.9 58.2 Total Latency 22.1 37.9 124.5 26S Limitations While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than 100K context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory. 27",
      "meta_data": {
        "arxiv_id": "2401.18079v6",
        "authors": [
          "Coleman Hooper",
          "Sehoon Kim",
          "Hiva Mohammadzadeh",
          "Michael W. Mahoney",
          "Yakun Sophia Shao",
          "Kurt Keutzer",
          "Amir Gholami"
        ],
        "published_date": "2024-01-31T18:58:14Z",
        "pdf_url": "https://arxiv.org/pdf/2401.18079v6.pdf"
      }
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abstract": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
      "full_text": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu * 1 Jiayi Yuan* 1 Hongye Jin 2 Shaochen (Henry) Zhong 1 Zhaozhuo Xu 3 Vladimir Braverman 1 Beidi Chen 4 Xia Hu 1 Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re- computations, significantly increases memory de- mands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straight- forward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element dis- tribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group ele- ments along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we de- veloped a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory (in- cluding model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼3.47× throughput on real LLM infer- ence workload. The source code is available at https://github.com/jy-yuan/KIVI. *Equal contribution . The order of authors is determined by flipping a coin. 1Rice University 2Texas A&M University 3Stevens Institute of Technology 4Carnegie Mellon University. Correspondence to: Zirui Liu <zl105@rice.edu>, Jiayi Yuan <jy101@rice.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Taylor et al., 2022; Yuan et al., 2023; Chuang et al., 2024). However, their deployment is very costly, requiring a large number of hardware accelerators such as GPUs. Given these substantial costs, one natural way to reduce the cost per request is to combine a sufficient number of requests together for batch processing. However, in this batch infer- ence scenario, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, is becoming the new memory and speed bottleneck. This bottleneck becomes more pronounced with larger batch sizes and longer context lengths. For instance, in 540B PaLM, with a batch size of 512 and a context length of 2048, KV cache alone can take 3TB. This is 3 times the size of the model’s parameters (Pope et al., 2023). Also, the GPU SRAM has to load the whole KV cache from the GPU device memory for every token generated, during which the computational cores are idle. Thus, reducing KV cache size in LLMs while maintaining accuracy is important. Existing works towards this problem can be roughly divided into three categories. First, some work suggests reducing the number of heads in KV cache, such as multi-query attention (Shazeer, 2019) and multi-group attention (Ainslie et al., 2023). However, these methods require either training the model from scratch or fine-tuning the existing model. Second, another research line reduces KV cache size by evicting unimportant tokens (Zhang et al., 2023). Third, some other works try to solve this problem from the system perspective, e.g., offloading KV cache (Sheng et al., 2023) or extending virtual memory and paging techniques into the attention mechanism (Kwon et al., 2023). To reduce the size of KV cache, the most simple and ef- fective way is to reduce the total bytes taken by KV cache, namely, quantization. Unlike the well-studied weight quan- tization (Lin et al., 2023; Xiao et al., 2023a; Zhao et al., 2024), to the best of our knowledge, only a few studies applied the vanilla 4bit round-to-nearest quantization to KV cache (Sheng et al., 2023; Zhang et al., 2023; Zhao et al., 2024) due to the streaming nature of KV cache or other complications. There is a lack of in-depth studies that ex- 1 arXiv:2402.02750v2  [cs.CL]  25 Jul 2024KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache per-token  quantization  per-channel  quantization  Figure 1: Definition of per-token and per-channel quantiza- tion. X ∈ Rlprompt×d is key/value cache where lprompt is the number of tokens and d is the number of channels. zX is the zero-point, sX is the scaling factor. plore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we study the element distribution of KV cache. Our analysis suggests: • For key cache, there are a few fixed channels whose magnitudes are very large, which is consistent with previous finding (Lin et al., 2023; Xiao et al., 2023a). Thus, as shown in Figure 1 right, key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In this way, it can confine the error to each individual channel, without impacting the other normal channels. • For value cache, there is no obvious outlier pattern. Al- though value cache has no obvious outlier pattern, we experimentally show that it can only be quantized per- token because it is used to calculate the attention output, which is essentially a value cache mixer. As shown in Figure 1 left, the per-token quantization can confine the error inside each individual token and ensure that the quantization of one token does not adversely impact the others. Based on the above insights, we propose KIVI, a plug-and- play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel key cache quantization, the quan- tization process spans different tokens, which cannot be directly implemented in this streaming setting. Since the number of tokens in key cache can be arbitrary, our key idea is to split key cache into two parts. The first part is the grouped key cache, which contains several groups of tokens and each group has a certain number of tokens. The second part is the residual key cache, which does not have a suffi- cient number of tokens to form a complete group. Similarly, we split value cache into the grouped and residual parts to maintain the accuracy. We only apply group-wise quanti- zation to the grouped key cache and value cache, while the residual key cache and value cache are kept in full precision. The grouped and residual parts can be combined using tiled matrix multiplication when computing attention scores. Our contributions are summarized as follows: • Extensive analysis regarding the outlier patterns and quantization error of KV cache in commonly- used LLMs. Our observations suggest that key cache should be quantized per-channel and value cache should be quantized per-token. We also explain in depth why these caches require different quantization approaches. • A new plug-and-play 2bit KV cache quantization algorithm without any fine-tuning, KIVI, with hardware-friendly implementation. We conduct an extensive evaluation for KIVI with Llama, Mistral, and Falcon on popular generation tasks. KIVI can efficiently compress KV cache to 2bit and bring 2.6× peak memory usage reduction for Llama-2-7B, with little to no accuracy drop. With our efficient system im- plementation, this memory reduction, in return, enables up to 4× larger batch size and brings2.35× ∼3.47× throughput. 2. Background: Attention Inference-Time Workflow The LLM attention inference-time workflow involves two phases: i) the prefill phase, where the input prompt is used to generate KV cache for each transformer layer of LLMs; and ii) the decoding phase, where the model uses and updates KV cache to generate the next token, one at a time. Prefill Phase. Let X ∈ Rb×lprompt×d be the input tensor, where b is the batch size, lprompt is the length of the input prompt, and d is the model hidden size. For convenience, we ignore the layer index here. The key, value tensors can be computed by XK = XWK, XV = XWV , where WK, WV ∈ Rd×d are the key and value layer weight, respectively. After obtaining XK and XV , they are cached in the memory for the ease of decoding. Decoding Phase. Let t ∈ Rb×1×d be the current input token embedding. Let tK = tWK and tV = tWV be the key and value layer output, respectively. We first update KV cache: XK ← Concat(XK, tK), XV ← Concat(XV , tV ), 2KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache then calculate the attention output as: tQ = tWQ, A = Softmax(tQX⊤ K), tO = AXV , (1) where WQ is the weight matrix of the query layer. For ease of illustration, we ignore the attention output layer and the other parts of the inference workflow. Memory and Speed Analysis. The above process is re- peated until a special token indicating the sentence’s con- clusion is reached. Let lgen be the number of generated tokens. From the above analysis, the shape of KV cache is b ×(lprompt + lgen) ×d. To get a sense of the scale, consider the OPT-175B model with a batch size b 512, a prompt length lprompt 512, and an output length lgen 32. The KV cache requires 1.2TB, which is 3.8 times the model weights (Sheng et al., 2023). Besides the memory, the inference speed is also decided by the KV cache size. The GPU needs to load KV cache from GPU main memory to GPU SRAM once for every token generated during which the computa- tional core of the chip is essentially idle (Pope et al., 2023; Kwon et al., 2023). 3. Methodology In scenarios with long contexts or batched inferences, the memory and speed bottlenecks are storing and loading KV cache. The most simple and effective way to alleviate this problem is to reduce the total bytes occupied by KV cache, specifically, quantization. Following this motivation, we first evaluate the performance of the existing quantization method in Section 3.1. Our observations suggest that key and value cache should be quantized along different dimen- sions. We analyze the rationale behind this observation in Section 3.2. Then based on the analysis, we propose KIVI, a new KV cache quantization method along with its streaming data structure, detailed in Section 3.3. 3.1. Preliminary Study of KV Cache Quantization As we analyzed in Section 2, KV cache functions as a streaming data structure, where the new tensor arrives se- quentially. Thus, optimization-based methods like GPTQ (Frantar et al., 2022) are unsuitable for quantizing KV cache due to the overhead. To the best of our knowledge, the most flexible way for quantizing KV cache is the round- to-nearest quantization. The B−bit integer quantization- dequantization process can be expressed as: Q(X) = ⌊X − zX sX ⌉, X′ = Q(X) · sX + zX, where zX = min X is the zero-point, sX = (max X − min X)/(2B −1) is the scaling factor, and ⌊·⌉ is the round- ing operation. Here we ignore the batch size for ease of Table 1: The results of simulated KV cache group-wise quantization with various configurations. The group size is set as 32. C stands for per-channel quantization and T stands for per-token quantization. Please check the whole evaluation in Table 3. Llama-2-13B CoQA TruthfulQA 16bit 66.37 29.53 4bit (K - T, V - T) 66.48 29.51 2bit (K - T, V - T) 52.93 24.98 2bit (K - C, V - C) 2.88 0.74 2bit (K - T, V - C) 2.80 0.26 2bit (K - C, V - T) 63.53 28.60 understanding. As shown in Figure 1, X is quantized along either the token or channel dimension group-wisely. Considering the streaming nature of KV cache, previous studies often apply per-token quantization to both key and value cache since the newly quantized KV cache can be naively added to the existing quantized one along the token dimension (Sheng et al., 2023). While per-channel quanti- zation is non-trivial, we have designed a padding method to implement per-channel quantization to explore its effect on both key and value cache. Setting. In Table 1, we show the results of fake KV cache group-wise quantization with different configurations on the Llama-2-13B model for the CoQA and TruthfulQA tasks. We use a group size of 32 for all configurations. Here fake quantization means we simulate the quantization process by first quantizing KV cache into lower precision and then dequantizing it in the attention layer. For per-channel quan- tization, if the number of tokens is not divided evenly into groups, we add zero-padding to ensure it can be grouped perfectly. In this way, we ensure that all tokens in KV cache are quantized for a fair comparison. The detailed experi- mental setting can be found in Section 4.1. Specifically, we observe that: OB 1. When using the commonly used per-token quanti- zation to both key and value caches, INT4 precision can maintain accuracy. However, reducing it to INT2 results in a notable accuracy drop. OB 2. When value cache is quantized per-channel, the accuracy significantly worsens regardless of how key cache is quantized. OB 3. When using a lower numerical precision such as INT2, the most accurate approach is to quantize key cache per-channel and value cache per-token. 3KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5Absolute Value Llama-2-13B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0Absolute Value Llama-2-13B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5 Llama-2-13B Layer 31 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0 2.5 Llama-2-13B Layer 31 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0 2 4 6 8 10Absolute Value Falcon-7B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5Absolute Value Falcon-7B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.0 2.5 5.0 7.5 10.0 Falcon-7B Layer 20 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5 Falcon-7B Layer 20 Head 0 Value Cache Figure 2: Magnitude of key and value cache for Llama-2-13B and Falcon-7B. We observe (1) for key cache, there are a few channels whose magnitudes are very large. (2) for value cache, there is no obvious outlier pattern. 3.2. Why Key and Value Cache Should Quantize Along Different Dimensions? In Table 1, we observe that quantizing key cache per-channel and value cache per-token to 2bit results in a very small accu- racy drop. Here we analyze why this configuration delivers better accuracy. In Figure 2 we visualize the original KV cache distribution at different layers. We observe that in key cache, some fixed channels exhibit very large mag- nitudes, whereas in value cache, there is no significant pattern for outliers. Analysis of Key Cache. The above observation for key cache aligns with previous findings that certain fixed columns in activations exhibit larger outliers (Dettmers et al., 2022; Lin et al., 2023). The persistence of outliers within each channel means that per-channel quantization can con- fine the quantization error to each individual channel with- out impacting the other normal channels. Thus, Figure 2 explains why key cache should be quantized per-channel. In Table 2 we show key cache relative reconstruction error ∥XK−X′ K XK ∥F , along with the relative attention score error ∥A−A′ A ∥F where A′ = Softmax(tQX ′⊤ K ). We observe that the per-token quantization can lead to almost 5× larger at- tention score error than per-channel quantization, which is consistent with Figure 2. Table 2: The relative error statistics averaged over all layers and all heads Llama-2-13B K Per-Token K Per-Channel Avg. ∥ XK−X′ K XK ∥F 13.67 4.55 Avg. ∥ A−A′ A ∥F 47.00 9.60 Attention sparsity 84.3% V Per-Token V Per-Channel Avg. ∥ XV −X′ V XV ∥F 4.57 3.73 Avg. ∆ 3.55 49.89 Analysis of Value Cache. Unlike key cache, value cache does not show the channel-wise outlier pattern. Furthermore, Figure 2 alone cannot explain OB2, which indicates value cache should only be quantized per-token. This is because Figure 2 implies that errors should be comparable for both per-token and per-channel quantization, given the absence of a clear pattern. As shown in Equation (1), value cache is used to calculate the attention output tO. Instead of analyzing the quantization error of value cache XV , in Table 2 we analyze the relative error ∆ = ∥AXV −AX′ V AXV ∥F with different quantization configurations. Surprisingly, we observe that the per-token quantization error is almost 15× 4KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Q_MatMul Concat Quantization by Channel Quantization by Token MatMul Prefill Phase Decoding Phase* KVCache * we omit the value cache and  attention output calculation Full Precision Tensor Low Precision Tensor Figure 3: The overview of KIVI algorithm. For ease of illustration, we omit the value cache and attention output parts. The detailed pseudo-code is provided in Algorithm 1. Here “Q_Matmul” is the mix-precision matrix multiplication which fuses the dequantization with matrix multiplication at the tiling level. smaller than per-channel quantization, which explains why OB2 happens. The intuition behind this observation stems from the attention sparsity. Equation (1) can be written as: [AXV ]i∗ = lpromptX j=1 Aij[XV ]j∗, (2) where [XV ]j∗ is the j-th row of XV . From Equation (2), the attention output is the weighted summation of value cache across various tokens, with the weights being the attention scores. Since the attention score is highly sparse (Tian et al., 2023), the output is just the combination of value caches of a few important tokens. The per-token quantization can confine the error to each individual token. Thus, quantizing other tokens does not affect the accuracy of important tokens. Consequently, per-token quantization leads to a much smaller relative error ∆. 3.3. KIVI: Algorithm and System Support Algorithm. As we previously analyzed, key cache should be quantized per-channel and value cache should be quan- tized per-token. Recall that key and value cache of newly generated tokens arrive sequentially. From the implemen- tation perspective, per-token quantization aligns well with streaming settings, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel quantiza- tion, the quantization process spans across different tokens, which cannot be directly implemented in the streaming set- ting. As shown in Figure 3, our key idea to solve this prob- lem is to group key cache everyG tokens and quantize them separately. Because the number of tokens in XK can be ar- bitrary, we splitXK into two parts, namely, the grouped part XKg = XK[: l − r] and residual part XKr = XK[l − r :], where l is the number of tokens inside the current key cache XK, r is the number of residual tokens, where l − r can be divisible by G. Since XKg can be evenly divided into (l − r)/G groups, we only store Q(XKg ) with group-wise quantization, while XKr is kept in full precision. During the decoding process, each newly arrived key cache tK is added to XKr and once XKr reaches R tokens, which is a hyperparameter - residual length, we quantize and concatenate it with the previously quantized Q(XKG). Then we reset XKr to an empty tensor. We note that R should be divisible by G. With tiled matrix multiplication, the raw attention logits is then calculated as: Ag = tQQ(X⊤ Kg ), XKr = Concat([XKr , tK]), Ar = tQX⊤ Kr , A = Concat([Ag, Ar]). (3) For value cache, similar to key cache, we also split it into two parts and keep the most recent value cache in full pre- cision, namely, XVg and XVr . Specifically, we maintain a queue and each newly arrived value cache is pushed into the queue. Once the queue reaches the predefined residual length R, the most outdated value cache is poped. Then the poped value cache is quantized per-token and concatenated with the previously quantized value cache along the token dimension. As shown in Figure 3, we also emphasize that during the prefill phase, the exact key and value tensors are passed to the next layers, although only the quantized KV cache is retained in memory. The whole algorithm can be found in Appendix A Algorithm 1. Analysis. In KIVI, the grouped key cacheXKg and value cache XVg is quantized, while the residual key cache XKr and value cache XVr is kept in full precision. By design, there are at most R tokens inside XKr or XVr . In practice, we set R ≤ 128 and the sequence length lprompt + lgen is often much longer than R. Thus the memory overhead from XKr and XVr is negligible when considering the benefit from extreme low-bit quantization, especially for the long context scenarios. Also, since the newly arrived key and 5KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache value tensors are added to XKr and XVr in full precision, KIVI maintains a full precision KV cache sliding window for the local relevant tokens. This window size is expected to be R 2 for key cache, and R for value cache. Later in the experiment section, we show that this full precision sliding window is crucial for obtaining desirable perfor- mance on hard tasks, such as GSM8K. System Support. We provide a hardware-friendly imple- mentation for running KIVI on GPUs. To minimize the overhead, we have fused the dequantization process with matrix multiplication, e.g., Q_MatMul in Figure 3, using CUDA. We also implement the group-wise quantization ker- nel in Triton. Our method is fully compatible with weight- only quantization. 4. Experiments 4.1. Settings Models. We evaluate KIVI using three popular model families: Llama/Llama-2 (Touvron et al., 2023a;b), Fal- con (Penedo et al., 2023) and Mistral (Jiang et al., 2023). Llama and Mistral model is based on multi-head attention, while Falcon is based on multi-query attention (Shazeer, 2019). We use the Hugging Face Transformers codebase and implement the KIVI algorithm upon it. Following previous work (Sheng et al., 2023), the group size G in Algorithm 1 for quantization is set as 32 across all experi- ments, the residual length R for key and value cache is set to 128. Tasks. As we analyzed in Section 2, the KV cache size grows larger with a longer context. Thus, we evaluateKIVI under the normal context length and long context setting, respectively. Specifically, we adopt generation tasks from LM-Eval (Gao et al., 2021) for normal context length eval- uation and LongBench (Bai et al., 2023) for long context evaluation, respectively1. For LM-eval, we adopt CoQA (Exact match accuracy), TruthfulQA (BLEU score), and GSM8K (Exact match accuracy). For LongBench, we chose tasks from four subgroups. Specifically, Qasper (F1 score) is a Single-Document QA task; QMSum (ROUGE score) and MultiNews (ROUGE score) are Summarization tasks; TREC (classification score), TriviaQA (F1 score), and SAM- Sum (ROUGE score) are Few-shot Learning tasks; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. The maximum sequence length in LongBench was set to 8192 for the Mistral model and 4096 for other models. We also consider the needle-in-a-haystack 1The closed-end tasks such as MMLU are not ideal to evaluate KIVI since they only involve one decoding step and directly fetch the output logits, which is not suitable for studying the impact of compressed KV cache. task (NIAH) to evaluate the model’s long context retrieval ability after quantizing KV cache. Detailed NIAH setting can be found in Appendix B. 4.2. Accuracy and Efficiency Analysis 4.2.1. C OMPARISON BETWEEN DIFFERENT QUANTIZATION CONFIGURATIONS We first utilize the fake quantization to demonstrate the effec- tiveness of our asymmetric quantization, namely, quantizing key cache per-channel and value cache per-token. Here fake quantization is exactly the same as in Table 1. The results are shown in Table 3. We observe that “2bit (K per-channel, V per-token)” consistently achieves the best results com- pared to all other configurations. This is consistent with our previous analysis. We also note that for hard generation tasks such as GSM8K, the fake “2bit (K per-channel, V per-token)” quantization results are significantly worse than the full precision counterparts. However, for KIVI in Table 3, we observe that the accuracy drop is only around 2% for GSM8K across different models. As we analyzed in Section 3.3, the difference between fake “2bit (K per-channel, V per-token)” quantization and KIVI is that KIVI maintains a full precision key and value cache sliding window for the local relevant tokens. This sliding window is crucial to maintaining accuracy for hard generation tasks such as mathematical reasoning. 4.2.2. A CCURACY COMPARISON ON GENERATION TASKS LM-Eval Results. We benchmark KIVI in CoQA, Truth- fulQA and GSM8K tasks using the LM-Eval framework. All dataset parameters were set to default. We compare the standard 16bit configuration with our KIVI compression techniques in Llama-2-7B, Llama-2-13B, Falcon-7B and Mistral-7B. As shown in Table 3, we observe that for the Llama and Mistral model, KIVI only has up to 2% accu- racy drop despite the KV cache being stored in 2bit. For instance, in the Llama-2-7B model, the transition from 16bit to 2bit only slightly decreases accuracy. Similar trends are observed in other Llama-family models. Since Falcon-7B adopts multi-query attention and only has one head for KV cache, it is already highly compressed compared to Llama- based models. Thus, in Table 3, 4bit KIVI is needed to maintain the accuracy, while 2bit KIVI may have a large accuracy drop in this case. LongBench Results. The performance of KIVI over vari- ous models in the LongBench dataset is summarised in Table 4. We apply KIVI to Llama2-7B, Llama2-13B, Llama2-7B- Chat, Llama2-13B-Chat, Falcon-7B and Mistral-7B. Table 4 suggests that KIVI is an effective method for KV cache compression with minimal impact on accuracy across var- 6KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens (a) Llama-3-8B-Instruct Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (b) Llama-3-8B-Instruct + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (c) Llama-3-8B-Instruct + KIVI-4 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens (d) Mistral-7B-Instruct-v0.2 Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (e) Mistral-7B-Instruct-v0.2 + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (f) Mistral-7B-Instruct-v0.2 + KIVI-4 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: Needle-in-a-Haystack results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2. Here we count the number of words instead of tokens to better account for the tokenizer difference. The final token length is noted in the upper right corners of each figure. Detailed setting can be found in Appendix B. ious hard long context generation tasks. We present ad- ditional results using Llama3-8b, Mistral-7B-v0.2, and LongChat-7B-v1.5, which can be found in Appendix D. NIAH Results. From Figure 4, we observe that KIVI can still maintain the retrieval ability of LLMs even with 2bit KV Cache. Detailed NIAH setting can be found in Appendix B. 4.2.3. A BLATION In this section, we benchmark KIVI on GSM8K, one of the hardest generation tasks, to show the effect of hyperpa- rameters group size G and residual length R on the model performance. For full results of KIVI with a residual length of 32, please refer to Appendix C. The effect of group size. We fix the residual length at 128 and vary the group sizes to 32, 64, and 128. From Table 5, we observe that group sizes 32 and 64 yield similar results, whereas the performance significantly decreases when the group size reaches 128. Note the zero-point and the scaling factor mentioned in Section 3.1 are calculated according to this group size; where the choice of group size will greatly impact the KV cache compression effect under a long input. The effect of residual length. We fix the group size at 32 and vary the residual length across 32, 64, 96, and 128. As shown in Table 5, there is no consistent pattern between residual lengths and model accuracy. Namely, while a resid- ual length of 128 achieves good results, 32 and 96 yield similar outcomes, but a residual length of 64 results in the worst performance. We emphasize that while we observe no significance among residual lengths of {32, 96, 128}, hav- ing a reasonably large residual length is important; as it brings much performance boosts on hard tasks like GSM8K, again as shown in Table 5. 4.2.4. E FFICIENCY COMPARISON To evaluate the wall-clock time efficiency ofKIVI, follow- ing vLLM (Kwon et al., 2023), we synthesize workloads based on ShareGPT (sha, 2023), which contain input and output texts of real LLM services. On average, the data set has an input prompt length lprompt of 161 and an output length lgen of 338 (Kwon et al., 2023). We increase the batch 7KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 3: Performance comparison between 16bit, 4-bit per- token quantization, four fake 2bit KV cache quantization similar to those in Table 1, KIVI-2 (2bit) / KIVI-4 (4bit) across various models. We emphasize that unlike KIVI, which preserves a small portion of full precision key cache XKr and value cache XVr , all tokens in fake KV cache quantization are quantized for a fair comparison. C stands for per-channel quantization and T stands for per-token quantization. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 4bit (K -T, V -T) 64.82 29.85 12.28 2bit (K -C, V -T) 59.08 33.10 5.76 2bit (K -T, V -T) 39.88 18.29 0.83 2bit (K -C, V -C) 3.60 0.27 0.00 2bit (K -T, V -C) 1.30 0.49 0.08 KIVI-4 63.78 30.80 13.80 KIVI-2 63.05 33.95 12.74 Llama-2-13B 16bit 66.37 29.53 22.67 4bit (K -T, V -T) 66.73 29.14 20.92 2bit (K -C, V -T) 63.53 28.60 12.21 2bit (K -T, V -T) 52.93 24.98 4.55 2bit (K -C, V -C) 2.88 0.74 0.00 2bit (K -T, V -C) 2.80 0.26 0.08 KIVI-4 66.38 29.49 23.65 KIVI-2 66.23 29.84 20.77 Falcon-7B 16bit 59.83 23.20 4.55 4bit (K -T, V -T) 58.53 22.94 3.26 2bit (K -C, V -T) 43.93 20.82 1.29 2bit (K -T, V -T) 25.72 0.91 0.53 2bit (K -C, V -C) 41.95 17.11 1.52 2bit (K -T, V -C) 19.53 0.94 0.15 KIVI-4 59.67 22.58 4.47 KIVI-2 57.48 24.98 3.41 Mistral-7B 16bit 67.40 30.45 38.36 4bit (K -T, V -T) 67.80 29.83 36.85 2bit (K -C, V -T) 61.65 29.64 26.46 2bit (K -T, V -T) 54.55 25.86 5.00 2bit (K -C, V -C) 24.40 24.86 2.27 2bit (K -T, V -C) 10.73 19.12 0.99 KIVI-4 66.95 30.49 37.30 KIVI-2 66.35 32.17 36.01 size until out of memory and report the peak memory usage and throughput between KIVI (with residual length 32 and 128) and FP16 baseline for the Llama-2-7B model. The hardware here is a single NVIDIA A100 GPU (80GB). As shown in Figure 5, with similar maximum memory us- age, KIVI enables up to 4× larger batch size and gives 2.35× ∼3.47× larger throughput. This throughput num- ber can grow larger with longer context length and output length. We also note that this speed-up can be greatly increased if we further fuse the KV cache quantization process with previous operations. We leave it as one of future work. Figure 5: Memory usage and throughput comparison be- tween 2bit KIVI and 16bit baseline. KIVI can achieve higher throughput by enabling a larger batch size. 5. Related Work Many machine learning systems and benchmark works con- sider scaling up LLM inference process (Pope et al., 2023; Yuan et al., 2024). Among them, quantization techniques have been widely applied (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023; Xu et al., 2023). A main branch of LLM quantization is weight-only quantization, which in- volves the quantization of model weights to lower precision. For instance, AWQ (Lin et al., 2023) cleverly quantizes model weights to INT4 and INT3 using an activation-aware manner. GPTQ (Frantar et al., 2022) utilizes approximate second-order information to quantize model weights both accurately and efficiently. SqueezeLLM (Kim et al., 2023) adopts the concept of non-uniform quantization based on sensitivity along with dense-and-sparse decomposition. This line of work is orthogonal to ours, as they can be combined. SmoothQuant (Xiao et al., 2023a) is a post-training quan- tization method that is more closely related to our work. This method uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. SmoothQuant can compress KV cache to 8bit with minor performance loss. However, it faces a significant accuracy drop when scaled down to 4bit or less (Zhao et al., 2024). FlexGen (Sheng et al., 2023) adopts 4-bit group-wise quantization for both key and value cache. 8KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 4: Performance evaluation of KIVI on various models across a range of benchmarks in LongBench. We highlight the average performance of our method. More similar results on Mistral-7B-v0.2 and LongChat-7b-v1.5 can be found in Table 10 and Table 9 Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-4 9.28 21.42 3.88 66.00 87.72 41.82 66.80 59.83 44.59 KIVI-2 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-4 9.16 20.86 3.21 69.00 86.97 44.26 65.30 57.08 44.48 KIVI-2 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-4 19.62 20.70 25.49 63.00 84.13 40.87 59.27 53.56 45.83 KIVI-2 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-4 23.00 20.36 26.06 67.50 87.20 42.04 52.55 52.77 46.44 KIVI-2 23.59 20.76 25.25 67.5 87.17 41.56 49.93 48.45 45.52 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-2 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-4 7.89 20.06 20.58 67.50 89.80 41.56 66.45 58.62 46.56 KIVI-2 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 Table 5: Ablation study of KIVI by changing group size G and residual length R. Model Group Size GSM8K Llama2-13B 32 20.77 64 21.00 128 17.29 Model Residual Length GSM8K Llama2-13B 32 20.62 64 19.86 96 20.55 128 20.77 One essential recipe of KIVI is the per-channel quantiza- tion scheme designed based on the observation made in Section 3.2 and Figure 2. ATOM (Zhao et al., 2024) also indicates that key cache exhibits more outliers compared to the value cache. KIVI provides further extensive analysis and leverages this observation to implement per-channel quantization. A similar observation and approach has been independently discovered and developed in the concurrent work KVQuant (Hooper et al., 2024). vLLM (Kwon et al., 2023) and S3 (Jin et al., 2023) are system-level works, which include memory management through the use of PagedAttention or memory usage pre- diction. They can lower the memory requirements of KV cache and simultaneously increase model throughput. This research direction is orthogonal to our work, since system- level optimizations can also be applied upon our algorithm. Several other works also consider compressing KV cache by evicting tokens. H2O (Zhang et al., 2023) retains only a small portion of tokens that contribute significantly to the attention scores. Similarly, Scissorhands (Liu et al., 2024) exploit the persistence of the importance hypothesis in KV cache sparsification. StreamingLLM (Xiao et al., 2023b) is based on the observation of “attention sink” and maintains only a few initial tokens to preserve performance. Unlike these works, our KIVI retains all input tokens and compresses them into lower precision. This line of work is orthogonal to ours, as they can also be combined together. 6. Conclusion and Future Work In this paper, we systematically analyze KV cache element distribution in popular LLMs. We conclude that key cache should be quantized per-channel and value cache should be quantized per token. Based on these observations, we propose KIVI, a plug-and-play 2bit KV cache quantization algorithm without the need for any tuning. In real LLM workload, KIVI allows up to 4× larger batch sizes and 3.47× throughput. In the future, we will further optimize the implementation to reduce the overhead of quantization process during the prefill and decoding phase. Acknowledgments The authors thank the anonymous reviewers for their help- ful comments. Dr. Beidi Chen is supported by a research gift from Moffett.AI. Dr. Vladimir Braverman is partially 9KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache supported by the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technol- ogy (KIAT) through the International Cooperative R&D pro- gram, the Naval Research (ONR) grant N00014-23-1-2737, and NSF CNS 2333887 award. Dr. Xia Hu is supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References ShareGPT Team. https://sharegpt.com/, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, et al. Understand- ing different design choices in training large time series models. arXiv preprint arXiv:2406.14045, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An- thony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A frame- work for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during genera- tive inference for higher throughput. arXiv preprint arXiv:2306.06000, 2023. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and An- shumali Shrivastava. Scissorhands: Exploiting the per- sistence of importance hypothesis for llm kv cache com- pression at test time. Advances in Neural Information Processing Systems, 36, 2024. Amirkeivan Mohtashami and Martin Jaggi. Landmark at- tention: Random-access infinite context length for trans- formers. arXiv preprint arXiv:2305.16300, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outper- forming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 10KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Ja- cob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans- former inference. Proceedings of Machine Learning and Systems, 5, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit- twieser, et al. Gemini 1.5: Unlocking multimodal un- derstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar- tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and ef- ficient post-training quantization for large language mod- els. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Large language models for healthcare data augmentation: An example on patient-trial matching. In AMIA Annual Symposium Proceedings, volume 2023, page 1324. Amer- ican Medical Informatics Association, 2023. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable ap- proaches. arXiv preprint arXiv:2407.01527, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quanti- zation for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196–209, 2024. 11KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache A. Detailed Implementations In this section, we present the algorithm for KIVI as discussed in Section 3.3. Specifically, we provide the pseudocode for KIVI when calculating the attention output in the prefill and decoding phases. Algorithm 1: The KIVI Prefill & Decoding Algorithm parameter: group size G, residual length R procedure Prefill: Input: X ∈ Rlprompt×d XK = XWK, XV = XWV XVg = XV [: lprompt − R], XVr = XV [lprompt − R :] Q(XVg ) ← GroupQuant(XVg , dim=token, numGroup=d//G) Q(XKg ), XKr ← KeyQuant(XK) KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return XK, XV end procedure Decoding: Input: KV cache, t ∈ R1×d tQ = tWQ, tK = tWK, tV = tWV Q(XKg ), XKr , Q(XVg ), XVr ← KV cache XKr ← Concat([XKr , tK], dim=token) XVr ← Concat([XVr , tV ], dim=token) if len(XKr ) = R then Q(XKr ), _ ←KeyQuant(XKr ) Q(XKg ) ← Concat([Q(XKg ), Q(XKr )], dim=token) XKr ← empty tensor. end if len(XVr ) > Rthen Q(XV ′r ) ← GroupQuant(XVr [: −R], dim=token, numGroup = d//G) Q(XVg ) ← Concat([Q(XVg ), Q(XV ′r )], dim=token) XVr ← XVr [−R :] end A ← Concat([tQQ(XKg )⊤, tQX⊤ Kr ], dim=token) Ag = Softmax(A)[: −R], Ar = Softmax(A)[−R :] tO ← AgQ(XVg ) + ArXVr KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return tO end function KeyQuant(XK ∈ Rl×d): r = l%R, XKg = XK[: l − r], XKr = XK[l − r :] Q(XKg ) ← GroupQuant(XKg , dim=channel, numGroup=l//G) return Q(XKg ), XKr end 12KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache B. NIAH Setting We largely follows the passkey retrieval prompt template of Mohtashami and Jaggi (2023) but using 7-digit passkey and Paul Graham Essays2 as the background filler, as set forth in Arize-ai and Reid et al. (2024): There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. <prefix filled by Paul Graham Essays> The pass key is <7-DIGIT PASS KEY>. Remember it. <7-DIGIT PASS KEY> is the pass key. <suffix filler> What is the pass key? The pass key is C. More Ablation Results In our efficiency evaluation, we observe that with a residual length of 32, KIVI achieves a significantly higher memory compression rate, which in turn leads to increased throughput. Additionally, our ablation study reveals that changing the residual length from 128 to 32 does not result in a substantial performance gap. We demonstrate KIVI with a residual length of 32 across all benchmark datasets. As shown in Tables 6 and 7, KIVI with a residual length of 32 also delivers performance comparable to that of the 16-bit full model. Table 6: Performance comparison between 16bit, KIVI-2 (2bit) / KIVI-4 (4bit) with residual length 128 and 32 across various models. R32 stands for residual length 32. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 KIVI-2 R128 63.05 33.95 12.74 KIVI-2 R32 62.85 33.01 13.57 Llama-2-13B 16bit 66.37 29.53 22.67 KIVI-2 R128 66.23 29.84 20.77 KIVI-2 R32 66.57 29.35 20.62 Falcon-7B 16bit 59.83 23.20 4.55 KIVI-4 R128 59.67 22.58 4.47 KIVI-4 R32 59.73 22.96 3.94 KIVI-2 R128 57.48 24.98 3.41 KIVI-2 R32 57.50 25.70 2.20 Mistral-7B 16bit 67.40 30.45 38.36 KIVI-2 R128 66.35 32.17 36.01 KIVI-2 R32 65.90 31.21 34.34 D. More Experimental Results We present additional results using Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5 in LongBench, which can be found in Table 8, Table 9 and Table 10, respectively. We also show result of Needle-in-a-Haystack Test in Figure 4. The settings largely follow the format of the original passkey retrieval task (Mohtashami and Jaggi, 2023) while including some modern modifications set forward by Arize-ai and the technical report of Gemini 1.5 (Reid et al., 2024). 2https://paulgraham.com/articles.html 13KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 7: Performance evaluation of KIVI with residual length 128 and 32 on various models across a range of benchmarks in LongBench. R32 stands for residual length 32. Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-2R128 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 KIVI-2R32 9.26 20.53 0.97 66.00 87.42 42.61 66.22 59.67 44.08 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-2R128 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 KIVI-2R32 8.38 20.74 7.01 69.50 87.78 44.43 64.89 55.31 44.75 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-2R128 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 KIVI-2R32 19.10 20.08 25.33 63.00 85.04 39.80 57.91 52.38 45.33 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-2R128 23.59 20.76 25.25 67.50 87.17 41.56 49.93 48.45 45.52 KIVI-2R32 23.56 20.90 25.45 67.50 87.42 41.40 48.93 48.81 45.49 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4R128 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-4R32 1.03 2.45 11.99 13.50 5.84 2.46 23.88 9.95 8.88 KIVI-2R128 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 KIVI-2R32 2.28 3.23 6.73 10.00 6.31 2.88 22.71 10.45 8.07 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-2R128 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 KIVI-2R32 6.84 19.81 17.20 66.50 89.63 42.82 65.13 58.06 45.74 Table 8: The results of Llama-3-8B-Instruct with KIVI on LongBench. The model has 8K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.71 44.24 44.54 46.82 21.49 36.42 30.03 22.67 w./KIVI-2 21.35 43.17 44.49 46.79 20.56 37.05 29.98 22.07 w./KIVI-4 21.01 44.83 44.60 46.96 21.43 36.48 30.22 22.44 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.79 57.00 51.22 90.23 42.53 74.50 67.00 45.21 w./KIVI-2 27.77 50.84 46.65 90.54 42.26 74.50 67.50 44.37 w./KIVI-4 27.97 57.36 52.03 90.33 42.97 74.50 66.50 45.31 Table 9: The results of Mistral-7B-Instruct-v0.2 with KIVI on LongBench. The model has 32K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.02 29.41 47.13 36.53 19.13 21.76 32.59 23.99 w./KIVI-2 20.61 28.73 44.88 35.47 17.95 20.68 32.55 23.65 w./KIVI-4 20.97 29.41 46.52 36.25 19.53 21.66 32.97 24.06 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.09 53.49 51.40 86.23 43.04 71.00 89.33 43.54 w./KIVI-2 26.54 53.03 51.16 86.00 43.34 71.00 80.83 42.43 w./KIVI-4 26.89 53.33 51.41 86.23 43.34 71.00 89.42 43.53 14KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 10: The results of LongChat-7B-v1.5-32K with KIVI on LongBench. The model has 32K context length. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 20.65 29.42 43.15 33.05 14.66 24.14 30.85 22.84 w./KIVI-2 20.79 28.69 41.02 32.91 13.82 23.00 30.47 22.59 w./KIVI-4 20.49 28.90 43.24 33.07 14.66 24.86 31.40 22.84 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 26.55 54.83 58.94 83.99 40.75 66.50 30.50 38.72 w./KIVI-2 26.28 54.11 57.62 83.19 41.28 66.50 32.25 38.30 w./KIVI-4 26.52 54.06 58.77 83.88 40.62 67.00 31.50 38.79 15",
      "meta_data": {
        "arxiv_id": "2402.02750v2",
        "doi": "10.13140/RG.2.2.28167.37282",
        "authors": [
          "Zirui Liu",
          "Jiayi Yuan",
          "Hongye Jin",
          "Shaochen Zhong",
          "Zhaozhuo Xu",
          "Vladimir Braverman",
          "Beidi Chen",
          "Xia Hu"
        ],
        "published_date": "2024-02-05T06:06:47Z",
        "pdf_url": "https://arxiv.org/pdf/2402.02750v2.pdf"
      }
    },
    {
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",
      "abstract": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
      "full_text": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models Akide Liu1 Jing Liu1 Zizheng Pan1 Yefei He2 Gholamreza Haffari1 Bohan Zhuang1,2† 1ZIP Lab, Monash University, Australia 2ZIP Lab, Zhejiang University, China Abstract A critical approach for efficiently deploying computationally demanding large lan- guage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we in- troduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our Mini- Cache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evalua- tion of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02×, enhances inference throughput by approximately 5×, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance. Project is available at https://minicache.vmv.re 1 Introduction Large Language Models (LLMs), exemplified by the GPT series [ 1, 2, 3] and the LLaMA series [4, 5, 6], have emerged as pivotal innovations within the artificial general intelligence, significantly enhancing the capabilities of natural language processing. However, these models are meticulously trained using extensive computational resources [7] and massive datasets [8], which enables them to produce text that effectively mimics human writing styles and conducts complex reasoning analysis, yet raises the challenge of efficient deployment and serving. Within the inference framework of †Corresponding author. Email: bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14366v2  [cs.CL]  7 Sep 2024(a)Cross-layer KV cachesimilarity (b)Merged layersvs EMscore on GSM8K Layer𝑙−1 Layer𝑙−2  ... Layer2 Layer1 LMHead Input Layer𝑙−3 Pruning/Quant. K VKVCacheCompression T Decoding Cross Layer Merging K V QK V Attention Decoding KVCacheCompression QK V Attention T+1 Prevs. MiniCache (c)Comparisonbetween MiniCacheand previous methods ...... CosineSimilarity  T T+1 Number of Layers Merged on LLaMA-3-70B Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle to deep layers. The x-axis uses index/2 to represent the similarities for each pair of layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding. LLMs, KV caches [9] are crucial for storing pre-computed keys and values, thus avoiding repeated calculations over the preceding context and substantially enhancing LLMs’ deployment efficiency. However, the increasing demand for longer sequence lengths results in voluminous cached states, leading to significant memory consumption during generation. For instance, a 175B GPT-3 model [2], with a batch size of 64 and a sequence length of 4,096 tokens (both prefilled and generated), requires approximately 1,208GB of GPU memory. This requirement is3.45× greater than the memory used to store the model’s weights. In this context, KV cache compression is of paramount importance due to its clear benefits: 1) it largely reduces the memory footprint allowing for faster generation and larger batch serving; 2) it significantly lowers the cost per token, demonstrating substantial commercial benefits. Existing KV cache compression efforts can be roughly categorized into two types, namely quantization and sparsity. The quantization approaches [11, 12] propose storing the KV states in low-bit numerical values. Typically, FlexGen [13] demonstrates that 4-bit KV cache quantization can achieve lossless performance. In contrast, sparsity-driven methods aim to retain only the salient tokens while evicting the rest, either heuristically [14, 15] or adaptively [16]. Some approaches [11] explore the intersection of these two types, by assigning high-bit to salient tokens and extremely low-bit to the rest of the tokens, achieving more aggressive memory gain. Despite these innovations, existing literature merely consider the intra-layer redundancy, while neglecting another important complementary direction – the inter-layer redundancy, as illustrated in the Figure 1(c). Our analysis begins by exploring the redundancy of KV cachesalong the depth dimension, as shown in Figure 1(a). We observe that KV cache states exhibit high similarity between neighbouring layers in 2the middle-to-deep portions of LLMs. This intriguing property suggests that states paired by position between adjacent layers can be accurately merged into a single state space with a strong performance guarantee, as illustrated in Figure 1(b). This approach significantly reduces the memory footprint without the need to retain individual states for each attention layer. Note that these observations are pertinent to dynamic inference strategies such as mixture-of-depths [17] and layer-wise early exiting [18, 19], which optimize computational paths by skipping non-critical layers to enhance training and inference efficiency. Furthermore, layer pruning methods [20] highlight considerable redundancy in deeper layers. However, despite these advancements, the redundancy of KV caches along the depth dimension has largely been overlooked. In this paper, we propose MiniCache, a simple yet effective cross-layer KV cache compression method aimed at advancing the inference efficiency of LLMs. MiniCache consists of two essential components. Firstly, we introduce an accurate cache merging strategy, employing a reparameteri- zation of state vectors that decompose them into the magnitude and direction components, akin to weight normalization [21]. This approach allows for effective interpolation of the directional compo- nent in polar coordinates while preserving the original state norms to retain as much information as possible. This interpolation refers to the cross-layer merging as shown in the Figure 1(c). Secondly, we recognize that a small subset of state pairs, characterized by low similarities but carrying largely distinct semantic meanings, are unsuitable for inter-layer merging. To address this, we propose a token retention strategy to minimize performance degradation, which involves separately retaining these outlier pairs. Our framework is notably memory-efficient, requiring storage for only a single high-dimensional directional component, along with minimal extra memory overhead. The over- head consists of a few unmergeable tokens and their corresponding indexes, as well as token-wise magnitudes to accurately restore the original states. We conduct extensive experiments with representative LLMs, including Mixtral-8x7B [ 22], Phi- 3-Mini [23], and LLaMA-3 [ 6] 8B and 70B, respectively. Our method is benchmarked across a diverse range of question answering and generation datasets [24, 25, 26, 27, 28, 29, 30, 31] using the lm-eval-harness [32]. Additionally, we evaluate our results on LongBench [33] for long-sequence generation. The results demonstrate that MiniCache can reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately 5× compared to fully cached baseline, clearly surpassing existing methods [11, 12, 14, 15]. Our contributions are summarized as follows: • We introduce MiniCache, a simple yet highly effective framework for KV cache compression. MiniCache pioneers the exploration of KV cache compression along the depth dimension, thereby significantly expanding its capabilities. • We observe a fascinating characteristic of cross-layer KV cache states: high similarity between adjacent layers in the middle to later stages of LLMs. Additionally, we find that not all state pairs are suitable for merging. • We propose an accurate and memory-efficient method for cross-layer cache merging, comprising a reparameterization strategy and a token retention mechanism. Our method complements existing KV cache compression approaches, further enhancing LLM serving efficiency. • Our MiniCache performs favourably against the state-of-the-art methods. Notably, our 4-bit MiniCache achieves a strong compression ratio of up to5.02×, 5× higher inference throughput, and 41% memory reduction compared to the FP16 full cache baseline with near-lossless performance. 2 Related Work Efficient inference for LLMs. Large Language Models (LLMs) are constrained by considerable computational and memory requirements during inference, particularly in resource-constrained environments. To mitigate these challenges, a variety of efficient inference techniques have been developed. For instance, dynamic inference methods [ 18, 34, 35, 36], represented by mixture-of- experts (MoE) [37, 38, 39, 40, 41], adaptively select specific sub-structures of the model during the inference process based on the input data, significantly improving inference efficiency while keeping model capacity. Techniques like Multi-Query Attention [42, 43], Kernel-driven attentions [44, 45, 46, 47], and low-rank attentions [41, 48, 49, 50] approximate the functionality of traditional attention mechanisms but with more efficient implementations. Quantization strategies [51, 52, 53, 54] 3involve converting the model’s weights and activations into a low bit-width format, thereby reducing memory footprint and computational intensity. Sparsification approaches [14, 15, 55, 56] eliminate unnecessary elements in both model weights and token representations, further enhancing efficiency. Some closely related works, such as MoD [17] and LayerSkips [19], considered the dynamic inference nature to ignore unimportant layers according to input. However, these methods require an additional fine-tuning process or carefully designed pre-training stages, which reduces the adaptivity of these methods. MiniCache relies on inter-layer similarity observations to perform cross-layer merging, significantly reducing memory demand. Model merging. Merging compression involves the aggregation of a model’s parameters and activations at various granularities. This process enhances the efficiency of inference in large models and facilitates huge redundancy [57]. Linear Mode Connectivity (LMC) [58] enables the fine-tuning of models from a shared pre-trained base. Commonly, weight averaging [ 59] is employed as an efficient technique to perform merge compression. Notably, Model Soup [60] utilizes linear averaging in this context. Advanced methods like TIES Merging [61], Model Breadcrumbs [62], and DARE [63] further enhance this process by sparsifying and combining model parameters, enabling the merging of models without sacrificing performance capabilities. Additionally, Spherical Linear intERPolation (SLERP) [64] extends beyond simple weight averaging by interpolating between model parameters. The Fisher information matrix [65] and RegMean-based methods [66] further optimize merges to produce ideal weights, minimizing the ℓ2 distance to generation outputs while preserving the privacy of the training data. However, most existing works focus on merging model parameters, with the concept of depth-wise mergeability not being thoroughly explored in prior research. MiniCache focuses on the KV cache token merging in the depth dimensional of LLMs. 3 Motivation In the below, we present our new observations in a novel cross-layer perspective. LLama 2 7BLLama 2 30BLLama 3 8BMixtral 8x7B Models 0 10 20 30 40 50Exact Match (%) Baseline Mean KV (a) Simple average baseline vs. full cache on GSM8K 0 20 40 60 80 100 T okens Index 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Cosine Similarity Layer 16 - 17 Layer 18 - 19 Layer 20 - 21 Layer 22 - 23 Layer 24 - 25 Layer 26 - 27 Layer 28 - 29 Layer 30 - 31 (b) Pairwise similarity in adjacent layers KV cache MathQA OpenBookQA PiQA RTE Winogrande 0.00.20.40.60.8 MiniCache Baseline Mean (c) Benchmark on five QA datasets Figure 2: Overall of our explorations and observations : (a) shows the strong baseline by performing average merging on the KV cache. (b) shows the pairwise similarity of cache states between adjacent layers. (c) compares the MiniCache, simple average, and full cache baseline across five different datasets. 3.1 Cross-Layer Redundancy in KV Cache Prior studies have revealed the ineffectiveness of middle-to-deep layers in LLMs [20]. Thus, layer- wise early exiting in this case can effectively avoid the redundant computation with minor effect on LLM performance [19, 67]. Inspired by this, we explore a layer-wise merging of KV cache in LLMs, starting with a simple baseline by averaging full tokens across adjacent layers. We provide our key observations as follows. Observation 1: KV cache shares a high similarity between adjacent layers. Based on LLaMA-3- 70B [6], we conduct zero-shot inference on the validation sets of three widely recognized benchmarks: COQA [68], GSM8K [10] and TruthfulQA [69]. In general, we find that KV cache in the shallow layers exhibits low similarity, whereas those in the middle-to-deep layers are very similar to one another based on angular distance, as shown in Figure 1(b). Next, we merge the KV cache across 4adjacent layers by conducting five-shot inference with LLaMA-2-7B, LLaMA-2-13B [5], LLaMA- 3-8B [6], and Mixtral-8x7B [22] on GSM8K [10]. Specifically, starting from the middle layer of each model, we merge the KV cache in the adjacent two layers. As shown in Figure 2(a), we observe a favourable performance for different LLMs, which reveals the huge potential for efficiency improvement by sharing the KV cache across adjacent layers during LLM decoding. Observation 2: Not all tokens are equally important to merge, a few distinct pairs require retention. Recent works [15, 16] in KV cache compression have found that keeping a few salient tokens at each layer, which contribute the majority of attention scores, could be sufficient to maintain LLM performance. In our case, we speculate that certain token pairs in adjacent layers also exhibit outlier behaviours, showing strong semantic differences that make them unsuitable for merging. Based on COQA [ 68] and LLaMA-2-7B [ 5], we investigate the similarities at the level of token pairs. As shown in Figure 2(b), we find a significant portion of token pairs share high similarities across adjacent layers. However, we observe a few outlier pairs, such as indices 0 and 15, with large margins of difference. We consider these tokens non-mergeable due to their significant differences. We also show that merging these distinct tokens results in performance degradation, corresponding to γ = 0 row in Table 2. Thus, while cross-layer merging is a promising strategy for reducing memory burdens, it must be implemented with careful consideration of token-level similarities to ensure optimal performance, as shown in Figure 2(c). 4 Method In this section, we introduce our MiniCache, a simple yet effective method aimed at trimming the KV cache redundancy in the depth dimension. This framework exploits the high similarity of KV cache states between adjacent layers and consists of two primary components: a reparameterization-based merging strategy and a token retention mechanism. The merging strategy compresses the KV cache states in adjacent layers to aggregate them into a single shared memory space, beginning from the middle of the model. The token retention mechanism mitigates information lost by retaining the highly distinct state pairs with minimal additional memory cost. With the merged cache, retention tokens, and magnitudes, we can accurately restore the original cache states for token decoding. 4.1 Cross-Layer Compression Our method commences with the identification of an optimal starting layer S. Observations in Section 3.1 indicate that the KV cache from middle-to-deep layers consistently exhibits patterns of high similarity across adjacent layers. Consequently, we select the starting layer from the middle of the LLM, specifically S = L/2. From this layer onward, the KV pairs are assumed to be sufficiently similar across adjacent layers to warrant their consolidation. Central to this approach is a merge function, F, which is designed to integrate the KV caches of consecutive layers into a single, unified cache. We define x as the vectorized cache state of a single token, where the superscript indicates the layer index and the subscripts k and v denote the keys and values, respectively. Specifically, for a pair of key/value tokens at the same position in layers l and l − 1, the merged cache is computed as cl,l−1 k = F(xl k, xl−1 k ), cl,l−1 v = F(xl v, xl−1 v ). (1) This consolidation process effectively eliminates the need to store and process the original memory- intensive keys and values in each layer independently. Instead, it approximates a shared cache across the adjacent layers. 4.2 KV Cache Merging and Restoration Reparameterization-based cache merging. To perform the pairwise merging, one solution is to directly average a pair of KV tokens, analogous to model merging [60, 61]. However, we observe that direct averaging can cause significant information loss. We conjecture that the distance between activations can be larger than that of weights due to the presence of outlier activation channels with extremely large magnitudes in LLMs [70, 71], while weights typically have relatively quite small magnitudes. A potential method to compensate for this information loss is to project from c to xl−1 and xl, then rescale the projected vectors based on their relative magnitudes to exactly restore the 5(a)Cross-layer Compression KV Store𝐶 Keep Rescale Recover (b)Restoration𝑙−1𝑙 𝑙−1 𝑙 Keep × Fetch𝐶\tforlayer 𝑙and𝑙−1KVcachecompressionat layer𝑙 original KV Cache merged KV Cache retentiontokencachemagnitudemergeoperation Figure 3: The illustration of the proposed methodMiniCache. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers l and l − 1, and merge them into shared states via Eq. (3). Additionally, we compute the ℓ2 norm for the caches to obtain their magnitudes. Furthermore, we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer l in C. (b) illustrates the restoration process for layers l and l − 1, which includes magnitude rescaling in Eq. (2) and retention token recovery. original states. However, this approach requires extensive additional storage and computations; for example, restoring xl−1 needs both c and xl, which undermines the benefits of cache merging. To efficiently merge token pairs, we draw inspiration from weight normalization [21], which disentangles model parameters into the magnitude and direction components to accelerate the convergence of stochastic gradient descent. Additionally, we take cues from DoRA [72], which employs a similar way to resemble the learning behavior of parameter-efficient fine-tuning compared to full fine-tuning. In our case, the reparameterization can be formulated as follows: ˆxl = el,l−1 · ∥xl∥ ∥el,l−1∥, ˆxl−1 = el,l−1 · ∥xl−1∥ ∥el,l−1∥, (2) where e is the directional vector. This decomposition ensures that el,l−1 ∥el,l−1∥ is a unit vector, and allows the restored states to match the ℓ2 norm of the original states, thereby preserving the cache’s information as much as possible. The restoration is shown as Figure 3(b). For brevity, we omit the subscripts k and v, as keys and values are decomposed in the same way. For estimating the directional component el,l−1, we follow SLERP [64], which adaptively handles the interpolation, which often resembles rotation-like transformations. The choice of SLERP as the merging function is strategic, as it facilitates interpolation along the shortest path on the unit sphere between two high-dimensional vectors, thereby preserving their geometric integrity, which refers to merge operation in Figure 3(a). This is crucial for maintaining the semantic and syntactic properties of the KV caches. The formula for SLERP in our context is: el,l−1 = sin((1 − t)Ωl,l−1) sin(Ωl,l−1) · xl−1 ∥xl−1∥ + sin(tΩl,l−1) sin(Ωl,l−1) · xl ∥xl∥, (3) where Ωl,l−1 = arccos \u0010 xl·xl−1 ∥xl∥∥xl−1∥ \u0011 represents the angle between vectors xl and xl−1, and sin(·) is the sine function. t is an interpolation hyperparameter that adjusts the relative influence of each vector on the resulting direction, tailored to the layer depth and specific characteristics of the KV pairs. Note that when we set t = 0.5, it will become an average merging along the geometry surface, which we consider special cases in Eq. (A). The merged cache for each token pair is then a concatenation of the directional vector, magnitude and Ωl,l−1, denoting as cl,l−1 = [el,l−1, ∥xl−1∥, ∥xl∥, Ωl,l−1], cached components as shown in Figure 3(a). Note that apart from storing the merged directional vector, we only need to store additional token-wise magnitude and angle scalars, which is memory efficient. In this way, we achieve substantial memory efficiencies through reduced redundancy while ensuring the retention of the critical functional characteristics of the original KV pairs across transformer layers. Unmergeable token retention. Highly distinct pairs are sensitive to merging operations, leading us to propose unmergeable token retention, as shown in Figure 3(a). Despite the high similarity between KV cache states across neighbouring layers, a few sensitive distinct pairs remain that are significantly 6difficult to merge and share. Aligned with previous studies, these distinct tokens carry substantial semantic meanings [15, 16]. We observe that merging sensitive tokens, which results in the loss of layer-specific information, can lead to significant performance degradation. Therefore, it is crucial to properly disentangle the shared and unique information between adjacent layers. To address this issue, we designed a token retention strategy to selectively retain tokens that cannot be merged based on their angular distance, defined as: d(xl, xl−1) = 1 π Ω. For the KV caches, the minimum and maximum angular distances are determined to identify the unmergeable tokens. The set of required token indices to keep, I, is obtained by: I = {i | di < dmin + (dmax − dmin) · γ}, (4) where γ is a predefined hyperparameter that controls the retention threshold. The tokens with indices in I are retained and not compressed during the merge, which ensures that performance does not decline by preventing the loss of unmergeable tokens. Next, let X ∈ Rn×h be either the key or value cache at one attention layer, where n denotes the number of tokens and h is the number of hidden dimensions, and E ∈ Rn×h be the shared KV cache states. For each pair of neighbouring two layers, the unmergeable tokens are selected along with the token dimension by Rl = Xl[I], Rl−1 = Xl−1[I], then restoring to our compressed caches by ˆXl[I] = Rl, ˆXl−1[I] = Rl−1, as shown in Figure 3(b). Overall, we share the final cache for the two layers as Cl,l−1 = [El,l−1, Rl, Rl−1, ∥Xl−1∥, ∥Xl∥, I]. This cache includes the shared KV cache states, retention of unmerged tokens, magnitude vectors for each layer, and the token-keeping index, respectively. These additional components are quite lightweight. Thus compared to full-layer caches, our method remains memory-efficient, as discussed in Sec. 4.3. Cache restoration. After obtaining the shared cacheCl,l−1, we further need to approximately restore the original cache states for the current token decoding, as shown in Fig. 3(b). Specifically, to restore Xl, we first rescale the directional shared states with the corresponding magnitude vector along the token dimension, denoted as El,l−1∥Xl∥. Subsequently, we perform retention token recovery by placing the sensitive tokens according to their token indices. 4.3 Efficiency Discussion Compression efficiency. We primarily analyze our memory efficiency in terms of the number of tokens used. Next, let r be the number of layers and and b is the batch size, s and n are input and output sequence length respectively. We consider the FP16 storage for KV cache. The full cache memory usage is given by 4brh(s + n). In our study, we begin merging layers from the middle to the deeper layers, consolidating the KV cache states of every two layers into a single shared state space. As a result, we effectively reduce the GPU memory usage in the decoding inference to 3brh(s + n), demonstrating a significant compression rate. Restoration efficiency. We then analyze the additional memory cost incurred during the restora- tion process, which During the magnitude rescaling phase, we save an additional norm vector for the corresponding layers in the KV cache. It is important to note that the norm vector is in the shape of Rb×s×1, which has a single channel dimension compared to the fully ranked original KV states. Additionally, we suppose that the retention threshold can be set to 0.05. Therefore, we have brh(0.05(s + n)) tokens retained without compression. Finally, our overall memory requirement is given by (3.1h + 2)br(s + n). The detailed derivation is shown in the Appendix E. 5 Experiments We demonstrated that our MiniCache can perform merging compression on the latter half of the layers of LLMs with minimal performance degradation. Implementation details. Our experiments are based on representative model families of LLMs, including a compact LLM Phi-3-Mini [23] and an MoE LLM Mixtral-8x7B [22]. Additionally, we adopt LLaMA-3 [6] 8B and 70B models to explore how our method generalizes to larger LLMs. We sample ten tasks from lm-eval-harness [32], including COPA [24], MathQA [25], OpenBookQA [26], PIQA [27], RTE [28], WinoGrande [29], XSUM [30], and CNN/Daily Mail [31]. We also evaluate long-sequence generation on LongBench [33]. We compare our method with a fully cached baseline, 7and other methods such as round-to-nearest quantization (RTN) [73], SmoothQuant [70] and KIVI [11]. For the proposed MiniCache, we set the interpolation parameter t to 0.6, indicating that the merged results have a smaller rotation angle to the next layer. Furthermore, we set the token retention threshold γ to 0.05, according to the statistics of unmergeable tokens across multiple datasets. In addition to our merging method, we also consider a strong baseline of average merging. For sequential loading of large models, we utilize NVIDIA 4 A100 80GB GPUs, more details refers to Appendix D. Main results. We evaluate MiniCache by merging KV caches across all layers on GSM8K, COQA, and TruthfulQA. The results are shown in Figure 4. In general, we demonstrate the general effective- ness of merging KV caches from middle-to-deep layers across different sized LLMs. Moreover, the proposed MiniCache demonstrates a consistent and significant advantage over the averaging baseline. We also illustrate the performance of merging KV caches across half of the layers with the blue lines, where MiniCache still maintains a robust performance and achieves the best compression ratio. Besides, we find that our method is even more effective for larger LLMs. For instance, based on LLaMA-3-70B, MiniCache shows nearly zero performance drop even with the KV cache in 87.5% of the layers merged on the COQA dataset. This highlights the adaptability and efficiency of our approach in handling large-scale models while ensuring minimal performance degradation. LongBench. We also conduct experiments to evaluate performance and quality in long-sequence gen- eration using the LongBench dataset [33], as shown in Table 1. Our experiments applied MiniCache over several models: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, and Mistral-7B-Instruct. It is important to note that our MiniCache method maintains orthogonality with all existing quan- tization and sparsity (refers to Table A) methods at both the model and token-wise levels. When combined with KIVI-4bit KV cache quantization, our approach achieves a compression rate of5.02×, with minimal impact on accuracy across various challenging long-context generation tasks. The combination of MiniCache and KIVI-4bit KV cache quantization demonstrates significant memory savings without compromising the model’s ability to handle long sequences effectively. This high- Figure 4: Performance comparisons between our proposed MiniCache with the “averaging baseline” and the “unmerged full cache baseline” on multiple datasets with Phi3-Mini, Mixtral-8x7B, LLaMA- 3-8B, and LLaMA-3-70B. More result details are shown in Appendix F. The x-axis indicates the number of layers merged. As more layers are merged, a greater reduction in memory usage is achieved. 8Table 1: Evaluation of different KV cache compression methods on LongBench. MiniCache builds on top of 4-bit KIVI [11] and achieves the best performance with the strongest compression rate. Model Method LCC RepoBench-P PR-en TREC 2wikimqa GovReport MQA-zh AverageCompressionRatio Llama-2-7B-Chat Baseline 58.16 52.19 10.12 64.00 31.12 27.09 10.12 36.41 1xRTN [73] 15.44 8.76 0.79 4.00 0.30 1.93 0.07 4.90 3.21xSmoothQuant [70] 35.31 32.18 0.79 28.75 7.45 11.83 1.68 16.28 2.15xKIVI-2 [11] 49.32 43.71 4.50 63.00 24.07 24.73 10.24 31.51 3.95xMiniCache 58.03 52.01 9.00 64.00 30.58 25.32 10.13 35.44 5.02x Llama-2-13B-Chat Baseline 48.06 50.08 14.25 68.50 13.09 27.76 7.23 32.71 1xRTN [73] 20.89 18.62 0.33 0.00 0.52 1.68 0.16 6.03 3.21xSmoothQuant [70] 32.17 33.86 2.65 48.00 3.53 12.47 0.47 19.16 2.15xKIVI-2 [11] 48.60 48.81 13.50 68.00 14.32 25.70 7.01 32.42 3.95xMiniCache 48.75 48.59 13.00 68.00 14.36 26.57 7.99 32.61 5.02x Mistral-7B Baseline 68.06 60.46 17.71 68.00 10.87 20.09 17.10 37.33 1xRTN [73] 27.98 26.18 3.34 13.00 1.11 2.49 0.45 10.51 3.21xSmoothQuant [70] 40.63 35.14 3.40 30.50 6.03 5.00 4.12 17.55 2.15xKIVI-2 [11] 65.16 58.33 12.43 65.00 11.03 13.22 13.87 33.43 3.95xMiniCache 68.89 60.98 13.92 67.00 10.50 18.06 7.88 35.75 5.02x Mistral-7B-Instruct Baseline 55.51 48.96 60.00 71.00 27.33 32.85 42.74 48.32 1xRTN [73] 32.36 33.23 0.67 1.00 2.25 10.03 2.30 11.55 3.21xSmoothQuant [70] 43.84 38.63 4.79 39.50 10.34 23.61 8.33 24.43 2.15xKIVI-2 [11] 53.13 48.60 47.50 69.00 20.68 29.37 33.88 43.74 3.95xMiniCache 54.79 51.02 64.14 71.00 24.97 31.46 27.54 46.99 5.02x lights the potential of our method to optimize large language models for tasks requiring extensive context, making them more efficient and scalable for real-world applications. Efficiency analysis. To assess the acceleration capabilities of MiniCache, we conduct evaluations based on the methodologies employed in vLLM [74] and KIVI [11]. We generate synthetic workloads derived from ShareGPT, which include real input and output texts from LLM services. The dataset features an average input prompt length of 161 tokens and an output length of 338 tokens. Using the LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU, we benchmark our method in a batch-serving scenario, comparing peak memory usage and throughput among 2-bit KIVI, 4-bit MiniCache, and an FP16 baseline. As illustrated in Figure 5, with a batch size of 128, MiniCache reduces memory usage by 25GB, achieving a 41% memory saving . In terms of throughput, MiniCache outperforms the FP16 baseline by approximately 5×. Additionally, despite utilizing 4-bit quantization, MiniCache benefits from merging and sharing KV caches across adjacent layers, resulting in a 1.29× higher throughput compared to the 2-bit KIVI. These results demonstrate that MiniCache offers a state-of-the-art trade-off between efficiency and performance. 50 100 150 200 250 300 Batch Size 20 30 40 50 60 70 80Peak Memory Usage (GB) Baseline FP16 KIVI 2 MINICache 4 (a) BS. vs. Peak Memory Usage 50 100 150 200 250 300 Batch Size 1000 1500 2000 2500 3000Throughput (tokens/sec)  Baseline FP16 KIVI 2 MINICache 4 (b) BS. vs. Decoding Throughput Figure 5: Memory usage and throughput comparison between our 4-bit MiniCache, 2-bit KIVI, and 16-bit Baseline. MiniCache can achieve higher throughput by enabling a larger batch size while reducing memory footprints via LLaMA-2-7B [5]. 0.3 0.4 0.5 0.6 0.7 Interpolation Parameter t 0.350 0.375 0.400 0.425 0.450 0.475 0.500Exact Match Scores GSM8k 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Normalized Frequency Frequency Figure 6: LLaMA-3-8B [ 6] to experiment on the GSM8K [10]. The right axis is the normalized frequency of the relative magni- tude ratio. Optional t shows a strong correlation with frequency. 6 Ablation Study Table 2: Comparisons of various token retention thresholds γ by LLaMA-2-7B [5] on three benchmarks. γ COQA GSM8K TruthfulQA 0 0.603 0.108 29.813 0.01 0.620 0.126 30.226 0.02 0.630 0.143 33.903 0.05 0.647 0.152 33.213 0.1 0.643 0.152 33.903 1 0.643 0.159 33.743 The effect of interpretation parameter t. We explore the effects of the interpretation parameter t on perfor- mance, particularly in relation to the relative magnitude ratio of adjacent layers, as shown in Figure 6. We maintain all settings constant, starting from layer S = 16 (halfway 9through the layers of LLaMA-3-8B), and vary the interpretation parameter t from 0.3 to 0.7. Our findings reveal several key points. When t = 0.5, the process resembles average merging, which is less effective for cross-layer merging. In contrast, when t = 0.6 is optimal, the merged representation exhibits the most robust performance, while indicating that more information is derived from the second term (xl) of the SLERP. The frequency results also indicate that the high frequencies are clustered around 0.4 and 0.6, corroborating our optimal t. Moreover, there is a strong correlation between the optimalt and the high frequency of the relative magnitude ratio of adjacent layers. This finding provides an opportunity to utilize the relative magnitude ratio to dynamically determine the interpretation parameter t. Dynamic t allows for more flexible weight control in SLERP merging for each layer-wise operation, thereby showing potential for further exploration. The effect of token retention thresholdγ. We investigate the impact of the token retention threshold γ on model performance across the three datasets, as shown in Table 2. A larger t generally means retaining more tokens for improved performance, but this comes at the cost of increased memory demand. The results suggest that setting γ to 0.05 achieves the best balance between performance and efficiency. 7 Conclusion and Future Work This paper presents a pioneering exploration of KV cache compression in the depth dimension, addressing a significant memory bottleneck in LLMs. Our proposed MiniCache offers a simple, effective, and training-free approach to compressing KV caches by leveraging the notable high similarities between KV caches in neighboring layers, starting from the midpoint of LLMs. We have demonstrated that MiniCache can significantly reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately five times compared to the FP16 baseline. In conclusion, MiniCache significantly advances the field of KV cache compression, offering a state-of-the-art balance between efficiency and performance. Future work will focus on enhancing the compression ratio by cross-multiple-layer merging, developing advanced merging algorithms such as Spherical Cubic Interpolation [ 75], and further optimizing memory usage for large-scale deployments in diverse application scenarios. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,” in NeurIPS, vol. 33, pp. 1877–1901, 2020. [2] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, and OTHERS, “Gpt-4 technical report,” 2023. [3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,et al., “Training language models to follow instructions with human feedback,” in NeurIPS, vol. 35, pp. 27730–27744, 2022. [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar,et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. [5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023. [6] “Introducing meta llama 3: The most capable openly available llm to date.”https://ai.meta. com/blog/meta-llama-3/, 2024. Accessed: 2024-05-04. [7] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad- ford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint arXiv:2001.08361, 2020. 10[8] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023. [9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [10] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., “Training verifiers to solve math word problems,”arXiv preprint arXiv:2110.14168, 2021. [11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu, “Kivi: Plug- and-play 2bit kv cache quantization with streaming asymmetric quantization,” arXiv preprint arXiv:2402.02750, 2024. [12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, “Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm,” arXiv preprint arXiv:2403.05527, 2024. [13] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative inference of large language models with a single gpu,” in ICML, pp. 31094–31116, PMLR, 2023. [14] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al., “H2o: Heavy-hitter oracle for efficient generative inference of large language models,” in NeurIPS, vol. 36, 2024. [15] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient streaming language models with attention sinks,” in ICLR, 2024. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, “Model tells you what to discard: Adaptive kv cache compression for llms,” ICLR, 2024. [17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, “Mixture-of- depths: Dynamically allocating compute in transformer-based language models,” arXiv preprint arXiv:2404.02258, 2024. [18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, “Bert loses patience: Fast and robust inference with early exit,” in NeurIPS, vol. 33, pp. 18330–18341, 2020. [19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, et al., “Layer skip: Enabling early exit inference and self-speculative decoding,” arXiv preprint arXiv:2404.16710, 2024. [20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, “The unreasonable ineffectiveness of the deeper layers,”arXiv preprint arXiv:2403.17887, 2024. [21] T. Salimans and D. P. Kingma, “Weight normalization: A simple reparameterization to accelerate training of deep neural networks,” in NeurIPS, vol. 29, 2016. [22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., “Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024. [23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., “Phi-3 technical report: A highly capable language model locally on your phone,” arXiv preprint arXiv:2404.14219, 2024. [24] M. Roemmele, C. A. Bejan, and A. S. Gordon, “Choice of plausible alternatives: An evaluation of commonsense causal reasoning.,” in AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90–95, 2011. 11[25] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y . Choi, and H. Hajishirzi, “Mathqa: Towards interpretable math word problem solving with operation-based formalisms,” inNAACL, pp. 2357–2367, 2019. [26] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? a new dataset for open book question answering,” in EMNLP, 2018. [27] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “Piqa: Reasoning about physical common- sense in natural language,” in AAAI, 2020. [28] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue: A multi- task benchmark and analysis platform for natural language understanding,” arXiv preprint arXiv:1804.07461, 2018. [29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi, “Winogrande: An adversarial winograd schema challenge at scale,” Communications of the ACM, vol. 64, no. 9, pp. 99–106, 2021. [30] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,” arXiv preprint arXiv:1808.08745, 2018. [31] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al., “Abstractive text summarization using sequence-to-sequence rnns and beyond,” arXiv preprint arXiv:1602.06023, 2016. [32] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 12 2023. [33] Y . Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al., “Longbench: A bilingual, multitask benchmark for long context understanding,” arXiv preprint arXiv:2308.14508, 2023. [34] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee, “Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference,” arXiv preprint arXiv:2307.02628, 2023. [35] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V . Tran, Y . Tay, and D. Metzler, “Confident adaptive language modeling,” in NeurIPS, vol. 35, pp. 17456–17472, 2022. [36] H. Wu and K. Tu, “Layer-condensed kv cache for efficient inference of large language models,” 2024. [37] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” Journal of Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022. [38] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharding,” arXiv preprint arXiv:2006.16668, 2020. [39] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y . Wu, et al., “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,” arXiv preprint arXiv:2401.06066, 2024. [40] C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram,et al., “Tutel: Adaptive mixture-of-experts at scale,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [41] DeepSeek-AI, “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,” arXiv preprint arXiv:2405.04434, 2024. [42] J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Lebrón, and S. Sanghai, “Gqa: Training generalized multi-query transformer models from multi-head checkpoints,” arXiv preprint arXiv:2305.13245, 2023. 12[43] N. Shazeer, “Fast transformer decoding: One write-head is all you need,” arXiv preprint arXiv:1911.02150, 2019. [44] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al., “Rethinking attention with performers,” arXiv preprint arXiv:2009.14794, 2020. [45] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, “Random feature attention,” in ICLR, 2021. [46] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, “Flashattention: Fast and memory-efficient exact attention with io-awareness,” in NeurIPS, vol. 35, pp. 16344–16359, 2022. [47] T. Dao, “Flashattention-2: Faster attention with better parallelism and work partitioning,” arXiv preprint arXiv:2307.08691, 2023. [48] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention with linear complexity,”arXiv preprint arXiv:2006.04768, 2020. [49] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer, “Luna: Linear unified nested attention,” in NeurIPS, vol. 34, pp. 2441–2453, 2021. [50] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh, “Set transformer: A framework for attention-based permutation-invariant neural networks,” in ICML, pp. 3744–3753, PMLR, 2019. [51] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq: Activation-aware weight quantization for llm compression and acceleration,” arXiv preprint arXiv:2306.00978, 2023. [52] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” in NeurIPS, vol. 36, 2023. [53] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer, “Llm.int8(): 8-bit matrix multiplication for transformers at scale,” in NeurIPS, vol. 35, pp. 30318–30332, 2022. [54] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [55] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang, et al., “Loraprune: Pruning meets low-rank parameter-efficient fine-tuning,” inACL findings, 2024. [56] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient transformer,”arXiv preprint arXiv:2001.04451, 2020. [57] S. K. Ainsworth, J. Hayase, and S. Srinivasa, “Git re-basin: Merging models modulo permutation symmetries,” in ICLR, 2023. [58] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur, “The role of permutation invariance in linear mode connectivity of neural networks,” arXiv preprint arXiv:2110.06296, 2021. [59] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson, “Loss surfaces, mode connectivity, and fast ensembling of dnns,” inNeurIPS, vol. 31, 2018. [60] M. Wortsman, G. Ilharco, S. Y . Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y . Carmon, S. Kornblith,et al., “Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,” in ICML, pp. 23965–23998, PMLR, 2022. [61] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal, “Ties-merging: Resolving interfer- ence when merging models,” in NeurIPS, vol. 36, 2023. [62] M. Davari and E. Belilovsky, “Model breadcrumbs: Scaling multi-task model merging with sparse masks,” arXiv preprint arXiv:2312.06795, 2023. [63] L. Yu, B. Yu, H. Yu, F. Huang, and Y . Li, “Language models are super mario: Absorbing abilities from homologous models as a free lunch,” arXiv preprint arXiv:2311.03099, 2023. 13[64] K. Shoemake, “Animating rotation with quaternion curves,” inProceedings of the 12th annual conference on Computer graphics and interactive techniques, pp. 245–254, 1985. [65] M. S. Matena and C. A. Raffel, “Merging models with fisher-weighted averaging,” inNeurIPS, vol. 35, pp. 17703–17716, 2022. [66] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng, “Dataless knowledge fusion by merging weights of language models,” arXiv preprint arXiv:2212.09849, 2022. [67] Y . Chen, X. Pan, Y . Li, B. Ding, and J. Zhou, “Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism,” arXiv preprint arXiv:2312.04916, 2023. [68] S. Reddy, D. Chen, and C. D. Manning, “Coqa: A conversational question answering challenge,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 249–266, 2019. [69] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021. [70] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant: Accurate and efficient post-training quantization for large language models,” in International Conference on Machine Learning, pp. 38087–38099, PMLR, 2023. [71] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [72] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen, “Dora: Weight-decomposed low-rank adaptation,” arXiv preprint arXiv:2402.09353, 2024. [73] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, “Up or down? adaptive rounding for post-training quantization,” in International Conference on Machine Learning, pp. 7197–7206, PMLR, 2020. [74] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. [75] D. H. Eberly, “Quaternion algebra and calculus,” 2002. [76] “Stanford crfm.” https://crfm.stanford.edu/2023/10/12/flashdecoding.html, 2024. Accessed: 2024-05-04. [77] Y . Liu, H. Li, K. Du, J. Yao, Y . Cheng, Y . Huang, S. Lu, M. Maire, H. Hoffmann, A. Holtzman, et al. , “Cachegen: Fast context loading for language model applications,” arXiv preprint arXiv:2310.07240, 2023. [78] L. Chen, H. Zhao, T. Liu, S. Bai, J. Lin, C. Zhou, and B. Chang, “An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models,” arXiv preprint arXiv:2403.06764, 2024. [79] S. Wei, T. Ye, S. Zhang, Y . Tang, and J. Liang, “Joint token pruning and squeezing towards more aggressive compression of vision transformers,” in CVPR, pp. 2092–2101, 2023. [80] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 2021. [81] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. 14Appendix A Additional Experiment Results Comparisons with token sparsity methods. We also compare MiniCache with the sparsity-based method H2O [14]. Our method outperforms H2O in most tasks on LongBench. Notably, our approach focuses on reducing inter-layer redundancy, whereas H2O focuses on intra-layer redundancy. Our approaches are orthogonal to sparsity-based methods. Table A: Comparison between MiniCache with token sparsity based method H2O, using mistral-7B- instruct on LongBench dataset. MethodsNrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews TREC TriviaQA SAMSum PCount PRe Lcc Baseline 26.82 33.06 49.28 42.77 27.33 19.27 32.85 24.25 27.06 71.0 86.23 42.98 2.75 86.98 55.51H2O[14] 22.61 29.06 47.22 36.54 20.6 16.25 30.0 23.8 26.75 70.5 86.16 42.97 3.46 86.38 53.72MiniCache27.0432.5949.38 43.91 24.97 18.3 31.46 23.85 26.64 71.0 86.93 43.6 3.04 79.5654.79 B Additional Related Work Preliminaries of KV cache in LLMs. LLM inference has been significantly improved in recent works [9, 13, 74] by optimizing the KV cache management. Overall, this line of research is typically done in two steps: 1) First, at the prefilling stage, LLM calculates the initial KV caches at each attention layer based on the input sequence and decodes the first token. Second, 2) at the decoding stage, LLM autoregressively predicts the next token, where its KV cache at each attention layer is added to the overall caches. Existing works have compressed KV cache in different aspects (e.g., quantization [11, 12], token pruning [14, 16] ). KV cache compression. In the prior study, various strategies for enhancing efficient transformer architectures are discussed, covering a spectrum of techniques aimed at optimizing performance and managing resource constraints. These methods include attention optimization [ 46, 47, 76], grouping queries [42, 43], sparse KV caching [16, 77, 78], shrinking tokens [15, 79], and improving long-context generation. Significant contributions come from projects such as H2O [ 15], GEAR [15], and KIVI [11]. Additionally, efforts to alleviate KV cache bottlenecks include strategies like multi-query attention [42] and multi-group attention [43], which propose reducing the number of heads in the KV cache. However, these methods often necessitate retraining or fine-tuning models. Other approaches focus on diminishing the size of the KV cache by selectively evicting less important tokens [14] and enhancing the system architecture through technologies like offloading the KV cache [80] or integrating techniques such as virtual memory and paging [81] into the attention mechanism. C Discussions and Limitations Alternative merging function. During our preliminary exploration, we initially considered an alter- native, simpler merge function for cross-layer compression: maximum norm-preserving interpolation. This function is designed to maintain the maximum norm of the vectors involved, ensuring that the most significant features are preserved during the merging process. The maximum norm-preserving interpolation in terms of Fmax can be defined as follows: Fmax(xl, xl−1) = ¯xl,l−1 ∥¯xl,l−1∥ · Max(∥xl∥, ∥xl−1∥). (A) Here ¯xl,l−1 represents the average vector between xl and xl−1. The function Fmax ensures that the merged vector preserves the direction of the average vector while scaling it to the maximum norm of the original KV states. Compared to the SLERP-based merge function, Fmax has less computational overhead and lower memory consumption. However, it is less accurate than SLERP. The choice between using FSLERP or Fmax depends on the specific requirements of the application. In our study, we primarily use SLERP to maximize performance. Societal impact. Our work shows a preliminary exploration of KV cache Compression in the depth dimension, a relatively unexplored yet critically bottlenecked area in large language models 15(LLMs). The proposed MiniCache provides a solution to improve the efficiency of LLM generation and is adaptable to existing intra-layer-based KV cache pruning and quantization technologies. Additionally, we proposed a simple yet effective approach that merges similar KV cache states in a cross-layer manner and effectively restores performance through a novel restoration technique. Our observations indicate that cross-layer similarity and mergeability can be applied in broad-efficient applications for post-training optimization in low-resource scenarios, such as deployment on mobile devices. Moreover, with effective optimization of KV cache memory demand, MiniCache further enhances long-context generation, which is a crucial paradigm for real-world applications, such as understanding concepts in textbooks. We aim for our work to advance the boundaries of two key challenges in the LLM industry and research: batch inference and long-context generation. Furthermore, in addition to the significant contributions of MiniCache for KV cache Compression, several challenges remain that are common to LLMs. Issues such as the truthfulness and security of LLMs are still unresolved. Ensuring the accuracy and reliability of generated content is critical, as LLMs can sometimes produce plausible but incorrect or misleading information. Additionally, safeguarding against security vulnerabilities, such as adversarial attacks or data leakage, is paramount to maintaining the integrity and confidentiality of user interactions. Addressing these challenges requires ongoing research and development to enhance the robustness and trustworthiness of LLMs. This effort must proceed alongside advancements in computational efficiency and performance, as exemplified by innovations like MiniCache. Limitations. The current merging algorithm based on Spherical Linear Interpolation (SLERP) has its limitations. SLERP is suitable only for merging two vectors and uses an interpolation approach that restricts our algorithm from merging multiple layers simultaneously and maximizing the compression ratio in further states. This limitation impacts the overall efficiency of KV cache compression and underscores the need for advanced techniques capable of handling more complex merging scenarios. Future research should focus on developing more sophisticated algorithms that can overcome these constraints, thereby enhancing the compression capabilities and overall performance of LLMs. D Additional Implementation Details Overview of the inference algorithm. The MiniCache inference implementation, as shown in Alg. 1, involves several key steps. When the interface starts, In the prefilling phase, we define the merging starting layer S. Before reaching layer S, the inference uses the original attention and cache logic. From layer S onward, we implement our merging algorithm, which operates in a cross-layer manner and is applied only at odd-numbered layers. During merging, we fetch the KV cache from the previous layer and save the merged shared states into the current layer’s KV cache. To reduce memory usage, we then remove the cache of the previous layer. Additionally, we store the magnitudes vector and retention-sensitive tokens for each layer. During the decoding phase, two scenarios arise after layer S. For even-numbered layers (first round), since the KV cache has been removed during the prefilling phase, we refer to the next layer (l + 1) to fetch the shared KV cache states. We then perform approximated scale restoration and retention token recovery. The new KV states from this phase are stored for use in the next round. In the second round, which involves odd-numbered layers, we use the new KV tokens from both the previous and current layers. After the restoration phase, we perform the merge operations and update the shared KV cache states in the stack. Cross-Layer merging and restoration algorithm. As outlined in Alg. 2, MiniCache algorithm involves several crucial steps to ensure efficient memory usage while maintaining the integrity of the Key-Value (KV) Cache states. Initially, given the KV cacheEl k,v, norm values ∥Xl k,v∥ unmerged tokens Rl k,v, retention indices Ik,v, and the next tokens tl, tl−1, the algorithm proceeds by rescaling the magnitude of the KV pairs. Specifically, ˆXl k and ˆXl v are computed by multiplying the normalized KV pairs El k,v with their respective magnitude norms ∥Xl k,v∥. Following this, the algorithm restores unmerged tokens using the retention indices, updating ˆXl k and ˆXl v accordingly. Next, the new tokens tk and tv are concatenated to the rescaled KV pairs along the token dimension. This augmented KV cache undergoes a softmax attention mechanism where the attention scores A are computed by taking the dot product of the query token tq with the transposed keys ( ˆXl k)⊤. The output token tO is then obtained by multiplying the attention scores A with the values ˆXl v. In cases where the previous token tl−1 exists, the algorithm performs a compression step. It concatenates the existing KV cache 16El k,v with the merged tokens resulting from the current and previous layers, effectively reducing redundancy and optimizing memory. If tl−1 is not available, the KV cache is updated by simply concatenating El k,v with the new tokens tl k,v, deferring the compression until the next iteration. The final output token tO is then returned, concluding the decoding process. In the merging function, the algorithm normalizes the KV pairs from both the current and previous layers. It calculates the angular distance Ω between the normalized vectors, ensuring that the interpolation occurs along the shortest path on the unit sphere. The merged KV cache is then obtained by weighted interpolation of the normalized vectors, preserving the geometric and semantic integrity of the original states. This comprehensive process allows MiniCache to achieve substantial memory efficiencies while maintaining the functional characteristics of the KV pairs across transformer layers. Layer𝑙 QK V Attention Cross-Layer Merging K V KVcachecompressionat layer𝑙 Layer𝑙−1  ... 𝜒! 𝜒!\"# 𝐾,𝑉 C 1 Keep Fetch 2Merge 3 Cache 4Delete Recovery Rescale Retention Recovery 5Fetch Contact New Token QK V Recovery Attention...... 6Fetch Cross Layer Merging Time 7 Prefilling Decoding Figure A: Overall prefilling and decoding logic for MiniCache involves performing cross-layer merging and recovery within our framework. MiniCache execution flow. Figure A delineates the pre-filling and decoding logic for the MiniCache framework, which incorporates cross-layer merging and error suppression to achieve memory effi- ciency and maintain functional integrity. Initially, in Step 1, the KV cache is fetched from the previous layer (Layer L−1) during the pre-filling phase. In Step 2, the fetched KV pairs from the current layer χL are merged with the KV pairs from the preceding layer χL−1, reducing redundancy through a 17merge function. Subsequently, in Step 3, the merged KV pairs are cached for future use, representing a consolidated data set from multiple layers. During the decoding phase, Step 4 involves deleting unnecessary or redundant KV pairs to optimize memory usage. In Step 5, the decoder fetches the required KV pairs from the cache for output generation. Step 6 applies error suppression mechanisms to the fetched KV pairs, including rescaling and retention recovery, to minimize errors introduced during the merging and compression processes. Finally, in Step 7, the cache is updated with the final KV pairs post-error suppression and adjustments, ensuring the cache contains the most accurate and efficient representation of the KV pairs for subsequent layers. This comprehensive approach guarantees substantial memory efficiencies while preserving the critical functional characteristics of the original KV pairs across transformer layers. Algorithm 1: The MiniCache Inference Algorithm 1 procedure MiniCache Inference: Input: Input Tokens: T ∈ Rtinput×d, number of layers L, merging beginning layer S Output: Output Tokens: O ∈ Rtoutput×d 2 for l ← 0 to S − 1 do 3 procedure Standard Prefill: 4 Standard Attention & Standard Cache 5 procedure Standard Decoding: 6 Standard Attention & Standard Cache // Start Merging from layer S 7 for l ← S to L do // Perform Merging in every two layers l%2 == 1 8 if l%2 == 1 and prefilling then 9 procedure MiniCache Prefill: Input: KV cache from Current layer l: Xl k,v, KV cache from Previous layer l − 1: Xl−1 k,v , token retention threshold: γ 10 Delete KV cache of the l − 1-th layer // layer l, l− 1 shares one Cache 11 Standard Attention & Standard Cache // Perform Merging in the second layer 12 if and decoding then 13 if l%2 == 0 then // first round in the cross-layer merging, fetch shared KV cache states from Cl+1 k,v 14 procedure MiniCache Decoding: Input: KV cache: Cl+1 k,v , Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl 15 else // second round in cross-layer merging, while tl−1 exist 16 procedure MiniCache Decoding: Input: KV cache: Cl k,v, Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl, tl−1 17 return O 18Algorithm 2: The MiniCache Prefill & Decoding Compression Algorithm Hyperparameter: number of layers L, merging beginning layer S 1 procedure MiniCache Prefill: Input: KV cache from current layer l: Xl k,v ∈ R2×tprompt×d, KV cache from previous layer l − 1: Xl−1 k,v ∈ R2×tprompt×d, token retention threshold: γ 2 El k, ∥Xl k∥, ∥Xl−1 k ∥, Ωk ← Merge(Xl k, Xl−1 k ) ; 3 El v, ∥Xl v∥, ∥Xl−1 v ∥, Ωv ← Merge(Xl v, Xl−1 v ) ; 4 El k,v ∈ Rtprompt×d | {z } compression output , ∥Xl,l−1 k,v ∥ ∈R4×tprompt×1 | {z } norms for rescaling ; 5 d(Xl, Xl−1)k,v = 1 π · Ωk,v // distance metrics 6 Ik,v = {i | di < dmin + (dmax − dmin) · γ} // retention indices 7 Rl,l−1 k ← Xl,l−1 k [Ik], Rl,l−1 v ← Xl,l−1 v [Iv] // unmerged tokens 8 return El k,v, ∥Xl,l−1 k,v ∥, Rl,l−1 k , Rl,l−1 v , Ik,v 9 procedure MiniCache Decoding: Input: KV cache: El k,v ∈ R2×tprompt×d, Norm: ∥Xl k,v∥ ∈R2×tprompt×1, Unmerged Tokens: Rl k,v ∈ R2×γ·tprompt×d, Retention indices: Ik,v ∈ R2×γ·tprompt×1, Next Token: tl ∈ R1×d, tl−1 ∈ R1×d 10 ˆXl k ← El k · ∥Xl k∥ ∥El k∥ ˆXl v ← El v · ∥Xl v∥ ∥Elv∥ // magnitude rescale 11 ˆXl k[Ik] = Rl k ˆXl v[Iv] = Rl v // token restoration 12 ˆXl k ← Concat( ˆXl k, tk, dim=token) ˆXl v ← Concat( ˆXl v, tv, dim=token) A ← Softmax(tq · ( ˆXl k)⊤) tO ← A · ˆXl v if tl−1 exists then 13 KV cache ← Concat(El k,v, Merge(tl k,v, tl−1 k,v ), dim=token) // perform compression 14 else 15 KV cache ← Concat(El k,v, tl k,v, dim=token) // wait for compression 16 return tO 17 function MiniCache Merge(Xl, Xl−1, t): 18 ⃗Xl ← Xl ∥Xl∥ 19 ⃗Xl−1 ← Xl−1 ∥Xl−1∥ 20 Ω ← arccos \u0010 Xl T ·Xl−1 T ∥Xl T ∥∥Xl−1 T ∥ \u0011 21 E ← sin((1−t)Ω) sin(Ω) ⃗X l + sin(tΩ) sin(Ω) ⃗X l−1 22 return E, ∥Xl∥, ∥Xl−1∥, Ω E Detailed Efficiency Derivation In this section, we provide a detailed derivation of the memory efficiency improvements outlined in Section 4.3. First, we consider the original KV cache memory usage, which is given by: 4brh(s + n). Here, r is the number of layers, b is the batch size, h is the hidden size, s is the input sequence length, and n is the output sequence length. To improve efficiency, we begin merging layers starting from the midpoint, S = 1 2 r, by consolidating the KV cache states of every two layers into a single shared state space. The memory usage derivation proceeds as follows: for the unmerged part of the cache (from layer 1 to S): 194brh(s + n) · 1 2 = 2brh(s + n). For the merged part of the cache (from layer S + 1 to r): 4brh(s + n) · 1 2 · 1 2 = brh(s + n). Combining these two parts, the total memory usage is: 2brh(s + n) + brh(s + n) = 3brh(s + n). Next, we consider the additional memory cost incurred during the restoration process. During this phase, we save additional normalized vectors for the layers in the KV cache. These vectors are of shape Rb×s×1, which means they have a single channel dimension compared to the fully ranked original KV states. The additional normalized vectors for layers from S onwards are given by: br(s + n) · 2 = 2br(s + n). We also introduce a retention threshold, which we set to 0.05. This means that 5% of the KV cache tokens are retained without compression: brh(0.05(s + n)). Combining these terms, the total additional memory for the restoration process is: 2br(s + n) + 0.1brh(s + n). Finally, summing the compressed memory usage and the restoration memory cost, the overall memory requirement is: 3brh(s + n) + 2br(s + n) + 0.1brh(s + n). This can be simplified by grouping the common factors: br(s + n) (3h + 2 + 0.1h) . Simplifying the expression inside the parentheses, we get: br(s + n) (3.1h + 2). Therefore, the total memory cost for the KV cache in the MiniCache Framework is: br(s + n)(3.1h + 2). This detailed derivation confirms the efficiency improvements discussed in Section 4.3, highlighting the significant reduction in memory usage achieved through our layer merging and restoration strategies. F Detailed Experiment Results 20Table B: Detailed performance comparison on GSM8K dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.799 0.729 5 0.800 0.640 10 0.792 0.578 15 0.815 0.545 20 0.801 0.560 25 0.812 0.544 30 0.799 0.556 35 0.810 0.557 40 0.790 0.551 45 0.725 0.539 50 0.638 0.541 55 0.638 0.501 60 0.625 0.497 65 0.635 0.511 70 0.623 0.497 75 0.615 0.493 Table C: Detailed performance comparison on COQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.705 0.706 4 0.705 0.699 8 0.706 0.696 12 0.704 0.691 16 0.704 0.690 20 0.703 0.690 24 0.701 0.690 28 0.702 0.690 32 0.702 0.688 36 0.703 0.688 40 0.697 0.687 44 0.698 0.685 48 0.699 0.678 52 0.699 0.672 56 0.701 0.668 60 0.704 0.657 64 0.706 0.635 68 0.691 0.611 72 0.689 0.565 76 0.641 0.526 21Table D: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 22.615 22.130 4 22.512 22.005 8 22.451 21.876 12 22.413 21.303 16 22.387 21.209 20 22.387 20.752 24 22.387 20.657 28 22.276 20.501 32 22.130 20.479 36 22.130 20.335 40 22.073 19.834 44 21.356 17.024 48 21.356 12.440 52 21.333 9.127 56 21.316 3.255 60 21.172 2.349 64 21.153 2.250 68 21.002 1.721 72 20.940 1.119 76 20.683 0.784 Table E: Detailed performance comparison on GSM8K dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.488 0.467 2 0.476 0.369 4 0.489 0.388 6 0.487 0.387 8 0.489 0.359 10 0.479 0.388 12 0.486 0.384 14 0.472 0.368 16 0.477 0.343 18 0.446 0.291 20 0.447 0.271 22 0.433 0.234 24 0.399 0.155 26 0.396 0.140 28 0.395 0.052 30 0.391 0.024 32 0.397 0.025 22Table F: Detailed performance comparison on COQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.676 0.676 2 0.676 0.571 4 0.675 0.566 6 0.674 0.564 8 0.674 0.561 10 0.673 0.560 12 0.672 0.560 14 0.670 0.559 16 0.670 0.558 18 0.669 0.555 20 0.669 0.552 22 0.668 0.549 24 0.667 0.543 26 0.667 0.537 28 0.666 0.536 30 0.666 0.531 32 0.665 0.528 Table G: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 32.520 32.524 2 32.231 28.431 4 31.645 28.197 6 31.485 27.894 8 31.008 27.796 10 30.964 27.704 12 30.798 27.371 14 30.798 27.093 16 30.798 26.643 18 30.798 26.517 20 30.798 26.355 22 30.798 26.011 24 30.798 25.044 26 30.798 15.254 28 30.798 14.791 30 30.765 9.419 32 30.390 6.068 23Table H: Detailed performance comparison on GSM8K dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.589 0.575 2 0.592 0.480 4 0.593 0.491 6 0.591 0.469 8 0.580 0.472 10 0.592 0.492 12 0.582 0.485 14 0.572 0.480 16 0.562 0.462 18 0.522 0.432 20 0.526 0.426 22 0.540 0.416 24 0.519 0.398 26 0.515 0.436 28 0.502 0.401 30 0.515 0.386 32 0.490 0.258 Table I: Detailed performance comparison on COQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.672 0.675 2 0.671 0.612 4 0.670 0.601 6 0.672 0.590 8 0.674 0.582 10 0.671 0.571 12 0.674 0.561 14 0.670 0.546 16 0.672 0.544 18 0.672 0.530 20 0.675 0.522 22 0.671 0.512 24 0.660 0.455 26 0.657 0.447 28 0.640 0.440 30 0.634 0.424 32 0.459 0.430 24Table J: Detailed performance comparison on TruthfulQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 21.686 19.465 2 21.385 19.405 4 21.368 19.251 6 21.038 19.094 8 21.038 18.265 10 20.216 17.019 12 20.026 15.902 14 19.723 15.505 16 19.641 15.028 18 19.641 14.723 20 19.546 14.543 22 18.756 14.122 24 18.402 13.834 26 18.366 13.789 28 17.738 12.091 30 16.827 12.008 32 16.635 0.430 Table K: Detailed performance comparison on GSM8K dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.774 0.774 2 0.765 0.667 4 0.757 0.661 6 0.754 0.659 8 0.748 0.657 10 0.750 0.645 12 0.750 0.616 14 0.752 0.575 16 0.739 0.491 18 0.742 0.417 20 0.692 0.272 22 0.685 0.206 24 0.640 0.110 26 0.545 0.061 28 0.500 0.039 30 0.460 0.036 32 0.447 0.028 25Table L: Detailed performance comparison on COQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.665 0.665 2 0.662 0.562 4 0.657 0.557 6 0.656 0.556 8 0.656 0.556 10 0.654 0.554 12 0.646 0.546 14 0.648 0.538 16 0.647 0.537 18 0.637 0.527 20 0.627 0.487 22 0.591 0.461 24 0.567 0.437 26 0.548 0.408 28 0.527 0.407 30 0.506 0.406 32 0.503 0.403 Table M: Detailed performance comparison on TruthfulQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 19.686 19.465 2 19.385 19.365 4 19.368 19.221 6 19.100 18.255 8 19.038 17.019 10 19.500 15.912 12 19.216 15.525 14 20.026 15.195 16 19.641 15.058 18 18.756 14.763 20 17.738 14.593 22 19.546 14.182 24 19.723 13.954 26 18.366 13.919 28 18.402 12.231 30 16.827 12.158 32 16.635 10.333 26",
      "meta_data": {
        "arxiv_id": "2405.14366v2",
        "authors": [
          "Akide Liu",
          "Jing Liu",
          "Zizheng Pan",
          "Yefei He",
          "Gholamreza Haffari",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T09:43:52Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14366v2.pdf"
      }
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
      "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit.",
      "full_text": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization Tianyi Zhang1, Jonah Yi1, Zhaozhuo Xu2, and Anshumali Shrivastava1,3 1Department of Computer Science, Rice University 2Department of Computer Science, Stevens Institute of Technology 3ThirdAI Corp. {tz21, jwy4, anshumali}@rice.edu, zxu79@stevens.edu May 8, 2024 Abstract Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit. 1 Introduction Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks, including text generation, language translation, and reasoning, without needing specific fine- tuning [25]. These impressive capabilities have empowered LLMs to find applications in numerous domains, such as law [14], education [15], and patient care [35]. However, the high computational demands and the prohibitive deployment costs of LLMs have created significant barriers, hindering their widespread adoption [14, 3]. Particularly, as LLMs move towards larger model size [9] and longer context length [34], they require faster graphics processing units (GPUs), or other special- ized processors, with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs. To accelerate LLM inference, key and value (KV) caching [20] has been proven to be an effective technique without affecting model quality. In autoregressive decoder-only LLMs, KV caching works through trading off memory for computations: the key and value activations of all previous tokens in the current batch are saved in memory to avoid their recomputation for generating the next token. However, since KV cache scales linearly with the number of tokens and batch size, it can quickly overwhelm the memory capacity of existing GPUs under long context or large batch size 1 arXiv:2405.03917v1  [cs.LG]  7 May 2024settings. In addition, since past key and value activations are not shared between sequences (except for maybe a common prompt), reading the KV cache from GPU memory becomes the primary inference bottleneck as opposed to the computation of the attention scores and value activations of the next token [10]. Thus, it is worthwhile to explore techniques for compressing KV cache for two primary benefits: 1. speeding up LLM inference through reducing the amount of memory reads for KV cache, 2. lowering the GPU memory requirements of inference for a given batch size and context length. Existing approaches typically compress KV cache through token eviction [37, 20] or activation quantization [21, 10]. While they can preserve model quality at moderate compression rates (4× compression or 4 bits per floating-point number), model quality quickly deteriorates at high compression rates (16× compression or 1 bit per floating-point number). In this work, we propose Coupled Quantization (CQ), a novel KV cache quantization method that preserves model quality up to 16× compression or 1 bit per floating-point number. Our approach is motivated by the observation that different channels within the same key/value activation embedding are highly inter-dependent. Thus, it is more information efficient to encode multiple channels of a key/value activation embedding than quantizing each channel independently. Existing KV cache quantization methods employ per-channel or per-token quantization strategies, which fail to exploit the inter-dependence between channels and suffer catastrophic model quality degradation at high compression rates. Our proposed method exploits the mutual dependency be- tween channels by jointly quantizing multiple channels and achieves better preservation of model quality than existing approaches in most cases, especially under low bit width settings. We sum- marize our contributions as follows, 1. We empirically observe the phenomenon that different channels within the same key/value activation embedding in an LLM share a high amount of dependency or mutual information, which is a key insight not leveraged by existing KV cache compression approaches. 2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that takes advantage of the reduced entropy of jointly encoding multiple channels. 3. Through extensive experiments, we demonstrate the effectiveness of CQ against the most competitive existing methods. Furthermore, we showcase the ability of CQ in preserving model quality at an extreme KV cache compression level of 1-bit. 2 Background In this section, we introduce the relevant background and context including the KV caching tech- nique, the von Neumann bottleneck of KV cache, and channel-wise quantization. 2.1 LLM Attention and KV Cache Decoder-only transformer-based LLMs employ masked self-attention [32], in which activations of the current token is only dependent on previous tokens and unaffected by future ones. This prop- erty enables training parallelism for the next-token prediction objective, and gives rise to the KV caching technique for efficient inference decoding. Consider the decoding step for thet-th token in a single head of attention in an LLM. The input embedding of thet-th token (a column vector),et, goes through three distinct transformations to become key, query, and value activation embeddings fK(et), fQ(et), fV (et), where the transformationsfK, fQ, fV are composed of linear projection and positional encoding such as RoPE [29]. The output embedding of attention for thet-th token is 2computed as attention(et) = \u0014 fV (e1) . . . fV (et) \u0015 softmax  \u0014 fK(e1) . . . fK(et) \u0015⊤ fQ(et) ! (1) Thus, computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens,fK(ei) and fV (ei) where i ∈ {1, . . . , t− 1}. These embeddings are cached in memory from previous decoding steps asKV cacheto avoid redundant computations and reduce inference latency. The size of KV cache can be calculated asb × n × l × 2 × h × c floating-point numbers, whereb is the batch size,n is the number of tokens in each sequence,l is the number of layers in the model,2 is for key and value,h is the number of key/value attention heads, and c is the number of channels in a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of KV quickly overwhelms limited GPU memory. 2.2 The von Neumann Bottleneck of KV Cache The attention computation in Equation 1 is primarily bottlenecked by GPU memory bandwidth, known as the von Neumann bottleneck, due to low compute-to-global-memory-access ratio. GPUs are able to perform computations significantly faster than reading from global memory, and previous works have shown that attention computation can be accelerated by avoiding unnecessary global memory reads/writes through kernel fusion [5]. During the decoding phase in LLM attention, com- puting the output for the current token requires fetching the cached key and value embeddings of all previous tokens from global memory, resulting in a low ratio of arithmetic operations to global memory reads [10]. Furthermore, each sequence in a batch retains its own KV cache, leading to poor utilization of the parallel processing capabilities of GPUs. Therefore, KV cache compression algorithms can mitigate the von Neumann bottleneck and improve LLM inference efficiency, even if the approach introduces additional computational overheads in decompression or dequantization. As GPU compute cores are mostly stalled by KV cache memory accesses in attention, quantization approaches can effectively reduce memory reads while introducing negligible latency from dequan- tization computations. 2.3 Channel-wise Quantization Existing KV cache quantization methods [10, 21] employ channel-wise quantization for keys and token-wise quantization for values, based on the observation that certain key channels exhibit outlier patterns in magnitude while value channels do not. Channel-wise and token-wise quantization are similar, except the direction along which the quantization centroids are learned. In non-uniform channel-wise quantization, a set of centroids is learned for each channel. SupposeA is a key or value activation matrix, andAi,∗ denotes the i-th channel ofA. Then, non-uniform b-bit channel-wise quantization aims to learn a set of centroidsC⋆ i ⊂ R for each channeli of A independently through the objective C⋆ i = arg min C⊂R |C|=2b \r\r\rAi,∗ − q(Ai,∗) \r\r\r 2 2 (2) where q quantizes each value inAi,∗ to the nearest centroid inC. 31 2 3 4 5 10 15Key Entropy (bits) Layer 1 1 2 3 4 5 10 Layer 2 1 2 3 4 5 10 Layer 3 1 2 3 4 5 10 Layer 4 1 2 3 4 Num. of Joint Channels 5 10Value Entropy (bits) 1 2 3 4 Num. of Joint Channels 5 10 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 Sum of Marginal Entropy Joint Entropy Figure 1: Growth rate of joint entropy versus sum of marginal entropies of the LLaMA-7b key/value activation embeddings on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that jointly quantizing more channels requires fewer bits than quantizing each channel independently. 3 Methodology In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for KV cache compression. 3.1 Motivations Our proposed approach is inspired by concepts in information theory [28]. We consider each channel in a key/value activation embedding as a random variableX1, X2, . . .. The amount of information (oruncertainty)inchannel X canbemeasuredby entropy, definedasH(X) =− R X p(x) log2 p(x) dx, where p(·) is the probability density function andX is the support of X. H(X) measures the theoretical number of bits needed for losslessly encoding the channelX, so it can be used to gauge how “quantizable” a channel is: ifH(X1) < H(X2), then channelX1 may be quantized to fewer bits than channelX2 while achieving the same quantization error. Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channelsX1, X2 is measured by joint entropy, defined as H(X1, X2) = − R X1 R X2 p(x1, x2) log2 p(x1, x2) dx2 dx1, wherep(·, ·) is the joint probability density function. The joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e.,H(X1, X2) =H(X1) +H(X2) − I(X1, X2), whereI(·, ·) is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have H(X1, X2) ≤ H(X1) +H(X2) (3) which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that 41 16 32 LLaMA-7b Key Correlations Layer 1  Layer 2  Layer 13  Layer 14  Layer 28  Layer 29  Layer 31  Layer 32 1 16 32 Channels 1 16 32 LLaMA-7b Value Correlations 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1.0 0.5 0.0 0.5 1.0 Figure 2: Correlation matrices of the first 32 channels of 8 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients. deep neural networks [11] and attention-based networks [7] tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency. It is hence beneficial to measure the difference between the joint entropy of multiple channels and the sum of their marginal entropies in key and value activation embeddings. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is intractable to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the “binning” trick [17] to estimate entropy. We first observe an empirical distribution of key and value channels by saving the KV cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy ofn channels X1, . . . , Xn is estimated with the Riemann sum, H(X1, . . . , Xn) ≈ X x1∈B1 ··· X xn∈Bn bp(x1, . . . , xn) log2 bp(x1, . . . , xn) (4) where Bi is the support of the binned or discretizedXi and bp(·) is the empirical probability density function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b into non- overlapping groups ofc contiguous channels, wherec ∈ {1, 2, 3, 4}, and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1 shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of three layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset, averaged over groups. We only show a maximum group size of 4, since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1, the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding decreases. This phenomenon is the foundation that motivates our proposed approach. In addition to presenting the estimated marginal and joint entropy, we also show the Pearson correlation coefficients between channels of LLaMA-7b key and value activation embeddings on WikiText-2. Correlation coefficient captures the linear dependency between two random variables. Heat maps for the correlation matrices of 8 layers are shown in Figure 2, while the correlation matrices of all layers are presented in Section 6 of the Appendix. The key and value channels 5Channel-wise Quantization Coupled Quantization q0 q1 channel 0 -0.57 0  0.21 1  -0.38 0  -0.13 1  key or value activation embedding channel 1 channel-wise centroids q -0.62 0  0.11 1  -0.38-0.10 0.30 2  -0.38 3  -0.13-0.24 coupled channels 0 & 1 key or value activation embedding multi-channel centroids Figure 3: A comparison of 1-bit channel-wise quantization and our proposed Coupled Quantization (using 2 bits per 2 channels as an example). The quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. Channel-wise quantization is ineffective at capturing the original values at low widths, while CQ leverages the dependency between channels to achieve low quantization errors. exhibit high levels of linear dependency and they are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients. 3.2 Coupled Quantization Motivated by the finding that distinct key/value channels exhibit high dependency, we propose Coupled Quantization (CQ), an information-efficient quantization approach for compressing LLM KV cache. Unlike existing KV cache quantization methods which quantizes channel-wise or token- wise, CQ performs channel-coupled quantization for keys and values. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group arecoupled, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each group of coupled channels are quantized to the nearest centroid in terms of L2 distance. We use the CQ-<c>c<b>b notation to denote the configuration of channel coupling and quantization bit width, where<c> is the number of channels in each group and<b> indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit channel-wise quantization in terms of storage overhead of quantized codes. A illustrative comparison of channel-wise quantization and CQ is shown in Figure 3. Although previous works [10, 21] opt to quantize keys channel-wise and values token-wise, we adopt channel-coupled quantization for both keys and values. Similar to existing approaches [10, 21], CQ quantizes keys before RoPE [29] is applied, which increases the quantization difficulty by introducing more outliers in key activations. 3.2.1 Centroid Learning In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ-c cb b configuration, a set of centroidsC⋆ i ⊂ Rc is learned 61c1b 2c2b 4c4b 8c8b CQ Conﬁg. 101 102 103 Perplexity on WikiText-2 2642.72 1614.02 883.60 32.12 620.08 28.39 10.47 8.095.68 LLaMA-7b 1-bit CQ Uniform CQ Fisher-guided CQ FP16 Baseline 1c2b 2c4b 4c8b CQ Conﬁg. 101 102 Perplexity on WikiText-2 204.56 24.46 6.866.37 6.08 5.97 LLaMA-7b 2-bit CQ 1c1b 2c2b 4c4b 8c8b CQ Conﬁg. 750 1000 1250 1500 1750 2000Quant. Error on WikiText-2 LLaMA-7b 1-bit CQ Uniform CQ Key Uniform CQ Value Fisher-guided CQ Key Fisher-guided CQ Value 1c2b 2c4b 4c8b CQ Conﬁg. 600 800 1000Quant. Error on WikiText-2 LLaMA-7b 2-bit CQ Figure 4: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity. independently for each channel groupi through the objective C⋆ i = arg min C∈Rc |C|=2b \r\r\rAic:(ic+c−1), ∗ − cq \u0000 Aic:(ic+c−1), ∗ \u0001\r\r\r 2 F (5) where Aic:(ic+c−1), ∗ is the sub-matrix ofA containing all coupled channels of thei-th group, and cq quantizes each column vector to the nearest centroid inC in terms of L2 distance. We use the k-means algorithm [22] with k-means++ initialization [1] to optimize the objective. LLMs are more sensitive to the quantization precision of certain weights than others [16]. To better preserve model quality, centroids of CQ should be learned to be biased towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrixF to identify the more influential key/value activations and guide the centroid learning process. This method was proposed by Li et al. [18] and used by Kim et al. [16] for channel-wise weight quantization, and we extend it to multi-channel CQ. For performing Fisher-guided centroid learning, we first save a key/value activation matrixA and its gradient g(A) = ∂ ∂A L(A) on a calibration dataset, whereL is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, which is the element-wise square of the gradient matrixdiag(F) = g(A) ⊙ g(A). We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of channel-coupled activations, and obtain the centroid setC⋆ i for thei-th channel group using the objective C⋆ i = arg min C⊂Rc |C|=2b X j g \u0000 Aic:(ic+c−1), j \u0001⊤g \u0000 Aic:(ic+c−1), j \u0001\r\r\rAic:(ic+c−1), j− cq(Aic:(ic+c−1), j) \r\r\r 2 F (6) We leverage weighted k-means to optimize the objective. The overhead of the centroid learning process and centroid storage are discussed in Section 4.3. We validate the effectiveness of our proposed channel-coupling and Fisher-guided centroid learn- ing by compressing LLaMA-7b KV cache to 1-bit and 2-bit, and present the perplexity results and quantization errors (∥A − cq(A)∥2 F averaged over layers) on WikiText-2 under different CQ con- figurations in Figure 4. The experimental setup is given in Section 4. As the number of coupled channels increases, perplexity and quantization errors improve significantly, approaching the FP16 7Table 1: Perplexity on WikiText-2 under different KV cache quantization methods at varying bit- width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. “NaN” means Not a Number, which is caused by numerical stability issues of quantization. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 5.68 5.09 5.12 4.57 4.76 INT4 4.00-4.01 5.98 5.32 5.66 5.01 4.97 INT4-gs128 4.16 5.77 5.16 5.32 4.71 4.82 NF4 4.00-4.01 5.87 5.23 5.47 4.90 4.91 NF4-gs128 4.16 5.77 5.17 5.30 4.71 4.83 KVQuant-4b 4.00-4.02 5.73 5.15 5.18 4.63 4.81 KVQuant-4b-1% 4.32-4.35 5.70 5.11 5.14 4.59 4.78 CQ-2c8b 4.00 5.70 5.11 5.14 4.59 4.79 INT2 2.00-2.01 11779 69965 4708 3942 573 INT2-gs128 2.14 37.37 41.77 117.88 93.09 51.96 NF2 2.00-2.02 3210.5 5785.6 13601 4035.6 902.51 NF2-gs128 2.14 351.23 141.19 634.59 642.44 252.85 KVQuant-2b 2.00-2.02 8.17 7.29 9.75 29.25 7.33 KVQuant-2b-1% 2.32-2.35 6.06 5.40 5.50 4.92 5.16 CQ-4c8b 2.00 5.97 5.32 5.42 4.81 5.11 KVQuant-1b 1.00-1.02 321.58 1617.40 NaN 4709.83 203.73 KVQuant-1b-1% 1.32-1.35 9.93 7.97 9.50 13.76 10.07 CQ-8c8b 1.00 8.09 7.02 7.75 6.55 7.25 CQ-8c10b 1.25 6.78 6.00 6.25 5.47 5.90 baseline performance. Although Fisher-guided centroid learning increases the quantization error, it better preserves the salient activations and achieve lower perplexity. 4 Experiments In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, metrics, datasets, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware Testbed Experiments are performed on a Linux server running Ubuntu 20.04, equipped with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs. Software ImplementationOur software implementation of CQ is based on PyTorch [24] and the HuggingFace Transformers library [33]. Evaluation Metrics and BenchmarksWe evaluate the quality of 5 popular open-source LLMs on various benchmarks under different KV cache quantization algorithms. The 5 LLMs con- sidered are 1. LLaMA-7b, 2. LLaMA-13b [30], 3. LLaMA-2-7b, 4. LLaMA-2-13b [31], 5. Mistral-7b [13]. We evaluate LLM quality using the perplexity metric on 2 datasets: WikiText-2 [23] and C4 [26], and accuracy on 3 benchmarks in zero-shot setting: WinoGrande [27], PIQA [2], and ARC Challenge [4]. Perplexity is evaluated on the test set of the datasets at the maximum context length 8Table 2: Perplexity on C4 under different KV cache quantization methods at varying bit-width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLama-2-7b LLaMA-2-13b Mistral-7b FP16 16 7.08 6.61 6.63 6.05 5.71 INT4 4.00-4.01 7.40 6.82 7.31 6.59 5.91 INT4-gs128 4.16 7.16 6.67 6.87 6.20 5.76 NF4 4.00-4.01 7.27 6.74 7.09 6.45 5.85 NF4-gs128 4.16 7.16 6.66 6.86 6.20 5.77 KVQuant-4b 4.00-4.02 7.13 6.65 6.70 6.11 5.75 KVQuant-4b-1% 4.32-4.35 7.09 6.62 6.65 6.06 5.72 CQ-2c8b 4.00 7.11 6.64 6.67 6.09 5.74 INT2 2.00-2.01 10892 100870 4708 4220 477 INT2-gs128 2.14 43.49 56.25 113.49 97.04 50.73 NF2 2.00-2.02 2850.1 4680.3 13081.2 4175.6 1102.3 NF2-gs128 2.14 248.32 118.18 420.05 499.82 191.73 KVQuant-2b 2.00-2.02 10.28 9.05 15.16 43.77 8.40 KVQuant-2b-1% 2.32-2.35 7.38 6.83 7.06 6.38 6.08 CQ-4c8b 2.00 7.52 6.96 7.23 6.52 6.17 KVQuant-1b 1.00-1.02 168.90 1316.41 362.94 4223.37 127.07 KVQuant-1b-1% 1.32-1.35 11.18 9.56 16.04 22.87 10.53 CQ-8c8b 1.00 12.13 10.53 12.49 10.53 9.89 CQ-8c10b 1.25 9.12 8.23 9.03 8.01 7.46 of the LLM (2048 for LLaMA, 4096 for LLaMA-2, and 8192 for Mistral). Baselines We compare our proposed approach with uncompressed FP16 KV cache and com- petitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization [6] (without grouping and with a group size of 128), 3. KVQuant [10] (without sparse outliers and with 1% outliers stored in sparse format). KVQuant-<b>b-1% is a dense-and-sparse method that requires an additional sparse matrix multiplication in addition to the dense matrix multiplication during inference, which introduces additional computational overhead. KVQuant and our proposed CQ both require learn- ing centroids on a calibration dataset, and we use the same calibration set of 16 sequences (each with 2048 tokens) of WikiText-2 for both methods. Calibration is performed once on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For each method, we report bits per floating-point number (FPN) to measure the compression rate, which is calculated as the number of bits in the quantized KV cache of each token divided by the number of FPN in the uncompressed KV cache of each token, excluding the constant storage overheads of centroids, scaling factors, and zero points. 4.1 Results The results of perplexity on WikiText-2 are presented in Table 1 and the results on C4 are presented in Table 2. CQ consistently outperforms non-dense-and-sparse quantization methods, especially in low bit-width regions. Despite using lower bit-width, CQ performs better or on par with the dense-and-sparse quantization method KVQuant-<b>b-1%. Dense-and-sparse quantization methods 9Table 3: Accuracy on 3 benchmarks under different KV cache quantization methods at varying bit-width. Bits Per FPN Task LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 WinoGrande 69.93 72.69 68.90 71.98 73.88 PIQA 78.67 79.16 78.07 79.16 80.58 ARC Challenge 41.72 46.42 43.43 48.29 50.34 KVQuant-4b 4.00-4.02 WinoGrande 69.53 72.61 67.96 71.59 73.88 PIQA 78.62 79.22 77.86 78.94 80.58 ARC Challenge 42.32 45.99 42.75 46.67 49.06 KVQuant-4b-1% 4.32-4.35 WinoGrande 70.72 73.40 68.67 72.30 73.72 PIQA 78.40 79.16 78.07 79.27 80.74 ARC Challenge 41.38 46.76 43.17 47.87 49.91 CQ-2c8b 4.00 WinoGrande 70.40 72.45 68.27 72.53 73.48 PIQA 78.61 79.11 77.91 78.62 80.52 ARC Challenge 41.55 45.99 43.34 47.78 49.15 KVQuant-2b 2.00-2.02 WinoGrande 53.59 59.35 51.70 51.30 63.46 PIQA 72.47 74.81 63.38 65.40 75.46 ARC Challenge 32.00 34.47 22.44 24.66 38.57 KVQuant-2b-1% 2.32-2.35 WinoGrande 68.03 71.43 67.64 70.17 70.80 PIQA 77.69 78.51 76.60 78.51 79.65 ARC Challenge 38.74 45.14 41.47 44.97 47.53 CQ-4c8b 2.00 WinoGrande 67.48 70.72 66.45 69.06 69.38 PIQA 76.11 78.29 76.12 77.42 79.49 ARC Challenge 38.48 44.03 39.93 44.11 45.65 KVQuant-1b 1.00-1.02 WinoGrande 50.51 48.46 50.91 49.41 49.80 PIQA 53.26 53.54 53.37 50.92 54.73 ARC Challenge 21.76 21.33 20.65 21.67 19.88 KVQuant-1b-1% 1.32-1.35 WinoGrande 56.67 61.01 57.77 57.30 58.17 PIQA 71.38 75.46 69.91 70.89 73.83 ARC Challenge 29.69 35.32 31.48 32.59 33.19 CQ-8c8b 1.00 WinoGrande 56.51 61.56 55.01 57.14 58.25 PIQA 71.16 73.99 71.22 73.01 75.24 ARC Challenge 30.20 33.79 30.20 34.30 33.79 CQ-8c10b 1.25 WinoGrande 60.46 65.27 59.19 62.98 63.93 PIQA 73.45 75.90 73.07 74.37 77.31 ARC Challenge 33.28 37.12 34.64 38.74 39.59 introduce additional inference overhead due to the extra sparse matrix multiplications for activation outliers which is inefficient on GPUs, while CQ does not have this limitation. The accuracy results on different benchmarks are shown in Table 3. CQ consistently outperforms the non-dense-and-sparse baseline KVQuant-<b>b at bit-width of 1 and 2, and performs better or on par with the dense-and-sparse baseline KVQuant-<b>b-1%. 4.2 Ablation Study We perform a set of ablation experiments to study the effectiveness of each component of our proposed approach. The results of the ablation experiments are shown in Table 4. We evaluate the perplexity of 2 models Mistral-7b and LLaMA-2-13b on WikiText-2 using CQ at 2 bits per FPN, withvaryingnumberofchannelscoupledandcomparinguniformcentroidlearningandFisher-guided centroid learning. Fisher-guided centroids significantly improve model quality as demonstrated by lower perplexity. With either uniform centroids or Fisher-guided centroids, perplexity improves as the number of coupled channels increases. Hence, our proposed techniques of channel coupling and 10Table 4: Ablation study: perplexity on WikiText-2 using CQ with varying number of coupled channels and fisher-guide centroids. Perplexity consistently improves as the number of coupled channels increases. Mistral-7b LLaMA-2-13b Bits Per FPN 2 2 2 2 2 2 2 2 2 2 2 2 Num. of Channels Coupled 1 2 4 1 2 4 1 2 4 1 2 4 Fisher-guided Centroids ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ Perplexity ↓ 97.76 16.29 5.42 5.34 5.20 5.11 890.42 171.96 6.62 6.06 4.91 4.81 Fisher-guided centroid learning are both effective for maintaining model quality. 4.3 Overhead of Centroid Learning and Storage In this section, we discuss the computational overhead of centroid learning and the memory overhead of centroid storage for CQ. The centroid learning process of CQ consists of many independent k- means runs, which can be time-consuming on CPUs. Hence, we leverage a GPU implementation to accelerate the learning process. In all our experiments, we use k-means++ initialization and run 100 iterations of k-means on a single GPU to obtain the centroids. The memory overhead of storing the centroids can be calculated asl × 2 × h × c × 2b 16-bit floating-point numbers, wherel is the number of layers,2 is for keys and values,h is the number of key/value attention heads,c is the number of channels in a single-head key/value activation embedding, andb is the bit width of quantized codes. The detailed learning and memory overhead for different CQ configurations and models are given in Table 5. CQ can easily scale to large model sizes with low learning and memory overheads. Table 5: Learning and memory overhead of different CQ configurations and models. The number of centroid parameters are shown in millions, and the percentage to the model weights is shown in brackets. Centroid Learning Time Parameter Count in Centroids CQ Config. 2c8b 4c8b 8c8b 2c8b 4c8b 8c8b LLaMA-7b 53 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-13b 94 mins 44 mins 22 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) LLaMA-2-7b 54 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-2-13b 83 mins 44 mins 23 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) Mistral-7b 13 mins 7 mins 4 mins 16.78M (0.232%) 16.78M (0.232%) 16.78M (0.232%) 5 Related Works The high memory requirements and computational demands of LLMs pose a great challenge to effi- cient inference. Post-training model weight quantization has been shown to be effective for reducing inference latency and memory requirements. GPTQ [8] scales approximate Hessian-based weight quantization to large-scale models, and AWQ [19] preserves the salient weights in LLMs to achieve better quantized model quality. SqueezeLLM [16] leverages sensitivity-based non-uniform clustering and dense-and-sparse quantization for preserving salient weights and outliers. In addition to weight quantization, KV cache compression approaches are also effective for improving inference efficiency. Scissorhands [20] and H2O [37] achieve KV cache compression while preserving model quality by 11evicting unimportant tokens and only storing pivotal tokens. KIVI [21] quantizes key activations channel-wise and value activations token-wise, and uses a residual to achieve better model quality. KVQuant [10] proposes sensitivity-based quantization and dense-and-sparse quantization for KV cache. FlashAttention [5] improves the inference efficiency of attention-based models on GPUs by fusing kernels to eliminate unnecessary global memory reads/writes, while NoMAD-Attention [36] accelerates attention computations on CPUs by leveraging in-register shuffles. Product Quantiza- tion [12] is an approximate nearest neighbor search method that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and jointly quantizing the dimensions within each subspace. 6 Conclusion In this work, we propose Coupled Quantization (CQ) for enabling more efficient LLM inference by compressing KV cache, which is the latency and throughput bottleneck in long context or large batch size settings. We observe that distinct channels of key/value activation embeddings exhibit high levels of dependency, which has not been leveraged by existing compression methods. Motivated by this insight, we propose channel coupling for exploiting the inter-channel dependency to achieve more information-efficient encoding of key/value activations. Furthermore, we propose Fisher-guided centroid learning to better preserve salient activations and model quality. Extensive experiments demonstrate that our method mostly outperforms existing methods in terms of model quality under the same quantization bit width. Moreover, CQ can preserve model quality reasonably well with KV cache quantized down to 1-bit. References [1] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, volume 7, pages 1027–1035, 2007. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020. [3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance.arXiv preprint arXiv:2305.05176, 2023. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35:16344–16359, 2022. [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient fine- tuning of quantized llms.Advances in Neural Information Processing Systems, 36, 2024. [7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. InInternational Conference on Machine Learning, pages 2793–2803. PMLR, 2021. 12[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post- training quantization for generative pre-trained transformers.arXiv preprint arXiv:2210.17323, 2022. [9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.15556, 2022. [10] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization.arXiv preprint arXiv:2401.18079, 2024. [11] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.arXiv preprint arXiv:2103.10427, 2021. [12] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2010. [13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap- lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. [14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [15] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education.Learning and individual differences, 103:102274, 2023. [16] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. [17] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004. [18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [20] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.Advances in Neural Information Process- ing Systems, 36, 2024. 13[21] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [22] Stuart Lloyd. Least squares quantization in pcm.IEEE transactions on information theory, 28(2):129–137, 1982. [23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An impera- tive style, high-performance deep learning library.Advances in neural information processing systems, 32, 2019. [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. [27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.Communications of the ACM, 64(9):99–106, 2021. [28] Claude Elwood Shannon. A mathematical theory of communication.The Bell system technical journal, 27(3):379–423, 1948. [29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024. [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. [33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of- the-art natural language processing. InProceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. [34] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long- context scaling of foundation models.arXiv preprint arXiv:2309.16039, 2023. 14[35] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records.NPJ digital medicine, 5(1):194, 2022. [36] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava. Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention.arXiv preprint arXiv:2403.01273, 2024. [37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for effi- cient generative inference of large language models.Advances in Neural Information Processing Systems, 36, 2024. 15Appendix A Correlation Matrices and Scatter Plots 0 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 5: Correlation matrix for the first 32 channels of pre-RoPEkey activation embeddings of all LLaMA-7b layers on WikiText-2. 160 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 6: Correlation matrix for the first 32 channels ofvalue activation embeddings of all LLaMA- 7b layers on WikiText-2. Figure 7: 2-dimensional scatter plots of pairs of channels inkey activation embeddings of 4 LLaMA- 7b layers on WikiText-2. 17Figure 8: 2-dimensional scatter plots of pairs of channels in value activation embeddings of 4 LLaMA-7b layers on WikiText-2. 18",
      "meta_data": {
        "arxiv_id": "2405.03917v1",
        "authors": [
          "Tianyi Zhang",
          "Jonah Yi",
          "Zhaozhuo Xu",
          "Anshumali Shrivastava"
        ],
        "published_date": "2024-05-07T00:25:20Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03917v1.pdf"
      }
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification",
      "abstract": "KV cache stores key and value states from previous tokens to avoid\nre-computation, yet it demands substantial storage space, especially for long\nsequences. Adaptive KV cache compression seeks to discern the saliency of\ntokens, preserving vital information while aggressively compressing those of\nless importance. However, previous methods of this approach exhibit significant\nperformance degradation at high compression ratios due to inaccuracies in\nidentifying salient tokens. In this paper, we present ZipCache, an accurate and\nefficient KV cache quantization method for LLMs. First, we construct a strong\nbaseline for quantizing KV cache. Through the proposed channel-separable\ntokenwise quantization scheme, the memory overhead of quantization parameters\nare substantially reduced compared to fine-grained groupwise quantization. To\nenhance the compression ratio, we propose normalized attention score as an\neffective metric for identifying salient tokens by considering the lower\ntriangle characteristics of the attention matrix. Moreover, we develop an\nefficient approximation method that decouples the saliency metric from full\nattention scores, enabling compatibility with fast attention implementations\nlike FlashAttention. Extensive experiments demonstrate that ZipCache achieves\nsuperior compression ratios, fast generation speed and minimal performance\nlosses compared with previous KV cache compression methods. For instance, when\nevaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of\ncompressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in\naccuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction\nin prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a\n$19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a\ninput length of $4096$.",
      "full_text": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification Yefei He1 Luoming Zhang1 Weijia Wu2 Jing Liu3 Hong Zhou1† Bohan Zhuang1,3† 1Zhejiang University, China 2National University of Singapore, Singapore 3ZIP Lab, Monash University, Australia Abstract KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. Ad- ditionally, the compression process introduces excessive overhead, substantially increasing memory burdens and the generation latency. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for large lan- guage models (LLMs). First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced com- pared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. The quantization bit-width for each token is then adaptively assigned based on their saliency. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and min- imal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by 4.98×, with only a 0.38% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096. 1 Introduction LLMs with the next-token-prediction scheme have achieved remarkable advancements in various text-related tasks, such as language understanding [13, 34, 10], content creation [1, 5, 36], coding [3, 29, 42] and mathematics [33, 23, 35]. In this generation scheme, the forthcoming token interacts with all previous tokens via the attention mechanism [38], where the query, key and value states will be †Corresponding author. Email: zhouhong_zju@zju.edu.cn, bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14256v1  [cs.LG]  23 May 2024calculated for each token. As the past tokens will not be altered, previously computed key and value states can be stored as KV cache to prevent re-computations, significantly improving the generation speed. However, as the batch size and the input context length grows, the stored KV cache emerges as a new memory bottleneck for LLMs. For example, when serving a 175B-parameter LLM [1] with a batch size of 64 and a context length of 4096, the KV cache can occupy 1.2TB of memory space, while the model weights only require 350GB. Meanwhile, the size of KV cache will continue to increase as decoding progresses. Therefore, the compression of KV cache is crucial for the efficient deployment of LLMs. Recent compression methods for KV cache can be broadly categorized into two types. The first type of methods compresses the KV cache uniformly, without considering the significance of individual tokens. To preserve performance, these methods often rely on either high-precision quantization [21] or maintaining recent tokens in full-precision [32], which undoubtedly compromise the compression ratio. Additionally, if salient tokens are not among the most recent ones, such as in information retrieval tasks, it may result in degraded performance. The other type of methods [ 46, 43, 16] compress KV cache adaptively by identifying salient tokens and compresses them separately. This approach aligns with the observation that a minority of tokens contribute the majority of attention scores [41], potentially achieving higher compression ratios than non-adaptive methods. However, current adaptive KV cache compression methods [ 46, 43] use accumulated attention scores as a metric of token saliency, which is insufficient in two aspects. First, accumulated attention scores is inaccurate in identifying important tokens. Due to the presence of attention masks, the attention matrix is a lower triangular matrix. Earlier tokens tend to have larger softmax attention values and more attention scores to be accumulated, as illustrated in Figure 3. Under this metric, the saliency of the most recent tokens can never surpass that of the first token, thereby introducing a bias in determining token saliency. Additionally, to obtain accumulated attention scores, full attention matrices must be explicitly computed and stored, which can be inefficient for serving LLMs. Given an input context length of l, fast attention implementations such as FlashAttention [8, 7] only require O(l) memory by computing attention output in blocks without retaining complete attention matrices. By contrast, storing full attention matrices requires O(l2) memory, and the large number of memory accesses significantly slows down the inference speed, as depicted in Figure 4. Figure 1: Accuracy and efficiency compar- isons across various KV cache compression methods. Data is collected with LLaMA3- 8B model on Line Retrieval dataset. Among these methods, ZipCache achieves the high- est accuracy, generation speed and compres- sion ratio. Details can be found in the sup- plementary material. To address these challenges, we introduce ZipCache, an efficient KV cache compression method that attains ex- ceptionally high compression ratios by accurate salient token identification. Figure 1 presents an overview of latency-accuracy comparisons among ZipCache and diverse KV cache compression methods. We start by designing an efficient quantization baseline for com- pressing the KV cache. To preserve performance, prede- cessor methods [32, 21] employ fine-grained groupwise quantization, which involves independent quantization for a small channel group within each token. However, this method necessitates storing extensive quantization parameters and results in significant memory overhead. By contrast, we introduce a channel-separable quanti- zation scheme that decouples the quantization along channel and token dimensions. This method signifi- cantly reduces the quantization overhead without com- promising performance. To accurately recognize salient tokens, we introduce a new token saliency metric based on normalized attention scores, which alleviates the bias towards earlier tokens that accumulate more val- ues. All tokens, without exception, will be quantized to the target bit-width based on their estimated saliency, boosting the overall compression ratio. Moreover, to ease integration with fast attention implementa- tions, we introduce an efficient approximation of the token saliency metric. This approximation only relies on computing and storing attention scores from a few number of tokens, which we refer to as probe tokens. An effective probe token selection strategy is then introduced to minimize performance loss. As a result, the majority of tokens can benefit from fast attention implementations, significantly enhancing the generation speed. 2In summary, our contributions are as follows: • We establish an efficient channel-separable quantization scheme for KV cache, which significantly reduces the overhead of quantization parameters without compromising performance compared to fine-grained groupwise quantization approach. • We propose an accurate metric for assessing token saliency based on normalized attention scores. All tokens are adaptively quantized according to their assessed saliency, thereby improving the overall compression ratio. • We further develop an efficient approximation method for the token saliency metric that integrates seamlessly with fast attention implementations, enhancing generation speed. • By integrating these three techniques, we present ZipCache, an accurate and efficient framework for KV cache compression. Extensive experiments demonstrate that ZipCache reaches a new state-of-the-art performance for KV cache compression in terms of compression ratio, accuracy and generation efficiency. 2 Related Work 2.1 Model Quantization Quantization is a prevalent technique for compressing deep neural networks by representing model weights and activations with lower numerical bit-widths. This technique can be categorized into two primary approaches based on the necessity of fine-tuning: post-training quantization (PTQ) [26, 17, 14] and quantization-aware training (QAT) [28, 31]. For large language models (LLMs), where fine-tuning can be data- and computation-intensive, PTQ is often the preferred method [40, 11, 45, 27]. In this paper, we also quantize KV cache in a post-training manner. For both approaches, quantization can be implemented at various levels of granularity, including channelwise, tokenwise, and groupwise approach. Typically, a finer quantization granularity involves the independent quantization of smaller parameter groups, which often results in improved performance albeit at the cost of more quantization parameters and increased memory overhead. In the context of LLMs, fine-grained quantization is frequently utilized due to the presence of outliers [22, 45]. However, for KV cache compression, this will greatly reduce the overall compression ratio. Mixed precision quantization [39, 44, 12, 2] allocates varying bit-widths to distinct parts of a model or tensor, enabling a more compact compression. This approach originates from the observation that model components exhibit differing sensitivities to quantization. Consequently, components with low sensitivity can utilize reduced bit-widths without impairing performance. For LLMs, previous studies [46, 43, 30, 18] have shown significant disparities in the importance of tokens, indicating that heavy compression of non-critical tokens has minimal impact on overall performance. This insight highlights the applicability of mixed precision quantization for compressing the KV cache. 2.2 KV Cache Compression While KV cache effectively prevents re-computation and significantly enhances generation speed, its memory footprint is notably substantial with long-context input. To alleviate this, many efforts have been made to reduce the KV cache size. Based on the compression method, these methods can be categorized into two groups: token dropping [46, 16, 30] and KV cache quantization [43, 21, 32]. The former identifies and drops unimportant tokens in the KV cache. For example, H2O [46] only maintain 20% heavy-hitted tokens and 20% recent tokens while evicting the rest. However, discarding tokens permanently erases their information, which proves to be suboptimal for tasks such as retrieval [43]. Conversely, the latter category employs quantization on the cached key and value states, and mixed precision quantization can further be applied once token importance is identified [ 43]. To tackle the outliers present in the KV cache, these methods extract the outlier as full precision [21] or use finer-grained quantization scheme [32], which increases the quantization overhead. In this study, we propose an efficient channel-separable quantization scheme with reduced quantization overhead and strong performance. Additionally, both categories of methods commonly adopt accumulated attention scores as the metric for token importance [46, 43]. However, we observe that this criterion is inaccurate and can result in significant performance deterioration at low bit-widths. In contrast, we achieve superior compression performance by utilizing a more accurate metric for identifying salient tokens. 33 Preliminary 3.1 Attention Block in LLMs Given an input prompt, the generation process of LLMs can be broadly categorized into two distinct phases: the prefill phase, which computes and stores the KV cache for input tokens, and the decoding phase, where new tokens are generated through a next-token-prediction scheme. Given input data X and an attention block with its weight matrices WQ, WK and WV, the prefill phase can be formulated as: Q = XWQ, K = XWK, V = XWV, (1) A = Softmax \u0012QKT √dk \u0013 , O = AV. (2) Here, dk is the dimension of the key, and A refers to the attention scores. K and V will be stored as KV cache. For clarity, we have omitted the output projection. For the decoding phase, given x as the embedding vector of the current token, the query q becomes a vector and the KV cache matrices will be updated as follow: q = xWQ, K = Concat(K, xWK), V = Concat(V, xWV). (3) The attention output are then computed as follows: a = Softmax \u0012qKT √dk \u0013 , o = aV. (4) To ensure clarity and consistency, we introduce notation to define the hyper-parameters used in the paper. Specifically, we denote the batch size as b, the number of attention heads as h, the sequence length as l, and the head dimension as d. 3.2 Model Quantization Uniform quantization is adopted in our study and all experiments. Given a floating-point vector x, it can be uniformly quantized to k-bit as follows: ˆx = QU (x, k) = clip(⌊x s ⌉ + z, 0, 2k − 1) · s. (5) Here, ⌊·⌉ denotes the round operation, s = max(x)−min(x) 2k−1 and z = −⌊min(x) s ⌉ are quantization parameters. It should be noted that the quantization parameters are stored in full-precision, which can lead to significant overhead if the quantization is fine-grained. 4 Method 4.1 A Strong Baseline for KV Cache Quantization Tokenwise quantization, as depicted in Figure 2(b) is prevalent in quantizing large language models (LLMs) due to the distinct representations of individual tokens. However, it has been widely observed, as illustrated in Figure 2(a), that outliers emerge within the channel dimensions of key and value matrices [43, 32], posing challenges for tokenwise quantization. To address this, recent work [32] resorts to groupwise quantization, where outlier channels are processed in distinct groups, as illustrated in Figure 2(c). However, this fine-grained quantization approach introduces excessive memory overhead, thereby significantly impacting the compression ratio. For instance, considering X ∈ Rb×h×l×d as the data to be quantized and a group size of n, tokenwise quantization only results in 2bl quantization parameters, while groupwise quantization would yield 2bhld n quantization parameters. Since these parameters are usually stored in full precision, this overhead would constitute a substantial portion of the storage cost for quantized data. Motivated by depthwise separable convolution [ 19], we introduce an efficient channel-separable tokenwise quantization scheme, which disentangles the channel and token dimensions. As shown in 4𝒔,𝒛∈𝑅𝑙 (b) Tokenwise Quantization (c) Groupwise Quantization (d) Channel-separable  Tokenwise Quantization 𝒄 ∈ 𝑅ℎ𝑑 (a) Visualization of  Key and Value States 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝒔,𝒛∈𝑅𝑙∗ℎ𝑑/𝑛 𝒔,𝒛∈𝑅𝑙 Figure 2: Visualization and different quantization granularities for key and value states. Here, we omit the batch dimension for simplicity. For keys, channel outliers emerge, yet token representations exhibit minimal differences. For values, both channel outliers and distinct token representations exist. Figure 2(d), our approach initiates by normalizing each channel of data X with a scaling factor c. For the i-th channel in X, the normalization process can be formulated as: Xi = Xi ci , where ci = p max(|Xi|). (6) After normalization, each channel is scaled to a closed magnitude, mitigating the influence of outliers during tokenwise quantization. Subsequently, tokenwise quantization can be reliably applied and the scales c are multiplied back to restore the magnitude of each channel. The process of channel-separable tokenwise quantization is summarized in the supplementary material. Within this quantization scheme, the total number of quantization parameters amounts to hd + 2bl, representing a notable reduction compared to groupwise quantization, while effectively balancing the outlier channels and the representation of each token. Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset. Key Cache Quantization Granularity Value Cache Quantization Granularity Quantization Parameters Compression Ratio Acc.(%) / / 0 1× 55.88 Groupwise Groupwise 4bhld/n 3.2× 54.51 Tokenwise Tokenwise 4bl 3.99× 49.81 Channelwise Tokenwise 2hd+ 2bl 4.00× 52.77 Channelwise Channel-separable Tokenwise 3hd+ 2bl 4.00× 54.74 As referred to Figure 2(a), since the differences in token representations are small in key cache, we employ channelwise quantization for the key cache to further reduce overhead and employ channel- separable tokenwise quantization for the value cache. As depicted in Table 1, this configuration yields superior performance with reduced quantization overhead compared with groupwise quantization, thereby establishing a robust baseline for KV cache quantization. 4.2 Accurate Salient Token Identification Adaptive KV cache compression [46, 43, 16] aims to discern the saliency of each token, keeping the information of salient tokens while evicting or aggressively compressing the rest, to achieve a higher compression ratio. These salient tokens, also referred to as \"Heavy Hitters\" [46], are often identified based on accumulated attention scores. Given attention score matrix A ∈ Rl×l, the saliency of token i is estimated by: pi = lX k=1 Ak,i. (7) 5So   there  are   five   eggs 1.00 0.00 0.00 0.00 0.00 0.33 0.67 0.00 0.00 0.00 0.35 0.30 0.35 0.00 0.00 0.04 0.06 0.14 0.76 0.00 0.02 0.02 0.04 0.06 0.86 egs five   are   there   So : 40% salient tokens 1.74 1.05 0.53 0.82 0.86 0.35 0.26 0.18 0.41 0.86 Question: There are 15 trees in the  grove… Let's think step by step… Question: If there are 3 cars… Let's think step by step… Question: Leah had 32 chocolates… Let's think step by step… … Question: Olivia has $23… Let's think step by step… Question: Janet’s ducks lay 16 eggs per  day…How much in dollars does she  make every day at the farmers' market? (a) (b) (c) 𝒑𝒊 ෥𝒑𝒊 Figure 3: (a) A toy example to illustrate accumulated attention scores and normalized attention scores. Initial tokens have larger attention scores and more values to be accumulated. (b) A sample from GSM8k dataset with chain-of-thoughts (CoT) prompting. (c) The probability of each token being selected as a salient token, measured by both accumulated and normalized attention scores. Tokens correspond to the final question are identified as low saliency by accumulated attention scores. Tokens with large saliency values are then considered salient tokens. However, this approach has inherent limitations due to the lower triangular nature of the attention score matrix, as illustrated in Figure 3(a). There are two primary issues. Firstly, earlier tokens benefit from having more values accumulated since the elements above the diagonal are all zero. For instance, in a sequence of length l, the initial token accumulates l positive values, whereas the final token only accumulates one. Secondly, Softmax function converts real numbers into probabilities, so that the earlier rows of the attention matrix tending to have higher values, as fewer numbers are involved in the Softmax calculation. Consequently, the accumulated attention score of the final token will always be smaller than that of the first, which exceeds 1. To address this, previous works, such as H2O [46], always maintain recent caches in full precision. Nevertheless, this solution is suboptimal since recent tokens are not necessarily the most significant ones. To enhance the evaluation of each token’s saliency, we introduce an accurate token saliency metric based on normalized attention scores ˜pi: ˜pi = Pl k=1 Ak,i nnz(A:,i) (8) Here, nnz(A:,i) denotes the number of non-zero elements in the i-th column of A. As evidenced in Figure 3(a), normalizing the accumulated attention scores mitigates the influence of excessively large values in the initial rows of the attention score matrix, thereby delivering a more precise assessment. To validate the efficacy of our new metric, we input a sample from GSM8k dataset with chain-of- thoughts (CoT) prompting to the LLaMA3-8B model and identify saliency of each token by Eq. 7 and Eq. 8, respectively. As depicted in Figure 3(b) and (c), the salient tokens are at the end of the prompt, which correspond to the question for LLM to answer. However, these tokens are identified as low saliency by accumulated attention scores. Under the KV cache compression framework, these tokens would either be discarded or quantized to extremely low bit-width, resulting in a significant performance deterioration. In contrast, our method accurately identifies the salient tokens. Additional experimental results regarding the accuracy of our method will be detailed in Section 5.2. 4.3 Efficient Approximation of Saliency Metric As analyzed in Section 4.2, adaptive KV cache compression requires the explicit computation of full attention scores, as referred to Figure 4(b), which clashes with fast attention implementations like FlashAttention [8, 7, 9]. As shown in Figure 4(c), FlashAttention computes attention outputs in tiles without storing the intermediate attention scores. To reconcile the efficiency of FlashAttention with the substantial compression offered by adaptive KV caching, we devise an effective approximation for Eq. 8 as a measure of token saliency. Specifically, we sample a small group of tokens, designated 6𝐀 = (b) Standard Attention 𝐀 (c) FlashAttention 𝑸 𝑽 𝑶 𝑽 𝐊𝑻 𝑸 𝐊𝑻 𝑶 Memory=𝑶(𝒍𝟐) More memory access & slower Memory=𝑶(𝒍) Less memory access & faster (a) Efficient Saliency Metric with Probe Tokens Input tokens 𝐀𝑝𝑟𝑜𝑏𝑒 Probe tokens (b) Standard Attention (c) FlashAttention Regular tokens Output : Intermediate attention scores Figure 4: (a): Efficient saliency metric only requires attention scores of probe tokens through standard attention, enabling fast computation for the majority of tokens through FlashAttention. (b): In standard attention, full attention scores are computed before deriving the attention output. (c): FlashAttention avoids large attention matrix memory transfers by partitioning input matrices into blocks for incremental computation. as probe tokens, and compute their attention scores Aprobe as follows: Aprobe = Softmax \u0012QprobeKT √dk \u0013 . (9) By substituting Aprobe into Eq. 8, we can approximate the saliency of all tokens. For the remaining non-probe tokens, their attention scores do not have to be computed explicitly, enabling the integration of fast attention implementations to expedite the generation process, as illustrated in Figure 4(a). Table 2: Performance comparisons of various probe strategies. Data is col- lected from LLaMA3-8B model on GSM8k dataset. We quantize 40% salient tokens to 4-bit and the remaining 60% tokens to 2-bit. The proportion of probe tokens is 10%. Probe Strategy Acc.(%) All tokens 52.54 Random tokens 47.46 Special tokens 46.78 Recent tokens 51.10 Random+recent tokens 52.08 However, the positions of the probe tokens will un- doubtedly affects the accuracy of the approximated token saliency and the selection of probe tokens is under explored. In this study, we suggest four strategies for sampling probe tokens: • Random tokens. The probe tokens are randomly sam- pled from all positions. • Special tokens. The special tokens and punctuation tokens will be treated as probe tokens. • Recent tokens. The most recent tokens are selected as probe tokens. • Random+recent tokens. The probe tokens will be di- vided into two parts, one using recent tokens and the other randomly selecting from the remaining tokens. It should be emphasized that our study diverges from prior research [16] in that, rather than directly choosing special or recent tokens as salient tokens, we opt to sample a subset of tokens as \"probes\" to detect the salient ones. As depicted in Table 2, we present a comprehensive comparison of the performance among four distinct sampling strategies. Among the four strategies examined, a hybrid approach that combines recent tokens with randomly selected tokens emerges as the most effective. Unless otherwise specified, this hybrid strategy with 5% recent tokens and 5% random tokens will be employed in our method. 5 Experiment 5.1 Implementation Details Models and datasets. To validate the efficacy of our proposed method, we conduct experiments with three open-source LLMs: Mistral [ 20], LLaMA2 [37] and LLaMA3. These models are evaluated on three challenging benchmarks: GSM8k [6] for math problem solving, HumanEval [4] for code 7generation, and Line Retrieval [25] for data retrieval. To ensure reproducibility, the reported results are obtained using the Language Model Evaluation Harness [15] and LongEval [24]. Quantization and generation settings. We employ mixed precision quantization for KV cache where salient tokens will be quantized to 4-bit while the remaining will be quantized to 2-bit. For both subsets, we apply channelwise quantization for the key cache and channel-separable tokenwise quantization for the value cache. The proportion of salient tokens will be denoted by \"Saliency Ratio\" in the experimental results. During the decoding process, ZipCache adopts a streaming strategy [21] and repeats the compression process for the KV cache whenever 100 new tokens are generated. 5.2 Comparison with SOTA methods 5.2.1 Evaluation on GSM8k We begin our evaluation on GSM8k dataset with chain-of-thoughts (CoT) prompting, and the results are presented in Table 3. This task requires LLM to solve mathematical problems and return the final answer without multiple options. This task poses considerable challenges and previous KV cache compression methods manifest notable declines in accuracy. For instance, KIVI [32] shows an accuracy drop of 7.89% on LLaMA3-8B model, indicating the suboptimality of preserving recent tokens in full precision instead of identifying salient ones. Moreover, there is a substantial decrease in accuracy, amounting to 20.4%, for MiKV [43] under the high compression ratio. This suggests that accumulated attention scores mistakenly identify salient tokens, resulting in the loss of vital information during compression. By contrast, the proposed normalized attention scores can accurately measure token saliency, leading to a substantial enhancement in accuracy by 18.27% for LLaMA3-8B models in comparison to MiKV . In comparison to GEAR [21], which quantizes the entire KV cache to 4-bit, our approach additionally quantizes 40% tokens to 2-bit with enhanced performance on Mistral-7B model. This underscores the superiority of accurate adaptive compression of KV cache. Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 41.62 H2O [46] 16/0 40.0% 2.50 × 1.67 GEAR [21] 4/4 100% 3.00 × 39.42 KIVI [32] 16/2 15.2% 3.46 × 39.04 MiKV [43] 4/2 60.0% 4.98 × 36.32 ZipCache 4/2 60.0% 4.98× 41.24 LLaMA2-7B FP16 16/16 100% 1 × 14.18 H2O [46] 16/0 40.0% 2.50 × 13.50 GEAR [21] 4/4 100% 3.00 × 12.96 KIVI [32] 16/2 15.2% 3.46 × 13.19 MiKV [43] 4/2 60.0% 4.98 × 9.02 ZipCache 4/2 60.0% 4.98× 13.50 LLaMA2-13B FP16 16/16 100% 1 × 28.05 H2O [46] 16/0 40.0% 2.50 × 26.00 GEAR [21] 4/4 100% 3.00 × 25.40 KIVI [32] 16/2 15.2% 3.46 × 27.29 MiKV [43] 4/2 60.0% 4.98 × 23.65 ZipCache 4/2 60.0% 4.98× 27.85 LLaMA3-8B FP16 16/16 100% 1 × 55.88 H2O [46] 16/0 40.0% 2.50 × 27.82 GEAR [21] 4/4 100% 3.00 × 49.43 KIVI [32] 16/2 15.2% 3.46 × 47.99 MiKV [43] 4/2 70.0% 4.69 × 35.48 ZipCache 4/2 70.0% 4.69× 53.75 5.2.2 Evaluation on Line Retrival We further evaluate the data retrieval performance of various KV cache compression methods on Line Retrieval [25] dataset, where LLMs are required to retrieve specific content from a record 8of lines using a corresponding line index. The accuracy results under various number of lines are depicted in Figure 5. Notably, all quantization-based compression methods exhibit superior performance compared to the eviction-based approach H2O [ 46]. For eviction-based methods, information is permanently discarded upon eviction, whereas quantization introduces only minor errors while preserving the integrity of the data. Additionally, in comparison to KIVI [32], which always maintains recent caches at full precision, our approach consistently achieves better retrieval accuracy. This can be attributed to the nature of retrieval tasks, where salient tokens may appear at any position within the context, rather than being confined to the most recent caches. Moreover, when compared to MiKV [43], which employs accumulated attention scores as a saliency metric, our method yields a remarkable 42% accuracy improvement when evaluated using 200 lines on the Mistral-7b model. This substantial enhancement once more highlights the effectiveness of normalized attention scores in identifying salient tokens. Additional experimental results on HumanEval [4] can be found in the supplementary material. 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA2-13B Full Cache ZipCache KIVI-2 MiKV H2O 100 200 300 400 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA3-8B Full Cache ZipCache KIVI-2 MiKV H2O 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) Mistral-7B Full Cache ZipCache KIVI-2 MiKV H2O Figure 5: Performance comparisons of various KV cache compression methods on Line Retrieval. 5.3 Generation Efficiency In this subsection, we compare the latency and memory consumption of ZipCache and MiKV [43] under various input lengths, as depicted in Figure 6. Data is collected by serving LLaMA3-8B model on a Nvidia A100 GPU. MiKV employs accumulated attention scores to estimate token saliency, necessitating the use of standard attention for both prefill and decoding phases. Conversely, through an efficient approximate saliency metric, ZipCache requires only the calculation of the attention matrix for 10% of the tokens, while the remaining 90% tokens can be computed using either FlashAttention [7] or FlashDecoding [9]. Consequently, ZipCache achieves faster inference speed and lower memory usage, boasting a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when the input length scales to 4096. (a) Prefill phase latency  (b) Decoding phase latency  (c) GPU memory Figure 6: Comparisons of prefill-phase, decoding-phase latency and memory consumption between MiKV and ZipCache. 96 Conclusion and Future Work In this paper, we have proposed ZipCache, an accurate and efficient mixed-precision quantization framework for compressing KV cache. To commence, we introduce a channel-separable quantization scheme for KV cache, effectively reducing the overhead of storing quantization parameters compared to traditional fine-grained quantization schemes without performance degradation. Additionally, we present a novel metric for accurately assessing token saliency based on normalized attention scores. This metric enables adaptive quantization of all tokens according to their saliency, leading to improved compression ratios without sacrificing model performance. Moreover, we introduce an efficient approximation method for the token saliency metric, seamlessly integrating with fast attention implementations such as FlashAttention and FlashDecoding. This enhancement signifi- cantly boosts generation speed and reduces GPU memory requirements. Our extensive experiments have demonstrated that ZipCache achieves state-of-the-art compression performance in terms of compression ratio, accuracy and generation speed. We believe that ZipCache will pave the way for more practical and scalable deployment of LLMs in various real-world applications. Limitations and Broader Impacts. While ZipCache presents promising advancements in KV cache mixed-quantization frameworks for LLMs, the saliency ratio is manually specified before evaluation and cannot be automatically adjusted based on task datasets. Moreover, similar to other generative models, ZipCache can potentially be used to generate malicious content. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [2] A. Chauhan, U. Tiwari, et al. Post training mixed precision quantization of neural networks using first- order information. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1343–1352, 2023. [3] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023. [6] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. [8] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [9] T. Dao, D. Haziza, F. Massa, and G. Sizov. Flash-decoding for long-context inference, 2023. [10] J. C. de Winter. Can chatgpt pass high school exams on english language comprehension? International Journal of Artificial Intelligence in Education, pages 1–16, 2023. [11] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318–30332, 2022. [12] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293–302, 2019. [13] M. Du, F. He, N. Zou, D. Tao, and X. Hu. Shortcut learning of large language models in natural language understanding. Communications of the ACM, 67(1):110–120, 2023. [14] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [15] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, 10L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In The Twelfth International Conference on Learning Representations, 2024. [17] Y . He, L. Liu, J. Liu, W. Wu, H. Zhou, and B. Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2023. [18] L. Hou, R. Y . Pang, T. Zhou, Y . Wu, X. Song, X. Song, and D. Zhou. Token dropping for efficient bert pretraining. arXiv preprint arXiv:2203.13240, 2022. [19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [21] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. [22] Y . J. Kim, R. Henry, R. Fahim, and H. H. Awadalla. Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms. arXiv preprint arXiv:2308.09723, 2023. [23] C. Li, W. Wang, J. Hu, Y . Wei, N. Zheng, H. Hu, Z. Zhang, and H. Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024. [24] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, , and H. Zhang. How long can open-source llms truly promise on context length?, June 2023. [25] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, and H. Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [26] Y . Li, R. Gong, X. Tan, Y . Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2020. [27] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [28] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang. Qllm: Accurate and efficient low-bitwidth quanti- zation for large language models. In The Twelfth International Conference on Learning Representations, 2024. [29] J. Liu, C. S. Xia, Y . Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. [30] Z. Liu, A. Desai, F. Liao, W. Wang, V . Xie, Z. Xu, A. Kyrillidis, and A. Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024. [31] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Krishnamoorthi, and V . Chandra. Llm- qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [32] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [33] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.arXiv preprint arXiv:2308.09583, 2023. [34] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018. [35] M. Tan, L. Wang, L. Jiang, and J. Jiang. Investigating math word problems using pretrained multilingual language models. arXiv preprint arXiv:2105.08928, 2021. [36] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [37] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 11[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [39] K. Wang, Z. Liu, Y . Lin, J. Lin, and S. Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8612–8620, 2019. [40] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [41] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [42] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1–10, 2022. [43] J. Y . Yang, B. Kim, J. Bae, B. Kwon, G. Park, E. Yang, S. J. Kwon, and D. Lee. No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024. [44] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y . Wang, M. Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pages 11875–11886. PMLR, 2021. [45] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.Advances in Neural Information Processing Systems, 35:27168–27183, 2022. [46] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2023. 12Appendix A Calculation of Overhead for Different Quantization Schemes Assuming b = 8, hd = l = 4096, and that the KV cache is quantized to4-bit, we proceed to calculate the actual compression ratio for different quantization granularities. For groupwise quantization with a group size of n = 32, the compression ratio Rgroup is given by: Rgroup = 2 × bhld × 16 2 × bhld × 4 +4bhld n × 16 = 3.200 (A) For tokenwise quantization, the compression ratio Rtoken can be calculated as: Rtoken = 2 × bhld × 16 2 × bhld × 4 + 4× bl × 16 = 3.992 (B) For our proposed quantization baseline, the compression ratio Rbaseline is determined by: Rbaseline = 2 × bhld × 16 2 × bhld × 4 + 3× hd × 16 + 2× bl × 16 = 3.995 (C) B Implementation Details of ZipCache In this section, we provide an overview of the channel-separable tokenwise quantization scheme in Algorithm 1. Additionally, we present the process of ZipCache’s prefill phase as described in Algorithm 2, as well as its decoding phase detailed in Algorithm 3. It is worth mentioning that during both the prefill and decoding phases, rather than calculating attention outputs separately for probe tokens and regular tokens followed by merging, FlashAttention [7] is utilized to compute the attention output for all tokens simultaneously. Additionally, attention scores of probe tokens are calculated. By bypassing the substantial memory accesses associated with matrix splitting and merging, this strategy enhances generation speed. Algorithm 1: Channel-separable Tokenwise Quantization (CSTQuant) procedure CSTQuant: Input: data X ∈ Rl×hd, target bit-width k for i ← 0 to hd do ci = p max(|Xi|) Xi = Xi ci // Normalizing each channel of X ˆX =TokenQuant(X, k) // Do tokenwise quantization for i ← 0 to hd do ˆXi = ˆXi × ci // Rescale each channel of X return ˆX C Additional Experimental Results C.1 Accuracy and Efficiency Comparisons of various KV cache compression methods In this section, we present the accuracy and efficiency comparisons of various KV cache compression methods, as presented in Table A. Data is collected by evaluating LLaMA3-8B model on200-line retrieval task with a Nvidia A100 GPU. We use a batch size of 8 and an average input length of 3072. Among these methods, ZipCache achieves the highest accuracy, compression ratio and generation speed. Specifically, in comparison to MiKV [43], which identifies salient tokens through accumulated attention scores, our method achieves a notable 10.0% accuracy improvement by accurately pinpointing salient tokens and a substantial38.0% decrease in prefill latency by integrating FlashAttention [7]. 13Algorithm 2: ZipCache for Prefill Phase procedure ZipCachePrefill: Input: Query states Q, key states K, value states V, saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl // Salient Token Identification Select probe tokens and compute their attention scores Aprobe by Eq. 9 Measure the token saliency ˜p with Aprobe by Eq. 8 // Computing Attention Output with FlashAttention O = FlashAttention(Q, K, V) // Compressing KV Cache Partition key states: Ksalient, Kregular = Split(K, ˜p, r%) Partition value states: Vsalient, Vregular = Split(V, ˜p, r%) Ksalient = ChannelQuant(Ksalient, kh), Vsalient = CSTQuant(Vsalient, kh) Kregular = ChannelQuant(Kregular, kl), Vregular = CSTQuant(Vregular, kl) ˆK = Concat(Ksalient, Kregular) ˆV = Concat(Vsalient, Vregular) // Return Attention Output and Compressed KV Cache return O, ( ˆK, ˆV) Algorithm 3: ZipCache for Decoding Phase procedure ZipCacheDecoding: Input: Query vector q, key vector k, value vector v, KV cache ( ˆK, ˆV), saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl, decoding token index i, probe attention score Aprobe K = Concat(k, ˆK) // Concatenate key cache V = Concat(v, ˆV) // Concatenate value cache o = FlashAttention(q, K, V) // Compute attention output i = i + 1 if i == 100then // Re-compress every 100 tokens Extract K[: −100] and V[: −100] and adaptively compress them with Aprobe Reset i = 0, Aprobe = None else if i >95 or randint(0, 100) < 5 then // probe tokens consists of 5% recent and 5% random tokens. Compute attention scores a of current token by Eq. 4 Aprobe = Concat(a, Aprobe) // Return Attention Output, KV Cache and Attention Scores from Probe Tokens return o, (K, V), Aprobe C.2 Evaluation on HumanEval In this section, we assess the performance of code generation across various KV cache compression methods, as summarized in Table B. Remarkably, ZipCache attains a compression ratio of 4.94× without sacrificing performance when tested with the Mistral-7B model, outperforming predecessor methods. Moreover, when evaluating on LLaMA3-8B model, our approach outperforms KIVI-2 [32] by 7.32% with a significantly higher compression ratio (4.39× vs. 2.55×). It should be noted that the average input length for this task is only119, while KIVI retains the recent 32 tokens in full-precision, thereby considerably diminishing its overall compression ratio. This underscores the advantage of ZipCache over methods that consistently retain information of recent tokens. 14Table A: Accuracy and efficiency comparisons over LLaMA3-8B on the200-line retrieval task. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. 0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 3072. Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Prefill-phase Latency (ms) FP16 16/16 100% 1 × 100 2340.11 H2O [46] 16/0 40.0% 2.50 × 0 4335.01 GEAR [21] 4/4 100% 3.00 × 100 5957.76 KIVI [32] 16/2 8.33% 4.36 × 96 4010.14 MiKV [43] 4/2 80.0% 4.43 × 90 4170.61 ZipCache 4/2 80.0% 4.43× 100 2584.01 Table B: Performance comparisons on HumanEval for code generation. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively.0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 120. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 29.27 H2O [46] 16/0 40.0% 2.50 × 14.63 GEAR [21] 4/4 100% 3.00 × 28.05 KIVI [32] 16/2 26.7% 2.55 × 28.05 MiKV [43] 4/2 60.0% 4.94 × 27.44 ZipCache 4/2 60.0% 4.94× 29.27 LLaMA2-7B FP16 16/16 100% 1 × 14.02 H2O [46] 16/0 40.0% 2.50 × 11.59 GEAR [21] 4/4 100% 3.00 × 13.02 KIVI [32] 16/2 26.7% 2.55 × 11.59 MiKV [43] 4/2 80.0% 4.39 × 10.37 ZipCache 4/2 80.0% 4.39× 12.80 LLaMA3-8B FP16 16/16 100% 1 × 33.54 H2O [46] 16/0 40.0% 2.50 × 15.85 GEAR [21] 4/4 100% 3.00 × 28.66 KIVI [32] 16/2 26.7% 2.55 × 25.61 MiKV [43] 4/2 80.0% 4.39 × 29.88 ZipCache 4/2 80.0% 4.39× 32.93 15",
      "meta_data": {
        "arxiv_id": "2405.14256v1",
        "authors": [
          "Yefei He",
          "Luoming Zhang",
          "Weijia Wu",
          "Jing Liu",
          "Hong Zhou",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T07:37:16Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14256v1.pdf"
      }
    }
  ]
}