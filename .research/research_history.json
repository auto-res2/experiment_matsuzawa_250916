{
  "research_topic": "グラフニューラルネットワークの過平滑化に関して改善したい",
  "queries": [
    "GNN over-smoothing",
    "GNN oversmoothing mitigation",
    "oversmoothing prevention GNN",
    "skip connections GNN",
    "DropEdge GNN oversmoothing"
  ],
  "research_study_list": [
    {
      "title": "A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks",
      "abstract": "Oversmoothing is a central challenge of building more powerful Graph Neural\nNetworks (GNNs). While previous works have only demonstrated that oversmoothing\nis inevitable when the number of graph convolutions tends to infinity, in this\npaper, we precisely characterize the mechanism behind the phenomenon via a\nnon-asymptotic analysis. Specifically, we distinguish between two different\neffects when applying graph convolutions -- an undesirable mixing effect that\nhomogenizes node representations in different classes, and a desirable\ndenoising effect that homogenizes node representations in the same class. By\nquantifying these two effects on random graphs sampled from the Contextual\nStochastic Block Model (CSBM), we show that oversmoothing happens once the\nmixing effect starts to dominate the denoising effect, and the number of layers\nrequired for this transition is $O(\\log N/\\log (\\log N))$ for sufficiently\ndense graphs with $N$ nodes. We also extend our analysis to study the effects\nof Personalized PageRank (PPR), or equivalently, the effects of initial\nresidual connections on oversmoothing. Our results suggest that while PPR\nmitigates oversmoothing at deeper layers, PPR-based architectures still achieve\ntheir best performance at a shallow depth and are outperformed by the graph\nconvolution approach on certain graphs. Finally, we support our theoretical\nresults with numerical experiments, which further suggest that the\noversmoothing phenomenon observed in practice can be magnified by the\ndifficulty of optimizing deep GNN models.",
      "full_text": "A NON -ASYMPTOTIC ANALYSIS OF OVERSMOOTHING IN GRAPH NEURAL NETWORKS Xinyi Wu1,2, Zhengdao Chen3*, William Wang2, and Ali Jadbabaie1,2 1Institute for Data, Systems, and Society (IDSS), Massachusetts Institute of Technology 2Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology 3Courant Institute of Mathematical Sciences, New York University ABSTRACT Oversmoothing is a central challenge of building more powerful Graph Neural Networks (GNNs). While previous works have only demonstrated that oversmoothing is inevitable when the number of graph convolutions tends to inﬁnity, in this paper, we precisely characterize the mechanism behind the phenomenon via a non-asymptotic analysis. Speciﬁcally, we distinguish between two different effects when applying graph convolutions—an undesirable mixing effect that homogenizes node representations in different classes, and a desirable denoising effect that homogenizes node representations in the same class. By quantifying these two effects on random graphs sampled from the Contextual Stochastic Block Model (CSBM), we show that oversmoothing happens once the mixing effect starts to dominate the denoising effect, and the number of layers required for this transition is O(log N/log(log N)) for sufﬁciently dense graphs with N nodes. We also extend our analysis to study the effects of Personalized PageRank (PPR), or equivalently, the effects of initial residual connections on oversmoothing. Our results suggest that while PPR mitigates oversmoothing at deeper layers, PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs. Finally, we support our theoretical results with numerical experiments, which further suggest that the oversmoothing phenomenon observed in practice can be magniﬁed by the difﬁculty of optimizing deep GNN models. 1 Introduction Graph Neural Networks (GNNs) are a powerful framework for learning with graph-structured data and have shown great promise in diverse domains such as molecular chemistry, physics, and social network analysis [1, 2, 3, 4, 5, 6, 7]. Most GNN models are built by stacking graph convolutions or message-passing layers [8], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes. The most representative and popular example is the Graph Convolutional Network (GCN) [9], which has demonstrated success in node classiﬁcation, a primary graph task which asks for node labels and identiﬁes community structures in real graphs. Despite these achievements, the choice of depth for these GNN models remains an intriguing question. GNNs often achieve optimal classiﬁcation performance when networks are shallow. Many widely used GNNs such as the GCN are no deeper than 4 layers [9, 10], and it has been observed that for deeper GNNs, repeated message-passing makes node representations in different classes indistinguishable and leads to lower node classiﬁcation accuracy—a phenomenon known as oversmoothing [9, 11, 12, 10, 13, 14, 15, 16]. Through the insight that graph convolutions can be regarded as low-pass ﬁlters on graph signals, prior studies have established that oversmoothing is inevitable when the number of layers in a GNN increases to inﬁnity [11, 13]. However, these asymptotic analyses do not fully explain the rapid occurrence of oversmoothing when we increase the network depth, let alone the fact that for some datasets, having no graph convolution is even optimal [17]. These observations motivate the following key questions about oversmoothing in GNNs: Why does oversmoothing happen at a relatively shallow depth? Can we quantitatively model the effect of applying a ﬁnite number of graph convolutions and theoretically predict the “sweet spot” for the choice of depth? In this paper, we propose a non-asymptotic analysis framework to study the effects of graph convolutions and oversmoothing using the Contextual Stochastic Block Model (CSBM) [18]. The CSBM mimics the community structure Correspondence to: xinyiwu@mit.edu *Now at Google. arXiv:2212.10701v2  [cs.LG]  1 Mar 2023A B Figure 1: Illustration of how oversmoothing happens. Stacking GNN layers will increase both the mixing and denoising effects counteracting each other. Depending on the graph characteristics, either the denoising effect dominates the mixing effect, resulting in less difﬁculty classifying nodes ( A), or the mixing effect dominates the denoising effect, resulting in more difﬁculty classifying nodes (B)—this is when oversmoothing starts to happen. of real graphs and enables us to evaluate the performance of linear GNNs through the probabilistic model with ground truth community labels. More importantly, as a generative model, the CSBM gives us full control over the graph structure and allows us to analyze the effect of graph convolutions non-asymptotically. In particular, we distinguish between two counteracting effects of graph convolutions: • mixing effect (undesirable): homogenizing node representations in different classes; • denoising effect (desirable): homogenizing node representations in the same class. Adding graph convolutions will increase both the mixing and denoising effects. As a result, oversmoothing happens not just because the mixing effect keeps accumulating as the depth increases, on which the asymptotic analyses are based [11, 13], but rather because the mixing effect starts to dominate the denoising effect (see Figure 1 for a schematic illustration). By quantifying both effects as a function of the model depth, we show that the turning point of the tradeoff between the two effects is O(log N/log(log N)) for graphs with N nodes sampled from the CSBM in sufﬁciently dense regimes. Besides new theory, this paper also presents numerical results directly comparing theoretical predictions and empirical results. This comparison leads to new insights highlighting the fact that the oversmoothing phenomenon observed in practice is often a mixture of pure oversmoothing and difﬁculty of optimizing weights in deep GNN models. In addition, we apply our framework to analyze the effects of Personalized PageRank (PPR) on oversmoothing. Personalized propagation of neural predictions (PPNP) and its approximate variant (APPNP) make use of PPR and its approximate variant, respectively, and were proposed as a solution to mitigate oversmoothing while retaining the ability to aggregate information from larger neighborhoods in the graph [12]. We show mathematically that PPR makes the model performance more robust to increasing number of layers by reducing the mixing effect at each layer, while it nonetheless reduces the desirable denoising effect at the same time. For graphs with a large size or strong community structure, the reduction of the denoising effect would be greater than the reduction of the mixing effect and thus PPNP and APPNP would perform worse than the baseline GNN on those graphs. Our contributions are summarized as follows: • We show that adding graph convolutions strengthens the denoising effect while exacerbates the mixing effect. Over- smoothing happens because the mixing effect dominates the denoising effect beyond a certain depth. For sufﬁciently dense CSBM graphs with N nodes, the required number of layers for this to happen is O(log N/log(log N)). • We apply our framework to rigorously characterize the effects of PPR on oversmoothing. We show that PPR reduces both the mixing effect and the denoising effect of message-passing and thus does not necessarily improve node classiﬁcation performance. • We verify our theoretical results in experiments. Through comparison between theory and experiments, we ﬁnd that the difﬁculty of optimizing weights in deep GNN architectures often aggravates oversmoothing. 2 Additional Related Work Oversmoothing problem in GNNs Oversmoothing is a well-known issue in deep GNNs, and many techniques have been proposed to relieve it practically [19, 20, 15, 21, 22]. On the theory side, by viewing GNN layers as a form of Laplacian ﬁlter, prior works have shown that as the number of layers goes to inﬁnity, the node representations within each connected component of the graph will converge to the same values [11, 13]. However, oversmoothing can be observed in GNNs with as few as 2 −4 layers [9, 10]. The early onset of oversmoothing renders it an important concern in practice, and it has not been satisfyingly explained by the previous asymptotic studies. Our work addresses this 2gap by quantifying the effects of graph convolutions as a function of model depth and justifying why oversmoothing happens in shallow GNNs. A recent study shared a similar insight of distinguishing between two competing effects of message-passing and showed the existence of an optimal number of layers for node prediction tasks on a latent space random graph model. But the result had no further quantiﬁcation and hence the oversmoothing phenomemon was still only characterized asymptotically [16]. Analysis of GNNs on CSBMs Stochastic block models (SBMs) and their contextual counterparts have been widely used to study node classiﬁcation problems [23, 24]. Recently there have been several works proposing to use CSBMs to theoretically analyze GNNs for the node classiﬁcation task. [ 25] used CSBMs to study the function of nonlinearity on the node classiﬁcation performance, while [26] used CSBMs to study the attention-based GNNs. More relevantly, [27, 28] showed the advantage of applying graph convolutions up to three times for node classiﬁcation on CSBM graphs. Nonetheless, they only focused on the desirable denoising effect of graph convolution instead of its tradeoff with the undesirable mixing effect, and therefore did not explain the occurance of oversmoothing. 3 Problem Setting and Main Results We ﬁrst introduce our theoretical analysis setup using the Contextual Stochastic Block Model (CSBM), a random graph model with planted community structure [18, 27, 28, 29, 25, 26]. We choose the CSBM to study GNNs for the node classiﬁcation task because the main goal of node classiﬁcation is to discover node communities from the data. The CSBM imitates the community structure of real graphs and provides a clear ground truth for us to evaluate the model performance. Moreover, it is a generative model which gives us full control of the data to perform a non-asymptotic analysis. We then present a set of theoretical results establishing bounds for the representation power of GNNs in terms of the best-case node classiﬁcation accuracy. The proofs of all the theorems and additional claims will be provided in the Appendix. 3.1 Notations We represent an undirected graph with N nodes by G= (A,X), where A∈{0,1}N×N is the adjacency matrix and X ∈RN is the node feature vector. For nodes u,v ∈[N], Auv = 1 if and only if uand vare connected with an edge in G, and Xu ∈R represents the node feature of u. We let 1 N denote the all-one vector of length N and D= diag(A1 N) be the degree matrix of G. 3.2 Theoretical Analysis Framework Contextual Stochastic Block Models We will focus on the case where the CSBM consists of two classes C1 and C2 of nodes of equal size, in total with N nodes. For any two nodes in the graph, if they are from the same class, they are connected by an edge independently with probability p, or if they are from different classes, the probability is q. For each node v∈Ci,i ∈{1,2}, the initial feature Xv is sampled independently from a Gaussian distribution N(µi,σ2), where µi ∈R,σ ∈(0,∞). Without loss of generality, we assume that µ1 <µ2. We denote a graph generated from such a CSBM as G(A,X) ∼CSBM(N,p,q,µ 1,µ2,σ2). We further impose the following assumption on the CSBM used in our analysis. Assumption 1. p,q = ω(log N/N) and p>q > 0. The choice p,q = ω(log N/N) ensures that the generated graph Gis connected almost surely [ 23] while being slightly more general than the p,q = ω(log2 N/N) regime considered in some concurrent works [27, 25]. In addition, this regime also guarantees that Ghas a small diameter. Real-world graphs are known to exhibit the “small-world\" phenomenon—even if the number of nodes N is very large, the diameter of graph remains small [ 30, 31]. We will see in the theoretical analysis (Section 3.3) how this small-diameter characteristic contributes to the occurrence of oversmoothing in shallow GNNs. We remark that our results in fact hold for the more general choice of p,q = Ω(log N/N), for which only the concentration bound in Theorem 1 needs to be modiﬁed in the threshold log N/N case where all the constants need a more careful treatment. Further, the choice p > qensures that the graph structure has homophily, meaning that nodes from the same class are more likely to be connected than nodes from different classes. This characteristic is observed in a wide range of real-world graphs [32, 29]. We note that this homophily assumption (p>q ) is not essential to our analysis, though we add it for simplicity since the discussion of homophily versus heterophily (p<q ) is not the focus of our paper. Graph convolution and linear GNN In this paper, our theoretical analysis focuses on the simpliﬁed linear GNN model deﬁned as follows: a graph convolution using the (left-)normalized adjacency matrix takes the operation 3h′= (D−1A)h, where hand h′are the input and output node representations, respectively. A linear GNN layer can then be deﬁned as h′= (D−1A)hW, where W is a learnable weight matrix. As a result, the output of nlinear GNN layers can be written as h(n) ∏n k=1 W(k), where h(n) = (D−1A)nX is the output of ngraph convolutions, and W(k) is the weight matrix of the kth layer. Since this is linear in h(n), it follows that n-layer linear GNNs have the equivalent representation power as linear classiﬁers applied to h(n). In practice, when building GNN models, nonlinear activation functions can be added between consecutive linear GNN layers. For additional results showing that adding certain nonlinearity would not improve the classiﬁcation performance, see Appendix K.1. Bayes error rate and z-score Thanks to the linearity of the model, we see that the representation of node v ∈Ci after ngraph convolutions is distributed as N(µ(n) i ,(σ(n))2), where the variance (σ(n))2 is shared between classes. The optimal node-wise classiﬁer in this case is the Bayes optimal classiﬁer, given by the following lemma. Lemma 1. Suppose the label y is drawn uniformly from {1,2}, and given y, x ∼ N(µ(n) y ,(σ(n))2). Then the Bayes optimal classiﬁer, which minimizes the probability of misclassiﬁcation among all classiﬁers, has decision boundary D= (µ1 + µ2)/2, and predicts y = 1,if x ≤D or y = 2,if x >D. The associated Bayes error rate is 1 −Φ(z(n)), where Φ denotes the cumulative distribution function of the standard Gaussian distribution and z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) is the z-score of Dwith respect to N(µ(n) 1 ,(σ(n))2). Lemma 1 states that we can estimate the optimal performance of an n-layer linear GNN through the z-score z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n). A higher z-score indicates a smaller Bayes error rate, and hence a better expected performance of node classiﬁcation. The z-score serves as a basis for our quantitative analysis of oversmoothing. In the following section, by estimating µ(n) 2 −µ(n) 1 and (σ(n))2, we quantify the two counteracting effects of graph convolutions and obtain bounds on the z-score z(n) as a function of n, which allows us to characterize oversmoothing quantitatively. Speciﬁcally, there are two potential interpretations of oversmoothing based on the z-score: (1) z(n) <z(n⋆), where n⋆ = arg maxn′z(n′); and (2) z(n) <z (0). They correspond to the cases (1) n>n ⋆; and (2) n>n 0, where n0 ≥0 corresponds to the number of layers that yield a z-score on par with z(0). The bounds on the z-score z(n), z(n) lower and z(n) upper, enable us to estimate n⋆ and n0 under different scenarios and provide insights into the optimal choice of depth. 3.3 Main Results We ﬁrst estimate the gap between the means µ(n) 2 −µ(n) 1 with respect to the number of layers n. µ(n) 2 −µ(n) 1 measures how much node representations in different classes have homogenized afternGNN layers, which is the undesirable mixing effect. Lemma 2. For n∈N ∪{0}, assuming D−1A≈E[D]−1E[A], µ(n) 2 −µ(n) 1 = (p−q p+ q )n (µ2 −µ1) . Lemma 2 states that the means µ(n) 1 and µ(n) 2 get closer exponentially fast and as n →∞, both µ(n) 1 and µ(n) 2 will converge to the same value (in this case ( µ(n) 1 + µ(n) 2 ) /2). The rate of change (p−q)/(p+ q) is determined by the intra-community edge density pand the inter-community edge density q. Lemma 2 suggests that graphs with higher inter-community density (q) or lower intra-community density (p) are expected to suffer from a higher mixing effect when we perform message-passing. We provide the following concentration bound for our estimate of µ(n) 2 −µ(n) 1 , which states that the estimate concentrates at a rate of O(1/ √ N(p+ q)). Theorem 1. Fix K ∈N and r> 0. There exists a constant C(r,K) such that with probability at least 1 −O(1/Nr), it holds for all 1 ≤k≤Kthat |(µ(k) 2 −µ(k) 1 ) − (p−q p+ q )k (µ2 −µ1)|≤ C√ N(p+ q) . 4We then study the variance (σ(n))2 with respect to the number of layers n. The variance (σ(n))2 measures how much the node representations in the same class have homogenized, which is the desirable denoising effect. We ﬁrst state that no matter how many layers are applied, there is a nontrivial ﬁxed lower bound for (σ(n))2 for a graph with N nodes. Lemma 3. For all n∈N ∪{0}, 1 Nσ2 ≤(σ(n))2 ≤σ2 . Lemma 3 implies that for a given graph, even as the number of layers ngoes to inﬁnity, the variance (σ(n))2 does not converge to zero, meaning that there is a ﬁxed lower bound for the denoising effect. See Appendix K.2 for the exact theoretical limit for the variance (σ(n))2 as ngoes to inﬁnity. We now establish a set of more precise upper and lower bounds for the variance (σ(n))2 with respect to the number of layers nin the following technical lemma. Lemma 4. Let a= Np/log N. With probability at least 1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 (σ(n))2 ≤min    ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k ,1   σ2 . Lemma 4 holds for all 1 ≤n≤N and directly leads to the following theorem with a clariﬁed upper bound where nis bounded by a constant K. Theorem 2. Let a = Np/log N. Fix K ∈N. There exists a constant C(K) such that with probability at least 1 −O(1/N), it holds for all 1 ≤n≤Kthat max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 ≤min { C min{a,2} 1 (N(p+ q))n,1 } σ2 . Theorem 2 states that the variance (σ(n))2 for each Gaussian distribution decreases more for larger graphs or denser graphs. Moreover, the upper bound implies that the variance (σ(n))2 will initially go down at least at a rate expo- nential in O(1/log N) before reaching the ﬁxed lower bound σ2/N suggested by Lemma 3. This means that after O(log N/log(log N)) layers, the desirable denoising effect homogenizing node representations in the same class will saturate and the undesirable mixing effect will start to dominate. Why does oversmoothing happen at a shallow depth? For each node, message-passing with different-class nodes homogenizes their representations exponentially. The exponential rate depends on the fraction of different-class neighbors among all neighbors (Lemma 2, mixing effect). Meanwhile, message-passing with nodes that have not been encountered before causes the denoising effect, and the magnitude depends on the absolute number of newly encountered neighbors. The diameter of the graph is approximately log N/log(Np) in the p,q = Ω(log N/N) regime [33], and thus is at most log N/log(log N) in our case. After the number of layers surpasses the diameter, for each node, there will be no nodes that have not been encountered before in message-passing and hence the denoising effect will almost vanish (Theorem 2, denoising effect). log N/log(log N) grows very slowly with N; for example, when N = 106,log N/log(log N) ≈8. This is why even in a large graph, the mixing effect will quickly dominate the denoising effect when we increase the number of layers, and so oversmoothing is expected to happen at a shallow depth1. Our theory suggests that the optimal number of layers, n⋆, is at most O(log N/log(log N)). For a more quantitative estimate, we can use Lemma 2 and Theorem 2 to compute bounds z(n) lower and z(n) upper for z= 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) and use them to infer n⋆ and n0, as deﬁned in Section 3.2. See Appendix H for detailed discussion. Next, we investigate the effect of increasing the dimension of the node features X. So far, we have only considered the case with one-dimensional node features. The following proposition states that if features in each dimension are independent, increasing input feature dimension decreases the Bayes error rate for a ﬁxed n. The intuition is that when node features provide more evidence for classiﬁcation, it is easier to classify nodes correctly. Proposition 1. Let the input feature dimension be d, X ∈RN×d. Without loss of generality, suppose for node vin Ci, initial node feature Xv ∼N([µi]d,σ2Id) independently. Then the Bayes error rate is 1 −Φ (√ d 2 (µ(n) 2 −µ(n) 1 ) σ(n) ) = 1 −Φ (√ d 2 z(n) ) , where Φ denotes the cumulative distribution function of the standard Gaussian distribution. Hence the Bayes error rate is decreasing in d, and as d→∞, it converges to 0. 1For mixing and denoising effects in real data, see Appendix K.5. 54 The effects of Personalized PageRank on oversmoothing Our analysis framework in Section 3.3 can also be applied to GNNs with other message-passing schemes. Speciﬁcally, we can analyze the performance of Personalized Propagation of Neural Predictions (PPNP) and its approximate variant, Approximate PPNP (APPNP), which were proposed for alleviating oversmoothing while still making use of multi-hop information in the graph. The main idea is to use Personalized PageRank (PPR) or the approximate Personalized PageRank (APPR) in place of graph convolutions [12]. Mathematically, the output of PPNP can be written as hPPNP = α(IN −(1 −α)(D−1A))−1X, while APPNP computes hAPPNP(n+1) = (1 −α)(D−1A)hAPPNP(n) + αX iteratively in n, where IN is the identity matrix of size N and in both cases αis the teleportation probability. Then for nodes in Ci,i ∈{1,2}, the node representations follow a Gaussian distribution N ( µPPNP i ,(σPPNP)2 ) after applying PPNP, or a Gaussian distributionN ( µAPPNP(n) i ,(σAPPNP(n))2 ) after applying nAPPNP layers. We quantify the effects on the means and variances for PPNP and APPNP in the CSBM case. We can similarly use them to calculate the z-score of (µ1 + µ2)/2 and compare it to the one derived for the baseline GNN in Section 3. The key idea is that the PPR propagation can be written as a weighted average of the standard message-passing, i.e. α(IN −(1 −α)(D−1A))−1 = ∑∞ k=0(1 −α)k(D−1A)k [34]. We ﬁrst state the resulting mixing effect measured by the difference between the two means. Proposition 2. Fix r> 0,K ∈N. For PPNP , with probability at least1−O(1/Nr), there exists a constantC(α,r,K ) such that µPPNP 2 −µPPNP 1 = p+ q p+ 2−α α q(µ2 −µ1) + ϵ. where the error term |ϵ|≤ C/ √ N(p+ q) + (1−α)K+1. Proposition 3. Let r> 0. For APPNP , with probability at least1 −O(1/Nr), µAPPNP(n) 2 −µAPPNP(n) 1 = ( p+ q p+ 2−α α q + (2 −2α)q αp+ (2 −α)q(1 −α)n (p−q p+ q )n) (µ2 −µ1) + ϵ. where the error term ϵis the same as the one deﬁned in Theorem 1 for the case of K = n. Both p+q p+ 2−α α q and (2−2α)q αp+(2−α)q(1 −α) ( p−q p+q ) are monotone increasing in α. Hence from Proposition 2 and 3, we see that with larger α, meaning a higher probability of teleportation back to the root node at each step of message-passing, PPNP and APPNP will indeed make the difference between the means of the two classes larger: while the difference in means for the baseline GNN decays as (p−q p+q )n , the difference for PPNP/APPNP is lower bounded by a constant. This validates the original intuition behind PPNP and APPNP that compared to the baseline GNN, they reduce the mixing effect of message-passing, as staying closer to the root node means aggregating less information from nodes of different classes. This advantage becomes more prominent when the number of layers nis larger, where the model performance is dominated by the mixing effect: as ntends to inﬁnity, while the means converge to the same value for the baseline GNN, their separation is lower-bounded for PPNP/APPNP. However, the problem with the previous intuition is that PPNP and APPNP will also reduce the denoising effect at each layer, as staying closer to the root node also means aggregating less information from new nodes that have not been encountered before. Hence, for an arbitrary graph, the result of the tradeoff after the reduction of both effects is not trivial to analyze. Here, we quantify the resulting denoising effect for CSBM graphs measured by the variances. We denote (σ(n))2 upper as the variance upper bound for depth nin Lemma 4. Proposition 4. For PPNP , with probability at least1 −O(1/N), it holds for all 1 ≤K ≤N that max {α2 min{a,2} 10 , 1 N } σ2 ≤(σPPNP)2 ≤max   α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 ,σ2   . Proposition 5. For APPNP , with probability at least1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) , 1 N } σ2 ≤(σAPPNP(n))2 , (σAPPNP(n))2 ≤min    ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 ,σ2   . 6By comparing the lower bounds in Proposition 4 and 5 with that in Theorem 2, we see that PPR reduces the beneﬁcial denoising effect of message-passing: for large or dense graphs, while the variances for the baseline GNN decay as 1/(Np)n, the variances for PPNP/APPNP are lower bounded by the constant α2 min{a,2}/10. In total, the mixing effect is reduced by a factor of (p−q p+q )n , while the denoising effect is reduced by a factor of 1/(Np)n. Hence PPR would cause greater reduction in the denoising effect than the improvement in the mixing effect for graphs whereN and pare large. This drawback would be especially notable at a shallow depth, where the denoising effect is supposed to dominate the mixing effect. APPNP would perform worse than the baseline GNN on these graphs in terms of the optimal classiﬁcation performance. We remark that in each APPNP layer, another way to interpret the termαX is to regard it as a residual connection to the initial representation X [15]. Thus, our theory also validates the empirical observation that adding initial residual connections allows us to build very deep models without catastrophic oversmoothing. However, our results suggest that initial residual connections do not guarantee an improvement in model performance by themselves. 5 Experiments In this section, we ﬁrst demonstrate our theoretical results in previous sections on synthetic CSBM data. Then we discuss the role of optimizing weights W(k) in GNN layers in the occurrence of oversmoothing through both synthetic data and the three widely used benchmarks: Cora, CiteSeer and PubMed [35]. Our results highlight the fact that the oversmoothing phenomenon observed in practice may be exacerbated by the difﬁculty of optimizing weights in deep GNN models. More details about the experiments are provided in Appendix J. 5.1 The effect of graph topology on oversmoothing We ﬁrst show how graph topology affects the occurrence of oversmoothing and the effects of PPR. We randomly generated synthetic graph data from CSBM( N = 2000 , p, q = 0 .0038, µ1 = 1 , µ2 = 1 .5, σ2 = 1 ). We used 60%/20%/20% random splits and ran GNN and APPNP with α= 0.1. For results in Figure 2, we report averages over 5 graphs and for results in Figure 3, we report averages over 5 runs. In Figure 2, we study how the strength of community structure affects oversmoothing. We can see that when graphs have a stronger community structure in terms of a higher intra-community edge density p, they would beneﬁt more from repeated message-passing. As a result, given the same set of node features, oversmoothing would happen later and a classiﬁer could achieve better classiﬁcation performance. A similar trend can also be observed in Figure 4A. Our theory predicts n⋆ and n0, as deﬁned in Section 3.2, with high accuracy. In Figure 3, we compare APPNP and GNN under different graph topologies. In all three cases, APPNP manifests its advantage of reducing the mixing effect compared to GNN when the number of layers is large, i.e. when the undesirable mixing effect is dominant. However, as Figure 3B,C show, when we have large graphs or graphs with strong community structure, APPNP’s disadvantage of concurrently reducing the denoising effect is more severe, particularly when the number of layers is small. As a result, APPNP’s optimal performance is worse than the baseline GNN. These observations accord well with our theoretical discussions in Section 4. A B C Figure 2: How the strength of community structure affects oversmoothing. When graphs have stronger community structure (i.e. higher a), oversmoothing would happen later. Our theory (gray bar) predicts the optimal number of layers n⋆ in practice (blue) with high accuracy (A). Given the same set of features, a classiﬁer has signiﬁcantly better performance on graphs with higher a(B,C). 7A (base case) B (larger graph) C (stronger community) Figure 3: Comparison of node classiﬁcation performance between the baseline GNN and APPNP. The performance of APPNP is more robust when we increase the model depth. However, compared to the base case (A), APPNP tends to have worse optimal performance than GNN on graphs with larger size ( B) or stronger community structure (C), as predicted by the theory. 5.2 The effect of optimizing weights on oversmoothing We investigate how adding learnable weightsW(k) in each GNN layers affects the node classiﬁcation performance in practice. Consider the case when all the GNN layers used have width one, meaning that the learnable weight matrix W(k) in each layer is a scalar. In theory, the effects of adding such weights on the means and the variances would cancel each other and therefore they would not affect the z-score of our interest and the classiﬁcation performance. Figure 4A shows the the value of n0 predicted by the z-score, the actual n0 without learnable weights in each GNN layer (meaning that we apply pure graph convolutions ﬁrst, and only train weights in the ﬁnal linear classiﬁer) according to the test accuracy and the actual n0 with learnable weights in each GNN layer according to the test accuracy. The results are averages over 5 graphs for each case. We empirically observe that GNNs with weights are much harder to train, and the difﬁculty increases as we increase the number of layers. As a result, n0 is smaller for the model with weights and the gap is larger when n0 is supposed to be larger, possibly due to greater difﬁculty in optimizing deeper architectures [36]. To relieve this potential optimization problem, we increase the width of each GNN layer [37]. Figure 4B,C presents the training and testing accuracies of GNNs with increasing width with respect to the number of layers on a speciﬁc synthetic example. The results are averages over 5 runs. We observe that increasing the width of the network mitigates the difﬁculty of optimizing weights, and the performance after adding weights is able to gradually match the performance without weights. This empirically validates our claim in Section 3.2 that adding learnable weights should not affect the representation power of GNN in terms of node classiﬁcation accuracy on CSBM graphs, besides empirical optimization issues. In practice, as we build deeper GNNs for more complicated tasks on real graph data, the difﬁculty of optimizing weights in deep GNN models persists. We revisit the multi-class node classiﬁcation task on the three widely used benchmark datasets: Cora, CiteSeer and PubMed [35]. We compare the performance of GNN without weights against the performance of GNN with weights in terms of test accuracy. We used 60%/20%/20% random splits, as in [38] and [39] and report averages over 5 runs. Figure 5 shows the same kind of difﬁculty in optimizing deeper models with A B C Figure 4: The effect of optimizing weights on oversmoothing using synthetic CSBM data. Compared to the GNN without weights, oversmoothing happens much sooner after adding learnable weights in each GNN layer, although these two models have the same representation power (A). As we increase the width of each GNN layer, the performance of GNN with weights is able to gradually match that of GNN without weights (B,C). 8Figure 5: The effect of optimizing weights on oversmoothing using real-world benchmark datasets. Adding learnable weights in each GNN layer does not improve node classiﬁcation performance but rather leads to optimization difﬁculty. learnable weights in each GNN layer as we have seen for the synthetic data. Increasing the width of each GNN layer still mitigates the problem for shallower models, but it becomes much more difﬁcult to tackle beyond 10 layers to the point that simply increasing the width could not solve it. As a result, although GNNs with and without weights are on par with each other when both are shallow, the former has much worse performance when the number of layers goes beyond 10. These results suggest that the oversmoothing phenomenon observed in practice is aggravated by the difﬁculty of optimizing deep GNN models. 6 Discussion Designing more powerful GNNs requires deeper understanding of current GNNs—how they work and why they fail. In this paper, we precisely characterize the mechanism of overmoothing via a non-asymptotic analysis and justify why oversmoothing happens at a shallow depth. Our analysis suggests that oversmoothing happens once the undesirable mixing effect homogenizing node representations in different classes starts to dominate the desirable denoising effect homogenizing node representations in the same class. Due to the small diameter characteristic of real graphs, the turning point of the tradeoff will occur after only a few rounds of message-passing, resulting in oversmoothing in shallow GNNs. It is worth noting that oversmoothing becomes an important problem in the literature partly because typical Convolutional Neural Networks (CNNs) used for image processing are much deeper than GNNs [40]. As such, researchers have been trying to use methods that have previously worked for CNNs to make current GNNs deeper [ 20, 15]. However, if we regard images as grids, images and real-world graphs have fundamentally different characteristics. In particular, images are giant grids, meaning that aggregating information between faraway pixels often requires a large number of layers. This contrasts with real-world graphs, which often have small diameters. Hence we believe that building more powerful GNNs will require us to think beyond CNNs and images and take advantage of the structure in real graphs. There are many outlooks to our work and possible directions for further research. First, while our use of the CSBM provided important insights into GNNs, it will be helpful to incorporate other real graph properties such as degree heterogeneity in the analysis. Additionally, further research can focus on the learning perspective of the problem. 7 Acknowledgement This research has been supported by a Vannevar Bush Fellowship from the Ofﬁce of the Secretary of Defense. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the Department of Defense or the U.S. Government. References [1] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IJCNN, 2005. [2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20:61–80, 2009. [3] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. 9[4] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timo- thy D. Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeurIPS, 2015. [5] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In NeurIPS, 2016. [6] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In NeurIPS, 2016. [7] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In ICLR, 2016. [8] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. [9] Thomas Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR, 2017. [10] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32:4–24, 2019. [11] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In AAAI, 2018. [12] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. [13] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In ICLR, 2020. [14] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In AAAI, 2020. [15] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020. [16] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. In NeurIPS, 2022. [17] Meng Liu, Zhengyang Wang, and Shuiwang Ji. Non-local graph neural networks. IEEE transactions on pattern analysis and machine intelligence, 2021. [18] Yash Deshpande, Andrea Montanari, Elchanan Mossel, and Subhabrata Sen. Contextual stochastic block models. In NeurIPS, 2018. [19] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018. [20] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In ICCV, 2019. [21] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864, 2020. [22] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020. [23] Emmanuel Abbe. Community detection and stochastic block models. Foundations and Trends in Communications and Information Theory, 14:1–162, 2018. [24] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural networks. In ICLR, 2019. [25] Rongzhe Wei, Haoteng Yin, J. Jia, Austin R. Benson, and Pan Li. Understanding non-linearity in graph neural networks from the bayesian-inference perspective. ArXiv, abs/2207.11311, 2022. [26] Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath. Graph attention retrospective. ArXiv, abs/2202.13060, 2022. [27] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Graph convolution for semi-supervised classiﬁca- tion: Improved linear separability and out-of-distribution generalization. In ICML, 2021. [28] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of graph convolutions in deep networks. ArXiv, abs/2204.09297, 2022. [29] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? In ICLR, 2022. 10[30] Michelle Girvan and Mark E. J. Newman. Community structure in social and biological networks. Proceedings of the National Academy of Sciences, 99:7821 – 7826, 2002. [31] Fan R. K. Chung. Graph theory in the information age. volume 57, pages 726–732, 2010. [32] David A. Easley and Jon M. Kleinberg. Networks, Crowds, and Markets: Reasoning about a Highly Connected World. 2010. [33] Fan Chung Graham and Linyuan Lu. The diameter of sparse random graphs. Advances in Applied Mathematics, 26:257–279, 2001. [34] Reid Andersen, Fan Chung Graham, and Kevin J. Lang. Local graph partitioning using pagerank vectors. In FOCS, 2006. [35] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016. [36] Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. In COLT, 2019. [37] Simon Shaolei Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In ICML, 2019. [38] Hongwei Wang and Jure Leskovec. Unifying graph convolutional neural networks and label propagation. ArXiv, abs/2002.06755, 2020. [39] Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label propagation and simple models out-performs graph neural networks. In ICLR, 2021. [40] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [41] Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition. In Stochastic Modelling and Applied Probability, 1996. [42] Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey.Internet mathematics, 3(1):79–127, 2006. [43] Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479–2506, 2016. [44] Linyuan Lu and Xing Peng. Spectra of edge-independent random graphs.The Electronic Journal of Combinatorics, 20:27, 2013. [45] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. [47] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. A Proof of Lemma 1 Following the deﬁnition of the Bayes optimal classiﬁer [41], B(x) = arg max i=1,2 P[y= i|x] , we get that the Bayes optimal classiﬁer has a linear decision boundary D= (µ1 + µ2)/2 such that the decision rule is {y= 1 if x≤D y= 2 if x> D. Probability of misclassiﬁcation could be written as P[y= 1,x> D] + P[y= 2,x ≤D] = P[x> D|y= 1]P[y= 1] + P[x≤D|y= 2]P[y= 2] = 1 2(P[x> D|y= 1] + P[x≤D|y= 2]) . 11When D= (µ1 + µ2)/2, the expression is called the Bayes error rate, which is the minimal probability of misclassiﬁca- tion among all classiﬁers. Geometrically, it is easy to see that the Bayes error rate equals 1 2 S, where Sis the overlapping area between the two Gaussian distributions N ( µ(n) 1 ,(σ(n))2 ) and N ( µ(n) 2 ,(σ(n))2 ) . Hence one can use the z-score of (µ1 + µ2)/2 with respect to either of the two Gaussian distributions to directly calculate the Bayes error rate. B Proof of Lemma 2 Under the heuristic assumption D−1A≈E[D]−1E[A], we can write µ(1) 1 = pµ1 + qµ2 p+ q , µ (1) 2 = pµ2 + qµ1 p+ q µ(k) 1 = pµ(k−1) 1 + qµ(k−1) 2 p+ q , µ (k) 2 = pµ(k−1) 2 + qµ(k−1) 1 p+ q , for all k∈N. Writing recursively, we get that µ(n) 1 = (p+ q)n + (p−q)n 2(p+ q)n µ1 + (p+ q)n −(p−q)n 2(p+ q)n µ2 , µ(n) 2 = (p+ q)n + (p−q)n 2(p+ q)n µ2 + (p+ q)n −(p−q)n 2(p+ q)n µ1 . C Proof of Theorem 1 We use ∥·∥2 to denote the spectral norm, ∥A∥2 = maxx:∥x∥=1 ∥Ax∥2. We denote ¯A= E[A], ¯D= E[D], d= A1 N and ¯d= E[d]i. We further deﬁne the following relevant vectors: w1 := 1 N, w 2 := ( 1 N/2 −1 N/2 ) , µ := ( µ11 N/2 µ21 N/2 ) . The quantity of interest is µ(k) 2 −µ(k) 1 = 1 N/2 w⊤ 2 (D−1A)kµ. C.1 Auxiliary results We record some properties of the adjacency matrices: 1. D−1Aand ¯D−1 ¯Ahave an eigenvalue of 1, corresponding to the (right) eigenvector w1. 2. If Jn = 1 n1 ⊤ n, where 1 n is all-one vector of length n, then ¯A:= ( pJN/2 qJN/2 qJN/2 pJN/2 ) . 3. ¯D= N 2 (p+ q)IN. 4. µ= αw1 + βw2, where α= µ1+µ2 2 and β = µ1−µ2 2 . To control the degree matrix D−1, we will use the following standard Chernoff bound [42]: Lemma 5 (Chernoff Bound). Let X1,...,X n be independent, S := ∑n i=1 Xi, and ¯S = E[S]. Then for all ε> 0, P(S ≤¯S−ε) ≤e−ε2/(2 ¯S), P(S ≥¯S+ ε) ≤e−ε2/(2( ¯S+ε/3)). We can thus derive a uniform lower bound on the degree of every vertex: Corollary 1. For every r >0, there is a constant C(r) such that whenever ¯d ≥Clog N, with probability at least 1 −N−r, 1 2 ¯d≤di ≤3 2 ¯d, for all 1 ≤i≤N. Consequently, with probability at least 1 −N−r, ∥D−1 −¯D−1∥2 ≤C/¯dfor some C. 12Proof. By applying Lemma 1 and a union bound, all degrees are within 1/2 ¯dof their expectations, with probability at least 1 −e−¯d/8+log N. Taking C = 8r+ 8 yields the desired lower bound. An analogous proof works for the upper bound. To show the latter part, write ∥D−1 −¯D−1∥2 = max 1≤i≤N |di −¯d| di¯d Using the above bounds, the numerator for each iis at most 1/2 ¯dand the denominator for each iis at least 1/2 ¯d2, with probability at least 1 −N−r. Combining the bounds yields the claim. We will also need a result on concentration of random adjacency matrices, which is a corollary of the sharp bounds derived in [43] Lemma 6 (Concentration of Adjacency Matrix) . For every r >0, there is a constant C(r) such that whenever ¯d≥log N, with probability at least 1 −N−r, ∥A−¯A∥2 <C √ ¯d. Proof. By corollary 3.12 from [43], there is a constant κsuch that P(∥A−¯A∥2 ≥3 √ ¯d+ t) ≤e−t2/κ+log N. Setting t= √ (1 + r) ¯d, C = 3 + √ (1 + r)κsufﬁces to achieve the desired bound. C.2 Sharp concentration of the random walk operator D−1A In this section, we aim to show the following concentration result for the random walk operator D−1A: Theorem 3. Suppose the edge probabilities are ω ( log N N ) , and let ¯dbe the average degree. For anyr, there exists a constant Csuch that for sufﬁciently large N, with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . Proof. We decompose the error E = D−1A−¯D−1 ¯A= D−1(A−¯A) + (D−1 −¯D−1) ¯A= T1 + T2, where T1 = D−1(A−¯A), T 2 = (D−1 −¯D−1) ¯A. We bound the two terms separately. Bounding T1: By Corollary 1, ∥D−1∥2 = max i1/di ≤2/¯d with probability 1 −N−r. Combining this with Lemma 6, we see that with probability at least 1 −2N−r, ∥D−1(A−¯A)∥2 ≤∥D−1∥2∥A−¯A∥2 ≤ C√¯d for some Cdepending only on r. Bounding T2: Similar to [44], we bound T2 by exploiting the low-rank structure of the expected adjacency matrix, ¯A. Recall that ¯Ahas a special block form. The eigendecomposition of ¯Ais thus ¯A= 2∑ j=1 λjw(j), where w(1) = 1√ N1 N,λ1 = N(p+q) 2 ,w(2) = 1√ N ( 1 N/2 −1 N/2 ) ,λ2 = N(p−q) 2 . 13Using the deﬁnition of the spectral norm, we can bound ∥T2∥2 as ∥T2∥2 ≤max ∥x∥=1 ∥(D−1 −¯D−1) ¯Ax∥2 ≤ max α∈R2,∥α∥=1 ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 . Note that when ∥α∥2 = 1, ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 2 = N∑ i=1 (1 di −1 ¯d )2   2∑ j=1 λjαjw(j) i   2 ≤ N∑ i=1 (1 di −1 ¯d )2 2∑ j=1 λ2 j(w(j) i )2 using Cauchy-Schwarz. Since |w(j) i |≤ 1√ N for all i,j, the second summation can be bounded by 1 N ∑2 j=1 λ2 j. Overall, the upper bound is now 1 N N∑ i=1 (di −¯d)2 (di¯d)2 2∑ j=1 λ2 j , Under the event of Corollary 1, di ≥C¯dfor some C <1. Under our setup, we also have λ2 1 = ¯d2, λ2 2 ≤ ¯d2. This means that the upper bound is 1 C2 ¯d2N∥d−¯d1 N∥2 2 , where dis the vector of node degrees. It remains to show that 1 N∥d−¯d1 N∥2 2 = O( ¯d). To do this, we use a form of Talagrand’s concentration inequality, given in [45]. Since the function 1√ N∥d−¯d1 N∥2 = 1√ N∥(A−¯dIN)1 N∥2 is a convex, 1-Lipschitz function of A, Theorem 6.10 from [45] guarantees that for any t> 0, P( 1√ N ∥d−¯d1 N∥2 >E[ 1√ N ∥d−¯d1 N∥2] + t) ≤e−t2/2. Using Jensen’s inequality, E[∥d−¯d1 N∥2] ≤ √ E[∥d−¯d1 N∥2 2] = √ N∑ i=1 Var(di) = √ NVar(d1) ≤ √ N¯d. If ¯d= ω(log N), we can guarantee that 1√ N ∥d−¯d1 N∥2 ≤C √ ¯d with probability at least 1 −e−(C−1)2 ¯d/2 = 1 −O(N−r) for an appropriate constant C. Thus we have shown that with high probability, T2 = O(1/ √¯d), which proves the claim. C.3 Proof of Theorem 1 Fix rand K. We desire to bound 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ. By the ﬁrst property of adjacency matrices in auxiliary results, it sufﬁces to bound β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2. 14where β = µ1−µ2 2 . We will show inductively that there is a Csuch that for every k= 1,...,K , ∥(D−1A)k −( ¯D−1 ¯A)k∥2 ≤C/ √ ¯d. If this is true, then Cauchy-Schwarz gives β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2 ≤β 1 N/2∥w2∥2∥(D−1A)k −( ¯D−1 ¯A)k∥2∥w2∥2 ≤C/ √ ¯d. By Theorem 3, we have that with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . So D−1A= ¯D−1 ¯A+ J where ∥J∥≤ C/ √¯d. Iterating, we have ∥(D−1A)k −( ¯D−1 ¯A)k∥2 = ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 (1) Inductively, (D−1A)k−1 = ( ¯D−1 ¯A)k−1 + H where ∥H∥2 ≤C/ √¯d. Plugging this in (1), we have ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 = ∥(( ¯D−1 ¯A)k−1 + H)( ¯D−1 ¯A+ J) −( ¯D−1 ¯A)k∥2 . Of these terms, ( ¯D−1 ¯A)k−1J has norm at most ∥J∥2, H( ¯D−1 ¯A) has norm at most ∥H∥2, and HJ has norm at most C/¯d.2 Hence the induction step is complete. We have thus shown that there is a constantC(r,K) such that with probability at least 1 −N−r, | 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ|≤ C ¯d . which proves the claim. By simulation one can verify that indeed 1 N/2 w⊤ 2 ( ¯D−1 ¯A)kµ ≈ ( p−q p+q )k (µ2 −µ1). Figure 6 presents µ(n) 1 ,µ(n) 2 calculated from simulation against predicted values from our theoretical results. The simulation results are averaged over 20 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 6: Comparison of the mean estimation in Lemma 2 against simulation results. D Proof of Lemma 3 Fix n and let the element in the ith row and jth column of (D−1A)n be p(n) ij . Consider a ﬁxed node i. The variance of the feature for node iafter nlayers of convolutions is (∑ j(p(n) ij )2)σ2, by the basic property of variance of sum. Since ∑ j|p(n) ij |= 1, it follows that ∑ j(p(n) ij )2 ≤1, which is the second inequality. To show the ﬁrst inequality, consider the following optimization problem: 2More precisely, the C becomes Ck, which is why we restrict the approximation guarantee to constant K. 15min p(n) ij ,1 ≤j ≤N ∑ j (p(n) ij )2 s.t. ∑ j p(n) ij = 1, p(n) ij ≥0, 1 ≤j ≤N This part of proof goes by contradiction. Suppose ∃k,l such that p(n) ik ̸= ∃p(n) il . Fixing all other p(n) ij ,j ̸= k,l, if we average p(n) ik and p(n) il , their sum of squares will strictly decrease while not breaking the constraints: 2 (p(n) ik + p(n) il 2 )2 −((p(n) ik )2 + (p(n) il )2) = −1 2(p(n) ik −p(n) il )2 <0 . So we obtain a contradiction. Thus to minimize ∑ j(p(n) ij )2, p(n) ij = 1 N,1 ≤j ≤N, and the mimimum is 1/N. E Proof of Lemma 4 The proof relies on the following deﬁnition of neighborhood size: in a graph G, we denote by Γk(x) the set of vertices in Gat distance kfrom a vertex x: Γk(x) = {y∈G : d(x,y) = k}. we deﬁne Nk(x) to be the set of vertices within distance kof x: Nk(x) = k⋃ i=0 Γi(x) . To prove the lower bound, we ﬁrst show an intermediate step that 1 |Nn|σ2 ≤(σ(n))2 . The proof is the same as the one for the ﬁrst inequality in Lemma 3, except we add in another constraint that for a ﬁxed i, the row pi·is |Nn(i)|-sparse. This implies that the minimum of ∑ j(p(n) ij )2 becomes 1/|Nn(i)|. The we apply the result on upper bound of neighborhood sizes in Erd˝os-Rényi graph G(N,p) (Lemma 2 [33]), as it also serves as upper bound of neighborhood sizes in SBM(N, p, q). The result implies that with probability at least 1 −O(1/N), we have |Nn|≤ 10 min{a,2}(Np)n,∀1 ≤n≤N. (2) We ignore ifor Nn because of all nodes are identical in CSBM, so the bound applies for every nodes in the graph. The proof of upper bound is combinatorial. Corollary 1 states that whenNis large, the degree of nodeiis approximately the expected degree in G, namely, E[degree] = N 2 (p+ q). Since p(n) ij = ∑ path P={i,v1,...,vn−1,j} 1 deg(i) 1 deg(v1)... 1 deg(vn−1) , (3) using the approximation of degrees, we get that p(n) ij = ( 2 N(p+ q) )n (# of paths P of length n between i and j) . Then we use a tree approximation to calculate the number of paths P of length nbetween iand jby regarding ias the root. Note that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 ∑ j∈Γn−2k (p(n) ij )2 (4) and for j ∈Γn−2k, a deterministic path P′of length n−2kis needed in order to reach j from i. This implies that there are only ksteps deviating from P′. There are (n−2k+ 1)k ways of choosing when to deviate. For each speciﬁc 16way of when to deviate, there are approximately E[degree]k ways of choosing the destinations for deviation. Hence in total, for j ∈Γn−2k, there are (n−2k+ 1)kE[degree]k path of length nbetween iand j. Thus p(n) ij = (n−2k+ 1)k ( 2 N(p+ q) )n−k . (5) Plug in (5) into (4), we get that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 |Γn−2k|(n−2k+ 1)2k ( 2 N(p+ q) )2n−2k (6) ≤ ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k (7) Again, (7) follows from using the upper bound on |Γn−2k|[33] such that with probability at least 1 −O(1/N), |Γn−2k|≤ 9 min{a,2}(Np)n−2k,∀1 ≤k≤⌊n 2 ⌋. Combining with Lemma 3, we obtain the ﬁnal result. Figure 7 presents variance calculated from simulation against predicted upper and lower bounds from our theoretical results. The simulation results are averaged over 1000 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 7: Comparison of the bounds on variance in Theorem 2 against simulation results. F Proof of Theorem 2 When we ﬁx K ∈N, only the upper bound in Theorem 2 will change. Note that now the upper bound in (7) can be written as ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k (p+ q 2p )2k( 2p p+ q )n( 2 N(p+ q) )n ≤ C min{a,2} ( C∑ k=0 (p+ q 2p )2k)( 2 N(p+ q) )n ≤ C min{a,2} ( 2 N(p+ q) )n . G Proof of Proposition 1 Let the node representation vector of node v after n graph convolutions be h(n) v . The Bayes error rate could be written as 1 2 (P[h(n) v > D|v ∈C1] + P[h(n) v ≤D|v ∈C2]). For d ∈N, due to the symmetry of our setup, one can easily see that the optimal linear decision boundary is the hyperplane ∑d j=1 xj = d 2 (µ1 + µ2). Then for v ∈C1, 17∑d j=1 (h(n) v )j ∼N(dµ(n) 1 ,d(σ(n))2) and for v∈C2, ∑d j=1 (h(n) v )j ∼N(dµ(n) 2 ,d(σ(n))2). Thus the Bayes error rate can be written as 1 2(P[ d∑ j=1 (h(n) v )j >D|v∈C1] + P[ d∑ j=1 (h(n) v )j ≤D|v∈C2]) = 1 2 ( 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) )) + 1 2 ( Φ ( d 2 (µ1 + µ2) −dµ(n) 2√ dσ(n) )) = 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) ) . The last equality follows from the fact that d 2 (µ1 + µ2) −dµ(n) 1 = −(d 2 (µ1 + µ2) −dµ(n) 2 ). H How to use the z-score to choose the number of layers The bounds of the z-score with respect to the number of layers, z(n) lower and z(n) upper allow us to calculate bounds for n⋆ and n0 under different scenarios. Speciﬁcally, 1. ∀n∈N,z(n) upper <z(0) = (µ2 −µ1)/σ, then n⋆ = n0 = 0, meaning that no graph convolution should be applied. 2. |{n∈N : z(n) upper ≥z(0)}|>0, and (a) ∀n∈N,z(n) lower < z(0), then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},which means that the number of graph convolutions should not exceed the upper bound of n0, or otherwise one gets worse performance than having no graph convolution. Note that in this case, since n⋆ ≤n0, we can only conclude that 0 ≤n⋆ ≤min{n∈N : z(n) upper ≤z(0)}. (b) |{n∈N : z(n) lower ≥z(0)}|>0, then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},and let arg max n z(n) lower = n⋆ ﬂoor, max { n≤n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } ≤n⋆ ≤min { n≥n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } , meaning that the number of layers one should apply for optimal node classiﬁcation performance is more than the lower bound of n⋆, and less than the upper bound of n⋆. I Proofs of Proposition 2-5 I.1 Proof of Proposition 2 Since the spectral radius of D−1Ais 1, α(Id−(1 −α)(D−1A))−1 = α ∞∑ k=0 (1 −α)k(D−1A)k. Apply Lemma 2, we get that µPPNP 2 −µPPNP 1 ≈ p+q p+ 2−α α q(µ2 −µ1). To bound the approximation error, similar to the proof of the concentration bound in Theorem 1, it sufﬁces to bound µ1 −µ2 N w⊤ 2 ( ∞∑ k=0 α(1 −α)k((D−1A)k −( ¯D−1 ¯A)k))w2 = µ1 −µ2 N w⊤ 2 (TK + TK+1,∞)w2 , where TK = ∑K k=0 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), TK+1,∞= ∑∞ k=K+1 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), and K ∈N up to our own choice. Bounding TK: Apply Theorem 1, ﬁx r> 0, there exists a constantC(r,K,α) such that with probability1−O(N−r), ∥TK∥2 ≤ C√¯d . 18Bounding TK+1,∞: We will show upper bound for(D−1A)k −( ¯D−1 ¯A)k that applies for all k∈N. Note that for every k∈N, (D−1A)k = D−1/2(D−1/2AD−1/2)kD1/2 = D−1/2(VΛkV⊤)D1/2 , where D−1/2AD−1/2 = VΛV⊤is the eigenvalue decomposition. Then ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤∥(D−1A)k∥2 + ∥( ¯D−1 ¯A)k∥2 = ∥(D−1A)k∥2 + 1 ≤∥D−1/2∥2∥(D−1/2AD−1/2)k∥2∥D−1/2∥2 + 1. Since ∥(D−1/2AD−1/2)k∥2 = 1 and by Corollary 1, with probability at least 1 −N−r, ∥D1/2∥2 ≤ √ 3 ¯d/2,∥D−1/2∥2 ≤ √ 2/¯d, the previous inequality becomes ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤ √ 3 + 1. Hence ∥TK+1,∞∥2 ≤(1 −α)K+1 . Combining the two results, we prove the claim. I.2 Proof of Proposition 3 The claim is a direct corollary of Theorem 1. I.3 Proof of Proposition 4 The covariance matrix ΣPPNP of hPPNP could be written as ΣPPNP = α2( ∞∑ k=0 (1 −α)k(D−1A)k)( ∞∑ l=0 (1 −α)l(D−1A)l)⊤σ2 . Note that the variance of node iequals α2 ∑∞ k,l=0(1 −α)k+l(D−1A)k i·((D−1A)l)⊤ i·, where i·refers row iof a matrix. Then by Cauchy-Schwarz Theorem, (D−1A)k i·((D−1A)l)⊤ i· ≤∥(D−1A)k i·∥∥((D−1A)l)i·∥ ≤ √ (σ(k))2(σ(l))2/σ2,for all 1 ≤k,l ≤N. Moreover, by Lemma 3, (σ(k))2 ≤σ2. Due to the identity of each node i, we get that with probability 1 −O(1/N), for all 1 ≤K ≤N, (σPPNP)2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + ∞∑ k=K+1 (1 −α)kσ )2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 . For the lower bound, note that with probability 1 −O(1/N), (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2k 1 Nk + ∞∑ k=N+1 (1 −α)2k 1 N ) σ2 , where Nk is the size of k-hop neighborhood. Then (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2kmin{a,2} 10 1 (Np)k ) σ2 ≥α2 min{a,2} 10 (Np)N+1 −(1 −α)2N+2 (Np)N(Np −(1 −α)2) σ2 ≥α2 min{a,2} 10 σ2 . It is easy to see that Lemma 3 applies to any message-passing scheme which could be regarded as a random walk on the graph. Combining with Lemma 3, we get the ﬁnal result. 19I.4 Proof of Proposition 5 Since hAPPNP(n) = ( α (n−1∑ k=0 (1 −α)k(D−1A)k ) + (1 −α)n(D−1A)n ) X Through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≤ ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 . For the lower bound, through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≥α2 n−1∑ k=0 (1 −α)2k(σ(k))2 + (1 −α)2n(σ(n))2 ≥α2 min{a,2} 10 (n−1∑ k=0 (1 −α)2k 1 (Np)k ) σ2 + min{a,2} 10 (1 −α)2n 1 (Np)nσ2 ≥min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) σ2 . Combining with Lemma 3, we get the ﬁnal result. J Experiments Here we provide more details on the models that we use in Section 5. In all cases we use the Adam optimizer and tune some hyperparameters for better performance. The hyperparameters used are summarized as follows. Data ﬁnal linear classiﬁer weights in GNN layer learning rate (width) iterations (width) synthetic 1 layer no 0.01 8000 yes 0.01(1,4,16)/0.001(64,256) 8000(1,4,16)/10000(64)/50000(256) Cora 3 layer with 32 hidden channels no 0.001 150 yes 0.001 200 CiteSeer 3 layer with 16 hidden channels no 0.001 100 yes 0.001 100 PubMed 3 layer with 32 hidden channels no 0.001 500 yes 0.001 500 We empirically ﬁnd that after adding in weights in each GNN layer, it takes much longer to train the model for one iteration, and the time increases when the depth or the width increases (Figure 8). Since for some combinations, it takes more than 200,000 iterations for the validation accuracy to ﬁnally increase, for each case, we only train for a reasonable amount of iterations. Figure 8: Iterations per second for each model. All models were implemented with PyTorch [46] and PyTorch Geometric [47]. 20K Additional Results K.1 Effect of nonlinearity on classiﬁcation performance In section 3, we consider the case of a simpliﬁed linear GNN. What would happen if we add nonlinearity after linear graph convolutions? Here, we consider the case of a GNN with a ReLU activation function added after nlinear graph convolutions, i.e. h(n)ReLU = ReLU((D−1A)nX). We show that adding such nonlinearity does not improve the classiﬁcation performance. Proposition 6. Applying a ReLU activation function after n linear graph convolutions does not decrease the Bayes error rate, i.e. Bayes error rate based on h(n)ReLU ≥ Bayes error rate based on h(n), and equality holds if µ1 ≥−µ2. Proof. If is known that if xfollows a Gaussian distribution, then ReLU(x) follows a Rectiﬁed Gaussian distribution. Following the deﬁnition of the Bayes optimal classiﬁer, we present a geometric proof in Figure 9 (see next page, top), where the dark blue bar denotes the location of 0 and the red bar denotes the decision boundary Dof the Bayes optimal classiﬁer, and the light blue area denotes the overlapping area S, which is twice the Bayes error rate. Figure 9: A geometric proof of Proposition 6. K.2 Exact limit of variance (σ(n))2 as n→∞ Proposition 7. Given a graph Gwith adjacency matrix A, let its degree vector be d= A1 N, where 1 N is the all-one vector of length N. If Gis connected and non-bipartite, the variance of each node i, denoted as (σi(n))2, converges asymptotically to ∥d∥2 2 ∥d∥2 1 , i.e. (σi (n))2 n→∞ −−−−→∥d∥2 2 ∥d∥2 1 . Then ∥d∥2 ∥d∥2 1 ≥ 1 N, and the equality holds if and only if Gis regular. Proof. Let ei denotes the standard basis unit vector with the ith entry equals 1, and all other entries equal 0. Since Gis connected and non-bipartite, the random walk represented by P = D−1Ais ergodic, meaning that e⊤ i P(n) n→∞ −−−−→π, where πis the stationary distribution of this random walk with πi = di ∥d∥1 . Then since norms are continuous functions, we conclude that (σi (n))2 = ∑ j (p(n) ij )2 = ∥e⊤ i P(n)∥2 2 n→∞ −−−−→∥π∥2 2 = ∥d∥2 2 ∥d∥2 1 . By Lemma 3, it follows that ∥d∥2 2 ∥d∥2 1 ≥ 1 N. The unique minimizer of ∥π∥2 2 subject to ∥π∥1 = 1 is π= 1 N1 N. This means that Gmust be regular to achieve the lower bound asymptotically. 21Under Assumption 1, the graph generated by our CSBM is almost surely connected. Here, we remain to show that with high probability, the graph will also be non-bipartite. Proposition 8. With probability at least 1 −O(1/(Np)3), a graph Ggenerated from CSBM(N, p, q, µ1, µ2, σ2) contains a triangle, which implies that it is non-bipartite. Proof. The proof goes by the classic probabilistic method. Let T∆ = ( N 3 )∑ i 1 τi denotes the number of triangles in G, where 1 τi equals 1 if potential triangle τi exists and 0 otherwise. Then by second moment method, P[T∆ = 0] ≤ Var(T∆) (E[T∆])2 = 1 E[T∆]) + ∑ i̸=jE[1 τi1 τj] −(E[T∆])2 (E[T∆])2 . Since E[T∆] = O(Np), ∑ i̸=jE[1 τi1 τj] = (1 + O(1/N))(E[T∆])2, we get that P[T∆ = 0] ≤O(1/(Np)3) + O(1/N) ≤O(1/(Np)3) . Hence P[Gis non-bipartite] ≥P[T∆ ≥1] ≥1 −O(1/(Np)3). K.3 Symmetric Graph Convolution D−1/2AD−1/2 Proposition 9. When using symmetric message-passing convolution D−1/2AD−1/2 instead, the variance (σ(n))2 is non-increasing with respect to the number of convolutional layers n. i.e. (σ(n+1))2 ≤(σ(n))2,n ∈N ∪{0}. Proof. We want to calculate the diagonal entries of the covariance matrix Σ(n) of (D−1/2AD−1/2)nX, where the covariance matrix of X is σ2IN. Hence Σ(n) = (D−1/2AD−1/2)n( (D−1/2AD−1/2)n)⊤ . Since D−1/2AD−1/2 is symmetric, let its eigendecomposition be VΛV⊤and we could rewrite Σ(n) = (VΛnV⊤)(VΛnV⊤) = VΛ2nV⊤. Notice that the closed form of the diagonal entries is diag(Σ(n)) = N∑ i=1 λ2n i |v|2 . Since for all 1 ≤i≤N, |λi|≤ 1,we obtain monotonicity of each entry of diag(Σ(n)), i.e. variance of each node. Although the proposition does not always hold for random walk message-passing convolution D−1A as one can construct speciﬁc counterexamples (Appendix K.4), in practice, variances are observed to be decreasing with respect with the number of layers. Moreover, we empirically observe that variance goes down more than the variance using symmetric message-passing convolutions. Figure 10 presents visualization of node representations comparing the change of variance with respect to the number of layers using random walk convolution and symmetric message-passing convolution. The data is generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 10: The change of variance with respect to the number of layers using random walk convolution D−1Aand symmetric message-passing convolution D−1/2AD−1/2. 22K.4 Counterexamples Here, we construct a speciﬁc example where the variance (σ(n))2 is not non-increasing with respect to the number of layers n(Figure 11A). We remark that such a non-monotone nature of change in variance is not caused by the bipartiteness of the graph, as a cycle graph with even number of nodes is also bipartite, but does not exhibit such phenomenon (Figure 11B). We conjecture the increase in variance is rather caused by the tree-like structure. A B Figure 11: Counterexamples. K.5 The mixing and denoising effects in practice In this section, we measure the mixing and denoising effects of graph convolutions identiﬁed by our theoretical results in practice, and show that the same tradeoff between the two counteracting effects exists for real-world graphs. For the mixing effect, we measure the pairwise L2 distances between the means of different classes, and for the denoising effect, we measure the within-class variances, both respect to the number of layers. Figure 12 gives a visualization of both metrics for all classes on Cora, CiteSeer and PubMed. We observe that similar to the synthetic CSBM data, adding graph convolutions increases both the mixing effect (homogenizing node representations in different classes, measured by the inter-class distances) and the denoising effect (homogenizing node representations in the same class, measured by the within-class distances). In addition, the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory. Figure 12: The existence of the mixing (top row) and denoising effects (bottom row) of graph convolutions in practice. Adding graph convolutions increases both effects and the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory in Section 3. 23",
      "meta_data": {
        "arxiv_id": "2212.10701v2",
        "authors": [
          "Xinyi Wu",
          "Zhengdao Chen",
          "William Wang",
          "Ali Jadbabaie"
        ],
        "published_date": "2022-12-21T00:33:59Z",
        "pdf_url": "https://arxiv.org/pdf/2212.10701v2.pdf"
      }
    },
    {
      "title": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization",
      "abstract": "Graph neural networks (GNNs), which learn the representation of a node by\naggregating its neighbors, have become an effective computational tool in\ndownstream applications. Over-smoothing is one of the key issues which limit\nthe performance of GNNs as the number of layers increases. It is because the\nstacked aggregators would make node representations converge to\nindistinguishable vectors. Several attempts have been made to tackle the issue\nby bringing linked node pairs close and unlinked pairs distinct. However, they\noften ignore the intrinsic community structures and would result in sub-optimal\nperformance. The representations of nodes within the same community/class need\nbe similar to facilitate the classification, while different classes are\nexpected to be separated in embedding space. To bridge the gap, we introduce\ntwo over-smoothing metrics and a novel technique, i.e., differentiable group\nnormalization (DGN). It normalizes nodes within the same group independently to\nincrease their smoothness, and separates node distributions among different\ngroups to significantly alleviate the over-smoothing issue. Experiments on\nreal-world datasets demonstrate that DGN makes GNN models more robust to\nover-smoothing and achieves better performance with deeper GNNs.",
      "full_text": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization Kaixiong Zhou Texas A&M University zkxiong@tamu.edu Xiao Huang The Hong Kong Polytechnic University xhuang.polyu@gmail.com Yuening Li Texas A&M University liyuening@tamu.edu Daochen Zha Texas A&M University daochen.zha@tamu.edu Rui Chen Samsung Research America rui.chen1@samsung.com Xia Hu Texas A&M University xiahu@tamu.edu Abstract Graph neural networks (GNNs), which learn the representation of a node by aggre- gating its neighbors, have become an effective computational tool in downstream applications. Over-smoothing is one of the key issues which limit the performance of GNNs as the number of layers increases. It is because the stacked aggregators would make node representations converge to indistinguishable vectors. Several attempts have been made to tackle the issue by bringing linked node pairs close and unlinked pairs distinct. However, they often ignore the intrinsic community structures and would result in sub-optimal performance. The representations of nodes within the same community/class need be similar to facilitate the classiﬁca- tion, while different classes are expected to be separated in embedding space. To bridge the gap, we introduce two over-smoothing metrics and a novel technique, i.e., differentiable group normalization (DGN). It normalizes nodes within the same group independently to increase their smoothness, and separates node distributions among different groups to signiﬁcantly alleviate the over-smoothing issue. Exper- iments on real-world datasets demonstrate that DGN makes GNN models more robust to over-smoothing and achieves better performance with deeper GNNs. 1 Introduction Graph neural networks (GNNs) [1, 2, 3] have emerged as a promising tool for analyzing networked data, such as biochemical networks [4, 5], social networks [6, 7], and academic networks [8, 9]. The successful outcomes have led to the development of many advanced GNNs, including graph convolu- tional networks [10], graph attention networks [11], and simple graph convolution networks [12]. Besides the exploration of graph neural network variants in different applications, understanding the mechanism and limitation of GNNs is also a crucial task. The core component of GNNs, i.e., a neighborhood aggregator updating the representation of a node iteratively via mixing itself with its neighbors’ representations [6, 13], is essentially a low-pass smoothing operation [14]. It is in line with graph structures since the linked nodes tend to be similar [15]. It has been reported that, as the number of graph convolutional layers increases, all node representations over a graph will converge to indistinguishable vectors, and GNNs perform poorly in downstream applications [16, 17, 18]. It is recognized as an over-smoothing issue. Such an issue prevents GNN models from going deeper to exploit the multi-hop neighborhood structures and learn better node representations. A lot of efforts have been devoted to alleviating the over-smoothing issue, such as regularizing the node distance [ 19], node/edge dropping [ 20, 21], batch and pair normalizations [ 22, 23, 24]. Preprint. Under review. arXiv:2006.06972v1  [cs.LG]  12 Jun 2020Most of existing studies focused on measuring the over-smoothing based on node pair distances. By using these measurements, representations of linked nodes are forced to be close to each other, while unlinked pairs are separated. Unfortunately, the global graph structures and group/community characteristics are ignored, which leads to sub-optimal performance. For example, to perform node classiﬁcation, an ideal solution is to assign similar vectors to nodes in the same class, instead of only the connected nodes. In the citation network Pubmed [25], 36% of unconnected node pairs belong to the same class. These node pairs should instead have a small distance to facilitate node classiﬁcation. Thus, we are motivated to tackle the over-smoothing issue in GNNs from a group perspective. Given the complicated group structures and characteristics, it remains a challenging task to tackle the over-smoothing issue in GNNs. First, the formation of over-smoothing is complex and related to both local node relations and global graph structures, which makes it hard to measure and quantify. Second, the group information is often not directly available in real-world networks. This prevents existing tools such as group normalization being directly applied to solve our problem [ 26]. For example, while the group of adjacent channels with similar features could be directly accessed in convolutional neural networks [27], it is nontrivial to cluster a network in a suitable way. The node clustering needs to be in line with the embeddings and labels, during the dynamic learning process. To bridge the gap, in this paper, we perform a quantitative study on the over-smoothing in GNNs from a group perspective. We aim to answer two research questions. First, how can we precisely measure the over-smoothing in GNNs? Second, how can we handle over-smoothing in GNNs? Through exploring these questions, we make three signiﬁcant contributions as follows. • Present two metrics to quantify the over-smoothing in GNNs: (1) Group distance ratio, clustering the network and measuring the ratio of inter-group representation distance over intra-group one; (2) Instance information gain, treating node instance independently and measuring the input information loss during the low-pass smoothing. • Propose differentiable group normalization to signiﬁcantly alleviate over-smoothing. It softly clusters nodes and normalizes each group independently, which prevents distinct groups from having close node representations to improve the over-smoothing metrics. • Empirically show that deeper GNNs, when equipped with the proposed differentiable group normalization technique, yield better node classiﬁcation accuracy. 2 Quantitative Analysis of Over-smoothing Issue In this work, we use the semi-supervised node classiﬁcation task as an example and illustrate how to handle the over-smoothing issue. A graph is represented byG= {V,E}, where Vand Erepresent the sets of nodes and edges, respectively. Each node v∈V is associated with a feature vector xv ∈Rd and a class label yv. Given a training set Vl accompanied with labels, the goal is to classify the nodes in the unlabeled set Vu = V\\V l via learning the mapping function based on GNNs. 2.1 Preliminaries Following the message passing strategy [ 28], GNNs update the representation of each node via aggregating itself and its neighbors’ representations. Mathematically, at thek-th layer, we have, N(k) v = AGG({a(k) vv′W(k)h(k−1) v′ : v′∈N(v)}), h (k) v = COM(a(k) vv W(k)h(k−1) v ,N(k) v ). (1) N(k) v and h(k) v denote the aggregated neighbor embedding and embedding of nodev, respectively. We initialize h(0) v = xv. N(v) = {v′|ev,v′ ∈E} represents the set of neighbors for node v, where ev,v′ denotes the edge that connects nodes vand v′. W(k) denotes the trainable matrix used to transform the embedding dimension. a(k) vv′ is the link weight over edge ev,v′, which could be determined based on the graph topology or learned by an attention layer. Symbol AGG denotes the neighborhood aggregator usually implemented by a summation pooling. To update nodev, function COM is applied to combine neighbor information and node embedding from the previous layer. It is observed that the weighted average in Eq. (1) smooths node embedding with its neighbors to make them similar. For a full GNN model with Klayers, the ﬁnal node representation is given by hv = h(K) v , which captures the neighborhood structure information within Khops. 22.2 Measuring Over-smoothing with Group Structures In GNNs, the neighborhood aggregation strategy smooths nodes’ representations over a graph [14]. It will make the representations of nodes converge to similar vectors as the number of layersKincreases. This is called the over-smoothing issue, and would cause the performance of GNNs deteriorates as K increases. To address the issue, the ﬁrst step is to measure and quantify the over-smoothing [19, 21]. Measurements in existing work are mainly based on the distances between node pairs [20, 24]. A small distance means that a pair of nodes generally have undistinguished representation vectors, which might triggers the over-smoothing issue. However, the over-smoothing is also highly related to global graph structures, which have not been taken into consideration. For some unlinked node pairs, we would need their representations to be close if they locate in the same class/community, to facilitate the node classiﬁcation task. Without the speciﬁc group information, the metrics based on pair distances may fail to indicate the over- smoothing. Thus, we propose two novel over-smoothing metrics, i.e., group distance ratio and instance information gain. They quantify the over-smoothing from global (communities/classes/groups) and local (node individuals) views, respectively. Deﬁnition 1 (Group Distance Ratio). Suppose that there areCclasses of node labels. We intuitively cluster nodes of the same class label into a group to formulate the labeled node community. Formally, let Li = {hiv}denote the group of representation vectors, where node vis associated with label i. We have a series of labeled groups{L1,··· ,LC}. Group distance ratio RGroup measures the ratio of inter-group distance over intra-group distance in the Euclidean space. We have: RGroup = 1 (C−1)2 ∑ i̸=j( 1 |Li||Lj| ∑ hiv∈Li ∑ hjv′∈Lj ||hiv −hjv′||2) 1 C ∑ i( 1 |Li|2 ∑ hiv,hiv′∈Li ||hiv −hiv′||2) , (2) where ||·|| 2 denotes the L2 norm of a vector and |·| denotes the set cardinality. The numerator (denominator) represents the average of pairwise representation distances between two different groups (within a group). One would prefer to reduce the intra-group distance to make representations of the same class similar, and increase the inter-group distance to relieve the over-smoothing issue. On the contrary, a small RGroup leads to the over-smoothing issue where all groups are mixed together, and the intra-group distance is maintained to hinder node classiﬁcation. Deﬁnition 2 (Instance Information Gain). In an attributed network, a node’s feature decides its class label to some extent. We treat each node instance independently, and deﬁne instance information gain GIns as how much input feature information is contained in the ﬁnal representation. Let Xand Hdenote the random variables of input feature and representation vector, respectively. We deﬁne their probability distributions with PX and PH, and use PXH to denote their joint distribution. GIns measures the dependency between node feature and representation via their mutual information: GIns = I(X; H) = ∑ xv∈X,hv∈H PXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv). (3) We list the details of variable deﬁnitions and mutual information calculation in the context of GNNs in Appendix. With the intensiﬁcation of the over-smoothing issue, nodes average the neighborhood information and lose their self features, which leads to a small value of GIns. 2.3 Illustration of Proposed Over-smoothing Metrics Based on the two proposed metrics, we take simple graph convolution networks (SGC) as an example, and analyze the over-smoothing issue on Cora dataset [ 25]. SGC simpliﬁes the model through removing all the trainable weights between layers to avoid the potential of overﬁtting [12]. So the over-smoothing issue would be the major cause of performance dropping in SGC. As shown by the red lines in Figure 1, the graph convolutions ﬁrst exploit neighborhood information to improve test accuracy up to K = 5, after which the over-smoothing issue starts to worsen the performance. At the same time, instance information gain GIns and group distance ratio RGroup decrease due to the over-smoothing issue. For the extreme case of K = 120, the input features are ﬁltered out and all groups of nodes converge to the same representation vector, leading to GIns = 0 and RGroup = 1, respectively. Our metrics quantify the smoothness of node representations based on group structures, but also have the similar variation tendency with test accuracy to indicate it well. 30 25 50 75 100 125 Layers 0.4 0.6 0.8Accuracy 0 25 50 75 100 125 Layers 0.06 0.08 0.10 0.12Instance gain 0 25 50 75 100 125 Layers 1.5 2.0 2.5 3.0Group distance None batch pair group Figure 1: The test accuracy, instance information gain, and group distance ratio of SGC on Cora. We compare differentiable group normalization with none, batch and pair normalizations. 3 Differentiable Group Normalization We start with a graph-regularized optimization problem [10, 19]. To optimize the over-smoothing metrics of GIns and RGroup, one traditional approach is to minimize the loss function: L= L0 −GIns −λRGroup. (4) L0 denotes the supervised cross-entropy loss w.r.t. representation probability vectors hv ∈RC×1 and class labels. λis a balancing factor. The goal of optimization problem Eq. (4) is to learn node representations close to the input features and informative for their class labels. Considering the labeled graph communities, it also improves the intra-group similarity and inter-group distance. However, it is non-trivial to optimize this objective function due to the non-derivative of non- parametric statistic GIns [29, 30] and the expensive computation of RGroup. 3.1 Proposed Technique for Addressing Over-smoothing Instead of directly optimizing regularized problem in Eq. (4), we propose the differentiable group normalization (DGN) applied between graph convolutional layers to normalize the node embeddings group by group. The key intuition is to cluster nodes into multiple groups and then normalize them independently. Consider the labeled node groups (or communities) in networked data. The node embeddings within each group are expected to be rescaled with a speciﬁc mean and variance to make them similar. Meanwhile, the embedding distributions from different groups are separated by adjusting their means and variances. We develop an analogue with the group normalization in convolutional neural networks (CNNs) [26], which clusters a set of adjacent channels with similar characteristics into a group and treats it independently. Compared with standard CNNs, the challenge in designing DGN is how to cluster nodes in a suitable way. The clustering needs to be in line with the embedding and labels, during the dynamic learning process. We address this challenge by learning a cluster assignment matrix, which softly maps nodes with close embeddings into a group. Under the supervision of training labels, the nodes close in the embedding space tend to share a common label. To be speciﬁc, we ﬁrst describe how DGN clusters and normalizes nodes in a group-wise fashion given an assignment matrix. After that, we discuss how to learn the assignment matrix to support differentiable node clustering. Group Normalization. Let H(k) = [ h(k) 1 ,··· ,h(k) n ]T ∈Rn×d(k) denote the embedding matrix generated from the k-th graph convolutional layer. TakingH(k) as input, DGN softly assigns nodes into groups and normalizes them independently to output a new embedding matrix for the next layer. Formally, we deﬁne the number of groups as G, and denote the cluster assignment matrix by S(k) ∈Rn×G. Gis a hyperparameter that could be tuned per dataset. The i-th column of S(k), i.e., S(k)[:,i], indicates the assignment probabilities of nodes in a graph to the i-th group. Supposing that S(k) has already been computed, we cluster and normalize nodes in each group as follows: H(k) i = S(k)[:,i] ◦H(k) ∈Rn×d(k) ; ˜H(k) i = γi(H(k) i −µi σi ) + βi ∈Rn×d(k) . (5) Symbol ◦denotes the row-wise multiplication. The left part in the above equation represents the soft node clustering for group i, whose embedding matrix is given by H(k) i . The right part performs the standard normalization operation. In particular, µi and σi denote the vectors of running mean 4and standard deviation of group i, respectively, and γi and βi denote the trainable scale and shift vectors, respectively. Given the input embedding H(k) and the series of normalized embeddings {˜H(k) 1 ,··· , ˜H(k) G }, DGN generates the ﬁnal embedding matrix ˜H(k) for the next layer as follows: ˜H(k) = H(k) + λ G∑ i=1 ˜H(k) i ∈Rn×d(k) . (6) λis a balancing factor as mentioned before. Inspecting the loss function in Eq. (4), DGN utilizes components H(k) and ∑G i=1 ˜H(k) i to improve terms GIns and RGroup, respectively. In particular, we preserve the input embedding H(k) to avoid over-normalization and keep the input feature of each node to some extent. Note that the linear combination of H(k) in DGN is different from the skip connection in GNN models [31, 32], which instead connects the embedding output H(k−1) from the last layer. The technique of skip connection could be included to further boost the model performance. Group normalization ∑G i=1 ˜H(k) i rescales the node embeddings within each group independently to make them similar. Ideally, we assign the close node embeddings with a common label to a group. Node embeddings of the group are then distributed closely around the corresponding running mean. Thus for different groups associate with distinct node labels, we disentangle their running means and separates the node embedding distributions. By applying DGN between the successive graph convolutional layers, we are able to optimize Problem (4) to mitigate the over-smoothing issue. Differentiable Clustering. We apply a linear model to compute the cluster assignment matrix S(k) used in Eq. (5). The mathematical expression is given by: S(k) = softmax(H(k)U(k)). (7) U(k) ∈Rd(k)×G denotes the trainable weights for a DGN module applied after the k-th graph convolutional layer. softmax function is applied in a row-wise way to produce the normalized probability vector w.r.t all the Ggroups for each node. Through the inner product between H(k) and U(k), the nodes with close embeddings are assigned to the same group with a high probability. Here we give a simple and effective way to compute S(k). Advanced neural networks could be applied. Time Complexity Analysis. Suppose that the time complexity of embedding normalization at each group is O(T), where T is a constant depending on embedding dimension d(k) and node number n. The time cost of group normalization ∑G i=1 ˜H(k) i is O(GT). Both the differentiable clustering (in Eq. (5)) and the linear model (in Eq. (7)) have a time cost of O(nd(k)G). Thus the total time complexity of a DGN layer is given by O(nd(k)G+ GT), which linearly increases with G. Comparison with Prior Work. To the best of our knowledge, the existing work mainly focuses on analyzing and improving the node pair distance to relieve the over-smoothing issue [19, 21, 24]. One of the general solutions is to train GNN models regularized by the pair distance [19]. Recently, there are two related studies applying batch normalization [22] or pair normalization [24] to keep the overall pair distance in a graph. Pair normalization is a “slim” realization of batch normalization by removing the trainable scale and shift. However, the metric of pair distance and the resulting techniques ignore global graph structure, and may achieve sub-optimal performance in practice. In this work, we measure over-smoothing of GNN models based on communities/groups and independent node instances. We then formulate the problem in Eq. (4) to optimize the proposed metrics, and propose DGN to solve it in an efﬁcient way, which in turn addresses the over-smoothing issue. 3.2 Evaluating Differentiable Group Normalization on Attributed Graphs We apply DGN to the SGC model to validate its effectiveness in relieving the over-smoothing issue. Furthermore, we compare with the other two available normalization techniques used upon GNNs, i.e., batch normalization and pair normalization. As shown in Figure 1, the test accuracy of DGN remains stable with the increase in the number of layers. By preserving the input embedding and normalizing node groups independently, DGN achieves superior performance in terms of instance information gain as well as group distance ratio. The promising results indicate that our DGN tackles the over-smoothing issue more effectively, compared with none, batch and pair normalizations. It should be noted that, the highest accuracy of 79.7% is achieved with DGN when K = 20. This observation contradicts with the common belief that GNN models work best with a few layers on 50 25 50 75 100 125 Layers 0.2 0.4 0.6 0.8Accuracy SGC 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GCN 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GAT None batch pair group Figure 2: The test accuracies of SGC, GCN, and GAT models on Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. current benchmark datasets [33]. With the integration of advanced techniques, such as DGN, we are able to exploit deeper GNN architectures to unleash the power of deep learning in network analysis. 3.3 Evaluation in Scenario with Missing Features To further illustrate that DGN could enable us to achieve better performance with deeper GNN architectures, we apply it to a more complex scenario. We assume that the attributes of nodes in the test set are missing. It is a common scenario in practice [24]. For example, in social networks, new users are often lack of proﬁles and tags [34]. To perform prediction tasks on new users, we would rely on the node attributes of existing users and their connections to new users. In such a scenario, we would like to apply more layers to exploit the neighborhood structure many hops away to improve node representation learning. Since the over-smoothing issue gets worse with the increasing of layer numbers, the beneﬁt of applying normalization will be more obvious in this scenario. We remove the input features of both validation and test sets in Cora, and replace them with zeros [24]. Figure 2 presents the results on three widely-used models, i.e., SGC, graph convolutional networks (GCN), and graph attention networks (GAT). Due to the over-smoothing issue, GNN models without any normalization fail to distinguish nodes quickly with the increasing number of layers. In contrast, the normalization techniques reach their highest performance at larger layer numbers, after which they drop slowly. We observe that DGN obtains the best performance with50, 20, and 8 layers for SGC, GCN, and GAT, respectively. These layer numbers are signiﬁcantly larger than those of the widely-used shallow models (e.g., two or three layers). 4 Experiments We now empirically evaluate the effectiveness and robustness of DGN on real-world datasets. We aim to answer three questions as follows. Q1: Compared with the state-of-the-art normalization methods, can DGN alleviate the over-smoothing issue in GNNs in a better way? Q2: Can DGN help GNN models achieve better performance by enabling deeper GNNs? Q3: How do the hyperparameters inﬂuence the performance of DGN? 4.1 Experiment Setup Datasets. Joining the practice of previous work, we evaluate GNN models by performing the node classiﬁcation task on four datasets: Cora, Citeseer, Pubmed [ 25], and CoauthorCS [35]. We also create graphs by removing features in validation and test sets. The dataset statistics are in Appendix. Implementations. Following the previous settings, we choose the hyperparameters of GNN models and optimizer as follows. We set the number of hidden units to 16 for GCN and GAT models. The number of attention heads in GAT is 1. Since a larger parameter size in GCN and GAT may lead to overﬁtting and affects the study of over-smoothing issue, we compare normalization methods by varying the number of layersKin {1,2,··· ,10,15,··· ,30}. For SGC, we increase the testing range and vary K in {1,5,10,20,··· ,120}. We train with a maximum of 1000 epochs using the Adam optimizer [36] and early stopping. Weights in GNN models are initialized with Glorot algorithm [37]. We use the following sets of hyperparameters for Citeseer, Cora, CoauthorCS: 0.6 (dropout rate), 5 ·10−4 (L2 regularization), 5 ·10−3 (learning rate), and for Pubmed: 0.6 (dropout rate), 1 ·10−3 (L2 regularization), 1 ·10−2 (learning rate). We run each experiment 5 times and report the average. 6Table 1: Test accuracy in percentage on attributed networks. Layers a/bdenote the layer number ain GCN & GAT and that of bin SGC. #Kdenotes the optimal layer numbers where DGN achieves the highest performance. Dataset Model Layers 2/5 Layers 15/60 Layers 30/120 #KNN BN PN DGN NN BN PN DGN NN BN PN DGN Cora GCN 82.2 73.9 71 .0 82 .0 18.1 70 .3 67 .2 75.2 13.1 67 .2 64 .3 73.2 2 GAT 80.9 77 .8 74 .4 81.1 16.8 33 .1 49 .6 71.8 13.0 25 .0 30 .2 51.3 2 SGC 75.8 76 .3 75 .4 77.9 29.4 72 .1 71 .7 77.8 25.1 51 .2 65 .5 73.7 20 Citeseer GCN 70.6 51.3 60 .5 69 .5 15.2 46 .9 46 .7 53.1 9.4 47 .9 47 .1 52.6 2 GAT 70.2 61.5 62 .0 69 .3 22.6 28 .0 41 .4 52.6 7.7 21 .4 33 .3 45.6 2 SGC 69.6 58.8 64 .8 69 .5 66.3 50.5 65 .0 63 .4 60.8 47 .3 63 .1 64.7 30 Pubmed GCN 79.3 74 .9 71 .1 79.5 22.5 73 .7 70 .6 76.1 18.0 70 .4 70 .4 76.9 2 GAT 77.8 76.2 72 .4 77 .5 37.5 56 .2 68 .8 75.9 18.0 46 .6 58 .2 73.3 5 SGC 71.5 76 .5 75 .8 76.8 34.2 75 .2 77 .1 77.4 23.1 71 .6 76 .7 77.1 10 Coauthors GCN 92.3 86 .0 77 .8 92.3 72.2 78 .5 69 .5 83.7 3.3 84.7 64.5 84 .4 1 GAT 91.5 89 .4 85 .9 91.8 6.0 77 .7 53 .1 84.5 3.3 16 .7 48 .1 75.5 1 SGC 89.9 88 .7 86 .0 90.2 10.2 59 .7 76 .4 81.3 5.8 30 .5 52 .6 60.8 1 Baselines. We compare with none normalization (NN), batch normalization (BN) [22, 23] and pair normalization (PN) [24]. Their technical details are listed in Appendix. DGN Conﬁgurations. The key hyperparameters include group number G and balancing factor λ. Depending on the number of class labels, we apply 5 groups to Pubmed and 10 groups to the others. The criterion is to use more groups to separate representation distributions in networked data accompanied with more class labels. λis tuned on validation sets to ﬁnd a good trade-off between preserving input features and group normalization. We introduce the selection of λin Appendix. 4.2 Experiment Results Studies on alleviating the over-smoothing problem.To answerQ1, Table 1 summarizes the results of applying different normalization techniques to GNN models on all datasets. We report the performance of GCN and GAT with 2/15/30 layers, and SGC with 5/60/120 layers due to space limit. We provide test accuracies, instance information gain and group distance ratio under all depths in Appendix. It can be observed that DGN has signiﬁcantly alleviated the over-smoothing issue. Given the same layers, DGN almost outperforms all other normalization methods for all cases and greatly slows down the performance dropping. It is because the self-preserved component H(k) in Eq. (6) keeps the informative input features and avoids over-normalization to distinguish different nodes. This component is especially crucial for models with a few layers since the over-smoothing issue has not appeared. The other group normalization component in Eq. (6) processes each group of nodes independently. It disentangles the representation similarity between groups, and hence reduces the over-smoothness of nodes over a graph accompanied with graph convolutions. Studies on enabling deeper and better GNNs. To answer Q2, we compare all of the concerned normalization methods over GCN, GAT, and SGC in the scenario with missing features. As we have discussed, normalization techniques will show their power in relieving the over-smoothing issue and exploring deeper architectures especially for this scenario. In Table 2, Acc represents the best test accuracy yielded by model equipped with the optimal layer number #K. We can observe that DGN signiﬁcantly outperforms the other normalization methods on all cases. The average improvements over NN, BN and PN achieved by DGN are 37.8%, 7.1% and 12.8%, respectively. Compared with vanilla GNN models without any normalization layer, the optimal models accompanied with normalization layers (especially for our DGN) usually possess larger values of #K. It demonstrates that DGN enables to explore deeper architectures to exploit neighborhood information with more hops away by tackling the over-smoothing issue. We present the comprehensive analyses in terms of test accuracy, instance information gain and group distance ratio under all depths in Appendix. Hyperparameter studies. We study the impact of hyperparameters, group number Gand balancing factor λ, on DGN in order to answer research question Q3. Over the GCN framework associated with 20 convolutional layers, we evaluate DGN by considering Gand λfrom sets [1,5,10,15,20,30] and [0.001,0.005,0.01,0.03,0.05,0.1], respectively. The left part in Figure 3 presents the test accuracy 7Table 2: The highest accuracy (%) and the accompanied optimal layers in the scenario with missing features. We calculate the average improvement achieved by DGN over each GNN framework. Model Norm Cora Citeseer Pubmed CoauthorCS Improvement%Acc #K Acc #K Acc #K Acc #K GCN NN 57.3 3 44.0 6 36.4 4 67.3 3 42.2 BN 71.8 20 45.1 25 70.4 30 82.7 30 5.2 PN 65.6 20 43.6 25 63.1 30 63.5 4 19.2 DGN 76.3 20 50.2 30 72.0 30 83.7 25 - GAT NN 50.1 2 40.8 4 38.5 4 63.7 3 51.0 BN 72.7 5 48.7 5 60.7 4 80.5 6 9.8 PN 68.8 8 50.3 6 63.2 20 66.6 3 14.7 DGN 75.8 8 54.5 5 72.3 20 83.6 15 - SGC NN 63.4 5 51.2 40 63.7 5 71.0 5 20.1 BN 78.5 20 50.4 20 72.3 50 84.4 20 6.2 PN 73.4 50 58.0 120 75.2 30 80.1 10 4.5 DGN 80.2 50 58.2 90 76.2 90 85.8 20 - 0.2 0.1 0.3 0.4 0.5 0.08 0.6 30 0.7 0.06 25 0.8 20 0.04  15 100.02 5 0 0 Figure 3: Left: Test accuracies of GCN with 20 layers on Cora with missing features, where hyperparameters Gand λare studied. Middle: Node representation visualization for GCN without normalization and with K = 20. Right: Node representation visualization for GCN with DGN layer and K = 20 (node colors represent classes, and black triangles denote the running means of groups). for each hyperparameter combination. We observe that: (i) The model performance is damaged greatly when λis close to zero (e.g.,λ= 0.001). In this case, group normalization contributes slightly in DGN, resulting in over-smoothing in the GCN model. (ii) Model performance is not sensitive to the value of G, and an appropriate λvalue could be tuned to optimize the trade-off between instance gain and group normalization. It is because DGN learns to use the appropriate number of groups by end-to-end training. In particular, some groups might not be used as shown in the right part of Figure 3, at which only 6 out of 10 groups (denoted by black triangles) are adopted. (iii) Even when G= 1, DGN still outperforms BN by utilizing the self-preserved component to achieve an accuracy of 74.7%, where λ= 0.1. Via increasing the group number, the model performance could be further improved, e.g., the accuracy of 76.3% where G= 10 and λ= 0.01. Node representation visualization. We investigate how DGN clusters nodes into different groups to tackle the over-smoothing issue. The middle and right parts of Figure 3 visualize the node representations achieved by GCN models without normalization tool and with the DGN approach, respectively. It is observed that the node representations of different classes mix together when the layer number reaches 20 in the GCN model without normalization. In contrast, our DGN method softly assigns nodes into a series of groups, whose running means at the corresponding normalization modules are highlighted with black triangles. Through normalizing each group independently, the running means are separated to improve inter-group distances and disentangle node representations. In particular, we notice that the running means locate at the borders among different classes (e.g., the upper-right triangle at the border between red and pink classes). That is because the soft assignment may cluster nodes of two or three classes into the same group. Compared with batch or pair normalization, the independent normalization for each group only includes a few classes in DGN. In this way, we relieve the representation noise from other node classes during normalization, and improve the group distance ratio as illustrated in Appendix. 85 Conclusion In this paper, we propose two over-smoothing metrics based on graph structures, i.e., group distance ratio and instance information gain. By inspecting GNN models through the lens of these two metrics, we present a novel normalization layer, DGN, to boost model performance against over- smoothing. It normalizes each group of similar nodes independently to separate node representations of different classes. Experiments on real-world classiﬁcation tasks show that DGN greatly slowed down performance degradation by alleviating the over-smoothing issue. DGN enables us to explore deeper GNNs and achieve higher performance in analyzing attributed networks and the scenario with missing features. Our research will facilitate deep learning models for potential graph applications. Broader Impact The successful outcome of this work will lead to advances in building up deep graph neural networks and dealing with complex graph-structured data. The developed metrics and algorithms have an immediate and strong impact on a number of ﬁelds, including (1) Over-smoothing Quantitative Analysis: GNN models tend to result in the over-smoothing issue with the increase in the number of layers. During the practical development of deeper GNN models, the proposed instance information gain and group distance ratio effectively indicate the over-smoothing issue, in order to push the model exploration toward a good direction. (2) Deep GNN Modeling: The proposed differentiable group normalization tool successfully tackles the over-smoothing issue and enables the modeling of deeper GNN variants. It encourages us to fully unleash the power of deep learning in processing the networked data. (3) Real-world Network Analytics Applications: The proposed research will broadly shed light on utilizing deep GNN models in various applications, such as social network analysis, brain network analysis, and e-commerce network analysis. For such complex graph-structured data, deep GNN models can exploit the multi-hop neighborhood information to boost the task performance. References [1] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008. [2] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. [3] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A comprehensive survey on graph neural networks. arXiv, 2019. [4] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeuIPS, pages 2224–2232, 2015. [5] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [6] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeuIPS, pages 1024–1034, 2017. [7] Xiao Huang, Qingquan Song, Yuening Li, and Xia Hu. Graph recurrent networks with at- tributed random walks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 732–740, 2019. [8] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1416–1424, 2018. [9] Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019. [10] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. ICLR, 2017. [11] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv, 1(2), 2017. 9[12] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Simplifying graph convolutional networks. arXiv preprint arXiv:1902.07153, 2019. [13] Kaixiong Zhou, Qingquan Song, Xiao Huang, Daochen Zha, Na Zou, and Xia Hu. Multi-channel graph neural networks. arXiv preprint arXiv:1912.08306, 2019. [14] Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ﬁlters. arXiv preprint arXiv:1905.09550, 2019. [15] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415–444, 2001. [16] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018. [17] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In International Conference on Learning Representations, 2020. [18] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou. Specae: Spectral autoencoder for anomaly detection in attributed networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2233–2236, 2019. [19] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. arXiv preprint arXiv:1909.03211, 2019. [20] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Representations. https://openreview. net/forum, 2020. [21] Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming- Chang Yang. Measuring and improving the use of graph information in graph neural networks, 2020. [22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. [23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015. [24] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint arXiv:1909.12223, 2019. [25] Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861, 2016. [26] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018. [27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [28] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1263–1272. JMLR. org, 2017. [29] Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017. [30] Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. Entropy, 21(12):1181, 2019. [31] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE International Conference on Computer Vision, pages 9267–9276, 2019. [32] Nezihe Merve Gürel, Hansheng Ren, Yujing Wang, Hui Xue, Yaming Yang, and Ce Zhang. An anatomy of graph neural networks going deep via the lens of mutual information: Exponential decay vs. full preservation. arXiv preprint arXiv:1910.04499, 2019. 10[33] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018. [34] Al Mamunur Rashid, George Karypis, and John Riedl. Learning preferences of new users in recommender systems: an information theoretic approach. Acm Sigkdd Explorations Newsletter, 10(2):90–100, 2008. [35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. [36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, 2014. [37] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedfor- ward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256, 2010. 11A Dataset Statistics For fair comparison with previous work, we perform the node classiﬁcation task on four benchmark datasets, including Cora, Citeseer, Pubmed [ 25], and CoauthorCS [ 35]. They have been widely adopted to study the over-smoothing issue in GNNs [21, 19, 24, 16, 20]. The detailed statistics are listed in Table 3. To further illustrate that the normalization techniques could enable deeper GNNs to achieve better performance, we apply them to a more complex scenario with missing features. For these four benchmark datasets, we create the corresponding scenarios by removing node features in both validation and testing sets. Table 3: Dataset statistics on Cora, Citeseer, Pubmed, and CoauthorCS. Cora Citeseer Pubmed CoauthorCS #Nodes 2708 3327 19717 18333 #Edges 5429 4732 44338 81894 #Features 1433 3703 500 6805 #Classes 7 6 3 15 #Training Nodes 140 120 60 600 #Validation Nodes 500 500 500 2250 #Testing Nodes 1000 1000 1000 15483 B Running Environment All the GNN models and normalization approaches are implemented in PyTorch, and tested on a machine with 24 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GB processors, GeForce GTX-1080 Ti 12 GB GPU, and 128GB memory size. We implement the group normalization in a parallel way. Thus the practical time cost of our DGN is comparable to that of traditional batch normalization. C GNN Models We test over three general GNN models to illustrate the over-smoothing issue, including graph convo- lutional networks (GCN) [10], graph attention networks (GAT) [11] and simple graph convolution (SGC) networks [12]. We list their neighbor aggregation functions in Table 4. Table 4: Neighbor aggregation function at a graph convolutional layer for GCN, GAT and SGC. Model Neighbor aggregation function GCN h(k) v = ReLU(∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) W(k)h(k−1) v′ ) GAT h(k) v = ReLU(∑ v′∈N(v)∪{v}a(k) vv′W(k)h(k−1) v′ ) SGC h(k) v = ∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) h(k−1) v′ Considering the message passing strategy as shown by Eq. (1) in the main manuscript, we explain the key properties of GCN, GAT and SGC as follows. GCN merges the information from node itself and its neighbors weighted by vertices’ degrees, wherea(k) vv′ = 1./ √ (|N(v)|+ 1) ·(|N(v′)|+ 1). Functions AGG and COM are realized by a summation pooling. The activation function of ReLU is then applied to non-linearly transform the latent embedding. Based on GCN, GAT uses an additional attention layer to learn link weight a(k) vv′. GAT aggregates neighbors with the trainable link weights, and achieves signiﬁcant improvements in a variety of applications. SGC is simpliﬁed from GCN by removing all trainable parameters W(k) and nonlinear activations between successive layers. It has been empirically shown that these simpliﬁcations do not negatively impact classiﬁcation accuracy, and even relive the problems of over-ﬁtting and vanishing gradients in deeper models. 12D Normalization Baselines Batch normalization is ﬁrst applied between the successive convolutional layers in CNNs [23]. It is extended to graph neural networks to improve node representation learning and generalization [22]. Taking embedding matrix H(k) as input after each layer, batch normalization scales the node rep- resentations using running mean and variance, and generates a new embedding matrix for the next graph convolutional layer. Formally, we have: ˜H(k) = γ(H(k) −µ σ ) + β ∈Rn×d(k) . µand σdenote the vectors of running mean and standard deviation, respectively; γ and β denote the trainable scale and shift vectors, respectively. Recently, pair normalization has been proposed to tackle the over-smoothing issue in GNNs, targeting at maintaining the average node pair distance over a graph [24]. Pair normalization is a simplifying realization of batch normalization by removing the trainable γ and β. In this work, we augment each graph convolutional layer via appending a normalization module, in order to validate the effectiveness of normalization technique in relieving over-smoothing and enabling deeper GNNs. E Hyperparameter Tuning in DGN The balancing factor, λ, is crucial to determine the trade-off between input feature preservation and group normalization in DGN. It needs to be tuned carefully as GNN models increase the number of layers. To be speciﬁc, we consider the candidate set {5 ·10−4,1 ·10−3,2 ·10−3,3 ·10−3,5 · 10−3,1 ·10−2,2 ·10−2,3 ·10−2,5 ·10−2}. For each speciﬁc model, we use a few epochs to choose the optimal λon the validation set, and then evaluate it on the testing set. We observe that the value of λtends to be larger in the model accompanied with more graph convolutional layers. That is because the over-smoothing issue gets worse with the increase in layer number. The group normalization is much more required to separate the node representations of different classes. F Instance Information Gain In this work, we adopt kernel-density estimators (KDE), one of the common non-parametric ap- proaches, to estimate the mutual information between input feature and representation vector [29, 30]. A key assumption in KDE is that the input feature (or output representation vector) of neural networks is distributed as a mixture of Gaussians. Since a neural network is a deterministic function of the input feature after training, the mutual information would be inﬁnite without such assumption. In the following, we ﬁrst formally deﬁne the Gaussian assumption, input probability distribution and representation probability distribution, and then present how to obtain the instance information gain based on the mutual information metric. Gaussian assumption. In the graph signal processing, it is common to assume that the collected input feature contains both true signal and noise. In other word, we have the input feature as follows: xv = ¯xv + ϵx. ¯xv denotes the true value, and ϵx ∼N(0,σ2I) denotes the added Gaussian noise with variance σ2. Therefore, input feature xv is a Gaussian variable centered on its true value. Input probability distribution. We treat the empirical distribution of input samples as true distri- bution. Given a dataset accompanied with nsamples, we have a series of input features{x1,··· ,xn} for all the samples. Each node feature is sampled with probability 1/|V|following the empirical uniform distribution. Let |V|denotes the number of samples, and let Xdenote the random variable of input features. Based on the above Gaussian assumption, probability PX(xv) of input feature xv is obtained by the product of 1/|V|with Gaussian probability centered on true value ¯xv. Representation probability distribution. Let Hdenote the random variable of node represen- tations. To obtain probability PH(hv) of continuous vector hv, a general approach is to bin and transform Hinto a new discrete variable. However, with the increasing dimensions of hv, it is non-trivial to statistically count the frequencies of all possible discrete values. Considering the task of node classiﬁcation, the index of largest element along vector hv ∈RC×1 is regarded as the label 13of a node. We propose a new binning approach that labels the whole vector hv with the largest index zv. In this way, we only have Cclasses of discrete values to facilitate the frequency counting. To be speciﬁc, let Pc denote the number of representation vectors whose indexes zv = c. The probability of a discrete variable with class cis given by: pc = PH(zv = c) = Pc∑C l=1 Pl . Mutual information calculation. Based on KDE approach, a lower bound of mutual information between input feature and representation vector can be calculated as: GIns = I(X; H) = ∑ xv∈X,hv∈HPXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv) = H(X) −H(X|H) ≥− 1 |V| ∑ ilog 1 |V| ∑ jexp(−1 2 ||xi−xj||2 2 4σ2 ) −∑C c=1 pc[−1 Pc ∑ i,zi=clog 1 Pc ∑ j,zj=cexp(−1 2 ||xi−xj||2 2 4σ2 )]. The sum over i,zi = crepresents a summation over all the input features whose representation vectors are labeled with zi = c. PXH(xv,hv) denotes the joint probability of xv and hv. The effectiveness of GIns in measuring mutual information between input feature and node representation has been demonstrated in the experimental results. As illustrated in Figures 4-7, GIns decreases with the increasing number of graph convolutional layers. This practical observation is in line with the human expert knowledge about neighbor aggregation strategy in GNNs. The neighbor aggregation function as shown in Table 4 is in fact a low-passing smoothing operation, which mixes the input feature of a node with those of its neighbors gradually. At the extreme cases where K = 30 or 120, we ﬁnd that GIns approaches to zero in GNN models without normalization. The loss of informative input feature leads to the dropping of node classiﬁcation accuracy. However, our DGN keeps the input information during graph convolutions and normalization to some extent, resulting in the largest GIns compared with the other normalization approaches. G Performance Comparison on Attributed Graphs In this section, we report the model performances in terms of test accuracy, instance information gain and group distance ratio achieved on all the concerned datasets in Figures 4-7. We make the following observations: • Comparing with other normalization techniques, our DGN generally slows down the dropping of test accuracy with the increase in layer number. Even for GNN models associated with a small number of layers (i.e., G≤5), DGN achieves the competitive performance compared with none normalization. The adoption of DGN module does not damage the model performance, and prevents model from suffering over-smoothing issue when GNN goes deeper. • DGN achieves the larger or comparable instance information gains in all cases, especially for GAT models. That is because DGN keeps embedding matrix H(k) and prevents over-normalization within each group. The preservation of H(k) saves input features to some extent after each layer of graph convolutions and normalization. In an attributed graph, the improved preservation of informative input features in the ﬁnal representations will signiﬁcantly facilitate the downstream node classiﬁcation. Furthermore, such preservation is especially crucial for GNN models with a few layers, since the over-smoothing issue has not appeared. • DGN normalizes each group of node representations independently to generally improve the group distance ratio, especially for models GCN and GAT. A larger value of group distance ratio means that the node representation distributions from all groups are disentangled to address the over-smoothing issue. Although the ratios of DGN are smaller than those of pair normalization in some cases upon SGC framework, we still achieve the largest test accuracy. That may be because the intra-group distance in DGN is much smaller than that of pair normalization. A small value of intra-group distance would facilitate the node classiﬁcation within the same group. We will further compare the intra-group distance in scenarios with missing features in the following experiments. 140 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 0.10 Instance gain None batch pair group 0 10 20 30 1 2 3 4 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0.10 0 10 20 30 1 2 3 4 0 50 100 Layers 0.4 0.6 0.8SGC 0 50 100 Layers 0.050 0.075 0.100 0.125 0 50 100 Layers 2 3 Figure 4: The test accuracy, instance information gain, and group distance ratio in attributed Cora. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 0.000 0.025 0.050 0.075 Instance gain None batch pair group 0 10 20 30 1.0 1.5 2.0 Group ratio 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 0.000 0.025 0.050 0.075 0 10 20 30 1.0 1.5 2.0 0 50 100 Layers 0.5 0.6 0.7SGC 0 50 100 Layers 0.06 0.08 0 50 100 Layers 1.25 1.50 1.75 2.00 Figure 5: The test accuracy, instance information gain, and group distance ratio in attributed Citeseer. We compare differentiable group normalization with none, batch and pair normalizations. 150 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 Instance gain None batch pair group 0 10 20 30 1 2 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0 10 20 30 1.0 1.5 2.0 2.5 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 0.025 0.050 0.075 0.100 0 50 100 Layers 1.5 2.0 Figure 6: The test accuracy, instance information gain, and group distance ratio in attributed Pubmed. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 0.0 0.1 0.2 0.3 Instance gain None batch pair group 0 10 20 30 2 4 6 Group ratio 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 0.0 0.1 0.2 0.3 0 10 20 30 2 4 6 8 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 0.0 0.1 0.2 0.3 0 50 100 Layers 2 4 6 Figure 7: The test accuracy, instance information gain, and group distance ratio in attributed Coau- thorCS. We compare differentiable group normalization with none, batch and pair normalizations. 16H Performance Comparison in Scenarios with Missing Features In this section, we report the model performances in terms of test accuracy, group distance ratio and intra-group distance achieved in scenarios with missing features in Figures 8-11. The intra-group distance is calculated by node pair distance averaged within the same group. Its mathematical expression is given by the denominator of Equation (3) in the main manuscript. We make the following observations: • DGN achieves the largest test accuracy by exploring the deeper neural architecture with a larger number of graph convolutional layers. In the scenarios with missing features, GNN model relies highly on the neighborhood structure to classify nodes. DGN enables the deeper GNN model to exploit neighborhood structure with multiple hops away, and at the same time relieves the over-smoothing issue. • Comparing with other normalization techniques, DGN generally improves the group distance ratio to relieve over-smoothing issue. Although in some cases the ratios are smaller than those of pair normalization upon SGC framework, we still achieve the comparable or even better test accuracy. That is because DGN has a smaller intra-group distance to facilitate node classiﬁcation within the same group, which is analyzed in the followings. • DGN obtains an appropriate intra-group distance to optimize the node classiﬁcation task. While the over-smoothing issue results in an extremely-small distance in the model without normalization, a larger one in pair normalization leads to the inaccurate node classiﬁcation within each group. That is because the pair normalization is designed to maintain the distance between each pair of nodes, no matter whether they locate in the same class group or not. The divergence of node representations in a group prevents a downstream classiﬁer to assign them the same class label. 170 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1 2 3 Group ratio None batch pair group 0 10 20 30 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1 2 3 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 2 3 0 50 100 Layers 0.5 1.0 Figure 8: The test accuracy, group distance ratio and intra-group distance in Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4GCN Test accuracy 0 10 20 30 1.1 1.2 1.3 Group ratio None batch pair group 0 10 20 30 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4GAT 0 10 20 30 1.0 1.2 1.4 0 10 20 30 0.0 0.5 1.0 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.2 1.4 0 50 100 Layers 0.5 1.0 Figure 9: The test accuracy, group distance ratio and intra-group distance in Citeseer with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 180 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1.0 1.2 1.4 1.6 Group ratio None batch pair group 0 10 20 30 0.0 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1.00 1.25 1.50 1.75 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.0 1.5 2.0 0 50 100 Layers 0.0 0.5 1.0 Figure 10: The test accuracy, group distance ratio and intra-group distance in Pubmed with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 2 3 4 Group ratio None batch pair group 0 10 20 30 0.00 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 2 3 4 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 2 4 0 50 100 Layers 0.0 0.5 1.0 Figure 11: The test accuracy, group distance ratio and intra-group distance in CoauthorCS with missing features. We compare differentiable group normalization with none, batch and pair normal- izations. 19",
      "meta_data": {
        "arxiv_id": "2006.06972v1",
        "authors": [
          "Kaixiong Zhou",
          "Xiao Huang",
          "Yuening Li",
          "Daochen Zha",
          "Rui Chen",
          "Xia Hu"
        ],
        "published_date": "2020-06-12T07:18:02Z",
        "pdf_url": "https://arxiv.org/pdf/2006.06972v1.pdf"
      }
    },
    {
      "title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks",
      "abstract": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where\nincreasing network depth leads to homogeneous node representations. While\nprevious work has established that Graph Convolutional Networks (GCNs)\nexponentially lose expressive power, it remains controversial whether the graph\nattention mechanism can mitigate oversmoothing. In this work, we provide a\ndefinitive answer to this question through a rigorous mathematical analysis, by\nviewing attention-based GNNs as nonlinear time-varying dynamical systems and\nincorporating tools and techniques from the theory of products of inhomogeneous\nmatrices and the joint spectral radius. We establish that, contrary to popular\nbelief, the graph attention mechanism cannot prevent oversmoothing and loses\nexpressive power exponentially. The proposed framework extends the existing\nresults on oversmoothing for symmetric GCNs to a significantly broader class of\nGNN models, including random walk GCNs, Graph Attention Networks (GATs) and\n(graph) transformers. In particular, our analysis accounts for asymmetric,\nstate-dependent and time-varying aggregation operators and a wide range of\ncommon nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",
      "full_text": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks Xinyi Wu1,2 Amir Ajorlou2 Zihui Wu3 Ali Jadbabaie1,2 1Institute for Data, Systems and Society (IDSS), MIT 2Laboratory for Information and Decision Systems (LIDS), MIT 3Department of Computing and Mathematical Sciences (CMS), Caltech {xinyiwu,ajorlou,jadbabai}@mit.edu zwu2@caltech.edu Abstract Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) ex- ponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on over- smoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU. 1 Introduction Graph neural networks (GNNs) have emerged as a powerful framework for learning with graph- structured data [4, 8, 9, 13, 20, 33, 39] and have shown great promise in diverse domains such as molecular biology [46], physics [1] and recommender systems [41]. Most GNN models follow the message-passing paradigm [12], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes. One notable drawback of repeated message-passing isoversmoothing, which refers to the phenomenon that stacking message-passing GNN layers makes node representations of the same connected component converge to the same vector [5, 19, 20, 25, 27, 32, 42]. As a result, whereas depth has been considered crucial for the success of deep learning in many fields such as computer vision [16], most GNNs used in practice remain relatively shallow and often only have few layers [20, 39, 43]. On the theory side, while previous works have shown that the symmetric Graph Convolution Networks (GCNs) with ReLU and LeakyReLU nonlinearities exponentially lose expressive power, analyzing the oversmoothing phenomenon in other types of GNNs is still an open question [5, 27]. In particular, the question of whether the graph attention mechanism can prevent oversmoothing has not been settled yet. Motivated by the capacity of graph attention to distinguish the importance of different edges in the graph, some works claim that oversmoothing is alleviated in Graph Attention Networks (GATs), heuristically crediting to GATs’ ability to learn adaptive node-wise aggregation operators via the attention mechanism [26]. On the other hand, it has been empirically observed that similar to arXiv:2305.16102v4  [cs.LG]  4 Jun 2024the case of GCNs, oversmoothing seems inevitable for attention-based GNNs such as GATs [32] or (graph) transformers [35]. The latter can be viewed as attention-based GNNs on complete graphs. In this paper, we provide a definitive answer to this question — attention-based GNNs also lose expressive power exponentially, albeit potentially at a slower exponential rate compared to GCNs. Given that attention-based GNNs can be viewed as nonlinear time-varying dynamical systems, our analysis is built on the theory of products of inhomogeneous matrices [ 14, 34] and the concept of joint spectral radius [ 31], as these methods have been long proved effective in the analysis of time-inhomogeneous markov chains and ergodicity of dynamical systems [2, 14, 34]. While classical results only apply to generic one-dimensional linear time-varying systems, we address four major challenges arising in analyzing attention-based GNNs: (1) the aggregation operators computed by attention are state-dependent, in contrast to conventional fixed graph convolutions; (2) the systems are multi-dimensional, which involves the coupling across feature dimensions; (3) the dynamics are nonlinear due to the nonlinear activation function in each layer; (4) the learnable weights and aggregation operators across different layers result in time-varying dynamical systems. Below, we highlight our key contributions: • As our main contribution, we establish that oversmoothing happens exponentially as model depth increases for attention-based GNNs, resolving the long-standing debate about whether attention-based GNNs can prevent oversmoothing. • We analyze attention-based GNNs through the lens of nonlinear, time-varying dynamical systems. The strength of our analysis stems from its ability to exploit the inherently common connectivity structure among the typically asymmetric state-dependent aggregation operators at different attentional layers. This enables us to derive rigorous theoretical results on the ergodicity of infinite products of matrices associated with the evolution of node representations across layers. Incorporating results from the theory of products of inhomogeneous matrices and their joint spectral radius, we then establish that oversmoothing happens at an exponential rate for attention- based GNNs from our ergodicity results. • Our analysis generalizes the existing results on oversmoothing for symmetric GCNs to a sig- nificantly broader class of GNN models with asymmetric, state-dependent and time-varying aggregation operators and nonlinear activation functions under general conditions. In partic- ular, our analysis can accommodate a wide range of common nonlinearities such as ReLU, LeakyReLU, and even non-monotone ones like GELU and SiLU. We validate our theoretical results on six real-world datasets with two attention-based GNN architectures and five common nonlinearities. 2 Related Work Oversmoothing problem in GNNs Oversmoothing is a well-known problem in deep GNNs, and many techniques have been proposed in order to mitigate it practically [ 6, 15, 21, 22, 30, 44, 48]. On the theory side, analysis of oversmoothing has only been carried out for the graph convolution case [5, 19, 27, 42]. In particular, by viewing graph convolutions as a form of Laplacian filter, prior works have shown that for GCNs, the node representations within each connected component of a graph will converge to the same value exponentially [ 5, 27]. However, oversmoothing is also empirically observed in attention-based GNNs such as GATs [32] or transformers [35]. Although some people hypothesize based on heuristics that attention can alleviate oversmoothing [ 26], a rigorous analysis of oversmoothing in attention-based GNNs remains open [5]. Theoretical analysis of attention-based GNNs Existing theoretical results on attention-based GNNs are limited to one-layer graph attention. Recent works in this line include Brody et al. [ 3] showing that the ranking of attention scores computed by a GAT layer is unconditioned on the query node, and Fountoulakis et al. [11] studying node classification performance of one-layer GATs on a random graph model. More relevantly, Wang et al. [ 40] made a claim that oversmoothing is asymptotically inevitable in GATs. Aside from excluding nonlinearities in the analysis, there are several flaws in the proof of their main result (Theorem 2). In particular, their analysis assumes the same stationary distribution for all the stochastic matrices output by attention at different layers. This is typically not the case given the state-dependent and time-varying nature of these matrices. In fact, the main challenge in analyzing multi-layer attention lies in the state-dependent and time-varying 2nature of these input-output mappings. Our paper offers novel contributions to the research on attention-based GNNs by developing a rich set of tools and techniques for analyzing multi-layer graph attention. This addresses a notable gap in the existing theory, which has primarily focused on one-layer graph attention, and paves the way for future research to study other aspects of multi-layer graph attention. 3 Problem Setup 3.1 Notations Let R be the set of real numbers and N be the set of natural numbers. We use the shorthands [n] := {1, . . . , n} and N≥0 := N ∪ {0}. We denote the zero-vector of length N by 0 ∈ RN and the all-one vector of length N by 1 ∈ RN . We represent an undirected graph with N nodes by G = (A, X), where A ∈ {0, 1}N×N is the adjacency matrix and X ∈ RN×d are the node feature vectors of dimension d. Let E(G) be the set of edges of G. For nodes i, j∈ [N], Aij = 1 if and only if i and j are connected with an edge in G, i.e., (i, j) ∈ E(G). For each i ∈ [N], Xi ∈ Rd represents the feature vector for node i. We denote the degree matrix of G by Ddeg = diag(A1) and the set of all neighbors of node i by Ni. Let ∥·∥ 2, ∥·∥ ∞, ∥·∥ F be the 2-norm, ∞-norm and Frobenius norm, respectively. We use∥·∥ max to denote the matrix max norm, i.e., for a matrix M ∈ Rm×n, ∥M∥max := max ij |Mij|. We use ≤ew to denote element-wise inequality. Lastly, for a matrix M, we denote its ith row by Mi· and jth column by M·j. 3.2 Graph attention mechanism We adopt the following definition of graph attention mechanism. Given node representation vectors Xi and Xj, we first apply a shared learnable linear transformation W ∈ Rd×d′ to each node, and then use an attention function Ψ : Rd′ × Rd′ → R to compute a raw attention coefficient eij = Ψ(W⊤Xi, W⊤Xj) that indicates the importance of node j’s features to node i. Then the graph structure is injected into the mechanism by performing masked attention, where for each node i, we only compute its attention to its neighbors. To make coefficients easily comparable across different nodes, we normalize eij among all neighboring nodes j of node i using the softmax function to get the normalized attention coefficients: Pij = softmaxj(eij) = exp(eij)P k∈Ni exp(eik) . The matrix P, where the entry in the ith row and the jth column is Pij, is a row stochastic matrix. We refer to P as an aggregation operator in message-passing. 3.3 Attention-based GNNs Having defined the graph attention mechanism, we can now write the update rule of a single graph attentional layer as X′ = σ(P XW) , where X and X′ are are the input and output node representations, respectively, σ(·) is a pointwise nonlinearity function, and the aggregation operator P is a function of XW . As a result, the output of the tth graph attentional layers can be written as X(t+1) = σ(P(t)X(t)W(t)) t ∈ N≥0, (1) where X(0) = X is the input node features, W(t) ∈ Rd′×d′ for t ∈ N and W(0) ∈ Rd×d′ . For the rest of this work, without loss of generality, we assume that d = d′. The above definition is based on single-head graph attention. Multi-head graph attention uses K ∈ N weight matrices W1, . . . , WK in each layer and averages their individual single-head outputs [11, 39]. Without loss of generality, we consider single graph attention in our analysis in Section 4, but we note that our results automatically apply to the multi-head graph attention setting since K is finite. 33.4 Measure of oversmoothing We use the following notion of oversmoothing, inspired by the definition proposed in Rusch et al. [32]1: Definition 1. For an undirected and connected graphG, µ : RN×d → R≥0 is called a node similarity measure if it satisfies the following axioms: 1. ∃c ∈ Rd such that Xi = c for all node i if and only if µ(X) = 0, for X ∈ RN×d; 2. µ(X + Y ) ≤ µ(X) + µ(Y ), for all X, Y∈ RN×d. Then oversmoothing with respect to µ is defined as the layer-wise convergence of the node-similarity measure µ to zero, i.e., lim t→∞ µ(X(t)) = 0. (2) We say oversmoothing happens at an exponential rate if there exists constantsC1, C2 > 0, such that for any t ∈ N, µ(X(t)) ≤ C1e−C2t. (3) We establish our results on oversmoothing for attention-based GNNs using the following node similarity measure: µ(X) := ∥X − 1γX∥F , where γX = 1⊤X N . (4) Proposition 1. ∥X − 1γX∥F is a node similarity measure. The proof of the above proposition is provided in Appendix B. Other common node similarity measures include the Dirichlet energy [ 5, 32].2 Our measure is mathematically equivalent to the measure inf Y =1c⊤,c∈Rd {∥X − Y ∥F } defined in Oono and Suzuki [ 27], but our form is more direct to compute. One way to see the equivalence is to consider the orthogonal projection into the space perpendicular to span{1}, denoted by B ∈ R(N−1)×N . Then our definition of µ satisfies ∥X − 1γx∥F = ∥BX∥F , where the latter quantity is exactly the measure defined in [27]. 3.5 Assumptions We make the following assumptions (in fact, quite minimal) in deriving our results: A1 The graph G is connected and non-bipartite. A2 The attention function Ψ(·, ·) is continuous. A3 The sequence {∥Qk t=0 |W(t)|∥max}∞ k=0 is bounded. A4 The point-wise nonlinear activation function σ(·) satisfies 0 ≤ σ(x) x ≤ 1 for x ̸= 0 and σ(0) = 0. We note that all of these assumptions are either standard or quite general. Specifically,A1 is a standard assumption for theoretical analysis on graphs. For graphs with more than one connected components, the same results apply to each connected component. A1 can also be replaced with requiring the graph G to be connected and have self-loops at each node. Non-bipartiteness and self-loops both ensure that long products of stochastic matrices corresponding to aggregation operators in different graph attentional layers will eventually become strictly positive. The assumptions on the GNN architecture A2 and A4 can be easily verified for commonly used GNN designs. For example, the attention function LeakyReLU(a⊤[W⊤Xi||W⊤Xj]), a∈ R2d′ used in the GAT [39], where [·||·] denotes concatenation, is a specific case that satisfies A2. Other 1We distinguish the definition of oversmoothing and the rate of oversmoothing. This parallels the notion of stability and its rate in dynamical systems. 2In fact, our results are not specific to our choice of node similarity measure µ and directly apply to any Lipschitz node similarity measure, including the Dirichlet energy under our assumptions. See Remark 2 after Theorem 1. 4architectures that satisfy A2 include GATv2 [3] and (graph) transformers [38]. As for A4, one way to satisfy it is to have σ be 1-Lipschitz and σ(x) ≤ 0 for x <0 and σ(x) ≥ 0 for x >0. Then it is easy to verify that most of the commonly used nonlinear activation functions such as ReLU, LeakyReLU, GELU, SiLU, ELU, tanh all satisfy A4. Lastly, A3 is to ensure boundedness of the node representations’ trajectories X(t) for all t ∈ N≥0. Such regularity assumptions are quite common in the asymptotic analysis of dynamical systems, as is also the case for the prior works analyzing oversmoothing in symmetric GCNs [5, 27]. 4 Main Results In this section, we lay out a road-map for deriving our main results, highlighting the key ideas of the proofs. The complete proofs are provided in the Appendices. We start by discussing the dynamical system formulation of attention-based GNNs in Section 4.1. By showing the boundedness of the node representations’ trajectories, we prove the existence of a common connectivity structure among aggregation operators across different graph attentional layers in Section 4.2. This implies that graph attention cannot fundamentally change the graph connectivity, a crucial property that will eventually lead to oversmoothing. In Section 4.3, we develop a framework for investigating the asymptotic behavior of attention-based GNNs by introducing the notion of ergodicity and its connections to oversmoothing. Then utilizing our result on common connectivity structure among aggregation operators, we establish ergodicity results for the systems associated with attention-based GNNs. In Section 4.4, we introduce the concept of the joint spectral radius for a set of matrices [31] and employ it to deduce exponential convergence of node representations to a common vector from our ergodicity results. Finally, we present our main result on oversmoothing in attention-based GNNs in Section 4.5 and comment on oversmoothing in GCNs in comparison with attention-based GNNs in Section 4.6. 4.1 Attention-based GNNs as nonlinear time-varying dynamical systems The theory of dynamical systems concerns the evolution of some state of interest over time. By view- ing the model depth t as the time variable, the input-output mapping at each graph attentional layer X(t+1) = σ(P(t)X(t)W(t)) describes a nonlinear time-varying dynamical system. The attention- based aggregation operator P(t) is state-dependent as it is a function of X(t)W(t). Given the notion of oversmoothing defined in Section 3.4, we are interested in characterizing behavior of X(t) as t → ∞. If the activation function σ(·) is the identity map, then repeated application of (1) gives X(t+1) = P(t) . . . P(0)XW (0) . . . W(t) . The above linear form would enable us to leverage the rich literature on the asymptotic behavior of the products of inhomogeneous row-stochastic matrices (see, e.g., [14, 34]) in analyzing the long-term behavior of attention-based GNNs. Such a neat expansion, however, is not possible when dealing with a nonlinear activation functionσ(·). To find a remedy, let us start by observing that element-wise application of σ to a vector y ∈ Rd can be written as σ(y) = diag \u0012σ(y) y \u0013 y , (5) where diag \u0010 σ(y) y \u0011 is a diagonal matrix with σ(yi)/yi on the ith diagonal entry. Defining σ(0)/0 := σ′(0) or 1 if the derivative does not exist along with the assumption σ(0) = 0 in A4, it is easy to check that the above identity still holds for vectors with zero entries. We can use (5) to write the ith column of X(t+1) as X(t+1) ·i = σ(P(t)(X(t)W(t))·i) = D(t) i P(t)(X(t)W(t))·i = D(t) i P(t) dX j=1 W(t) ji X(t) .j , (6) where D(t) i is a diagonal matrix. It follows from the assumption on the nonlinearities A4 that diag(0) ≤ew D(t) i ≤ew diag(1) . 5We define D to be the set of all possible diagonal matrices D(t) i satisfying the above inequality: D := {diag(d) : d ∈ RN , 0 ≤ew d ≤ew 1}. Using (6) recursively, we arrive at the following formulation forX(t+1) ·i : X(t+1) ·i = X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) ·j0 . (7) 4.2 Common connectivity structure among aggregation operators across different layers We can use the formulation in (7) to show the boundedness of the node representations’ trajectories X(t) for all t ∈ N≥0, which in turn implies the boundedness of the input to graph attention in each layer, X(t)W(t). Lemma 1. Under assumptions A3-A4, there exists C >0 such that ∥X(t)∥max ≤ C for all t ∈ N≥0. For a continuous Ψ(·, ·)3, the following lemma is a direct consequence of Lemma 1, suggesting that the graph attention mechanism cannot fundamentally change the connectivity pattern of the graph. Lemma 2. Under assumptions A2-A4, there exists ϵ >0 such that for all t ∈ N≥0 and for any (i, j) ∈ E(G), we have P(t) ij ≥ ϵ. One might argue that Lemma 2 is an artifact of the continuity of the softmax function. The softmax function is, however, often favored in attention mechanisms because of its trainability in back propagation compared to discontinuous alternatives such as hard thresholding. Besides trainability issues, it is unclear on a conceptual level whether it is reasonable to absolutely drop an edge from the graph as is the case for hard thresholding. Lemma 2 is an important step towards the main convergence result of this work, which states that all the nodes will converge to the same representation vector at an exponential rate. We define the family of row-stochastic matrices satisfying Lemma 2 below. Definition 2. Let ϵ > 0. We define PG,ϵ to be the set of row-stochastic matrices satisfying the following conditions: 1. ϵ ≤ Pij ≤ 1, if (i, j) ∈ E(G), 2. Pij = 0, if (i, j) /∈ E(G). 4.3 Ergodicity of infinite products of matrices Ergodicity, in its most general form, deals with the long-term behavior of dynamical systems. The oversmoothing phenomenon in GNNs defined in the sense of (2) concerns the convergence of all rows of X(t) to a common vector. To this end, we define ergodicity in our analysis as the convergence of infinite matrix products to a rank-one matrix with identical rows. Definition 3 (Ergodicity). Let B ∈ R(N−1)×N be the orthogonal projection onto the space orthogo- nal to span{1}. A sequence of matrices {M(n)}∞ n=0 is ergodic if lim t→∞ B tY n=0 M(n) = 0 . We will take advantage of the following properties of the projection matrixB already established in Blondel et al. [2]: 1. B1 = 0; 2. ∥Bx∥2 = ∥x∥2 for x ∈ RN if x⊤1 = 0; 3. Given any row-stochastic matrix P ∈ RN×N , there exists a unique matrix ˜P ∈ R(N−1)×(N−1) such that BP = ˜P B . 3More generally, for Ψ(·, ·) that outputs bounded attention scores for bounded inputs. 6We can use the existing results on the ergodicity of infinite products of inhomogeneous stochastic matrices [14, 34] to show that any sequence of matrices in PG,ϵ is ergodic. Lemma 3. Fix ϵ >0. Consider a sequence of matrices {P(t)}∞ t=0 in PG,ϵ. That is, P(t) ∈ PG,ϵ for all t ∈ N≥0. Then {P(t)}∞ t=0 is ergodic. The main proof strategy for Lemma 3 is to make use of the Hilbert projective metric and the Birkhoff contraction coefficient. These are standard mathematical tools to prove that an infinite product of inhomogeneous stochastic matrices is ergodic. We refer interested readers to the textbooks [14, 34] for a comprehensive study of these subjects. Despite the nonlinearity of σ(·), the formulation (7) enables us to express the evolution of the feature vector trajectories as a weighted sum of products of matrices of the form DP where D ∈ Dand P ∈ PG,ϵ. We define the set of such matrices as MG,ϵ := {DP : D ∈ D, P∈ PG,ϵ}. A key step in proving oversmoothing for attention-based GNNs under our assumptions is to show the ergodicity of the infinite products of matrices in MG,ϵ. In what follows, we lay out the main ideas of the proof, and refer readers to Appendix F for the details. Consider a sequence {D(t)P(t)}∞ t=0 in MG,ϵ, that is, D(t)P(t) ∈ MG,ϵ for all t ∈ N≥0. For t0 ≤ t1, define Qt0,t1 := D(t1)P(t1) . . . D(t0)P(t0), δ t = ∥D(t) − IN ∥∞ , where IN denotes the N × N identity matrix. The common connectivity structure among P(t)’s established in Section 4.2 allows us to show that long products of matrices DP from MG,ϵ will eventually become a contraction in ∞-norm. More precisely, we can show that there exists T ∈ N and 0 < c <1 such that for all t ∈ N≥0, ∥Qt,t+T ∥∞ ≤ 1 − cδt. Next, define βk := Qk t=0(1 − cδt) and let β := lim k→∞ βk. Note that β is well-defined because the partial product is non-increasing and bounded from below. We can use the above contraction property to show the following key lemma. Lemma 4. Let βk := Qk t=0(1 − cδt) and β := lim k→∞ βk. 1. If β = 0, then lim k→∞ Q0,k = 0 ; 2. If β >0, then lim k→∞ BQ0,k = 0 . The ergodicity of sequences of matrices in MG,ϵ immediately follows from Lemma 4, which in turn implies oversmoothing as defined in (2). Lemma 5. Any sequence {D(t)P(t)}∞ t=0 in MG,ϵ is ergodic. Remark The proof techniques developed in [ 5, 27] are restricted to symmetric matrices hence cannot be extended to more general family of GNNs, as they primarily rely on matrix norms for con- vergence analysis. Analyses solely using matrix norms are often too coarse to get meaningful results when it comes to asymmetric matrices. For instance, while the matrix 2-norm and matrix eigenvalues are directly related for symmetric matrices, the same does not generally hold for asymmetric matrices. Our analysis, on the other hand, exploits the inherently common connectivity structure among these matrices in deriving the ergodicity results in Lemma 3-5. 4.4 Joint spectral radius Using the ergodicity results in the previous section, we can establish that oversmoothing happens in attention-based GNNs. To show that oversmoothing happens at an exponential rate, we introduce the notion of joint spectral radius, which is a generalization of the classical notion of spectral radius of a single matrix to a set of matrices [7, 31]. We refer interested readers to the textbook [18] for a comprehensive study of the subject. 7Definition 4 (Joint Spectral Radius). For a collection of matricesA, the joint spectral radiusJSR(A) is defined to be JSR(A) = lim sup k→∞ sup A1,A2,...,Ak∈M ∥A1A2...Ak∥ 1 k , and it is independent of the norm used. In plain words, the joint spectral radius measures the maximal asymptotic growth rate that can be obtained by forming long products of matrices taken from the set A.To analyze the convergence rate of products of matrices in MG,ϵ to a rank-one matrix with identical rows, we treat the two cases of linear and nonlinear activation functions, separately. For the linear case, where σ(·) is the identity map, we investigate the dynamics induced by P(t)’s on the subspace orthogonal to span{1} and use the third property of the orthogonal projection B established in Section 4.3 to write BP1P2 . . . Pk = ˜P1 ˜P2... ˜PkB, where each ˜Pi is the unique matrix in R(N−1)×(N−1) that satisfies BPi = ˜PiB. Let us define ˜PG,ϵ := { ˜P : BP = ˜P B, P∈ PG,ϵ }. We can use Lemma 3 to show that the joint spectral radius of ˜PG,ϵ is strictly less than 1. Lemma 6. Let 0 < ϵ <1. Under assumptions A1-A4, JSR( ˜PG,ϵ) < 1. For the nonlinear case, let 0 < δ <1 and define Dδ := {diag(d) : d ∈ RN , 0 ≤ew d ≤ew δ}, MG,ϵ,δ := {DP : D ∈ Dδ, P ∈ PG,ϵ}. Then again, using the ergodicity result from the previous section, we establish that the joint spectral radius of MG,ϵ,δ is also less than 1. Lemma 7. Let 0 < ϵ, δ <1. Under assumptions A1-A4, JSR(MG,ϵ,δ) < 1. The above lemma is specifically useful in establishing exponential rate for oversmoothing when dealing with nonlinearities for which Assumption 4 holds in the strict sense, i.e. 0 ≤ σ(x) x < 1 (e.g., GELU and SiLU nonlinearities). Exponential convergence, however, can still be established under a weaker requirement, making it applicable to ReLU and Leaky ReLU, as we will see in Theorem 1. It follows from the definition of the joint spectral radius that ifJSR(A) < 1, for anyJSR(A) < q <1, there exists a C for which ∥A1A2...Aky∥ ≤Cqk∥y∥ (8) for all y ∈ RN−1 and A1, A2, ..., Ak ∈ A. 4.5 Main Theorems We have all the ingredients to prove our main results. As a final step, given Lemma 5 and(8), how could we incorporate the weight matrices W(t) into the final analysis? Recall that the formulation of X(t+1) ·i in (7), then ∥BX(t+2) ·i ∥2 can be bounded as ∥BX(t+1) ·i ∥2 = X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) ·j0 ≤ C∥(|W(0)|...|W(t)|)·i∥1 sup D(n)∈D 0≤n≤t \r\r\rBD(t)P(t)...D(0)P(0) \r\r\r 2 ≤ C′ sup D(n)∈D 0≤n≤t \r\r\rBD(t)P(t)...D(0)P(0) \r\r\r 2 where C = max j∈[d] ∥X(0) ·j ∥2, and the last inequality is due to the assumption A3 as it implies that there exists C′′ ∈ R such that for all t ≥ 0, i ∈ [d], ∥(|W(0)|...|W(t)|)·i∥1 ≤ C′′. It is then evident that the term sup D(n)∈D 0≤n≤t \r\r\rBD(t)P(t)...D(0)P(0) \r\r\r 2 (9) 8determines both the convergence and its rate, i.e. if (9) converges to zero, so does µ(X(t)); if (9) converges to zero exponentially fast, the same applies for µ(X(t)). Lemma 6 and Lemma 7 on the joint spectral radius give sufficient conditions to satisfy the above key condition on (9). Specifically, applying (8) to the recursive expansion of X(t+1) ·i in (7) using the 2-norm, we can prove the exponential convergence of µ(X(t)) to zero for the similarity measure µ(·) defined in (4), which in turn implies the convergence of node representations to a common representation at an exponential rate. This completes the proof of the main result of this paper, which states that oversmoothing defined in (2) is unavoidable for attention-based GNNs, and that an exponential convergence rate can be attained under general conditions. Theorem 1. Under assumptions A1-A4, if in addition, • (linear) σ(·) is the identity map, or • (nonlinear) there exists K ∈ N and 0 < δ <1 for which the following holds: For all m ∈ N≥0, there is nm ∈ {0} ∪[K − 1] such that for any c ∈ [d], σ(X(mK+nm) rcc )/X(mK+nm) rcc ≤ δ for some rc ∈ [N], ( ⋆) then there exists q <1 and C1(q) > 0 such that µ(X(t)) ≤ C1qt , ∀t ≥ 0 . As a result, node representations X(t) exponentially converge to the same value as the model depth t → ∞. Theorem 1 establishes that oversmoothing is asymptotically inevitable for attention-based GNNs with general nonlinearities. Despite similarity-based importance assigned to different nodes via the aggregation operator P(t), such attention-based mechanisms are yet unable to fundamentally change the connectivity structure of P(t), resulting in node representations converging to a common vector. Our results hence indirectly support the emergence of alternative ideas for changing the graph connectivity structure such as edge-dropping [15, 30] or graph-rewiring [22], in an effort to mitigate oversmoothing. Remark 1. For nonlinearities such as SiLU or GELU, the condition (⋆) is automatically satisfied under A3-A4. For ReLU and LeakyReLU, this is equivalent to requiring that there existsK ∈ N such that for all m ∈ N≥0, there exists nm ∈ {0} ∪[K − 1] where for any c ∈ [d], X(mK+nm) rcc < 0 for some rc ∈ [d]. Remark 2. We note that our results are not specific to the choice of node similarity measure µ(X) = ∥X − 1γX ∥F considered in our analysis. In fact, exponential convergence of any other Lipschitz node similarity measure µ′ to 0 is a direct corollary of Theorem 1. To see this, observe that for a node similarity measure µ′ with a Lipschitz constant L, it holds that µ′(X) = |µ′(X) − µ′(1γX )| ≤L∥X − 1γX ∥F = Lµ(X). In particular, the Dirichlet energy is Lipschitz given that the input X has a compact domain, es- tablished in Lemma 1. Hence our theory directly implies the exponential convergence of Dirichlet energy. Besides utilizing the properties of specific nonlinearity functions σ(·), another way to derive the convergence of µ(X(t)) to zero under general class of nonlinearities D is to restrict the class of weights W(t). To see this, write the update rule of X(t+1) in the vectorized form: vec \u0010 X(t+1) \u0011 = ˜D(t) \u0012\u0010 W(t) \u0011⊤ ⊗ P(t) \u0013 vec \u0010 X(t) \u0011 , (10) where ˜D(t) ∈ RNd×Nd represents the effect of σ(·) on each entry of vec \u0000 X(t)\u0001 . We restrict the class of W(t) to satisfy the following stricter condition: Alternative A3 (A3’) For any t ∈ N≥0, W(t) ∈ Rd×d is a column substochastic matrix4. Further- more, there exists 0 < ξ <1 such that for all t ∈ N≥0, 4Column substochastic means that the sum of entries in each column is no greater than one. 91. W(t) ii ≥ ξ, for all i ∈ [d]; 2. If W(t) ij > 0, W(t) ij ≥ ξ and W(t) ji ≥ ξ for all i ̸= j ∈ [d]. Then Lemma 5 directly implies the convergence of µ(X(t)) to zero as follows: Theorem 2. Under assumptions A1, A2, A3’ and A4, lim t→∞ µ(X(t)) = 0 . Remark 3. We note that the above condition requires W(t) to have a symmetric sparsity pattern (symmetric entries do not need to be equal). Such a symmetric pattern is necessary for the result to hold. To see the necessity, consider the following counterexample: Counterexample Let N = 2, d= 2, and X(t) ·1 and X(t) ·2 satisfy the following update rule:    X(t+1) ·1 = D1 \u0010 W11P X(t) ·1 + W21P X(t) ·2 \u0011 X(t+1) ·2 = D2 \u0010 W12P X(t) ·1 + W22P X(t) ·2 \u0011 (11) where D1 = \u0014 1 0 0 1 \u0015 D2 = \u0014 1/2 0 0 1 \u0015 P = \u0014 0.5 0 .5 0.5 0 .5 \u0015 , and W = \u0014 W11 W12 W21 W22 \u0015 = \u0014 2/3 0 1/3 1 \u0015 . Then X = \u0014 1/3 1 2/3 1 \u0015 is a fixed point for the system defined in (11). As a result, if X(0) starts at X, µ(X(t)) will not converge to zero. Remark 4. If the sparsity pattern of W(t) can be represented as a connected graphG′ with self-loops at each node, then the convergence of µ(X(t)) happens in a stronger sense: each entry of X(t) converges to the same value — more than just each row ofX(t) converging to the same vector. Such a result, might also be related to the feature overcorrelation phenomenon observed in GNNs [17]. 4.6 Comparison with the GCN Computing or approximating the joint spectral radius for a given set of matrices is known to be hard in general [37], yet it is straightforward to lower bound JSR( ˜PG,ϵ) as stated in the next proposition. Proposition 2. Let λ be the second largest eigenvalue of D−1/2 deg AD−1/2 deg . Then under assumptions A1-A4, it holds that λ ≤ JSR( ˜PG,ϵ). In the linear case, the upper bound q on the convergence rate that we get for graph attention in Theorem 1 is lower bounded by JSR( ˜PG,ϵ). A direct consequence of the above result is that q is at least as large as λ. On the other hand, previous work has already established that in the graph convolution case, the convergence rate of µ(X(t)) is O(λt) [5, 27]. It is thus natural to expect attention-based GNNs to potentially have better expressive power at finite depth than GCNs, even though they both inevitably suffer from oversmoothing. This is also evident from the numerical experiments that we present in the next section. 5 Numerical Experiments In this section, we validate our theoretical findings via numerical experiments using the three commonly used homophilic benchmark datasets: Cora, CiteSeer, and PubMed [ 45] and the three commonly used heterophilic benchmark datasets: Cornell, Texas, and Wisconsin [29]. We note that 10100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cora GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 CiteSeer GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 PubMed GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cora GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 CiteSeer GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 PubMed GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cornell GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 T exas GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 Wisconsin GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cornell GCN 100 101 102 number of layers 10 4 10 3 10 2 10 1 100 101 102 T exas GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 Wisconsin GCN ReLU LeakyReLU (0.01) LeakyReLU (0.4) LeakyReLU (0.8) GELU Figure 1: Evolution of µ(X(t)) (in log-log scale) on the largest connected component of each dataset (top 2 rows: homophilic graphs; bottom 2 rows: heterophilic graphs). Oversmoothing happens exponentially in both GCNs and GATs with the rates varying depending on the choice of activation function. Notably, GCNs demonstrate faster rates of oversmoothing compared to GATs. our theoretical results are developed for generic graphs and thus hold for datasets exhibiting either homophily or heterophily and even those that are not necessarily either of the two. More details about the experiments are provided in Appendix L. For each dataset, we trained a 128-layer single-head GAT and a 128-layer GCN with the random walk graph convolution D−1 degA, each having 32 hidden dimensions and trained using the standard features and splits. The GCN with the random walk graph convolution is a special type of attention- based GNNs where the attention function is constant. For each GNN model, we considered various nonlinear activation functions: ReLU, LeakyReLU (with three different negative slope values: 0.01, 0.4 and 0.8) and GELU. Here, we chose GELU as an illustration of the generality of our assumption on nonlinearities, covering even non-monotone activation functions such as GELU. We ran each experiment 10 times. Figure 1 shows the evolution ofµ(X(t)) in log-log scale on the largest connected component of each graph as we forward pass the input X into a trained model. The solid curve is the average over 10 runs and the band indicates one standard deviation around the average. We observe that, as predicted by our theory, oversmoothing happens at an exponential rate for both GATs and GCNs, regardless of the choice of nonlinear activation functions in the GNN architectures. Notably, GCNs exhibit a significantly faster rate of oversmoothing compared to GATs. This aligns the observation made in Section 4.6, expecting a potentially better expressive power for GATs than GCNs at finite depth. Furthermore, the exponential convergence rate of oversmoothing varies among GNNs with different nonlinear activation functions. From a theory perspective, as different activation functions constitute different subsets of MG,ϵ and different sets of matrices have different joint spectral radii, it is not surprising that the choice of nonlinear activation function would affect the convergence rate. In particular, among the nonlinearties we considered, ReLU in fact magnifies oversmoothing the second most. As a result, although ReLU is often the default choice for the standard implementation of many GNN architectures [10, 20], one might wish to consider switching to other nonliearities to better mitigate oversmoothing. 116 Conclusion Oversmoothing is one of the central challenges in developing more powerful GNNs. In this work, we reveal new insights on oversmoothing in attention-based GNNs by rigorously providing a negative answer to the open question of whether graph attention can implicitly prevent oversmoothing. By analyzing the graph attention mechanism within the context of nonlinear time-varying dynamical systems, we establish that attention-based GNNs lose expressive power exponentially as model depth increases. We upper bound the convergence rate for oversmoothing under very general assumptions on the nonlinear activation functions. One may try to tighten the bounds by refining the analysis separately for each of the commonly used activation functions. Future research should also aim to improve the design of graph attention mechanisms based on our theoretical insights and utilize our analysis techniques to study other aspects of multi-layer graph attention. Acknowledgments The authors deeply appreciate Bernard Chazelle for noticing a mistake in the previous version of the paper and suggesting an alternative idea for the proof. XW would like to thank Jennifer Tang and William Wang for helpful discussions throughout the project. The authors are also grateful to Zhijian Zhuo and Yifei Wang for identifying an error in the first draft of the paper, thank the anonymous NeurIPS reviewers for providing valuable feedback, and acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing computing resources that have contributed to the research results reported within this paper. This research has been supported in part by ARO MURI W911NF-19-0217, ONR N00014-20-1-2394, and the MIT-IBM Watson AI Lab. References [1] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In NeurIPS, 2016. [2] Vincent D. Blondel, Julien M. Hendrickx, Alexander Olshevsky, and John N. Tsitsiklis. Con- vergence in multiagent coordination, consensus, and flocking. Proceedings of the 44th IEEE Conference on Decision and Control, pages 2996–3000, 2005. [3] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? InICLR, 2022. [4] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. [5] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. In ICML Graph Representation Learning and Beyond (GRL+) Workshop, 2020. [6] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020. [7] Ingrid Daubechies and Jeffrey C. Lagarias. Sets of matrices all infinite products of which converge. Linear Algebra and its Applications, 161:227–263, 1992. [8] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS, 2016. [9] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez- Bombarelli, Timothy D. Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In NeurIPS, 2015. [10] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. 12[11] Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath. Graph attention retrospective. ArXiv, abs/2202.13060, 2022. [12] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. [13] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IJCNN, 2005. [14] Darald J. Hartfiel. Nonhomogeneous Matrix Products. 2002. [15] Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick G. Duffield, Krishna R. Narayanan, and Xiaoning Qian. Bayesian graph neural networks with adaptive connection sampling. In ICML, 2020. [16] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [17] Wei Jin, Xiaorui Liu, Yao Ma, Charu C. Aggarwal, and Jiliang Tang. Feature overcorrelation in deep graph neural networks: A new perspective. In KDD, 2022. [18] Raphaël M. Jungers. The Joint Spectral Radius: Theory and Applications. 2009. [19] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. In NeurIPS, 2022. [20] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017. [21] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. [22] Johannes Klicpera, Stefan Weißenberger, and Stephan Günnemann. Diffusion improves graph learning. In Neural Information Processing Systems, 2019. [23] Peter D. Lax. Functional Analysis. 2002. [24] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times . 2008. [25] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI, 2018. [26] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in graph convolutional networks. In NeurIPS, 2020. [27] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In ICLR, 2020. [28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. [29] Hongbin Pei, Bingzhen Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In ICLR, 2020. [30] Yu Rong, Wen bing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In ICLR, 2020. [31] Gian-Carlo Rota and W. Gilbert Strang. A note on the joint spectral radius. 1960. [32] T.Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph neural networks. ArXiv, abs/2303.10993, 2023. 13[33] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20:61–80, 2009. [34] Eugene Seneta. Non-negative Matrices and Markov Chains. 2008. [35] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, and James Tin-Yau Kwok. Revisiting over-smoothing in bert from the perspective of graph. In ICLR, 2022. [36] Jacques Theys. Joint spectral radius: theory and approximations. Ph. D. dissertation, 2005. [37] John N. Tsitsiklis and Vincent D. Blondel. The Lyapunov exponent and joint spectral radius of pairs of matrices are hard—when not impossible—to compute and to approximate.Mathematics of Control, Signals and Systems, 10:31–40, 1997. [38] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [39] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In ICLR, 2018. [40] Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Improving graph attention networks with large margin-based constraints. ArXiv, abs/1910.11945, 2019. [41] Shiwen Wu, Wentao Zhang, Fei Sun, and Bin Cui. Graph neural networks in recommender systems: A survey. ACM Computing Surveys, 55:1 – 37, 2020. [42] Xinyi Wu, Zhengdao Chen, William Wang, and Ali Jadbabaie. A non-asymptotic analysis of oversmoothing in graph neural networks. In ICLR, 2023. [43] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32:4–24, 2019. [44] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018. [45] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016. [46] Jiaxuan You, Bowen Liu, Rex Ying, Vijay S. Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In NeurIPS, 2018. [47] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In ICLR, 2020. [48] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020. 14A Basic Facts about Matrix Norms In this section, we list some basic facts about matrix norms that will be helpful in comprehending the subsequent proofs. A.1 Matrix norms induced by vector norms Suppose a vector norm∥·∥α on Rn and a vector norm∥·∥β on Rm are given. Any matrix M ∈ Rm×n induces a linear operator from Rn to Rm with respect to the standard basis, and one defines the corresponding induced norm or operator norm by ∥M∥α,β = sup \u001a∥Mv∥β ∥v∥α , v∈ Rn, v̸= 0 \u001b . If the p-norm for vectors (1 ≤ p ≤ ∞) is used for both spaces Rn and Rm, then the corresponding operator norm is ∥M∥p = sup v̸=0 ∥Mv∥p ∥v∥p . The matrix 1-norm and ∞-norm can be computed by ∥M∥1 = max 1≤j mX i=1 |Mij|, that is, the maximum absolute column sum of the matrix M; ∥M∥∞ = max 1≤m nX j=1 |Mij|, that is, the maximum absolute row sum of the matrix M. Remark In the special case of p = 2, the induced matrix norm ∥ · ∥2 is called the spectral norm, and is equal to the largest singular value of the matrix. For square matrices, we note that the name “spectral norm\" does not imply the quantity is directly related to the spectrum of a matrix, unless the matrix is symmetric. Example We give the following example of a stochastic matrixP, whose spectral radius is 1, but its spectral norm is greater than 1. P = \u0014 0.9 0 .1 0.25 0 .75 \u0015 ∥P∥2 ≈ 1.0188 A.2 Matrix (p, q)-norms The Frobenius norm of a matrix M ∈ Rm×n is defined as ∥M∥F = vuut nX j=1 mX i=1 |Mij|2 , and it belongs to a family of entry-wise matrix norms: for 1 ≤ p, q≤ ∞, the matrix (p, q)-norm is defined as ∥M∥p,q =   nX j=1  mX i=1 |Mij|p !q/p  1/q . The special case p = q = 2 is the Frobenius norm ∥ · ∥F , and p = q = ∞ yields the max norm ∥ · ∥max. 15A.3 Equivalence of norms For any two matrix norms ∥ · ∥α and ∥ · ∥β, we have that for all matrices M ∈ Rm×n, r∥M∥α ≤ ∥M∥β ≤ s∥M∥α for some positive numbers r and s. In particular, the following inequality holds for the 2-norm ∥ · ∥2 and the ∞-norm ∥ · ∥∞: 1√n∥M∥∞ ≤ ∥M∥2 ≤ √m∥M∥∞ . B Proof of Proposition 1 It is straightforward to check that ∥X −1γX∥F satisfies the two axioms of a node similarity measure: 1. ∥X − 1γX∥F = 0 ⇐⇒X = 1γX ⇐⇒Xi = γX for all node i. 2. Let γX = 1⊤X N and γY = 1⊤Y N , then γX + γY = 1⊤(X+Y ) N = γX+Y . So µ(X + Y ) = ∥(X + Y ) − 1(γX + γY )∥F = ∥X − 1γX + Y − 1γY ∥F ≤ ∥X − 1γX∥F + ∥Y − 1γY ∥F = µ(X) + µ(Y ) . C Proof of Lemma 1 According to the formulation (7): X(t+1) ·i = X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) ·j0 , we thus obtain that ∥X(t+1) ·i ∥∞ = \r\r\r\r\r\r X jt+1=i ,(jt,...,j0)∈[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) ·j0 \r\r\r\r\r\r ∞ ≤ X jt+1=i ,(jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rD(t) jt+1 P(t)...D(0) j1 P(0) \r\r\r ∞ \r\r\rX(0) ·j0 \r\r\r ∞ ≤ X jt+1=i ,(jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rX(0) ·j0 \r\r\r ∞ ≤ C0   X jt+1=i ,(jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !  = C0∥(|W(0)|...|W(t)|)·i∥1 , where C0 equals the maximal entry in |X(0)|. The assumption A3 implies that there exists C′ > 0 such that for all t ∈ N≥0 and i ∈ [d], ∥(|W(0)|...|W(t)|)·i∥1 ≤ C′N . Hence there exists C′′ > 0 such that for all t ∈ N≥0 and i ∈ [d], we have ∥X(t) ·i ∥∞ ≤ C′′ , proving the existence of C >0 such that ∥X(t)∥max ≤ C for all t ∈ N≥0. D Proof of Lemma 2 Lemma 2 is a direct corollary of Lemma 1 and the assumption that Ψ(·, ·) assigns bounded attention scores to bounded inputs. 16E Proof of Lemma 3 E.1 Auxiliary results We make use of the following sufficient condition for the ergodicity of the infinite products of row-stochastic matrices. Lemma 8 (Corollary 5.1 [14]). Consider a sequence of row-stochastic matrices {S(t)}∞ t=0. Let at and bt be the smallest and largest entries in S(t), respectively. If P∞ t=0 at bt = ∞, then {S(t)}∞ t=0 is ergodic. In order to make use of the above result, we first show that long products of P(t)’s from PG,ϵ will eventually become strictly positive. For t0 ≤ t1, we denote P(t1:t0) = P(t1) . . . P(t0) . Lemma 9. Under the assumption A1, there exist T ∈ N and c >0 such that for all t0 ≥ 0, c ≤ P(t0+T:t0) ij ≤ 1 , ∀1 ≤ i, j≤ N . Proof. Fix any T ∈ N≥0. Since ∥P(t)∥∞ ≤ 1 for any P(t) ∈ PG,ϵ, it follows that ∥P(t0+T:t0)∥∞ ≤ 1 and hence P(t0+T:t0) ij ≤ 1, for all 1 ≤ i, j≤ N. To show the lower bound, without loss of generality, we will show that there existT ∈ N and c >0 such that P(T:0) ij ≥ c ,∀1 ≤ i, j≤ N . Since each P(t) has the same connectivity pattern as the original graph G, it follows from the assumption A1 that there exists T ∈ N such that P(T:0) is a positive matrix, following a similar argument as the one for Proposition 1.7 in [24]: For each pair of nodes i, j, since we assume that the graph G is connected, there exists r(i, j) such that P(r(i,j):0) ij > 0. on the other hand, since we also assume each node has a self-loop, P(t:0) ii > 0 for all t ≥ 0 and hence for t ≥ r(i, j), P(t:0) ij ≥ P(t−r(i,j)) ii P(r(i,j):0) ij > 0 . For t ≥ t(i) := max j∈G r(i, j), we have P(t:0) ij > 0 for all node j in G. Finally, if t ≥ T := max i∈G t(i), then P(t:0) ij > 0 for all pairs of nodes i, jin G. Notice that P(T:0) ij is a weighted sum of walks of length T between nodes i and j, and hence P(T:0) ij > 0 if and only if there exists a walk of length T between nodes i and j. Since for all t ∈ N≥0, P(t) ij ≥ ϵ if (i, j) ∈ E(G), we conclude that P(T:0) ij ≥ ϵT := c. E.2 Proof of Lemma 3 Given the sequence {P(t)}∞ t=0, we use T ∈ N from Lemma 9 and define ¯P(k) := P((k+1)T:kT ) . Then {P(t)}∞ t=0 is ergodic if and only if { ¯P(k)}∞ k=0 is ergodic. Notice that by Lemma 9, for all k ∈ N≥0, there exists c >0 such that c ≤ ¯P(k) ij ≤ 1 , ∀1 ≤ i, j≤ N. Then Lemma 3 is a direct consequence of Lemma 8. F Proof of Lemma 5 F.1 Notations and auxiliary results Consider a sequence {D(t)P(t)}∞ t=0 in MG,ϵ. For t0 ≤ t1, define Qt0,t1 := D(t1)P(t1)...D(t0)P(t0) 17and δt = ∥D(t) − IN ∥∞ , where IN denotes the N × N identity matrix. It is also useful to define ˆQt0,t1 :=P(t1)Qt0,t1−1 :=P(t1)D(t1−1)P(t1−1)...D(t0)P(t0). We start by proving the following key lemma, which states that long products of matrices inMG,ϵ eventually become a contraction in ∞-norm. Lemma 10. There exist 0 < c <1 and T ∈ N such that for all t0 ≤ t1, ∥ ˆQt0,t1+T ∥∞ ≤ (1 − cδt1 )∥ ˆQt0,t1 ∥∞ . Proof. First observe that for every T ≥ 0, ∥ ˆQt0,t1+T ∥∞ ≤ ∥P(t1+T)D(t1+T−1)P(t1+T−1)...D(t1+1)P(t1+1)D(t1)∥∞∥ ˆQt0,t1 ∥∞ ≤ ∥P(t1+T)P(t1+T−1)...P(t1+1)D(t1)∥∞∥ ˆQt0,t1 ∥∞ , where the second inequality is based on the following element-wise inequality: P(t1+T)D(t1+T−1)P(t1+T−1)...D(t1+1)P(t1+1) ≤ew P(t1+T)P(t1+T−1)...P(t1+1) . By Lemma 9, there exist T ∈ N and 0 < c <1 such that (P(t1+T)...P(t1+1))ij ≥ c, ∀1 ≤ i, j≤ N . Since the matrix product P(t1+T)P(t1+T−1)...P(t1+1) is row-stochastic, multiplying it with the diagonal matrix D(t1) from right decreases the row sums by at least c(1 − D(t1) min) = cδt1 , where D(t1) min here denotes the smallest diagonal entry of the diagonal matrix D(t1). Hence, ∥P(t1+T)P(t1+T−1)...P(t1+1)D(t1)∥∞ ≤ 1 − cδt1 . F.2 Proof of Lemma 4 Now define βk := Qk t=0(1 − cδt) and let β := lim k→∞ βk. Note that β is well-defined because the partial product is non-increasing and bounded from below. Then we present the following result, which is stated as Lemma 4 in the main paper and from which the ergodicity of any sequence in MG,ϵ is an immediate result. Lemma 4. Let βk := Qk t=0(1 − cδt) and β := lim k→∞ βk. 1. If β = 0, then lim k→∞ Q0,k = 0 ; 2. If β >0, then lim k→∞ BQ0,k = 0 . Proof. We will prove the two cases separately. [Case β = 0 ] We will show that β = 0 implies lim k→∞ ∥ ˆQ0,k∥∞ = 0 , and as a result, lim k→∞ ∥Q0,k∥∞ = 0. For 0 ≤ j ≤ T − 1, let us define βj := ∞Y k=0 (1 − δj+kT ) . Then by Lemma 10, we get that lim k→∞ ∥ ˆQ0,kT ∥∞ ≤ βj∥ ˆQ0,j∥∞ . By construction, β = ΠT−1 j=0 βj. Hence, if β = 0 then βj0 = 0 for some 0 ≤ j0 ≤ T − 1, which yields lim k→∞ ∥ ˆQ0,k∥∞ = 0. Consequently, lim k→∞ ∥Q0,k∥∞ = 0 implies that lim k→∞ Q0,k = 0. 18[Case β >0] First observe that if β >0, then ∀0 < η <1, there exist m ∈ N≥0 such that ∞Y t=m (1 − cδt) > 1 − η . (12) Using 1 − x ≤ e−x for all x ∈ R, we deduce ∞Y t=m e−cδt > 1 − η . It also follows from (12) that 1 − cδt > 1 − η, or equivalently δt < η c for t ≥ m. Choosing η <c 2 thus ensures that δt < 1 2 for t ≥ m. Putting this together with the fact that, there exists5 b >0 such that 1 − x ≥ e−bx for all x ∈ [0, 1 2 ], we obtain ∞Y t=m (1 − δt) ≥ ∞Y t=m e−bδt > (1 − η) b c := 1 − η′ . (13) Define the product of row-stochastic matrices P(M:m) := P(M) . . . P(m). It is easy to verify the following element-wise inequality:  MY t=m (1 − δt) ! P(M:m) ≤ew Qm,M ≤ew P(M:m) , which together with (13) leads to (1 − η′)P(M:m) ≤ew Qm,M ≤ew P(M:m) . (14) Therefore, ∥BQm,M ∥∞ = ∥B(Qm,M − P(M:m)) + BP (M:m)∥∞ ≤ ∥B(Qm,M − P(M:m))∥∞ + ∥BP (M:m)∥∞ = ∥B(Qm,M − P(M:m))∥∞ ≤ ∥B∥∞∥Qm,M − P(M:m)∥∞ ≤ η′∥B∥∞ ≤ η′√ N , where the last inequality is due to the fact that ∥B∥2 = 1. By definition, Q0,M = Qm,M Q0,m−1, and hence ∥BQ0,M ∥∞ ≤ ∥BQm,M ∥∞∥Q0,m−1∥∞ ≤ ∥BQm,M ∥∞ ≤ η′√ N . (15) The above inequality (15) holds when taking M → ∞. Then taking η → 0 implies η′ → 0 and together with (15), we conclude that lim M→∞ ∥BQ0,M ∥∞ = 0 , and therefore, lim M→∞ BQ0,M = 0 . F.3 Proof of Lemma 5 Notice that both cases β = 0 and β >0 in Lemma 4 imply the ergodicity of {D(t)P(t)}∞ t=0. Hence the statement is a direct corollary of Lemma 4. 5Choose, e.g., b = 2 log 2. 19G Proof of Lemma 6 In order to show that JSR( ˜PG,ϵ) < 1, we start by making the following observation. Lemma 11. A sequence {P(n)}∞ n=0 is ergodic if and only if Qt n=0 ˜P(n) converges to the zero matrix. Proof. For any t ∈ N≥0, it follows from the third property of the orthogonal projection B (see, Page 6 of the main paper) that B tY n=0 P(n) = tY n=0 ˜M(n)P . Hence {P(n)}∞ n=0 is ergodic ⇐⇒ lim t→∞ B tY n=0 P(n) = 0 ⇐⇒ lim t→∞ tY n=0 ˜P(n)B = 0 ⇐⇒ lim t→∞ tY n=0 ˜P(n) = 0 . Next, we utilize the following result, as a means to ensure a joint spectral radius strictly less than 1 for a bounded set of matrices. Lemma 12 (Proposition 3.2 in [36]). For any bounded set of matrices M, JSR(M) < 1 if and only if for any sequence {M(n)}∞ n=0 in M, Qt n=0 M(n) converges to the zero matrix. Here, “bounded\" means that there exists an upper bound on the norms of the matrices in the set. Note that PG,ϵ is bounded because ∥P∥∞ = 1, for all P ∈ MG,ϵ. To show that ˜PG,ϵ is also bounded, let ˜P ∈ ˜PG,ϵ, then by definition, we have ˜P B= BP, P∈ MG,ϵ ⇒ ˜P = BP BT , since BBT = IN−1. As a result, ∥ ˜P∥2 = ∥BP BT ∥2 ≤ ∥P∥2 ≤ √ N , where the first inequality is due to ∥B∥2 = ∥B⊤∥2 = 1, and the second ineuality follows from ∥P∥∞ = 1. Combining Lemma 5, Lemma 11 and Lemma 12, we conclude that JSR( ˜PG,ϵ) < 1. H Proof of Lemma 7 Note that any sequence {M(n)}∞ n=0 in MG,ϵ,δ satisfies β = 0, where β is defined in Lemma 4. This implies that for any sequence {M(n)}∞ n=0 in MG,ϵ,δ, we have lim t→∞ tY n=0 M(n) = 0 . Since ∥M∥∞ ≤ δ for all M ∈ MG,ϵ,δ, again by Lemma 12, we conclude that JSR(MG,ϵ,δ) < 1. I Proof of Theorem 1 To derive the exponential convergence rate, consider the linear and nonlinear cases separately: 20I.1 Bounds for the two cases [Case: linear] In the linear case where all D = IN , it follows from Lemma 6 that ∥BX(t+1) ·i ∥2 = \r\r\r\r\r\r X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 W(k) jkjk+1 ! BP (t)...P(0)X(0) ·j0 \r\r\r\r\r\r 2 ≤ X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rBP (t)...P(0)X(0) ·j0 \r\r\r 2 = X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\r ˜P(t)... ˜P(0)BX(0) ·j0 \r\r\r 2 ≤ X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f ! Cqt+1 \r\r\rBX(0) ·j0 \r\r\r 2 ≤ C′qt+1   X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !  = C′qt+1∥(|W(0)|...|W(t)|)·i∥1 , (16) where C′ = Cmax j∈[d] ∥BX(0) ·j ∥2 and ∥ · ∥1 denotes the 1-norm. Specifically, the first inequality follows from the triangle inequality, and the second inequality is due to the property of the joint spectral radius in (8), where JSR( ˜PG,ϵ) < q <1. [Case: nonlinear] Consider T ∈ N defined in Lemma 10, where there exists a1 ∈ N≥0 and a2 ∈ {0} ∪[K − 1] such that T = a1K + a2. Given the condition (⋆), and Lemma 10, there exists 0 < c <1 such that for all m ∈ N≥0, ∥ ˆQmK+nm,mK+nm+T ∥∞ ≤ (1 − cδ′) , where δ′ = 1 − δ. Note that since for all nm, m∈ N≥0, a2 + nm ≤ 2K, we get that nm + T ≤ (a1 + 2)K . Since for all m ∈ N≥0, ∥ ˆQmK+nm:(m+a1+2)K+nm+a1+2 ∥∞ ≤ (1 − cδ′) , it implies that for all n ∈ N≥0, ∥Q0:n(a1+2)K∥∞ ≤ (1 − cδ′)n = \u0010 (1 − cδ′) n n(a1+2)K \u0011n(a1+2)K = \u0010 (1 − cδ′) 1 (a1+2)K \u0011n(a1+2)K . Denote q := (1 − cδ′) 1 (a1+2)K . By the equivalence of norms, we get that for all n ∈ N≥0, ∥Q0:n(a1+2)K∥2 ≤ √ Nqn(a1+2)K . Then for any j ∈ {0} ∪[(a1 + 2)K − 1], ∥Q0:n(a1+2)K+j∥2 ≤ √ Nq−jqn(a1+2)K+j ≤ Cqn(a1+2)K+j where C := √ Nq−(a1+2)K+1. Rewriting the indices, we conclude that ∥Q0:t∥2 ≤ Cqt , ∀t ≥ 0 . The above bound implies that for any q satisfies q ≤ q <1, 21∥BX(t+1) ·i ∥2 = \r\r\r\r\r\r X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 W(k) jkjk+1 ! BD(t) jt+1 P(t)...D(0) j1 P(0)X(0) ·j0 \r\r\r\r\r\r 2 ≤ X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rBD(t) jt+1 P(t)...D(0) j1 P(0)X(0) ·j0 \r\r\r 2 ≤ X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rD(t) jt+1 P(t)...D(0) j1 P(0) \r\r\r \r\r\rX(0) ·j0 \r\r\r 2 ≤ C′qt+1   X jt+1=i, (jt,...,j0)∈[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !  = C′qt+1∥(|W(0)|...|W(t)|)·i∥1 , (17) where again, C′ = Cmax j∈[d] ∥BX(0) ·j ∥2. I.2 Proof of the exponential convergence Based on the inequality extablished for both linear and nonlinear case in (16) and (17), we derive the rest of the proof. Since ∥Bx∥2 = ∥x∥2 if x⊤1 = 0 for x ∈ RN , we also have that if X⊤1 = 0 for X ∈ RN×d, then ∥BX∥F = ∥X∥F , using which we obtain that µ(X(t+1)) = ∥X(t+1) − 1γX(t+1) ∥F = ∥BX(t+1)∥F = vuut dX i=1 ∥BX(t+1) ·i ∥2 2 ≤ C′qt+1 vuut dX i=1 ∥(|W(0)|...|W(t)|)·i∥2 1 ≤ C′qt+1 vuut  dX i=1 ∥|(W(0)|...|W(t)|)·i∥1 !2 = C′qt+1∥|(W(0)|...|W(t)|∥1,1 , where ∥ · ∥1,1 denotes the matrix (1, 1)-norm (recall from Section A.2 that for a matrix M ∈ Rm×n, we have ∥M∥1,1 = Pm i=1 Pn j=1 |Mij|). The assumption A3 implies that there exists C′′ such that for all t ∈ N≥0, ∥(|W(0)|...|W(t)|)∥1,1 ≤ C′′d2 . Thus we conclude that there exists C1 such that for all t ∈ N≥0, µ(X(t)) ≤ C1qt . Remark 5. Similar to the linear case, one can also use Lemma 7 to establish exponential rate for oversmoothing when dealing with nonlinearities for which Assumption 4 holds in the strict sense, i.e. 0 ≤ σ(x) x < 1 (e.g., GELU and SiLU nonlinearities). Here, we presented an alternative proof requiring weaker conditions, making the result directly applicable to nonlinearities such as ReLU and Leaky ReLU. 22J Proof of Theorem 2 Note that (W(t))⊤ ⊗ P(t) =   W(t) 11 P(t) W(t) 21 P(t) ... W (t) d1 P(t) W(t) 12 P(t) W(t) 22 P(t) ... W (t) d2 P(t) ... ... ... ... W(t) 1d P(t) W(t) 2d P(t) ... W (t) dd P(t)   ∈ RNd×Nd ≥0 is a row substochastic matrix such that for all t ≥ 0, the nonzero entries of (W(t))⊤ ⊗ P(t) are uniformly bounded below by ξϵ. Moreover, We can interpret the sparsity pattern of this matrix as connectivities of a meta graphGm with Nd nodes, where the node (f −1)N + n represents the feature f of node n in the original graph G. We denote such a node as (n, f), where n ∈ [N], f∈ [d]. Then: • When only W(t) ii > 0 for all i ∈ [d], together with A1, it ensures that for a fixed feature i, all the nodes (n, i), n∈ [N] fall into the same connected component of Gm, which has the same connectivity pattern as the original graphG. As a result, the connected component is non-bipartite. • If in addition, W(t) ij , W(t) ji > 0 for i ̸= j ∈ [d], the two connected components representing the features i and j will merge into one connected component, which is subsequently non-bipartite as well. Then we apply Lemma 5 to each connected component separately and conclude the statement of the theorem. K Proof of Proposition 2 Since D−1 degA is similar to D−1/2 deg AD−1/2 deg , they have the same spectrum. For D−1 degA, the smallest nonzero entry has value 1/dmax, where dmax is the maximum node degree in G. On the other hand, it follows from the definition of PG,ϵ that ϵdmax ≤ 1 . Therefore, ϵ ≤ 1/dmax and thus D−1 degA ∈ PG,ϵ. We proceed by proving the following result. Lemma 13. For any M in M, the spectral radius of M denoted by ρ(M), satisfies ρ(M) ≤ JSR(M) . Proof. Gelfand’s formula states that ρ(M) = lim k→∞ ∥Mk∥ 1 k , where the quantity is independent of the norm used [23]. Then comparing with the definition of the joint spectral radius, we can immediately conclude the statement. Let B(D−1 degA) = ˜P B. By definition, ˜P ∈ ˜PG,ϵ since D−1 degA ∈ PG,ϵ as shown before the lemma. Moreover, the spectrum of ˜P is the spectrum of D−1 degA after reducing the multiplicity of eigenvalue 1 by one. Under the assumption A1, the eigenvalue1 of D−1 degA has multiplicity 1, and hence ρ( ˜P) = λ, where λ is the second largest eigenvalue ofD−1 degA. Putting this together with Lemma 13, we conclude that λ ≤ JSR( ˜PG,ϵ) as desired. L Numerical Experiments Here we provide more details on the numerical experiments presented in Section 5. All models were implemented with PyTorch [28] and PyTorch Geometric [10]. 23Datasets • We used torch_geometric.datasets.planetoid provided in PyTorch Geometric for the three homophilic datasets: Cora, CiteSeer, and PubMed with their default training and test splits. • We used torch_geometric.datasets.WebKB provided in PyTorch Geometric for the three heterophilic datasets: Cornell, Texas, and Wisconsin with their default training and test splits. • Dataset summary statistics are presented in Table 1. Dataset Type #Nodes %Nodes in LCC Cora 2,708 91.8 % CiteSeer homophilic 3,327 63.7 % PubMed 19,717 100 % Cornell 183 100 % Texas heterophilic 183 100 % Wisconsin 251 100 % Flickr large-scale 89,250 100 % Table 1: Dataset summary statistics. LCC: Largest connected component. Model details • For GAT, we consider the architecture proposed in Veliˇckovi´c et al. [39] with each attentional layer sharing the parameter a in LeakyReLU(a⊤[W⊤Xi||W⊤Xj]), a∈ R2d′ to compute the attention scores. • For GCN, we consider the standard random walk graph convolution D−1 degA. That is, the update rule of each graph convolutional layer can be written as X′ = D−1 degAXW , where X and X′ are the input and output node representations, respectively, and W is the shared learnable weight matrix in the layer. Compute We trained all of our models on a Tesla V100 GPU. Training details In all experiments, we used the Adam optimizer using a learning rate of 0.00001 and 0.0005 weight decay and trained for 1000 epoch Results on large-scale dataset In addition to the numerical results presented in Sec- tion 5, we also conducted the same experiment on a large-scale dataset Flickr [ 47] using torch_geometric.datasets.Flickr. Figure 2 visualizes the results. 100 101 102 number of layers 10 2 10 1 100 101 102 103 104 105 (X) Flickr GAT 100 101 102 number of layers 10 2 10 1 100 101 102 103 104 105 Flickr GCN ReLU LeakyReLU (0.01) LeakyReLU (0.4) LeakyReLU (0.8) GELU Figure 2: Evolution of µ(X(t)) (in log-log scale) on the largest connected component of the large- scale benchmark dataset: Flickr. 24",
      "meta_data": {
        "arxiv_id": "2305.16102v4",
        "authors": [
          "Xinyi Wu",
          "Amir Ajorlou",
          "Zihui Wu",
          "Ali Jadbabaie"
        ],
        "published_date": "2023-05-25T14:31:59Z",
        "pdf_url": "https://arxiv.org/pdf/2305.16102v4.pdf"
      }
    },
    {
      "title": "Graph Neural Networks Do Not Always Oversmooth",
      "abstract": "Graph neural networks (GNNs) have emerged as powerful tools for processing\nrelational data in applications. However, GNNs suffer from the problem of\noversmoothing, the property that the features of all nodes exponentially\nconverge to the same vector over layers, prohibiting the design of deep GNNs.\nIn this work we study oversmoothing in graph convolutional networks (GCNs) by\nusing their Gaussian process (GP) equivalence in the limit of infinitely many\nhidden features. By generalizing methods from conventional deep neural networks\n(DNNs), we can describe the distribution of features at the output layer of\ndeep GCNs in terms of a GP: as expected, we find that typical parameter choices\nfrom the literature lead to oversmoothing. The theory, however, allows us to\nidentify a new, non-oversmoothing phase: if the initial weights of the network\nhave sufficiently large variance, GCNs do not oversmooth, and node features\nremain informative even at large depth. We demonstrate the validity of this\nprediction in finite-size GCNs by training a linear classifier on their output.\nMoreover, using the linearization of the GCN GP, we generalize the concept of\npropagation depth of information from DNNs to GCNs. This propagation depth\ndiverges at the transition between the oversmoothing and non-oversmoothing\nphase. We test the predictions of our approach and find good agreement with\nfinite-size GCNs. Initializing GCNs near the transition to the\nnon-oversmoothing phase, we obtain networks which are both deep and expressive.",
      "full_text": "arXiv:2406.02269v2  [stat.ML]  15 Nov 2024 Graph Neural Networks Do Not Always Oversmooth Bastian Epping1, Alexandre René1, Moritz Helias2,3, Michael T . Schaub1 1R WTH Aachen University, Aachen, Germany 2Department of Physics, R WTH Aachen University, Aachen, Ger many 3Institute for Advanced Simulation (IAS-6), Computational and Systems Neuroscience, Jülich Research Centre, Jülich, Germany epping@cs.rwth-aachen.de, rene@cs.rwth-aachen.de, m.helias@fz-juelich.de, schaub@cs.rwth-aachen.de Abstract Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications. However, GNNs suffer from the problem of over- smoothing, the property that features of all nodes exponent ially converge to the same vector over layers, prohibiting the design of deep GNNs . In this work we study oversmoothing in graph convolutional networks (GC Ns) by using their Gaussian process (GP) equivalence in the limit of inﬁnitely many hidden features. By generalizing methods from conventional deep neural netw orks (DNNs), we can describe the distribution of features at the output laye r of deep GCNs in terms of a GP: as expected, we ﬁnd that typical parameter choices fr om the literature lead to oversmoothing. The theory, however, allows us to ide ntify a new , non- oversmoothing phase: if the initial weights of the network h ave sufﬁciently large variance, GCNs do not oversmooth, and node features remain informative even at large depth. W e demonstrate the validity of this predicti on in ﬁnite-size GCNs by training a linear classiﬁer on their output. Moreover, us ing the linearization of the GCN GP , we generalize the concept of propagation depth of information from DNNs to GCNs. This propagation depth diverges at the tra nsition between the oversmoothing and non-oversmoothing phase. W e test the predictions of our approach and ﬁnd good agreement with ﬁnite-size GCNs. Initi alizing GCNs near the transition to the non-oversmoothing phase, we obtain ne tworks which are both deep and expressive. 1 Introduction Graph neural networks (GNNs) reach state of the art performance in diverse application domains with relational data that can be represented on a graph, tran sferring the success of machine learning to data on graphs [47, 12, 23, 7]. Despite their good performa nce, GNNs come with the limitation of oversmoothing, a phenomenon where node features converge to the same state exponentially fast for increasing depth [35, 45, 30, 2]. Consequently, only shallo w networks are used in practice [19, 1]. In contrast, it is known that the depth (i.e. the number of lay ers) is key to the success of deep neu- ral networks (DNNs) [32, 33]. While for conventional DNNs sh allow networks are proven to be highly expressive [6], in practice deep networks are much ea sier to train and are thus the commonly used architectures [36]. Furthermore, in most GNN architec tures each layer only exchanges infor- mation between neighboring nodes. Deep GNNs are therefore n ecessary to exchange information between nodes that are far apart in the graph [9]. In this stud y, we investigate oversmoothing in graph convolutional networks (GCNs) [19]. T o study the effect of depth, we consider the propagation of f eatures through the network: given some input x(0) α, each intermediate layer l produces features x(l) α which are fed to the next layer. W e 38th Conference on Neural Information Processing Systems ( NeurIPS 2024).follow the same approach that has successfully been employe d in previous work to design trainable DNNs [37]: consider two nearly identical inputs x(0) αand x(0) βand ask whether the intermediate features x(l) α and x(l) β become more or less similar as a function of depth l. In the former case, the inputs may eventually become indistinguishable. In the lat ter case, the inputs become less similar over layers: the distance between them increases over layer s [32, 37] until eventually it is bounded by the non-linearities of the network. The distance then typic ally converges to a ﬁxed value determined by the network architecture, independent of the inputs. One can therefore identify two phases: One says that a network is regular if two inputs eventually converge to the same value as function of l; conversely, one says that a network is chaotic if two inputs remain distinct for all depths [24]. Neither phase is ideal for training deep networks since in both cases all the information from the inputs is eventual ly lost; the typical depth at which this happens is called the information propagation depth . However, this propagation depth diverges at the transition between the two phases, allowing informatio n – in principle – to propagate inﬁnitely deep into the network. While these results are calculated in the limit of inﬁnitely many features in each hidden layer, the information propagation depth has be en found to be a good indicator of how deep a network can be trained [37]. A usual approach for conve ntional DNNs is thus to initialize them at the transition to chaos. Indeed, Schoenholz et al. [3 7] were able to use this approach to train fully-connected, feedforward networks with hundreds of la yers. Similar methods have recently been adapted to the study of transformers, successfully predict ing the best hyperparameters for training [5]. In this work we address the oversmoothing problem of GCNs by e xtending the framework described above in the limit of inﬁnite feature dimensions from DNNs to GCNs: here the two different inputs x(0) αand x(0) βcorrespond to the input features on two nodes, labeled α and β. The mixing of information across different nodes implies that output fea tures on node α depend on input features on node β and vice versa. Thus it is not possible to look at the distance of x(l) α and x(l) β independently for each pair α and β as in the DNN case. Rather, one has to solve for the distance be tween each distinct pair of nodes in the graph simultaneously: the one d imensional problem for DNNs thus becomes a multidimensional problem for GCNs. However, by li nearizing the multidimensional GCN dynamics, we can generalize the notion of information pr opagation depth to GCNs: instead of being a single value, we ﬁnd that a given GCN architecture c omes with a set of potentially different information propagation depths, each correspon ding to one eigendirection of the linearized dynamics of the system. This approach allows us to extend the concept of a regular and a chaotic phase to GCNs: in the regular phase, which describes most of the GCNs studied in th e current literature, distances between node features shrink over layers and exponentially attain t he same value. W e therefore call this the oversmoothing phase. On the other hand, if one increases the variance of the weights at initialization, it is possible to transition into the chaotic phase. In this p hase, distances between node features converge to a ﬁxed but ﬁnite distance at inﬁnite depth. The co nvergence point is fully determined by the underlying graph structure and the hyperparameters o f the GCN and may differ for different pairs of nodes. GCNs initialized in this phase thus do not suf fer from oversmoothing. W e ﬁnd that the convergence point is informative about the topology of t he underlying graph and may be used for node classiﬁcation with GCNs of more than 1, 000 layers. Near the transition point, GCNs at large depth offer a trade-off between feature information and inf ormation contained in the neighborhood relation of the graph. W e test the predictions of this theory and ﬁnd good agreement in comparison to ﬁnite-size GCNs applied to the contextual stochastic blo ck model [8]. On the citation network Cora [19] we reach the performance reported in the original w ork by Kipf and W elling [19] beyond 100 layers. Our approach applies to graphs with arbitrary topol ogies, depths, and non-linearities. 2 Related W ork Oversmoothing is a well-known challenge within the GNN literature [21, 35]. On the theoretical side, rigorous techniques have been used to prove that overs moothing is inevitable. The authors in [30] show that GCNs with the ReLU non-linearity exponential ly lose expressive power and only carry information of the node degree in the inﬁnite layer lim it. While the authors notice that their upper bound for oversmoothing does not hold for large singul ar values of the weight matrices, they 2do not identify a non-oversmoothing phase in their model. Th ese results have been extended in [2] to handle non-linearities different from ReLU. Also graph a ttention networks have been proven to oversmooth inevitably [45]. Their proof, however, makes as sumptions on the weight matrices which, as we will show , exclude networks in the chaotic, non-oversm oothing phase. On the applied side, a variety of heuristics have been develo ped to mitigate oversmoothing [49, 4, 3, 22, 40, 15]. E.g., the authors in [49] introduce a normalizat ion layer which can be added to a variety of deep GNNs to make them trainable. Another approach is to in troduce residual connections and identity mappings, directly feeding the input to layers dee p in the network [4, 46]. Other studies suggest to train GNNs to a limited number of layers to obtain t he optimal amount of smoothing [18, 46]. The recent review [17] proposes a uniﬁed view to ord er existing heuristics and guide further research. While these heuristics improve performa nce at large depths, they also add to the complexity of the model and impose design choices. Our appro ach, on the other hand, explains why increasing the weight variance at initialization is sufﬁci ent to prevent oversmoothing. 3 Background 3.1 Network architecture In this paper we study a standard graph convolutional network (GCN) architecture [19] with an input feature matrix X(0) ∈ RN×d0 , where N is the number of nodes in the graph and d0 the number of input features. Bold symbols throughout represent vector o r matrix quantities in feature space. The structure of the graph is represented by a shift operator A ∈ RN×N . W e write the features of the network’s l-th layer as X(l) ∈ RN×dl ; they are computed recursively as X(l) = φ(AX(l−1)W (l)⊤ + 1b(l)⊤) , (1) with φ an elementwise non-linear activation function, b(l) an optional bias term, weight matrices W (l) ∈ Rdl×dl−1 , and 1 ∈ RN a vector of all ones. W e note that many GNN architectures stud ied in the literature are unbiased, which can be recovered by set ting b(l) to zero. W e use a noisy linear readout, so that the output of the network is given by Y = AX(L)W (L+1)⊤ + 1b(L+1)⊤ + ǫ , (2) with ǫ ∈ RN×dL+1 being independent Gaussian random variables: ǫα,i i.i.d. ∼ N (0, σ2 ro). The readout noise ǫ is included both to promote robust outputs and to prevent num erical issues in the matrix inversion in Equations (11) and (12). W e use dL+1 to denote the dimension of outputs. For the following it will be useful to consider the activity o f individual nodes. T o avoid ambiguity in the indexing, we use lower Greek indices for nodes and uppe r Latin indices for layers. W e thus rewrite (1) as x(l) α = φ(h(l) α ) , (3) h(l) α = ∑ β Aαβ W (l)x(l−1) β + b(l) , (4) yα = h(L+1) α + ǫα , (5) where x(l) α ∈ Rdl is the feature vector of node α in layer l and yα ∈ RdL+1 the network output for node α. The values h(l) α ∈ Rdl are linear functions of the features x(l−1) β and represent the input to the activation functions; we therefore refer to them as pr eactivations. The non-linearity φ(x) is applied elementwise to the preactivations h(l) α . While we leave φ(x) general for the development of the theory, we use φ(x) = erf( √ π 2 x) for the experiments in Section 4; this choice allows us to car ry out certain integrals analytically. The scaling factor in t he erf is chosen such that ∂φ ∂x (0) = 1 . W e use independent and identical Gaussian priors for all weight ma trices and biases, W (l) ij i.i.d. ∼ N (0, σ2 w dl ) and b(l) i i.i.d. ∼ N (0, σ2 b ) with W (l) ij and b(l) i being the matrix or vector entries of W (l) and b(l), respectively. As a shift operator, we choose A = I − g dmax (D − A) , (6) 3where A is the adjacency matrix, I the identity in RN×N , Dαβ = δαβ ∑ γ Aαγ is the degree matrix and dmax is the maximal degree. The parameter g ∈ (0, 1) allows us to weigh the off-diagonal elements compared to the diagonal ones. By construction the shift operator is row-stochastic, which means that it has constant sums over columns ∑ β Aαβ = 1 . W e will make use of this property in our analysis in Section 4.2. The generalization to non-st ochastic shift operators will be shortly addressed later. 3.2 Gaussian process equivalence of GCNs In a classic machine learning setting, such as classiﬁcation, one draws random initial values for all parameters and subsequently trains the parameters by optim izing the weights and biases to minimize a loss function. This learned parameter set is then used to cl assify unlabeled inputs. In this paper we take a Bayesian point of view in which the network paramete rs are random variables, inducing a probability distribution over outputs which becomes Gauss ian in the limit of inﬁnitely many features. Thus inﬁnitely wide neural networks are equivalent to Gauss ian processes (GPs) [27, 43, 34]. In the study of DNNs this is a standard approach, yielding results w hich empirically hold also for ﬁnite-size networks trained with gradient descent [37]. In previous work [28, 14, 29] it has been shown that also the GCN architecture described in Sec- tion 3.1 is equivalent to a GP in the limit of inﬁnite feature s pace dimensions, dl → ∞ for all hidden layers l = 1 , . . . , L , while input and readout layer still have tunable, ﬁnitely m any features. In the GP description, all features are Gaussian random variables with zero mean and identical prior vari- ance in each feature dimension. The description of the GCN th us reduces to a multivariate normal, H(l) ∼ N (0, K(l)) , (7) where H(l) is the vector of hidden node features of layer l, H(l) = ( h(l) 0 , h(l) 1 , . . . , h (l) N )⊤ under the prior distribution of weights and biases. The covariance ma trices K(l) ∈ RN×N are determined recursively: knowing that the h(l) α follow a zero-mean Gaussian with covariance ⟨h(l) δ h(l) γ ⟩ = K(l) δγ , we deﬁne C(l) γδ = ⣨ φ ( h(l) γ ) φ ( h(l) δ )⟩ h(l) γ ,h(l) δ . (8) For simplicity we use φ(x) = erf( √ π 2 x) for which Equation (8) can be evaluated analytically; see Appendix A for details. It follows from (3) that K(l+1) αβ = σ2 b + σ2 w ∑ γ,δ Aαγ Aβδ C(l) γδ , (9) as shown in [28, 29]. In a semi-supervised node classiﬁcation setting, we split the underlying graph into Ntest unlabeled test nodes and Ntrain labeled training nodes ( Ntest + Ntrain = N); we correspondingly split the output random variable Yi ∈ RN for output dimension i into Y ⋆ i ∈ RNtest and Y D i ∈ RNtrain . Features on the test nodes are predicted by conditioning on t he values of the training nodes: p ( Y ∗ i = y∗ i | Y D i = yD i ) . This leads to the following posterior for the unobserved lab els (see [34, 20] for details): Y ⋆ i ∼ N (mGP i, KGP) , (10) mGP i= K(L+1) ⋆D (K(L+1) DD + Iσ2 ro)−1Y D i , (11) KGP = K(L+1) ⋆⋆ − K(L+1) ⋆D (K(L+1) DD + Iσ2 ro )−1(K(L+1) ⋆D )⊤ . (12) Here the ⋆ and D indices represent test and training data, respectively, i. e. KDD ∈ RNtrain×Ntrain is the covariance matrix of outputs of all training nodes and K⋆D ∈ RNtest×Ntrain is the covariance between test data and training data. Finally, I is here the identity in RNtrain×Ntrain . 3.3 Feature distance T o measure and quantify how much a given GCN instance oversmo othes we use the squared Eu- clidean distance between pairs of nodes, and normalize by th e number of node features dl so that the 4measure stays ﬁnite in the GP limit dl → ∞ . This allows us to quantitatively test the predictions of our approach on the node-resolved distances of features. T o summarize the amount of oversmooth- ing across the GCN, we also deﬁne the measure µ(X) as the average squared Euclidean distance across all pairs of nodes: d(xα, xβ ) = 1 dl ||xα − xβ ||2 2= C′ αα + C′ ββ − 2C′ αβ , (13) µ(X) = 1 2N(N − 1) N∑ α=1 N∑ β=α+1 d(xα, xβ ) . (14) Here C′ αβ = xα ·xβ dl is the normalized scalar product. W e use the notation C′ αβ to avoid confusion with the expectation value Cαβ deﬁned in the GCN GP (8). In the inﬁnite feature dimensions li mit, the quantities C′ αβ in Equation (14) converge to the GCN GP quantities Cαβ deﬁned by (8). In the following sections we will therefore use the Cαβ as predictions for the C′ αβ of ﬁnite-size GCNs. The normalization for d(xα, xβ ) and µ(X) can be interpreted as an average (squared) feature distance , independent of the size of the graph and the number of feature dimensions. 4 Results 4.1 Propagation depths W e are interested in analyzing GCNs at large depth. W e a prior i assume that at inﬁnite depth the GCN converges to an equilibrium in which covariances are sta tic over layers K(l) αβ l→∞ − − − →Keq αβ , irrespective of whether the GCN is in the oversmoothing or th e chaotic phase. A posteriori we show that this assumption indeed holds. Since the ﬁxed point Keq is independent of the input, a GCN at equilibrium cannot use information from the input to make predictions (although, as we will see, in the non-oversmoothing phase it can still use the graph str ucture). In the following we analyze the equilibrium covariance Keq to which GCNs with different σ2 w, σ2 b and A converge to, how they behave near this equilibrium, and at which rate it is approac hed. Close to equilibrium, the covariance matrix K(l) can be written as a perturbation around Keq αβ: K(l) αβ = Keq αβ + ∆ (l) αβ . (15) Under the assumption that the perturbation ∆ (l) αβ is small, we can linearize the GCN GP ∆ (l+1) αβ = ∑ γ,δ Hαβ,γδ ∆ (l) γδ + O((∆ (l))2) , (16) Hαβ,γδ = σ2 w ∑ θ,φ 1 2 (1 + δγ,δ )AαθAβφ ∂Cθφ ∂Kγδ [Keq] , (17) where we use square brackets to denote the point around which we linearize. The factor 1 2 (1 + δγ,δ ) is introduced to correctly count on- and off-diagonal eleme nts of the covariance matrix, while the shift operators A and the derivative ∂Cθφ ∂Kγδ [Keq] originate from the message passing and the non- linearity φ, respectively. The latter would result in a Kronecker delta ∂Cθφ ∂Kγδ [Keq] = δθφ,γδ for linear networks. The calculation for Hαβ,γδ is done in detail in Appendix B. A conceptually similar linearization has been done in [37] f or DNNs. In the DNN case, different inputs to the networks—which correspond to input features o n different nodes here—can be treated separately, leading to decoupling of Equation (16). The shi ft operator in the GCN dynamics, in contrast, couples features on neighboring nodes – the matri x Hαβ,γδ is in general not diagonal. W e can still achieve a decoupling by interpreting Equation ( 16) as a matrix multiplication, if αβ and γδ are understood as double indices, and by ﬁnding the eigendir ections of the matrix H ∈ RN2×N2 . T aking the right eigenvectors V (i) αβ as basis vectors, we can decompose the covariance matrix ∆ (l) αβ = ∑ i ∆ (l) i V (i) αβ and thus obtain the overlaps ∆ (l) i which evolve independently over layers. If the ﬁxed 5point Keq is attractive, all eigenvalues have absolute values smalle r than one: |λi| < 1. This allows us to deﬁne the propagation depth ξi := − 1 ln(λi) for each eigendirection, very similar to the DNN case [37]. In this form, the linear update equation (16) simp liﬁes to ∆ (l+d) i = λd i∆ (l) i = exp( −d/ξi)∆ (l) i , (18) thus decoupling the system. For details on the linearizatio n and some properties of the transition matrix H refer to Appendix B. 4.2 The non-oversmoothing phase of GCNs In this section we establish the chaotic, non-oversmoothing phase of GCNs, and show that this phase can be reached by simple tuning of the weight variance σ2 w at initialization. W e start by noticing that a GCN is at a state of zero feature distance µ(X(l)) = 0 , if the covariance matrix has constant entries, K(l) αβ = k(l): Constant entries in K(l) αβ imply that all preactivations are the same, h(l) α = h(l) β , which in turn implies C(l) αβ = c(l) (by Equation (8)); the latter is equivalent to features bein g the same, x(l) α = x(l) β . Due to our choice of the shift operator, the state of zero dis tance (and thus of K(l) αβ = k(l)) is always a ﬁxed point. Assuming that C(l) αβ = c(l), we obtain K(l+1) αβ = σ2 b + σ2 w ∑ γ Aαγ    =1 ∑ δ Aβδ    =1 c(l) = k(l+1) . (19) In an overmoothing GCN, this ﬁxed point is also attractive, m eaning that also pairs of feature inputs x(0) α, x(0) βwhich initially have non-zero distance d(x(0) α, x(0) β) ̸= 0 (and thus µ(X(0)) ̸= 0 ) eventu- ally converge to the point of vanishing distance. The chaoti c, non-oversmoothing phase of a GCN is determined by the condition that this point of constant co variance K(l) αβ = k(l) becomes unstable. More formally, this can be written in terms of eigenvalues of the linearized dynamics as max{|λp i|} ? > 1 . (20) Here and in the following we will use the superscript p to denote that the linearization is done around the state of constant covariance across nodes in both the ove rsmoothing and non-oversmoothing phase. The propagation depth ξi := − 1 ln(λi) diverges at the phase transition where one λi ap- proaches 1. Intuitively speaking, Equation (20) asks whether a small p erturbation from the zero distance case diminishes ( max{|λp i|} < 1), in which case the network dynamics is regular, or grows (max{|λp i|} > 1), in which case the network is chaotic and thus does not overs mooth. The value of max{|λp i|} depends on the choices of A, σ2 w and σ2 b (by the dependence of Keq on σ2 b ). In the following we will concentrate on tuning σ2 w to reach the non-oversmoothing phase. 4.2.1 Complete graph T o illustrate the implications of the analysis described ab ove, we ﬁrst consider a particularly sim- ple GCN on a complete graph; this allows us to calculate the co ndition for the transition to chaos analytically, and gain some insight into the interesting pa rameter regimes. Moreover, we use this pedagogical example to show that although the GP equivalenc e is only true in the limit of inﬁnite hidden feature dimensions, dl → ∞ , our results still describe ﬁnite-size GCNs well. For a complete graph with adjacency matrix Aαβ = 1 − δαβ, our choice of shift operator A in (6) has entries Aαβ = g N−1 + δαβ (1 − Ng N−1 ). This model is a worst-case scenario for oversmoothing, since the adjacency matrix leads to inputs that are shared ac ross all nodes of the network. W e make the ansatz that the equilibrium covariance is of the form Keq αβ = Keq c + δαβ (Keq a − Keq c ) due to symmetry which reduces the problem to only two variables. In this formulation we can use similar methods as in the DNN case [37] to determine the non-oversmoo thing condition on the l.h.s in (20) (Details are given in Appendix C). Figure 1 shows how a GCN on a complete graph can be engineered t o be non-oversmoothing by simple tuning of the weight variance σ2 w. Panel a) shows that increasing the weight variance 6Figure 1: Simulations and GP prior of a GCN on a complete graph with N = 5 nodes, shift operator Aαβ = g N−1 + δαβ(1 − Ng N−1 ), vanishing bias σ2 b = 0 and φ(x) = erf( √π 2 x). a) The phase diagram dependent on σ2 w and g. The equilibrium feature distance µ(X) obtained from computing the GCN GP prior for L = 4 , 000 layers is shown as a heatmap, the red line is the theoretical p rediction for the transition to the non-oversmoothing phase. b) Same as in a) but color coding shows whether µ(X) is close to zero (black) or not (white) with precision 10−5. The red line again shows the theoretically predicted phase transition. c) Feature distance µ(X(l)) for a random input X(0) αi i.i.d. ∼ N (0, 1) as a function of layer l. Parameters are written in the panel in matching colors and m arked with color coded crosses in the phase diagram in panel b). Feature dimen sion of the hidden layers is dl = 200 , crosses show the mean of 50 network realizations, solid curves the theoretical predic tions. σ2 w or decreasing the size of the off-diagonal elements g both shift the network towards the non- oversmoothing phase. Both parameters also increase the equ ilibrium feature distance beyond the transition. The theoretical prediction for the transition is calculated in Appendix C and shown as the red line. Panel b) conﬁrms the accuracy of this calculation. Larger values of g increase smoothing, and thus larger values of σ2 w are needed to compensate. Moreover, our formalism allows us to pre- dict the evolution of feature distances over layers correct ly, as can be conﬁrmed in panel c). W e ﬁnd again that GCNs with parameters past the transition do not ov ersmooth. 4.2.2 General graphs For general graphs, the transition to the non-oversmoothing phase given by Equation (20) can be determined numerically. As a proof of concept, we demonstra te this approach for the Contextual Stochastic Block Model (CSBM) [8], a common synthetic model which allows generating a graph with two communities and community-wise correlated featur es on the nodes. Pairs of nodes within the same community have higher probability of being connect ed and have feature vectors which are more strongly correlated, compared with pairs of nodes from different communities. Given the underlying graph structure, we can construct the l inear map H from Equation (16) and the analytical solution for Cθφ in Appendix A. Finding the set of eigenvalues is then a standa rd task. W e show the applicability of our formalism in Figure 2 b y showing that GCNs degenerate to a zero distance state state exactly when σ2 w < σ 2 w,crit. Panel a) shows how this procedure correctly predicts the transition in the given CSBM instance: The maxi mum feature distance between any pair of nodes increases from zero at the point where the state K(l) αβ = k(l) becomes unstable. This means that beyond this point, the GCN has feature vectors tha t differ across nodes and therefore does not oversmooth. This is more explicitly shown in panels b) and c), where the equilibrium feature distance is plotted as a heatmap. At point A (panel b)), within the oversmoothing phase, all equilibrium feature distances are indeed zero, the network therefore converges to a state in which all features are the same. At point B (panel c)) on the other hand, pairs of nodes exist that have ﬁnite distance. In the latter case, one can recognize the com munity structure of the CSBM: the lower left and upper right quadrants are lighter than the dia gonal ones, indicating larger feature distances across communities than within. The equilibrium state thus contains information about the graph topology. This phenomenon is also observed in pane l d), where we show the predicted feature distance averaged for nodes within or between class es as a function of layers compared to ﬁnite-size simulations. Again, theoretical predictions m atch with simulations. Thus also on more general graphs the presented formalism predicts the transi tion point between the oversmoothing and the non-oversmoothing phase, corresponding to a transitio n between regular and chaotic behavior. 7Figure 2: The non-oversmoothing phase in a contextual stoch astic block model instance with pa- rameters N = 100 , d = 5 , λ = 1 . The shift operator is chosen according to (6) with g = 0 .3, and σ2 b = 0 and φ(x) = erf( √π 2 x). a) The maximum feature distance between any pair of nodes in equilibrium obtained from computing the GCN GP prior for L = 4 , 000 layers (blue) and the largest eigenvalue of the linearized GCN GP dynamics at the zero dist ance state as a function of weight variance σ2 w. The red line marks the point where maxi{|λp i|} = 1 . b) Heatmap of the equilibrium distance matrix with entries dαβ = d(xα, xβ) (Equation (13)) at σ2 w = 1 .3, marked as point A in panel a). Colorbar shared with the plot in c). c) Same as b) but at point B with σ2 w = 2 . d) Features distances d(l) αβ = d(x(l) α , x(l) β ) as a function of layers for random inputs X(0) αi i.i.d. ∼ N (0, 1) and a ﬁnite-size GCN with dl = 200 , averaged for distances for pairs of nodes within the same co mmu- nity (red) and across communities (purple). W e discuss how the assumptions on weight matrices in related theoretical work [2, 45] exclude net- works in the chaotic phase in Appendix D, explaining why the n on-oversmoothing phase has not been reported before. In Appendix E we observe how increasin g the weight variance increases the oversmoothing measure µ(X) in equilibrium also in the case of the more common shift opera tor proposed in the original work [19], despite the fact that thi s shift operator does not have the over- smoothed ﬁxed point in the sense of Equation (19). 4.3 Implications for performance Lastly we want to investigate the implications of the non-oversmoothing phase on performance. W e do this by applying the GCN GP as well as a ﬁnite-size GCN to the task of node classiﬁcation in the CSBM model and measure their performance, shown in Figure 3. Panel a) shows how the general- ization error of the GCN GP changes depending on the weight va riance σ2 w and the number of layers L. In the oversmoothing phase where most GCNs in the literatur e are initialized (see Appendix D), the generalization error increases signiﬁcantly already a fter only a couple of layers. W e observe the best performance near the transition to chaos where the GCN G P stays informative up to 100 lay- ers. In panel b) we test the generalization error for even dee per networks. While the generalization error increases to one (being random chance) in the oversmoo thing phase, GCN GPs in the chaotic phase stay informative even at more than a thousand layers. T his can be explained by Figure 2: For such deep networks, the dynamics are very close to the equili brium and thus no information of the input features X(0) is transferred to the output. The state, however, still cont ains information of the network topology from the adjacency matrix, leading to b etter than random chance performance. In panel c) we explicitly show the layer dependence of the gen eralization error for the GCN GP at the critical point, in the oversmoothing and in the chaotic p hase. Again, we see a fast performance drop for oversmoothing networks, while in the chaotic phase and at the critical point the GCN GP obtains good performance also at large depths, with perform ance peaking at L ≈ 15 layers. Tuning the weight variance thus not only prevents oversmoothing, b ut may also allow the construction of GCNs with more layers and possibly better generalization pe rformance. In the study of deep networks, results obtained in the limit o f inﬁnite feature dimensions dl → ∞ often are also applicable for ﬁnite-size networks [32, 37]. In panel d) we conduct a preliminary analysis for ﬁnite-size GCNs by measuring the performance o f randomly initialized GCNs for which we only train the readout layer via gradient descent. Indeed , we observe similar behavior as for the GCN GP: Performance drops rapidly over layers in the oversmo othing phase, while performance stays high over many layers at the critical point and in the ch aotic phase and peaks at L ≈ 15 layers. 8Figure 3: Generalization error (mean squared error) of the G aussian process for a CSBM with parameters N = 20 , d = 5 , λ = 1 , γ = 1 and µ = 4 . The shift operator is deﬁned in (6) with g = 0 .1, other parameters are σ2 b = 0 , φ(x) = erf( √π 2 x) and σro = 0 .01. In all panels we use Ntrain = 10 training nodes and Ntest = 10 test nodes, ﬁve training nodes from each of the two communities. Labels are ±1 for the two communities, respectively. For all panels, we show averages over 50 CSBM instances. a) Heatmap of the generalization error of the GCN GP dependent on number of layers L and weight variance σ2 w. The red line shows the transition to the non-oversmoothing phase. b) Generalization error dependent on weight variance σ2 w and depths L = 1 , 4, 16, 64, 256, 1024 from turquoise to dark blue. c) Generalization error dependent on the layer for the GCN GP at the critical line σ2 w = σ2 w,crit, in the oversmoothing phase σ2 w = σ2 w,crit − 1 and the non-oversmoothing phase σ2 w = σ2 w,crit + 1. d) Performance of randomly initialized ﬁnite- size GCNs with dl = 200 for l = 1 , . . . , L where only the linear readout layer is trained with gradient descent (details in Appendix F) at the critical lin e σ2 w = σ2 w,crit, in the oversmoothing phase σ2 w = σ2 w,crit − 1 and the non-oversmoothing phase σ2 w = σ2 w,crit + 1. Figure 4: GCN GP performance on the Cora dat- set [38].a) Generalization error (mean squared error) as a function of layers L and weight vari- ance σ2 w − σ2 w,crit. for our stochastic shift operator (6) with g = 0 .9. The value of σ2 w,crit. ≈ 1 is determined numerically in Appendix G. b) Layer dependent generalization error and accuracy for GCNs near the transitionσ2 w = σ2 w,crit. +0.1. Grey dashed line shows accuracy obtained for GCNs in the original work [19]. Numerical details in Ap- pendix F. Additionally, we test the performance of the GCN GP on the real world citation network Cora [38]. Evaluating the eigenvalue condition (20) would be computationally expensive for such a large dataset, therefore we ﬁnd the tran- sition by numerically evaluating the feature dis- tanceµ(X) in equilibrium and search for the σ2 w at which this distance becomes non-zero. This procedure results in σ2 w,crit ≈ 1 and is presented in more detail in Appendix G. Fig- ure 4 panel a) shows the performance of the GCN GP dependent on the number of layers Land weight variance σ2 w: as for the CSBM in Figure 3 we observe that the performance for deep GCN GPs is best near the transition in the non-oversmoothing phase. Furthermore, GCN GPs with more layers achieve lower gen- eralization error. This is shown more directly in panel b). There we observe the layer depen- dence for GCN GPs near the transition in the non-oversmoothing regime. Indeed, the accu- racy increases up to a hundred layers, reaching the accuracy of ﬁnite-size GCNs stated in [19]. Near the transition, accuracy increases for up toL = 100 , and the generalization error improves even beyond this. W e hypothesize that this many layers are re quired for high performance partly due to our choice of the shift operator. The Cora dataset has a maximum degree dmax = 168 leading to small off-diagonal elements for the choice of our shift op erator: Recall that in Equation (6), the parameter g is constrained to be g ∈ (0, 1). As a consequence, the off-diagonal elements of the shift operator are Aij < 1 dmax = 1 168 . Many convolutional layers are then needed to incorporate information from a node’s neighbors. 9One might wonder whether it is possible to initialize weight s in say the oversmoothing regime, and transition to the non-oversmoothing regime during trainin g. W e argue that this is possible in the case of Langevin training (Appendix H). 5 Discussion In this study we used the equivalence of GCNs and GPs to investigate oversmoothing, the property that features at different nodes converge over layers to the same feature vector in an exponential man- ner. By extending concepts such as the propagation depth and chaos from the study of conventional deep feedforward neural networks [37], we are able to derive a condition to avoid oversmoothing. This condition is phrased in terms of an eigenvalue problem o f the linearized GCN GP dynamics around the state where all features are the same: This state i s stable if all eigenvalues are smaller than one, thus the networks do oversmooth. If one eigenvalue , however, is larger than one, the state where the features are the same on all nodes becomes unstable . While most GCNs studied in the literature are in the oversmoothing phase [2, 45], the non-o versmoothing phase can be reached by a simple tuning of the weight variance at initialization. An a nalogy can be drawn between the chaotic phase of DNNs and the non-oversmoothing phase of GCNs. Previ ous theoretical works have proven that oversmoothing is inevitable in some GNN architectures , among them GCNs; these works, how- ever, make crucial assumptions on the weight matrices, cons training their variances to be in what we identify as the oversmoothing phase. Near the transition , we ﬁnd GCNs which are both deep and expressive, matching the originally reported GCN performa nce [19] on the Cora dataset with GCN GPs beyond 100 layers. Limitations. The current analysis is based on the equivalence of GCNs and G Ps which strictly holds only in the limit of inﬁnite feature dimension. GCNs wi th large feature vectors ( dl = 200 ) are well described by the theory, as shown in Section 4. For a s mall number of feature dimensions, however, we expect deviations from the asymptotic results. Throughout the main part of this work, we assumed a row-stochastic shift operator which made the eq uilibrium Keq in the oversmoothing phase particularly simple. For other shift operators, we ex pect qualitatively similar results while the equilibrium Keq may look different in detail. In our preliminary experiment s on the common shift operator from [19] (Appendix E), we indeed ﬁnd that increasi ng the weight variance increases the distances between features also in this case. W e hypothesiz e that this effect makes the equilibrium more informative of the graph topology, as in the stochastic shift operator case. The choice of non-linearity is unrestricted, but in the general case nume rical integration of (8) is needed. T o determine whether a given weight variance is in the non-ov ersmoothing phase, one calculates the eigenvalues of the linearized GCN GP dynamics which take the form of an N2 × N2 matrix (see Equation (16)), this has a run time of O(N6). While this becomes computationally expensive for large graphs, the conceptual insights of the presented a nalysis remain. In practical applications with large graphs one may reduce the computational load by de termining the transition point via computation of the GCN GP prior until it is close to equilibri um. This procedure has a runtime of O(N3Leq) where Leq is the number of layers after which the process is sufﬁcientl y close to equilibrium. One might then do an interval search on the weig ht variance until the transition point is determined with sufﬁcient accuracy. Oulook.Formulating GCNs with the help of GPs can be considered the le ading order in the number of feature space dimension dl when approximating ﬁnite-size GCNs. Computing correction s for ﬁnite numbers of hidden feature dimensions would allow the c haracterization of feature learning in such networks, similar as in standard deep networks [26, 48, 39]. Moreover, the generalization of this formalism to more general shift operators and other GNN architectures [49, 4] like GA Ts [41] are possible directions of future research. In the special c ase of GA Ts we expect similar results to the GCN analyzed here, since the shift operator is construct ed using a softmax and therefore also is row-stochastic. Acknowledgements Funded by the European Union (ERC, HIGH-HOPeS, 101039827).V iews and opinions expressed are however those of the author(s) only and do not necessaril y reﬂect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. W e also acknowle dge funding by the German Research 10Council (DFG) within the Collaborative Research Center “Sp arsity and Singular Structures” (SfB 1481; Project A07). References [1] Uri Alon and Eran Y ahav. On the Bottleneck of Graph Neural Networks and its Practical Implications, March 2021. arXiv:2006.05205 [cs, stat]. [2] Chen Cai and Y usu W ang. A Note on Over-Smoothing for Graph Neural Networks, June 2020. arXiv:2006.13318 [cs, stat]. [3] Deli Chen, Y ankai Lin, W ei Li, Peng Li, Jie Zhou, and Xu Sun . Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the T opological V iew. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 34(04):3438–3445, April 2020. Number: 04. [4] Ming Chen, Zhewei W ei, Zengfeng Huang, Bolin Ding, and Y a liang Li. Simple and Deep Graph Convolutional Networks. In Proceedings of the 37th International Conference on Ma- chine Learning , pages 1725–1735. PMLR, November 2020. ISSN: 2640-3498. [5] Aditya Cowsik, T amra Nebabu, Xiao-Liang Qi, and Surya Ga nguli. Geometric Dynamics of Signal Propagation Predict Trainability of Transformer s, March 2024. arXiv:2403.02579 [cond-mat]. [6] G. Cybenko. Approximation by superpositions of a sigmoi dal function. Mathematics of Con- trol, Signals and Systems , 2(4):303–314, December 1989. [7] Michaël Defferrard, Xavier Bresson, and Pierre V anderg heynst. Convolutional Neural Net- works on Graphs with Fast Localized Spectral Filtering. In Advances in Neural Information Processing Systems , volume 29. Curran Associates, Inc., 2016. [8] Y ash Deshpande, Subhabrata Sen, Andrea Montanari, and E lchanan Mossel. Contextual Stochastic Block Models. In Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. [9] V ijay Prakash Dwivedi, Ladislav Rampášek, Michael Galk in, Ali Parviz, Guy W olf, Anh Tuan Luu, and Dominique Beaini. Long Range Graph Benchmark. Advances in Neural Information Processing Systems , 35:22326–22340, December 2022. [10] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matth ieu W yart. Disentangling feature and lazy training in deep neural networks. Journal of Statistical Mechanics: Theory and Experi- ment, 2020(11):113301, November 2020. Publisher: IOP Publishi ng and SISSA. [11] Jean Ginibre. Statistical Ensembles of Complex, Quate rnion, and Real Matrices. Journal of Mathematical Physics , 6(3):440–449, March 1965. [12] William L. Hamilton. Graph Representation Learning . Morgan & Claypool Publishers, September 2020. [13] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der W alt, Ralf Gommers, Pauli V ir- tanen, David Cournapeau, Eric Wieser, Julian T aylor, Sebas tian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkw ijk, Matthew Brett, Allan Hal- dane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, P ierre Gérard-Marchant, Kevin Sheppard, T yler Reddy, W arren W eckesser, Hameer Abbasi, Ch ristoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. Publisher: Nature Publishing Group. [14] Jilin Hu, Jianbing Shen, Bin Y ang, and Ling Shao. Inﬁnit ely Wide Graph Convolutional Net- works: Semi-supervised Learning via Gaussian Processes, F ebruary 2020. arXiv:2002.12168 [cs, stat]. [15] Tianjin Huang, Tianlong Chen, Meng Fang, Vlado Menkovs ki, Jiaxu Zhao, Lu Y in, Y ulong Pei, Decebal Constantin Mocanu, Zhangyang W ang, Mykola Pecheni zkiy, and Shiwei Liu. Y ou Can Have Better Graph Neural Networks by Not Training W eight s at All: Finding Untrained GNNs Tickets, February 2024. arXiv:2211.15335 [cs]. [16] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neu ral T angent Kernel: Convergence and Generalization in Neural Networks. In Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. 11[17] Y ufei Jin and Xingquan Zhu. A TNP A: A Uniﬁed V iew of Overs moothing Alleviation in Graph Neural Networks, May 2024. arXiv:2405.01663 [cs]. [18] Nicolas Keriven. Not too little, not too much: a theoret ical analysis of graph (over)smoothing. Advances in Neural Information Processing Systems , 35:2268–2281, December 2022. [19] Thomas N. Kipf and Max W elling. Semi-Supervised Classi ﬁcation with Graph Convolutional Networks, February 2017. arXiv:1609.02907 [cs, stat]. [20] Jaehoon Lee, Y asaman Bahri, Roman Novak, Samuel S. Scho enholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep Neural Networks as Gaussia n Processes, March 2018. arXiv:1711.00165 [cs, stat]. [21] Qimai Li, Zhichao Han, and Xiao-ming Wu. Deeper Insight s Into Graph Convolutional Net- works for Semi-Supervised Learning. Proceedings of the AAAI Conference on Artiﬁcial Intel- ligence, 32(1), April 2018. Number: 1. [22] Sitao Luan, Mingde Zhao, Xiao-W en Chang, and Doina Prec up. Training Matters: Unlocking Potentials of Deeper Graph Convolutional Neural Networks, October 2022. arXiv:2008.08838 [cs, stat]. [23] Y ao Ma and Jiliang T ang. Deep Learning on Graphs . Cambridge University Press, September 2021. Google-Books-ID: _A VDEAAA QBAJ. [24] L. Molgedey, J. Schuchhardt, and H. G. Schuster. Suppre ssing chaos in neural networks by noise. Physical Review Letters , 69(26):3717–3719, December 1992. Publisher: American Physical Society. [25] Gadi Naveh, Oded Ben David, Haim Sompolinsky, and Zohar Ringel. Predicting the outputs of ﬁnite deep neural networks trained with noisy gradients. Physical Review E , 104(6):064301, December 2021. Publisher: American Physical Society. [26] Gadi Naveh and Zohar Ringel. A self consistent theory of Gaussian Processes captures fea- ture learning effects in ﬁnite CNNs. In Advances in Neural Information Processing Systems , volume 34, pages 21352–21364. Curran Associates, Inc., 202 1. [27] Radford M Neal. Priors for inﬁnite networks (tech. rep. no. crg-tr-94-1). University of T oronto , 415, 1994. [28] Y in Cheng Ng, Nicolò Colombo, and Ricardo Silva. Bayesi an Semi-supervised Learning with Graph Gaussian Processes. In Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. [29] Zehao Niu, Mihai Anitescu, and Jie Chen. Graph Neural Ne twork-Inspired Kernels for Gaus- sian Processes in Semi-Supervised Learning, February 2023 . arXiv:2302.05828 [cs, stat]. [30] Kenta Oono and T aiji Suzuki. Graph Neural Networks Expo nentially Lose Expressive Power for Node Classiﬁcation, January 2021. arXiv:1905.10947 [c s, stat]. [31] Fabian Pedregosa, Gaël V aroquaux, Alexandre Gramfort , V incent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron W eiss, V incent Dubourg, Jake V ander- plas, Alexandre Passos, David Cournapeau, Matthieu Bruche r, Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research , 12(85):2825–2830, 2011. [32] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Soh l-Dickstein, and Surya Ganguli. Ex- ponential expressivity in deep neural networks through tra nsient chaos. In Advances in Neural Information Processing Systems , volume 29. Curran Associates, Inc., 2016. [33] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli , and Jascha Sohl-Dickstein. On the Expressive Power of Deep Neural Networks. In Proceedings of the 34th International Conference on Machine Learning , pages 2847–2854. PMLR, July 2017. ISSN: 2640-3498. [34] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. Adaptive computation and machine learning. MIT Press, Cam bridge, Mass, 2006. OCLC: ocm61285753. [35] T . Konstantin Rusch, Michael M. Bronstein, and Siddhar tha Mishra. A Survey on Oversmooth- ing in Graph Neural Networks, March 2023. arXiv:2303.10993 [cs]. 12[36] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, Februa ry 2014. arXiv:1312.6120 [cond- mat, q-bio, stat]. [37] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, an d Jascha Sohl-Dickstein. Deep Infor- mation Propagation, April 2017. arXiv:1611.01232 [cs, sta t]. [38] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise G etoor, Brian Galligher, and Tina Eliassi- Rad. Collective Classiﬁcation in Network Data. AI Magazine , 29(3):93–93, September 2008. Number: 3. [39] Inbar Seroussi, Gadi Naveh, and Zohar Ringel. Separati on of scales and a thermodynamic description of feature learning in some CNNs. Nature Communications , 14(1):908, February 2023. Number: 1 Publisher: Nature Publishing Group. [40] Y unchong Song, Chenghu Zhou, Xinbing W ang, and Zhouhan Lin. Ordered GNN: Or- dering Message Passing to Deal with Heterophily and Over-sm oothing, February 2023. arXiv:2302.01524 [cs]. [41] Petar V eli ˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pie tro Liò, and Y oshua Bengio. Graph Attention Networks, February 2018. ar Xiv:1710.10903 [cs, stat]. [42] Pauli V irtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, T yler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, W arren W ecke sser, Jonathan Bright, Stéfan J. van der W alt, Matthew Brett, Joshua Wilson, K. Jarrod Millma n, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Care y, ˙ Ilhan Polat, Y u Feng, Eric W . Moore, Jake V anderPlas, Denis Laxalde, Josef Perktold, Rob ert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. R ibeiro, Fabian Pedregosa, and Paul van Mulbregt. SciPy 1.0: fundamental algorithms for sc ientiﬁc computing in Python. Nature Methods , 17(3):261–272, March 2020. Publisher: Nature Publishing Group. [43] Christopher Williams. Computing with Inﬁnite Network s. In Advances in Neural Information Processing Systems , volume 9. MIT Press, 1996. [44] Christopher K. I. Williams. Computation with Inﬁnite N eural Networks. Neural Computation , 10(5):1203–1216, July 1998. [45] Xinyi Wu, Amir Ajorlou, Zihui Wu, and Ali Jadbabaie. Dem ystifying Oversmoothing in Attention-Based Graph Neural Networks, October 2023. arXi v:2305.16102 [cs, stat]. [46] Xinyi Wu, Zhengdao Chen, William W ang, and Ali Jadbabai e. A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks, February 2023. arX iv:2212.10701 [cs, stat]. [47] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Che ngqi Zhang, and Philip S. Y u. A Comprehensive Survey on Graph Neural Networks. IEEE T ransactions on Neural Networks and Learning Systems , 32(1):4–24, January 2021. Conference Name: IEEE Transact ions on Neural Networks and Learning Systems. [48] Jacob A. Zavatone-V eth, William L. T ong, and Cengiz Peh levan. Contrasting random and learned features in deep Bayesian linear regression. Physical Review E , 105(6):064118, June 2022. Publisher: American Physical Society. [49] Lingxiao Zhao and Leman Akoglu. PairNorm: T ackling Ove rsmoothing in GNNs, February 2020. arXiv:1909.12223 [cs, stat]. 13A Analytical solution for expectation values T o evaluate our theory, we need to compute expectation value s given in the form of (8) which we restate here for readability C(l) γδ = ⣨ φ ( h(l) γ ) φ ( h(l) δ )⟩ h(l) γ ,h(l) δ , (21) where h(l) γ and h(l) δ are zero mean random Gaussian variables with ⟨h(l) γ h(l) δ ⟩ = K(l) γδ . This can be evaluated numerically for general non-linearities φ(x). For simplicity, however, we choose φ(x) = erf( √π 2 x) in our experiments where the scaling factor in the erf is chosen such that ∂φ ∂x (0) = 1 . In this case, the expectation value can be evaluated analytica lly to be C(l) γδ = 2 π arcsin ( π 2 K(l) γδ√ 1 + π 2 K(l) γγ √ 1 + π 2 K(l) δδ ) . (22) as shown in [44]. B The linearized GP of GCNs W e start from the full GCN GP iterative map (8,(9)), which we r estate here for readability K(l+1) αβ =Tαβ [K(l)] = σ2 b + σ2 w ∑ γ,δ Aαγ Aβδ C(l) γδ [K(l)] (23) where h(l) γ and h(l) δ are drawn from a 0-mean Gaussian distribution with covariance ⟨h(l) γ h(l) δ ⟩ = K(l) γδ . The full covariance matrix of layer l is denoted as K(l). Here, we distinguish between the iterative maps Tαβ : RN×N → R of which there are N2, one for each pair of nodes α and β, and the entries K(l+1) αβ of the covariance matrix in the layer l + 1 . Notice that Tαβ = Tβα due to the symmetry of the covariance matrix. In the maps Tαβ, the covariance matrix of the previous layer only shows up in the expectation value Cγδ = ⣨ φ ( h(l) γ ) φ ( h(l) δ )⟩ h(l) γ ,h(l) δ such that the linearized dynamics around a ﬁxed point (being equilibrium Kﬁx αβ = Keq αβ or zero distance state Kﬁx αβ = k) with K(l) αβ = Kﬁx αβ + ∆ (l) αβ read K(l+1) αβ = Kﬁx αβ + ∆ (l+1) αβ = Tαβ[Kﬁx ]   =Kfix αβ + ∑ γ<δ ∂Tαβ ∂Kγδ [Kﬁx ]∆ (l) γδ + O((∆ (l))2) (24) =Kﬁx αβ + ∑ γ,δ σ2 w ∑ θ,φ 1 2 (1 + δγ,δ )Aαθ Aβφ ∂Cθφ ∂Kγδ [Kﬁx ]    ≡Hαβ,γδ ∆ (l) γδ + O((∆ (l))2) , (25) where we restrict the sum in (24) to γ < δ since Kγδ and Kδγ are the same quantity. From (24) follows ∆ (l+1) αβ = ∑ γ,δ Hαβ,γδ ∆ (l) γδ + O((∆ (l))2) (26) which is Equation (16) in the main text. While H is not symmetric in general, Hαβ,γδ ̸= Hγδ,αβ , it is symmetric in the ﬁrst and second pair of covariance indi ces, Hαβ,γδ = Hβα,γδ and Hαβ,γδ = Hαβ,δγ due to symmetry of the covariance matrices, Kαβ = Kβα. In the main text, we look for the right eigenvectors of H fulﬁlling λiV (i) αβ = ∑ γ,δ Hαβ,γδ V (i) γδ . (27) 14These are for general non-symmetric matrices not orthogona l. In order to decompose ∆ (l) to over- laps with the eigenvectors V (i) αβ we need to ﬁnd the dual vectors U(i) αβ fulﬁlling ∑ α,β U(i) αβ V (j) αβ =δij (28) from which we can deﬁne ∆ (l) i = ∑ α,β U(i) αβ ∆ (l) αβ (29) such that ∆ (l) αβ = ∑ i ∆ (l) i V (i) αβ (30) as stated in the main text. C Investigation of the complete graph model In this section we analytically investigate the complete graph model as deﬁned in the main text. Speciﬁcally, we consider networks with the shift operator Aαβ = g N − 1 + δαβ (1 − Ng N − 1 ) (31) and φ = erf( √πx/2). Due to the symmetry of the system we make the ansatz Keq αβ = Keq a + δαβ (Keq c − Keq a ) , (32) meaning that we assume constant variances Keq a across nodes and that all pairs of nodes have the same covariance Keq c , reducing the system to only two unknown variables. The equi librium is a ﬁxed point of the iterative map of the GCN GP . With the special choice of shift operator in (31) this becomes K(l+1) a = σ2 b + gaC(l) a + gcC(l) c (33) and K(l) c = σ2 b + haC(l) a + h′cC(l) c (34) with constants ga = (1 + g2 N − 1)σ2 w (35) gc = 2( g + g2(N − 2) N − 1 )σ2 w (36) ha = 2( g N − 1 + g2(N − 2) (N − 1)2 )σ2 w (37) hc = (1 + g2 (N − 1)2 + 2 g2(N − 2)(N − 3) (N − 1)2 + 4 g(N − 2) (N − 1) )σ2 w (38) and C(l) a = ⟨φ(h(l) α )φ(h(l) α )⟩ (39) C(l) c = ⟨φ(h(l) α )φ(h(l) β )⟩ for α ̸= β . (40) The preactivations are Gaussian distributed with zero mean and covariance ⟨h(l) α h(l) β ⟩ = K(l) αβ . In the oversmoothing phase, we know that µ(X) = 0 in equilibrium. W e have seen in Section 4.2 that this corresponds to Keq αβ = keq, implying for our ansatz that Keq c = Keq a . W e will ﬁnd the tran- sition to chaos by calculating where this state becomes unst able with regard to small perturbations. Speciﬁcally, we deﬁne c(l) = K(l) c K(l) a and look for the parameter point where 1 ? > ∂c(l+1) ∂c(l) ⏐ ⏐ ⏐ c(l)=1 . (41) 15The authors in [37] used this approach to ﬁnd the transition t o chaos for DNNs. The correlation coefﬁcient is c(l+1) = K(l+1) c K(l+1) a = σ2 b + haC(l) a + hcC(l) c σ2 b + gaC(l) a + gcC(l) c (42) and Equation (41) thus becomes ∂c(l+1) ∂c(l) ⏐ ⏐ ⏐ c(l)=1 = hc ∂C (l+1) c ∂c(l) K(l+1) a − gc ∂C (l+1) c ∂c(l) K(l+1) c (K(l+1) a )2 . (43) Since we look at this equation at the perfectly correlated st ate c(l) = 1 , we know that K(l) a = K(l) c (implying that C(l) a = K(l) c ) and can determine K(l) a as the solution of the ﬁxed point equation K(l+1) a = σ2 b + (ga + gc)C(l) a (44) to which the GCN GP dynamics reduce in the zero distance state (by using the fact that ∑ β Aαβ = 1 an C(l) αβ = c(l)). Lastly, we can calculate ∂C (l+1) c ∂c(l) ⏐ ⏐ ⏐ c=1 = ∂ ∂c(l) ( 2 π arcsin ( π 2 K(l) a c(l) 1 + π 2 K(l) a )) ⏐ ⏐ ⏐ ⏐ ⏐ c(l)=1 (45) = 2 π 1√ 1 − ( π 2 K(l) a c(l) 1+ π 2 K(l) a ) 2 K(l) a 2 π + K(l) a ⏐ ⏐ ⏐ ⏐ ⏐ c(l)=1 (46) = 2 π 1√ 1 − ( K(l) a 2 π +K(l) a ) 2 K(l) a 2 π + K(l) a , (47) where we used the known solution for the expectation value Cc for φ(x) = erf( √πx/2) from Appendix A. Plugging this into Equation (43) lets us calcula te ∂c(l+1) ∂c(l) ⏐ ⏐ ⏐ c(l)=1 and thus determine the transition to the non-oversmoothing phase. This is plotted as a red line in Figure 1 panel a) and b). D Restriction of weight matrices in related work In this section we will show histograms of critical weight variances σ2 w,crit and discuss how the assumptions in [2] and [45] exclude networks in the non-over smoothing phase. Our results thus stand in no conﬂict with the results of these works. Here we want to argue that the assumptions on the weight matrices in related work constrains their architectures to the oversmoothing phase. W e start with [45 ] in which the authors study graph attention networks (GA Ts). Although we study a standard GCN here, we hypothesize that increasing the weight variance at initialization likewise prevents ov ersmoothing in other architecture, such as the GA T in [45]. The critical assumption constraining them t o the oversmoothing phase is their assumption A3, stating that {|| ∏ k l=0 |W (l)|||max}∞ k=0 is bounded where ||M||max = max i,j |Mi,j |. For our setting of randomly drawn weight matrices W (l) ij i.i.d. ∼ N (0, σ2 w dl ) with W (l) ij ∈ RN×N , this restricts us to values σ2 w ≤ 1. This can be seen by using the circular law from random matrix theory [11]: It is known that the eigenvalues of a matrix with i.i.d. random Gaussian entries of the form above have eigenvalues uniformly distributed in a circ le around 0 in the complex plane with radius √ σ2w in the limit dl → ∞ . Thus, the maximal real part of any eigenvalue of this matrix is √ σ2w. Thus we can estimate || ∏ k l=0 |W (l)|||max ≤ c( √ σ2w)k with a constant c. T o enter the non-oversmoothing phase, we need σ2 w > 1. In this case, the latter expression diverges for k → ∞ , thus being excluded by the proof in [45]. Indeed, for the CSBM s we investigated in this work, all critical weight variances σ2 w,crit are larger than 1 as shown in Figure 5. Also in our model of the complete graph and the CSBM investigated in Section 4.2.2 al l σ2 w,crit are larger than 1, compare Figure 1 and Figure 2. 161.0 1.2 1.4 2 w, crit 0.0 2.5 5.0 7.5 Histogram of 2 w, crit Figure 5: Histogram of σ2 w,crit for the 50 CSBM instances used in the experiment of Figure 3. The point 1 is marked for comparison to related work. The authors of [2] and [30] also study GCNs,; however they con sider a different shift operator than in this work. In both of [2] and [30] the authors ﬁnd that t heir GCN models exponentially loose expressive power if sλ < 1, with λ being the maximal singular value/eigenvalue of the shift op erator and s being the maximal singular value of all weight matrices. Aga in, the maximal singular value is limited (dependent on λ), which in our approach translates to a limit on σ2 w. While the authors notice that their bounds do not hold for large singular values s, they do not observe a non-oversmoothing phase in their models. E Non-oversmoothing GCNs with non-stochastic shift operators Here we investigate how increasing the weight variance can m itigate oversmoothing also in the case of the original shift operator proposed by Kipf and W elling i n [19] being AKW = ( D′)−1/2A′(D′)−1/2 (48) with A′ = I + A and D′ ij = δij ∑ k A′ ik. This shift operator AKW is not row-stochastic, therefore the state Kαβ = k is not a ﬁxed point for k ̸= 0 , as can be seen from Equation (19). Thus we need to differentiate between two kind of ﬁxed points: Either, al l features are zero, for which Kαβ = 0 and µ(X) = 0 , or some Kαβ ̸= 0 in which case we have µ(X) > 0. Importantly, for this shift operator, there is no intermediate case for which Kαβ = k with k ̸= 0 . Consequently there is no oversmoothing regime with respect to the measure µ(X) where node features are non-zero. For an in depth study of this case (48), the deﬁnition of another ove rsmoothing measure µ′ incorporating the different values of ∑ β (AKW )αβ for different α may be more appropriate. For our purposes, it will sufﬁce to analyze the shift operator (48) with the measu re µ(X) from Equation (14). Figure 6 panel a) shows how µ(X) in equilibrium increases for larger weight variances σ2 w for the shift operator (48). For comparison, we also show the result s obtained with our row-stochastic shift operator (6). Thus we ﬁnd that also for the shift operator AKW the pairwise distance between features can be increased by increasing the weight variance σ2 w. The non-existence of an oversmoothed ﬁxed point except for the special case Kαβ = 0 which we argued for above is observed in panel b) and c): The oversmoothing measure µ(X) is zero for AKW if and only if all entries of the covariance matrix are zero, implying that all preactivations and thus also all features are zero. This is a qualitative difference to our shift operator A for which we ﬁnd equilibrium states with non-zero maxα Keq αα but still µ(X) = 0 . F Details of numerical experiments T o conduct our experiments we use NumPy [13], SciPy [42] (bot h available under a BSD-3-Clause License) and Scikit-learn (sklearn) [31] (available under a New BSD License). The code is publicly available under https://github.com/bepping/non-oversmoothing-gcns. For our experi- ments with the Cora dataset, we use the readin methods from [1 9] which are available under a MIT license (Copyright (c) 2016 Thomas Kipf). Computations wer e performed on CPUs.Requirements for the experiments with synthetic data are (approximately ): • Figure 1: 10mins on a single core laptop. 17Figure 6: Oversmoothing in GCN GPs with the commonly used shi ft operator (48) (called Kipf & W elling in the label) and our row-stochastic shift operato r (6). a) Feature distance µ(X) in equilibrium dependent on weight variance σ2 w obtained from simulating the GCN GP priors for Leq = 4 , 000 layers. Shown are the averages (solid lines) and standard de viations (shaded areas) over 20 CSBM initializations with λ = 1 , d = 5 and N = 30 nodes.b) Scatter plot of maximal variance of all nodes maxα{Keq αα} and feature distance µ(X) in equilibrium for the same data as in plot a). c) Same as b) but zoomed into lower left corner. • Figure 2: 10h on a single node on an internal CPU cluster. Mos t of the computation time is needed for evaluating max(λp i) in panel a). • Figure 3: 2h on a single core laptop. In panel d), the last lay er of ﬁnite-size GCNs is trained with the standard settings from sklearn.linear_model.SGD Regressor(). • Figure 5: Byproduct of Figure 2. • Figure 6: 10min on a single core laptop. W e also experimented on the real world benchmark dataset Cor a [38]. This is a citation network with 2708 nodes, representing publications, and 5429 edges, representing the citations between them (we use undirected edges). The publications are divide d into seven classes, and the task is to predict these classes for the unlabeled nodes. Node featu res are of 0/1 valued vectors indicating the absence/presence of words from a dictionary in the title s of the respective publication. The dictionary consists of 1433 unique words. Requirements for the experiments with the Cor a dataset are: • Figure 4: 1h on a single core laptop. • Figure 7: 15h on a single core laptop. G Non-oversmoothing transition in the Cora dataset For Figure 4 we numerically determined the transition to thenon-oversmoothing regime for the Cora dataset. The transition was estimated by measuring the distance µ(X) at equilibrium (i.e., after many layers), and determining at which value of σ2 w it becomes larger than a small distance ǫ = 10 −5. The results of this experiment are shown in Figure 7. W e ﬁnd t he critical point to be σ2 w,crit = 1 while using a step size of δσ2 w = 0 .01. H T ransitioning between regimes during training In this section argue why we think it is possible to transition from the oversmoothing to the non- oversmoothing regime or vice versa during training. In the GP limit of inﬁnitely many hidden features with a ﬁniteamount of training data, the variance of weights of a neural network is the same before and after tra ining. This is because weights only change marginally in this limit, also known as the lazy train ing regime [16, 10]. W e will use this fact to argue that Langevin training is capable of transitio ning from one regime to the other. Langevin training is a gradient based training scheme with e xternal noise and a decay term. The gradient ﬂow equation is given as dWij dt = −γWij − ∇Wij L + Bij , 18Figure 7: Node distance measure µ(X) at equilibrium obtained from computing the GCN GP prior for L = 4 , 000 layers as a function of σ2 w. The transition to the non-oversmoothing regime is estimated by checking where the node distance measure is lar ger than ǫ = 10 −5, marked as the red line. where Wij are the weights to be trained, L is the loss function and Bij denotes external white noise ⟨Bij (t)Bkl(s)⟩ = δi,kδj,lδ(t − s)D with strength D where δa,b and δ(a − b) denote the Kronecker or Dirac delta, respectively. Both γ and D are parameters of this training scheme. It is known that the distribution of weights converges to the posterior weig ht distribution of a neural network GP with weight variance σ2 w = D 2γ in the inﬁnite feature limit [25] which, as we have noticed ab ove, is the same as the prior distribution. Since the Langevin distribution converges to the GP weight posterior for any initial distribution, one can imagine an initial distribution with a variance that ini tializes the network in the oversmoothing regime, while the parameters γ and D are chosen such that σ2 w = D 2γ > σ 2 w,crit implying that after training most weight realizations will be in the non-oversm oothing regime. Thus, in this case the GCN would have transitioned from one regime to the other. Whi le this argument is made in the limit of inﬁnitely many hidden features, we think that qualitativ ely similar results are possible for a large but ﬁnite number of hidden feature dimensions. The transiti on in the reverse direction is possible by the same argument only with the initial and the ﬁnal variance s exchanged. 19",
      "meta_data": {
        "arxiv_id": "2406.02269v2",
        "authors": [
          "Bastian Epping",
          "Alexandre René",
          "Moritz Helias",
          "Michael T. Schaub"
        ],
        "published_date": "2024-06-04T12:47:13Z",
        "venue": "Advances in Neural Information Processing Systems 37 (NeurIPS\n  2024)",
        "pdf_url": "https://arxiv.org/pdf/2406.02269v2.pdf"
      }
    },
    {
      "title": "Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature",
      "abstract": "Graph Neural Networks (GNNs) had been demonstrated to be inherently\nsusceptible to the problems of over-smoothing and over-squashing. These issues\nprohibit the ability of GNNs to model complex graph interactions by limiting\ntheir effectiveness in taking into account distant information. Our study\nreveals the key connection between the local graph geometry and the occurrence\nof both of these issues, thereby providing a unified framework for studying\nthem at a local scale using the Ollivier-Ricci curvature. Specifically, we\ndemonstrate that over-smoothing is linked to positive graph curvature while\nover-squashing is linked to negative graph curvature. Based on our theory, we\npropose the Batch Ollivier-Ricci Flow, a novel rewiring algorithm capable of\nsimultaneously addressing both over-smoothing and over-squashing.",
      "full_text": "Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Khang Nguyen 1 2 Hieu Nong 1 Vinh Nguyen1 Nhat Ho 3 Stanley Osher 4 Tan Nguyen5 Abstract Graph Neural Networks (GNNs) had been demon- strated to be inherently susceptible to the prob- lems of over-smoothing and over-squashing. These issues prohibit the ability of GNNs to model complex graph interactions by limiting their effectiveness in taking into account distant information. Our study reveals the key connec- tion between the local graph geometry and the occurrence of both of these issues, thereby pro- viding a unified framework for studying them at a local scale using the Ollivier-Ricci curva- ture. Specifically, we demonstrate that over- smoothing is linked to positive graph curvature while over-squashing is linked to negative graph curvature. Based on our theory, we propose the Batch Ollivier-Ricci Flow, a novel rewiring algo- rithm capable of simultaneously addressing both over-smoothing and over-squashing. 1. Introduction A collection of entities with a set of relations between them is among the simplest, and yet most general, types of struc- ture. It came as no surprise that numerous real world data are naturally represented by graphs (Harary, 1967; Hsu & Lin, 2008; Estrada & Bonchev, 2013), motivating many re- cent advancements in the study of Graph Neural Networks (GNNs). This has lead to a wide range of successful appli- cations, including physical modeling (Battaglia et al., 2016; Kipf et al., 2018; Sanchez-Gonzalez et al., 2018), chemical and biological inference (Duvenaud et al., 2015; Gilmer et al., 2017), recommender systems (van den Berg et al., 1FPT Software AI Center, Vietnam 2Faculty of Mathemat- ics and Computer Science, University of Science, Vietnam Na- tional University Ho Chi Minh City, Vietnam 3Department of Statistics and Data Sciences, University of Texas at Austin, USA 4Department of Mathematics, University of California, Los An- geles, USA 5Department of Mathematics, National University of Singapore, Singapore. Correspondence to: Khang Nguyen <khang.nguyenhoang.vn@gmail.com>, Tan Nguyen <tanmn- guyen89@gmail.com>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 2017; Ying et al., 2018; Fan et al., 2019; Wu et al., 2019), generative models (Li et al., 2018b; Bojchevski et al., 2018; Shi et al., 2020), financial prediction (Chen et al., 2018b; Matsunaga et al., 2019; Yang et al., 2019), and knowledge graphs (Shang et al., 2019; Zhang et al., 2019). Despite their success, popular GNN designs suffer from two notable setbacks that hamper their performance in practical applications that require long-range interactions. The first common problem encountered by GNNs is known as over- smoothing (Li et al., 2018a). Over-smoothing occurs when node features quickly converge to each other and become in- distinguishable as the number of layers increases. This issue puts a limit on the depth of a GNN, prohibiting the model’s capability of capturing complex relationships in the data. Another plight plaguing GNNs is known as over-squashing (Alon & Yahav, 2021). This phenomenon happens when the number of nodes within the receptive field of a particular node grows exponentially with the number of layers, leading to the squashing of exponentially-growing amount of infor- mation into fixed-size node features. Such over-squashing hinders the ability of GNNs to effectively process distant information and capture long-range dependencies between nodes in the graph, especially in the case of deep GNNs that require many layers. Together, over-smoothing and over-squashing impair the performance of modern GNNs, impeding their application to many important settings that involve very large graphs. (Cai & Wang, 2020; Alon & Yahav, 2021). Understanding and alleviating either of these problems has been the main focus in many recent studies of GNNs (Oono & Suzuki, 2020; Cai & Wang, 2020; Zhao & Akoglu, 2020; Karhadkar et al., 2023). Notably, using a notion of graph curvature, Topping et al. (2022) suggested that over-squashing behav- iors could arise from local graph structures. This pioneering idea implies that local graph properties can be used to study and improve GNN performance. It has been recently noted that over-smoothing and over-squashing are somewhat re- lated problems (Karhadkar et al., 2023). Nevertheless, to the best of our knowledge, there has been no work in the lit- erature that offers a common framework to understand these problems. Such a unified approach presents a potentially crucial theoretical contribution to our understanding of the over-smoothing and over-squashing issues. It enables the development of novel architectures and methods that can 1 arXiv:2211.15779v3  [cs.LG]  31 May 2023Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature effectively learn complex and long-range graph interactions, thereby broadening the applications of GNNs on practical tasks. Contribution. We present a unified theoretical framework to study both the over-smoothing and over-squashing phe- nomena in GNNs at the local level using the Ollivier-Ricci curvature (Ollivier, 2009), an inherent local geometric prop- erty of graphs. Our key contributions are three-fold: 1. We prove that very positively curved edges cause node representations to become similar, thereby establishing a link between the over-smoothing issue and high edge curvature. 2. We prove that low curvature value characterizes graph bottlenecks and demonstrate a connection between the over-squashing issue and negative edge curvature. 3. Based on our theoretical results, we propose the Batch Ollivier-Ricci Flow (BORF), a novel curvature- based rewiring method designed to mitigate the over- smoothing and over-squashing issues simultaneously. Organization. We structure this paper as follows. First, we discuss related works in Section 2. In Section 3, we give a brief summary of the relevant backgrounds in the study of GNNs and provide a concise formulation for the Ollivier- Ricci curvature on graphs. In Section 4, we present our central analysis showing positive graph curvature is associ- ated with over-smoothing, while negative graph curvature is associated with over-squashing. In Section 5, we introduce the novel graph rewiring method BORF, which modifies the local graph geometry to suppress over-smoothing and alle- viate over-squashing inducing connections. We empirically demonstrate the superior performance of our method com- pared to other state-of-the-art rewiring methods in Section 6. The paper ends with concluding remarks in Section 7. Technical proofs and other additional materials are provided in the Appendix. Notation. We denote scalars by lower- or upper-case letters and vectors and matrices by lower- and upper-case boldface letters, respectively. We use G = (V, E) to denote a simple, connected graph G with vertex set V and edge set E. Graph vertices are also referred to as nodes, and the characters u, v, w, p, qare reserved for representing them. We write u ∼ v if (u, v) ∈ E. The shortest path distance between two vertices u, vis denoted by d(u, v). We let Nu = {p ∈ V | p ∼ u} be the 1-hop neighborhood and ˜Nu = Nu ∪ {u} be the extended neighborhood of u. The characters n, mare used to denote the degrees of the vertices u, v. 2. Related Work Over-smoothing: First recognised by (Li et al., 2018a), who observed that GCN with non-linearity removed induces a smoothing effect on data features, over-smoothing has been one of the focal considerations in the study of GNNs. A dynamical system approach was used by (Oono & Suzuki, 2020) to show that under considerable assumptions, even GCN with ReLU can not escape this plight. Follow-up work by (Cai & Wang, 2020) generalized and improved this approach. Designing ways to alleviate or purposefully avoid the problem is a lively research area (Luan et al., 2019; Zhao & Akoglu, 2020; Rusch et al., 2022). Notably, randomly removing edges from the base graph consistently improves GNN performance (Rong et al., 2020). Over-squashing: The inability of GNNs to effectively take into account distant information has long been observed (Xu et al., 2018). Alon & Yahav (2021) showed that this phenomenon cound be explained by the existence of local bottlenecks in the graph structure. It was shown by (Topping et al., 2022) that graph curvature provided an insightful way to study and address the over-squashing problem. Meth- ods have been designed to tackle over-squashing, including those by (Banerjee et al., 2022) and (Karhadkar et al., 2023). Graph curvature: Efforts have been made to extend the geometric notion of curvature to settings other than smooth manifolds, including on graphs (Bakry & ´Emery, 1985; Forman, 2003). Among these, the Ollivier’s Ricci curvature (Ollivier, 2009) is arguably the superior attempt due to its proven compatibility with the classical notion of curvature in differential geometry (Ollivier, 2009; van der Hoorn et al., 2021). Graph curvature has been utilised in the study of complex networks (Ni et al., 2015; Sia et al., 2019), and a number of works have experimented with its use in GNNs (Topping et al., 2022; Bober et al., 2022). 3. Preliminaries We begin by summarizing the relevant backgrounds on mes- sage passing neural networks and the over-smoothing and over-squashing issues of GNNs. We also provide a concise formulation for the Ollivier-Ricci curvature on graphs. 3.1. Message Passing Neural Networks Message passing neural networks (MPNNs) (Gilmer et al., 2017) are a unified framework for a broad range of graph neural networks. It encompasses virtually every popular GNN design to date, including graph convolutional network (GCN) (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), graph attention network (GAT) (Veli ˇckovi´c et al., 2018), graph isomorphism network (GIN) (Xu et al., 2019), etc. The key idea behind MPNNs is that by aggregat- ing information from local neighborhoods, a neural network 2Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 1.Popular GNNs are instances of Equation (1): GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), GAT (Veliˇckovi´c et al., 2018), and GIN (Xu et al., 2019). GNN ψk ϕk L GCN 1 linear activation mean GraphSAGE2 linear activation mean GAT linear activation weighted mean GIN 3 identity MLP sum can effectively use both node feature data and the graph topology to learn relevant information. Let X ∈ R|V|×d be the node feature matrix of a graph G, where d is the number of feature channels. Let Xk be the node feature matrix at layer k, with the convention that X0 = X. The features of node u at layer k is denoted by Xk u, and is exactly the transpose of the u-th row of Xk. A general formulation for an MPNN can be given by Xk+1 u = ϕk   M p∈ ˜Nu ψk(Xk p )  , (1) where ψk is a message function, L is an aggregating func- tion, and ϕk is an update function. Table 1 summarizes the choice for ψk, ϕk, and Lin four popular GNN architectures. We give further discussion on how Equation (1) accommo- dates different designs of GNNs in Appendix A. From now on, we will use MPNN and GNN interchangeably. Traditionally, GNNs are designed to operate directly on the input graphs. In many cases, this leads to significant downsides due to possible undesirable characteristics of the dataset. Hence, it has been proposed that by conducting the learning process on a modified version of the input graphs, we can improve upon the scale and performance of graph models (Hamilton et al., 2017; Gasteiger et al., 2019). One such approach is known as graph rewiring, which involves modifying the set of edges E within a graph as a preprocessing step. We give a brief overview of two novel rewiring algorithms, SDRF (Topping et al., 2022) and FoSR (Karhadkar et al., 2023), along with a comparison between them and our proposed method in Section 5. 3.2. The Over-smoothing and Over-squashing Issues of GNNs Over-smoothing has generally been described as the phe- nomenon where the feature representation of every node becomes similar to each other as the number of layers in a 1If the symmetrically normalized Laplacian is replaced by the normalized Laplacian. See Appendix A. 2Mean aggregator variant. 3GIN-0 variant. k = 0 k = 1 k = 2 k = 3 Figure 1.Over-smoothing induced by the averaging operation. GNN increases (Li et al., 2018a). If over-smoothing occurs, for every two neighbor nodes u, v, it must happen that \f\fXk u − Xk v \f\f → 0 as k → ∞. (2) Equation (2) can be thought of as the local smoothing be- havior, observed in the neighborhood of two nodes u ∼ v. A global formulation for feature representation similarity is obtained by summing up terms of the form \f\fXk u − Xk v \f\f for all neighbors u, v. Formally, we obtain a formulation for the global over-smoothing issue based on local observations X (u,v)∈E \f\fXk u − Xk v \f\f → 0 as k → ∞. (3) That is, if the term P (u,v)∈E \f\fXk u − Xk v \f\f converges to zero, we say that the model experiences over-smoothing. This formulation is similar to the definition based on the node-wise Dirichlet energy utilised in (Rusch et al., 2022). The Dirichlet energy was first proposed as a viable way to measure the over-smoothing issue by (Cai & Wang, 2020). Figure 1 visualizes the over-smoothing behavior of a simple graph containing 6 nodes from 3 classes with different RGB color features. At the start, the nodes can easily be divided into 3 classes. When the mean operator is applied repeatedly for k times across k GNN layers, those nodes rapidly con- verge to having similar colors. At the final step k = 3, the nodes in the GNN have become virtually indistinguishable, suggesting that they have experienced over-smoothing. On the other hand, over-squashing is an inherent pitfall of GNNs that occurs when bottlenecks in the graph structure impede the graph’s ability to propagate information among its vertices. We observe from Equation (1) that messages can only be transmitted by a distance of 1 at each layer. Hence, two nodes of distance K receive information from each other if and only if the GNN has at least K layers. For any given node u, the set of nodes whose messages can 3Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Figure 2.Bottlenecks inhibit the message passing capability of MPNNs. These bottlenecks are highlighted by bold red lines. reach u is called the receptive field of u. As the number of layers increases, the size of each node’s receptive field increases exponentially (Chen et al., 2018a). This causes messages between exponentially-growing number of distant vertices to be squashed into fixed size vectors, limiting the model’s ability to capture long range dependencies (Alon & Yahav, 2021). As illustrated in Figure 2, graph bottlenecks contribute to this problem by enforcing the maximal rate of expansion to the receptive field, while providing minimal connection between either sides of the bottleneck. Thus, a graph containing many bottlenecks causes GNNs to suffer from over-squashing. 3.3. Ollivier-Ricci Curvature on Graph The Ricci curvature is a geometric object ubiquitous in the field of differential geometry. At a local neighborhood of a Riemannian manifold, the Ricci curvature of the space characterizes the average geodesic dispersion, i.e., whether straight paths in a given direction of nearby points have the tendency to remain parallel (zero curvature), converge (pos- itive curvature), or diverge (negative curvature). Crucially, the definition of the Ricci curvature depends on the ability to specify directions, or more precisely, tangent vectors, within the space considered. To circumvent the lack of a tangent structure on graphs, the Ollivier-Ricci curvature (Ollivier, 2009) considers random walks from nearby points. We define a random walk µ on a graph G as a family of probability measure µu(·) on the vertex set V for all u ∈ V. For a vertex p ∈ V, it is intuitive to think of µu(p) as the probability that a random walker starting from u will end up at p after some number of steps. Then, for any u, v∈ V, we can consider the L1 Wasserstein transport distance W1(µu, µv) given by W1(µu, µv) = inf π∈Π(µu,µv)   X (p,q)∈V2 π(p, q)d(p, q)  , where Π(µu, µv) is the family of joint probability distribu- tions of µu and µv. This measures 4 the minimal distance 4More precisely, W1(µu, µv) measures the minimal distance that random walkers from u must travel to meet the ran- dom walkers from v. The Ollivier-Ricci curvature κ(u, v) is then defined based on the ratio between the random walker distance W1(µu, µv) and the original distance d(u, v) κ(u, v) = 1 − W1(µu, µv) d(u, v) . (4) Such a definition captures the behavior that κ(u, v) = 0 if the random walkers tend to stay at equal distance,κ(u, v) < 0 if they tend to diverge, and κ(u, v) > 0 if they tend to converge. Since curvature is intrinsically a local concept, it makes sense to only examine small neighborhoods when defining any curvature notion. On Riemannian manifolds, various definitions of curvature are constructed using differentials and derivatives on arbitrarily small neighborhoods. On a graph, the smallest neighborhood has radius 1, and so it is natural to consider the uniform 1-step random walk µ given by µu(p) = ( 1 deg u if p ∼ u, 0 otherwise. Hence, the Ollivier-Ricci curvature on graphs κ is defined by Equation (4), where W1(µu, µv) is the optimal value of the objective function in the linear optimization problem minimize X p∈Nu X q∈Nv d(p, q)π(p, q), subject to X p∈Nu π(p, q) = 1 deg v , (5) X q∈Nv π(p, q) = 1 deg u . Our analysis in Section 4 is based on this specific formula- tion of the Ollivier-Ricci curvature. 4. Analysis Based on Graph Curvature Throughout this section, we assume u ∼ v ∈ Vare neigh- boring vertices with deg u = n, deg v = m, and n ≥ m. We note that u ∼ v implies 0 ≤ d(p, q) ≤ 3 for all neigh- bors p, qof u, v, and so 0 ≤ W1(µu, µv) ≤ 3. From Equa- tion (4), the following bound holds −2 ≤ κ(u, v) ≤ 1. Hence, a curvature value close to 1 is considered very posi- tive, while a value close to −2 is considered very negative. To motivate our findings, we remark that the curvature κ(u, v) characterizes how well-connected the neighbor- hoods ˜N(u) and ˜N(v) are. Figure 3 illustrates how dif- ferent local graph structures give rise to different graph one must move the random walk from u in order to obtain the random walk from v. 4Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature u v κ(u, v) = ? u v κ(u, v) > 0 u v κ(u, v) = 0 u v κ(u, v) < 0 Figure 3.Different edge curvatures give rise to different local graph structures. curvature. Red and blue are used to color the neighborhoods ˜Nu\\{v} and ˜Nv\\{u}. The color violet is used to signal shared vertices or edges connecting from one neighborhood to the other. If the neighborhoods mostly coincide then the transport cost is very low, leading to a positive curvature value. In this case, messages can be transmitted freely and easily between both neighborhoods. In contrast, if the neigh- borhoods only have minimal connections then the transport cost is high, leading to a negative curvature value. Each such connection will then act as a bottleneck, limiting the effectiveness of the message-passing mechanism. 4.1. Positive Graph Curvature and Over-smoothing We identify the key connection between positive graph cur- vature and the occurrence of the over-smoothing issue. Lemma 4.1. The following inequality holds |Nu ∩ Nv| max(m, n) ≥ κ(u, v). Lemma 4.1 says that the curvature κ(u, v) is a lower bound for the proportion of shared neighbors between u and v. A closer inspection of Equation (1) reveals a fundamen- tal characteristic of GNNs: at the k-th layer, every node p broadcasts an identical message ψk(Xk p ) to each vertex u in its 1-hop neighborhood. These messages are then aggre- gated and used to update the features of u. If κ(u, v) is very positive then the neighborhoodsNu and Nv mostly coincide. Hence, they incorporate roughly the same information, and their variance diminishes. This gives us significant insight into why over-smoothing happens, and is made precise by the following theorem. Theorem 4.2. Consider the update rule given by Equa- tion (1). Suppose the edge curvature κ(u, v) > 0. For some k, assume the update function ϕk is L-Lipschitz, \f\fXk p \f\f ≤ C for all p ∈ N(u) ∪ N(v), and the message function ψk is bounded, i.e. |ψk(x)| ≤M|x|, ∀x. There exists a posi- tive function h : (0, 1) → R+ dependent on the constants L, M, C, nsatisfying • if L is the sum operation then h is constant; • if L is the mean operation then h is decreasing; such that \f\fXk+1 u − Xk+1 v \f\f ≤ (1 − κ(u, v))h(κ(u, v)). (6) In both cases, we clearly have lim x→1 (1 − x)h(x) = 0. (7) This result applies to a wide range of GNNs, including those in Table 1 with the exception of GAT, due to the fact that GAT employs the attention mechanism to create a learnable weighted mean function as the aggregator. Nevertheless, if the variance between attention weights are low, we expect the general behavior to still hold true. Theorem 4.2 conclusively shows that very positively curved edges force local node features to become similar. If the graph is positively curved everywhere or if it contains mul- tiple subgraphs having this characteristic, we can expect that the node features will quickly converge to indistinguish- able representations. In other words, positive edge curva- ture induces the mixing behavior observed by (Li et al., 2018a), causing over-smoothing to occur at a faster rate. This suggests the occurrence of over-smoothing in shal- low GNNs can be explained by an abundance of positively curved edges. Any global analysis of the issue based on local observations is hindered by the complexity in dealing with graph struc- tures. Nevertheless, by restricting our attention to a more manageable class of graphs - the class of regular graphs, we obtain Proposition 4.3. This serves to illustrate how posi- tive local graph curvature can affect the long term global behavior of a typical GNN. Proposition 4.3. Consider the update rule given by Equa- tion (1). Assume the graph is regular. Suppose there exists a constant δ > 0 such that for all edges (u, v) ∈ E, the curvature is bounded by κ(u, v) ≥ δ >0. For all k ≥ 1, assume the functions ϕk are L-Lipschitz, L is realised as the mean operation, \f\fX0 p \f\f ≤ C for all p ∈ V, and the message functions ψk are bounded linear operators, i.e. 5Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature |ψk(x)| ≤M|x|, ∀x. The following inequality holds for k ≥ 1 and any neighboring vertices u ∼ v \f\fXk u − Xk v \f\f ≤ 2 3C \u00123LM⌊(1 − δ)n⌋ n + 1 \u0013k . (8) Furthermore, for any u, v∈ Vthat are not necessarily neighbors, the following inequality holds \f\fXk u − Xk v \f\f ≤ 2 3 \u00162 δ \u0017 C \u00123LM⌊(1 − δ)n⌋ n + 1 \u0013k . (9) The conclusion of Proposition 4.3 says that if every edge curvature in a regular graph G is bounded from below by a sufficiently high constant δ then the difference between the features of any pair of neighboring nodes, or indeed, any pair of nodes at all, exponentially converges to0 in a typical GNN. This leads to the over-smoothing issue formulated in Equation (3) since for appropriate constants C1, C2 > 0, we have X (u,v)∈E \f\fXk u − Xk v \f\f ≤ C1e−C2k. In real-world graphs, it is often the case that not all edges in a graph is positively curved. Nevertheless, we expect an abundance of edges with overly high curvature will either cause or worsen the over-smoothing issue in GNNs. 4.2. Negative Graph Curvature and Over-squashing In this section, we demonstrate the intimate connection between negative graph curvature and the occurrence of local bottlenecks, which in turn causes over-squashing. Message-passing across local neighborhoods is facilitated by connections of the form (p, q) with p ∈ ˜Nu\\{v} and q ∈ ˜Nv\\{u}. As visualized by Figure 4a, such edges (col- ored in violet) provide information pathways between ˜Nu and ˜Nv. However, a large number of these edges concen- trated on a relatively few vertices will create new bottle- necks, instead of providing good message channels. Figure 4b illustrates this point, as there are way too many edges connecting to the emphasized node but too little edges con- necting between other neighbors. Since n ≥ m, there is a natural squashing of information as messages are transmit- ted from Nu to Nv of ratio n m . We identify the edges that provide good pathways as those that do not exacerbate this ratio and restrict our attention to these edges. We characterize the effect of edge curvature on graph bottle- necks in the following proposition. Proposition 4.4. Let ˜E be union of the edge set E with the set of all possible self-loops. Let S be the subset of ˜E containing edges of the form (p, q) with p ∈ ˜Nu\\{v} and u v (a) Violet edges connecting ˜Nu\\{v} and ˜Nv\\{u} serve as information pathways. u v (b) Edges may exacerbate the situation by creating new bot- tlenecks. Figure 4.Identifying connections that enable effective message- passing at local neighborhoods. q ∈ ˜Nv\\{u}. Supposing each vertex w is a vertex of at most n m edges in S. The following inequality holds |S| ≤n(κ(u, v) + 2) 2 . (10) Recall that the curvature is deemed very negative if it is close to −2. Proposition 4.4 shows that very negative edge curvature values prohibit the number of information path- ways from ˜Nu to ˜Nv, and very negatively curved edges induce local bottlenecks. This, in turn, contributes to the oc- currence of the over-squashing issue as proposed by (Alon & Yahav, 2021). We note that an adequate measure for the over-squashing issue is currently lacking in the literature (see Appendix B). Inspired by the influence distribution introduced by Xu et al. (2018), the next theorem asserts that negative edge curvature directly causes the decaying importance of distant nodes in GNNs with non-linearity removed. This demonstrates the effect of edge curvature on the over-squashing issue. Theorem 4.5. Consider the update rule given by Equa- tion (1). Suppose ψk, ϕk are linear operators for all k, andL is the sum operation. If u, vare neighboring vertices with neighborhoods as in Proposition 4.4 and S is defined similarly then for all p ∈ ˜Nu\\{v}, q ∈ ˜Nv\\{u}, we have \u0014∂Xk+2 u ∂Xkq \u0015 = α X w∈V \u0014∂Xk+2 u ∂Xkw \u0015 , \u0014∂Xk+2 v ∂Xkp \u0015 = β X w∈V \u0014∂Xk+2 v ∂Xkw \u0015 , (11) where \u0002y x \u0003 is used to denote the Jacobian of y with regard to x, and α, βsatisfy α ≤ |S| + 2P w∈ ˜Nv (deg(w) + 1), β ≤ |S| + 2P w∈ ˜Nu (deg(w) + 1). (12) To understand the meaning of Theorem 4.5, let us fix k = 0 and assume G is a regular graph with node degree n. 6Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Equations (11) and (12), along with Proposition 4.4, say that the contribution by the vertex q to the vertex u relative to the contribution of all other vertices, measured by the scaling term α in regard to the Jacobians, is bounded by α ≤ n(κ(u, v) + 2) + 4 2(n + 1)2 . Hence, if n is large and connections between the two neigh- borhoods are sparse, then κ(u, v) is very negative. In such a case, we expect that the messages from each node of Nv make up only 2 (n+1)2 of the total sum of information. They thus hardly have any effect on u, even when the distance be- tween them is only 2. An analogous result holds for the case when L is the mean operation without much modification. We have thus shown that negative curvature characterizes graph bottlenecks. As such, GNNs that operate on graphs with a large volume of negatively curved edges are expected to suffer from over-squashing. 5. BORF: Batch Ollivier-Ricci Flow Our theoretical results suggest the strikingly simple geo- metric connection between the over-smoothing and over- squashing issues: over-smoothing happens when there is a large proportion of edges with very positive curvature, while over-squashing occurs when there is a large proportion of edges with very negative curvature. As a natural exten- sion, we propose the Batch Ollivier-Ricci Flow (BORF), a graph rewiring algorithm capable of simultaneously miti- gating these issues by suppressing the over-smoothing and alleviateing the over-squashing inducing graph edges (see Algorithm 1). For each of N batches, BORF first finds the h edges (u1, v1), . . . ,(uh, vh) with minimal curvature and k edges (u1, v1), . . . ,(uk, vk) with maximal curvature within the graph. Note that we index the minimally curved and maxi- mally curved edges by subscripts and superscripts, respec- tively. Then, it tries to uniformly alleviate graph bottlenecks by adding connections to the set of h minimally curved edges. To save on computation time, BORF does not re- calculate the graph curvature within each batch. Instead, for each edge with minimal curvature (uj, vj), it reuses the already calculated optimal transport plan πj between µuj and µvj to decide which edge should be added. Recall that the formula for the optimal transport cost is W1(µuj , µvj ) = X (p,q) πj(p, q)d(p, q). Hence, to minimize the transport cost, it makes sense to rewire the two nodes that contribute the most to this sum. Specifically, we choose to add to G the edge (p∗, q∗) such that (p∗, q∗) = argmax πj(p, q)d(p, q). Algorithm 1 Batch Ollivier-Ricci Flow (BORF) Input: graph G = (V, E), # rewiring batches N, # edges added per batch h, # edges removed per batch k for i = 1 to N do Find h edges (u1, v1), . . . ,(uh, vh) with minimal Ollivier-Ricci curvature κ, along with each summand πj(p, q)d(p, q) in their optimal transportation cost sum for all p, q∈ Vand j = 1, h Find k edges (u1, v1), . . . ,(uk, vk) with maximal Ollivier-Ricci curvature κ for j = 1 to h do Add to G the edge (p∗, q∗) given by (p∗, q∗) = argmax πj(p, q)d(p, q) end for Remove edges (u1, v1), . . . ,(uk, vk) from G end for If there are multiple candidates, we arbitrarily choose one. Finally, BORF removes the k maximally curved edges (u1, v1), . . . ,(uk, vk) whose presence might prime the over- smoothing behavior to occur. With such design, BORF can effectively limit both ends of the curvature spectrum, simultaneously suppressing over- smoothing and alleviating over-squashing inducing connec- tions. Furthermore, depending on data characteristics, we may change the behaviours of BORF to either be a net edge add, net edge minus, or net zero rewiring algorithm. Thereby, BORF permits fine-tuned and fluid adjustments of the total number of edges and their curvature range. Computational complexity. Let R be the number of edges to be rewired, E = |E| be the number of edges in the graph, D be the maximal degree of vertices on the graph, andh+k be the number of edges rewired per batch by BORF. Solving the transportation problem to calculate the curvature for each edge using the network simplex algorithm is known to run in cubic time O(D3) (Ahuja et al., 1993). Once we have calculated the curvature of an edge, the optimal transport plan for that edge is already available for free. Hence, for each batch, the computational cost is O(ED3). The number of batches is R/(h + k). Thus, the total cost for BORF is O \u0000 RED3/(h + k) \u0001 for each graph in consideration. Many efficient methods for approximating the Wasserstein distance, which is the computational bottleneck in BORF, have been proposed. In particular, log-linear and linear time approximations for the Wasserstein distance have re- cently been developed in (Bonneel et al., 2014; Atasu & Mittelholzer, 2019), and some of them are already known to be topologically equivalent to the Wasserstein distance itself under appropriate assumptions (Bayraktar & Guo, 2021). By incorporating these approximations to calculate 7Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature the Ollivier-Ricci curvature, we may significantly reduce the computational cost of BORF. We leave studying such modifications as directions for future work. Other rewiring algorithms. SDRF (Topping et al., 2022) and FoSR (Karhadkar et al., 2023) are state-of-the-art rewiring algorithms for GNNs, designed with the purpose of alleviating the over-squashing issue. SDRF is based on the edge metric Balanced Forman curvature (BFC), which is actually a lower bound for the Ollivier-Ricci curvature. At its core, SDRF iteratively finds the edge with the lowest BFC, calculate the change in BFC for every possible edge that can be added, then add the one edge that affects the greatest change to the BFC of the aforementioned edge. On the other hand, FoSR is based on the heuristics that the spec- tral gap characterizes the connectivity of a graph. At each step, it approximates which missing edge would maximally improve the spectral gap and add that edge to the graph. Comparison to SDRF. BORF shares some similarities to SDRF, but with notable differences. Since SDRF is based on BFC, it can only accurately enforce a lower bound on the Ollivier-Ricci curvature. Furthermore, its design lacks the capability to be used as a net edge minus rewiring algorithm. As such, it is ill-equipped to deal with the over-smoothing issue or to be used on denser graphs. Another difference is BORF calculates the graph curvature very infrequently, while SDRF has to constantly recalculate for each possible new edge. Finally, by rewiring edges in batch, BORF affects a uniform change across the graph. This helps to preserve the graph topology and prevent the possibility that a small outlier subgraph gets continually rewired, while other parts of the graph do not see any geometrical improvement. Comparison to FoSR.Unlike BORF and SDRF, FoSR does not have the ability to remove edges. Hence, despite over- smoothing and over-squashing being problems on the two ends of the same spectrum, the algorithm is incapable of addressing the first issue. It is also very challenging to pre- dict where new edges will be added and what changes FoSR would make to the graph topology. This might complicate attempts by users to analyse the performance changes made by the algorithm. 6. Experiments In this section, we empirically verify the effectiveness of BORF on a variety of tasks compared to other rewiring alternatives. We seek to demonstrate the poten- tial of curvature-based rewiring methods, and more gen- erally, geometric-aware techniques in improving the per- formance of GNNs. Our codes for the experiments are available at https://github.com/hieubkvn123/ revisiting-gnn-curvature. Datasets. We conduct our experiments on a range of widely used node classification and graph classification tasks. For node classification, we report our results on the datasets CORA, CITESEER (Yang et al., 2016), TEXAS, COR- NELL, WISCONSIN (Pei et al., 2020) and CHAMELEON (Rozemberczki et al., 2021). For graph classification, we val- idate our method on the following benchmarks: ENZYMES, IMDB, MUTAG and PROTEINS from the TUDataset (Mor- ris et al., 2020). A summary of dataset statistics is available in Appendix F. Experiment details. We compare BORF to no graph rewiring and two other state-of-the-art rewiring methods: SDRF (Topping et al., 2022) and FoSR (Karhadkar et al., 2023). In designing our experiments, we prioritize fairness and comprehensiveness, rather than aiming to obtain the best possible performance for each dataset presented. We applied each method as a preprocessing step to all graphs in the datasets considered, before feeding the rewired graph data into a GNN to evaluate performance. For baseline GNNs, we employed the popular graph architectures GCN (Kipf & Welling, 2017) and GIN (Xu et al., 2019). For each task and baseline model, we used the same settings of GNN and optimization hyper-parameters across all rewiring methods to rule out hyper-parameter tuning as a source of performance gain. The setting for each rewiring option was obtained by tuning every hyper-parameter available for each method with the exception of the temperature τ of SDRF, which we set to ∞. Each configuration is evaluated using the validation set. The test set accuracy of the configura- tion with the best validation performance is then recorded. For each experiment, we accumulate the result across 100 random trials and report the mean test accuracy, along with the 95% confidence interval. Further experiment details are available in Appendix D. Results. Table 2 and Table 3 summarize our experiment re- sults for node classification and graph classification datasets, respectively. BORF outperforms all other methods in every node classification tasks on both GCN and GIN. It is worth mentioning that the 95% confidence interval of BORF is almost always smaller than other methods, indicating a con- sistent level of performance. This result agrees with what is expected since SDRF and FoSR are not suited for dealing with the over-smoothing issue, which heavily degrades the model’s performance on node classification tasks. On graph classification datasets, BORF achieves higher test accuracy compared to other rewiring options in most settings. Ablation study on edge addition/removal. To investigate the role of edge addition and removal, we compare BORF’s performance on GCN at high depths when using the best rewiring settings found in previous experiments against ver- sions of BORF where edges are only removed (only allevi- ates over-smoothing), edges are only added (only alleviates over-squashing), or edges are removed and added equally. 8Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 2.Classification accuracies of GCN and GIN with None, SDRF, FoSR, and BORF rewiring on various node classification datasets. Best results are highlighted in bold. GCN GIN DATA SET NONE SDRF F OSR BORF N ONE SDRF F OSR BORF CORA 86.7 ± 0.3 86 .3 ± 0.3 85 .9 ± 0.3 87.5 ±0.2 76.0 ± 0.6 74 .9 ± 0.1 75 .1 ± 0.8 78.4 ±0.4 CITESEER 72.3 ± 0.3 72 .6 ± 0.3 72 .3 ± 0.3 73.8 ±0.2 59.3 ± 0.9 60 .3 ± 0.8 61 .7 ± 0.7 63.1 ±0.8 TEXAS 44.2 ± 1.5 43 .9 ± 1.6 46 .0 ± 1.6 49.4 ±1.2 53.5 ± 3.1 50 .3 ± 3.7 47 .0 ± 3.7 63.1 ±1.7 CORNELL 41.5 ± 1.8 42 .2 ± 1.5 40 .2 ± 1.6 50.8 ±1.1 36.5 ± 2.2 40 .0 ± 2.1 35 .6 ± 2.4 48.6 ±1.2 WISCONSIN 44.6 ± 1.4 46 .2 ± 1.2 48 .3 ± 1.3 50.3 ±0.9 48.5 ± 2.2 48 .8 ± 1.9 48 .5 ± 2.1 54.9 ±1.2 CHAMELEON 59.2 ± 0.6 59 .4 ± 0.5 59 .3 ± 0.6 61.5 ±0.4 58.1 ± 2.1 58 .4 ± 2.1 56 .3 ± 2.2 65.3 ±0.8 Table 3.Classification accuracies of GCN and GIN with None, SDRF, FoSR, and BORF rewiring on various graph classification datasets. Best results are highlighted in bold. GCN GIN DATA SET NONE SDRF F OSR BORF N ONE SDRF F OSR BORF ENZYMES 25.5 ± 1.3 26 .1 ± 1.1 27.4 ±1.1 24.7 ± 1.0 31 .3 ± 1.2 33 .5 ± 1.3 25 .3 ± 1.2 35.5 ±1.2 IMDB 49.3 ± 1.0 49 .1 ± 0.9 49 .6 ± 0.8 50.1 ±0.9 69.0 ± 1.3 68 .6 ± 1.2 69 .5 ± 1.1 71.3 ±1.5 MUTAG 68.8 ± 2.1 70 .5 ± 2.1 75 .6 ± 1.7 75.8 ±1.9 75.5 ± 2.9 77 .3 ± 2.3 75 .2 ± 3.0 80.8 ±2.5 PROTEINS 70.6 ± 1.0 71 .4 ± 0.8 72.3 ±0.9 71.0 ± 0.8 69 .7 ± 1.0 72 .2 ± 0.9 74.2 ±0.8 71.3 ± 1.0 Table 4.Classification accuracies of GCN at depths 5, 7, and 9 with different BORF rewiring options on Cornell and Mutag datasets. DATA SET # LAYERS NONE BEST SETTINGS ONLY REMOVE ONLY ADD REMOVE & ADD EQUALLY CORNELL 5 41.3 ± 1.4 45 .5 ± 1.1 46 .4 ± 1.2 44 .7 ± 1.3 45 .9 ± 1.2 7 39.5 ± 1.7 41 .5 ± 1.5 43 .2 ± 1.3 42 .8 ± 1.4 41 .8 ± 1.3 9 35.5 ± 1.4 40 .9 ± 1.3 41 .9 ± 1.6 40 .3 ± 2.0 39 .9 ± 1.6 MUTAG 5 67.7 ± 1.6 75 .4 ± 2.1 68 .5 ± 2.8 76 .1 ± 2.2 71 .8 ± 1.2 7 64.1 ± 2.1 72 .1 ± 1.3 65 .1 ± 1.5 75 .2 ± 2.4 66 .2 ± 1.9 9 63.1 ± 1.2 69 .7 ± 1.5 60 .7 ± 2.5 70 .4 ± 1.7 61 .3 ± 1.5 Further experimental details are described in Section E.1. We report our experimental results in Table 4. At all depths, the GNN can learn relevant information from the datasets preprocessed with BORF more effectively, even when the rewiring algorithm is constrained to only alleviate either over-smoothing or over-squashing. Indeed, the un- rewired datasets record the worst performance in 5 out of 6 scenarios considered. This indicates that both issues nega- tively impact the classification accuracy as the depth count increases. Our results suggest both BORF’s abilities to re- duce over-smoothing and over-squashing play an essential role in improving the performance of GNN models. 7. Conclusion In this paper, we established the novel correspondence be- tween the Ollivier-Ricci curvature on graphs with both the over-smoothing and the over-squashing issues. In specific, we showed that positive graph curvature is associated with over-smoothing, while negative graph curvature is associ- ated with over-squashing. Based on our theoretical results, we proposed Batch Ollivier-Ricci Flow, a novel curvature based rewiring method that can effectively improve GNN performance by tackling both the over-smoothing and over- squashing problems at the same time. It is interesting to note that by different definitions of the random walk µu, we may be able to capture different behaviors of the local graph structures using graph curvature. Limitations and societal impacts. Our current formulation only demonstrates that the over-squashing issue is related to negative Ollivier-Ricci curvature at a local scale under some constraints on the graph structure and GNN design. We leave providing a more general treatment of the problem for future work. Our theory and proposed method have no discernible negative societal impact. Acknowledgements This material is based on research sponsored by the NSF Grant# 2030859 to the Computing Research Association for the CIFellows Project (CIF2020-UCLA-38). SJO acknowl- edges support from the ONR N00014-20-1-2093/N00014- 20-1-2787 and the NSF DMS 2208272 and 1952339. 9Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature References Ahuja, R., Magnanti, T., and Orlin, J. Network Flows: Theory, Algorithms, and Applications . Prentice Hall, 1993. ISBN 9780136175490. Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In In- ternational Conference on Learning Representations , 2021. URL https://openreview.net/forum? id=i80OPhOCVH2. Atasu, K. and Mittelholzer, T. Linear-complexity data- parallel earth mover’s distance approximations. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed- ings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learn- ing Research, pp. 364–373. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ atasu19a.html. Bakry, D. and ´Emery, M. Diffusions hypercontractives. In Az´ema, J. and Yor, M. (eds.),S´eminaire de Probabilit´es XIX 1983/84 , pp. 177–206, Berlin, Heidelberg, 1985. Springer Berlin Heidelberg. ISBN 978-3-540-39397-9. Banerjee, P. K., Karhadkar, K., Wang, Y . G., Alon, U., and Mont´ufar, G. Oversquashing in GNNs through the lens of information contraction and graph expansion. In 2022 58th Annual Allerton Conference on Communica- tion, Control, and Computing (Allerton), pp. 1–8, 2022. Battaglia, P., Pascanu, R., Lai, M., Jimenez Rezende, D., and kavukcuoglu, k. Interaction networks for learning about objects, relations and physics. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips. cc/paper_files/paper/2016/file/ 3147da8ab4a0437c15ef51a5cc7f2dc4-Paper. pdf. Bayraktar, E. and Guo, G. Strong equivalence between metrics of Wasserstein type. Electronic Communications in Probability, 26:1 – 13, 2021. doi: 10.1214/21-ECP383. URL https://doi.org/10.1214/21-ECP383. Bober, J., Monod, A., Saucan, E., and Webster, K. N. Rewiring networks for graph neural network training us- ing discrete geometry. arXiv preprint arXiv:2207.08026, 2022. Bojchevski, A., Shchur, O., Z ¨ugner, D., and G ¨unnemann, S. NetGAN: Generating graphs via random walks. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learn- ing, volume 80 of Proceedings of Machine Learn- ing Research , pp. 610–619. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/ bojchevski18a.html. Bonneel, N., Rabin, J., Peyre, G., and Pfister, H. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 2014. Bourne, D. P., Cushing, D., Liu, S., M ¨unch, F., and Peyerimhoff, N. Ollivier–Ricci idleness functions of graphs. SIAM Journal on Discrete Mathematics, 32(2): 1408–1424, 2018. doi: 10.1137/17M1134469. URL https://doi.org/10.1137/17M1134469. Bronstein, M. M., Bruna, J., Cohen, T., and Veli ˇckovi´c, P. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges . arXiv, 2021. doi: 10.48550/ ARXIV .2104.13478. URL https://arxiv.org/ abs/2104.13478. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318, 2020. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduc- tion. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learn- ing, volume 80 of Proceedings of Machine Learn- ing Research, pp. 942–950. PMLR, 10–15 Jul 2018a. URL https://proceedings.mlr.press/v80/ chen18p.html. Chen, Y ., Wei, Z., and Huang, X. Incorporating corpo- ration relationship via graph convolutional neural net- works for stock price prediction. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM ’18, pp. 1655–1658, New York, NY , USA, 2018b. Association for Com- puting Machinery. ISBN 9781450360142. doi: 10. 1145/3269206.3269269. URL https://doi.org/ 10.1145/3269206.3269269. Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P. Convolutional networks on graphs for learning molecular fingerprints. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28. Curran As- sociates, Inc., 2015. URL https://proceedings. neurips.cc/paper/2015/file/ f9be311e65d81a9ad8150a60844bb94c-Paper. pdf. Dwivedi, V . P., Ramp´aˇsek, L., Galkin, M., Parviz, A., Wolf, G., Luu, A. T., and Beaini, D. Long range graph bench- 10Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature mark. In Koyejo, S., Mohamed, S., Agarwal, A., Bel- grave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 22326– 22340. Curran Associates, Inc., 2022. Estrada, E. and Bonchev, D. Chemical Graph Theory, pp. 1538–1558. Discrete Mathematics and Its Applications Series. Taylor & Francis, 12 2013. ISBN 9781439880180. doi: 10.1201/b16132-92. Fan, W., Ma, Y ., Li, Q., He, Y ., Zhao, E., Tang, J., and Yin, D. Graph neural networks for social recommen- dation. In The World Wide Web Conference , WWW ’19, pp. 417–426, New York, NY , USA, 2019. Associa- tion for Computing Machinery. ISBN 9781450366748. doi: 10.1145/3308558.3313488. URL https://doi. org/10.1145/3308558.3313488. Fey, M. and Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Repre- sentation Learning on Graphs and Manifolds, 2019. Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Bois- bunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V ., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. POT: Python optimal transport. Journal of Ma- chine Learning Research, 22(78):1–8, 2021. URL http: //jmlr.org/papers/v22/20-451.html. Forman. Bochner’s method for cell complexes and com- binatorial Ricci curvature. Discrete & Computational Geometry, 29(3):323–374, Feb 2003. ISSN 1432-0444. doi: 10.1007/s00454-002-0743-x. URL https:// doi.org/10.1007/s00454-002-0743-x . Gasteiger, J., Weiß enberger, S., and G ¨unnemann, S. Diffusion improves graph learning. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 23c894276a2c5a16470e6a31f4618d73-Paper. pdf. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In Precup, D. and Teh, Y . W. (eds.), Proceed- ings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learn- ing Research, pp. 1263–1272. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/ gilmer17a.html. Hamilton, W., Ying, Z., and Leskovec, J. Inductive repre- sentation learning on large graphs. In Guyon, I., Luxburg, U. V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.),Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper. pdf. Harary, F. Graph Theory and Theoretical Physics . Aca- demic Press, 1967. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., Fern´andez del R´ıo, J., Wiebe, M., Peterson, P., G ´erard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585:357–362, 2020. doi: 10.1038/ s41586-020-2649-2. Hsu, L. and Lin, C. Graph Theory and Interconnection Networks. CRC Press, 2008. ISBN 9781420044829. Karhadkar, K., Banerjee, P. K., and Mont ´ufar, G. FoSR: First-order spectral rewiring for addressing oversquash- ing in GNNs. In International Conference on Learning Representations, 2023. Kipf, T. and Welling, M. Semi-supervised classification with graph convolutional networks. In International Con- ference on Learning Representations, 2017. Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. Neural relational inference for interacting systems. In Dy, J. and Krause, A. (eds.), Proceed- ings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learn- ing Research, pp. 2688–2697. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/ kipf18a.html. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the Thirty-Second AAAI Conference on Ar- tificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelli- gence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018a. ISBN 978-1-57735-800-8. Li, Y ., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P. Learning deep generative models of graphs. arXiv preprint arXiv:2207.08026, 2018b. 11Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Luan, S., Zhao, M., Chang, X.-W., and Precup, D. Break the ceiling: Stronger multi-scale deep graph convolutional networks. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Ad- vances in Neural Information Processing Systems , vol- ume 32. Curran Associates, Inc., 2019. Matsunaga, D., Suzumura, T., and Takahashi, T. Ex- ploring graph neural networks for stock market pre- dictions with rolling window analysis. arXiv preprint arXiv:1909.10660, 2019. Morris, C., Kriege, N. M., Bause, F., Kersting, K., Mutzel, P., and Neumann, M. Tudataset: A collection of bench- mark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020) , 2020. URL www.graphlearning. io. Ni, C.-C., Lin, Y .-Y ., Gao, J., David Gu, X., and Saucan, E. Ricci curvature of the Internet topology. In 2015 IEEE Conference on Computer Communications (INFOCOM), pp. 2758–2766, 2015. doi: 10.1109/INFOCOM.2015. 7218668. Ollivier, Y . Ricci curvature of Markov chains on metric spaces. Journal of Functional Analysis, 256(3):810–864, 2009. ISSN 0022-1236. doi: 10.1016/j.jfa.2008.11.001. Oono, K. and Suzuki, T. Graph neural networks exponen- tially lose expressive power for node classification. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=S1ldO2EFPr. Paeng, S.-H. V olume and diameter of a graph and Ollivier’s Ricci curvature. Eur. J. Comb., 33:1808–1819, 2012. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai- son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Wallach, H., Larochelle, H., Beygelzimer, A., d’Alch ´e Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Infor- mation Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-GCN: Geometric graph convolutional networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=S1e2agrFvS. Rong, Y ., Huang, W., Xu, T., and Huang, J. DropEdge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=Hkx1qkrKPr. Rozemberczki, B., Allen, C., and Sarkar, R. Multi-scale attributed node embedding. Journal of Complex Net- works, 9(2), 05 2021. ISSN 2051-1329. doi: 10. 1093/comnet/cnab014. URL https://doi.org/10. 1093/comnet/cnab014. Rusch, T. K., Chamberlain, B., Rowbottom, J., Mishra, S., and Bronstein, M. Graph-coupled oscillator net- works. In Chaudhuri, K., Jegelka, S., Song, L., Szepes- vari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learn- ing, volume 162 of Proceedings of Machine Learn- ing Research , pp. 18888–18909. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/ v162/rusch22a.html. Sanchez-Gonzalez, A., Heess, N., Springenberg, J. T., Merel, J., Riedmiller, M., Hadsell, R., and Battaglia, P. Graph networks as learnable physics engines for inference and control. In Dy, J. and Krause, A. (eds.), Proceed- ings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learn- ing Research, pp. 4470–4479. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/ sanchez-gonzalez18a.html. Scaman, K. and Virmaux, A. Lipschitz regularity of deep neural networks: analysis and efficient esti- mation. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Process- ing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips. cc/paper_files/paper/2018/file/ d54e99a6c03704e95e6965532dec148b-Paper. pdf. Shang, C., Tang, Y ., Huang, J., Bi, J., He, X., and Zhou, B. End-to-end structure-aware convolutional net- works for knowledge base completion. Proceedings of the AAAI Conference on Artificial Intelligence , 33 (01):3060–3067, Jul. 2019. doi: 10.1609/aaai.v33i01. 33013060. URL https://ojs.aaai.org/index. php/AAAI/article/view/4164. Shi, C., Xu, M., Zhu, Z., Zhang, W., Zhang, M., and Tang, J. GraphAF: a flow-based autoregressive model for molecular graph generation. In International Confer- ence on Learning Representations, 2020. URL https: //openreview.net/forum?id=S1esMkHYPr. Sia, J., Jonckheere, E., and Bogdan, P. Ollivier- Ricci curvature-based method to community detec- 12Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature tion in complex networks. Scientific Reports , 9(1): 9800, Jul 2019. ISSN 2045-2322. doi: 10.1038/ s41598-019-46079-x. URL https://doi.org/10. 1038/s41598-019-46079-x . Tantau, T.The TikZ and PGF Packages, 2023. URL https: //tikz.dev/. Topping, J., Giovanni, F. D., Chamberlain, B. P., Dong, X., and Bronstein, M. M. Understanding over-squashing and bottlenecks on graphs via curvature. In In- ternational Conference on Learning Representations , 2022. URL https://openreview.net/forum? id=7UmjRGzp-A. van den Berg, R., Kipf, T. N., and Welling, M. Graph convolutional matrix completion. arXiv preprint arXiv:1706.02263, 2017. van der Hoorn, P., Cunningham, W. J., Lippner, G., Tru- genberger, C., and Krioukov, D. Ollivier-Ricci curvature convergence in random geometric graphs.Phys. Rev. Res., 3:013211, Mar 2021. doi: 10.1103/PhysRevResearch.3. 013211. URL https://link.aps.org/doi/10. 1103/PhysRevResearch.3.013211. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V ., Bengio, S., Wallach, H., Fergus, R., Vish- wanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As- sociates, Inc., 2017. URL https://proceedings. neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In- ternational Conference on Learning Representations , 2018. URL https://openreview.net/forum? id=rJXMpikCZ. Wu, Q., Zhang, H., Gao, X., He, P., Weng, P., Gao, H., and Chen, G. Dual graph attention networks for deep latent representation of multifaceted social ef- fects in recommender systems. In The World Wide Web Conference , WWW ’19, pp. 2091–2102, New York, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450366748. doi: 10.1145/ 3308558.3313442. URL https://doi.org/10. 1145/3308558.3313442. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.- i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Confer- ence on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5453–5462. PMLR, 10–15 Jul 2018. URLhttps://proceedings.mlr. press/v80/xu18c.html. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https:// openreview.net/forum?id=ryGs6iA5Km. Yang, Y ., Wei, Z., Chen, Q., and Wu, L. Using ex- ternal knowledge for financial event prediction based on graph neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM ’19, pp. 2161–2164, New York, NY , USA, 2019. Association for Comput- ing Machinery. ISBN 9781450369763. doi: 10.1145/ 3357384.3358156. URL https://doi.org/10. 1145/3357384.3358156. Yang, Z., Cohen, W., and Salakhudinov, R. Revisiting semi-supervised learning with graph embeddings. In Bal- can, M. F. and Weinberger, K. Q. (eds.),Proceedings of The 33rd International Conference on Machine Learn- ing, volume 48 of Proceedings of Machine Learning Re- search, pp. 40–48, New York, New York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.mlr. press/v48/yanga16.html. Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., and Leskovec, J. Graph convolutional neural net- works for web-scale recommender systems. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–983, 2018. Zhang, F., Liu, X., Tang, J., Dong, Y ., Yao, P., Zhang, J., Gu, X., Wang, Y ., Shao, B., Li, R., and Wang, K. OAG: To- ward linking large-scale heterogeneous entity graphs. In Proceedings of the 25th ACM SIGKDD International Con- ference on Knowledge Discovery & Data Mining, KDD ’19, pp. 2585–2595, New York, NY , USA, 2019. Associa- tion for Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330785. URL https://doi. org/10.1145/3292500.3330785. Zhao, L. and Akoglu, L. PairNorm: Tackling oversmooth- ing in GNNs. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=rkecl1rtwB. 13Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Supplement for “Revisiting Over-smoothing and Over-squashing using Ollivier-Ricci Curvature” In this supplementary material, we first present how Equation (1) accommodates different designs of GNNs in Appendix A. Then, we discuss the lack of an appropriate measure for the over-squashing issue in Appendix B. Skipped proofs within the main text are provided in Appendix C. We present our experiment settings in Appendix D and additional experiment results in Appendix E. Dataset statistics are available in Appendix F. Finally, we provide our hardware specifications and list of libraries in Appendix G. A. Message Passing Neural Networks In its most general form, a typical layer of a message passing neural network is given by the following update rule (Bronstein et al., 2021): Hu = ϕ   Xu, M v∈Nu Λ(Xu, Xv) ! . Here, Λ , L and ϕ are the message, aggregate and update functions. Different designs of MPNNs amount to different choices for these functions. The additional input of Xu to ϕ represents an optional skip-connection. In practice, we found that the skip connection of each vertex u is often implemented by considering a message passing scheme where each node sends a message to itself. This can be thought of as adding self-loops to the graph, and its impact has been studied by Xu et al. (2018) and Topping et al., (2022). Then,Λ could be realized as a learnable affine transformation ψ of Xv, the aggregating function L could either be chosen as a sum, mean, weighted mean, or max operation, and ϕ is a suitable activation function. Hence, we arrive at Equation (1), which we restate below Xk+1 u = ϕk   M p∈ ˜Nu ψk(Xk p )  . For example, graph convolutional network (GCN) (Kipf & Welling, 2017) defines its layer as Hu = σ   X v∈ ˜Nu 1 cuv WXv  , with cuv = q | ˜Nu|| ˜Nv|. The mean aggregator variant (but not other variants) of GraphSAGE (Hamilton et al., 2017) uses the same formulation but with cuv = | ˜Nu|. Both choices of cuv have the exact same spectral characteristics and act identically in theory (Li et al., 2018a), which leads to an averaging behavior based on node degrees. Similarly, graph attention networks (GAT) (Veliˇckovi´c et al., 2018) defines its single head layer as Hu = σ   X v∈ ˜Nu auvWXv  . The difference here being auv is now a learnable function given by the attention mechanism (Vaswani et al., 2017). Finally, graph isomorphism network (GIN) (Xu et al., 2019) is formulated by Hu = MLP   (1 + ϵ)Xu + X v∈Nu Xv ! , where MLP is a multilayer perceptron. GIN achieves its best performance with the model GIN-0, in which ϵ is set to be 0. We remark that most nonlinear activation functions such as ReLU, Leaky ReLU, Tanh, Sigmoid, softmax, etc., has a simple and explicit Lipschitz-constant (which equals to 1 more often than not) (Scaman & Virmaux, 2018). 14Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature B. Measuring Over-squashing It is intuitive to think that if N vertices in ˜Nu contribute to the feature representation of some vertex u by the permutation invariant update rule 1, we should expect each such vertex to provide 1 N of the total contribution. If this is repeated over and over, such as in a tree, the exponentially decaying dependence of distant vertices is to be expected. However, quantifying this phenomenon is actually quite difficult. The first work to call attention to the over-squashing problem (Alon & Yahav, 2021) measures whether breaking the bottleneck improves the results of long-range problems, instead of measuring the decaying importance itself. A definition for the over-squashing issue that took inspiration from the vanishing gradient problem was given by (Topping et al., 2022), where it was suggested that ∂Xk u ∂X0v can be used to evaluate the decreasing importance of distant vertex v to u. However, this quantity does not actually measure the squashing behavior experienced by all GNNs, but only by those using an aggregation function with a natural decaying effect such as GCNs. Furthermore, we argue that in Theorem 4 of (Topping et al., 2022), there is no general squashing behavior being described. Given some negatively curved edge(i, j), the conclusion of the theorem provides a bound for the average effect of messages from the node i to a set of neighbors Qj of the node j. In other words, this describes how one single node on one specific side of the edge (the side with the lower degree) can transmit messages to other nodes and not the other way around. To visualize this, let us look at Figure 4a. The theorem measures how effectively the red node u can send messages to a number of blue nodes in the neighborhood of v. Clearly, there is no squashing of information from the surrounding receptive field being considered: the message was sent from one single node, and there is no other message to be squashed. In reality, this theorem is describing the average dilution of information sent out by the node i to other nodes k of distance 2 from i. This dilution is caused by the choice of using the normalized adjacency matrix as the aggregating function. If we use the sum aggregating function as in GIN, then such diluting process does not take place. Clearly, it is the relative importance of a vertex compared to the contribution of all other vertices that is at the heart of the matter. To this end, we have found that the closest notion to our description actually predates the observation of the over-squashing issue. Xu et al. (2018) introduced the notion of influence distribution to quantify the relative importance of vertices to each other. It is defined as Iu(v) = sum \u0010h ∂Xk u ∂X0v i\u0011 P p∈V sum \u0010h ∂Xku ∂X0p i\u0011 where the sum is taken over all entries of each Jacobian matrix. Unfortunately, this definition is quite unwieldy to use in any sort of analysis. We would like to remark that the theoretical proofs in (Xu et al., 2018) are only partially correct. They have made a mistake by claiming E \u0012 X1Pn i=1 Xi \u0013 = E(X1)Pn i=1 E(Xi) for random variables Xi. C. Proofs In this Appendix, we provide proofs for key results in the paper. We state without proof the following lemma, which is Lemma 4.1 in (Bourne et al., 2018). Lemma C.1. Let µ1, µ2 be probability measures on a space V . Then there exists an optimal transport plan π transporting µ1 to µ2 with the following property: For all x ∈ V with µ1(x) ≤ µ2(x), we have π(x, x) = µ1(x). C.1. Proof of Lemma 4.1 Proof. Without loss of generality, assume n ≥ m. Let π be an optimal transport plan between µu and µv satisfying the condition in Lemma C.1. That is, π(p, p) = 1 n for all p ∈ N(u) ∩ N(v). We have W1(µu, µv) = X p∈Nu X q∈Nv π(p, q)d(p, q) = X (p,q)∈Nu×Nv p̸=q π(p, q)d(p, q) + X p∈N(u)∩N(v) π(p, p)d(p, p). 15Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature It is obvious that d(p, p) = 0 for any vertex p and d(p, q) ≥ 1 for any vertices p ̸= q. We have W1(µu, µv) ≥ X (p,q)∈Nu×Nv p̸=q π(p, q) + 0 = 1 − X p∈Nu∩Nv π(p, p) = 1 − |Nu ∩ Nv| n . Hence, we have |Nu ∩ Nv| max(m, n) ≥ 1 − W1(µu, µv) = κ(u, v). C.2. Proof of Theorem 4.2 As ϕk is L-Lipschitz, we have \f\fXk+1 u − Xk+1 v \f\f = \f\f\f\f\f\f ϕk   M p∈ ˜Nu ψk(Xk p )   − ϕk   M q∈ ˜Nv ψk(Xk q )   \f\f\f\f\f\f ≤ L \f\f\f\f\f\f M p∈ ˜Nu ψk(Xk p ) − M q∈ ˜Nv ψk(Xk q ) \f\f\f\f\f\f . (13) Theorem 4.1 tells us that | ˜Nv\\ ˜Nu| ≤ |˜Nu\\ ˜Nv| = n + 1 − |Nu ∩ Nv| −2 ≤ n − nκ(u, v). Hence, there are at most ⌊(1 − κ(u, v))n⌋ vertices in the extended neighborhood of u that is not present in the extended neighborhood of v and vice versa. The symmetric difference ˜Nu △ ˜Nv satisfies | ˜Nu △ ˜Nv| = |( ˜Nu\\ ˜Nv) ∪ ( ˜Nv\\ ˜Nu)| ≤2(1 − κ(u, v))n. • If L is realized as the sum operation, we obtain from Equation (13) \f\fXk+1 u − Xk+1 v \f\f ≤ L \f\f\f\f\f\f X p∈Nu∪{u} ψk(Xk p ) − X q∈Nv∪{v} ψk(Xk q ) \f\f\f\f\f\f = L \f\f\f\f\f\f X p∈ ˜Nu\\ ˜Nv ψk(Xk p ) − X q∈ ˜Nv\\ ˜Nu ψk(Xk q ) \f\f\f\f\f\f ≤ L X p∈ ˜Nu△ ˜Nv \f\fψk(Xk p ) \f\f ≤ (1 − κ(u, v))2LCMn. We can now set h ≡ 2LCMn . 16Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature • If L is realized as the mean operation, we obtain from Equation (13) \f\fXk+1 u − Xk+1 v \f\f ≤ L \f\f\f\f\f\f X p∈ ˜Nu 1 n + 1ψk(Xk p ) − X q∈ ˜Nv 1 m + 1ψk(Xk q ) \f\f\f\f\f\f ≤ L X p∈( ˜Nu∩ ˜Nv) \u0012 1 m + 1 − 1 n + 1 \u0013\f\fψk(Xk p ) \f\f + L \f\f\f\f\f\f X p∈ ˜Nu\\ ˜Nv 1 n + 1ψk(Xk p ) − X q∈ ˜Nv\\ ˜Nu 1 m + 1ψk(Xk q ) \f\f\f\f\f\f . (14) We have n ≥ m = |Nv| ≥ |Nu ∩ Nv| ≥κ(u, v)n, and 1 m + 1 − 1 n + 1 ≤ 1 m − 1 n ≤ 1 κ(u, v)n − 1 n = 1 − κ(u, v) κ(u, v)n . Therefore, Equation (14) gives \f\fXk+1 u − Xk+1 v \f\f ≤ L X p∈ ˜Nu∩ ˜Nv 1 − κ(u, v) κ(u, v)n \f\fψk(Xk p ) \f\f + L X p∈ ˜Nu△ ˜Nv 1 κ(u, v)n + 1 \f\fψk(Xk p ) \f\f ≤ L(n + 1)1 − κ(u, v) κ(u, v)n CM + 2(1 − κ(u, v))nL 1 κ(u, v)n + 1CM ≤ (1 − κ(u, v))LCM \u0012 n + 1 κ(u, v)n + 2 n κ(u, v)n + 1 \u0013 . We can now set h(x) = LCM (n+1 xn + 2 n nx+1 ). Clearly, the functions h as defined satisfy the conditions given in Theorem 4.2. C.3. Proof of Proposition 4.3 We will use proof by induction. For all edgesu ∼ v, repeating the argument in Theorem 4.2, we get| ˜Nu △ ˜Nv| ≤2(1−δ)n. Then, the base case k = 1 follows since |X1 u − X1 v | = \f\f\f\f\f\f ϕ1   1 n + 1 X p∈ ˜Nu ψ(Xp)   − ϕ1   1 n + 1 X q∈ ˜Nv ψ(Xq)   \f\f\f\f\f\f ≤ L \f\f\f\f\f\f 1 n + 1 X p∈ ˜Nu ψ(Xp) − 1 n + 1 X q∈ ˜Nv ψ(Xq) \f\f\f\f\f\f = L n + 1 \f\f\f\f\f\f X p∈ ˜Nu\\ ˜Nv ψ(Xp) − X q∈ ˜Nv\\ ˜Nu ψ(Xq) \f\f\f\f\f\f ≤ L n + 1 X p∈ ˜Nu△ ˜Nv |ψ(Xp)| ≤ 2⌊(1 − δ)n⌋ n + 1 LCM. 17Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Suppose the statement is true for k and consider the case k + 1. We have for all u ∼ v: \f\fXk+1 u − Xk+1 v \f\f ≤ L 1 n + 1 \f\f\f\f\f\f X p∈ ˜Nu ψk(Xk p ) − X q∈ ˜Nv ψk(Xk q ) \f\f\f\f\f\f = L 1 n + 1 \f\f\f\f\f\f X p∈ ˜Nu\\ ˜Nv ψk(Xk p ) − X q∈ ˜Nv\\ ˜Nu ψk(Xk q ) \f\f\f\f\f\f = L 1 n + 1 \f\f\f\f\f\f ψk   X p∈ ˜Nu\\ ˜Nv Xk p − X q∈ ˜Nv\\ ˜Nu Xk q   \f\f\f\f\f\f ≤ LM 1 n + 1 \f\f\f\f\f\f X p∈ ˜Nu\\ ˜Nv Xk p − X q∈ ˜Nv\\ ˜Nu Xk q \f\f\f\f\f\f . (15) For each p ∈ ˜Nu\\ ˜Nv, match it with one and only one q ∈ ˜Nv\\ ˜Nu. For any node pair (p, q), they are connected by the path p ∼ u ∼ v ∼ q, where the difference in norm of features at layerk of each 1-hop connection is at most 2 3 C \u0010 3LM⌊(1−δ)n⌋ n+1 \u0011k . Hence, we have |Xk p − Xk q | ≤2C \u00123LM⌊(1 − δ)n⌋ n + 1 \u0013k . Substituting this into equation (15), and by noting that there are at most ⌊(1 − δ)n⌋ pairs, we get |Xk+1 u − Xk+1 v | ≤LM 1 n + 1 X (p,q) \f\fXk p − Xk q \f\f ≤ LM 1 n + 1⌊(1 − δ)n⌋2C \u00123LM⌊(1 − δ)n⌋ n + 1 \u0013k = 2 3C \u00123LM⌊(1 − δ)n⌋ n + 1 \u0013k+1 . By induction, we have shown inequality (8) holds for all k ≥ 1 and u ∼ v. It is known that if the curvature of every edge in a graph is positive and bounded away from zero by δ >0 then the diameter of the graph does not exceed ⌊2/δ⌋ (Paeng, 2012). Hence, for any two nodes u, v∈ V, the shortest path between them is of length at most ⌊2/δ⌋. Apply the inequality (8) for each pair of neighboring nodes on this shortest path, we obtain the inequality (9). C.4. Proof of Proposition 4.4 Note that S consists of elements of the form (p, q) where either p = q or p ̸= q. The first type corresponds to mutual neighbors of u, v, while the second type corresponds to neighbors of u, vthat share an edge. Denote the number of edges of the first type as n0 and the number of edges of the second type as n1. A transport plan π between µu and µv can be obtained as followed. • For every vertexp such that (p, p) ∈ S, the mass of µu(p) = 1 n remains in place at p with cost π(p, p)×0 = 1 n ×0 = 0 • For each edge (p, q) ∈ S with p ̸= q, transport the mass of 1 n from p to q with cost π(p, q) × 1 = 1 n × 1 = 1 n . The assumption that each vertex w is a vertex of at most n m edges ensures that the total mass transported to each vertex is no greater than 1 m . • The remaining mass is 1 − n0 1 n − n1 1 n . Transport this amount arbitrarily to obtain a valid optimal transport plan. This transport plan has cost X p∈ ˜Nu X q∈ ˜Nv π(p, q)d(p, q) ≤ n00 + n1 1 n + \u0012 1 − n0 1 n − n1 1 n \u0013 3 = 3 − 3n0 1 n − 2n1 1 n. 18Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 5.SDRF’s best hyper-parameter settings. GCN GIN DATASET # ITERATION C+ # REWIRED # ITERATION C+ # REWIRED CORA 12 0 24 50 ∞ 50 CITESEER 175 ∞ 175 25 ∞ 25 TEXAS 87 0 174 37 0 74 CORNELL 100 0 200 25 0 50 WISCONSIN 25 0 50 150 ∞ 150 CHAMELEON 50 0 100 87 0 174 ENZYMES 15 0 30 5 0 10 IMDB 10 0 20 10 ∞ 10 MUTAG 20 ∞ 20 10 0 20 PROTEINS 5 0 10 15 0 30 We have κ(u, v) = 1 − W1(µu, µv) ≥ 1 − \u0012 3 − 3n0 1 n − 2n1 1 n \u0013 = −2 + 3n0 + 2n1 n . Therefore, we obtain |S| = n0 + n1 ≤ n 2 3n0 + 2n1 n ≤ nκ(u, v) + 2 2 . We can observe from the proof that a stronger result holds: 3n0 + 2n1 ≤ n(κ(u, v) + 2). C.5. Proof of Theorem 4.5 Since ϕk and ψk are linear operators for all k, their Jacobians Jϕk , Jψk are constant matrices. By inspection of Equation (1), we see that a vertex w ∈ V can only transmit a message to u if there exists a vertex w′ such that w′ ∈ ˜Nu ∩ ˜Nw. Moreover, the chain rule gives \u0014∂Xk+2 u ∂Xkw \u0015 = X w′∈ ˜Nu∩ ˜Nw \" ∂Xk+2 u ∂Xk+1 w′ #\" ∂Xk+1 w′ ∂Xkw # = X w′∈ ˜Nu∩ ˜Nw Jϕk+1 Jψk+1 Jϕk Jψk . Therefore, h ∂Xk+2 u ∂Xkw i is the number of distinct paths (that might contain self-loops) from w to u times Jϕk+1 Jψk+1 Jϕk Jψk . The number of distinct paths without self-loops from q ∈ ˜Nv\\{u} to u is not greater than |S| as defined in Proposition 4.4. With self-loops, this rises to at most |S| + 2, which corresponds to the case where q ∼ u. On the other hand, P w∈V h ∂Xk+2 u ∂Xkw i equals the number of distinct paths with self-loops with one end at u times Jϕk+1 Jψk+1 Jϕk Jψk . Clearly, we have X w∈V \u0014∂Xk+2 u ∂Xkw \u0015 =   X w∈ ˜Nu (deg(w) + 1)  Jϕk+1 Jψk+1 Jϕk Jψk . Let α = |S| + 2P w∈ ˜Nu (deg(w) + 1), then Proposition 4.4 gives us the required inequality. We can chooseβ by the same process. D. Experiment Settings D.1. Rewiring hyper-parameters We report the best rewiring settings for every task and baseline GNN architecture. For SDRF, we set the temperature τ = ∞ and only tuned the Ric upper bound C+ and iteration count. For FoSR, we tuned the iteration count. For BORF, we 19Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 6.FoSR’s best iteration count settings. DATASET GCN GIN CORA 150 50 CITESEER 100 200 TEXAS 50 150 CORNELL 125 75 WISCONSIN 175 25 CHAMELEON 50 25 ENZYMES 40 5 IMDB 5 20 MUTAG 10 20 PROTEINS 30 10 Table 7.BORF’s best hyper-parameter settings. GCN GIN DATASET n h k # REWIRED n h k # REWIRED CORA 3 20 10 90 3 20 30 150 CITESEER 3 20 10 90 3 10 20 90 TEXAS 3 30 10 120 1 20 10 30 CORNELL 2 20 30 100 3 10 20 90 WISCONSIN 2 30 20 100 2 50 30 160 CHAMELEON 3 20 20 120 3 30 30 180 ENZYMES 1 3 2 5 3 3 1 12 IMDB 1 3 0 3 1 4 2 6 MUTAG 1 20 3 23 1 3 1 4 PROTEINS 3 4 1 15 2 4 3 14 tuned the number of batches n, number of edges added per batch h, and number of edges removed per batch k. The exact hyper-parameters for SDRF, FoSR and BORF are available in Table 5, Table 6, and Table 7, respectively. We also report the total amount of edges each method rewired, which equals the total number of added and removed connections for SDRF and BORF. The number of edges rewired by FoSR is the same as the iteration count. For node classification tasks, each dataset is one big graph. The column titled “# REWIRED ” reports the total number of edges rewired in this graph. For graph classification tasks, each dataset is a collection of smaller graphs. The column titled “# REWIRED ” reports the number of edges rewired for each of these smaller graphs. We did not vary the number of edges rewired for each smaller graph in graph classification datasets. D.2. Architecture and experiment settings For graph and node classification, we utilized fixed model architectures with fixed numbers of GNN layers across all datasets. We used 3 GNN layers for node classification and 4 GNN layers for graph classification tasks. All the intermediate GNN layers (except for the input and output layers) have the same number of input and output dimensions specified by the hidden dimensions. After every GNN layer, we also added a drop-out layer with a fixed drop-out probability and an activation layer. The specific hidden dimensions, drop-out probabilities, and final activation layers for both node and graph classification tasks are specified in the architecture settings in Table 8. For each graph and node classification experiment, we randomly split the dataset into train, validation and test sets 100 times corresponding to 100 trials. For each trial, the GNN model is trained on the train set using the Adam optimizer and validated using the validation set. The test accuracy corresponding to the best accuracy on the validation set is recorded as the test accuracy of the current trial. After all 100 trials are finished, the mean test accuracy and the 95% confidence interval across all trials are computed and recorded in Tables 2 and 3. We also implemented a callback that stops the training process upon no improvement on the validation accuracy for 100 epochs. The train and validation fractions used to split the dataset 20Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 8.Experiment settings for node and graph classification tasks (Note: The train fraction is with respect to the entire dataset while the validation fraction is with respect to the train set). TASK LEARNING RATE #TRIALS /RUN STOP PATIENCE TRAIN FRACTION VALIDATION FRACTION NODE 0.001 100 100 0 .6 0 .2 GRAPH 0.001 100 100 0 .8 0 .1 Table 9.Architecture settings for node and graph classification tasks. TASK DROP -OUT PROBABILITY #GNN LAYERS HIDDEN DIMENSIONS FINAL ACTIVATION NODE 0.5 3 128 RELU GRAPH 0.5 4 64 RELU is specified in Table 9. E. Additional experiments and ablation studies E.1. Ablation study - Effect of edge addition/removal In this section, we investigate the role of edge addition and removal in helping BORF improve GNN performance. We compare BORF’s performance at high depths when using the best rewiring settings found in previous experiments against versions of BORF where it only removes edges, only adds edges (only alleviates over-squashing), or adds & removes edges equally. Specifically, we consider the following cases • BORF with the best settings for 3 GNN layers on node classification datasets and 4 GNN layers for graph classification datasets, as reported in Table 7. • BORF removes the same number of edges k as reported in Table 7 but adds h = 0 new edge. • BORF adds the same number of edges h as reported in Table 7 but removesk = 0 edge. • BORF adds and removes an equal amount of edges h = k, taken to be the average of h and k as reported in Table 7. We use GCN as the baseline GNN and conduct our ablation study on all datasets used in previous experiments at depths 5, 7, and 9. Other experiment details are kept identical to previous experiments, as documented in Section D.2. We report the experiment results in tables 10 and 11. We note that the rewiring hyper-parameters were not tuned for higher depths. As such, the results in the columns “ BEST SETTINGS ” are not the best performance achievable with BORF. With more depth-specific tuning, we expect to be able to obtain even better classification accuracy for both node classification and graph classification tasks. We observe that the un-rewired datasets record the worst performance in most scenarios considered. This indicates that GCN frequently suffers from both over-smoothing and over-squashing at high depths and that BORF helps the GNN achieve better performance even when it is restricted to only relieving over-smoothing or only relieving over-squashing. E.2. Experiment results on long-range graph benchmark We provide additional empirical results in this section to demonstrate the effectiveness of BORF on the long-range graph classification dataset Peptides-func introduced by (Dwivedi et al., 2022). In table 12, We compare three rewiring algorithms: SDRF, FoSR, and BORF by tuning the hyper-parameters of each method and report the mean average precision (mAP) of the best setting for each algorithm. For each hyper-parameter setting, we run our experiment with 4 random splits and 4 random seeds per split, similar to the setting in (Dwivedi et al., 2022). However, for our experiments, we utilized a lower hidden dimensions of 64 rather than 300. For both SDRF and FoSR, we performed hyper-parameters tuning by trying different numbers of iterations from 25 to 200 with an increment step of 25 to find the optimal number of rewiring iterations. For SDRF, we only tuned the hyper-parameter C+ using values 0 and ∞. For BORF, we set the range of rewiring batches 21Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 10.Classification accuracies of GCN at depths 5, 7, and 9 with different BORF rewiring options on node classification datasets. DATASET #LAYERS NONE BEST SETTINGS ONLY ADD ONLY REMOVE REMOVE + ADD EQUALLY CORA 5 84.2 ±0.8 85 .1 ±1.3 84 .6 ±1.1 84 .9 ±0.9 85 .0 ±0.7 7 82.1 ±1.1 82 .6 ±0.9 81 .8 ±1.2 82 .2 ±0.7 83 .1 ±1.1 9 78.9 ±0.7 78 .4 ±1.2 77 .5 ±0.9 78 .2 ±1.4 77 .9 ±1.2 CITESEER 5 69.2 ±1.1 72 .1 ±0.8 70 .5 ±1.4 71 .2 ±1.2 71 .8 ±1.5 7 65.4 ±1.5 68 .3 ±0.9 66 .8 ±1.1 67 .5 ±0.9 68 .1 ±1.2 9 61.7 ±0.9 62 .6 ±1.6 61 .5 ±1.2 62 .2 ±1.3 63 .2 ±1.5 TEXAS 5 40.3 ±1.3 43 .3 ±0.7 43 .5 ±0.8 41 .1 ±1.0 42 .8 ±0.9 7 37.2 ±1.1 40 .1 ±1.6 41 .4 ±1.2 39 .6 ±0.5 40 .8 ±0.6 9 32.1 ±0.9 34 .4 ±1.2 36 .5 ±1.1 35 .1 ±0.7 35 .8 ±0.6 CORNELL 5 41.3 ±1.4 45 .5 ±1.1 44 .7 ±1.3 46 .4 ±1.2 45 .9 ±1.2 7 39.5 ±1.7 41 .5 ±1.5 42 .8 ±1.4 43 .2 ±1.3 41 .8 ±1.3 9 35.5 ±1.4 40 .9 ±1.3 40 .3 ±2.0 41 .9 ±1.6 39 .9 ±1.6 WISCONSIN 5 40.1 ±1.2 47 .2 ±0.6 44 .4 ±0.9 45 .2 ±1.1 45 .1 ±0.7 7 35.5 ±1.1 42 .3 ±1.0 39 .7 ±0.9 40 .9 ±0.7 39 .2 ±1.2 9 31.2 ±0.8 36 .4 ±0.6 34 .2 ±0.9 35 .8 ±1.2 34 .7 ±1.1 CHAMELEON 5 57.2 ±0.4 58 .8 ±0.5 58 .1 ±0.7 58 .6 ±0.9 58 .9 ±1.1 7 55.3 ±0.5 56 .1 ±0.8 55 .9 ±0.6 56 .3 ±0.4 55 .8 ±0.9 9 51.1 ±0.7 52 .0 ±0.4 51 .8 ±0.6 51 .3 ±0.8 52 .1 ±1.2 Table 11.Classification accuracies of GCN at depths 5, 7, and 9 with different BORF rewiring options on graph classification datasets. DATASET #LAYERS NONE BEST SETTINGS ONLY ADD ONLY REMOVE REMOVE + ADD EQUALLY ENZYMES 5 24.2 ± 1.3 24 .5 ± 1.0 24 .8 ± 1.6 23 .8 ± 1.1 25 .0 ± 1.4 7 21.7 ± 1.5 22 .3 ± 1.2 21 .9 ± 1.7 21 .1 ± 1.2 22 .1 ± 1.3 9 20.8 ± 0.9 20 .4 ± 1.1 20 .8 ± 1.2 19 .7 ± 1.3 20 .1 ± 1.2 IMDB 5 47.6 ± 0.8 49 .1 ± 1.0 48 .8 ± 1.1 46 .9 ± 1.4 48 .6 ± 1.2 7 44.3 ± 1.5 45 .4 ± 1.2 44 .7 ± 1.1 43 .7 ± 1.3 44 .2 ± 1.4 9 39.2 ± 1.0 41 .8 ± 1.1 40 .7 ± 1.6 39 .3 ± 1.1 41 .3 ± 1.2 MUTAG 5 67.7 ± 1.6 75 .4 ± 2.1 76 .1 ± 2.2 68 .5 ± 2.8 71 .8 ± 1.2 7 64.1 ± 2.1 72 .1 ± 1.3 75 .2 ± 2.4 65 .1 ± 1.5 66 .2 ± 1.9 9 63.1 ± 1.2 69 .7 ± 1.5 70 .4 ± 1.7 60 .7 ± 2.5 61 .3 ± 1.5 PROTEINS 5 69.2 ± 0.8 70 .1 ± 1.0 69 .5 ± 1.1 69 .3 ± 1.2 69 .9 ± 1.4 7 67.3 ± 1.0 68 .1 ± 0.9 68 .3 ± 0.8 67 .5 ± 1.1 67 .9 ± 1.2 9 64.5 ± 1.1 65 .1 ± 1.1 64 .9 ± 1.2 64 .7 ± 1.0 65 .0 ± 1.3 Table 12.Classification accuracies of GCN and GIN with None, SDRF, FoSR, and BORF rewiring on the Peptides-func dataset. Best results are highlighted in bold. LAYER TYPE NONE SDRF F OSR BORF GCN 40.1 ± 2.1 41 .5 ± 1.5 44.2 ±2.1 43.9 ± 2.7 GIN 46.1 ± 2.4 46 .3 ± 1.7 48 .3 ± 1.8 50.2 ±1.7 from 1 to 4. For each number of rewiring batches, we tested for the following pairs of edge addition - removal settings: 30 - 20, 40 - 10. In table 13, we report the best settings of SDRF, FoSR, and BORF for both GCN and GIN layer types. We observe that on GCN, BORF’s performance is comparable with that of FoSR. Both of these rewiring algorithms significantly improve the model performance compared to the baseline and SDRF. On GIN, BORF is the best performer 22Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 13.Hyper-parameter settings of SDRF, FoSR and BORF for GCN and GIN tested on the Peptides-func graph classification dataset. LAYER TYPE SDRF F OSR BORF GCN 100 ITERATIONS - C+ = 0 25 ITERATIONS 4 BATCHES - ADD 30 - REMOVE 20 GIN 50 ITERATIONS - C+ = 0 150 ITERATIONS 2 BATCHES - ADD 40 - REMOVE 10 overall. E.3. Graph topology changes by different rewiring algorithms In this section, we provide empirical data comparing the change in graph topology enacted by the rewiring algorithms SDRF, FoSR, and BORF. Similar to (Topping et al., 2022), for each dataset, we record the node degrees’ base 2 logarithm distribution before and after applying BORF, SDRF and FoSR rewiring settings tuned for GCN as documented in Section D.1. We visualize the difference by utilizing the kernel density functions. The L1 Wasserstein distance between the kernel density functions of these rewiring methods and the original node degree density is also calculated to measure the extent of topological change after rewiring. (a) Cora W1(Original, SDRF) = 0.00115 W1(Original, FoSR) = 0.00372 W1(Original, BORF) = 0.00082 (b) Texas W1(Original, SDRF) = 0.00831 W1(Original, FoSR) = 0.01024 W1(Original, BORF) = 0.00971 (c) Citeseer W1(Original, SDRF) = 0.00056 W1(Original, FoSR) = 0.00184 W1(Original, BORF) = 0.00072 (d) Cornell W1(Original, SDRF) = 0.00857 W1(Original, FoSR) = 0.01534 W1(Original, BORF) = 0.00641 (e) Chameleon W1(Original, SDRF) = 0.00043 W1(Original, FoSR) = 0.00069 W1(Original, BORF) = 0.00026 (f) Wisconsin W1(Original, SDRF) = 0.00836 W1(Original, FoSR) = 0.01723 W1(Original, BORF) = 0.00451 Figure 5.Kernel density functions of graph degrees’ base 2 logarithm before and after rewiring using BORF, SDRF and FoSR. Our data shows that both BORF and SDRF change the graph topology only minimally according to this metric, while rewiring with FoSR can have a much more drastic effect. F. Dataset Statistics We provide a summary of statistics of all datasets used in Table 14 and Table 15. We also report the mean and standard deviation of the Ollivier Ricci curvature for each dataset. On node classification tasks, this is exactly the statistics of the set of edge curvature values. On graph classification tasks, this is the statistics of the mean curvature value of all graphs within the dataset. 23Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature Table 14.Statistics of node classification datasets. CORNELL TEXAS WISCONSIN CORA CITESEER CHAMELEON #NODES 140 135 184 2485 2120 832 #EDGES 219 251 362 5069 3679 12355 #FEATURES 1703 1703 1703 1433 3703 2323 #CLASSES 5 5 5 7 6 5 DIRECTED TRUE TRUE TRUE FALSE FALSE TRUE ORC M EAN -0.39 -0.24 -0.59 -0.19 -0.31 0.64 ORC STD 0.52 0.45 0.71 0.68 0.78 0.58 Table 15.Statistics of graph classification datasets. ENZYMES IMDB MUTAG PROTEINS #GRAPHS 600 1000 188 1113 #NODES 2-126 12-136 10 - 28 4-620 #EDGES 2 - 298 52 - 2498 20 - 66 10 - 2098 AVG #NODES 32.63 19.77 17.93 39.06 AVG #EDGES 124.27 193.062 39.58 145.63 #CLASSES 6 2 2 2 DIRECTED FALSE FALSE FALSE FALSE ORC M EAN 0.13 0.58 -0.27 0.17 ORC STD 0.15 0.19 0.05 0.20 Table 16.Server specifications for conducting all experiments. SERVER ID C OMPONENTS SPECIFICATIONS 1 ARCHITECTURE X86 64 OS U BUNTU 20.04.5 LTS X86 64 CPU I NTEL I 7-10700KF (16) @ 5.100GH Z GPU NVIDIA G EFORCE RTX 2080 T I REV. A RAM 12G B 2 ARCHITECTURE X86 64 OS U BUNTU 20.04.5 LTS X86 64 CPU AMD EPYC 7742 64- CORE GPU NVIDIA A100 T ENSOR CORE RAM 40G B G. Hardware Specifications and Libraries All experiments were implemented in Python using PyTorch (Paszke et al., 2019), Numpy (Harris et al., 2020), PyG (PyTorch Geometric) (Fey & Lenssen, 2019), POT (Python Optimal Transport) (Flamary et al., 2021) with figures created using TikZ (Tantau, 2023). PyTorch, PyG and NumPy are made available under the BSD license, POT under MIT license, and TikZ under the GNU General Public license. We conducted our experiments on two local servers with the specifications laid out in Table 16. 24",
      "meta_data": {
        "arxiv_id": "2211.15779v3",
        "authors": [
          "Khang Nguyen",
          "Hieu Nong",
          "Vinh Nguyen",
          "Nhat Ho",
          "Stanley Osher",
          "Tan Nguyen"
        ],
        "published_date": "2022-11-28T21:21:31Z",
        "pdf_url": "https://arxiv.org/pdf/2211.15779v3.pdf"
      }
    },
    {
      "title": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization",
      "abstract": "Graph neural networks (GNNs), which learn the representation of a node by\naggregating its neighbors, have become an effective computational tool in\ndownstream applications. Over-smoothing is one of the key issues which limit\nthe performance of GNNs as the number of layers increases. It is because the\nstacked aggregators would make node representations converge to\nindistinguishable vectors. Several attempts have been made to tackle the issue\nby bringing linked node pairs close and unlinked pairs distinct. However, they\noften ignore the intrinsic community structures and would result in sub-optimal\nperformance. The representations of nodes within the same community/class need\nbe similar to facilitate the classification, while different classes are\nexpected to be separated in embedding space. To bridge the gap, we introduce\ntwo over-smoothing metrics and a novel technique, i.e., differentiable group\nnormalization (DGN). It normalizes nodes within the same group independently to\nincrease their smoothness, and separates node distributions among different\ngroups to significantly alleviate the over-smoothing issue. Experiments on\nreal-world datasets demonstrate that DGN makes GNN models more robust to\nover-smoothing and achieves better performance with deeper GNNs.",
      "full_text": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization Kaixiong Zhou Texas A&M University zkxiong@tamu.edu Xiao Huang The Hong Kong Polytechnic University xhuang.polyu@gmail.com Yuening Li Texas A&M University liyuening@tamu.edu Daochen Zha Texas A&M University daochen.zha@tamu.edu Rui Chen Samsung Research America rui.chen1@samsung.com Xia Hu Texas A&M University xiahu@tamu.edu Abstract Graph neural networks (GNNs), which learn the representation of a node by aggre- gating its neighbors, have become an effective computational tool in downstream applications. Over-smoothing is one of the key issues which limit the performance of GNNs as the number of layers increases. It is because the stacked aggregators would make node representations converge to indistinguishable vectors. Several attempts have been made to tackle the issue by bringing linked node pairs close and unlinked pairs distinct. However, they often ignore the intrinsic community structures and would result in sub-optimal performance. The representations of nodes within the same community/class need be similar to facilitate the classiﬁca- tion, while different classes are expected to be separated in embedding space. To bridge the gap, we introduce two over-smoothing metrics and a novel technique, i.e., differentiable group normalization (DGN). It normalizes nodes within the same group independently to increase their smoothness, and separates node distributions among different groups to signiﬁcantly alleviate the over-smoothing issue. Exper- iments on real-world datasets demonstrate that DGN makes GNN models more robust to over-smoothing and achieves better performance with deeper GNNs. 1 Introduction Graph neural networks (GNNs) [1, 2, 3] have emerged as a promising tool for analyzing networked data, such as biochemical networks [4, 5], social networks [6, 7], and academic networks [8, 9]. The successful outcomes have led to the development of many advanced GNNs, including graph convolu- tional networks [10], graph attention networks [11], and simple graph convolution networks [12]. Besides the exploration of graph neural network variants in different applications, understanding the mechanism and limitation of GNNs is also a crucial task. The core component of GNNs, i.e., a neighborhood aggregator updating the representation of a node iteratively via mixing itself with its neighbors’ representations [6, 13], is essentially a low-pass smoothing operation [14]. It is in line with graph structures since the linked nodes tend to be similar [15]. It has been reported that, as the number of graph convolutional layers increases, all node representations over a graph will converge to indistinguishable vectors, and GNNs perform poorly in downstream applications [16, 17, 18]. It is recognized as an over-smoothing issue. Such an issue prevents GNN models from going deeper to exploit the multi-hop neighborhood structures and learn better node representations. A lot of efforts have been devoted to alleviating the over-smoothing issue, such as regularizing the node distance [ 19], node/edge dropping [ 20, 21], batch and pair normalizations [ 22, 23, 24]. Preprint. Under review. arXiv:2006.06972v1  [cs.LG]  12 Jun 2020Most of existing studies focused on measuring the over-smoothing based on node pair distances. By using these measurements, representations of linked nodes are forced to be close to each other, while unlinked pairs are separated. Unfortunately, the global graph structures and group/community characteristics are ignored, which leads to sub-optimal performance. For example, to perform node classiﬁcation, an ideal solution is to assign similar vectors to nodes in the same class, instead of only the connected nodes. In the citation network Pubmed [25], 36% of unconnected node pairs belong to the same class. These node pairs should instead have a small distance to facilitate node classiﬁcation. Thus, we are motivated to tackle the over-smoothing issue in GNNs from a group perspective. Given the complicated group structures and characteristics, it remains a challenging task to tackle the over-smoothing issue in GNNs. First, the formation of over-smoothing is complex and related to both local node relations and global graph structures, which makes it hard to measure and quantify. Second, the group information is often not directly available in real-world networks. This prevents existing tools such as group normalization being directly applied to solve our problem [ 26]. For example, while the group of adjacent channels with similar features could be directly accessed in convolutional neural networks [27], it is nontrivial to cluster a network in a suitable way. The node clustering needs to be in line with the embeddings and labels, during the dynamic learning process. To bridge the gap, in this paper, we perform a quantitative study on the over-smoothing in GNNs from a group perspective. We aim to answer two research questions. First, how can we precisely measure the over-smoothing in GNNs? Second, how can we handle over-smoothing in GNNs? Through exploring these questions, we make three signiﬁcant contributions as follows. • Present two metrics to quantify the over-smoothing in GNNs: (1) Group distance ratio, clustering the network and measuring the ratio of inter-group representation distance over intra-group one; (2) Instance information gain, treating node instance independently and measuring the input information loss during the low-pass smoothing. • Propose differentiable group normalization to signiﬁcantly alleviate over-smoothing. It softly clusters nodes and normalizes each group independently, which prevents distinct groups from having close node representations to improve the over-smoothing metrics. • Empirically show that deeper GNNs, when equipped with the proposed differentiable group normalization technique, yield better node classiﬁcation accuracy. 2 Quantitative Analysis of Over-smoothing Issue In this work, we use the semi-supervised node classiﬁcation task as an example and illustrate how to handle the over-smoothing issue. A graph is represented byG= {V,E}, where Vand Erepresent the sets of nodes and edges, respectively. Each node v∈V is associated with a feature vector xv ∈Rd and a class label yv. Given a training set Vl accompanied with labels, the goal is to classify the nodes in the unlabeled set Vu = V\\V l via learning the mapping function based on GNNs. 2.1 Preliminaries Following the message passing strategy [ 28], GNNs update the representation of each node via aggregating itself and its neighbors’ representations. Mathematically, at thek-th layer, we have, N(k) v = AGG({a(k) vv′W(k)h(k−1) v′ : v′∈N(v)}), h (k) v = COM(a(k) vv W(k)h(k−1) v ,N(k) v ). (1) N(k) v and h(k) v denote the aggregated neighbor embedding and embedding of nodev, respectively. We initialize h(0) v = xv. N(v) = {v′|ev,v′ ∈E} represents the set of neighbors for node v, where ev,v′ denotes the edge that connects nodes vand v′. W(k) denotes the trainable matrix used to transform the embedding dimension. a(k) vv′ is the link weight over edge ev,v′, which could be determined based on the graph topology or learned by an attention layer. Symbol AGG denotes the neighborhood aggregator usually implemented by a summation pooling. To update nodev, function COM is applied to combine neighbor information and node embedding from the previous layer. It is observed that the weighted average in Eq. (1) smooths node embedding with its neighbors to make them similar. For a full GNN model with Klayers, the ﬁnal node representation is given by hv = h(K) v , which captures the neighborhood structure information within Khops. 22.2 Measuring Over-smoothing with Group Structures In GNNs, the neighborhood aggregation strategy smooths nodes’ representations over a graph [14]. It will make the representations of nodes converge to similar vectors as the number of layersKincreases. This is called the over-smoothing issue, and would cause the performance of GNNs deteriorates as K increases. To address the issue, the ﬁrst step is to measure and quantify the over-smoothing [19, 21]. Measurements in existing work are mainly based on the distances between node pairs [20, 24]. A small distance means that a pair of nodes generally have undistinguished representation vectors, which might triggers the over-smoothing issue. However, the over-smoothing is also highly related to global graph structures, which have not been taken into consideration. For some unlinked node pairs, we would need their representations to be close if they locate in the same class/community, to facilitate the node classiﬁcation task. Without the speciﬁc group information, the metrics based on pair distances may fail to indicate the over- smoothing. Thus, we propose two novel over-smoothing metrics, i.e., group distance ratio and instance information gain. They quantify the over-smoothing from global (communities/classes/groups) and local (node individuals) views, respectively. Deﬁnition 1 (Group Distance Ratio). Suppose that there areCclasses of node labels. We intuitively cluster nodes of the same class label into a group to formulate the labeled node community. Formally, let Li = {hiv}denote the group of representation vectors, where node vis associated with label i. We have a series of labeled groups{L1,··· ,LC}. Group distance ratio RGroup measures the ratio of inter-group distance over intra-group distance in the Euclidean space. We have: RGroup = 1 (C−1)2 ∑ i̸=j( 1 |Li||Lj| ∑ hiv∈Li ∑ hjv′∈Lj ||hiv −hjv′||2) 1 C ∑ i( 1 |Li|2 ∑ hiv,hiv′∈Li ||hiv −hiv′||2) , (2) where ||·|| 2 denotes the L2 norm of a vector and |·| denotes the set cardinality. The numerator (denominator) represents the average of pairwise representation distances between two different groups (within a group). One would prefer to reduce the intra-group distance to make representations of the same class similar, and increase the inter-group distance to relieve the over-smoothing issue. On the contrary, a small RGroup leads to the over-smoothing issue where all groups are mixed together, and the intra-group distance is maintained to hinder node classiﬁcation. Deﬁnition 2 (Instance Information Gain). In an attributed network, a node’s feature decides its class label to some extent. We treat each node instance independently, and deﬁne instance information gain GIns as how much input feature information is contained in the ﬁnal representation. Let Xand Hdenote the random variables of input feature and representation vector, respectively. We deﬁne their probability distributions with PX and PH, and use PXH to denote their joint distribution. GIns measures the dependency between node feature and representation via their mutual information: GIns = I(X; H) = ∑ xv∈X,hv∈H PXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv). (3) We list the details of variable deﬁnitions and mutual information calculation in the context of GNNs in Appendix. With the intensiﬁcation of the over-smoothing issue, nodes average the neighborhood information and lose their self features, which leads to a small value of GIns. 2.3 Illustration of Proposed Over-smoothing Metrics Based on the two proposed metrics, we take simple graph convolution networks (SGC) as an example, and analyze the over-smoothing issue on Cora dataset [ 25]. SGC simpliﬁes the model through removing all the trainable weights between layers to avoid the potential of overﬁtting [12]. So the over-smoothing issue would be the major cause of performance dropping in SGC. As shown by the red lines in Figure 1, the graph convolutions ﬁrst exploit neighborhood information to improve test accuracy up to K = 5, after which the over-smoothing issue starts to worsen the performance. At the same time, instance information gain GIns and group distance ratio RGroup decrease due to the over-smoothing issue. For the extreme case of K = 120, the input features are ﬁltered out and all groups of nodes converge to the same representation vector, leading to GIns = 0 and RGroup = 1, respectively. Our metrics quantify the smoothness of node representations based on group structures, but also have the similar variation tendency with test accuracy to indicate it well. 30 25 50 75 100 125 Layers 0.4 0.6 0.8Accuracy 0 25 50 75 100 125 Layers 0.06 0.08 0.10 0.12Instance gain 0 25 50 75 100 125 Layers 1.5 2.0 2.5 3.0Group distance None batch pair group Figure 1: The test accuracy, instance information gain, and group distance ratio of SGC on Cora. We compare differentiable group normalization with none, batch and pair normalizations. 3 Differentiable Group Normalization We start with a graph-regularized optimization problem [10, 19]. To optimize the over-smoothing metrics of GIns and RGroup, one traditional approach is to minimize the loss function: L= L0 −GIns −λRGroup. (4) L0 denotes the supervised cross-entropy loss w.r.t. representation probability vectors hv ∈RC×1 and class labels. λis a balancing factor. The goal of optimization problem Eq. (4) is to learn node representations close to the input features and informative for their class labels. Considering the labeled graph communities, it also improves the intra-group similarity and inter-group distance. However, it is non-trivial to optimize this objective function due to the non-derivative of non- parametric statistic GIns [29, 30] and the expensive computation of RGroup. 3.1 Proposed Technique for Addressing Over-smoothing Instead of directly optimizing regularized problem in Eq. (4), we propose the differentiable group normalization (DGN) applied between graph convolutional layers to normalize the node embeddings group by group. The key intuition is to cluster nodes into multiple groups and then normalize them independently. Consider the labeled node groups (or communities) in networked data. The node embeddings within each group are expected to be rescaled with a speciﬁc mean and variance to make them similar. Meanwhile, the embedding distributions from different groups are separated by adjusting their means and variances. We develop an analogue with the group normalization in convolutional neural networks (CNNs) [26], which clusters a set of adjacent channels with similar characteristics into a group and treats it independently. Compared with standard CNNs, the challenge in designing DGN is how to cluster nodes in a suitable way. The clustering needs to be in line with the embedding and labels, during the dynamic learning process. We address this challenge by learning a cluster assignment matrix, which softly maps nodes with close embeddings into a group. Under the supervision of training labels, the nodes close in the embedding space tend to share a common label. To be speciﬁc, we ﬁrst describe how DGN clusters and normalizes nodes in a group-wise fashion given an assignment matrix. After that, we discuss how to learn the assignment matrix to support differentiable node clustering. Group Normalization. Let H(k) = [ h(k) 1 ,··· ,h(k) n ]T ∈Rn×d(k) denote the embedding matrix generated from the k-th graph convolutional layer. TakingH(k) as input, DGN softly assigns nodes into groups and normalizes them independently to output a new embedding matrix for the next layer. Formally, we deﬁne the number of groups as G, and denote the cluster assignment matrix by S(k) ∈Rn×G. Gis a hyperparameter that could be tuned per dataset. The i-th column of S(k), i.e., S(k)[:,i], indicates the assignment probabilities of nodes in a graph to the i-th group. Supposing that S(k) has already been computed, we cluster and normalize nodes in each group as follows: H(k) i = S(k)[:,i] ◦H(k) ∈Rn×d(k) ; ˜H(k) i = γi(H(k) i −µi σi ) + βi ∈Rn×d(k) . (5) Symbol ◦denotes the row-wise multiplication. The left part in the above equation represents the soft node clustering for group i, whose embedding matrix is given by H(k) i . The right part performs the standard normalization operation. In particular, µi and σi denote the vectors of running mean 4and standard deviation of group i, respectively, and γi and βi denote the trainable scale and shift vectors, respectively. Given the input embedding H(k) and the series of normalized embeddings {˜H(k) 1 ,··· , ˜H(k) G }, DGN generates the ﬁnal embedding matrix ˜H(k) for the next layer as follows: ˜H(k) = H(k) + λ G∑ i=1 ˜H(k) i ∈Rn×d(k) . (6) λis a balancing factor as mentioned before. Inspecting the loss function in Eq. (4), DGN utilizes components H(k) and ∑G i=1 ˜H(k) i to improve terms GIns and RGroup, respectively. In particular, we preserve the input embedding H(k) to avoid over-normalization and keep the input feature of each node to some extent. Note that the linear combination of H(k) in DGN is different from the skip connection in GNN models [31, 32], which instead connects the embedding output H(k−1) from the last layer. The technique of skip connection could be included to further boost the model performance. Group normalization ∑G i=1 ˜H(k) i rescales the node embeddings within each group independently to make them similar. Ideally, we assign the close node embeddings with a common label to a group. Node embeddings of the group are then distributed closely around the corresponding running mean. Thus for different groups associate with distinct node labels, we disentangle their running means and separates the node embedding distributions. By applying DGN between the successive graph convolutional layers, we are able to optimize Problem (4) to mitigate the over-smoothing issue. Differentiable Clustering. We apply a linear model to compute the cluster assignment matrix S(k) used in Eq. (5). The mathematical expression is given by: S(k) = softmax(H(k)U(k)). (7) U(k) ∈Rd(k)×G denotes the trainable weights for a DGN module applied after the k-th graph convolutional layer. softmax function is applied in a row-wise way to produce the normalized probability vector w.r.t all the Ggroups for each node. Through the inner product between H(k) and U(k), the nodes with close embeddings are assigned to the same group with a high probability. Here we give a simple and effective way to compute S(k). Advanced neural networks could be applied. Time Complexity Analysis. Suppose that the time complexity of embedding normalization at each group is O(T), where T is a constant depending on embedding dimension d(k) and node number n. The time cost of group normalization ∑G i=1 ˜H(k) i is O(GT). Both the differentiable clustering (in Eq. (5)) and the linear model (in Eq. (7)) have a time cost of O(nd(k)G). Thus the total time complexity of a DGN layer is given by O(nd(k)G+ GT), which linearly increases with G. Comparison with Prior Work. To the best of our knowledge, the existing work mainly focuses on analyzing and improving the node pair distance to relieve the over-smoothing issue [19, 21, 24]. One of the general solutions is to train GNN models regularized by the pair distance [19]. Recently, there are two related studies applying batch normalization [22] or pair normalization [24] to keep the overall pair distance in a graph. Pair normalization is a “slim” realization of batch normalization by removing the trainable scale and shift. However, the metric of pair distance and the resulting techniques ignore global graph structure, and may achieve sub-optimal performance in practice. In this work, we measure over-smoothing of GNN models based on communities/groups and independent node instances. We then formulate the problem in Eq. (4) to optimize the proposed metrics, and propose DGN to solve it in an efﬁcient way, which in turn addresses the over-smoothing issue. 3.2 Evaluating Differentiable Group Normalization on Attributed Graphs We apply DGN to the SGC model to validate its effectiveness in relieving the over-smoothing issue. Furthermore, we compare with the other two available normalization techniques used upon GNNs, i.e., batch normalization and pair normalization. As shown in Figure 1, the test accuracy of DGN remains stable with the increase in the number of layers. By preserving the input embedding and normalizing node groups independently, DGN achieves superior performance in terms of instance information gain as well as group distance ratio. The promising results indicate that our DGN tackles the over-smoothing issue more effectively, compared with none, batch and pair normalizations. It should be noted that, the highest accuracy of 79.7% is achieved with DGN when K = 20. This observation contradicts with the common belief that GNN models work best with a few layers on 50 25 50 75 100 125 Layers 0.2 0.4 0.6 0.8Accuracy SGC 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GCN 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GAT None batch pair group Figure 2: The test accuracies of SGC, GCN, and GAT models on Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. current benchmark datasets [33]. With the integration of advanced techniques, such as DGN, we are able to exploit deeper GNN architectures to unleash the power of deep learning in network analysis. 3.3 Evaluation in Scenario with Missing Features To further illustrate that DGN could enable us to achieve better performance with deeper GNN architectures, we apply it to a more complex scenario. We assume that the attributes of nodes in the test set are missing. It is a common scenario in practice [24]. For example, in social networks, new users are often lack of proﬁles and tags [34]. To perform prediction tasks on new users, we would rely on the node attributes of existing users and their connections to new users. In such a scenario, we would like to apply more layers to exploit the neighborhood structure many hops away to improve node representation learning. Since the over-smoothing issue gets worse with the increasing of layer numbers, the beneﬁt of applying normalization will be more obvious in this scenario. We remove the input features of both validation and test sets in Cora, and replace them with zeros [24]. Figure 2 presents the results on three widely-used models, i.e., SGC, graph convolutional networks (GCN), and graph attention networks (GAT). Due to the over-smoothing issue, GNN models without any normalization fail to distinguish nodes quickly with the increasing number of layers. In contrast, the normalization techniques reach their highest performance at larger layer numbers, after which they drop slowly. We observe that DGN obtains the best performance with50, 20, and 8 layers for SGC, GCN, and GAT, respectively. These layer numbers are signiﬁcantly larger than those of the widely-used shallow models (e.g., two or three layers). 4 Experiments We now empirically evaluate the effectiveness and robustness of DGN on real-world datasets. We aim to answer three questions as follows. Q1: Compared with the state-of-the-art normalization methods, can DGN alleviate the over-smoothing issue in GNNs in a better way? Q2: Can DGN help GNN models achieve better performance by enabling deeper GNNs? Q3: How do the hyperparameters inﬂuence the performance of DGN? 4.1 Experiment Setup Datasets. Joining the practice of previous work, we evaluate GNN models by performing the node classiﬁcation task on four datasets: Cora, Citeseer, Pubmed [ 25], and CoauthorCS [35]. We also create graphs by removing features in validation and test sets. The dataset statistics are in Appendix. Implementations. Following the previous settings, we choose the hyperparameters of GNN models and optimizer as follows. We set the number of hidden units to 16 for GCN and GAT models. The number of attention heads in GAT is 1. Since a larger parameter size in GCN and GAT may lead to overﬁtting and affects the study of over-smoothing issue, we compare normalization methods by varying the number of layersKin {1,2,··· ,10,15,··· ,30}. For SGC, we increase the testing range and vary K in {1,5,10,20,··· ,120}. We train with a maximum of 1000 epochs using the Adam optimizer [36] and early stopping. Weights in GNN models are initialized with Glorot algorithm [37]. We use the following sets of hyperparameters for Citeseer, Cora, CoauthorCS: 0.6 (dropout rate), 5 ·10−4 (L2 regularization), 5 ·10−3 (learning rate), and for Pubmed: 0.6 (dropout rate), 1 ·10−3 (L2 regularization), 1 ·10−2 (learning rate). We run each experiment 5 times and report the average. 6Table 1: Test accuracy in percentage on attributed networks. Layers a/bdenote the layer number ain GCN & GAT and that of bin SGC. #Kdenotes the optimal layer numbers where DGN achieves the highest performance. Dataset Model Layers 2/5 Layers 15/60 Layers 30/120 #KNN BN PN DGN NN BN PN DGN NN BN PN DGN Cora GCN 82.2 73.9 71 .0 82 .0 18.1 70 .3 67 .2 75.2 13.1 67 .2 64 .3 73.2 2 GAT 80.9 77 .8 74 .4 81.1 16.8 33 .1 49 .6 71.8 13.0 25 .0 30 .2 51.3 2 SGC 75.8 76 .3 75 .4 77.9 29.4 72 .1 71 .7 77.8 25.1 51 .2 65 .5 73.7 20 Citeseer GCN 70.6 51.3 60 .5 69 .5 15.2 46 .9 46 .7 53.1 9.4 47 .9 47 .1 52.6 2 GAT 70.2 61.5 62 .0 69 .3 22.6 28 .0 41 .4 52.6 7.7 21 .4 33 .3 45.6 2 SGC 69.6 58.8 64 .8 69 .5 66.3 50.5 65 .0 63 .4 60.8 47 .3 63 .1 64.7 30 Pubmed GCN 79.3 74 .9 71 .1 79.5 22.5 73 .7 70 .6 76.1 18.0 70 .4 70 .4 76.9 2 GAT 77.8 76.2 72 .4 77 .5 37.5 56 .2 68 .8 75.9 18.0 46 .6 58 .2 73.3 5 SGC 71.5 76 .5 75 .8 76.8 34.2 75 .2 77 .1 77.4 23.1 71 .6 76 .7 77.1 10 Coauthors GCN 92.3 86 .0 77 .8 92.3 72.2 78 .5 69 .5 83.7 3.3 84.7 64.5 84 .4 1 GAT 91.5 89 .4 85 .9 91.8 6.0 77 .7 53 .1 84.5 3.3 16 .7 48 .1 75.5 1 SGC 89.9 88 .7 86 .0 90.2 10.2 59 .7 76 .4 81.3 5.8 30 .5 52 .6 60.8 1 Baselines. We compare with none normalization (NN), batch normalization (BN) [22, 23] and pair normalization (PN) [24]. Their technical details are listed in Appendix. DGN Conﬁgurations. The key hyperparameters include group number G and balancing factor λ. Depending on the number of class labels, we apply 5 groups to Pubmed and 10 groups to the others. The criterion is to use more groups to separate representation distributions in networked data accompanied with more class labels. λis tuned on validation sets to ﬁnd a good trade-off between preserving input features and group normalization. We introduce the selection of λin Appendix. 4.2 Experiment Results Studies on alleviating the over-smoothing problem.To answerQ1, Table 1 summarizes the results of applying different normalization techniques to GNN models on all datasets. We report the performance of GCN and GAT with 2/15/30 layers, and SGC with 5/60/120 layers due to space limit. We provide test accuracies, instance information gain and group distance ratio under all depths in Appendix. It can be observed that DGN has signiﬁcantly alleviated the over-smoothing issue. Given the same layers, DGN almost outperforms all other normalization methods for all cases and greatly slows down the performance dropping. It is because the self-preserved component H(k) in Eq. (6) keeps the informative input features and avoids over-normalization to distinguish different nodes. This component is especially crucial for models with a few layers since the over-smoothing issue has not appeared. The other group normalization component in Eq. (6) processes each group of nodes independently. It disentangles the representation similarity between groups, and hence reduces the over-smoothness of nodes over a graph accompanied with graph convolutions. Studies on enabling deeper and better GNNs. To answer Q2, we compare all of the concerned normalization methods over GCN, GAT, and SGC in the scenario with missing features. As we have discussed, normalization techniques will show their power in relieving the over-smoothing issue and exploring deeper architectures especially for this scenario. In Table 2, Acc represents the best test accuracy yielded by model equipped with the optimal layer number #K. We can observe that DGN signiﬁcantly outperforms the other normalization methods on all cases. The average improvements over NN, BN and PN achieved by DGN are 37.8%, 7.1% and 12.8%, respectively. Compared with vanilla GNN models without any normalization layer, the optimal models accompanied with normalization layers (especially for our DGN) usually possess larger values of #K. It demonstrates that DGN enables to explore deeper architectures to exploit neighborhood information with more hops away by tackling the over-smoothing issue. We present the comprehensive analyses in terms of test accuracy, instance information gain and group distance ratio under all depths in Appendix. Hyperparameter studies. We study the impact of hyperparameters, group number Gand balancing factor λ, on DGN in order to answer research question Q3. Over the GCN framework associated with 20 convolutional layers, we evaluate DGN by considering Gand λfrom sets [1,5,10,15,20,30] and [0.001,0.005,0.01,0.03,0.05,0.1], respectively. The left part in Figure 3 presents the test accuracy 7Table 2: The highest accuracy (%) and the accompanied optimal layers in the scenario with missing features. We calculate the average improvement achieved by DGN over each GNN framework. Model Norm Cora Citeseer Pubmed CoauthorCS Improvement%Acc #K Acc #K Acc #K Acc #K GCN NN 57.3 3 44.0 6 36.4 4 67.3 3 42.2 BN 71.8 20 45.1 25 70.4 30 82.7 30 5.2 PN 65.6 20 43.6 25 63.1 30 63.5 4 19.2 DGN 76.3 20 50.2 30 72.0 30 83.7 25 - GAT NN 50.1 2 40.8 4 38.5 4 63.7 3 51.0 BN 72.7 5 48.7 5 60.7 4 80.5 6 9.8 PN 68.8 8 50.3 6 63.2 20 66.6 3 14.7 DGN 75.8 8 54.5 5 72.3 20 83.6 15 - SGC NN 63.4 5 51.2 40 63.7 5 71.0 5 20.1 BN 78.5 20 50.4 20 72.3 50 84.4 20 6.2 PN 73.4 50 58.0 120 75.2 30 80.1 10 4.5 DGN 80.2 50 58.2 90 76.2 90 85.8 20 - 0.2 0.1 0.3 0.4 0.5 0.08 0.6 30 0.7 0.06 25 0.8 20 0.04  15 100.02 5 0 0 Figure 3: Left: Test accuracies of GCN with 20 layers on Cora with missing features, where hyperparameters Gand λare studied. Middle: Node representation visualization for GCN without normalization and with K = 20. Right: Node representation visualization for GCN with DGN layer and K = 20 (node colors represent classes, and black triangles denote the running means of groups). for each hyperparameter combination. We observe that: (i) The model performance is damaged greatly when λis close to zero (e.g.,λ= 0.001). In this case, group normalization contributes slightly in DGN, resulting in over-smoothing in the GCN model. (ii) Model performance is not sensitive to the value of G, and an appropriate λvalue could be tuned to optimize the trade-off between instance gain and group normalization. It is because DGN learns to use the appropriate number of groups by end-to-end training. In particular, some groups might not be used as shown in the right part of Figure 3, at which only 6 out of 10 groups (denoted by black triangles) are adopted. (iii) Even when G= 1, DGN still outperforms BN by utilizing the self-preserved component to achieve an accuracy of 74.7%, where λ= 0.1. Via increasing the group number, the model performance could be further improved, e.g., the accuracy of 76.3% where G= 10 and λ= 0.01. Node representation visualization. We investigate how DGN clusters nodes into different groups to tackle the over-smoothing issue. The middle and right parts of Figure 3 visualize the node representations achieved by GCN models without normalization tool and with the DGN approach, respectively. It is observed that the node representations of different classes mix together when the layer number reaches 20 in the GCN model without normalization. In contrast, our DGN method softly assigns nodes into a series of groups, whose running means at the corresponding normalization modules are highlighted with black triangles. Through normalizing each group independently, the running means are separated to improve inter-group distances and disentangle node representations. In particular, we notice that the running means locate at the borders among different classes (e.g., the upper-right triangle at the border between red and pink classes). That is because the soft assignment may cluster nodes of two or three classes into the same group. Compared with batch or pair normalization, the independent normalization for each group only includes a few classes in DGN. In this way, we relieve the representation noise from other node classes during normalization, and improve the group distance ratio as illustrated in Appendix. 85 Conclusion In this paper, we propose two over-smoothing metrics based on graph structures, i.e., group distance ratio and instance information gain. By inspecting GNN models through the lens of these two metrics, we present a novel normalization layer, DGN, to boost model performance against over- smoothing. It normalizes each group of similar nodes independently to separate node representations of different classes. Experiments on real-world classiﬁcation tasks show that DGN greatly slowed down performance degradation by alleviating the over-smoothing issue. DGN enables us to explore deeper GNNs and achieve higher performance in analyzing attributed networks and the scenario with missing features. Our research will facilitate deep learning models for potential graph applications. Broader Impact The successful outcome of this work will lead to advances in building up deep graph neural networks and dealing with complex graph-structured data. The developed metrics and algorithms have an immediate and strong impact on a number of ﬁelds, including (1) Over-smoothing Quantitative Analysis: GNN models tend to result in the over-smoothing issue with the increase in the number of layers. During the practical development of deeper GNN models, the proposed instance information gain and group distance ratio effectively indicate the over-smoothing issue, in order to push the model exploration toward a good direction. (2) Deep GNN Modeling: The proposed differentiable group normalization tool successfully tackles the over-smoothing issue and enables the modeling of deeper GNN variants. It encourages us to fully unleash the power of deep learning in processing the networked data. (3) Real-world Network Analytics Applications: The proposed research will broadly shed light on utilizing deep GNN models in various applications, such as social network analysis, brain network analysis, and e-commerce network analysis. For such complex graph-structured data, deep GNN models can exploit the multi-hop neighborhood information to boost the task performance. References [1] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008. [2] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. [3] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A comprehensive survey on graph neural networks. arXiv, 2019. [4] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeuIPS, pages 2224–2232, 2015. [5] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [6] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeuIPS, pages 1024–1034, 2017. [7] Xiao Huang, Qingquan Song, Yuening Li, and Xia Hu. Graph recurrent networks with at- tributed random walks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 732–740, 2019. [8] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1416–1424, 2018. [9] Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019. [10] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. ICLR, 2017. [11] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv, 1(2), 2017. 9[12] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Simplifying graph convolutional networks. arXiv preprint arXiv:1902.07153, 2019. [13] Kaixiong Zhou, Qingquan Song, Xiao Huang, Daochen Zha, Na Zou, and Xia Hu. Multi-channel graph neural networks. arXiv preprint arXiv:1912.08306, 2019. [14] Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ﬁlters. arXiv preprint arXiv:1905.09550, 2019. [15] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415–444, 2001. [16] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018. [17] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In International Conference on Learning Representations, 2020. [18] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou. Specae: Spectral autoencoder for anomaly detection in attributed networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2233–2236, 2019. [19] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. arXiv preprint arXiv:1909.03211, 2019. [20] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Representations. https://openreview. net/forum, 2020. [21] Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming- Chang Yang. Measuring and improving the use of graph information in graph neural networks, 2020. [22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. [23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015. [24] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint arXiv:1909.12223, 2019. [25] Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861, 2016. [26] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018. [27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [28] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1263–1272. JMLR. org, 2017. [29] Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017. [30] Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. Entropy, 21(12):1181, 2019. [31] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE International Conference on Computer Vision, pages 9267–9276, 2019. [32] Nezihe Merve Gürel, Hansheng Ren, Yujing Wang, Hui Xue, Yaming Yang, and Ce Zhang. An anatomy of graph neural networks going deep via the lens of mutual information: Exponential decay vs. full preservation. arXiv preprint arXiv:1910.04499, 2019. 10[33] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018. [34] Al Mamunur Rashid, George Karypis, and John Riedl. Learning preferences of new users in recommender systems: an information theoretic approach. Acm Sigkdd Explorations Newsletter, 10(2):90–100, 2008. [35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. [36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, 2014. [37] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedfor- ward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256, 2010. 11A Dataset Statistics For fair comparison with previous work, we perform the node classiﬁcation task on four benchmark datasets, including Cora, Citeseer, Pubmed [ 25], and CoauthorCS [ 35]. They have been widely adopted to study the over-smoothing issue in GNNs [21, 19, 24, 16, 20]. The detailed statistics are listed in Table 3. To further illustrate that the normalization techniques could enable deeper GNNs to achieve better performance, we apply them to a more complex scenario with missing features. For these four benchmark datasets, we create the corresponding scenarios by removing node features in both validation and testing sets. Table 3: Dataset statistics on Cora, Citeseer, Pubmed, and CoauthorCS. Cora Citeseer Pubmed CoauthorCS #Nodes 2708 3327 19717 18333 #Edges 5429 4732 44338 81894 #Features 1433 3703 500 6805 #Classes 7 6 3 15 #Training Nodes 140 120 60 600 #Validation Nodes 500 500 500 2250 #Testing Nodes 1000 1000 1000 15483 B Running Environment All the GNN models and normalization approaches are implemented in PyTorch, and tested on a machine with 24 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GB processors, GeForce GTX-1080 Ti 12 GB GPU, and 128GB memory size. We implement the group normalization in a parallel way. Thus the practical time cost of our DGN is comparable to that of traditional batch normalization. C GNN Models We test over three general GNN models to illustrate the over-smoothing issue, including graph convo- lutional networks (GCN) [10], graph attention networks (GAT) [11] and simple graph convolution (SGC) networks [12]. We list their neighbor aggregation functions in Table 4. Table 4: Neighbor aggregation function at a graph convolutional layer for GCN, GAT and SGC. Model Neighbor aggregation function GCN h(k) v = ReLU(∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) W(k)h(k−1) v′ ) GAT h(k) v = ReLU(∑ v′∈N(v)∪{v}a(k) vv′W(k)h(k−1) v′ ) SGC h(k) v = ∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) h(k−1) v′ Considering the message passing strategy as shown by Eq. (1) in the main manuscript, we explain the key properties of GCN, GAT and SGC as follows. GCN merges the information from node itself and its neighbors weighted by vertices’ degrees, wherea(k) vv′ = 1./ √ (|N(v)|+ 1) ·(|N(v′)|+ 1). Functions AGG and COM are realized by a summation pooling. The activation function of ReLU is then applied to non-linearly transform the latent embedding. Based on GCN, GAT uses an additional attention layer to learn link weight a(k) vv′. GAT aggregates neighbors with the trainable link weights, and achieves signiﬁcant improvements in a variety of applications. SGC is simpliﬁed from GCN by removing all trainable parameters W(k) and nonlinear activations between successive layers. It has been empirically shown that these simpliﬁcations do not negatively impact classiﬁcation accuracy, and even relive the problems of over-ﬁtting and vanishing gradients in deeper models. 12D Normalization Baselines Batch normalization is ﬁrst applied between the successive convolutional layers in CNNs [23]. It is extended to graph neural networks to improve node representation learning and generalization [22]. Taking embedding matrix H(k) as input after each layer, batch normalization scales the node rep- resentations using running mean and variance, and generates a new embedding matrix for the next graph convolutional layer. Formally, we have: ˜H(k) = γ(H(k) −µ σ ) + β ∈Rn×d(k) . µand σdenote the vectors of running mean and standard deviation, respectively; γ and β denote the trainable scale and shift vectors, respectively. Recently, pair normalization has been proposed to tackle the over-smoothing issue in GNNs, targeting at maintaining the average node pair distance over a graph [24]. Pair normalization is a simplifying realization of batch normalization by removing the trainable γ and β. In this work, we augment each graph convolutional layer via appending a normalization module, in order to validate the effectiveness of normalization technique in relieving over-smoothing and enabling deeper GNNs. E Hyperparameter Tuning in DGN The balancing factor, λ, is crucial to determine the trade-off between input feature preservation and group normalization in DGN. It needs to be tuned carefully as GNN models increase the number of layers. To be speciﬁc, we consider the candidate set {5 ·10−4,1 ·10−3,2 ·10−3,3 ·10−3,5 · 10−3,1 ·10−2,2 ·10−2,3 ·10−2,5 ·10−2}. For each speciﬁc model, we use a few epochs to choose the optimal λon the validation set, and then evaluate it on the testing set. We observe that the value of λtends to be larger in the model accompanied with more graph convolutional layers. That is because the over-smoothing issue gets worse with the increase in layer number. The group normalization is much more required to separate the node representations of different classes. F Instance Information Gain In this work, we adopt kernel-density estimators (KDE), one of the common non-parametric ap- proaches, to estimate the mutual information between input feature and representation vector [29, 30]. A key assumption in KDE is that the input feature (or output representation vector) of neural networks is distributed as a mixture of Gaussians. Since a neural network is a deterministic function of the input feature after training, the mutual information would be inﬁnite without such assumption. In the following, we ﬁrst formally deﬁne the Gaussian assumption, input probability distribution and representation probability distribution, and then present how to obtain the instance information gain based on the mutual information metric. Gaussian assumption. In the graph signal processing, it is common to assume that the collected input feature contains both true signal and noise. In other word, we have the input feature as follows: xv = ¯xv + ϵx. ¯xv denotes the true value, and ϵx ∼N(0,σ2I) denotes the added Gaussian noise with variance σ2. Therefore, input feature xv is a Gaussian variable centered on its true value. Input probability distribution. We treat the empirical distribution of input samples as true distri- bution. Given a dataset accompanied with nsamples, we have a series of input features{x1,··· ,xn} for all the samples. Each node feature is sampled with probability 1/|V|following the empirical uniform distribution. Let |V|denotes the number of samples, and let Xdenote the random variable of input features. Based on the above Gaussian assumption, probability PX(xv) of input feature xv is obtained by the product of 1/|V|with Gaussian probability centered on true value ¯xv. Representation probability distribution. Let Hdenote the random variable of node represen- tations. To obtain probability PH(hv) of continuous vector hv, a general approach is to bin and transform Hinto a new discrete variable. However, with the increasing dimensions of hv, it is non-trivial to statistically count the frequencies of all possible discrete values. Considering the task of node classiﬁcation, the index of largest element along vector hv ∈RC×1 is regarded as the label 13of a node. We propose a new binning approach that labels the whole vector hv with the largest index zv. In this way, we only have Cclasses of discrete values to facilitate the frequency counting. To be speciﬁc, let Pc denote the number of representation vectors whose indexes zv = c. The probability of a discrete variable with class cis given by: pc = PH(zv = c) = Pc∑C l=1 Pl . Mutual information calculation. Based on KDE approach, a lower bound of mutual information between input feature and representation vector can be calculated as: GIns = I(X; H) = ∑ xv∈X,hv∈HPXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv) = H(X) −H(X|H) ≥− 1 |V| ∑ ilog 1 |V| ∑ jexp(−1 2 ||xi−xj||2 2 4σ2 ) −∑C c=1 pc[−1 Pc ∑ i,zi=clog 1 Pc ∑ j,zj=cexp(−1 2 ||xi−xj||2 2 4σ2 )]. The sum over i,zi = crepresents a summation over all the input features whose representation vectors are labeled with zi = c. PXH(xv,hv) denotes the joint probability of xv and hv. The effectiveness of GIns in measuring mutual information between input feature and node representation has been demonstrated in the experimental results. As illustrated in Figures 4-7, GIns decreases with the increasing number of graph convolutional layers. This practical observation is in line with the human expert knowledge about neighbor aggregation strategy in GNNs. The neighbor aggregation function as shown in Table 4 is in fact a low-passing smoothing operation, which mixes the input feature of a node with those of its neighbors gradually. At the extreme cases where K = 30 or 120, we ﬁnd that GIns approaches to zero in GNN models without normalization. The loss of informative input feature leads to the dropping of node classiﬁcation accuracy. However, our DGN keeps the input information during graph convolutions and normalization to some extent, resulting in the largest GIns compared with the other normalization approaches. G Performance Comparison on Attributed Graphs In this section, we report the model performances in terms of test accuracy, instance information gain and group distance ratio achieved on all the concerned datasets in Figures 4-7. We make the following observations: • Comparing with other normalization techniques, our DGN generally slows down the dropping of test accuracy with the increase in layer number. Even for GNN models associated with a small number of layers (i.e., G≤5), DGN achieves the competitive performance compared with none normalization. The adoption of DGN module does not damage the model performance, and prevents model from suffering over-smoothing issue when GNN goes deeper. • DGN achieves the larger or comparable instance information gains in all cases, especially for GAT models. That is because DGN keeps embedding matrix H(k) and prevents over-normalization within each group. The preservation of H(k) saves input features to some extent after each layer of graph convolutions and normalization. In an attributed graph, the improved preservation of informative input features in the ﬁnal representations will signiﬁcantly facilitate the downstream node classiﬁcation. Furthermore, such preservation is especially crucial for GNN models with a few layers, since the over-smoothing issue has not appeared. • DGN normalizes each group of node representations independently to generally improve the group distance ratio, especially for models GCN and GAT. A larger value of group distance ratio means that the node representation distributions from all groups are disentangled to address the over-smoothing issue. Although the ratios of DGN are smaller than those of pair normalization in some cases upon SGC framework, we still achieve the largest test accuracy. That may be because the intra-group distance in DGN is much smaller than that of pair normalization. A small value of intra-group distance would facilitate the node classiﬁcation within the same group. We will further compare the intra-group distance in scenarios with missing features in the following experiments. 140 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 0.10 Instance gain None batch pair group 0 10 20 30 1 2 3 4 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0.10 0 10 20 30 1 2 3 4 0 50 100 Layers 0.4 0.6 0.8SGC 0 50 100 Layers 0.050 0.075 0.100 0.125 0 50 100 Layers 2 3 Figure 4: The test accuracy, instance information gain, and group distance ratio in attributed Cora. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 0.000 0.025 0.050 0.075 Instance gain None batch pair group 0 10 20 30 1.0 1.5 2.0 Group ratio 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 0.000 0.025 0.050 0.075 0 10 20 30 1.0 1.5 2.0 0 50 100 Layers 0.5 0.6 0.7SGC 0 50 100 Layers 0.06 0.08 0 50 100 Layers 1.25 1.50 1.75 2.00 Figure 5: The test accuracy, instance information gain, and group distance ratio in attributed Citeseer. We compare differentiable group normalization with none, batch and pair normalizations. 150 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 Instance gain None batch pair group 0 10 20 30 1 2 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0 10 20 30 1.0 1.5 2.0 2.5 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 0.025 0.050 0.075 0.100 0 50 100 Layers 1.5 2.0 Figure 6: The test accuracy, instance information gain, and group distance ratio in attributed Pubmed. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 0.0 0.1 0.2 0.3 Instance gain None batch pair group 0 10 20 30 2 4 6 Group ratio 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 0.0 0.1 0.2 0.3 0 10 20 30 2 4 6 8 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 0.0 0.1 0.2 0.3 0 50 100 Layers 2 4 6 Figure 7: The test accuracy, instance information gain, and group distance ratio in attributed Coau- thorCS. We compare differentiable group normalization with none, batch and pair normalizations. 16H Performance Comparison in Scenarios with Missing Features In this section, we report the model performances in terms of test accuracy, group distance ratio and intra-group distance achieved in scenarios with missing features in Figures 8-11. The intra-group distance is calculated by node pair distance averaged within the same group. Its mathematical expression is given by the denominator of Equation (3) in the main manuscript. We make the following observations: • DGN achieves the largest test accuracy by exploring the deeper neural architecture with a larger number of graph convolutional layers. In the scenarios with missing features, GNN model relies highly on the neighborhood structure to classify nodes. DGN enables the deeper GNN model to exploit neighborhood structure with multiple hops away, and at the same time relieves the over-smoothing issue. • Comparing with other normalization techniques, DGN generally improves the group distance ratio to relieve over-smoothing issue. Although in some cases the ratios are smaller than those of pair normalization upon SGC framework, we still achieve the comparable or even better test accuracy. That is because DGN has a smaller intra-group distance to facilitate node classiﬁcation within the same group, which is analyzed in the followings. • DGN obtains an appropriate intra-group distance to optimize the node classiﬁcation task. While the over-smoothing issue results in an extremely-small distance in the model without normalization, a larger one in pair normalization leads to the inaccurate node classiﬁcation within each group. That is because the pair normalization is designed to maintain the distance between each pair of nodes, no matter whether they locate in the same class group or not. The divergence of node representations in a group prevents a downstream classiﬁer to assign them the same class label. 170 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1 2 3 Group ratio None batch pair group 0 10 20 30 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1 2 3 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 2 3 0 50 100 Layers 0.5 1.0 Figure 8: The test accuracy, group distance ratio and intra-group distance in Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4GCN Test accuracy 0 10 20 30 1.1 1.2 1.3 Group ratio None batch pair group 0 10 20 30 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4GAT 0 10 20 30 1.0 1.2 1.4 0 10 20 30 0.0 0.5 1.0 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.2 1.4 0 50 100 Layers 0.5 1.0 Figure 9: The test accuracy, group distance ratio and intra-group distance in Citeseer with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 180 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1.0 1.2 1.4 1.6 Group ratio None batch pair group 0 10 20 30 0.0 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1.00 1.25 1.50 1.75 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.0 1.5 2.0 0 50 100 Layers 0.0 0.5 1.0 Figure 10: The test accuracy, group distance ratio and intra-group distance in Pubmed with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 2 3 4 Group ratio None batch pair group 0 10 20 30 0.00 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 2 3 4 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 2 4 0 50 100 Layers 0.0 0.5 1.0 Figure 11: The test accuracy, group distance ratio and intra-group distance in CoauthorCS with missing features. We compare differentiable group normalization with none, batch and pair normal- izations. 19",
      "meta_data": {
        "arxiv_id": "2006.06972v1",
        "authors": [
          "Kaixiong Zhou",
          "Xiao Huang",
          "Yuening Li",
          "Daochen Zha",
          "Rui Chen",
          "Xia Hu"
        ],
        "published_date": "2020-06-12T07:18:02Z",
        "pdf_url": "https://arxiv.org/pdf/2006.06972v1.pdf"
      }
    },
    {
      "title": "A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks",
      "abstract": "Oversmoothing is a central challenge of building more powerful Graph Neural\nNetworks (GNNs). While previous works have only demonstrated that oversmoothing\nis inevitable when the number of graph convolutions tends to infinity, in this\npaper, we precisely characterize the mechanism behind the phenomenon via a\nnon-asymptotic analysis. Specifically, we distinguish between two different\neffects when applying graph convolutions -- an undesirable mixing effect that\nhomogenizes node representations in different classes, and a desirable\ndenoising effect that homogenizes node representations in the same class. By\nquantifying these two effects on random graphs sampled from the Contextual\nStochastic Block Model (CSBM), we show that oversmoothing happens once the\nmixing effect starts to dominate the denoising effect, and the number of layers\nrequired for this transition is $O(\\log N/\\log (\\log N))$ for sufficiently\ndense graphs with $N$ nodes. We also extend our analysis to study the effects\nof Personalized PageRank (PPR), or equivalently, the effects of initial\nresidual connections on oversmoothing. Our results suggest that while PPR\nmitigates oversmoothing at deeper layers, PPR-based architectures still achieve\ntheir best performance at a shallow depth and are outperformed by the graph\nconvolution approach on certain graphs. Finally, we support our theoretical\nresults with numerical experiments, which further suggest that the\noversmoothing phenomenon observed in practice can be magnified by the\ndifficulty of optimizing deep GNN models.",
      "full_text": "A NON -ASYMPTOTIC ANALYSIS OF OVERSMOOTHING IN GRAPH NEURAL NETWORKS Xinyi Wu1,2, Zhengdao Chen3*, William Wang2, and Ali Jadbabaie1,2 1Institute for Data, Systems, and Society (IDSS), Massachusetts Institute of Technology 2Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology 3Courant Institute of Mathematical Sciences, New York University ABSTRACT Oversmoothing is a central challenge of building more powerful Graph Neural Networks (GNNs). While previous works have only demonstrated that oversmoothing is inevitable when the number of graph convolutions tends to inﬁnity, in this paper, we precisely characterize the mechanism behind the phenomenon via a non-asymptotic analysis. Speciﬁcally, we distinguish between two different effects when applying graph convolutions—an undesirable mixing effect that homogenizes node representations in different classes, and a desirable denoising effect that homogenizes node representations in the same class. By quantifying these two effects on random graphs sampled from the Contextual Stochastic Block Model (CSBM), we show that oversmoothing happens once the mixing effect starts to dominate the denoising effect, and the number of layers required for this transition is O(log N/log(log N)) for sufﬁciently dense graphs with N nodes. We also extend our analysis to study the effects of Personalized PageRank (PPR), or equivalently, the effects of initial residual connections on oversmoothing. Our results suggest that while PPR mitigates oversmoothing at deeper layers, PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs. Finally, we support our theoretical results with numerical experiments, which further suggest that the oversmoothing phenomenon observed in practice can be magniﬁed by the difﬁculty of optimizing deep GNN models. 1 Introduction Graph Neural Networks (GNNs) are a powerful framework for learning with graph-structured data and have shown great promise in diverse domains such as molecular chemistry, physics, and social network analysis [1, 2, 3, 4, 5, 6, 7]. Most GNN models are built by stacking graph convolutions or message-passing layers [8], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes. The most representative and popular example is the Graph Convolutional Network (GCN) [9], which has demonstrated success in node classiﬁcation, a primary graph task which asks for node labels and identiﬁes community structures in real graphs. Despite these achievements, the choice of depth for these GNN models remains an intriguing question. GNNs often achieve optimal classiﬁcation performance when networks are shallow. Many widely used GNNs such as the GCN are no deeper than 4 layers [9, 10], and it has been observed that for deeper GNNs, repeated message-passing makes node representations in different classes indistinguishable and leads to lower node classiﬁcation accuracy—a phenomenon known as oversmoothing [9, 11, 12, 10, 13, 14, 15, 16]. Through the insight that graph convolutions can be regarded as low-pass ﬁlters on graph signals, prior studies have established that oversmoothing is inevitable when the number of layers in a GNN increases to inﬁnity [11, 13]. However, these asymptotic analyses do not fully explain the rapid occurrence of oversmoothing when we increase the network depth, let alone the fact that for some datasets, having no graph convolution is even optimal [17]. These observations motivate the following key questions about oversmoothing in GNNs: Why does oversmoothing happen at a relatively shallow depth? Can we quantitatively model the effect of applying a ﬁnite number of graph convolutions and theoretically predict the “sweet spot” for the choice of depth? In this paper, we propose a non-asymptotic analysis framework to study the effects of graph convolutions and oversmoothing using the Contextual Stochastic Block Model (CSBM) [18]. The CSBM mimics the community structure Correspondence to: xinyiwu@mit.edu *Now at Google. arXiv:2212.10701v2  [cs.LG]  1 Mar 2023A B Figure 1: Illustration of how oversmoothing happens. Stacking GNN layers will increase both the mixing and denoising effects counteracting each other. Depending on the graph characteristics, either the denoising effect dominates the mixing effect, resulting in less difﬁculty classifying nodes ( A), or the mixing effect dominates the denoising effect, resulting in more difﬁculty classifying nodes (B)—this is when oversmoothing starts to happen. of real graphs and enables us to evaluate the performance of linear GNNs through the probabilistic model with ground truth community labels. More importantly, as a generative model, the CSBM gives us full control over the graph structure and allows us to analyze the effect of graph convolutions non-asymptotically. In particular, we distinguish between two counteracting effects of graph convolutions: • mixing effect (undesirable): homogenizing node representations in different classes; • denoising effect (desirable): homogenizing node representations in the same class. Adding graph convolutions will increase both the mixing and denoising effects. As a result, oversmoothing happens not just because the mixing effect keeps accumulating as the depth increases, on which the asymptotic analyses are based [11, 13], but rather because the mixing effect starts to dominate the denoising effect (see Figure 1 for a schematic illustration). By quantifying both effects as a function of the model depth, we show that the turning point of the tradeoff between the two effects is O(log N/log(log N)) for graphs with N nodes sampled from the CSBM in sufﬁciently dense regimes. Besides new theory, this paper also presents numerical results directly comparing theoretical predictions and empirical results. This comparison leads to new insights highlighting the fact that the oversmoothing phenomenon observed in practice is often a mixture of pure oversmoothing and difﬁculty of optimizing weights in deep GNN models. In addition, we apply our framework to analyze the effects of Personalized PageRank (PPR) on oversmoothing. Personalized propagation of neural predictions (PPNP) and its approximate variant (APPNP) make use of PPR and its approximate variant, respectively, and were proposed as a solution to mitigate oversmoothing while retaining the ability to aggregate information from larger neighborhoods in the graph [12]. We show mathematically that PPR makes the model performance more robust to increasing number of layers by reducing the mixing effect at each layer, while it nonetheless reduces the desirable denoising effect at the same time. For graphs with a large size or strong community structure, the reduction of the denoising effect would be greater than the reduction of the mixing effect and thus PPNP and APPNP would perform worse than the baseline GNN on those graphs. Our contributions are summarized as follows: • We show that adding graph convolutions strengthens the denoising effect while exacerbates the mixing effect. Over- smoothing happens because the mixing effect dominates the denoising effect beyond a certain depth. For sufﬁciently dense CSBM graphs with N nodes, the required number of layers for this to happen is O(log N/log(log N)). • We apply our framework to rigorously characterize the effects of PPR on oversmoothing. We show that PPR reduces both the mixing effect and the denoising effect of message-passing and thus does not necessarily improve node classiﬁcation performance. • We verify our theoretical results in experiments. Through comparison between theory and experiments, we ﬁnd that the difﬁculty of optimizing weights in deep GNN architectures often aggravates oversmoothing. 2 Additional Related Work Oversmoothing problem in GNNs Oversmoothing is a well-known issue in deep GNNs, and many techniques have been proposed to relieve it practically [19, 20, 15, 21, 22]. On the theory side, by viewing GNN layers as a form of Laplacian ﬁlter, prior works have shown that as the number of layers goes to inﬁnity, the node representations within each connected component of the graph will converge to the same values [11, 13]. However, oversmoothing can be observed in GNNs with as few as 2 −4 layers [9, 10]. The early onset of oversmoothing renders it an important concern in practice, and it has not been satisfyingly explained by the previous asymptotic studies. Our work addresses this 2gap by quantifying the effects of graph convolutions as a function of model depth and justifying why oversmoothing happens in shallow GNNs. A recent study shared a similar insight of distinguishing between two competing effects of message-passing and showed the existence of an optimal number of layers for node prediction tasks on a latent space random graph model. But the result had no further quantiﬁcation and hence the oversmoothing phenomemon was still only characterized asymptotically [16]. Analysis of GNNs on CSBMs Stochastic block models (SBMs) and their contextual counterparts have been widely used to study node classiﬁcation problems [23, 24]. Recently there have been several works proposing to use CSBMs to theoretically analyze GNNs for the node classiﬁcation task. [ 25] used CSBMs to study the function of nonlinearity on the node classiﬁcation performance, while [26] used CSBMs to study the attention-based GNNs. More relevantly, [27, 28] showed the advantage of applying graph convolutions up to three times for node classiﬁcation on CSBM graphs. Nonetheless, they only focused on the desirable denoising effect of graph convolution instead of its tradeoff with the undesirable mixing effect, and therefore did not explain the occurance of oversmoothing. 3 Problem Setting and Main Results We ﬁrst introduce our theoretical analysis setup using the Contextual Stochastic Block Model (CSBM), a random graph model with planted community structure [18, 27, 28, 29, 25, 26]. We choose the CSBM to study GNNs for the node classiﬁcation task because the main goal of node classiﬁcation is to discover node communities from the data. The CSBM imitates the community structure of real graphs and provides a clear ground truth for us to evaluate the model performance. Moreover, it is a generative model which gives us full control of the data to perform a non-asymptotic analysis. We then present a set of theoretical results establishing bounds for the representation power of GNNs in terms of the best-case node classiﬁcation accuracy. The proofs of all the theorems and additional claims will be provided in the Appendix. 3.1 Notations We represent an undirected graph with N nodes by G= (A,X), where A∈{0,1}N×N is the adjacency matrix and X ∈RN is the node feature vector. For nodes u,v ∈[N], Auv = 1 if and only if uand vare connected with an edge in G, and Xu ∈R represents the node feature of u. We let 1 N denote the all-one vector of length N and D= diag(A1 N) be the degree matrix of G. 3.2 Theoretical Analysis Framework Contextual Stochastic Block Models We will focus on the case where the CSBM consists of two classes C1 and C2 of nodes of equal size, in total with N nodes. For any two nodes in the graph, if they are from the same class, they are connected by an edge independently with probability p, or if they are from different classes, the probability is q. For each node v∈Ci,i ∈{1,2}, the initial feature Xv is sampled independently from a Gaussian distribution N(µi,σ2), where µi ∈R,σ ∈(0,∞). Without loss of generality, we assume that µ1 <µ2. We denote a graph generated from such a CSBM as G(A,X) ∼CSBM(N,p,q,µ 1,µ2,σ2). We further impose the following assumption on the CSBM used in our analysis. Assumption 1. p,q = ω(log N/N) and p>q > 0. The choice p,q = ω(log N/N) ensures that the generated graph Gis connected almost surely [ 23] while being slightly more general than the p,q = ω(log2 N/N) regime considered in some concurrent works [27, 25]. In addition, this regime also guarantees that Ghas a small diameter. Real-world graphs are known to exhibit the “small-world\" phenomenon—even if the number of nodes N is very large, the diameter of graph remains small [ 30, 31]. We will see in the theoretical analysis (Section 3.3) how this small-diameter characteristic contributes to the occurrence of oversmoothing in shallow GNNs. We remark that our results in fact hold for the more general choice of p,q = Ω(log N/N), for which only the concentration bound in Theorem 1 needs to be modiﬁed in the threshold log N/N case where all the constants need a more careful treatment. Further, the choice p > qensures that the graph structure has homophily, meaning that nodes from the same class are more likely to be connected than nodes from different classes. This characteristic is observed in a wide range of real-world graphs [32, 29]. We note that this homophily assumption (p>q ) is not essential to our analysis, though we add it for simplicity since the discussion of homophily versus heterophily (p<q ) is not the focus of our paper. Graph convolution and linear GNN In this paper, our theoretical analysis focuses on the simpliﬁed linear GNN model deﬁned as follows: a graph convolution using the (left-)normalized adjacency matrix takes the operation 3h′= (D−1A)h, where hand h′are the input and output node representations, respectively. A linear GNN layer can then be deﬁned as h′= (D−1A)hW, where W is a learnable weight matrix. As a result, the output of nlinear GNN layers can be written as h(n) ∏n k=1 W(k), where h(n) = (D−1A)nX is the output of ngraph convolutions, and W(k) is the weight matrix of the kth layer. Since this is linear in h(n), it follows that n-layer linear GNNs have the equivalent representation power as linear classiﬁers applied to h(n). In practice, when building GNN models, nonlinear activation functions can be added between consecutive linear GNN layers. For additional results showing that adding certain nonlinearity would not improve the classiﬁcation performance, see Appendix K.1. Bayes error rate and z-score Thanks to the linearity of the model, we see that the representation of node v ∈Ci after ngraph convolutions is distributed as N(µ(n) i ,(σ(n))2), where the variance (σ(n))2 is shared between classes. The optimal node-wise classiﬁer in this case is the Bayes optimal classiﬁer, given by the following lemma. Lemma 1. Suppose the label y is drawn uniformly from {1,2}, and given y, x ∼ N(µ(n) y ,(σ(n))2). Then the Bayes optimal classiﬁer, which minimizes the probability of misclassiﬁcation among all classiﬁers, has decision boundary D= (µ1 + µ2)/2, and predicts y = 1,if x ≤D or y = 2,if x >D. The associated Bayes error rate is 1 −Φ(z(n)), where Φ denotes the cumulative distribution function of the standard Gaussian distribution and z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) is the z-score of Dwith respect to N(µ(n) 1 ,(σ(n))2). Lemma 1 states that we can estimate the optimal performance of an n-layer linear GNN through the z-score z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n). A higher z-score indicates a smaller Bayes error rate, and hence a better expected performance of node classiﬁcation. The z-score serves as a basis for our quantitative analysis of oversmoothing. In the following section, by estimating µ(n) 2 −µ(n) 1 and (σ(n))2, we quantify the two counteracting effects of graph convolutions and obtain bounds on the z-score z(n) as a function of n, which allows us to characterize oversmoothing quantitatively. Speciﬁcally, there are two potential interpretations of oversmoothing based on the z-score: (1) z(n) <z(n⋆), where n⋆ = arg maxn′z(n′); and (2) z(n) <z (0). They correspond to the cases (1) n>n ⋆; and (2) n>n 0, where n0 ≥0 corresponds to the number of layers that yield a z-score on par with z(0). The bounds on the z-score z(n), z(n) lower and z(n) upper, enable us to estimate n⋆ and n0 under different scenarios and provide insights into the optimal choice of depth. 3.3 Main Results We ﬁrst estimate the gap between the means µ(n) 2 −µ(n) 1 with respect to the number of layers n. µ(n) 2 −µ(n) 1 measures how much node representations in different classes have homogenized afternGNN layers, which is the undesirable mixing effect. Lemma 2. For n∈N ∪{0}, assuming D−1A≈E[D]−1E[A], µ(n) 2 −µ(n) 1 = (p−q p+ q )n (µ2 −µ1) . Lemma 2 states that the means µ(n) 1 and µ(n) 2 get closer exponentially fast and as n →∞, both µ(n) 1 and µ(n) 2 will converge to the same value (in this case ( µ(n) 1 + µ(n) 2 ) /2). The rate of change (p−q)/(p+ q) is determined by the intra-community edge density pand the inter-community edge density q. Lemma 2 suggests that graphs with higher inter-community density (q) or lower intra-community density (p) are expected to suffer from a higher mixing effect when we perform message-passing. We provide the following concentration bound for our estimate of µ(n) 2 −µ(n) 1 , which states that the estimate concentrates at a rate of O(1/ √ N(p+ q)). Theorem 1. Fix K ∈N and r> 0. There exists a constant C(r,K) such that with probability at least 1 −O(1/Nr), it holds for all 1 ≤k≤Kthat |(µ(k) 2 −µ(k) 1 ) − (p−q p+ q )k (µ2 −µ1)|≤ C√ N(p+ q) . 4We then study the variance (σ(n))2 with respect to the number of layers n. The variance (σ(n))2 measures how much the node representations in the same class have homogenized, which is the desirable denoising effect. We ﬁrst state that no matter how many layers are applied, there is a nontrivial ﬁxed lower bound for (σ(n))2 for a graph with N nodes. Lemma 3. For all n∈N ∪{0}, 1 Nσ2 ≤(σ(n))2 ≤σ2 . Lemma 3 implies that for a given graph, even as the number of layers ngoes to inﬁnity, the variance (σ(n))2 does not converge to zero, meaning that there is a ﬁxed lower bound for the denoising effect. See Appendix K.2 for the exact theoretical limit for the variance (σ(n))2 as ngoes to inﬁnity. We now establish a set of more precise upper and lower bounds for the variance (σ(n))2 with respect to the number of layers nin the following technical lemma. Lemma 4. Let a= Np/log N. With probability at least 1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 (σ(n))2 ≤min    ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k ,1   σ2 . Lemma 4 holds for all 1 ≤n≤N and directly leads to the following theorem with a clariﬁed upper bound where nis bounded by a constant K. Theorem 2. Let a = Np/log N. Fix K ∈N. There exists a constant C(K) such that with probability at least 1 −O(1/N), it holds for all 1 ≤n≤Kthat max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 ≤min { C min{a,2} 1 (N(p+ q))n,1 } σ2 . Theorem 2 states that the variance (σ(n))2 for each Gaussian distribution decreases more for larger graphs or denser graphs. Moreover, the upper bound implies that the variance (σ(n))2 will initially go down at least at a rate expo- nential in O(1/log N) before reaching the ﬁxed lower bound σ2/N suggested by Lemma 3. This means that after O(log N/log(log N)) layers, the desirable denoising effect homogenizing node representations in the same class will saturate and the undesirable mixing effect will start to dominate. Why does oversmoothing happen at a shallow depth? For each node, message-passing with different-class nodes homogenizes their representations exponentially. The exponential rate depends on the fraction of different-class neighbors among all neighbors (Lemma 2, mixing effect). Meanwhile, message-passing with nodes that have not been encountered before causes the denoising effect, and the magnitude depends on the absolute number of newly encountered neighbors. The diameter of the graph is approximately log N/log(Np) in the p,q = Ω(log N/N) regime [33], and thus is at most log N/log(log N) in our case. After the number of layers surpasses the diameter, for each node, there will be no nodes that have not been encountered before in message-passing and hence the denoising effect will almost vanish (Theorem 2, denoising effect). log N/log(log N) grows very slowly with N; for example, when N = 106,log N/log(log N) ≈8. This is why even in a large graph, the mixing effect will quickly dominate the denoising effect when we increase the number of layers, and so oversmoothing is expected to happen at a shallow depth1. Our theory suggests that the optimal number of layers, n⋆, is at most O(log N/log(log N)). For a more quantitative estimate, we can use Lemma 2 and Theorem 2 to compute bounds z(n) lower and z(n) upper for z= 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) and use them to infer n⋆ and n0, as deﬁned in Section 3.2. See Appendix H for detailed discussion. Next, we investigate the effect of increasing the dimension of the node features X. So far, we have only considered the case with one-dimensional node features. The following proposition states that if features in each dimension are independent, increasing input feature dimension decreases the Bayes error rate for a ﬁxed n. The intuition is that when node features provide more evidence for classiﬁcation, it is easier to classify nodes correctly. Proposition 1. Let the input feature dimension be d, X ∈RN×d. Without loss of generality, suppose for node vin Ci, initial node feature Xv ∼N([µi]d,σ2Id) independently. Then the Bayes error rate is 1 −Φ (√ d 2 (µ(n) 2 −µ(n) 1 ) σ(n) ) = 1 −Φ (√ d 2 z(n) ) , where Φ denotes the cumulative distribution function of the standard Gaussian distribution. Hence the Bayes error rate is decreasing in d, and as d→∞, it converges to 0. 1For mixing and denoising effects in real data, see Appendix K.5. 54 The effects of Personalized PageRank on oversmoothing Our analysis framework in Section 3.3 can also be applied to GNNs with other message-passing schemes. Speciﬁcally, we can analyze the performance of Personalized Propagation of Neural Predictions (PPNP) and its approximate variant, Approximate PPNP (APPNP), which were proposed for alleviating oversmoothing while still making use of multi-hop information in the graph. The main idea is to use Personalized PageRank (PPR) or the approximate Personalized PageRank (APPR) in place of graph convolutions [12]. Mathematically, the output of PPNP can be written as hPPNP = α(IN −(1 −α)(D−1A))−1X, while APPNP computes hAPPNP(n+1) = (1 −α)(D−1A)hAPPNP(n) + αX iteratively in n, where IN is the identity matrix of size N and in both cases αis the teleportation probability. Then for nodes in Ci,i ∈{1,2}, the node representations follow a Gaussian distribution N ( µPPNP i ,(σPPNP)2 ) after applying PPNP, or a Gaussian distributionN ( µAPPNP(n) i ,(σAPPNP(n))2 ) after applying nAPPNP layers. We quantify the effects on the means and variances for PPNP and APPNP in the CSBM case. We can similarly use them to calculate the z-score of (µ1 + µ2)/2 and compare it to the one derived for the baseline GNN in Section 3. The key idea is that the PPR propagation can be written as a weighted average of the standard message-passing, i.e. α(IN −(1 −α)(D−1A))−1 = ∑∞ k=0(1 −α)k(D−1A)k [34]. We ﬁrst state the resulting mixing effect measured by the difference between the two means. Proposition 2. Fix r> 0,K ∈N. For PPNP , with probability at least1−O(1/Nr), there exists a constantC(α,r,K ) such that µPPNP 2 −µPPNP 1 = p+ q p+ 2−α α q(µ2 −µ1) + ϵ. where the error term |ϵ|≤ C/ √ N(p+ q) + (1−α)K+1. Proposition 3. Let r> 0. For APPNP , with probability at least1 −O(1/Nr), µAPPNP(n) 2 −µAPPNP(n) 1 = ( p+ q p+ 2−α α q + (2 −2α)q αp+ (2 −α)q(1 −α)n (p−q p+ q )n) (µ2 −µ1) + ϵ. where the error term ϵis the same as the one deﬁned in Theorem 1 for the case of K = n. Both p+q p+ 2−α α q and (2−2α)q αp+(2−α)q(1 −α) ( p−q p+q ) are monotone increasing in α. Hence from Proposition 2 and 3, we see that with larger α, meaning a higher probability of teleportation back to the root node at each step of message-passing, PPNP and APPNP will indeed make the difference between the means of the two classes larger: while the difference in means for the baseline GNN decays as (p−q p+q )n , the difference for PPNP/APPNP is lower bounded by a constant. This validates the original intuition behind PPNP and APPNP that compared to the baseline GNN, they reduce the mixing effect of message-passing, as staying closer to the root node means aggregating less information from nodes of different classes. This advantage becomes more prominent when the number of layers nis larger, where the model performance is dominated by the mixing effect: as ntends to inﬁnity, while the means converge to the same value for the baseline GNN, their separation is lower-bounded for PPNP/APPNP. However, the problem with the previous intuition is that PPNP and APPNP will also reduce the denoising effect at each layer, as staying closer to the root node also means aggregating less information from new nodes that have not been encountered before. Hence, for an arbitrary graph, the result of the tradeoff after the reduction of both effects is not trivial to analyze. Here, we quantify the resulting denoising effect for CSBM graphs measured by the variances. We denote (σ(n))2 upper as the variance upper bound for depth nin Lemma 4. Proposition 4. For PPNP , with probability at least1 −O(1/N), it holds for all 1 ≤K ≤N that max {α2 min{a,2} 10 , 1 N } σ2 ≤(σPPNP)2 ≤max   α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 ,σ2   . Proposition 5. For APPNP , with probability at least1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) , 1 N } σ2 ≤(σAPPNP(n))2 , (σAPPNP(n))2 ≤min    ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 ,σ2   . 6By comparing the lower bounds in Proposition 4 and 5 with that in Theorem 2, we see that PPR reduces the beneﬁcial denoising effect of message-passing: for large or dense graphs, while the variances for the baseline GNN decay as 1/(Np)n, the variances for PPNP/APPNP are lower bounded by the constant α2 min{a,2}/10. In total, the mixing effect is reduced by a factor of (p−q p+q )n , while the denoising effect is reduced by a factor of 1/(Np)n. Hence PPR would cause greater reduction in the denoising effect than the improvement in the mixing effect for graphs whereN and pare large. This drawback would be especially notable at a shallow depth, where the denoising effect is supposed to dominate the mixing effect. APPNP would perform worse than the baseline GNN on these graphs in terms of the optimal classiﬁcation performance. We remark that in each APPNP layer, another way to interpret the termαX is to regard it as a residual connection to the initial representation X [15]. Thus, our theory also validates the empirical observation that adding initial residual connections allows us to build very deep models without catastrophic oversmoothing. However, our results suggest that initial residual connections do not guarantee an improvement in model performance by themselves. 5 Experiments In this section, we ﬁrst demonstrate our theoretical results in previous sections on synthetic CSBM data. Then we discuss the role of optimizing weights W(k) in GNN layers in the occurrence of oversmoothing through both synthetic data and the three widely used benchmarks: Cora, CiteSeer and PubMed [35]. Our results highlight the fact that the oversmoothing phenomenon observed in practice may be exacerbated by the difﬁculty of optimizing weights in deep GNN models. More details about the experiments are provided in Appendix J. 5.1 The effect of graph topology on oversmoothing We ﬁrst show how graph topology affects the occurrence of oversmoothing and the effects of PPR. We randomly generated synthetic graph data from CSBM( N = 2000 , p, q = 0 .0038, µ1 = 1 , µ2 = 1 .5, σ2 = 1 ). We used 60%/20%/20% random splits and ran GNN and APPNP with α= 0.1. For results in Figure 2, we report averages over 5 graphs and for results in Figure 3, we report averages over 5 runs. In Figure 2, we study how the strength of community structure affects oversmoothing. We can see that when graphs have a stronger community structure in terms of a higher intra-community edge density p, they would beneﬁt more from repeated message-passing. As a result, given the same set of node features, oversmoothing would happen later and a classiﬁer could achieve better classiﬁcation performance. A similar trend can also be observed in Figure 4A. Our theory predicts n⋆ and n0, as deﬁned in Section 3.2, with high accuracy. In Figure 3, we compare APPNP and GNN under different graph topologies. In all three cases, APPNP manifests its advantage of reducing the mixing effect compared to GNN when the number of layers is large, i.e. when the undesirable mixing effect is dominant. However, as Figure 3B,C show, when we have large graphs or graphs with strong community structure, APPNP’s disadvantage of concurrently reducing the denoising effect is more severe, particularly when the number of layers is small. As a result, APPNP’s optimal performance is worse than the baseline GNN. These observations accord well with our theoretical discussions in Section 4. A B C Figure 2: How the strength of community structure affects oversmoothing. When graphs have stronger community structure (i.e. higher a), oversmoothing would happen later. Our theory (gray bar) predicts the optimal number of layers n⋆ in practice (blue) with high accuracy (A). Given the same set of features, a classiﬁer has signiﬁcantly better performance on graphs with higher a(B,C). 7A (base case) B (larger graph) C (stronger community) Figure 3: Comparison of node classiﬁcation performance between the baseline GNN and APPNP. The performance of APPNP is more robust when we increase the model depth. However, compared to the base case (A), APPNP tends to have worse optimal performance than GNN on graphs with larger size ( B) or stronger community structure (C), as predicted by the theory. 5.2 The effect of optimizing weights on oversmoothing We investigate how adding learnable weightsW(k) in each GNN layers affects the node classiﬁcation performance in practice. Consider the case when all the GNN layers used have width one, meaning that the learnable weight matrix W(k) in each layer is a scalar. In theory, the effects of adding such weights on the means and the variances would cancel each other and therefore they would not affect the z-score of our interest and the classiﬁcation performance. Figure 4A shows the the value of n0 predicted by the z-score, the actual n0 without learnable weights in each GNN layer (meaning that we apply pure graph convolutions ﬁrst, and only train weights in the ﬁnal linear classiﬁer) according to the test accuracy and the actual n0 with learnable weights in each GNN layer according to the test accuracy. The results are averages over 5 graphs for each case. We empirically observe that GNNs with weights are much harder to train, and the difﬁculty increases as we increase the number of layers. As a result, n0 is smaller for the model with weights and the gap is larger when n0 is supposed to be larger, possibly due to greater difﬁculty in optimizing deeper architectures [36]. To relieve this potential optimization problem, we increase the width of each GNN layer [37]. Figure 4B,C presents the training and testing accuracies of GNNs with increasing width with respect to the number of layers on a speciﬁc synthetic example. The results are averages over 5 runs. We observe that increasing the width of the network mitigates the difﬁculty of optimizing weights, and the performance after adding weights is able to gradually match the performance without weights. This empirically validates our claim in Section 3.2 that adding learnable weights should not affect the representation power of GNN in terms of node classiﬁcation accuracy on CSBM graphs, besides empirical optimization issues. In practice, as we build deeper GNNs for more complicated tasks on real graph data, the difﬁculty of optimizing weights in deep GNN models persists. We revisit the multi-class node classiﬁcation task on the three widely used benchmark datasets: Cora, CiteSeer and PubMed [35]. We compare the performance of GNN without weights against the performance of GNN with weights in terms of test accuracy. We used 60%/20%/20% random splits, as in [38] and [39] and report averages over 5 runs. Figure 5 shows the same kind of difﬁculty in optimizing deeper models with A B C Figure 4: The effect of optimizing weights on oversmoothing using synthetic CSBM data. Compared to the GNN without weights, oversmoothing happens much sooner after adding learnable weights in each GNN layer, although these two models have the same representation power (A). As we increase the width of each GNN layer, the performance of GNN with weights is able to gradually match that of GNN without weights (B,C). 8Figure 5: The effect of optimizing weights on oversmoothing using real-world benchmark datasets. Adding learnable weights in each GNN layer does not improve node classiﬁcation performance but rather leads to optimization difﬁculty. learnable weights in each GNN layer as we have seen for the synthetic data. Increasing the width of each GNN layer still mitigates the problem for shallower models, but it becomes much more difﬁcult to tackle beyond 10 layers to the point that simply increasing the width could not solve it. As a result, although GNNs with and without weights are on par with each other when both are shallow, the former has much worse performance when the number of layers goes beyond 10. These results suggest that the oversmoothing phenomenon observed in practice is aggravated by the difﬁculty of optimizing deep GNN models. 6 Discussion Designing more powerful GNNs requires deeper understanding of current GNNs—how they work and why they fail. In this paper, we precisely characterize the mechanism of overmoothing via a non-asymptotic analysis and justify why oversmoothing happens at a shallow depth. Our analysis suggests that oversmoothing happens once the undesirable mixing effect homogenizing node representations in different classes starts to dominate the desirable denoising effect homogenizing node representations in the same class. Due to the small diameter characteristic of real graphs, the turning point of the tradeoff will occur after only a few rounds of message-passing, resulting in oversmoothing in shallow GNNs. It is worth noting that oversmoothing becomes an important problem in the literature partly because typical Convolutional Neural Networks (CNNs) used for image processing are much deeper than GNNs [40]. As such, researchers have been trying to use methods that have previously worked for CNNs to make current GNNs deeper [ 20, 15]. However, if we regard images as grids, images and real-world graphs have fundamentally different characteristics. In particular, images are giant grids, meaning that aggregating information between faraway pixels often requires a large number of layers. This contrasts with real-world graphs, which often have small diameters. Hence we believe that building more powerful GNNs will require us to think beyond CNNs and images and take advantage of the structure in real graphs. There are many outlooks to our work and possible directions for further research. First, while our use of the CSBM provided important insights into GNNs, it will be helpful to incorporate other real graph properties such as degree heterogeneity in the analysis. Additionally, further research can focus on the learning perspective of the problem. 7 Acknowledgement This research has been supported by a Vannevar Bush Fellowship from the Ofﬁce of the Secretary of Defense. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the Department of Defense or the U.S. Government. References [1] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IJCNN, 2005. [2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20:61–80, 2009. [3] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. 9[4] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timo- thy D. Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeurIPS, 2015. [5] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In NeurIPS, 2016. [6] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In NeurIPS, 2016. [7] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In ICLR, 2016. [8] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. [9] Thomas Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR, 2017. [10] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32:4–24, 2019. [11] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In AAAI, 2018. [12] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. [13] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In ICLR, 2020. [14] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In AAAI, 2020. [15] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020. [16] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. In NeurIPS, 2022. [17] Meng Liu, Zhengyang Wang, and Shuiwang Ji. Non-local graph neural networks. IEEE transactions on pattern analysis and machine intelligence, 2021. [18] Yash Deshpande, Andrea Montanari, Elchanan Mossel, and Subhabrata Sen. Contextual stochastic block models. In NeurIPS, 2018. [19] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018. [20] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In ICCV, 2019. [21] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864, 2020. [22] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020. [23] Emmanuel Abbe. Community detection and stochastic block models. Foundations and Trends in Communications and Information Theory, 14:1–162, 2018. [24] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural networks. In ICLR, 2019. [25] Rongzhe Wei, Haoteng Yin, J. Jia, Austin R. Benson, and Pan Li. Understanding non-linearity in graph neural networks from the bayesian-inference perspective. ArXiv, abs/2207.11311, 2022. [26] Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath. Graph attention retrospective. ArXiv, abs/2202.13060, 2022. [27] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Graph convolution for semi-supervised classiﬁca- tion: Improved linear separability and out-of-distribution generalization. In ICML, 2021. [28] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of graph convolutions in deep networks. ArXiv, abs/2204.09297, 2022. [29] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? In ICLR, 2022. 10[30] Michelle Girvan and Mark E. J. Newman. Community structure in social and biological networks. Proceedings of the National Academy of Sciences, 99:7821 – 7826, 2002. [31] Fan R. K. Chung. Graph theory in the information age. volume 57, pages 726–732, 2010. [32] David A. Easley and Jon M. Kleinberg. Networks, Crowds, and Markets: Reasoning about a Highly Connected World. 2010. [33] Fan Chung Graham and Linyuan Lu. The diameter of sparse random graphs. Advances in Applied Mathematics, 26:257–279, 2001. [34] Reid Andersen, Fan Chung Graham, and Kevin J. Lang. Local graph partitioning using pagerank vectors. In FOCS, 2006. [35] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016. [36] Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. In COLT, 2019. [37] Simon Shaolei Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In ICML, 2019. [38] Hongwei Wang and Jure Leskovec. Unifying graph convolutional neural networks and label propagation. ArXiv, abs/2002.06755, 2020. [39] Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label propagation and simple models out-performs graph neural networks. In ICLR, 2021. [40] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [41] Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition. In Stochastic Modelling and Applied Probability, 1996. [42] Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey.Internet mathematics, 3(1):79–127, 2006. [43] Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479–2506, 2016. [44] Linyuan Lu and Xing Peng. Spectra of edge-independent random graphs.The Electronic Journal of Combinatorics, 20:27, 2013. [45] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. [47] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. A Proof of Lemma 1 Following the deﬁnition of the Bayes optimal classiﬁer [41], B(x) = arg max i=1,2 P[y= i|x] , we get that the Bayes optimal classiﬁer has a linear decision boundary D= (µ1 + µ2)/2 such that the decision rule is {y= 1 if x≤D y= 2 if x> D. Probability of misclassiﬁcation could be written as P[y= 1,x> D] + P[y= 2,x ≤D] = P[x> D|y= 1]P[y= 1] + P[x≤D|y= 2]P[y= 2] = 1 2(P[x> D|y= 1] + P[x≤D|y= 2]) . 11When D= (µ1 + µ2)/2, the expression is called the Bayes error rate, which is the minimal probability of misclassiﬁca- tion among all classiﬁers. Geometrically, it is easy to see that the Bayes error rate equals 1 2 S, where Sis the overlapping area between the two Gaussian distributions N ( µ(n) 1 ,(σ(n))2 ) and N ( µ(n) 2 ,(σ(n))2 ) . Hence one can use the z-score of (µ1 + µ2)/2 with respect to either of the two Gaussian distributions to directly calculate the Bayes error rate. B Proof of Lemma 2 Under the heuristic assumption D−1A≈E[D]−1E[A], we can write µ(1) 1 = pµ1 + qµ2 p+ q , µ (1) 2 = pµ2 + qµ1 p+ q µ(k) 1 = pµ(k−1) 1 + qµ(k−1) 2 p+ q , µ (k) 2 = pµ(k−1) 2 + qµ(k−1) 1 p+ q , for all k∈N. Writing recursively, we get that µ(n) 1 = (p+ q)n + (p−q)n 2(p+ q)n µ1 + (p+ q)n −(p−q)n 2(p+ q)n µ2 , µ(n) 2 = (p+ q)n + (p−q)n 2(p+ q)n µ2 + (p+ q)n −(p−q)n 2(p+ q)n µ1 . C Proof of Theorem 1 We use ∥·∥2 to denote the spectral norm, ∥A∥2 = maxx:∥x∥=1 ∥Ax∥2. We denote ¯A= E[A], ¯D= E[D], d= A1 N and ¯d= E[d]i. We further deﬁne the following relevant vectors: w1 := 1 N, w 2 := ( 1 N/2 −1 N/2 ) , µ := ( µ11 N/2 µ21 N/2 ) . The quantity of interest is µ(k) 2 −µ(k) 1 = 1 N/2 w⊤ 2 (D−1A)kµ. C.1 Auxiliary results We record some properties of the adjacency matrices: 1. D−1Aand ¯D−1 ¯Ahave an eigenvalue of 1, corresponding to the (right) eigenvector w1. 2. If Jn = 1 n1 ⊤ n, where 1 n is all-one vector of length n, then ¯A:= ( pJN/2 qJN/2 qJN/2 pJN/2 ) . 3. ¯D= N 2 (p+ q)IN. 4. µ= αw1 + βw2, where α= µ1+µ2 2 and β = µ1−µ2 2 . To control the degree matrix D−1, we will use the following standard Chernoff bound [42]: Lemma 5 (Chernoff Bound). Let X1,...,X n be independent, S := ∑n i=1 Xi, and ¯S = E[S]. Then for all ε> 0, P(S ≤¯S−ε) ≤e−ε2/(2 ¯S), P(S ≥¯S+ ε) ≤e−ε2/(2( ¯S+ε/3)). We can thus derive a uniform lower bound on the degree of every vertex: Corollary 1. For every r >0, there is a constant C(r) such that whenever ¯d ≥Clog N, with probability at least 1 −N−r, 1 2 ¯d≤di ≤3 2 ¯d, for all 1 ≤i≤N. Consequently, with probability at least 1 −N−r, ∥D−1 −¯D−1∥2 ≤C/¯dfor some C. 12Proof. By applying Lemma 1 and a union bound, all degrees are within 1/2 ¯dof their expectations, with probability at least 1 −e−¯d/8+log N. Taking C = 8r+ 8 yields the desired lower bound. An analogous proof works for the upper bound. To show the latter part, write ∥D−1 −¯D−1∥2 = max 1≤i≤N |di −¯d| di¯d Using the above bounds, the numerator for each iis at most 1/2 ¯dand the denominator for each iis at least 1/2 ¯d2, with probability at least 1 −N−r. Combining the bounds yields the claim. We will also need a result on concentration of random adjacency matrices, which is a corollary of the sharp bounds derived in [43] Lemma 6 (Concentration of Adjacency Matrix) . For every r >0, there is a constant C(r) such that whenever ¯d≥log N, with probability at least 1 −N−r, ∥A−¯A∥2 <C √ ¯d. Proof. By corollary 3.12 from [43], there is a constant κsuch that P(∥A−¯A∥2 ≥3 √ ¯d+ t) ≤e−t2/κ+log N. Setting t= √ (1 + r) ¯d, C = 3 + √ (1 + r)κsufﬁces to achieve the desired bound. C.2 Sharp concentration of the random walk operator D−1A In this section, we aim to show the following concentration result for the random walk operator D−1A: Theorem 3. Suppose the edge probabilities are ω ( log N N ) , and let ¯dbe the average degree. For anyr, there exists a constant Csuch that for sufﬁciently large N, with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . Proof. We decompose the error E = D−1A−¯D−1 ¯A= D−1(A−¯A) + (D−1 −¯D−1) ¯A= T1 + T2, where T1 = D−1(A−¯A), T 2 = (D−1 −¯D−1) ¯A. We bound the two terms separately. Bounding T1: By Corollary 1, ∥D−1∥2 = max i1/di ≤2/¯d with probability 1 −N−r. Combining this with Lemma 6, we see that with probability at least 1 −2N−r, ∥D−1(A−¯A)∥2 ≤∥D−1∥2∥A−¯A∥2 ≤ C√¯d for some Cdepending only on r. Bounding T2: Similar to [44], we bound T2 by exploiting the low-rank structure of the expected adjacency matrix, ¯A. Recall that ¯Ahas a special block form. The eigendecomposition of ¯Ais thus ¯A= 2∑ j=1 λjw(j), where w(1) = 1√ N1 N,λ1 = N(p+q) 2 ,w(2) = 1√ N ( 1 N/2 −1 N/2 ) ,λ2 = N(p−q) 2 . 13Using the deﬁnition of the spectral norm, we can bound ∥T2∥2 as ∥T2∥2 ≤max ∥x∥=1 ∥(D−1 −¯D−1) ¯Ax∥2 ≤ max α∈R2,∥α∥=1 ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 . Note that when ∥α∥2 = 1, ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 2 = N∑ i=1 (1 di −1 ¯d )2   2∑ j=1 λjαjw(j) i   2 ≤ N∑ i=1 (1 di −1 ¯d )2 2∑ j=1 λ2 j(w(j) i )2 using Cauchy-Schwarz. Since |w(j) i |≤ 1√ N for all i,j, the second summation can be bounded by 1 N ∑2 j=1 λ2 j. Overall, the upper bound is now 1 N N∑ i=1 (di −¯d)2 (di¯d)2 2∑ j=1 λ2 j , Under the event of Corollary 1, di ≥C¯dfor some C <1. Under our setup, we also have λ2 1 = ¯d2, λ2 2 ≤ ¯d2. This means that the upper bound is 1 C2 ¯d2N∥d−¯d1 N∥2 2 , where dis the vector of node degrees. It remains to show that 1 N∥d−¯d1 N∥2 2 = O( ¯d). To do this, we use a form of Talagrand’s concentration inequality, given in [45]. Since the function 1√ N∥d−¯d1 N∥2 = 1√ N∥(A−¯dIN)1 N∥2 is a convex, 1-Lipschitz function of A, Theorem 6.10 from [45] guarantees that for any t> 0, P( 1√ N ∥d−¯d1 N∥2 >E[ 1√ N ∥d−¯d1 N∥2] + t) ≤e−t2/2. Using Jensen’s inequality, E[∥d−¯d1 N∥2] ≤ √ E[∥d−¯d1 N∥2 2] = √ N∑ i=1 Var(di) = √ NVar(d1) ≤ √ N¯d. If ¯d= ω(log N), we can guarantee that 1√ N ∥d−¯d1 N∥2 ≤C √ ¯d with probability at least 1 −e−(C−1)2 ¯d/2 = 1 −O(N−r) for an appropriate constant C. Thus we have shown that with high probability, T2 = O(1/ √¯d), which proves the claim. C.3 Proof of Theorem 1 Fix rand K. We desire to bound 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ. By the ﬁrst property of adjacency matrices in auxiliary results, it sufﬁces to bound β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2. 14where β = µ1−µ2 2 . We will show inductively that there is a Csuch that for every k= 1,...,K , ∥(D−1A)k −( ¯D−1 ¯A)k∥2 ≤C/ √ ¯d. If this is true, then Cauchy-Schwarz gives β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2 ≤β 1 N/2∥w2∥2∥(D−1A)k −( ¯D−1 ¯A)k∥2∥w2∥2 ≤C/ √ ¯d. By Theorem 3, we have that with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . So D−1A= ¯D−1 ¯A+ J where ∥J∥≤ C/ √¯d. Iterating, we have ∥(D−1A)k −( ¯D−1 ¯A)k∥2 = ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 (1) Inductively, (D−1A)k−1 = ( ¯D−1 ¯A)k−1 + H where ∥H∥2 ≤C/ √¯d. Plugging this in (1), we have ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 = ∥(( ¯D−1 ¯A)k−1 + H)( ¯D−1 ¯A+ J) −( ¯D−1 ¯A)k∥2 . Of these terms, ( ¯D−1 ¯A)k−1J has norm at most ∥J∥2, H( ¯D−1 ¯A) has norm at most ∥H∥2, and HJ has norm at most C/¯d.2 Hence the induction step is complete. We have thus shown that there is a constantC(r,K) such that with probability at least 1 −N−r, | 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ|≤ C ¯d . which proves the claim. By simulation one can verify that indeed 1 N/2 w⊤ 2 ( ¯D−1 ¯A)kµ ≈ ( p−q p+q )k (µ2 −µ1). Figure 6 presents µ(n) 1 ,µ(n) 2 calculated from simulation against predicted values from our theoretical results. The simulation results are averaged over 20 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 6: Comparison of the mean estimation in Lemma 2 against simulation results. D Proof of Lemma 3 Fix n and let the element in the ith row and jth column of (D−1A)n be p(n) ij . Consider a ﬁxed node i. The variance of the feature for node iafter nlayers of convolutions is (∑ j(p(n) ij )2)σ2, by the basic property of variance of sum. Since ∑ j|p(n) ij |= 1, it follows that ∑ j(p(n) ij )2 ≤1, which is the second inequality. To show the ﬁrst inequality, consider the following optimization problem: 2More precisely, the C becomes Ck, which is why we restrict the approximation guarantee to constant K. 15min p(n) ij ,1 ≤j ≤N ∑ j (p(n) ij )2 s.t. ∑ j p(n) ij = 1, p(n) ij ≥0, 1 ≤j ≤N This part of proof goes by contradiction. Suppose ∃k,l such that p(n) ik ̸= ∃p(n) il . Fixing all other p(n) ij ,j ̸= k,l, if we average p(n) ik and p(n) il , their sum of squares will strictly decrease while not breaking the constraints: 2 (p(n) ik + p(n) il 2 )2 −((p(n) ik )2 + (p(n) il )2) = −1 2(p(n) ik −p(n) il )2 <0 . So we obtain a contradiction. Thus to minimize ∑ j(p(n) ij )2, p(n) ij = 1 N,1 ≤j ≤N, and the mimimum is 1/N. E Proof of Lemma 4 The proof relies on the following deﬁnition of neighborhood size: in a graph G, we denote by Γk(x) the set of vertices in Gat distance kfrom a vertex x: Γk(x) = {y∈G : d(x,y) = k}. we deﬁne Nk(x) to be the set of vertices within distance kof x: Nk(x) = k⋃ i=0 Γi(x) . To prove the lower bound, we ﬁrst show an intermediate step that 1 |Nn|σ2 ≤(σ(n))2 . The proof is the same as the one for the ﬁrst inequality in Lemma 3, except we add in another constraint that for a ﬁxed i, the row pi·is |Nn(i)|-sparse. This implies that the minimum of ∑ j(p(n) ij )2 becomes 1/|Nn(i)|. The we apply the result on upper bound of neighborhood sizes in Erd˝os-Rényi graph G(N,p) (Lemma 2 [33]), as it also serves as upper bound of neighborhood sizes in SBM(N, p, q). The result implies that with probability at least 1 −O(1/N), we have |Nn|≤ 10 min{a,2}(Np)n,∀1 ≤n≤N. (2) We ignore ifor Nn because of all nodes are identical in CSBM, so the bound applies for every nodes in the graph. The proof of upper bound is combinatorial. Corollary 1 states that whenNis large, the degree of nodeiis approximately the expected degree in G, namely, E[degree] = N 2 (p+ q). Since p(n) ij = ∑ path P={i,v1,...,vn−1,j} 1 deg(i) 1 deg(v1)... 1 deg(vn−1) , (3) using the approximation of degrees, we get that p(n) ij = ( 2 N(p+ q) )n (# of paths P of length n between i and j) . Then we use a tree approximation to calculate the number of paths P of length nbetween iand jby regarding ias the root. Note that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 ∑ j∈Γn−2k (p(n) ij )2 (4) and for j ∈Γn−2k, a deterministic path P′of length n−2kis needed in order to reach j from i. This implies that there are only ksteps deviating from P′. There are (n−2k+ 1)k ways of choosing when to deviate. For each speciﬁc 16way of when to deviate, there are approximately E[degree]k ways of choosing the destinations for deviation. Hence in total, for j ∈Γn−2k, there are (n−2k+ 1)kE[degree]k path of length nbetween iand j. Thus p(n) ij = (n−2k+ 1)k ( 2 N(p+ q) )n−k . (5) Plug in (5) into (4), we get that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 |Γn−2k|(n−2k+ 1)2k ( 2 N(p+ q) )2n−2k (6) ≤ ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k (7) Again, (7) follows from using the upper bound on |Γn−2k|[33] such that with probability at least 1 −O(1/N), |Γn−2k|≤ 9 min{a,2}(Np)n−2k,∀1 ≤k≤⌊n 2 ⌋. Combining with Lemma 3, we obtain the ﬁnal result. Figure 7 presents variance calculated from simulation against predicted upper and lower bounds from our theoretical results. The simulation results are averaged over 1000 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 7: Comparison of the bounds on variance in Theorem 2 against simulation results. F Proof of Theorem 2 When we ﬁx K ∈N, only the upper bound in Theorem 2 will change. Note that now the upper bound in (7) can be written as ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k (p+ q 2p )2k( 2p p+ q )n( 2 N(p+ q) )n ≤ C min{a,2} ( C∑ k=0 (p+ q 2p )2k)( 2 N(p+ q) )n ≤ C min{a,2} ( 2 N(p+ q) )n . G Proof of Proposition 1 Let the node representation vector of node v after n graph convolutions be h(n) v . The Bayes error rate could be written as 1 2 (P[h(n) v > D|v ∈C1] + P[h(n) v ≤D|v ∈C2]). For d ∈N, due to the symmetry of our setup, one can easily see that the optimal linear decision boundary is the hyperplane ∑d j=1 xj = d 2 (µ1 + µ2). Then for v ∈C1, 17∑d j=1 (h(n) v )j ∼N(dµ(n) 1 ,d(σ(n))2) and for v∈C2, ∑d j=1 (h(n) v )j ∼N(dµ(n) 2 ,d(σ(n))2). Thus the Bayes error rate can be written as 1 2(P[ d∑ j=1 (h(n) v )j >D|v∈C1] + P[ d∑ j=1 (h(n) v )j ≤D|v∈C2]) = 1 2 ( 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) )) + 1 2 ( Φ ( d 2 (µ1 + µ2) −dµ(n) 2√ dσ(n) )) = 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) ) . The last equality follows from the fact that d 2 (µ1 + µ2) −dµ(n) 1 = −(d 2 (µ1 + µ2) −dµ(n) 2 ). H How to use the z-score to choose the number of layers The bounds of the z-score with respect to the number of layers, z(n) lower and z(n) upper allow us to calculate bounds for n⋆ and n0 under different scenarios. Speciﬁcally, 1. ∀n∈N,z(n) upper <z(0) = (µ2 −µ1)/σ, then n⋆ = n0 = 0, meaning that no graph convolution should be applied. 2. |{n∈N : z(n) upper ≥z(0)}|>0, and (a) ∀n∈N,z(n) lower < z(0), then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},which means that the number of graph convolutions should not exceed the upper bound of n0, or otherwise one gets worse performance than having no graph convolution. Note that in this case, since n⋆ ≤n0, we can only conclude that 0 ≤n⋆ ≤min{n∈N : z(n) upper ≤z(0)}. (b) |{n∈N : z(n) lower ≥z(0)}|>0, then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},and let arg max n z(n) lower = n⋆ ﬂoor, max { n≤n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } ≤n⋆ ≤min { n≥n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } , meaning that the number of layers one should apply for optimal node classiﬁcation performance is more than the lower bound of n⋆, and less than the upper bound of n⋆. I Proofs of Proposition 2-5 I.1 Proof of Proposition 2 Since the spectral radius of D−1Ais 1, α(Id−(1 −α)(D−1A))−1 = α ∞∑ k=0 (1 −α)k(D−1A)k. Apply Lemma 2, we get that µPPNP 2 −µPPNP 1 ≈ p+q p+ 2−α α q(µ2 −µ1). To bound the approximation error, similar to the proof of the concentration bound in Theorem 1, it sufﬁces to bound µ1 −µ2 N w⊤ 2 ( ∞∑ k=0 α(1 −α)k((D−1A)k −( ¯D−1 ¯A)k))w2 = µ1 −µ2 N w⊤ 2 (TK + TK+1,∞)w2 , where TK = ∑K k=0 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), TK+1,∞= ∑∞ k=K+1 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), and K ∈N up to our own choice. Bounding TK: Apply Theorem 1, ﬁx r> 0, there exists a constantC(r,K,α) such that with probability1−O(N−r), ∥TK∥2 ≤ C√¯d . 18Bounding TK+1,∞: We will show upper bound for(D−1A)k −( ¯D−1 ¯A)k that applies for all k∈N. Note that for every k∈N, (D−1A)k = D−1/2(D−1/2AD−1/2)kD1/2 = D−1/2(VΛkV⊤)D1/2 , where D−1/2AD−1/2 = VΛV⊤is the eigenvalue decomposition. Then ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤∥(D−1A)k∥2 + ∥( ¯D−1 ¯A)k∥2 = ∥(D−1A)k∥2 + 1 ≤∥D−1/2∥2∥(D−1/2AD−1/2)k∥2∥D−1/2∥2 + 1. Since ∥(D−1/2AD−1/2)k∥2 = 1 and by Corollary 1, with probability at least 1 −N−r, ∥D1/2∥2 ≤ √ 3 ¯d/2,∥D−1/2∥2 ≤ √ 2/¯d, the previous inequality becomes ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤ √ 3 + 1. Hence ∥TK+1,∞∥2 ≤(1 −α)K+1 . Combining the two results, we prove the claim. I.2 Proof of Proposition 3 The claim is a direct corollary of Theorem 1. I.3 Proof of Proposition 4 The covariance matrix ΣPPNP of hPPNP could be written as ΣPPNP = α2( ∞∑ k=0 (1 −α)k(D−1A)k)( ∞∑ l=0 (1 −α)l(D−1A)l)⊤σ2 . Note that the variance of node iequals α2 ∑∞ k,l=0(1 −α)k+l(D−1A)k i·((D−1A)l)⊤ i·, where i·refers row iof a matrix. Then by Cauchy-Schwarz Theorem, (D−1A)k i·((D−1A)l)⊤ i· ≤∥(D−1A)k i·∥∥((D−1A)l)i·∥ ≤ √ (σ(k))2(σ(l))2/σ2,for all 1 ≤k,l ≤N. Moreover, by Lemma 3, (σ(k))2 ≤σ2. Due to the identity of each node i, we get that with probability 1 −O(1/N), for all 1 ≤K ≤N, (σPPNP)2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + ∞∑ k=K+1 (1 −α)kσ )2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 . For the lower bound, note that with probability 1 −O(1/N), (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2k 1 Nk + ∞∑ k=N+1 (1 −α)2k 1 N ) σ2 , where Nk is the size of k-hop neighborhood. Then (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2kmin{a,2} 10 1 (Np)k ) σ2 ≥α2 min{a,2} 10 (Np)N+1 −(1 −α)2N+2 (Np)N(Np −(1 −α)2) σ2 ≥α2 min{a,2} 10 σ2 . It is easy to see that Lemma 3 applies to any message-passing scheme which could be regarded as a random walk on the graph. Combining with Lemma 3, we get the ﬁnal result. 19I.4 Proof of Proposition 5 Since hAPPNP(n) = ( α (n−1∑ k=0 (1 −α)k(D−1A)k ) + (1 −α)n(D−1A)n ) X Through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≤ ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 . For the lower bound, through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≥α2 n−1∑ k=0 (1 −α)2k(σ(k))2 + (1 −α)2n(σ(n))2 ≥α2 min{a,2} 10 (n−1∑ k=0 (1 −α)2k 1 (Np)k ) σ2 + min{a,2} 10 (1 −α)2n 1 (Np)nσ2 ≥min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) σ2 . Combining with Lemma 3, we get the ﬁnal result. J Experiments Here we provide more details on the models that we use in Section 5. In all cases we use the Adam optimizer and tune some hyperparameters for better performance. The hyperparameters used are summarized as follows. Data ﬁnal linear classiﬁer weights in GNN layer learning rate (width) iterations (width) synthetic 1 layer no 0.01 8000 yes 0.01(1,4,16)/0.001(64,256) 8000(1,4,16)/10000(64)/50000(256) Cora 3 layer with 32 hidden channels no 0.001 150 yes 0.001 200 CiteSeer 3 layer with 16 hidden channels no 0.001 100 yes 0.001 100 PubMed 3 layer with 32 hidden channels no 0.001 500 yes 0.001 500 We empirically ﬁnd that after adding in weights in each GNN layer, it takes much longer to train the model for one iteration, and the time increases when the depth or the width increases (Figure 8). Since for some combinations, it takes more than 200,000 iterations for the validation accuracy to ﬁnally increase, for each case, we only train for a reasonable amount of iterations. Figure 8: Iterations per second for each model. All models were implemented with PyTorch [46] and PyTorch Geometric [47]. 20K Additional Results K.1 Effect of nonlinearity on classiﬁcation performance In section 3, we consider the case of a simpliﬁed linear GNN. What would happen if we add nonlinearity after linear graph convolutions? Here, we consider the case of a GNN with a ReLU activation function added after nlinear graph convolutions, i.e. h(n)ReLU = ReLU((D−1A)nX). We show that adding such nonlinearity does not improve the classiﬁcation performance. Proposition 6. Applying a ReLU activation function after n linear graph convolutions does not decrease the Bayes error rate, i.e. Bayes error rate based on h(n)ReLU ≥ Bayes error rate based on h(n), and equality holds if µ1 ≥−µ2. Proof. If is known that if xfollows a Gaussian distribution, then ReLU(x) follows a Rectiﬁed Gaussian distribution. Following the deﬁnition of the Bayes optimal classiﬁer, we present a geometric proof in Figure 9 (see next page, top), where the dark blue bar denotes the location of 0 and the red bar denotes the decision boundary Dof the Bayes optimal classiﬁer, and the light blue area denotes the overlapping area S, which is twice the Bayes error rate. Figure 9: A geometric proof of Proposition 6. K.2 Exact limit of variance (σ(n))2 as n→∞ Proposition 7. Given a graph Gwith adjacency matrix A, let its degree vector be d= A1 N, where 1 N is the all-one vector of length N. If Gis connected and non-bipartite, the variance of each node i, denoted as (σi(n))2, converges asymptotically to ∥d∥2 2 ∥d∥2 1 , i.e. (σi (n))2 n→∞ −−−−→∥d∥2 2 ∥d∥2 1 . Then ∥d∥2 ∥d∥2 1 ≥ 1 N, and the equality holds if and only if Gis regular. Proof. Let ei denotes the standard basis unit vector with the ith entry equals 1, and all other entries equal 0. Since Gis connected and non-bipartite, the random walk represented by P = D−1Ais ergodic, meaning that e⊤ i P(n) n→∞ −−−−→π, where πis the stationary distribution of this random walk with πi = di ∥d∥1 . Then since norms are continuous functions, we conclude that (σi (n))2 = ∑ j (p(n) ij )2 = ∥e⊤ i P(n)∥2 2 n→∞ −−−−→∥π∥2 2 = ∥d∥2 2 ∥d∥2 1 . By Lemma 3, it follows that ∥d∥2 2 ∥d∥2 1 ≥ 1 N. The unique minimizer of ∥π∥2 2 subject to ∥π∥1 = 1 is π= 1 N1 N. This means that Gmust be regular to achieve the lower bound asymptotically. 21Under Assumption 1, the graph generated by our CSBM is almost surely connected. Here, we remain to show that with high probability, the graph will also be non-bipartite. Proposition 8. With probability at least 1 −O(1/(Np)3), a graph Ggenerated from CSBM(N, p, q, µ1, µ2, σ2) contains a triangle, which implies that it is non-bipartite. Proof. The proof goes by the classic probabilistic method. Let T∆ = ( N 3 )∑ i 1 τi denotes the number of triangles in G, where 1 τi equals 1 if potential triangle τi exists and 0 otherwise. Then by second moment method, P[T∆ = 0] ≤ Var(T∆) (E[T∆])2 = 1 E[T∆]) + ∑ i̸=jE[1 τi1 τj] −(E[T∆])2 (E[T∆])2 . Since E[T∆] = O(Np), ∑ i̸=jE[1 τi1 τj] = (1 + O(1/N))(E[T∆])2, we get that P[T∆ = 0] ≤O(1/(Np)3) + O(1/N) ≤O(1/(Np)3) . Hence P[Gis non-bipartite] ≥P[T∆ ≥1] ≥1 −O(1/(Np)3). K.3 Symmetric Graph Convolution D−1/2AD−1/2 Proposition 9. When using symmetric message-passing convolution D−1/2AD−1/2 instead, the variance (σ(n))2 is non-increasing with respect to the number of convolutional layers n. i.e. (σ(n+1))2 ≤(σ(n))2,n ∈N ∪{0}. Proof. We want to calculate the diagonal entries of the covariance matrix Σ(n) of (D−1/2AD−1/2)nX, where the covariance matrix of X is σ2IN. Hence Σ(n) = (D−1/2AD−1/2)n( (D−1/2AD−1/2)n)⊤ . Since D−1/2AD−1/2 is symmetric, let its eigendecomposition be VΛV⊤and we could rewrite Σ(n) = (VΛnV⊤)(VΛnV⊤) = VΛ2nV⊤. Notice that the closed form of the diagonal entries is diag(Σ(n)) = N∑ i=1 λ2n i |v|2 . Since for all 1 ≤i≤N, |λi|≤ 1,we obtain monotonicity of each entry of diag(Σ(n)), i.e. variance of each node. Although the proposition does not always hold for random walk message-passing convolution D−1A as one can construct speciﬁc counterexamples (Appendix K.4), in practice, variances are observed to be decreasing with respect with the number of layers. Moreover, we empirically observe that variance goes down more than the variance using symmetric message-passing convolutions. Figure 10 presents visualization of node representations comparing the change of variance with respect to the number of layers using random walk convolution and symmetric message-passing convolution. The data is generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 10: The change of variance with respect to the number of layers using random walk convolution D−1Aand symmetric message-passing convolution D−1/2AD−1/2. 22K.4 Counterexamples Here, we construct a speciﬁc example where the variance (σ(n))2 is not non-increasing with respect to the number of layers n(Figure 11A). We remark that such a non-monotone nature of change in variance is not caused by the bipartiteness of the graph, as a cycle graph with even number of nodes is also bipartite, but does not exhibit such phenomenon (Figure 11B). We conjecture the increase in variance is rather caused by the tree-like structure. A B Figure 11: Counterexamples. K.5 The mixing and denoising effects in practice In this section, we measure the mixing and denoising effects of graph convolutions identiﬁed by our theoretical results in practice, and show that the same tradeoff between the two counteracting effects exists for real-world graphs. For the mixing effect, we measure the pairwise L2 distances between the means of different classes, and for the denoising effect, we measure the within-class variances, both respect to the number of layers. Figure 12 gives a visualization of both metrics for all classes on Cora, CiteSeer and PubMed. We observe that similar to the synthetic CSBM data, adding graph convolutions increases both the mixing effect (homogenizing node representations in different classes, measured by the inter-class distances) and the denoising effect (homogenizing node representations in the same class, measured by the within-class distances). In addition, the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory. Figure 12: The existence of the mixing (top row) and denoising effects (bottom row) of graph convolutions in practice. Adding graph convolutions increases both effects and the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory in Section 3. 23",
      "meta_data": {
        "arxiv_id": "2212.10701v2",
        "authors": [
          "Xinyi Wu",
          "Zhengdao Chen",
          "William Wang",
          "Ali Jadbabaie"
        ],
        "published_date": "2022-12-21T00:33:59Z",
        "pdf_url": "https://arxiv.org/pdf/2212.10701v2.pdf"
      }
    },
    {
      "title": "PairNorm: Tackling Oversmoothing in GNNs",
      "abstract": "The performance of graph neural nets (GNNs) is known to gradually decrease\nwith increasing number of layers. This decay is partly attributed to\noversmoothing, where repeated graph convolutions eventually make node\nembeddings indistinguishable. We take a closer look at two different\ninterpretations, aiming to quantify oversmoothing. Our main contribution is\nPairNorm, a novel normalization layer that is based on a careful analysis of\nthe graph convolution operator, which prevents all node embeddings from\nbecoming too similar. What is more, PairNorm is fast, easy to implement without\nany change to network architecture nor any additional parameters, and is\nbroadly applicable to any GNN. Experiments on real-world graphs demonstrate\nthat PairNorm makes deeper GCN, GAT, and SGC models more robust against\noversmoothing, and significantly boosts performance for a new problem setting\nthat benefits from deeper GNNs. Code is available at\nhttps://github.com/LingxiaoShawn/PairNorm.",
      "full_text": "arXiv:1909.12223v2  [cs.LG]  13 Feb 2020 Published as a conference paper at ICLR 2020 PA I R NO R M : T AC K L I N G OV E R S M O OT H I N G I N GNN S Lingxiao Zhao Carnegie Mellon University Pittsburgh, P A 15213, USA {lingxia1}@andrew.cmu.edu Leman Akoglu Carnegie Mellon University Pittsburgh, P A 15213, USA {lakoglu}@andrew.cmu.edu ABSTRACT The performance of graph neural nets (GNNs) is known to gradu ally decrease with increasing number of layers. This decay is partly attri buted to oversmooth- ing, where repeated graph convolutions eventually make nod e embeddings indis- tinguishable. W e take a closer look at two different interpr etations, aiming to quantify oversmoothing. Our main contribution is P A IR NO RM , a novel normal- ization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too simila r. What is more, PA IR NO RM is fast, easy to implement without any change to network arch itecture nor any additional parameters, and is broadly applicable to any GNN. Experiments on real-world graphs demonstrate that P A IR NO RM makes deeper GCN, GA T , and SGC models more robust against oversmoothing, and signiﬁca ntly boosts per- formance for a new problem setting that beneﬁts from deeper G NNs. Code is available at https://github.com/LingxiaoShawn/PairNorm. 1 I NTRODUC TI ON Graph neural networks (GNNs) is a family of neural networks t hat can learn from graph structured data. Starting with the success of GCN (Kipf & W elling, 2017) on achieving state-of-the-art per- formance on semi-supervised classiﬁcation, several varia nts of GNNs have been developed for this task; including GraphSAGE (Hamilton et al., 2017), GA T (V el ickovic et al., 2018), SGC (Wu et al., 2019), and GMNN (Qu et al., 2019) to name a few most recent ones . A key issue with GNNs is their depth limitations. It has been o bserved that deeply stacking the layers often results in signiﬁcant drops in performance for GNNs, such as GCN and GA T , even beyond just a few (2–4) layers. This drop is associated with a number of factors; including the vanishing gradients in back-propagation, overﬁtting due t o the increasing number of parameters, as well as the phenomenon called oversmoothing. Li et al. (2018 ) was the ﬁrst to call attention to the oversmoothing problem. Having shown that the graph convolu tion is a type of Laplacian smoothing , they proved that after repeatedly applying Laplacian smoot hing many times, the features of the nodes in the (connected) graph would converge to similar values—t he issue coined as “ oversmoothing”. In effect, oversmoothing hurts classiﬁcation performance by causing the node representations to be indistinguishable across different classes. Later, sever al others have alluded to the same problem (Xu et al., 2018; Klicpera et al., 2019; Rong et al., 2019; Li e t al., 2019) (See §5 Related W ork). In this work, we address the oversmoothing problem in deep GN Ns. Speciﬁcally, we propose (to the best of our knowledge) the ﬁrst normalization layer for GNNs that is applied in-between intermediate layers during training. Our normalization has the effect of preventing the output features of distant nodes to be too similar or indistinguishable, while at the sa me time allowing those of connected nodes in the same cluster become more similar. W e summarize o ur main contributions as follows. • Normalization to T ackle Oversmoothing in GNNs: W e introduce a normalization scheme, called P A IR NO RM , that makes GNNs signiﬁcantly more robust to oversmoothing and as a result enables the training of deeper models without sacriﬁcing pe rformance. Our proposed scheme capitalizes on the understanding that most GNNs perform a sp ecial form of Laplacian smoothing, which makes node features more similar to one another. The ke y idea is to ensure that the total pairwise feature distances remains a constant across layers, which in turn leads to distant pairs having less similar features, preventing feature mixing ac ross clusters. 1Published as a conference paper at ICLR 2020 • Speed and Generality: PA IR NO RM is very straightforward to implement and introduces no additional parameters. It is simply applied to the output fe atures of each layer (except the last one) consisting of simple operations, in particular center ing and scaling, that are linear in the input size. Being a simple normalization step between layer s, P A IR NO RM is not speciﬁc to any particular GNN but rather applies broadly. •Use Case for Deeper GNNs: While P A IR NO RM prevents performance from dropping signif- icantly with increasing number of layers, it does not necess arily yield increased performance in absolute terms. W e ﬁnd that this is because shallow architec tures with no more than 2–4 layers is sufﬁcient for the often-used benchmark datasets in the li terature. In response, we motivate a real-world scenario wherein a notable portion of the nodes h ave no feature vectors. In such set- tings, nodes beneﬁt from a larger range (i.e., neighborhood , hence a deeper GNN) to “recover” effective feature representations. Through extensive exp eriments, we show that GNNs employing our P A IR NO RM signiﬁcantly outperform the ‘vanilla’ GNNs when deeper mod els are beneﬁcial to the classiﬁcation task. 2 U NDERSTAN DI NG OVERSMOOT HIN G In this work, we consider the semi-supervised node classiﬁc ation (SSNC) problem on a graph. In the general setting, a graph G = (V, E, X) is given in which each node i ∈ V is associated with a feature vector xi ∈ Rd where X = [x1, . . . , xn]T denotes the feature matrix, and a subset Vl ⊂ V of the nodes are labeled, i.e. yi ∈ { 1, . . . , c } for each i ∈ V l where c is the number of classes. Let A ∈ Rn×n be the adjacency matrix and D = diag(deg1, . . . , deg n) ∈ Rn×n be the degree matrix of G. Let ˜A = A + I and ˜D = D + I denote the augmented adjacency and degree matrices with added self-loops on all nodes, respectively. Let ˜Asym = ˜D−1/2 ˜A ˜D−1/2 and ˜Arw = ˜D−1 ˜A denote symmetrically and nonsymmetrically normalized adjacency matrices with self-loops. The task is to learn a hypothesis that predicts yi from xi that generalizes to the unlabeled nodes Vu = V\\Vl. In Section 3.2, we introduce a variant of this setting where only a subset F ⊂ V of the nodes have feature vectors and the rest are missing. 2.1 T H E OV E RS M O OT H IN G PRO BL E M Although GNNs like GCN and GA T achieve state-of-the-art res ults in a variety of graph-based tasks, these models are not very well-understood, especially why t hey work for the SSNC problem where only a small amount of training data is available. The succes s appears to be limited to shallow GNNs, where the performance gradually decreases with the in creasing number of layers. This decrease is often attributed to three contributing factors : (1) overﬁtting due to increasing number of parameters, (2) difﬁculty of training due to vanishing gr adients, and (3) oversmoothing due to many graph convolutions. Among these, perhaps the least understood one is oversmoothing, which indeed lacks a for- mal deﬁnition. In their analysis of GCN’s working mechanism , Li et al. (2018) showed that the graph convolution of GCN is a special form of Laplacian smoot hing. The standard form being (I − γI)X + γ ˜Arw X, the graph convolution lets γ = 1 and uses the symmetrically normalized Laplacian to obtain ˜X = ˜Asym X, where the new features ˜ xof a node is the weighted average of its own and its neighbors’ features. This smoothing allows the n ode representations within the same cluster become more similar, and in turn helps improve SSNC p erformance under the cluster as- sumption (Chapelle et al., 2006). However when GCN goes deep , the performance can suffer from oversmoothing where node representations from different c lusters become mixed up. Let us refer to this issue of node representations becoming too similar as node-wise oversmoothing. Another way of thinking about oversmoothing is as follows. R epeatedly applying Laplacian smooth- ing too many times would drive node features to a stationary p oint, washing away all the information from these features. Let x·j ∈ Rn denote the j-th column of X. Then, for any x·j ∈ Rn: lim k→∞ ˜Ak symx·j = π j and π j ∥π j∥1 = π , (1) where the normalized solution π ∈ Rn satisﬁes π i = √degi∑ i √degi for all i ∈ [n]. Notice that π is independent of the values x·j of the input feature and is only a function of the graph struct ure (i.e., 2Published as a conference paper at ICLR 2020 degree). In other words, (Laplacian) oversmoothing washes away the signal from all the features, making them indistinguishable. W e will refer to this viewpo int as feature-wise oversmoothing. T o this end we propose two measures, row-diff and col-diff, t o quantify these two types of over- smoothing. Let H(k) ∈ Rn×d be the representation matrix after k graph convolutions, i.e. H(k) = ˜Ak symX. Let h(k) i ∈ Rd be the i-th row of H(k) and h(k) ·i ∈ Rn be the i-th column of H(k). Then we deﬁne row-diff( H(k)) and col-diff( H(k)) as follows. row-diff(H(k)) = 1 n2 ∑ i,j∈[n]   h(k) i − h(k) j    2 (2) col-diff(H(k)) = 1 d2 ∑ i,j∈[d]   h(k) ·i / ∥h(k) ·i ∥1 − h(k) ·j / ∥h(k) ·j ∥1    2 (3) The row-diff measure is the average of all pairwise distance s between the node features (i.e., rows of the representation matrix) and quantiﬁes node-wise oversm oothing, whereas col-diff is the average of pairwise distances between ( L1-normalized1) columns of the representation matrix and quantiﬁes feature-wise oversmoothing. 2.2 S T U DY IN G OV E RS M O OT H IN G W IT H SGC Although oversmoothing can be a cause of performance drop wi th increasing number of layers in GCN, adding more layers also leads to more parameters (due to learned linear projections W(k) at each layer k) which magnify the potential of overﬁtting. Furthermore, d eeper models also make the training harder as backpropagation suffers from vanishing gradients. In order to decouple the effect of oversmoothing from these o ther two factors, we study the over- smoothing problem using the SGC model (Wu et al., 2019). (Res ults on other GNNs are presented in §4.) SGC is simpliﬁed from GCN by removing all projection para meters of graph convolution layers and all nonlinear activations between layers. The es timation of SGC is simply written as: ˆY = softmax( ˜AK symX W ) (4) where K is the number of graph convolutions, and W ∈ Rd×c denote the learnable parameters of a logistic regression classiﬁer. Note that SGC has aﬁxed number of parameters that does not depend on the number of gra ph convolutions (i.e. layers). In effect, it is guarded agains t the inﬂuence of overﬁtting and vanishing gradient problem with more layers. This leaves us only with o versmoothing as a possible cause of performance degradation with increasing K. Interestingly, the simplicity of SGC does not seem to be a sacriﬁce; it has been observed that it achieves similar o r better accuracy in various relational classiﬁcation tasks (Wu et al., 2019). 0 20 40 Layers 0.5 1.0 1.5Loss train_loss val_loss test_loss 0 20 40 Layers 0.4 0.6 0.8 1.0Accuracy train_acc val_acc test_acc 0 20 40 Layers 2 4 6Distance row_diff 0 20 40 Layers 0.1 0.2 0.3 0.4Distance col_diff PairNorm Original Figure 1: (best in color) SGC’s performance (dashed lines) w ith increasing graph convolutions ( K) on Cora dataset (train/val/test split is 3%/10%/87%). For each K, we train SGC in 500 epochs, save the model with the best validation accuracy, and report all measures based on the saved model. Measures row-diff and col-diff are computed based on the ﬁna l layer representation of the saved model. (Solid lines depict after applying our method P A IR NO RM , which we discuss in §3.2.) Dashed lines in Figure 1 illustrate the performance of SGC on the Cora dataset as we increase the number of layers ( K). The training (cross-entropy) loss monotonically increa ses with larger K, potentially because graph convolution mixes node represen tations with their neighbors’ and makes them less distinguishable (training becomes harder). On th e other hand, graph convolutions (i.e., smoothing) improve generalization ability, reducing the g ap between training and validation/test loss 1 W e normalize each column j as the Laplacian smoothing stationary point π j is not scale-free. See Eq. (1). 3Published as a conference paper at ICLR 2020 up to K = 4, after which (over)smoothing begins to hurt performance. T he row-diff and col-diff both continue decreasing monotonically with K, providing supporting evidence for oversmoothing. 3 T ACKLING OVERSMOOTH IN G 3.1 P RO P O S E D PA IR NO RM W e start by establishing a connection between graph convolu tion and an optimization problem, that is graph-regularized least squares (GRLS), as shown by NT & M aehara (2019). Let ¯X ∈ Rn×d be a new node representation matrix, with ¯ xi ∈ Rd depicting the i-th row of ¯X. Then the GRLS problem is given as min ¯X ∑ i∈V ∥¯ xi − xi∥2 ˜D + ∑ (i,j)∈E ∥¯ xi − ¯ xj ∥2 2 (5) where ∥zi∥2 ˜D = zT i˜Dzi. The ﬁrst term can be seen as total degree-weighted least squ ares. The second is a graph-regularization term that measures the variation of the new features over the graph structure. The goal of the optimization problem can be state d as estimating new “denoised” features ¯ xi’s that are not too far off of the input features xi’s and are smooth over the graph structure. The GRLS problem has a closed form solution ¯X = (2I − ˜Arw )−1X, for which ˜Arw X is the ﬁrst- order T aylor approximation, that is ˜Arw X ≈ ¯X. By exchanging ˜Arw with ˜Asym we obtain the same form as the graph convolution, i.e., ˜X = ˜Asym X ≈ ¯X. As such, graph convolution can be viewed as an approximate solution of (5), where it minimizes the var iation over the graph structure while keeping the new representations close to the original. The optimization problem in (5) facilitates a closer look tothe oversmoothing problem of graph convolution. Ideally, we want to obtain smoothing over node s within the same cluster, however avoid smoothing over nodes from different clusters. The obj ective in (5) dictates only the ﬁrst goal via the graph-regularization term. It is thus prone to overs moothing when convolutions are applied repeatedly. T o circumvent the issue and fulﬁll both goals si multaneously, we can add a negative term such as the sum of distances between disconnected pairs as follows. min ¯X ∑ i∈V ∥¯ xi − xi∥2 ˜D + ∑ (i,j)∈E ∥¯ xi − ¯ xj ∥2 2 − λ ∑ (i,j)/∈E ∥¯ xi − ¯ xj ∥2 2 (6) where λ is a balancing scalar to account for different volume and imp ortance of the two goals. 2 By deriving the closed-form solution of (6) and approximati ng it with ﬁrst-order T aylor expansion, one can get a revised graph convolution operator with hyperp arameter λ. In this paper, we take a different route. Instead of a completely new graph convolut ion operator, we propose a general and efﬁcient “patch”, called P A IR NO RM , that can be applied to any form of graph convolution having the potential of oversmoothing. Let˜X (the output of graph convolution) and ˙X respectively be the input and output of P A IR NO RM . Observing that the output of graph convolution ˜X = ˜Asym X only achieves the ﬁrst goal, P A IR NO RM serves as a normalization layer that works on ˜X to achieve the second goal of keeping disconnected pair representations farther off. Speciﬁcally, P A IR NO RM normalizes ˜X such that the total pairwise squared distance TPSD ( ˙X) :=∑ i,j∈[n] ∥ ˙ xi − ˙ xj ∥2 2is the same as TPSD (X). That is,∑ (i,j)∈E ∥ ˙ xi − ˙ xj ∥2 2 + ∑ (i,j)/∈E ∥ ˙ xi − ˙ xj ∥2 2 = ∑ (i,j)∈E ∥xi − xj ∥2 2 + ∑ (i,j)/∈E ∥xi − xj ∥2 2 . (7) By keeping the total pairwise squared distance unchanged, the term ∑ (i,j)/∈E ∥ ˙ xi − ˙ xj ∥2 2is guar- anteed to be at least as large as the original value ∑ (i,j)/∈E ∥xi − xj ∥2 2 since the other term∑ (i,j)∈E ∥ ˙ xi − ˙ xj ∥2 2≈ ∑ (i,j)∈E ∥˜ xi − ˜ xj ∥2 2is shrunk through the graph convolution. In practice, instead of always tracking the original value T PSD(X), we can maintain a constant TPSD value C across all layers, where C is a hyperparameter that could be tuned per dataset. T o normalize ˜X to constant TPSD, we need to ﬁrst compute TPSD ( ˜X). Directly computing TPSD involves n2 pairwise distances that is O(n2d), which can be time consuming for large datasets. 2 There exist other variants of (6) that achieve similar goals , and we leave the space for future exploration. 4Published as a conference paper at ICLR 2020 Equivalently, normalization can be done via a two-step appr oach where TPSD is rewritten as 3 TPSD( ˜X) = ∑ i,j∈[n] ∥˜ xi − ˜ xj ∥2 2 = 2 n2 ( 1 n n∑ i=1 ∥˜ xi∥2 2− ∥ 1 n n∑ i=1 ˜ xi∥2 2 ) . (8) The ﬁrst term (ignoring the scale 2n2) in Eq. (8) represents the mean squared length of node representations, and the second term depicts the squared le ngth of the mean of node represen- tations. T o simplify the computation of (8), we subtract the row-wise mean from each ˜ xi, i.e., ˜ xc i = ˜ xi − 1 n ∑ n i˜ xi where ˜ xc idenotes the centered representation. Note that this shifti ng does not affect the TPSD, and furthermore drives the term ∥ 1 n ∑ n i=1 ˜ xi∥2 2to zero, where computing TPSD ( ˜X) boils down to calculating the squared Frobenius norm of ˜Xc and overall takes O(nd). That is, TPSD( ˜X) =TPSD( ˜Xc) = 2n∥ ˜Xc∥2 F . (9) In summary, our proposed P A IR NO RM (with input ˜X and output ˙X) can be written as a two-step, center-and-scale, normalization procedure: ˜ xc i= ˜ xi − 1 n n∑ i=1 ˜ xi (Center) (10) ˙ xi = s · ˜ xc i √ 1 n ∑ n i=1 ∥˜ xc i∥2 2 = s√ n · ˜ xc i √ ∥ ˜Xc∥2 F (Scale) (11) After scaling the data remains centered, that is, ∥ ∑ n i=1 ˙ xi∥2 2= 0. In Eq. (11), s is a hyperparameter that determines C. Speciﬁcally, TPSD( ˙X) = 2n∥ ˙X∥2 F= 2n ∑ i ∥s · ˜ xc i √ 1 n ∑ i ∥˜ xc i∥2 2 ∥2 2= 2n s2 1 n ∑ i ∥˜ xc i∥2 2 ∑ i ∥˜ xc i∥2 2= 2n2s2 (12) Then, ˙X := PA IR NO RM ( ˜X) has row-wise mean 0 (i.e., is centered) and constant total pairwise squared distance C = 2n2s2. An illustration of P A IR NO RM is given in Figure 2. The output of PA IR NO RM is input to the next convolution layer. graph conv center  PairNorm  rescale  X ˜X c˜X ˙X Figure 2: Illustration of P A IR NO RM , comprising centering and rescaling steps. 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GAT train_acc val_acc test_acc Figure 3: (best in color) Performance comparison of the original (dashed) vs. PA IR NO RM -enhanced (solid) GCN and GA T models with increasing layers on Cora. W e also derive a variant of P A IR NO RM by replacing ∑ n i=1 ∥˜ xc i∥2 2 in Eq. (11) with n∥˜ xc i∥2 2, such that the scaling step computes ˙ xi = s · ˜ xc i ∥˜ xc i∥2 . W e call it P A IR NO RM -S I (for Scale Individually), which imposes more re- striction on node representations, such that all have the sameL2-norm s. In practice we found that both P A IR NO RM and P A IR NO RM -S I work well for SGC, whereas P A IR NO RM -S I provides better and more stable results for GCN and GA T . The reason why GCN and GA T require stricter normalization may be because they have more parameters and are more prone to overﬁtting. In Appx. A.6 we p rovide additional measures to demonstrate why P A IR NO RM and P A IR NO RM -S I work. In all experiments, we employ P A IR NO RM for SGC and P A IR NO RM -S I for both GCN and GA T . PA IR NO RM is effective and efﬁcient in solving the oversmoothing prob lem of GNNs. As a general normalization layer, it can be used for any GNN. Solid lines i n Figure 1 present the performance 3 See Appendix A.1 for the detailed derivation. 5Published as a conference paper at ICLR 2020 of SGC on Cora with increasing number of layers, where we employ P A IR NO RM after each graph convolution layer, as compared to ‘vanilla’ versions. Simi larly, Figure 3 is for GCN and GA T (PA IR NO RM is applied after the activation of each graph convolution). Note that the performance decay with P A IR NO RM -at-work is much slower. (See Fig.s 5–6 in Appx. A.3 for other datasets.) While P A IR NO RM enables deeper models that are more robust to oversmoothing , it may seem odd that the overall test accuracy does not improve. In fact, the benchmark graph datasets often used in the literature require no more than 4 layers, after which p erformance decays (even if slowly). In the next section, we present a realistic use case setting for which deeper models are more likely to provide higher performance, where the beneﬁt of P A IR NO RM becomes apparent. 3.2 A C A S E WH E RE DE E P E R GNN S A RE BE N E FICIA L In general, oversmoothing gets increasingly more severe as the number of layers goes up. A task would beneﬁt from employing P A IR NO RM more if it required a large number of layers to achieve its best performance. T o this effect we study the “missing fe ature setting”, where a subset of the nodes lack feature vectors. Let M ⊆ V u be the set where ∀m ∈ M , xm = ∅, i.e., all of their features are missing. W e denote with p = |M|/ |Vu| the missing fraction. W e call this variant of the task as semi-supervised node classiﬁcation with missin g vectors (SSNC-MV). Intuitively, one would require a larger number of propagation steps (hence, a deeper GNN) to be able to “recover” effective feature representations for these nodes. SSNC-MV is a general and realistic problem that ﬁnds severalapplications in the real world. For example, the credit lending problem of identifying low- vs. high-risk customers (nodes) can be modeled as SSNC-MV where a large fraction of nodes do not exhi bit any meaningful features (e.g., due to low-volume activity). In fact, many graph-based clas siﬁcation tasks with the cold-start issue (entity with no history) can be cast into SSNC-MV. T o our know ledge, this is the ﬁrst work to study the SSNC-MV problem using GNN models. Figure 4 presents the performance of SGC, GCN, and GA T modelson Cora with increasing number of layers, where we remove feature vectors from all the unlab eled nodes, i.e. p = 1. The models with P A IR NO RM achieve a higher test accuracy compared to those without, wh ich they typically reach at a larger number of layers. (See Fig. 7 in Appx. A.4 for results on other datasets.) 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-SGC 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GAT train_acc val_acc test_acc Figure 4: (best in color) Comparison of ‘vanilla’ vs. P A IR NO RM -enhanced SGC, GCN, and GA T performance on Cora for p = 1. Green diamond symbols depict the layer at which validation accuracy peaks. P A IR NO RM boosts overall performance by enabling more robust deep GNN s. 4 E XPERIME NT S In section 3 we have shown the robustness of P A IR NO RM -enhanced models against increasing num- ber of layers in SSNC problem. In this section we design exten sive experiments to evaluate the effectiveness of P A IR NO RM under the SSNC-MV setting, over SGC, GCN and GA T models. 4.1 E X P E RIM E N T SE T U P Datasets. W e use 4 well-known benchmark datasets in GNN domain: Cora, Citeseer, Pubmed (Sen et al., 2008), and CoauthorCS (Shchur et al., 2018). Their statistics are reported in Appx . A.2. For Cora, Citeseer and Pubmed, we use the same dataset splits as Kipf & W elling (2017), wher e all nodes outside train and validation are used as test set. F or CoauthorCS, we randomly split all nodes into train/val/test as 3%/ 10%/ 87%, and keep the same split for all experiments. Models. W e use three different GNN models as our base model: SGC (Wu et al., 2019), GCN (Kipf & W elling, 2017), and GA T (V elickovic et al., 2018). W e compare our P A IR NO RM with residual connection method (He et al., 2016) over base model s (except SGC since there is no “resid- 6Published as a conference paper at ICLR 2020 ual connected” SGC), as we surprisingly ﬁnd it can slow down o versmoothing and beneﬁt SSNC- MV problem. Similar to us, residual connection is a general t echnique that can be applied to any model without changing its architecture. W e focus on the com parison between the base models and PA IR NO RM -enhanced models, rather than achieving the state of the art performance for SSNC and SSNC-MV. There exist a few other work addressing oversmooth ing (Klicpera et al., 2019; Li et al., 2018; Rong et al., 2019; Xu et al., 2018) however they design s pecialized architectures and not sim- ple “patch” procedures like P A IR NO RM that can be applied on top of any GNN. Hyperparameters. W e choose the hyperparameter s of P A IR NO RM from {0. 1, 1, 10, 50, 100} over validation set for SGC, while keeping it ﬁxed at s = 1for both GCN and GA T due to resource limitations. W e set the #hidden units of GCN and GA T (#attent ion heads is set to 1) to 32 and 64 respectively for all datasets. Dropout with rate 0. 6 and L2 regularization with penalty 5 · 10−4 are applied to GCN and GA T . For SGC, we vary number of layers in {1, 2, . . . 10, 15, . . . , 60} and for GCN and GA T in {2, 4, . . . , 12, 15, 20, . . . , 30}. Conﬁgurations. For P A IR NO RM -enhanced models, we apply P A IR NO RM after each graph convo- lution layer (i.e., after activation if any) in the base mode l. For residual-connected models with t skip steps, we connect the output of l-th layer to (l + t)-th, that is, H(l+t) new = H(l+t) + H(l) where H(l) denotes the output of l-th graph convolution (after activation). For the SSNC-MV s etting, we randomly erase p fraction of the feature vectors from nodes in validation and test sets (for which we input vector 0 ∈ Rd), whereas all training (labeled) nodes keep their original features (See 3.2). W e run each experiment within 1000 epochs 5 times and report the average performance. W e mainly use a single GTX-1080ti GPU, with some SGC experiments ran on an Intel i7-8700k CPU. 4.2 E X P E RIM E N T RE S U LT S W e ﬁrst show the global performance gain of applying P A IR NO RM to SGC for SSNC-MV under varying feature missing rates as shown in T able 1. P A IR NO RM -enhanced SGC performs similar or better over 0% missing, while it signiﬁcantly outperforms vanilla SGC for most other settings, especially for larger missing rates. #L denotes the best num ber of layers for the model that yields the largest average validation accuracy (over 5 runs), for w hich we report the average test accuracy (Acc). Notice the larger #L values for SGC-PN compared to van illa SGC, which shows the power of P A IR NO RM for enabling “deep” SGC models by effectively tackling over smoothing. Similar to Wu et al. (2019) who showed that the simple SGC mode l achieves comparable or better performance as other GNNs for various tasks, we found P A IR NO RM -enhanced SGC to follow the same trend when compared with P A IR NO RM -enhanced GCN and GA T , for all SSNC-MV settings. Due to its simplicity and extreme efﬁciency, we believe P A IR NO RM -enhanced SGC sets a strong baseline for the SSNC-MV problem. T able 1: Comparison of ‘vanilla’ vs. PA IR NO RM -enhanced SGC performance in Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with missing rate ranging from 0% to 100%. Showing test accuracy at #L (K in Eq. 4) layers, at which model achieves best validation acc uracy. Missing Percentage 0% 20% 40% 60% 80% 100% Dataset Method Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Cora SGC 0.815 4 0.806 5 0.786 3 0.742 4 0.733 3 0.423 15 SGC-PN 0.811 7 0.799 7 0.797 7 0.783 20 0.780 25 0.745 40 Citeseer SGC 0.689 10 0.684 6 0.668 8 0.657 9 0.565 8 0.290 2 SGC-PN 0.706 3 0.695 3 0.653 4 0.641 5 0.590 50 0.486 50 Pubmed SGC 0.754 1 0.748 1 0.723 4 0.746 2 0.659 3 0.399 35 SGC-PN 0.782 9 0.781 7 0.778 60 0.782 7 0.772 60 0.719 40 CoauthorCS SGC 0.914 1 0.898 2 0.877 2 0.824 2 0.751 4 0.318 2 SGC-PN 0.915 2 0.909 2 0.899 3 0.891 4 0.880 8 0.860 20 W e next employ P A IR NO RM -S I for GCN and GA T under the same setting, comparing it with the residual (skip) connections technique. Results are shown i n T able 2 and T able 3 respectively for GCN and GA T . Due to space and resource limitations, we only sh ow results for 0% and 100% miss- ing rate scenarios. (W e provide results for other missing ra tes ( 70, 80, 90%) over 1 run only in Appx. A.5.) W e observe similar trend for GCN and GA T: (1) vanilla mo del suffers from performance drop under SSNC-MV with increasing missing rate; (2) both residu al connections and P A IR NO RM -S I enable deeper models and improve performance (note the larg er #L and Acc); (3) GCN-PN and 7Published as a conference paper at ICLR 2020 GA T -PN achieve performance that is comparable or better tha n just using skips; (4) performance can be further improved (albeit slightly) by using skips alo ng with P A IR NO RM -S I.4 T able 2: Comparison of ‘vanilla’ and (P A IR NO RM -S I/ residual)-enhanced GCN performance on Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% feature missing rate. t represents the skip-step of residual connection. (See A.5 F ig. 8 for more settings.) Dataset Cora Citeseer Pubmed CoauthorCS Missing(%) 0% 100% 0% 100% 0% 100% 0% 100% Method Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L GCN 0.821 2 0.582 2 0.695 2 0.313 2 0.779 2 0.449 2 0.877 2 0.452 4 GCN-PN 0.790 2 0.731 10 0.660 2 0.498 8 0.780 30 0.745 25 0.910 2 0.846 12 GCN-t1 0.822 2 0.721 15 0.696 2 0.441 12 0.780 2 0.656 25 0.898 2 0.727 12 GCN-t1-PN 0.780 2 0.724 30 0.648 2 0.465 10 0.756 15 0.690 12 0.898 2 0.830 20 GCN-t2 0.820 2 0.722 10 0.691 2 0.432 20 0.779 2 0.645 20 0.882 4 0.630 20 GCN-t2-PN 0.785 4 0.740 30 0.650 2 0.508 12 0.770 15 0.725 30 0.911 2 0.839 20 T able 3: Comparison of ‘vanilla’ and (P A IR NO RM -S I/ residual)-enhanced GA T performance on Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% feature missing rate. t represents the skip-step of residual connection. (See A.5 F ig. 9 for more settings.) Dataset Cora Citeseer Pubmed CoauthorCS Missing(%) 0% 100% 0% 100% 0% 100% 0% 100% Method Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L GA T 0.823 2 0.653 4 0.693 2 0.428 4 0.774 6 0.631 4 0.892 4 0.737 4 GA T -PN 0.787 2 0.718 6 0.670 2 0.483 4 0.774 12 0.714 10 0.916 2 0.843 8 GA T -t1 0.822 2 0.706 8 0.693 2 0.461 6 0.769 4 0.698 8 0.899 4 0.842 10 GA T -t1-PN 0.787 2 0.710 10 0.658 6 0.500 10 0.757 4 0.684 12 0.911 2 0.844 20 GA T -t2 0.820 2 0.691 8 s0.692 2 0.461 6 0.774 8 0.702 8 0.895 4 0.803 6 GA T -t2-PN 0.788 4 0.738 12 0.672 4 0.517 10 0.776 15 0.704 12 0.917 2 0.855 30 5 R ELATED WORK Oversmoothing in GNNs: Li et al. (2018) was the ﬁrst to call attention to the oversmoo thing problem. Xu et al. (2018) introduced Jumping Knowledge Netw orks, which employ skip connec- tions for multi-hop message passing and also enable differe nt neighborhood ranges. Klicpera et al. (2019) proposed a propagation scheme based on personalized Pagerank that ensures locality (via teleports) which in turn prevents oversmoothing. Li et al. ( 2019) built on ideas from ResNet to use residual as well as dense connections to train deep GCNs. Dro pEdge Rong et al. (2019) proposed to alleviate oversmoothing through message passing reduct ion via removing a certain fraction of edges at random from the input graph. These are all specializ ed solutions that introduce additional parameters and/or a different network architecture. Normalization Schemes for Deep-NNs:There exist various normalization schemes proposed for deep neural networks, including batch normalization Ioffe & Szegedy (2015), weight normalization Salimans & Kingma (2016), layer normalization Ba et al. (201 6), and so on. Conceptually these have substantially different goals (e.g., reducing traini ng time), and were not proposed for graph neural networks nor the oversmoothing problem therein. Imp ortant difference to note is that larger depth in regular neural-nets does not translate to more hops of propagation on a graph structure. 6 C ONCLUSIO N W e investigated the oversmoothing problem in GNNs and propo sed P A IR NO RM , a novel normal- ization layer that boosts the robustness of deep GNNs agains t oversmoothing. P A IR NO RM is fast to compute, requires no change in network architecture nor any extra parameters, and can be applied to any GNN. Experiments on real-world classiﬁcation tasks sho wed the effectiveness of P A IR NO RM , where it provides performance gains when the task beneﬁts fr om more layers. Future work will explore other use cases of deeper GNNs that could further sho wcase P A IR NO RM ’s advantages. 4 Notice a slight performance drop when P AIR NORM is applied at 0% rate. For this setting, and the datasets we ha ve, shallow networks are sufﬁcient and smoothing through only a few (2-4) layers i mproves generalization ability for the SSNC problem (recal l Figure 1 solid lines). PAIR NOR M has a small reversing effect in these scenarios, hence the sm all performance drop. 8Published as a conference paper at ICLR 2020 REFERENC ES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer n ormalization. CoRR, abs/1607.06450, 2016. Olivier Chapelle, Bernhard Sch ¨ olkopf, and Alexander Zien . Semi-Supervised Learning . 2006. William L. Hamilton, Zhitao Y ing, and Jure Leskovec. Induct ive representation learning on large graphs. In NIPS, pp. 1024–1034, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep R esidual Learning for Image Recognition. In Proceedings of 2016 IEEE Conference on Computer V ision and P attern Recog- nition, pp. 770–778. IEEE, 2016. Sergey Ioffe and Christian Szegedy. Batch normalization: A ccelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. Thomas N. Kipf and Max W elling. Semi-supervised classiﬁcat ion with graph convolutional net- works. In International Conference on Learning Representations (IC LR). OpenReview .net, 2017. Johannes Klicpera, Aleksandar Bojchevski, and Stephan G ¨ u nnemann. Combining neural networks with personalized pagerank for classiﬁcation on graphs. In International Conference on Learning Representations (ICLR) , 2019. Guohao Li, Matthias M ¨ uller, Ali Thabet, and Bernard Ghanem . Can GCNs go as deep as CNNs? CoRR, abs/1904.03751, 2019. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper Insights int o Graph Convolutional Networks for Semi-Supervised Learning. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intell i- gence, pp. 3538–3545, 2018. Hoang NT and T akanori Maehara. Revisiting graph neural netw orks: All we have is low-pass ﬁlters. CoRR, abs/1905.09550, 2019. Meng Qu, Y oshua Bengio, and Jian T ang. Gmnn: Graph markov neu ral networks. In International Conference on Machine Learning , pp. 5241–5250, 2019. Y u Rong, W enbing Huang, Tingyang Xu, and Junzhou Huang. The t ruly deep graph convolutional networks for node classiﬁcation. CoRR, abs/1907.10903, 2019. Tim Salimans and Durk P Kingma. W eight normalization: A simp le reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems , pp. 901–909, 2016. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoo r, Brian Galligher, and Tina Eliassi-Rad. Collective classiﬁcation in network data. AI magazine , 29(3):93–93, 2008. Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevsk i, and Stephan G ¨ unnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868 , 2018. Petar V elickovic, Guillem Cucurull, Arantxa Casanova, Adr iana Romero, Pietro Li, and Y oshua Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR). OpenReview .net, 2018. Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fif ty, T ao Y u, and Kilian Q. W einberger. Simplifying graph convolutional networks. In ICML, volume 97 of Proceedings of Machine Learning Research , pp. 6861–6871. PMLR, 2019. Keyulu Xu, Chengtao Li, Y onglong Tian, T omohiro Sonobe, Ken -ichi Kawarabayashi, and Stefanie Jegelka. Representation Learning on Graphs with Jumping Kn owledge Networks. In Proceedings of the 35th International Conference on Machine Learning , volume 80, pp. 5453–5462, 2018. 9Published as a conference paper at ICLR 2020 A A PPENDIX A.1 D E RIV AT IO N O F EQ. 8 TPSD( ˜X) = ∑ i,j∈[n] ∥˜ xi − ˜ xj ∥2 2= ∑ i,j∈[n] (˜ xi − ˜ xj )T (˜ xi − ˜ xj ) (13) = ∑ i,j∈[n] (˜ xT i˜ xi + ˜ xT j˜ xj − 2˜ xT i˜ xj ) (14) = 2n ∑ i∈[n] ˜ xT i˜ xi − 2 ∑ i,j∈[n] ˜ xT i˜ xj (15) = 2n ∑ i∈[n] ∥˜ xi∥2 2− 21T ˜X ˜XT 1 (16) = 2n ∑ i∈[n] ∥˜ xi∥2 2− 2∥1T ˜X∥2 2 (17) = 2n2 ( 1 n n∑ i=1 ∥˜ xi∥2 2− ∥ 1 n n∑ i=1 ˜ xi∥2 2 ) . (18) A.2 D ATA S E T STAT IS T ICS T able 4: Dataset statistics. Name #Nodes #Edges #Features #Classes Label Rate Cora 2708 5429 1433 7 0.052 Citeseer 3327 4732 3703 6 0.036 Pubmed 19717 44338 500 3 0.003 CoauthorCS 18333 81894 6805 15 0.030 A.3 A D D IT IO NA L PE RF O RM A N CE PL OT S W IT H IN CRE A S IN G NU M BE R O F LAY E RS 0 20 40 Layers 0.5 1.0Loss train_loss val_loss test_loss 0 20 40 Layers 0.6 0.8 1.0Accuracy train_acc val_acc test_acc 0 20 40 Layers 4 6 8Distance row_diff 0 20 40 Layers 0.2 0.3 0.4Distance col_diff citeseer (random split: 3%/10%/87%) PairNorm Original 0 20 40 Layers 0.4 0.6 0.8 1.0Loss train_loss val_loss test_loss 0 20 40 Layers 0.4 0.6 0.8Accuracy train_acc val_acc test_acc 0 20 40 Layers 0 2 4Distance row_diff 0 20 40 Layers 0.02 0.04Distance col_diff pubmed  (random split: 3%/10%/87%) PairNorm Original 0 20 40 Layers 0.5 1.0 1.5Loss train_loss val_loss test_loss 0 20 40 Layers 0.4 0.6 0.8 1.0Accuracy train_acc val_acc test_acc 0 20 40 Layers 0.0 2.5 5.0 7.5 10.0Distance row_diff 0 20 40 Layers 0.00 0.05 0.10 0.15 0.20Distance col_diff coauthor_CS (random split: 3 %/10%/87%) PairNorm Original Figure 5: Comparison of ‘vanilla’ vs. P A IR NO RM -enhanced SGC, corresponding to Figure 1, for datasets (from top to bottom) Citeseer, Pubmed, and CoauthorCS. P A IR NO RM provides im- proved robustness to performance decay due to oversmoothin g with increasing number of layers. 10Published as a conference paper at ICLR 2020 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GAT train_acc val_acc test_acc 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GAT train_acc val_acc test_acc 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GAT train_acc val_acc test_acc Figure 6: Comparison of ‘vanilla’ (dashed) vs. P A IR NO RM -enhanced (solid) GCN (left) and GA T (right) models, corresponding to Figure 3, for datasets (fr om top to bottom) Citeseer, Pubmed, and CoauthorCS. P A IR NO RM provides improved robustness against performance decay wi th increasing number of layers. 11Published as a conference paper at ICLR 2020 A.4 A D D IT IO NA L PE RF O RM A N CE PL OT S W IT H IN CRE A S IN G NU M BE R O F LAY E RS U N D E R SSNC-MV W IT H p = 1 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-SGC 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GAT train_acc val_acc test_acc 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-SGC 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GAT train_acc val_acc test_acc 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-SGC 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GAT train_acc val_acc test_acc Figure 7: Comparison of ‘vanilla’ (dashed) vs. P A IR NO RM -enhanced (solid) (from left to right) SGC, GCN, and GA T model performance under SSNC-MV for p = 1, corresponding to Figure 4, for datasets (from top to bottom) Citeseer, Pubmed, and CoauthorCS. Green diamond symbols depict the layer at which validation accuracy peaks. P A IR NO RM boosts overall performance by enabling more robust deep GNNs. 12Published as a conference paper at ICLR 2020 A.5 A D D IT IO NA L EX P E RIM E N T S U N D E R SSNC-MV W IT H IN CRE A S IN G MIS S IN G FRACT IO N p In this section we report additional experiment results und er the SSNC-MV setting with varying missing fraction, in particular p = {0. 7, 0. 8, 0. 9, 1} and also report the base case where p = 0 for comparison. Figure 8 presents results on all four datasets for GCN vs. PA IR NO RM -enhanced GCN (denoted PN for short). The models without any skip connections are de noted by *-0, with one-hop skip connection by *-1, and with one and two-hop skip connections by *-2. Barcharts on the right report the best layer that each model produced the highest validati on accuracy, and those on the left report the corresponding test accuracy. Figure 9 presents corresp onding results for GA T . W e discuss the take-aways from these ﬁgures on the following page. GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2 0.6 0.7 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc cora GCN GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2 0.4 0.6 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc citeseer GCN GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20.4 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc pubmed GCN GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2 0.4 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc coauthor_CS GCN Figure 8: Supplementary results to T able 2 for GCN on (from to p to bottom) Cora, Citeseer, Pubmed, and CoauthorCS. 13Published as a conference paper at ICLR 2020 W e make the following observations based on Figures 8 and 9: • Performance of ‘vanilla’ GCN and GA T models without skip con nections (i.e., GCN-0 and GA T -0) drop monotonically as we increase missing fraction p. • PA IR NO RM -enhanced ‘vanilla’ models (PN-0, no skips) perform compar ably or better than GCN-0 and GA T -0 in all cases, especially as p increases. In other words, with P A IR NO RM at work, model performance is more robust against missing da ta. • Best number of layers for GCN-0 as we increase p only changes between 2-4. For GA T -0, it changes mostly between 2-6. • PA IR NO RM -enhanced ‘vanilla’ models (PN-0, no skips) can go deeper, i .e., they can lever- age a larger range of #layers (2-12) as we increase p. Speciﬁcally, GCN-PN-0 (GA T -PN-0) uses equal number or more layers than GCN-0 (GA T -0) in almost all cases. • Without any normalization, adding skip connections helps— GCN/GA T -1 and GCN/GA T -2 are better than GCN/GA T -0, especially as we increase p. • With P A IR NO RM but no-skip, performance is comparable or better than just a dding skips. • Adding skips on top of P A IR NO RM does not seem to introduce any notable gains. In summary, simply employing our P A IR NO RM for GCN and GA T provides robustness against oversmoothing that allows them to go deeper and achieve impr oved performance under SSNC-MV. GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc cora GAT GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.4 0.6 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc citeseer GAT GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.5 0.6 0.7 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc pubmed GAT GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.7 0.8 0.9 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc coauthor_CS GAT Figure 9: Supplementary results to T able 3 for GA T on (from to p to bottom) Cora, Citeseer, Pubmed, and CoauthorCS. 14Published as a conference paper at ICLR 2020 A.6 C A S E S T U DY : A D D IT IO NA L ME A S U RE S F O R PA IR NO RM A N D PA IR NO RM -SI W IT H SGC A N D GCN T o better understand why P A IR NO RM and P A IR NO RM -SI are helpful for training deep GNNs, we report additional measures for (SGC and GCN) with (P A IR NO RM and P A IR NO RM -SI) over the Cora dataset. In the main text, we claim TPSD (total pairwise squa red distances) is constant across layers for SGC with P A IR NO RM (for GCN/GA T this is not guaranteed because of the inﬂuence o f activation function and dropout layer). In this section we e mpirically measure pairwise (squared) distances for both SGC and GCN, with P A IR NO RM and P A IR NO RM -SI. A.6.1 SGC W IT H PA IR NO RM A N D PA IR NO RM -SI T o verify our analysis of P A IR NO RM for SGC, and understand how the variant of P A IR NO RM (PA IR NO RM -SI) works, we measure the average pairwise squared distanc e (APSD) as well as the average pairwise distance (APD) between the representatio ns for two categories of node pairs: (1) connected pairs (nodes that are directly connected in graph ) and (2) random pairs (uniformly ran- domly chosen among the node set). APSD of random pairs reﬂect s the TPSD, and APD of random pairs reﬂects the total pairwise distance (TPD). Under the h omophily assumption of the labels w .r.t. the graph structure, we want APD or APSD of connected pairs to be small while keeping APD or APSD of random pairs relatively large. The results are shown in Figure 10. Without normalization, SGC suffers from fast diminishing APD and APSD of random pairs. As we have proved, P A IR NO RM normalizes APSD to be constant across layers, however it does not normalize APD, which appears to d ecrease linearly with increasing num- ber of layers. Surprisingly, although P A IR NO RM -SI is not theoretically proved to have a constant APSD and APD, empirically it achieves more stable APSD and AP D than P A IR NO RM . W e were not able to prove this phenomenon mathematically, and leave it for further investigation. 0 10 20 30 40 50 0 10 20 30Average squared distance SGC connected pairs random pairs 0 10 20 30 40 50 Layers  0 1 2 3 4 5 6Average distance  0 10 20 30 40 50 0 10 20 30 SGC + PairNorm connected pairs random pairs 0 10 20 30 40 50 Layers  0 1 2 3 4 5 6 0 10 20 30 40 50 0 10 20 30 SGC + PairNorm-SI connected pairs random pairs 0 10 20 30 40 50 Layers  1 2 3 4 5 6 Dataset: cora Figure 10: Measuring average distance (squared and not-squ ared) between representations at each layer for SGC, SGC with P A IR NO RM , and SGC with P A IR NO RM -SI. The setting is the same with Figure 1 and they share the same performance. APD does not capture the full information of the distribution of pairwise distances. T o show how the distribution changes by increasing number of layers, we use T ensorboard to plot the histograms of pairwise distances, as shown in Figure 11. Comparing SGC a nd SGC with P A IR NO RM , adding PA IR NO RM keeps the left shift (shrinkage) of the distribution of rand om pair distances much slower than without normalization, while still sharing similar be havior of the distribution of connected pairwise distances. P A IR NO RM -SI seems to be more powerful in keeping the median and mean of the distribution of random pair distances stable, while “sp reading” the distribution out by increasing the variance. The performance of P A IR NO RM and P A IR NO RM -SI are similar, however it seems that PA IR NO RM -SI is more powerful in stabilizing TPD and TPSD. 15Published as a conference paper at ICLR 2020 distr. of random pair distances  Layers  Distance  Layers  SGC   SGC + PairNorm SGC + PairNorm-SI  distr. of connected pair distances distr. of connected pair di stances  distr. of random pair distances distr. of random pair distances  Distance Distance  Dataset:  Cora  distr. of connected pair distances  Figure 11: Measuring distribution of distances between rep resentations at each layer for SGC, SGC with P A IR NO RM , and SGC with P A IR NO RM -SI. Supplementary results for Figure 10. A.6.2 GCN W IT H PA IR NO RM A N D PA IR NO RM -SI 0 2 4 6 8 10 0.000 0.002 0.004 0.006 0.008 0.010Average squared distance GCN connected pairs random pairs 0 2 4 6 8 10 Layers  0.00 0.02 0.04 0.06 0.08 0.10Average distance  0 2 4 6 8 10 0 20 40 60 80 100 120 GCN + PairNorm connected pairs random pairs 0 2 4 6 8 10 Layers  2 4 6 8 10 0 2 4 6 8 10 0.0 0.2 0.4 0.6 0.8 GCN + PairNorm-SI connected pairs random pairs 0 2 4 6 8 10 Layers  0.2 0.4 0.6 0.8 Dataset: cora Figure 12: Measuring average distance (squared and not-squ ared) between representations at each layer for GCN, GCN with P A IR NO RM , and GCN with P A IR NO RM -SI. W e trained three 12-layer GCNs with #hidden=128 and dropout=0.6 in 1000 epochs. Respe ctive test set accuracies are 31.09%, 77.77%, 75.09%. Note that the scale of distances is n ot comparable across models, since they have learnable parameters that scale these distances d ifferently. The formal analysis for P A IR NO RM and P A IR NO RM -SI is based on SGC. GCN (and other GNNs) has learnable parameters, dropout layers, and activation l ayers, all of which complicate direct math- ematical analyses. Here we perform similar empirical measu rements for pairwise distances to get a rough sense of how P A IR NO RM and P A IR NO RM -SI work with GCN based on the Cora dataset. Figures 12 and 13 demonstrate how P A IR NO RM and P A IR NO RM -SI can help train a relatively deep (12 layers) GCN. Notice that oversmoothing occurs very quickly for GCN without any normalization, where both con- nected and random pair distances reach zero (!). In contrast , GCN with P A IR NO RM or P A IR NO RM - SI is able to keep random pair distances relatively apart whi le allowing connected pair distances to shrink. As also stated in main text, using P A IR NO RM -SI for GCN and GA T is relatively more 16Published as a conference paper at ICLR 2020 stable than using P A IR NO RM in general cases (notice the near-constant random pair dist ances in the rightmost subﬁgures). There are several possible expla nations for why P A IR NO RM -SI is more stable. First, as shown in Figure 10 and Figure 12, P A IR NO RM -SI not only keeps APSD stable but also APD, further, the plots of distributions of pairwise di stances (Figures 11 and 13) also show the power of P A IR NO RM -SI (notice the large gap between smaller connected pairwis e distances and the larger random pairwise distances). Second, we conjecture t hat restricting representations to reside on a sphere can make training stable and faster, which we also observe empirically by studying the training curves. Third, GCN and GA T tend to overﬁt easily for the SSNC problem, due to many learnable parameters across layers and limited labeled inp ut data, therefore it is possible that adding more restriction on these models helps reduce overﬁtting. GCN GCN + PairNorm GCN + PairNorm-SI Dataset:  Cora  distr. of connected pair distances distr. of connected pair di stances distr. of connected pair distances  distr. of random pair distances distr. of random pair distances d istr. of random pair distances  Layers  Distance  Layers  Distance Distance  Figure 13: Measuring distribution of distances between rep resentations at each layer for GCN, GCN with P A IR NO RM , and GCN with P A IR NO RM -SI. Supplementary results for Figure 12. All in all, these empirical measurements as illustrated thr oughout the ﬁgures in this section demon- strates that P A IR NO RM and P A IR NO RM -SI successfully address the oversmoothing problem for deep GNNs. Our work is the ﬁrst to propose a normalization layer speciﬁcally designed for graph neural networks, which we hope will kick-start more work in t his area toward training more robust and effective GNNs. 17",
      "meta_data": {
        "arxiv_id": "1909.12223v2",
        "authors": [
          "Lingxiao Zhao",
          "Leman Akoglu"
        ],
        "published_date": "2019-09-26T16:20:37Z",
        "pdf_url": "https://arxiv.org/pdf/1909.12223v2.pdf"
      }
    },
    {
      "title": "Node Dependent Local Smoothing for Scalable Graph Learning",
      "abstract": "Recent works reveal that feature or label smoothing lies at the core of Graph\nNeural Networks (GNNs). Concretely, they show feature smoothing combined with\nsimple linear regression achieves comparable performance with the carefully\ndesigned GNNs, and a simple MLP model with label smoothing of its prediction\ncan outperform the vanilla GCN. Though an interesting finding, smoothing has\nnot been well understood, especially regarding how to control the extent of\nsmoothness. Intuitively, too small or too large smoothing iterations may cause\nunder-smoothing or over-smoothing and can lead to sub-optimal performance.\nMoreover, the extent of smoothness is node-specific, depending on its degree\nand local structure. To this end, we propose a novel algorithm called\nnode-dependent local smoothing (NDLS), which aims to control the smoothness of\nevery node by setting a node-specific smoothing iteration. Specifically, NDLS\ncomputes influence scores based on the adjacency matrix and selects the\niteration number by setting a threshold on the scores. Once selected, the\niteration number can be applied to both feature smoothing and label smoothing.\nExperimental results demonstrate that NDLS enjoys high accuracy --\nstate-of-the-art performance on node classifications tasks, flexibility -- can\nbe incorporated with any models, scalability and efficiency -- can support\nlarge scale graphs with fast training.",
      "full_text": "Node Dependent Local Smoothing for Scalable Graph Learning Wentao Zhang1, Mingyu Yang1, Zeang Sheng1, Yang Li1 Wen Ouyang2, Yangyu Tao2, Zhi Yang1,3, Bin Cui1,3,4 1School of CS, Peking University 2Tencent Inc. 3 Key Lab of High Conﬁdence Software Technologies, Peking University 4Institute of Computational Social Science, Peking University (Qingdao), China 1{wentao.zhang, ymyu, shengzeang18, liyang.cs, yangzhi, bin.cui}@pku.edu.cn 2{gdpouyang, brucetao}@tencent.com Abstract Recent works reveal that feature or label smoothing lies at the core of Graph Neural Networks (GNNs). Concretely, they show feature smoothing combined with simple linear regression achieves comparable performance with the carefully designed GNNs, and a simple MLP model with label smoothing of its prediction can outperform the vanilla GCN. Though an interesting ﬁnding, smoothing has not been well understood, especially regarding how to control the extent of smoothness. Intuitively, too small or too large smoothing iterations may cause under-smoothing or over-smoothing and can lead to sub-optimal performance. Moreover, the extent of smoothness is node-speciﬁc, depending on its degree and local structure. To this end, we propose a novel algorithm called node-dependent local smoothing (NDLS), which aims to control the smoothness of every node by setting a node- speciﬁc smoothing iteration. Speciﬁcally, NDLS computes inﬂuence scores based on the adjacency matrix and selects the iteration number by setting a threshold on the scores. Once selected, the iteration number can be applied to both feature smoothing and label smoothing. Experimental results demonstrate that NDLS enjoys high accuracy – state-of-the-art performance on node classiﬁcations tasks, ﬂexibility – can be incorporated with any models, scalability and efﬁciency – can support large scale graphs with fast training. 1 Introduction In recent years, Graph Neural Networks (GNNs) have received a surge of interest with the state-of-the- art performance on many graph-based tasks [2, 41, 12, 39, 33, 34]. Recent works have found that the success of GNNs can be mainly attributed to smoothing, either at feature or label level. For example, SGC [32] shows using smoothed features as input to a simple linear regression model achieves comparable performance with lots of carefully designed and complex GNNs. At the smoothing stage, features of neighbor nodes are aggregated and combined with the current node’s feature to form smoothed features. This process is often iterated multiple times. The smoothing is based on the assumption that labels of nodes that are close to each other are highly correlated, therefore, the features of nodes nearby should help predict the current node’s label. One crucial and interesting parameter of neighborhood feature aggregation is the number of smoothing iterations k, which controls how much information is being gathered. Intuitively, an aggregation process of kiterations (or layers) enables a node to leverage information from nodes that are k-hop away [26, 38]. The choice of k is closely related to the structural properties of graphs and has a 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2110.14377v1  [cs.LG]  27 Oct 2021(a) Two nodes with different local structures /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000026/uni00000027/uni00000029  /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (b) The CDF of LSI in different graphs Figure 1: (Left) The node in dense region has larger smoothed area within two iterations of propagation. (Right) The CDF of LSI in three citation networks. signiﬁcant impact on the model performance. However, most existing GNNs only consider the ﬁxed-length propagation paradigm – a uniform kfor all the nodes. This is problematic since the number of iterations should be node dependent based on its degree and local structures. For example, as shown in Figure 1(a), the two nodes have rather different local structures, with the left red one resides in the center of a dense cluster and the right red one on the periphery with few connections. The number of iterations to reach an optimal level of smoothness are rather different for the two nodes. Ideally, poorly connected nodes (e.g., the red node on the right) needs large iteration numbers to efﬁciently gather information from other nodes while well-connected nodes (e.g., the red node on the left) should keep the iteration number small to avoid over-smoothing. Though some learning-based approaches have proposed to adaptively aggregate information for each node through gate/attention mechanism or reinforcement learning [ 29, 21, 40, 27], the performance gains are at the cost of increased training complexity, hence not suitable for scalable graph learning. In this paper, we propose a simple yet effective solution to this problem. Our approach, called node-dependent local smoothing (NDLS), calculates a node-speciﬁc iteration number for each node, referred to as local smooth iteration (LSI). Once the LSI for a speciﬁc node is computed, the corresponding local smoothing algorithm only aggregates the information from the nodes within a distance less than its LSI as the new feature. The LSI is selected based on inﬂuence scores, which measure how other nodes inﬂuence the current node. NDLS sets the LSI for a speciﬁc node to be the minimum number of iterations so that the inﬂuence score is ϵ-away from the over-smoothing score, deﬁned as the inﬂuence score at inﬁnite iteration. The insight is that each node’s inﬂuence score should be at a reasonable level. Since the nodes with different local structures have different “smoothing speed”, we expect the iteration number to be adaptive. Figure 1(b) illustrates Cumulative Distribution Function (CDF) for the LSI of individual nodes in real-world graphs. The heterogeneous and long-tail property exists in all the datasets, which resembles the characteristics of the degree distribution of nodes in real graphs. Based on NDLS, we propose a new graph learning algorithm with three stages: (1) feature smoothing with NDLS (NDLS-F); (2) model training with smoothed features; (3) label smoothing with NDLS (NDLS-L). Note that in our framework, the graph structure information is only used in pre-processing and post-processing steps, i.e., stages (1) and (3) (See Figure 2). Our NDLS turns a graph learning problem into a vanilla machine learning problem with independent samples. This simplicity enables us to train models on larger-scale graphs. Moreover, our NDLS kernel can act as a drop-in replacement for any other graph kernels and be combined with existing models such as Multilayer Perceptron (MLP), SGC [32], SIGN [28], S2GC [42] and GBP [6]. Extensive evaluations on seven benchmark datasets, including large-scale datasets like ogbn- papers100M [16], demonstrates that NDLS achieves not only the state-of-the-art node classiﬁcation performance but also high training scalability and efﬁciency. Especially, NDLS outperforms APPNP [29] and GAT [30] by a margin of 1.0%-1.9% and 0.9%-2.4% in terms of test accuracy, while achieving up to 39×and 186×training speedups, respectively. 2 Preliminaries In this section, we ﬁrst introduce the semi-supervised node classiﬁcation task and review the prior models, based on which we derive our method in Section 3. Consider a graphG= (V, E) with |V|= n 2nodes and |E|= medges, the adjacency matrix (including self loops) is denoted as ˜A ∈Rn×n and the feature matrix is denoted as X = {x1,x2...,xn}in which xi ∈Rf represents the feature vector of node vi. Besides, Y = {y1,y2...,yl}is the initial label matrix consisting of one-hot label indicator vectors. The goal is to predict the labels for nodes in the unlabeled set Vu with the supervision of labeled set Vl. GCN smooths the representation of each node via aggregating its own representations and the ones of its neighbors’. This process can be deﬁned as X(k+1) = δ ( ˆAX(k)W(k) ) , ˆA = ˜Dr−1 ˜A˜D−r, (1) where ˆA is the normalized adjacency matrix, r∈[0,1] is the convolution coefﬁcient, and ˜D is the diagonal node degree matrix with self loops. Here X(k) and X(k+1) are the smoothed node features of layer k and k+ 1respectively while X(0) is set to X, the original feature matrix. In addition, W(k) is a layer-speciﬁc trainable weight matrix at layer k, and δ(·) is the activation function. By setting r= 0.5, 1 and 0, the convolution matrix ˜Dr−1 ˜A˜D−r represents the symmetric normalization adjacency matrix ˜D−1/2 ˜A˜D−1/2 [20], the transition probability matrix ˜A˜D−1 [37], and the reverse transition probability matrix ˜D−1 ˜A [35], respectively. SGC. For each GCN layer deﬁned in Eq. 1, if the non-linear activation function δ(·) is an identity function and W(k) is an identity matrix, we get the smoothed feature after k-iterations propagation as X(k) = ˆAkX. Recent studies have observed that GNNs primarily derive their beneﬁts from performing feature smoothing over graph neighborhoods rather than learning non-linear hierarchies of features as implied by the analogy to CNNs [ 25, 10, 15]. By hypothesizing that the non-linear transformations between GCN layers are not critical, SGC [32] ﬁrst extracts the smoothed features X(k) then feeds them to a linear model, leading to higher scalability and efﬁciency. Following the design principle of SGC, piles of works have been proposed to further improve the performance of SGC while maintaining high scalability and efﬁciency, such as SIGN [28], S2GC [42] and GBP [6]. Over-Smoothing [22] issue. By continually smoothing the node feature with inﬁnite number of propagation in SGC, the ﬁnal smoothed feature X(∞) is X(∞) = ˆA∞X, ˆA∞ i,j = (di + 1)r(dj + 1)1−r 2m+ n , (2) where ˆA∞is the ﬁnal smoothed adjacency matrix, ˆA∞ i,j is the weight between nodes vi and vj, di and dj are the node degrees for vi and vj, respectively. Eq. (2) shows that as we smooth the node feature with an inﬁnite number of propagations in SGC, the ﬁnal feature is over-smoothed and unable to capture the full graph structure information since it only relates with the node degrees of target nodes and source nodes. For example, if we set r= 0or 1, all nodes will have the same smoothed features because only the degrees of the source or target nodes have been considered. 3 Local Smoothing Iteration (LSI) The features after k iterations of smoothing is X(k) = ˆAkX. Inspired by [ 35], we measure the inﬂuence of node vj on node vi by measuring how much a change in the input feature of vj affects the representation of vi after kiterations. For any node vi, the inﬂuence vector captures the inﬂuences of all other nodes. Considering the hth feature of X, we deﬁne an inﬂuence matrix Ih(k): Ih(k)ij = ∂ˆX(k) ih ∂ˆX(0) jh . (3) I(k) = ˆAk,˜Ii = ˆA∞ (4) Since Ih(k) is independent to h, we replace Ih(k) with I(k), which can be further represented as I(k) = Ih(k), ∀h ∈{1,2,..,f }, where f indicates the number of features of X. We denote I(k)i as the ith row of I(k), and ˜I as I(∞). Given the normalized adjacency matrix ˆA, we can 3have I(k) = ˆAk and ˜I = ˆA∞. According to Eq. (2), ˜I converges to a unique stationary matrix independent of the distance between nodes, resulting in that the aggregated features of nodes are merely relative with their degrees (i.e., over-smoothing). We denote I(k)i as the ith row of I(k), and it means the inﬂuence from the other nodes to the node vi after k iterations of propagation. We introduce a new concept local smoothing iteration (parameterized by ϵ), which measures the minimal number of iterations krequired for the inﬂuence of other nodes on node vi to be within an ϵ-distance to the over-smoothing stationarity ˜Ii. Deﬁnition 3.1. Local-Smoothing Iteration (LSI, parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −I(k)i||2 <ϵ}, (5) where ||·||2 is two-norm, and ϵis an arbitrary small constant with ϵ> 0. Here ϵis a graph-speciﬁc parameter, and a smaller ϵindicates a stronger smoothing effect. The ϵ-distance to the over-smoothing stationarity ˜Ii ensures that the smooth effect on node vi is sufﬁcient and bounded to avoid over-smoothing. As shown in Figure 1(b), we can have that the distribution of LSI owns the heterogeneous and long-tail property, where a large percentage of nodes have much smaller LSI than the rest. Therefore, the required LSI to approach the stationarity is heterogeneous across nodes. Now we discuss the connection between LSI and node local structure, showcasing nodes in the sparse region (e.g., both the degrees of itself and its neighborhood are low) can greatly prolong the iteration to approach over-smoothing stationarity. This heterogeneity property is not fully utilized in the design of current GNNs, leaving the model design in a dilemma between unnecessary iterations for a majority of nodes and insufﬁcient iterations for the rest of nodes. Hence, by adaptively choosing the iteration based on LSI for different nodes, we can signiﬁcantly improve model performance. Theoretical Properties of LSI. We now analyze the factors determining the LSI of a speciﬁc node. To facilitate the analysis, we set the coefﬁcient r = 0for the normalized adjacency matrix ˆA in Eq. (1), thus ˆA = ˜D−1 ˜A. The proofs of following theorems can be found in Appendix A.1. Theorem 3.1. Given feature smoothing X(k) = ˆAkX with ˆA = ˜D−1 ˜A, we have K(i,ϵ) ≤logλ2  ϵ √ ˜di 2m+ n  , (6) where λ2 is the second largest eigenvalue of ˆA, ˜di denotes the degree of node vi plus 1 (i.e., ˜di = di + 1), and m, ndenote the number of edges and nodes respectively. Note that λ2 ≤1. Theorem 3.1 shows that the upper-bound of the LSI is positively correlated with the scale of the graph (m,n), the sparsity of the graph (small λ2 means strong connection and low sparsity, and vice versa), and negatively correlated with the degree of nodevi. Theorem 3.2. For any nodes iin a graph G, K(i,ϵ) ≤max {K(j,ϵ),j ∈N(i)}+ 1, (7) where N(i) is the set of node vi’s neighbours. Theorem 3.2 indicates that the difference between two neighboring nodes’ LSIs is no more than1, therefore the nodes with a super-node as neighbors (or neighbor’s neighbors) may have small LSIs. That is to say, the sparsity of the local area, where a node locates, also affects its LSI positively. Considering Theorems 3.1 and 3.2 together, we can have a union upper-bound of K(i,ϵ) as K(i,ϵ) ≤min   max {K(j,ϵ),j ∈N(i)}+ 1,logλ2  ϵ √ ˜di 2m+ n     . (8) 4 NDLS Pipeline The basic idea of NDLS is to utilize the LSI heterogeneity to perform a node-dependent aggregation over a neighborhood within a distance less than the speciﬁc LSI for each node. Further, we propose 4Adjacent Matrix A Final Prediction �Y ⋯ MLP Soft label �Y Stage 1: preprocessing Stage 2: training Stage 3: postprocessing  Feature X �X Smooth Feature with NDLS-F (replaceable) Adjacent Matrix A B A B1.0 B 0.3 0.3 0.7 … LSI Estimation A A’s feature smoothed  under optimal steps … 1.0 A0.8 0.3 0.2 0.2 0.1 … … LSI(A) = 4 LSI(B) = 2 0.1 0.3 … B’s feature smoothed  under optimal steps Details about NDLS-F B A Smooth Label with NDLS-L �XB�XA Figure 2: Overview of the proposed NDLS method, including (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). NDLS-F and NDLS-L correspond to pre-processing and post-processing steps respectively. a simple pipeline with three main parts (See Figure 2): (1) a node-dependent local smoothing of the feature (NDLS-F) over the graph, (2) a base prediction result with the smoothed feature, (3) a node-dependent local smoothing of the label predictions (NDLS-L) over the graph. Note this pipeline is not trained in an end-to-end way, the stages (1) and (3) in NDLS are only the pre-processing and post-processing steps, respectively. Furthermore, the graph structure is only used in the pre/post- processing NDLS steps, not for the base predictions. Compared with prior GNN models, this key design enables higher scalability and a faster training process. Based on the graph structure, we ﬁrst compute the node-dependent local smoothing iteration that maintains a proper distance to the over-smoothing stationarity. Then the corresponding local smoothing kernel only aggregates the information (feature or prediction) for each node from the nodes within a distance less than its LSI value. The combination of NDLS-F and NDLS-L takes advantage of both label smoothing (which tends to perform fairly well on its own without node features) and the node feature smoothing. We will see that combining these complementary signals yields state-of-the- art predictive accuracy. Moreover, our NDLS-F kernel can act as a drop-in replacement for graph kernels in other scalable GNNs such as SGC, S2GC, GBP, etc. 4.1 Smooth Features with NDLS-F Once the node-dependent LSI K(i,ϵ) for a speciﬁc node iis obtained, we smooth the initial input feature Xi of node iwith node-dependent LSI as: ˜Xi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 X(k) i . (9) To capture sufﬁcient neighborhood information, for each node vi, we average its multi-scale features {X(k) i |k≤K(i,ϵ)}obtained by aggregating information within khops from the node vi. The matrix form of the above equation can be formulated as ˜X(ϵ) = max i K(i,ϵ) ∑ k=0 M(k)X(k), M(k) ij = { 1 K(i,ϵ)+1 , i = j and k ≤K(i,ϵ) 0, otherwise , (10) where M(k) is a set of diagonal matrix. 4.2 Simple Base Prediction With the smoothed feature ˜X according to Eq. 9, we then train a model to minimize the loss –∑ vi∈Vl ℓ ( yi,f( ˜Xi) ) , where ˜Xi denotes the ith row of ˜X, ℓis the cross-entropy loss function, and f( ˜Xi) is the predictive label distribution for node vi. In NDLS, the default f is a MLP model and 5ˆY = f( ˜X) is its soft label predicted (softmax output). Note that, many other models such as Random Forest [24] and XGBoost [7] could also be used in NDLS (See more results in Appendix A.2). 4.3 Smooth Labels with NDLS-L Similar to the feature propagation, we can also propagate the soft label ˆY with ˆY(k) = ˆAk ˆY. Considering the inﬂuence matrix of softmax label Jh(k). Jh(k)ij = ∂ˆY(k) ih ∂ˆY(0) jh . (11) According to the deﬁnition above we have that Jh(k) =Ih(k),∀h∈{1,2,..,f }. (12) Therefore, local smoothing can be further applied to address over-smoothing in label propagation. Concretely, we smooth an initial soft label ˆYi of node vi with NDLS as follows ˜Yi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 ˆY(k) i . (13) Similarly, the matrix form of the above equation can be formulated as ˜Y(ϵ) = max i K(i,ϵ) ∑ k=0 M(k) ˆY(k), (14) where M(k) follows the deﬁnition in Eq. (10). 5 Comparison with Existing Methods Decoupled GNNs. The aggregation and transformation operations in coupled GNNs (i.e., GCN [18], GAT [30] and JK-Net [ 35]) are inherently intertwined in Eq. (1), so the propagation iterations L always equals to the transformation iterations K. Recently, some decoupled GNNs (e.g., PPNP [20], PPRGo [1], APPNP [20], AP-GCN [29] and DAGNN [25]) argue the entanglement of these two operations limits the propagation depth and representation ability of GNNs, so they ﬁrst do the transformation and then smooth and propagate the predictive soft label with higher depth in an end-to-end manner. Especially, AP-GCN and DAGNN both use a learning mechanism to learn propagation adaptively. Unfortunately, all these coupled and decoupled GNNs are hard to scale to large graphs – scalability issue since they need to repeatedly perform an expensive recursive neighborhood expansion in multiple propagations of the features or soft label predicted. NDLS addresses this issue by dividing the training process into multiple stages. Sampling-based GNNs. An intuitive method to tackle the recursive neighborhood expansion problem is sampling. As a node-wise sampling method, GraphSAGE [14] samples the target nodes as a mini-batch and samples a ﬁxed size set of neighbors for computing. VR-GCN [5] analyzes the variance reduction on node-wise sampling, and it can reduce the size of samples with an additional memory cost. In the layer level, Fast-GCN [ 3] samples a ﬁxed number of nodes at each layer, and ASGCN [17] proposes the adaptive layer-wise sampling with better variance control. For the graph-wise sampling, Cluster-GCN [8] clusters the nodes and only samples the nodes in the clusters, and GraphSAINT [37] directly samples a subgraph for mini-batch training. We don’t use sampling in NDLS since the sampling quality highly inﬂuences the classiﬁcation performance. Linear Models. Following SGC [32], some recent methods remove the non-linearity between each layer in the forward propagation. SIGN [28] allows using different local graph operators and proposes to concatenate the different iterations of propagated features. S2GC [42] proposes the simple spectral graph convolution to average the propagated features in different iterations. In addition, GBP [ 6] further improves the combination process by weighted averaging, and all nodes in the same layer share the same weight. In this way, GBP considers the smoothness in a layer perspective way. Similar 6Table 1: Algorithm analysis for existing scalable GNNs. n, m, c, and f are the number of nodes, edges, classes, and feature dimensions, respectively. bis the batch size, and krefers to the number of sampled nodes. Lcorresponds to the number of times we aggregate features, K is the number of layers in MLP classiﬁers. For the coupled GNNs, we always have K = L. Type Method Preprocessing and postprocessingTraining Inference Memory Node-wise samplingGraphSAGE - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Layer-wise samplingFastGCN - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Graph-wise samplingCluster-GCN O(m) O(Lmf+Lnf2) O(Lmf+Lnf2) O(bLf+Lf2) Linear model SGC O(Lmf) O(nf2) O(nf2) O(bf+f2)S2GC O(Lmf) O(nf2) O(nf2) O(bf+f2)SIGN O(Lmf) O(Knf2) O(Knf2) O(bLf+Kf2) GBP O(Lnf+L √mlgnε ) O(Knf2) O(Knf2) O(bf+Kf2)Linear model NDLS O(Lmf+Lmc) O(Knf2) O(Knf2) O(bf+Kf2) Table 2: Overview of datasets and task types (T/I represents Transductive/Inductive). Dataset #Nodes #Features #Edges #Classes #Train/Val/Test Type Description Cora 2,708 1,433 5,429 7 140/500/1,000 T citation network Citeseer 3,327 3,703 4,732 6 120/500/1,000 T citation network Pubmed 19,717 500 44,338 3 60/500/1,000 T citation network Industry 1,000,000 64 1,434,382 253 5K/10K/30K T short-form video network ogbn-papers100M 111,059,956 128 1,615,685,872 172 1,207K/125K/214K T citation network Flickr 89,250 500 899,756 7 44K/22K/22K I image network Reddit 232,965 602 11,606,919 41 155K/23K/54K I social network to these works, we also use a linear model for higher training scalability. The difference lies in that we consider the smoothness from a node-dependent perspective and each node in NDLS has a personalized aggregation iteration with the proposed local smoothing mechanism. Table 1 compares the asymptotic complexity of NDLS with several representative and scalable GNNs. In the stage of the preprocessing, the time cost of clustering in Cluster-GCN is O(m) and the time complexity of most linear models is O(Lmf). Besides, NDLS has an extra time cost O(Lmc) for the postprocessing in label smoothing. GBP conducts this process approximately with a bound of O(Lnf + L √mlg n ε ), where εis a error threshold. Compared with the sampling-based GNNs, the linear models usually have smaller training and inference complexity, i.e., higher efﬁciency. Memory complexity is a crucial factor in large-scale graph learning because it is difﬁcult for memory-intensive algorithms such as GCN and GAT to train large graphs on a single machine. Compared with SIGN, both GBP and NDLS do not need to store smoothed features in different iterations, and the feature storage complexity can be reduced from O(bLf) to O(bf). 6 Experiments In this section, we verify the effectiveness of NDLS on seven real-world graph datasets. We aim to answer the following four questions. Q1: Compared with current SOTA GNNs, can NDLS achieve higher predictive accuracy and why? Q2: Are NDLS-F and NDLS-L better than the current feature and label smoothing mechanisms (e.g., the weighted feature smoothing in GBP and the adaptive label smoothing in DAGNN)? Q3: Can NDLS obtain higher efﬁciency over the considered GNN models? Q4: How does NDLS perform on sparse graphs (i.e., low label/edge rate, missing features)? 6.1 Experimental Setup Datasets. We conduct the experiments on (1) six publicly partitioned datasets, including four citation networks (Citeseer, Cora, PubMed, and ogbn-papers100M) in [ 18, 16] and two social networks (Flickr and Reddit) in [37], and (2) one short-form video recommendation graph (Industry) from our industrial cooperative enterprise. The dataset statistics are shown in Table 2 and more details about these datasets can be found in Appendix A.3. Baselines. In the transductive setting, we compare our method with (1) the coupled GNNs: GCN [18], GAT [ 30] and JK-Net [ 35]; (2) the decoupled GNNs: APPNP [ 20], AP-GCN [ 29], 7Table 3: Results of transductive settings. OOM means “out of memory”. Type Models Cora Citeseer PubMed Industry ogbn-papers100M Coupled GCN 81.8 ±0.5 70.8 ±0.5 79.3 ±0.7 45.9 ±0.4 OOM GAT 83.0 ±0.7 72.5 ±0.7 79.0 ±0.3 46.8 ±0.7 OOM JK-Net 81.8 ±0.5 70.7 ±0.7 78.8 ±0.7 47.2 ±0.3 OOM Decoupled APPNP 83.3 ±0.5 71.8 ±0.5 80.1 ±0.2 46.7 ±0.6 OOM AP-GCN 83.4 ±0.3 71.3 ±0.5 79.7 ±0.3 46.9 ±0.7 OOM PPRGo 82.4 ±0.2 71.3 ±0.5 80.0 ±0.4 46.6 ±0.5 OOM DAGNN (Gate) 84.4±0.5 73.3 ±0.6 80.5 ±0.5 47.1 ±0.6 OOM DAGNN (NDLS-L)∗ 84.4±0.6 73.6 ±0.7 80.9 ±0.5 47.2 ±0.7 OOM Linear MLP 61.1 ±0.6 61.8 ±0.8 72.7 ±0.6 41.3 ±0.8 47.2 ±0.3 SGC 81.0 ±0.2 71.3 ±0.5 78.9 ±0.5 45.2 ±0.3 63.2 ±0.2 SIGN 82.1 ±0.3 72.4 ±0.8 79.5 ±0.5 46.3 ±0.5 64.2 ±0.2 S2GC 82.7 ±0.3 73.0 ±0.2 79.9 ±0.3 46.6 ±0.6 64.7 ±0.3 GBP 83.9 ±0.7 72.9 ±0.5 80.6 ±0.4 46.9 ±0.7 65.2 ±0.3 Linear NDLS-F+MLP∗ 84.1±0.6 73.5 ±0.5 81.1 ±0.6 47.5 ±0.7 65.3 ±0.5 MLP+NDLS-L∗ 83.9±0.6 73.1 ±0.8 81.1 ±0.6 46.9 ±0.7 64.6 ±0.4 SGC+NDLS-L∗ 84.2±0.2 73.4 ±0.5 81.1 ±0.4 47.1 ±0.6 64.9 ±0.3 NDLS∗ 84.6±0.5 73.7 ±0.6 81.4 ±0.4 47.7 ±0.5 65.6 ±0.3 DAGNN (Gate) [25], and PPRGo [1]; (3) the linear-model-based GNNs: MLP, SGC [32], SIGN [28], S2GC [42] and GBP [6]. In the inductive setting, the compared baselines are sampling-based GNNs: GraphSAGE [14], FastGCN [3], ClusterGCN [8] and GraphSAINT [37]. Detailed descriptions of these baselines are provided in Appendix A.4. Implementations. To alleviate the inﬂuence of randomness, we repeat each method ten times and report the mean performance. The hyper-parameters of baselines are tuned by OpenBox [23] or set according to the original paper if available. Please refer to Appendix A.5 for more details. /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000015 /uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048 /uni00000017/uni00000018 /uni00000017/uni00000019 /uni00000017/uni0000001a /uni00000017/uni0000001b/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000036/uni0000002a/uni00000026/uni00000014/uni0000005b /uni0000002a/uni00000026/uni00000031 /uni00000016/uni00000016/uni0000005b /uni00000024/uni00000033/uni00000033/uni00000031/uni00000033 /uni0000001a/uni0000001b/uni0000005b /uni00000031/uni00000027/uni0000002f/uni00000036 /uni00000015/uni0000005b /uni000000362/uni0000002a/uni00000026 /uni00000014/uni0000005b /uni0000002a/uni00000025/uni00000033/uni00000014/uni0000005b /uni0000002a/uni00000024/uni00000037 /uni00000016/uni0000001a/uni00000015/uni0000005b/uni00000024/uni00000033/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000014/uni00000015/uni0000005b /uni0000002d/uni0000002e/uni00000010/uni00000031/uni00000048/uni00000057 /uni00000014/uni00000014/uni00000016/uni0000005b /uni00000035/uni00000048/uni00000056/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000016/uni00000015/uni0000005b /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni00000016/uni0000005b Figure 3: Performance along with training time on the Industry dataset. Table 4: Results of inductive settings. Models Flickr Reddit GraphSAGE 50.1 ±1.3 95.4 ±0.0 FastGCN 50.4 ±0.1 93.7 ±0.0 ClusterGCN 48.1 ±0.5 95.7 ±0.0 GraphSAINT 51.1 ±0.1 96.6 ±0.1 NDLS-F+MLP∗ 51.9±0.2 96.6 ±0.1 GraphSAGE+NDLS-L∗ 51.5±0.4 96.3 ±0.0 NDLS∗ 52.6±0.4 96.8 ±0.1 6.2 Experimental Results. End-to-end comparison. To answerQ1, Table 3 and 4 show the test accuracy of considered methods in transductive and inductive settings. In the inductive setting, NDLS outperforms one of the most competitive baselines – GraphSAINT by a margin of 1.5% and 0.2% on Flickr and Reddit. NDLS exceeds the best GNN model among all considered baselines on each dataset by a margin of 0.2% to 0.8% in the transductive setting. In addition, we observe that with NDLS-L, the model performance of MLP, SGC, NDLS-F+MLP, and GraphSAGE can be further improved by a large margin. For example, the accuracy gain for MLP is 21.8%, 11.3%, 8.4%, and 5.6% on Cora, Citseer, PubMed, and Industry, respectively. To answerQ2, we replace the gate mechanism in the vanilla DAGNN with NDLS-L and refer to this method as DAGNN (NDLS-L). Surprisingly, DAGNN (NDLS-L) achieves at least comparable or (often) higher test accuracy compared with AP-GCN and DAGNN (Gate), and it shows that NDLS-L performs better than the learned mechanism in label smoothing. Furthermore, by replacing the original graph kernels with NDLS-F, NDLS-F+MLP outperforms both S2GC and GBP on all compared datasets. This demonstrates the effectiveness of the proposed NDLS. Training Efﬁciency. To answer Q3, we evaluate the efﬁciency of each method on a real-world industry graph dataset. Here, we pre-compute the smoothed features of each linear-model-based 8/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (a) Feature Sparsity /uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000028/uni00000047/uni0000004a/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (b) Edge Sparsity /uni00000014/uni00000013/uni00000015/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (c) Label Sparsity Figure 4: Test accuracy on PubMed dataset under different levels of feature, edge and label sparsity. /uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000027/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (a) LSI along with the node degree  (b) The visualization of LSI Figure 5: (Left) LSI distribution along with the node degree in three citation networks. (Right) The visualization of LSI in Zachary’s karate club network. Nodes with larger radius have larger LSIs. GNN, and the time for pre-processing is also included in the training time. Figure 3 illustrates the results on the industry dataset across training time. Compared with linear-model-based GNNs, we observe that (1) both the coupled and decoupled GNNs require a signiﬁcantly larger training time; (2) NDLS achieves the best test accuracy while consuming comparable training time with SGC. Performance on Sparse Graphs. To reply Q4, we conduct experiments to test the performance of NDLS on feature, edge, and label sparsity problems. For feature sparsity, we assume that the features of unlabeled nodes are partially missing. In this scenario, it is necessary to calculate a personalized propagation iteration to “recover” each node’s feature representation. To simulate edge sparsity settings, we randomly remove a ﬁxed percentage of edges from the original graph. Besides, we enumerate the number of nodes per class from 1 to 20 in the training set to measure the effectiveness of NDLS given different levels of label sparsity. The results in Figure 4 show that NDLS outperforms all considered baselines by a large margin across different levels of feature, edge, and label sparsity, thus demonstrating that our method is more robust to the graph sparsity problem than the linear-model-based GNNs. Interpretability. As mentioned by Q1, we here answer why NDLS is effective. One theoretical property of LSI is that the value correlates with the node degree negatively. We divide nodes into several groups, and each group consists of nodes with the same degree. And then we calculate the average LSI value for each group in the three citation networks respectively. Figure 5(a) depicts that nodes with a higher degree have a smaller LSI, which is consistent with Theorem 3.1. We also use NetworkX [13] to visualize the LSI in Zachary’s karate club network [36]. Figure 5(b), where the radius of each node corresponds to the value of LSI, shows three interesting observations: (1) nodes with a larger degree have smaller LSIs; (2) nodes in the neighbor area have similar LSIs; (3) nodes adjacent to a super-node have smaller LSIs. The ﬁrst observation is consistent with Theorem 3.1, and the latter two observations show consistency with Theorem 3.2. 7 Conclusion In this paper, we present node-dependent local smoothing (NDLS), a simple and scalable graph learning method based on the local smoothing of features and labels. NDLS theoretically analyzes 9what inﬂuences the smoothness and gives a bound to guide how to control the extent of smoothness for different nodes. By setting a node-speciﬁc smoothing iteration, each node in NDLS can smooth its feature/label to a local-smoothing state and then help to boost the model performance. Extensive experiments on seven real-world graph datasets demonstrate the high accuracy, scalability, efﬁciency, and ﬂexibility of NDLS against the state-of-the-art GNNs. Broader Impact NDLS can be employed in areas where graph modeling is the foremost choice, such as citation networks, social networks, chemical compounds, transaction graphs, road networks, etc. The effectiveness of NDLS when improving the predictive performance in those areas may bring a broad range of societal beneﬁts. For example, accurately predicting the malicious accounts on transaction networks can help identify criminal behaviors such as stealing money and money laundering. Prediction on road networks can help avoid trafﬁc overload and save people’s time. A signiﬁcant beneﬁt of NDLS is that it offers a node-dependent solution. However, NDLS faces the risk of information leakage in the smoothed features or labels. In this regard, we encourage researchers to understand the privacy concerns of NDLS and investigate how to mitigate the possible information leakage. Acknowledgments and Disclosure of Funding This work is supported by NSFC (No. 61832001, 6197200), Beijing Academy of Artiﬁcial Intelligence (BAAI), PKU-Baidu Fund 2019BD006, and PKU-Tencent Joint Research Lab. Zhi Yang and Bin Cui are the corresponding authors. References [1] A. Bojchevski, J. Klicpera, B. Perozzi, A. Kapoor, M. Blais, B. Rózemberczki, M. Lukasik, and S. Günnemann. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2464–2473, 2020. [2] L. Cai and S. Ji. A multi-scale approach for graph link prediction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3308–3315, 2020. [3] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. [4] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In ICLR, 2018. [5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 941–949, 2018. [6] M. Chen, Z. Wei, B. Ding, Y . Li, Y . Yuan, X. Du, and J. Wen. Scalable graph neural networks via bidirectional propagation. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [7] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 785–794, 2016. [8] W. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. InProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 257–266, 2019. 10[9] F. R. Chung and F. C. Graham. Spectral graph theory. Number 92. American Mathematical Soc., 1997. [10] G. Cui, J. Zhou, C. Yang, and Z. Liu. Adaptive graph encoder for attributed graph embedding. In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 976–985, 2020. [11] H. Dihe. An introduction to markov process in random environment [j]. Acta Mathematica Scientia, 5, 2010. [12] Q. Guo, X. Qiu, X. Xue, and Z. Zhang. Syntax-guided text generation via graph neural network. Sci. China Inf. Sci., 64(5), 2021. [13] A. Hagberg, P. Swart, and D. S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008. [14] W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 1024–1034, 2017. [15] X. He, K. Deng, X. Wang, Y . Li, Y . Zhang, and M. Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 639–648, 2020. [16] W. Hu, M. Fey, H. Ren, M. Nakata, Y . Dong, and J. Leskovec. OGB-LSC: A large-scale challenge for machine learning on graphs. CoRR, abs/2103.09430, 2021. [17] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 4563–4572, 2018. [18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [19] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018. [20] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. [21] K.-H. Lai, D. Zha, K. Zhou, and X. Hu. Policy-gnn: Aggregation optimization for graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 461–471, 2020. [22] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. [23] Y . Li, Y . Shen, W. Zhang, Y . Chen, H. Jiang, M. Liu, J. Jiang, J. Gao, W. Wu, Z. Yang, C. Zhang, and B. Cui. Openbox: A generalized black-box optimization service. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 3209–3219, 2021. [24] A. Liaw, M. Wiener, et al. Classiﬁcation and regression by randomforest. R news, 2(3):18–22, 2002. [25] M. Liu, H. Gao, and S. Ji. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 338–348, 2020. 11[26] X. Miao, N. M. Gürel, W. Zhang, Z. Han, B. Li, W. Min, S. X. Rao, H. Ren, Y . Shan, Y . Shao, Y . Wang, F. Wu, H. Xue, Y . Yang, Z. Zhang, Y . Zhao, S. Zhang, Y . Wang, B. Cui, and C. Zhang. Degnn: Improving graph neural networks with graph decomposition. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 1223–1233, 2021. [27] X. Miao, W. Zhang, Y . Shao, B. Cui, L. Chen, C. Zhang, and J. Jiang. Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture. IEEE Transactions on Knowledge and Data Engineering, 2021. [28] E. Rossi, F. Frasca, B. Chamberlain, D. Eynard, M. M. Bronstein, and F. Monti. SIGN: scalable inception graph neural networks. CoRR, abs/2004.11198, 2020. [29] I. Spinelli, S. Scardapane, and A. Uncini. Adaptive propagation graph convolutional network. IEEE Transactions on Neural Networks and Learning Systems, 2020. [30] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] K. Wang, Z. Shen, C. Huang, C.-H. Wu, Y . Dong, and A. Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396–413, 2020. [32] F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 6861–6871, 2019. [33] S. Wu, W. Zhang, F. Sun, and B. Cui. Graph neural networks in recommender systems: A survey. CoRR, abs/2011.02260, 2020. [34] S. Wu, Y . Zhang, C. Gao, K. Bian, and B. Cui. Garg: Anonymous recommendation of point- of-interest in mobile networks by graph convolution network. Data Science and Engineering, 5(4):433–447, 2020. [35] K. Xu, C. Li, Y . Tian, T. Sonobe, K. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 5449–5458, 2018. [36] W. W. Zachary. An information ﬂow model for conﬂict and ﬁssion in small groups. Journal of anthropological research, 33(4):452–473, 1977. [37] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [38] W. Zhang, Y . Jiang, Y . Li, Z. Sheng, Y . Shen, X. Miao, L. Wang, Z. Yang, and B. Cui. ROD: reception-aware online distillation for sparse graphs. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 2232–2242, 2021. [39] W. Zhang, X. Miao, Y . Shao, J. Jiang, L. Chen, O. Ruas, and B. Cui. Reliable data distillation on graph convolutional network. In Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020, pages 1399–1414, 2020. [40] W. Zhang, Z. Sheng, Y . Jiang, Y . Xia, J. Gao, Z. Yang, and B. Cui. Evaluating deep graph neural networks. CoRR, abs/2108.00955, 2021. [41] X. Zhang, H. Liu, Q. Li, and X. Wu. Attributed graph clustering via adaptive graph convolution. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4327–4333, 2019. [42] H. Zhu and P. Koniusz. Simple spectral graph convolution. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. 12A Appendix A.1 Proofs of Theorems We represent the adjacency matrix and the diagonal degree matrix of graphGby Aand Drespectively, represent D+I and A+I by ˜Dand ˜A. Then we denote ˜D−1 ˜Aas a transition matrix P. Suppose P is connected, which means the graph is connected, for any initial distribution π0, let ˜π(π0) = lim k→∞ π0Pk, (15) then according to [11], for any initial distribution π0 ˜π(π0)i = 1 n n∑ j=1 Pji, (16) where ˜πi denotes the ith component of ˜π(π0), and ndenotes the number of nodes in graph. If matrix P is unconnected, we can divide P into connected blocks. Then for each blocks(denoted as Bg), there always be ˜π(π0)i = 1 ng ∑ j∈Bg Pji ∗ ∑ j∈Bg π0j, (17) where ng is the number of nodes in Bg. To make the proof concise, we will assume matrix P is connected, otherwise we can perform the same operation inside each block. Therefore, ˜π is independent to π0, thus we replace ˜π(π0) by ˜π. Deﬁnition A.1 (Local Mixing Time). The local mixing time (parameterized by ϵ) with an initial distribution is deﬁned as T(π0,ϵ) = min{t: ||˜π−π0Pt||2 <ϵ}, (18) where \"||·||2\" symbols two-nor m. In order to consider the impact of each node to the others separately, letπ0 = ei, where ei is a one-hot vector with the ith component equal to 1, and the other components equal to 0. According to [ 9] we have lemma A.1. Lemma A.1. |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (19) where λ2 is the second large eigenvalue of P and ˜di denotes the degree of node vi plus 1 (to include itself). ˜di = di + 1, ˜dj = dj + 1, Theorem A.2. T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n), (20) where mand ndenote the number of edges and nodes in graph Gseparately. ˜di = di + 1, Proof. [9] shows that when π0 = ei, |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (21) where (eiPt)j symbols the jth element of eiPt. We denote eiPt as πi(t), then ||˜π−πi(t)||2 2 = n∑ j=1 (˜πj −πi(t)j)2 ≤ n∑ j=1 ˜dj ˜di λ2t 2 = 2m+ n ˜di λ2t 2 , (22) 13which means ||˜π−πi(t)||2 ≤ √ 2m+ n ˜di λt 2. (23) Now let ϵ= √ 2m+ n ˜di λt 2, there exists T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). Next consider the real situation in SGC with n×m-dimension matrix X(0) as input, where nis the number of nodes, mis the number of features. We apply P as the normalized adjacent matrix.(The deﬁnition of P is the same as ˜A in main text). In feature propagation we have X(t) =PtX(0), Now consider the hth feature of X, we deﬁne an n×ninﬂuence matrix Ihij(t) = ∂X(t)ih ∂X(0)jh , (24) Because Ih(k) is independent to h, we replace Ih(k) by I(k), which can be formulated as I(k) =Ih(k), ∀h∈{1,2,..,f }, (25) where f symbols the number of features of X. Deﬁnition A.2 (Local Smoothing Iteration). The Local Smoothing Iteration (parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −Ii(k)||2 <ϵ}. (26) According to Theorem A.2, there exists Theorem A.3 (Theorem 3.1 in main text). When the normalized adjacent matrix is P, K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). (27) Proof. From equation (9) we can derive that ||eiP∞−eiPk||2 ≤ √ 2m+ n ˜di λk 2. Because Ii(k) =Pk i = eiPk Ii(∞) =P∞ i = eiP∞, we have ||Ii(∞) −Ii(k)||2 ≤ √ 2m+ n ˜di λk 2. Now let ϵ= √ 2m+ n ˜di λk 2, there exists K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). 14Therefore, we expand Theorem A.3 to the propagation in SGC or our method. What is remarkable, Theorem A.3 requires P, which is equal to ˜D−1 ˜Aas the normalized adjacent matrix. From Theorem A.3 we can conclude that the node which has a lager degree may need more steps to propagate. At the same time, we have another bond of local mixing time as following. Theorem A.4. For each node vi in graph G, there always exits T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. (28) where N(i) is the set of node vi’s neighbours. Proof. ∥|˜π−eiPt+1||2 = 1 |N(i)| ∑ j∈N(i) ||˜π−ejPt||2 ≤ max j∈N(i) ||˜π−ejPt||2. (29) Therefore, when max j∈N(i) ||˜π−ejPt||2 ≤ϵ, there exists ||˜π−eiPt+1||2 ≤ϵ. Thus we can derive that T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. As we extend Theorem A.2 to Theorem A.3, according to Theorem A.4, there always be Theorem A.5 (Theorem 3.2 in main text). For each node vi in graph G, there always exits K(i,ϵ) ≤max{K(j,ϵ),j ∈N(i)}+ 1. (30) A.2 Results with More Base Models Our proposed NDLS consists of three stages: (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). In stage (2), the default option of the base model is a Multilayer Perceptron (MLP). Besides MLP, many other models can also be used in stage (2) to generate soft labels. To verify it, here we replace the MLP in stage (2) with popular machine learning models Random Forest [24] and XGBoost [7], and measure their node classiﬁcation performance on PubMed dataset. The experiment results are shown in Table 3 where Random Forest and XGBoost are abbreviated as RF and XGB respectively. Compared to the vanilla model, both Random Forest and XGBoost achieve signiﬁcant performance gain with the addition of our NDLS. With the help of NDLS, Random Forest and XGBoost outperforms their base models by 6.1% and 7.5% respectively. From Table 3, we can observe that both NDLS-F and NDLS-L can contribute great performance boost to the base model, where the gains are at least 5%. When all equipped with both NDLS-F and NDLS-L, XGBoost beat the default MLP, achieving a test accuracy of81.6%. Although Random Forest – 80.5% – cannot outperform the other two models, it is still a competitive model. The above experiment demonstrates that the base model selection in stage (2) is rather ﬂexible in our NDLS. Both traditional machine learning methods and neural networks are promising candidates in the proposed method. A.3 Dataset Description Cora, Citeseer, and Pubmed1 are three popular citation network datasets, and we follow the public training/validation/test split in GCN [18]. In these three networks, papers from different topics are 1https://github.com/tkipf/gcn/tree/master/gcn/data 15Table 5: Results of different base models on PubMed. Base Models Models Accuracy Gain MLP Base 72.7 ±0.6 - + NDLS-F 81.1 ±0.6 + 8.4 + NDLS-L 81.1 ±0.6 + 8.4 + NDLS (both) 81.4 ±0.4 + 8.7 RF Base 74.4 ±0.2 - + NDLS-F 80.3 ±0.1 + 5.9 + NDLS-L 80.0 ±0.2 + 5.6 + NDLS (both) 80.5 ±0.4 + 6.1 XGB Base 74.1 ±0.2 - + NDLS-F 81.0 ±0.3 + 6.9 + NDLS-L 79.8 ±0.2 + 5.7 + NDLS (both) 81.6 ±0.3 + 7.5 considered as nodes, and the edges are citations among the papers. The node attributes are binary word vectors, and class labels are the topics papers belong to. Reddit is a social network dataset derived from the community structure of numerous Reddit posts. It is a well-known inductive training dataset, and the training/validation/test split in our experiment is the same as the one in GraphSAGE [14]. Flickr originates from NUS-wide 2 and contains different types of images based on the descriptions and common properties of online images. The public version of Reddit and Flickr provided by GraphSAINT3 is used in our paper. Industry is a short-form video graph, collected from a real-world mobile application from our industrial cooperative enterprise. We sampled 1,000,000 users and videos from the app, and treat these items as nodes. The edges in the generated bipartite graph represent that the user clicks the short-form videos. Each user has 64 features and the target is to category these short-form videos into 253 different classes. ogbn-papers100M is a directed citation graph of 111 million papers indexed by MAG [31]. Among its node set, approximately 1.5 million of them are arXiv papers, each of which is manually labeled with one of arXiv’s subject areas. Currently, this dataset is much larger than any existing public node classiﬁcation datasets. A.4 Compared Baselines The main characteristic of all baselines are listed below: • GCN [18]: GCN is a novel and efﬁcient method for semi-supervised classiﬁcation on graph-structured data. • GAT [30]: GAT leverages masked self-attention layers to specify different weights to different nodes in a neighborhood, thus better represent graph information. • JK-Net [35]: JK-Net is a ﬂexible network embedding method that could gather different neighborhood ranges to enable better structure-aware representation. • APPNP [19]: APPNP uses the relationship between graph convolution networks (GCN) and PageRank to derive improved node representations. • AP-GCN [29]: AP-GCN uses a halting unit to decide a receptive range of a given node. 2http://lms.comp.nus.edu.sg/research/NUS-WIDE.html 3https://github.com/GraphSAINT/GraphSAINT 16Table 6: URLs of baseline codes. Type Baselines URLs Coupled GCN https://github.com/rusty1s/pytorch_geometric GAT https://github.com/rusty1s/pytorch_geometric Decoupled APPNP https://github.com/rusty1s/pytorch_geometric PPRGo https://github.com/TUM-DAML/pprgo_pytorch AP-GCN https://github.com/spindro/AP-GCN DAGNN https://github.com/divelab/DeeperGNN Sampling GraphSAGE https://github.com/williamleif/GraphSAGE GraphSAINT https://github.com/GraphSAINT/GraphSAINT FastGCN https://github.com/matenure/FastGCN Cluster-GCN https://github.com/benedekrozemberczki/ClusterGCN Linear SGC https://github.com/Tiiiger/SGC SIGN https://github.com/twitter-research/sign S2GC https://github.com/allenhaozhu/SSGC GBP https://github.com/chennnM/GBP NDLS https://github.com/zwt233/NDLS • DAGNN [25]: DAGNN proposes to decouple the representation transformation and propagation, and show that deep graph neural networks without this entanglement can leverage large receptive ﬁelds without suffering from performance deterioration. • PPRGo [1]: utilizes an efﬁcient approximation of information diffusion in GNNs resulting in signiﬁcant speed gains while maintaining state-of-the-art prediction performance. • GraphSAGE [14]: GraphSAGE is an inductive framework that leverages node attribute information to efﬁciently generate representations on previously unseen data. • FastGCN [4]: FastGCN interprets graph convolutions as integral transforms of embedding functions under probability measures. • Cluster-GCN [8]: Cluster-GCN is a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. • GraphSAINT [37]: GraphSAINT constructs mini-batches by sampling the training graph, rather than the nodes or edges across GCN layers. • SGC [32]: SGC simpliﬁes GCN by removing nonlinearities and collapsing weight matrices between consecutive layers. • SIGN [28]: SIGN is an efﬁcient and scalable graph embedding method that sidesteps graph sampling in GCN and uses different local graph operators to support different tasks. • S2GC [42]: S2GC uses a modiﬁed Markov Diffusion Kernel to derive a variant of GCN, and it can be used as a trade-off of low-pass and high-pass ﬁlter which captures the global and local contexts of each node. • GBP [6]: GBP utilizes a localized bidirectional propagation process from both the feature vectors and the training/testing nodes Table 6 summarizes the github URLs of the compared baselines. Following the original paper, we implement JK-Net by ourself since there is no ofﬁcial version available. A.5 Implementation Details Hyperparameter details. In stage (1), when computing the Local Smoothing Iteration, the maximal value of k in equation (12) is set to 200 and the optimal ϵ value is get by means of a grid search from {0.01, 0.03, 0.05}. In stage (2), we use a simple two-layer MLP to get the base prediction. The hidden size is set to 64 in small datasets – Cora, Citeseer and Pubmed. While in larger datasets – Flicker, Reddit, Industry and ogbn-papers100M, the hidden size is set to 256. As for the dropout percentage and the learning rate, we use a grid search from {0.2, 0.4, 0.6, 0.8} and {0.1, 17Table 7: Performance comparison between C&S and NDLS-L Methods Cora Citeseer PubMed ogbn-papers100M MLP+C&S 87.2 76.6 88.3 63.9 MLP+NDLS-L 88.1 78.3 88.5 64.6 Table 8: Performance comparison under varied label rate on the Cora dataset. Methods 2% 5% 10% 20% 40% 60% MLP+S 63.1 77.8 82.6 84.2 85.4 86.4 MLP+C&S 62.8 76.7 82.8 84.9 86.4 87.2 MLP+NDLS-L 77.4 83.9 85.3 86.5 87.6 88.1 Table 9: Performance comparison after combining the node-dependent idea with C&S. Methods Cora Citeseer PubMed MLP+C&S 76.7 70.8 76.5 MLP+C&S+nd 79.9 71.1 78.4 0.01, 0.001} respectively. In stage (3), during the computation of the Local Smoothing Iteration, the maximal value of kis set to 40. The optimal value of ϵis obtained through the same process in stage (1). Implementation environment. The experiments are conducted on a machine with Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz, and a single NVIDIA TITAN RTX GPU with 24GB memory. The operating system of the machine is Ubuntu 16.04. As for software versions, we use Python 3.6, Pytorch 1.7.1 and CUDA 10.1. A.6 Comparison and Combination with Correct&Smooth Similar to our NDLS-L, Correct and Smooth (C&S) also applies post-processing on the model prediction. Therefore, we compare NDLS-L with C&S below. Adaptivity to node. C&S adopts a propagation scheme based on Personalized PageRank (PPR), which always maintains certain input information to slow down the occurrence of over-smoothing. The expected number of smoothing iterations is controlled by the restart probability, which is a constant for all nodes. Therefore, C&S still falls into the routine of ﬁxed smoothing iteration. Instead, NDLS-L employs node-speciﬁc smoothing iterations. We compare each method’s performance (test accuracy, %) under the same data split as in the C&S paper (60%/20%/20% on three citation networks, ofﬁcial split on ogbn-papers100M), and the experimental results in Table 7 show that NDLS-L outperforms C&S in different datasets. Sensitivity to label rate. During the “Correct” stage, C&S propagates uncertainties from the training data across the graph to correct the base predictions. However, the uncertainties might not be accurate when the number of training nodes is relatively small, thus even degrading the performance. To conﬁrm the above assumption, we conduct experiments on the Cora dataset under different label rates, and the experimental results are provided in Table 8. As illustrated, the result of C&S drops much faster than NDLS-L’s when the label rate decreases. What’s more, MLP+S (removing the “Correct” stage) outperforms MLP+C&S when the label rate is low as expected. Compared with C&S, NDLS is more general in terms of smoothing types. C&S can only smooth label predictions. Instead, NDLS can smooth both node features and label predictions and combine them to boost the model performance further. 18Table 10: Efﬁciency comparison on the PubMed dataset. SGC S2GC GBP NDLS SIGN JK-Net DAGNN GCN ResGCN APPNP GAT Time 1.00 1.19 1.20 1.50 1.59 11.42 14.39 20.43 20.49 28.88 33.23 Accuracy 78.9 79.9 80.6 81.4 79.5 78.8 80.5 79.3 78.6 80.1 79.0 Table 11: Performance comparison on the ogbn-arxiv dataset. MLP MLP+C&S GCN SGC SIGN DAGNN JK-Net S2GC GBP NDLS GAT Accuracy55.50 71.58 71.74 71.72 71.95 72.09 72.19 72.21 72.45 73.04 73.56 Node Adaptive C&S. The node-dependent mechanism in our NDLS can easily be combined with C&S. The two stages of C&S both contain a smoothing process using the personalized PageRank matrix, where a coefﬁcient controls the remaining percentage of the original node feature. Here, we can precompute the smoothed node features after the same smoothing step yet under different values like 0.1, 0.2, ..., 0.9. After that, we adopt the same strategy in our NDLS: for each node, we choose the ﬁrst in the ascending order that the distance from the smoothed node feature to the stationarity is less than a tuned hyperparameter. By this means, the smoothing process in C&S can be carried out in a node-dependent way. We also evaluate the performance of C&S combined with the node-dependent idea (represented as C&S+nd) on the three citation networks under ofﬁcial splits, and the experimental results in Table 9 show that C&S combined with NDLS consistently outperforms the original version of C&S. A.7 Training Efﬁciency Study we measure the training efﬁciency of the compared baselines on the widely used PubMed dataset. Using the training time of SGC as the baseline, the relative training time and the corresponding test accuracy of NDLS and the baseline methods are shown in Table 10. Compared with other baselines, NDLS can get the highest test accuracy while maintaining competitive training efﬁciency. A.8 Experiments on ogbn-arxiv We also conduct experiments on the ogbn-arxiv dataset. The experiment results (test accuracy, %) are provided in Table 11. Although GAT outperforms NDLS on ogbn-arxiv dataset, it is hard to scale to large graphs like ogbn-papers100M dataset. Note that MLP+C&S on the OGB leaderboard makes use of not only the original node feature but also diffusion embeddings and spectral embeddings. Here we remove the latter two embeddings for fairness, and the authentic MLP+C&S achieves 71.58% on the ogbn-arxiv dataset. 19",
      "meta_data": {
        "arxiv_id": "2110.14377v1",
        "authors": [
          "Wentao Zhang",
          "Mingyu Yang",
          "Zeang Sheng",
          "Yang Li",
          "Wen Ouyang",
          "Yangyu Tao",
          "Zhi Yang",
          "Bin Cui"
        ],
        "published_date": "2021-10-27T12:24:41Z",
        "venue": "NeurIPS 2021",
        "pdf_url": "https://arxiv.org/pdf/2110.14377v1.pdf"
      }
    },
    {
      "title": "Node Dependent Local Smoothing for Scalable Graph Learning",
      "abstract": "Recent works reveal that feature or label smoothing lies at the core of Graph\nNeural Networks (GNNs). Concretely, they show feature smoothing combined with\nsimple linear regression achieves comparable performance with the carefully\ndesigned GNNs, and a simple MLP model with label smoothing of its prediction\ncan outperform the vanilla GCN. Though an interesting finding, smoothing has\nnot been well understood, especially regarding how to control the extent of\nsmoothness. Intuitively, too small or too large smoothing iterations may cause\nunder-smoothing or over-smoothing and can lead to sub-optimal performance.\nMoreover, the extent of smoothness is node-specific, depending on its degree\nand local structure. To this end, we propose a novel algorithm called\nnode-dependent local smoothing (NDLS), which aims to control the smoothness of\nevery node by setting a node-specific smoothing iteration. Specifically, NDLS\ncomputes influence scores based on the adjacency matrix and selects the\niteration number by setting a threshold on the scores. Once selected, the\niteration number can be applied to both feature smoothing and label smoothing.\nExperimental results demonstrate that NDLS enjoys high accuracy --\nstate-of-the-art performance on node classifications tasks, flexibility -- can\nbe incorporated with any models, scalability and efficiency -- can support\nlarge scale graphs with fast training.",
      "full_text": "Node Dependent Local Smoothing for Scalable Graph Learning Wentao Zhang1, Mingyu Yang1, Zeang Sheng1, Yang Li1 Wen Ouyang2, Yangyu Tao2, Zhi Yang1,3, Bin Cui1,3,4 1School of CS, Peking University 2Tencent Inc. 3 Key Lab of High Conﬁdence Software Technologies, Peking University 4Institute of Computational Social Science, Peking University (Qingdao), China 1{wentao.zhang, ymyu, shengzeang18, liyang.cs, yangzhi, bin.cui}@pku.edu.cn 2{gdpouyang, brucetao}@tencent.com Abstract Recent works reveal that feature or label smoothing lies at the core of Graph Neural Networks (GNNs). Concretely, they show feature smoothing combined with simple linear regression achieves comparable performance with the carefully designed GNNs, and a simple MLP model with label smoothing of its prediction can outperform the vanilla GCN. Though an interesting ﬁnding, smoothing has not been well understood, especially regarding how to control the extent of smoothness. Intuitively, too small or too large smoothing iterations may cause under-smoothing or over-smoothing and can lead to sub-optimal performance. Moreover, the extent of smoothness is node-speciﬁc, depending on its degree and local structure. To this end, we propose a novel algorithm called node-dependent local smoothing (NDLS), which aims to control the smoothness of every node by setting a node- speciﬁc smoothing iteration. Speciﬁcally, NDLS computes inﬂuence scores based on the adjacency matrix and selects the iteration number by setting a threshold on the scores. Once selected, the iteration number can be applied to both feature smoothing and label smoothing. Experimental results demonstrate that NDLS enjoys high accuracy – state-of-the-art performance on node classiﬁcations tasks, ﬂexibility – can be incorporated with any models, scalability and efﬁciency – can support large scale graphs with fast training. 1 Introduction In recent years, Graph Neural Networks (GNNs) have received a surge of interest with the state-of-the- art performance on many graph-based tasks [2, 41, 12, 39, 33, 34]. Recent works have found that the success of GNNs can be mainly attributed to smoothing, either at feature or label level. For example, SGC [32] shows using smoothed features as input to a simple linear regression model achieves comparable performance with lots of carefully designed and complex GNNs. At the smoothing stage, features of neighbor nodes are aggregated and combined with the current node’s feature to form smoothed features. This process is often iterated multiple times. The smoothing is based on the assumption that labels of nodes that are close to each other are highly correlated, therefore, the features of nodes nearby should help predict the current node’s label. One crucial and interesting parameter of neighborhood feature aggregation is the number of smoothing iterations k, which controls how much information is being gathered. Intuitively, an aggregation process of kiterations (or layers) enables a node to leverage information from nodes that are k-hop away [26, 38]. The choice of k is closely related to the structural properties of graphs and has a 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2110.14377v1  [cs.LG]  27 Oct 2021(a) Two nodes with different local structures /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000026/uni00000027/uni00000029  /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (b) The CDF of LSI in different graphs Figure 1: (Left) The node in dense region has larger smoothed area within two iterations of propagation. (Right) The CDF of LSI in three citation networks. signiﬁcant impact on the model performance. However, most existing GNNs only consider the ﬁxed-length propagation paradigm – a uniform kfor all the nodes. This is problematic since the number of iterations should be node dependent based on its degree and local structures. For example, as shown in Figure 1(a), the two nodes have rather different local structures, with the left red one resides in the center of a dense cluster and the right red one on the periphery with few connections. The number of iterations to reach an optimal level of smoothness are rather different for the two nodes. Ideally, poorly connected nodes (e.g., the red node on the right) needs large iteration numbers to efﬁciently gather information from other nodes while well-connected nodes (e.g., the red node on the left) should keep the iteration number small to avoid over-smoothing. Though some learning-based approaches have proposed to adaptively aggregate information for each node through gate/attention mechanism or reinforcement learning [ 29, 21, 40, 27], the performance gains are at the cost of increased training complexity, hence not suitable for scalable graph learning. In this paper, we propose a simple yet effective solution to this problem. Our approach, called node-dependent local smoothing (NDLS), calculates a node-speciﬁc iteration number for each node, referred to as local smooth iteration (LSI). Once the LSI for a speciﬁc node is computed, the corresponding local smoothing algorithm only aggregates the information from the nodes within a distance less than its LSI as the new feature. The LSI is selected based on inﬂuence scores, which measure how other nodes inﬂuence the current node. NDLS sets the LSI for a speciﬁc node to be the minimum number of iterations so that the inﬂuence score is ϵ-away from the over-smoothing score, deﬁned as the inﬂuence score at inﬁnite iteration. The insight is that each node’s inﬂuence score should be at a reasonable level. Since the nodes with different local structures have different “smoothing speed”, we expect the iteration number to be adaptive. Figure 1(b) illustrates Cumulative Distribution Function (CDF) for the LSI of individual nodes in real-world graphs. The heterogeneous and long-tail property exists in all the datasets, which resembles the characteristics of the degree distribution of nodes in real graphs. Based on NDLS, we propose a new graph learning algorithm with three stages: (1) feature smoothing with NDLS (NDLS-F); (2) model training with smoothed features; (3) label smoothing with NDLS (NDLS-L). Note that in our framework, the graph structure information is only used in pre-processing and post-processing steps, i.e., stages (1) and (3) (See Figure 2). Our NDLS turns a graph learning problem into a vanilla machine learning problem with independent samples. This simplicity enables us to train models on larger-scale graphs. Moreover, our NDLS kernel can act as a drop-in replacement for any other graph kernels and be combined with existing models such as Multilayer Perceptron (MLP), SGC [32], SIGN [28], S2GC [42] and GBP [6]. Extensive evaluations on seven benchmark datasets, including large-scale datasets like ogbn- papers100M [16], demonstrates that NDLS achieves not only the state-of-the-art node classiﬁcation performance but also high training scalability and efﬁciency. Especially, NDLS outperforms APPNP [29] and GAT [30] by a margin of 1.0%-1.9% and 0.9%-2.4% in terms of test accuracy, while achieving up to 39×and 186×training speedups, respectively. 2 Preliminaries In this section, we ﬁrst introduce the semi-supervised node classiﬁcation task and review the prior models, based on which we derive our method in Section 3. Consider a graphG= (V, E) with |V|= n 2nodes and |E|= medges, the adjacency matrix (including self loops) is denoted as ˜A ∈Rn×n and the feature matrix is denoted as X = {x1,x2...,xn}in which xi ∈Rf represents the feature vector of node vi. Besides, Y = {y1,y2...,yl}is the initial label matrix consisting of one-hot label indicator vectors. The goal is to predict the labels for nodes in the unlabeled set Vu with the supervision of labeled set Vl. GCN smooths the representation of each node via aggregating its own representations and the ones of its neighbors’. This process can be deﬁned as X(k+1) = δ ( ˆAX(k)W(k) ) , ˆA = ˜Dr−1 ˜A˜D−r, (1) where ˆA is the normalized adjacency matrix, r∈[0,1] is the convolution coefﬁcient, and ˜D is the diagonal node degree matrix with self loops. Here X(k) and X(k+1) are the smoothed node features of layer k and k+ 1respectively while X(0) is set to X, the original feature matrix. In addition, W(k) is a layer-speciﬁc trainable weight matrix at layer k, and δ(·) is the activation function. By setting r= 0.5, 1 and 0, the convolution matrix ˜Dr−1 ˜A˜D−r represents the symmetric normalization adjacency matrix ˜D−1/2 ˜A˜D−1/2 [20], the transition probability matrix ˜A˜D−1 [37], and the reverse transition probability matrix ˜D−1 ˜A [35], respectively. SGC. For each GCN layer deﬁned in Eq. 1, if the non-linear activation function δ(·) is an identity function and W(k) is an identity matrix, we get the smoothed feature after k-iterations propagation as X(k) = ˆAkX. Recent studies have observed that GNNs primarily derive their beneﬁts from performing feature smoothing over graph neighborhoods rather than learning non-linear hierarchies of features as implied by the analogy to CNNs [ 25, 10, 15]. By hypothesizing that the non-linear transformations between GCN layers are not critical, SGC [32] ﬁrst extracts the smoothed features X(k) then feeds them to a linear model, leading to higher scalability and efﬁciency. Following the design principle of SGC, piles of works have been proposed to further improve the performance of SGC while maintaining high scalability and efﬁciency, such as SIGN [28], S2GC [42] and GBP [6]. Over-Smoothing [22] issue. By continually smoothing the node feature with inﬁnite number of propagation in SGC, the ﬁnal smoothed feature X(∞) is X(∞) = ˆA∞X, ˆA∞ i,j = (di + 1)r(dj + 1)1−r 2m+ n , (2) where ˆA∞is the ﬁnal smoothed adjacency matrix, ˆA∞ i,j is the weight between nodes vi and vj, di and dj are the node degrees for vi and vj, respectively. Eq. (2) shows that as we smooth the node feature with an inﬁnite number of propagations in SGC, the ﬁnal feature is over-smoothed and unable to capture the full graph structure information since it only relates with the node degrees of target nodes and source nodes. For example, if we set r= 0or 1, all nodes will have the same smoothed features because only the degrees of the source or target nodes have been considered. 3 Local Smoothing Iteration (LSI) The features after k iterations of smoothing is X(k) = ˆAkX. Inspired by [ 35], we measure the inﬂuence of node vj on node vi by measuring how much a change in the input feature of vj affects the representation of vi after kiterations. For any node vi, the inﬂuence vector captures the inﬂuences of all other nodes. Considering the hth feature of X, we deﬁne an inﬂuence matrix Ih(k): Ih(k)ij = ∂ˆX(k) ih ∂ˆX(0) jh . (3) I(k) = ˆAk,˜Ii = ˆA∞ (4) Since Ih(k) is independent to h, we replace Ih(k) with I(k), which can be further represented as I(k) = Ih(k), ∀h ∈{1,2,..,f }, where f indicates the number of features of X. We denote I(k)i as the ith row of I(k), and ˜I as I(∞). Given the normalized adjacency matrix ˆA, we can 3have I(k) = ˆAk and ˜I = ˆA∞. According to Eq. (2), ˜I converges to a unique stationary matrix independent of the distance between nodes, resulting in that the aggregated features of nodes are merely relative with their degrees (i.e., over-smoothing). We denote I(k)i as the ith row of I(k), and it means the inﬂuence from the other nodes to the node vi after k iterations of propagation. We introduce a new concept local smoothing iteration (parameterized by ϵ), which measures the minimal number of iterations krequired for the inﬂuence of other nodes on node vi to be within an ϵ-distance to the over-smoothing stationarity ˜Ii. Deﬁnition 3.1. Local-Smoothing Iteration (LSI, parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −I(k)i||2 <ϵ}, (5) where ||·||2 is two-norm, and ϵis an arbitrary small constant with ϵ> 0. Here ϵis a graph-speciﬁc parameter, and a smaller ϵindicates a stronger smoothing effect. The ϵ-distance to the over-smoothing stationarity ˜Ii ensures that the smooth effect on node vi is sufﬁcient and bounded to avoid over-smoothing. As shown in Figure 1(b), we can have that the distribution of LSI owns the heterogeneous and long-tail property, where a large percentage of nodes have much smaller LSI than the rest. Therefore, the required LSI to approach the stationarity is heterogeneous across nodes. Now we discuss the connection between LSI and node local structure, showcasing nodes in the sparse region (e.g., both the degrees of itself and its neighborhood are low) can greatly prolong the iteration to approach over-smoothing stationarity. This heterogeneity property is not fully utilized in the design of current GNNs, leaving the model design in a dilemma between unnecessary iterations for a majority of nodes and insufﬁcient iterations for the rest of nodes. Hence, by adaptively choosing the iteration based on LSI for different nodes, we can signiﬁcantly improve model performance. Theoretical Properties of LSI. We now analyze the factors determining the LSI of a speciﬁc node. To facilitate the analysis, we set the coefﬁcient r = 0for the normalized adjacency matrix ˆA in Eq. (1), thus ˆA = ˜D−1 ˜A. The proofs of following theorems can be found in Appendix A.1. Theorem 3.1. Given feature smoothing X(k) = ˆAkX with ˆA = ˜D−1 ˜A, we have K(i,ϵ) ≤logλ2  ϵ √ ˜di 2m+ n  , (6) where λ2 is the second largest eigenvalue of ˆA, ˜di denotes the degree of node vi plus 1 (i.e., ˜di = di + 1), and m, ndenote the number of edges and nodes respectively. Note that λ2 ≤1. Theorem 3.1 shows that the upper-bound of the LSI is positively correlated with the scale of the graph (m,n), the sparsity of the graph (small λ2 means strong connection and low sparsity, and vice versa), and negatively correlated with the degree of nodevi. Theorem 3.2. For any nodes iin a graph G, K(i,ϵ) ≤max {K(j,ϵ),j ∈N(i)}+ 1, (7) where N(i) is the set of node vi’s neighbours. Theorem 3.2 indicates that the difference between two neighboring nodes’ LSIs is no more than1, therefore the nodes with a super-node as neighbors (or neighbor’s neighbors) may have small LSIs. That is to say, the sparsity of the local area, where a node locates, also affects its LSI positively. Considering Theorems 3.1 and 3.2 together, we can have a union upper-bound of K(i,ϵ) as K(i,ϵ) ≤min   max {K(j,ϵ),j ∈N(i)}+ 1,logλ2  ϵ √ ˜di 2m+ n     . (8) 4 NDLS Pipeline The basic idea of NDLS is to utilize the LSI heterogeneity to perform a node-dependent aggregation over a neighborhood within a distance less than the speciﬁc LSI for each node. Further, we propose 4Adjacent Matrix A Final Prediction �Y ⋯ MLP Soft label �Y Stage 1: preprocessing Stage 2: training Stage 3: postprocessing  Feature X �X Smooth Feature with NDLS-F (replaceable) Adjacent Matrix A B A B1.0 B 0.3 0.3 0.7 … LSI Estimation A A’s feature smoothed  under optimal steps … 1.0 A0.8 0.3 0.2 0.2 0.1 … … LSI(A) = 4 LSI(B) = 2 0.1 0.3 … B’s feature smoothed  under optimal steps Details about NDLS-F B A Smooth Label with NDLS-L �XB�XA Figure 2: Overview of the proposed NDLS method, including (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). NDLS-F and NDLS-L correspond to pre-processing and post-processing steps respectively. a simple pipeline with three main parts (See Figure 2): (1) a node-dependent local smoothing of the feature (NDLS-F) over the graph, (2) a base prediction result with the smoothed feature, (3) a node-dependent local smoothing of the label predictions (NDLS-L) over the graph. Note this pipeline is not trained in an end-to-end way, the stages (1) and (3) in NDLS are only the pre-processing and post-processing steps, respectively. Furthermore, the graph structure is only used in the pre/post- processing NDLS steps, not for the base predictions. Compared with prior GNN models, this key design enables higher scalability and a faster training process. Based on the graph structure, we ﬁrst compute the node-dependent local smoothing iteration that maintains a proper distance to the over-smoothing stationarity. Then the corresponding local smoothing kernel only aggregates the information (feature or prediction) for each node from the nodes within a distance less than its LSI value. The combination of NDLS-F and NDLS-L takes advantage of both label smoothing (which tends to perform fairly well on its own without node features) and the node feature smoothing. We will see that combining these complementary signals yields state-of-the- art predictive accuracy. Moreover, our NDLS-F kernel can act as a drop-in replacement for graph kernels in other scalable GNNs such as SGC, S2GC, GBP, etc. 4.1 Smooth Features with NDLS-F Once the node-dependent LSI K(i,ϵ) for a speciﬁc node iis obtained, we smooth the initial input feature Xi of node iwith node-dependent LSI as: ˜Xi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 X(k) i . (9) To capture sufﬁcient neighborhood information, for each node vi, we average its multi-scale features {X(k) i |k≤K(i,ϵ)}obtained by aggregating information within khops from the node vi. The matrix form of the above equation can be formulated as ˜X(ϵ) = max i K(i,ϵ) ∑ k=0 M(k)X(k), M(k) ij = { 1 K(i,ϵ)+1 , i = j and k ≤K(i,ϵ) 0, otherwise , (10) where M(k) is a set of diagonal matrix. 4.2 Simple Base Prediction With the smoothed feature ˜X according to Eq. 9, we then train a model to minimize the loss –∑ vi∈Vl ℓ ( yi,f( ˜Xi) ) , where ˜Xi denotes the ith row of ˜X, ℓis the cross-entropy loss function, and f( ˜Xi) is the predictive label distribution for node vi. In NDLS, the default f is a MLP model and 5ˆY = f( ˜X) is its soft label predicted (softmax output). Note that, many other models such as Random Forest [24] and XGBoost [7] could also be used in NDLS (See more results in Appendix A.2). 4.3 Smooth Labels with NDLS-L Similar to the feature propagation, we can also propagate the soft label ˆY with ˆY(k) = ˆAk ˆY. Considering the inﬂuence matrix of softmax label Jh(k). Jh(k)ij = ∂ˆY(k) ih ∂ˆY(0) jh . (11) According to the deﬁnition above we have that Jh(k) =Ih(k),∀h∈{1,2,..,f }. (12) Therefore, local smoothing can be further applied to address over-smoothing in label propagation. Concretely, we smooth an initial soft label ˆYi of node vi with NDLS as follows ˜Yi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 ˆY(k) i . (13) Similarly, the matrix form of the above equation can be formulated as ˜Y(ϵ) = max i K(i,ϵ) ∑ k=0 M(k) ˆY(k), (14) where M(k) follows the deﬁnition in Eq. (10). 5 Comparison with Existing Methods Decoupled GNNs. The aggregation and transformation operations in coupled GNNs (i.e., GCN [18], GAT [30] and JK-Net [ 35]) are inherently intertwined in Eq. (1), so the propagation iterations L always equals to the transformation iterations K. Recently, some decoupled GNNs (e.g., PPNP [20], PPRGo [1], APPNP [20], AP-GCN [29] and DAGNN [25]) argue the entanglement of these two operations limits the propagation depth and representation ability of GNNs, so they ﬁrst do the transformation and then smooth and propagate the predictive soft label with higher depth in an end-to-end manner. Especially, AP-GCN and DAGNN both use a learning mechanism to learn propagation adaptively. Unfortunately, all these coupled and decoupled GNNs are hard to scale to large graphs – scalability issue since they need to repeatedly perform an expensive recursive neighborhood expansion in multiple propagations of the features or soft label predicted. NDLS addresses this issue by dividing the training process into multiple stages. Sampling-based GNNs. An intuitive method to tackle the recursive neighborhood expansion problem is sampling. As a node-wise sampling method, GraphSAGE [14] samples the target nodes as a mini-batch and samples a ﬁxed size set of neighbors for computing. VR-GCN [5] analyzes the variance reduction on node-wise sampling, and it can reduce the size of samples with an additional memory cost. In the layer level, Fast-GCN [ 3] samples a ﬁxed number of nodes at each layer, and ASGCN [17] proposes the adaptive layer-wise sampling with better variance control. For the graph-wise sampling, Cluster-GCN [8] clusters the nodes and only samples the nodes in the clusters, and GraphSAINT [37] directly samples a subgraph for mini-batch training. We don’t use sampling in NDLS since the sampling quality highly inﬂuences the classiﬁcation performance. Linear Models. Following SGC [32], some recent methods remove the non-linearity between each layer in the forward propagation. SIGN [28] allows using different local graph operators and proposes to concatenate the different iterations of propagated features. S2GC [42] proposes the simple spectral graph convolution to average the propagated features in different iterations. In addition, GBP [ 6] further improves the combination process by weighted averaging, and all nodes in the same layer share the same weight. In this way, GBP considers the smoothness in a layer perspective way. Similar 6Table 1: Algorithm analysis for existing scalable GNNs. n, m, c, and f are the number of nodes, edges, classes, and feature dimensions, respectively. bis the batch size, and krefers to the number of sampled nodes. Lcorresponds to the number of times we aggregate features, K is the number of layers in MLP classiﬁers. For the coupled GNNs, we always have K = L. Type Method Preprocessing and postprocessingTraining Inference Memory Node-wise samplingGraphSAGE - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Layer-wise samplingFastGCN - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Graph-wise samplingCluster-GCN O(m) O(Lmf+Lnf2) O(Lmf+Lnf2) O(bLf+Lf2) Linear model SGC O(Lmf) O(nf2) O(nf2) O(bf+f2)S2GC O(Lmf) O(nf2) O(nf2) O(bf+f2)SIGN O(Lmf) O(Knf2) O(Knf2) O(bLf+Kf2) GBP O(Lnf+L √mlgnε ) O(Knf2) O(Knf2) O(bf+Kf2)Linear model NDLS O(Lmf+Lmc) O(Knf2) O(Knf2) O(bf+Kf2) Table 2: Overview of datasets and task types (T/I represents Transductive/Inductive). Dataset #Nodes #Features #Edges #Classes #Train/Val/Test Type Description Cora 2,708 1,433 5,429 7 140/500/1,000 T citation network Citeseer 3,327 3,703 4,732 6 120/500/1,000 T citation network Pubmed 19,717 500 44,338 3 60/500/1,000 T citation network Industry 1,000,000 64 1,434,382 253 5K/10K/30K T short-form video network ogbn-papers100M 111,059,956 128 1,615,685,872 172 1,207K/125K/214K T citation network Flickr 89,250 500 899,756 7 44K/22K/22K I image network Reddit 232,965 602 11,606,919 41 155K/23K/54K I social network to these works, we also use a linear model for higher training scalability. The difference lies in that we consider the smoothness from a node-dependent perspective and each node in NDLS has a personalized aggregation iteration with the proposed local smoothing mechanism. Table 1 compares the asymptotic complexity of NDLS with several representative and scalable GNNs. In the stage of the preprocessing, the time cost of clustering in Cluster-GCN is O(m) and the time complexity of most linear models is O(Lmf). Besides, NDLS has an extra time cost O(Lmc) for the postprocessing in label smoothing. GBP conducts this process approximately with a bound of O(Lnf + L √mlg n ε ), where εis a error threshold. Compared with the sampling-based GNNs, the linear models usually have smaller training and inference complexity, i.e., higher efﬁciency. Memory complexity is a crucial factor in large-scale graph learning because it is difﬁcult for memory-intensive algorithms such as GCN and GAT to train large graphs on a single machine. Compared with SIGN, both GBP and NDLS do not need to store smoothed features in different iterations, and the feature storage complexity can be reduced from O(bLf) to O(bf). 6 Experiments In this section, we verify the effectiveness of NDLS on seven real-world graph datasets. We aim to answer the following four questions. Q1: Compared with current SOTA GNNs, can NDLS achieve higher predictive accuracy and why? Q2: Are NDLS-F and NDLS-L better than the current feature and label smoothing mechanisms (e.g., the weighted feature smoothing in GBP and the adaptive label smoothing in DAGNN)? Q3: Can NDLS obtain higher efﬁciency over the considered GNN models? Q4: How does NDLS perform on sparse graphs (i.e., low label/edge rate, missing features)? 6.1 Experimental Setup Datasets. We conduct the experiments on (1) six publicly partitioned datasets, including four citation networks (Citeseer, Cora, PubMed, and ogbn-papers100M) in [ 18, 16] and two social networks (Flickr and Reddit) in [37], and (2) one short-form video recommendation graph (Industry) from our industrial cooperative enterprise. The dataset statistics are shown in Table 2 and more details about these datasets can be found in Appendix A.3. Baselines. In the transductive setting, we compare our method with (1) the coupled GNNs: GCN [18], GAT [ 30] and JK-Net [ 35]; (2) the decoupled GNNs: APPNP [ 20], AP-GCN [ 29], 7Table 3: Results of transductive settings. OOM means “out of memory”. Type Models Cora Citeseer PubMed Industry ogbn-papers100M Coupled GCN 81.8 ±0.5 70.8 ±0.5 79.3 ±0.7 45.9 ±0.4 OOM GAT 83.0 ±0.7 72.5 ±0.7 79.0 ±0.3 46.8 ±0.7 OOM JK-Net 81.8 ±0.5 70.7 ±0.7 78.8 ±0.7 47.2 ±0.3 OOM Decoupled APPNP 83.3 ±0.5 71.8 ±0.5 80.1 ±0.2 46.7 ±0.6 OOM AP-GCN 83.4 ±0.3 71.3 ±0.5 79.7 ±0.3 46.9 ±0.7 OOM PPRGo 82.4 ±0.2 71.3 ±0.5 80.0 ±0.4 46.6 ±0.5 OOM DAGNN (Gate) 84.4±0.5 73.3 ±0.6 80.5 ±0.5 47.1 ±0.6 OOM DAGNN (NDLS-L)∗ 84.4±0.6 73.6 ±0.7 80.9 ±0.5 47.2 ±0.7 OOM Linear MLP 61.1 ±0.6 61.8 ±0.8 72.7 ±0.6 41.3 ±0.8 47.2 ±0.3 SGC 81.0 ±0.2 71.3 ±0.5 78.9 ±0.5 45.2 ±0.3 63.2 ±0.2 SIGN 82.1 ±0.3 72.4 ±0.8 79.5 ±0.5 46.3 ±0.5 64.2 ±0.2 S2GC 82.7 ±0.3 73.0 ±0.2 79.9 ±0.3 46.6 ±0.6 64.7 ±0.3 GBP 83.9 ±0.7 72.9 ±0.5 80.6 ±0.4 46.9 ±0.7 65.2 ±0.3 Linear NDLS-F+MLP∗ 84.1±0.6 73.5 ±0.5 81.1 ±0.6 47.5 ±0.7 65.3 ±0.5 MLP+NDLS-L∗ 83.9±0.6 73.1 ±0.8 81.1 ±0.6 46.9 ±0.7 64.6 ±0.4 SGC+NDLS-L∗ 84.2±0.2 73.4 ±0.5 81.1 ±0.4 47.1 ±0.6 64.9 ±0.3 NDLS∗ 84.6±0.5 73.7 ±0.6 81.4 ±0.4 47.7 ±0.5 65.6 ±0.3 DAGNN (Gate) [25], and PPRGo [1]; (3) the linear-model-based GNNs: MLP, SGC [32], SIGN [28], S2GC [42] and GBP [6]. In the inductive setting, the compared baselines are sampling-based GNNs: GraphSAGE [14], FastGCN [3], ClusterGCN [8] and GraphSAINT [37]. Detailed descriptions of these baselines are provided in Appendix A.4. Implementations. To alleviate the inﬂuence of randomness, we repeat each method ten times and report the mean performance. The hyper-parameters of baselines are tuned by OpenBox [23] or set according to the original paper if available. Please refer to Appendix A.5 for more details. /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000015 /uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048 /uni00000017/uni00000018 /uni00000017/uni00000019 /uni00000017/uni0000001a /uni00000017/uni0000001b/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000036/uni0000002a/uni00000026/uni00000014/uni0000005b /uni0000002a/uni00000026/uni00000031 /uni00000016/uni00000016/uni0000005b /uni00000024/uni00000033/uni00000033/uni00000031/uni00000033 /uni0000001a/uni0000001b/uni0000005b /uni00000031/uni00000027/uni0000002f/uni00000036 /uni00000015/uni0000005b /uni000000362/uni0000002a/uni00000026 /uni00000014/uni0000005b /uni0000002a/uni00000025/uni00000033/uni00000014/uni0000005b /uni0000002a/uni00000024/uni00000037 /uni00000016/uni0000001a/uni00000015/uni0000005b/uni00000024/uni00000033/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000014/uni00000015/uni0000005b /uni0000002d/uni0000002e/uni00000010/uni00000031/uni00000048/uni00000057 /uni00000014/uni00000014/uni00000016/uni0000005b /uni00000035/uni00000048/uni00000056/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000016/uni00000015/uni0000005b /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni00000016/uni0000005b Figure 3: Performance along with training time on the Industry dataset. Table 4: Results of inductive settings. Models Flickr Reddit GraphSAGE 50.1 ±1.3 95.4 ±0.0 FastGCN 50.4 ±0.1 93.7 ±0.0 ClusterGCN 48.1 ±0.5 95.7 ±0.0 GraphSAINT 51.1 ±0.1 96.6 ±0.1 NDLS-F+MLP∗ 51.9±0.2 96.6 ±0.1 GraphSAGE+NDLS-L∗ 51.5±0.4 96.3 ±0.0 NDLS∗ 52.6±0.4 96.8 ±0.1 6.2 Experimental Results. End-to-end comparison. To answerQ1, Table 3 and 4 show the test accuracy of considered methods in transductive and inductive settings. In the inductive setting, NDLS outperforms one of the most competitive baselines – GraphSAINT by a margin of 1.5% and 0.2% on Flickr and Reddit. NDLS exceeds the best GNN model among all considered baselines on each dataset by a margin of 0.2% to 0.8% in the transductive setting. In addition, we observe that with NDLS-L, the model performance of MLP, SGC, NDLS-F+MLP, and GraphSAGE can be further improved by a large margin. For example, the accuracy gain for MLP is 21.8%, 11.3%, 8.4%, and 5.6% on Cora, Citseer, PubMed, and Industry, respectively. To answerQ2, we replace the gate mechanism in the vanilla DAGNN with NDLS-L and refer to this method as DAGNN (NDLS-L). Surprisingly, DAGNN (NDLS-L) achieves at least comparable or (often) higher test accuracy compared with AP-GCN and DAGNN (Gate), and it shows that NDLS-L performs better than the learned mechanism in label smoothing. Furthermore, by replacing the original graph kernels with NDLS-F, NDLS-F+MLP outperforms both S2GC and GBP on all compared datasets. This demonstrates the effectiveness of the proposed NDLS. Training Efﬁciency. To answer Q3, we evaluate the efﬁciency of each method on a real-world industry graph dataset. Here, we pre-compute the smoothed features of each linear-model-based 8/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (a) Feature Sparsity /uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000028/uni00000047/uni0000004a/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (b) Edge Sparsity /uni00000014/uni00000013/uni00000015/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (c) Label Sparsity Figure 4: Test accuracy on PubMed dataset under different levels of feature, edge and label sparsity. /uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000027/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (a) LSI along with the node degree  (b) The visualization of LSI Figure 5: (Left) LSI distribution along with the node degree in three citation networks. (Right) The visualization of LSI in Zachary’s karate club network. Nodes with larger radius have larger LSIs. GNN, and the time for pre-processing is also included in the training time. Figure 3 illustrates the results on the industry dataset across training time. Compared with linear-model-based GNNs, we observe that (1) both the coupled and decoupled GNNs require a signiﬁcantly larger training time; (2) NDLS achieves the best test accuracy while consuming comparable training time with SGC. Performance on Sparse Graphs. To reply Q4, we conduct experiments to test the performance of NDLS on feature, edge, and label sparsity problems. For feature sparsity, we assume that the features of unlabeled nodes are partially missing. In this scenario, it is necessary to calculate a personalized propagation iteration to “recover” each node’s feature representation. To simulate edge sparsity settings, we randomly remove a ﬁxed percentage of edges from the original graph. Besides, we enumerate the number of nodes per class from 1 to 20 in the training set to measure the effectiveness of NDLS given different levels of label sparsity. The results in Figure 4 show that NDLS outperforms all considered baselines by a large margin across different levels of feature, edge, and label sparsity, thus demonstrating that our method is more robust to the graph sparsity problem than the linear-model-based GNNs. Interpretability. As mentioned by Q1, we here answer why NDLS is effective. One theoretical property of LSI is that the value correlates with the node degree negatively. We divide nodes into several groups, and each group consists of nodes with the same degree. And then we calculate the average LSI value for each group in the three citation networks respectively. Figure 5(a) depicts that nodes with a higher degree have a smaller LSI, which is consistent with Theorem 3.1. We also use NetworkX [13] to visualize the LSI in Zachary’s karate club network [36]. Figure 5(b), where the radius of each node corresponds to the value of LSI, shows three interesting observations: (1) nodes with a larger degree have smaller LSIs; (2) nodes in the neighbor area have similar LSIs; (3) nodes adjacent to a super-node have smaller LSIs. The ﬁrst observation is consistent with Theorem 3.1, and the latter two observations show consistency with Theorem 3.2. 7 Conclusion In this paper, we present node-dependent local smoothing (NDLS), a simple and scalable graph learning method based on the local smoothing of features and labels. NDLS theoretically analyzes 9what inﬂuences the smoothness and gives a bound to guide how to control the extent of smoothness for different nodes. By setting a node-speciﬁc smoothing iteration, each node in NDLS can smooth its feature/label to a local-smoothing state and then help to boost the model performance. Extensive experiments on seven real-world graph datasets demonstrate the high accuracy, scalability, efﬁciency, and ﬂexibility of NDLS against the state-of-the-art GNNs. Broader Impact NDLS can be employed in areas where graph modeling is the foremost choice, such as citation networks, social networks, chemical compounds, transaction graphs, road networks, etc. The effectiveness of NDLS when improving the predictive performance in those areas may bring a broad range of societal beneﬁts. For example, accurately predicting the malicious accounts on transaction networks can help identify criminal behaviors such as stealing money and money laundering. Prediction on road networks can help avoid trafﬁc overload and save people’s time. A signiﬁcant beneﬁt of NDLS is that it offers a node-dependent solution. However, NDLS faces the risk of information leakage in the smoothed features or labels. In this regard, we encourage researchers to understand the privacy concerns of NDLS and investigate how to mitigate the possible information leakage. Acknowledgments and Disclosure of Funding This work is supported by NSFC (No. 61832001, 6197200), Beijing Academy of Artiﬁcial Intelligence (BAAI), PKU-Baidu Fund 2019BD006, and PKU-Tencent Joint Research Lab. Zhi Yang and Bin Cui are the corresponding authors. References [1] A. Bojchevski, J. Klicpera, B. Perozzi, A. Kapoor, M. Blais, B. Rózemberczki, M. Lukasik, and S. Günnemann. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2464–2473, 2020. [2] L. Cai and S. Ji. A multi-scale approach for graph link prediction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3308–3315, 2020. [3] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. [4] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In ICLR, 2018. [5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 941–949, 2018. [6] M. Chen, Z. Wei, B. Ding, Y . Li, Y . Yuan, X. Du, and J. Wen. Scalable graph neural networks via bidirectional propagation. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [7] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 785–794, 2016. [8] W. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. InProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 257–266, 2019. 10[9] F. R. Chung and F. C. Graham. Spectral graph theory. Number 92. American Mathematical Soc., 1997. [10] G. Cui, J. Zhou, C. Yang, and Z. Liu. Adaptive graph encoder for attributed graph embedding. In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 976–985, 2020. [11] H. Dihe. An introduction to markov process in random environment [j]. Acta Mathematica Scientia, 5, 2010. [12] Q. Guo, X. Qiu, X. Xue, and Z. Zhang. Syntax-guided text generation via graph neural network. Sci. China Inf. Sci., 64(5), 2021. [13] A. Hagberg, P. Swart, and D. S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008. [14] W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 1024–1034, 2017. [15] X. He, K. Deng, X. Wang, Y . Li, Y . Zhang, and M. Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 639–648, 2020. [16] W. Hu, M. Fey, H. Ren, M. Nakata, Y . Dong, and J. Leskovec. OGB-LSC: A large-scale challenge for machine learning on graphs. CoRR, abs/2103.09430, 2021. [17] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 4563–4572, 2018. [18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [19] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018. [20] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. [21] K.-H. Lai, D. Zha, K. Zhou, and X. Hu. Policy-gnn: Aggregation optimization for graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 461–471, 2020. [22] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. [23] Y . Li, Y . Shen, W. Zhang, Y . Chen, H. Jiang, M. Liu, J. Jiang, J. Gao, W. Wu, Z. Yang, C. Zhang, and B. Cui. Openbox: A generalized black-box optimization service. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 3209–3219, 2021. [24] A. Liaw, M. Wiener, et al. Classiﬁcation and regression by randomforest. R news, 2(3):18–22, 2002. [25] M. Liu, H. Gao, and S. Ji. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 338–348, 2020. 11[26] X. Miao, N. M. Gürel, W. Zhang, Z. Han, B. Li, W. Min, S. X. Rao, H. Ren, Y . Shan, Y . Shao, Y . Wang, F. Wu, H. Xue, Y . Yang, Z. Zhang, Y . Zhao, S. Zhang, Y . Wang, B. Cui, and C. Zhang. Degnn: Improving graph neural networks with graph decomposition. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 1223–1233, 2021. [27] X. Miao, W. Zhang, Y . Shao, B. Cui, L. Chen, C. Zhang, and J. Jiang. Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture. IEEE Transactions on Knowledge and Data Engineering, 2021. [28] E. Rossi, F. Frasca, B. Chamberlain, D. Eynard, M. M. Bronstein, and F. Monti. SIGN: scalable inception graph neural networks. CoRR, abs/2004.11198, 2020. [29] I. Spinelli, S. Scardapane, and A. Uncini. Adaptive propagation graph convolutional network. IEEE Transactions on Neural Networks and Learning Systems, 2020. [30] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] K. Wang, Z. Shen, C. Huang, C.-H. Wu, Y . Dong, and A. Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396–413, 2020. [32] F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 6861–6871, 2019. [33] S. Wu, W. Zhang, F. Sun, and B. Cui. Graph neural networks in recommender systems: A survey. CoRR, abs/2011.02260, 2020. [34] S. Wu, Y . Zhang, C. Gao, K. Bian, and B. Cui. Garg: Anonymous recommendation of point- of-interest in mobile networks by graph convolution network. Data Science and Engineering, 5(4):433–447, 2020. [35] K. Xu, C. Li, Y . Tian, T. Sonobe, K. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 5449–5458, 2018. [36] W. W. Zachary. An information ﬂow model for conﬂict and ﬁssion in small groups. Journal of anthropological research, 33(4):452–473, 1977. [37] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [38] W. Zhang, Y . Jiang, Y . Li, Z. Sheng, Y . Shen, X. Miao, L. Wang, Z. Yang, and B. Cui. ROD: reception-aware online distillation for sparse graphs. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 2232–2242, 2021. [39] W. Zhang, X. Miao, Y . Shao, J. Jiang, L. Chen, O. Ruas, and B. Cui. Reliable data distillation on graph convolutional network. In Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020, pages 1399–1414, 2020. [40] W. Zhang, Z. Sheng, Y . Jiang, Y . Xia, J. Gao, Z. Yang, and B. Cui. Evaluating deep graph neural networks. CoRR, abs/2108.00955, 2021. [41] X. Zhang, H. Liu, Q. Li, and X. Wu. Attributed graph clustering via adaptive graph convolution. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4327–4333, 2019. [42] H. Zhu and P. Koniusz. Simple spectral graph convolution. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. 12A Appendix A.1 Proofs of Theorems We represent the adjacency matrix and the diagonal degree matrix of graphGby Aand Drespectively, represent D+I and A+I by ˜Dand ˜A. Then we denote ˜D−1 ˜Aas a transition matrix P. Suppose P is connected, which means the graph is connected, for any initial distribution π0, let ˜π(π0) = lim k→∞ π0Pk, (15) then according to [11], for any initial distribution π0 ˜π(π0)i = 1 n n∑ j=1 Pji, (16) where ˜πi denotes the ith component of ˜π(π0), and ndenotes the number of nodes in graph. If matrix P is unconnected, we can divide P into connected blocks. Then for each blocks(denoted as Bg), there always be ˜π(π0)i = 1 ng ∑ j∈Bg Pji ∗ ∑ j∈Bg π0j, (17) where ng is the number of nodes in Bg. To make the proof concise, we will assume matrix P is connected, otherwise we can perform the same operation inside each block. Therefore, ˜π is independent to π0, thus we replace ˜π(π0) by ˜π. Deﬁnition A.1 (Local Mixing Time). The local mixing time (parameterized by ϵ) with an initial distribution is deﬁned as T(π0,ϵ) = min{t: ||˜π−π0Pt||2 <ϵ}, (18) where \"||·||2\" symbols two-nor m. In order to consider the impact of each node to the others separately, letπ0 = ei, where ei is a one-hot vector with the ith component equal to 1, and the other components equal to 0. According to [ 9] we have lemma A.1. Lemma A.1. |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (19) where λ2 is the second large eigenvalue of P and ˜di denotes the degree of node vi plus 1 (to include itself). ˜di = di + 1, ˜dj = dj + 1, Theorem A.2. T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n), (20) where mand ndenote the number of edges and nodes in graph Gseparately. ˜di = di + 1, Proof. [9] shows that when π0 = ei, |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (21) where (eiPt)j symbols the jth element of eiPt. We denote eiPt as πi(t), then ||˜π−πi(t)||2 2 = n∑ j=1 (˜πj −πi(t)j)2 ≤ n∑ j=1 ˜dj ˜di λ2t 2 = 2m+ n ˜di λ2t 2 , (22) 13which means ||˜π−πi(t)||2 ≤ √ 2m+ n ˜di λt 2. (23) Now let ϵ= √ 2m+ n ˜di λt 2, there exists T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). Next consider the real situation in SGC with n×m-dimension matrix X(0) as input, where nis the number of nodes, mis the number of features. We apply P as the normalized adjacent matrix.(The deﬁnition of P is the same as ˜A in main text). In feature propagation we have X(t) =PtX(0), Now consider the hth feature of X, we deﬁne an n×ninﬂuence matrix Ihij(t) = ∂X(t)ih ∂X(0)jh , (24) Because Ih(k) is independent to h, we replace Ih(k) by I(k), which can be formulated as I(k) =Ih(k), ∀h∈{1,2,..,f }, (25) where f symbols the number of features of X. Deﬁnition A.2 (Local Smoothing Iteration). The Local Smoothing Iteration (parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −Ii(k)||2 <ϵ}. (26) According to Theorem A.2, there exists Theorem A.3 (Theorem 3.1 in main text). When the normalized adjacent matrix is P, K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). (27) Proof. From equation (9) we can derive that ||eiP∞−eiPk||2 ≤ √ 2m+ n ˜di λk 2. Because Ii(k) =Pk i = eiPk Ii(∞) =P∞ i = eiP∞, we have ||Ii(∞) −Ii(k)||2 ≤ √ 2m+ n ˜di λk 2. Now let ϵ= √ 2m+ n ˜di λk 2, there exists K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). 14Therefore, we expand Theorem A.3 to the propagation in SGC or our method. What is remarkable, Theorem A.3 requires P, which is equal to ˜D−1 ˜Aas the normalized adjacent matrix. From Theorem A.3 we can conclude that the node which has a lager degree may need more steps to propagate. At the same time, we have another bond of local mixing time as following. Theorem A.4. For each node vi in graph G, there always exits T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. (28) where N(i) is the set of node vi’s neighbours. Proof. ∥|˜π−eiPt+1||2 = 1 |N(i)| ∑ j∈N(i) ||˜π−ejPt||2 ≤ max j∈N(i) ||˜π−ejPt||2. (29) Therefore, when max j∈N(i) ||˜π−ejPt||2 ≤ϵ, there exists ||˜π−eiPt+1||2 ≤ϵ. Thus we can derive that T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. As we extend Theorem A.2 to Theorem A.3, according to Theorem A.4, there always be Theorem A.5 (Theorem 3.2 in main text). For each node vi in graph G, there always exits K(i,ϵ) ≤max{K(j,ϵ),j ∈N(i)}+ 1. (30) A.2 Results with More Base Models Our proposed NDLS consists of three stages: (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). In stage (2), the default option of the base model is a Multilayer Perceptron (MLP). Besides MLP, many other models can also be used in stage (2) to generate soft labels. To verify it, here we replace the MLP in stage (2) with popular machine learning models Random Forest [24] and XGBoost [7], and measure their node classiﬁcation performance on PubMed dataset. The experiment results are shown in Table 3 where Random Forest and XGBoost are abbreviated as RF and XGB respectively. Compared to the vanilla model, both Random Forest and XGBoost achieve signiﬁcant performance gain with the addition of our NDLS. With the help of NDLS, Random Forest and XGBoost outperforms their base models by 6.1% and 7.5% respectively. From Table 3, we can observe that both NDLS-F and NDLS-L can contribute great performance boost to the base model, where the gains are at least 5%. When all equipped with both NDLS-F and NDLS-L, XGBoost beat the default MLP, achieving a test accuracy of81.6%. Although Random Forest – 80.5% – cannot outperform the other two models, it is still a competitive model. The above experiment demonstrates that the base model selection in stage (2) is rather ﬂexible in our NDLS. Both traditional machine learning methods and neural networks are promising candidates in the proposed method. A.3 Dataset Description Cora, Citeseer, and Pubmed1 are three popular citation network datasets, and we follow the public training/validation/test split in GCN [18]. In these three networks, papers from different topics are 1https://github.com/tkipf/gcn/tree/master/gcn/data 15Table 5: Results of different base models on PubMed. Base Models Models Accuracy Gain MLP Base 72.7 ±0.6 - + NDLS-F 81.1 ±0.6 + 8.4 + NDLS-L 81.1 ±0.6 + 8.4 + NDLS (both) 81.4 ±0.4 + 8.7 RF Base 74.4 ±0.2 - + NDLS-F 80.3 ±0.1 + 5.9 + NDLS-L 80.0 ±0.2 + 5.6 + NDLS (both) 80.5 ±0.4 + 6.1 XGB Base 74.1 ±0.2 - + NDLS-F 81.0 ±0.3 + 6.9 + NDLS-L 79.8 ±0.2 + 5.7 + NDLS (both) 81.6 ±0.3 + 7.5 considered as nodes, and the edges are citations among the papers. The node attributes are binary word vectors, and class labels are the topics papers belong to. Reddit is a social network dataset derived from the community structure of numerous Reddit posts. It is a well-known inductive training dataset, and the training/validation/test split in our experiment is the same as the one in GraphSAGE [14]. Flickr originates from NUS-wide 2 and contains different types of images based on the descriptions and common properties of online images. The public version of Reddit and Flickr provided by GraphSAINT3 is used in our paper. Industry is a short-form video graph, collected from a real-world mobile application from our industrial cooperative enterprise. We sampled 1,000,000 users and videos from the app, and treat these items as nodes. The edges in the generated bipartite graph represent that the user clicks the short-form videos. Each user has 64 features and the target is to category these short-form videos into 253 different classes. ogbn-papers100M is a directed citation graph of 111 million papers indexed by MAG [31]. Among its node set, approximately 1.5 million of them are arXiv papers, each of which is manually labeled with one of arXiv’s subject areas. Currently, this dataset is much larger than any existing public node classiﬁcation datasets. A.4 Compared Baselines The main characteristic of all baselines are listed below: • GCN [18]: GCN is a novel and efﬁcient method for semi-supervised classiﬁcation on graph-structured data. • GAT [30]: GAT leverages masked self-attention layers to specify different weights to different nodes in a neighborhood, thus better represent graph information. • JK-Net [35]: JK-Net is a ﬂexible network embedding method that could gather different neighborhood ranges to enable better structure-aware representation. • APPNP [19]: APPNP uses the relationship between graph convolution networks (GCN) and PageRank to derive improved node representations. • AP-GCN [29]: AP-GCN uses a halting unit to decide a receptive range of a given node. 2http://lms.comp.nus.edu.sg/research/NUS-WIDE.html 3https://github.com/GraphSAINT/GraphSAINT 16Table 6: URLs of baseline codes. Type Baselines URLs Coupled GCN https://github.com/rusty1s/pytorch_geometric GAT https://github.com/rusty1s/pytorch_geometric Decoupled APPNP https://github.com/rusty1s/pytorch_geometric PPRGo https://github.com/TUM-DAML/pprgo_pytorch AP-GCN https://github.com/spindro/AP-GCN DAGNN https://github.com/divelab/DeeperGNN Sampling GraphSAGE https://github.com/williamleif/GraphSAGE GraphSAINT https://github.com/GraphSAINT/GraphSAINT FastGCN https://github.com/matenure/FastGCN Cluster-GCN https://github.com/benedekrozemberczki/ClusterGCN Linear SGC https://github.com/Tiiiger/SGC SIGN https://github.com/twitter-research/sign S2GC https://github.com/allenhaozhu/SSGC GBP https://github.com/chennnM/GBP NDLS https://github.com/zwt233/NDLS • DAGNN [25]: DAGNN proposes to decouple the representation transformation and propagation, and show that deep graph neural networks without this entanglement can leverage large receptive ﬁelds without suffering from performance deterioration. • PPRGo [1]: utilizes an efﬁcient approximation of information diffusion in GNNs resulting in signiﬁcant speed gains while maintaining state-of-the-art prediction performance. • GraphSAGE [14]: GraphSAGE is an inductive framework that leverages node attribute information to efﬁciently generate representations on previously unseen data. • FastGCN [4]: FastGCN interprets graph convolutions as integral transforms of embedding functions under probability measures. • Cluster-GCN [8]: Cluster-GCN is a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. • GraphSAINT [37]: GraphSAINT constructs mini-batches by sampling the training graph, rather than the nodes or edges across GCN layers. • SGC [32]: SGC simpliﬁes GCN by removing nonlinearities and collapsing weight matrices between consecutive layers. • SIGN [28]: SIGN is an efﬁcient and scalable graph embedding method that sidesteps graph sampling in GCN and uses different local graph operators to support different tasks. • S2GC [42]: S2GC uses a modiﬁed Markov Diffusion Kernel to derive a variant of GCN, and it can be used as a trade-off of low-pass and high-pass ﬁlter which captures the global and local contexts of each node. • GBP [6]: GBP utilizes a localized bidirectional propagation process from both the feature vectors and the training/testing nodes Table 6 summarizes the github URLs of the compared baselines. Following the original paper, we implement JK-Net by ourself since there is no ofﬁcial version available. A.5 Implementation Details Hyperparameter details. In stage (1), when computing the Local Smoothing Iteration, the maximal value of k in equation (12) is set to 200 and the optimal ϵ value is get by means of a grid search from {0.01, 0.03, 0.05}. In stage (2), we use a simple two-layer MLP to get the base prediction. The hidden size is set to 64 in small datasets – Cora, Citeseer and Pubmed. While in larger datasets – Flicker, Reddit, Industry and ogbn-papers100M, the hidden size is set to 256. As for the dropout percentage and the learning rate, we use a grid search from {0.2, 0.4, 0.6, 0.8} and {0.1, 17Table 7: Performance comparison between C&S and NDLS-L Methods Cora Citeseer PubMed ogbn-papers100M MLP+C&S 87.2 76.6 88.3 63.9 MLP+NDLS-L 88.1 78.3 88.5 64.6 Table 8: Performance comparison under varied label rate on the Cora dataset. Methods 2% 5% 10% 20% 40% 60% MLP+S 63.1 77.8 82.6 84.2 85.4 86.4 MLP+C&S 62.8 76.7 82.8 84.9 86.4 87.2 MLP+NDLS-L 77.4 83.9 85.3 86.5 87.6 88.1 Table 9: Performance comparison after combining the node-dependent idea with C&S. Methods Cora Citeseer PubMed MLP+C&S 76.7 70.8 76.5 MLP+C&S+nd 79.9 71.1 78.4 0.01, 0.001} respectively. In stage (3), during the computation of the Local Smoothing Iteration, the maximal value of kis set to 40. The optimal value of ϵis obtained through the same process in stage (1). Implementation environment. The experiments are conducted on a machine with Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz, and a single NVIDIA TITAN RTX GPU with 24GB memory. The operating system of the machine is Ubuntu 16.04. As for software versions, we use Python 3.6, Pytorch 1.7.1 and CUDA 10.1. A.6 Comparison and Combination with Correct&Smooth Similar to our NDLS-L, Correct and Smooth (C&S) also applies post-processing on the model prediction. Therefore, we compare NDLS-L with C&S below. Adaptivity to node. C&S adopts a propagation scheme based on Personalized PageRank (PPR), which always maintains certain input information to slow down the occurrence of over-smoothing. The expected number of smoothing iterations is controlled by the restart probability, which is a constant for all nodes. Therefore, C&S still falls into the routine of ﬁxed smoothing iteration. Instead, NDLS-L employs node-speciﬁc smoothing iterations. We compare each method’s performance (test accuracy, %) under the same data split as in the C&S paper (60%/20%/20% on three citation networks, ofﬁcial split on ogbn-papers100M), and the experimental results in Table 7 show that NDLS-L outperforms C&S in different datasets. Sensitivity to label rate. During the “Correct” stage, C&S propagates uncertainties from the training data across the graph to correct the base predictions. However, the uncertainties might not be accurate when the number of training nodes is relatively small, thus even degrading the performance. To conﬁrm the above assumption, we conduct experiments on the Cora dataset under different label rates, and the experimental results are provided in Table 8. As illustrated, the result of C&S drops much faster than NDLS-L’s when the label rate decreases. What’s more, MLP+S (removing the “Correct” stage) outperforms MLP+C&S when the label rate is low as expected. Compared with C&S, NDLS is more general in terms of smoothing types. C&S can only smooth label predictions. Instead, NDLS can smooth both node features and label predictions and combine them to boost the model performance further. 18Table 10: Efﬁciency comparison on the PubMed dataset. SGC S2GC GBP NDLS SIGN JK-Net DAGNN GCN ResGCN APPNP GAT Time 1.00 1.19 1.20 1.50 1.59 11.42 14.39 20.43 20.49 28.88 33.23 Accuracy 78.9 79.9 80.6 81.4 79.5 78.8 80.5 79.3 78.6 80.1 79.0 Table 11: Performance comparison on the ogbn-arxiv dataset. MLP MLP+C&S GCN SGC SIGN DAGNN JK-Net S2GC GBP NDLS GAT Accuracy55.50 71.58 71.74 71.72 71.95 72.09 72.19 72.21 72.45 73.04 73.56 Node Adaptive C&S. The node-dependent mechanism in our NDLS can easily be combined with C&S. The two stages of C&S both contain a smoothing process using the personalized PageRank matrix, where a coefﬁcient controls the remaining percentage of the original node feature. Here, we can precompute the smoothed node features after the same smoothing step yet under different values like 0.1, 0.2, ..., 0.9. After that, we adopt the same strategy in our NDLS: for each node, we choose the ﬁrst in the ascending order that the distance from the smoothed node feature to the stationarity is less than a tuned hyperparameter. By this means, the smoothing process in C&S can be carried out in a node-dependent way. We also evaluate the performance of C&S combined with the node-dependent idea (represented as C&S+nd) on the three citation networks under ofﬁcial splits, and the experimental results in Table 9 show that C&S combined with NDLS consistently outperforms the original version of C&S. A.7 Training Efﬁciency Study we measure the training efﬁciency of the compared baselines on the widely used PubMed dataset. Using the training time of SGC as the baseline, the relative training time and the corresponding test accuracy of NDLS and the baseline methods are shown in Table 10. Compared with other baselines, NDLS can get the highest test accuracy while maintaining competitive training efﬁciency. A.8 Experiments on ogbn-arxiv We also conduct experiments on the ogbn-arxiv dataset. The experiment results (test accuracy, %) are provided in Table 11. Although GAT outperforms NDLS on ogbn-arxiv dataset, it is hard to scale to large graphs like ogbn-papers100M dataset. Note that MLP+C&S on the OGB leaderboard makes use of not only the original node feature but also diffusion embeddings and spectral embeddings. Here we remove the latter two embeddings for fairness, and the authentic MLP+C&S achieves 71.58% on the ogbn-arxiv dataset. 19",
      "meta_data": {
        "arxiv_id": "2110.14377v1",
        "authors": [
          "Wentao Zhang",
          "Mingyu Yang",
          "Zeang Sheng",
          "Yang Li",
          "Wen Ouyang",
          "Yangyu Tao",
          "Zhi Yang",
          "Bin Cui"
        ],
        "published_date": "2021-10-27T12:24:41Z",
        "venue": "NeurIPS 2021",
        "pdf_url": "https://arxiv.org/pdf/2110.14377v1.pdf"
      }
    },
    {
      "title": "PairNorm: Tackling Oversmoothing in GNNs",
      "abstract": "The performance of graph neural nets (GNNs) is known to gradually decrease\nwith increasing number of layers. This decay is partly attributed to\noversmoothing, where repeated graph convolutions eventually make node\nembeddings indistinguishable. We take a closer look at two different\ninterpretations, aiming to quantify oversmoothing. Our main contribution is\nPairNorm, a novel normalization layer that is based on a careful analysis of\nthe graph convolution operator, which prevents all node embeddings from\nbecoming too similar. What is more, PairNorm is fast, easy to implement without\nany change to network architecture nor any additional parameters, and is\nbroadly applicable to any GNN. Experiments on real-world graphs demonstrate\nthat PairNorm makes deeper GCN, GAT, and SGC models more robust against\noversmoothing, and significantly boosts performance for a new problem setting\nthat benefits from deeper GNNs. Code is available at\nhttps://github.com/LingxiaoShawn/PairNorm.",
      "full_text": "arXiv:1909.12223v2  [cs.LG]  13 Feb 2020 Published as a conference paper at ICLR 2020 PA I R NO R M : T AC K L I N G OV E R S M O OT H I N G I N GNN S Lingxiao Zhao Carnegie Mellon University Pittsburgh, P A 15213, USA {lingxia1}@andrew.cmu.edu Leman Akoglu Carnegie Mellon University Pittsburgh, P A 15213, USA {lakoglu}@andrew.cmu.edu ABSTRACT The performance of graph neural nets (GNNs) is known to gradu ally decrease with increasing number of layers. This decay is partly attri buted to oversmooth- ing, where repeated graph convolutions eventually make nod e embeddings indis- tinguishable. W e take a closer look at two different interpr etations, aiming to quantify oversmoothing. Our main contribution is P A IR NO RM , a novel normal- ization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too simila r. What is more, PA IR NO RM is fast, easy to implement without any change to network arch itecture nor any additional parameters, and is broadly applicable to any GNN. Experiments on real-world graphs demonstrate that P A IR NO RM makes deeper GCN, GA T , and SGC models more robust against oversmoothing, and signiﬁca ntly boosts per- formance for a new problem setting that beneﬁts from deeper G NNs. Code is available at https://github.com/LingxiaoShawn/PairNorm. 1 I NTRODUC TI ON Graph neural networks (GNNs) is a family of neural networks t hat can learn from graph structured data. Starting with the success of GCN (Kipf & W elling, 2017) on achieving state-of-the-art per- formance on semi-supervised classiﬁcation, several varia nts of GNNs have been developed for this task; including GraphSAGE (Hamilton et al., 2017), GA T (V el ickovic et al., 2018), SGC (Wu et al., 2019), and GMNN (Qu et al., 2019) to name a few most recent ones . A key issue with GNNs is their depth limitations. It has been o bserved that deeply stacking the layers often results in signiﬁcant drops in performance for GNNs, such as GCN and GA T , even beyond just a few (2–4) layers. This drop is associated with a number of factors; including the vanishing gradients in back-propagation, overﬁtting due t o the increasing number of parameters, as well as the phenomenon called oversmoothing. Li et al. (2018 ) was the ﬁrst to call attention to the oversmoothing problem. Having shown that the graph convolu tion is a type of Laplacian smoothing , they proved that after repeatedly applying Laplacian smoot hing many times, the features of the nodes in the (connected) graph would converge to similar values—t he issue coined as “ oversmoothing”. In effect, oversmoothing hurts classiﬁcation performance by causing the node representations to be indistinguishable across different classes. Later, sever al others have alluded to the same problem (Xu et al., 2018; Klicpera et al., 2019; Rong et al., 2019; Li e t al., 2019) (See §5 Related W ork). In this work, we address the oversmoothing problem in deep GN Ns. Speciﬁcally, we propose (to the best of our knowledge) the ﬁrst normalization layer for GNNs that is applied in-between intermediate layers during training. Our normalization has the effect of preventing the output features of distant nodes to be too similar or indistinguishable, while at the sa me time allowing those of connected nodes in the same cluster become more similar. W e summarize o ur main contributions as follows. • Normalization to T ackle Oversmoothing in GNNs: W e introduce a normalization scheme, called P A IR NO RM , that makes GNNs signiﬁcantly more robust to oversmoothing and as a result enables the training of deeper models without sacriﬁcing pe rformance. Our proposed scheme capitalizes on the understanding that most GNNs perform a sp ecial form of Laplacian smoothing, which makes node features more similar to one another. The ke y idea is to ensure that the total pairwise feature distances remains a constant across layers, which in turn leads to distant pairs having less similar features, preventing feature mixing ac ross clusters. 1Published as a conference paper at ICLR 2020 • Speed and Generality: PA IR NO RM is very straightforward to implement and introduces no additional parameters. It is simply applied to the output fe atures of each layer (except the last one) consisting of simple operations, in particular center ing and scaling, that are linear in the input size. Being a simple normalization step between layer s, P A IR NO RM is not speciﬁc to any particular GNN but rather applies broadly. •Use Case for Deeper GNNs: While P A IR NO RM prevents performance from dropping signif- icantly with increasing number of layers, it does not necess arily yield increased performance in absolute terms. W e ﬁnd that this is because shallow architec tures with no more than 2–4 layers is sufﬁcient for the often-used benchmark datasets in the li terature. In response, we motivate a real-world scenario wherein a notable portion of the nodes h ave no feature vectors. In such set- tings, nodes beneﬁt from a larger range (i.e., neighborhood , hence a deeper GNN) to “recover” effective feature representations. Through extensive exp eriments, we show that GNNs employing our P A IR NO RM signiﬁcantly outperform the ‘vanilla’ GNNs when deeper mod els are beneﬁcial to the classiﬁcation task. 2 U NDERSTAN DI NG OVERSMOOT HIN G In this work, we consider the semi-supervised node classiﬁc ation (SSNC) problem on a graph. In the general setting, a graph G = (V, E, X) is given in which each node i ∈ V is associated with a feature vector xi ∈ Rd where X = [x1, . . . , xn]T denotes the feature matrix, and a subset Vl ⊂ V of the nodes are labeled, i.e. yi ∈ { 1, . . . , c } for each i ∈ V l where c is the number of classes. Let A ∈ Rn×n be the adjacency matrix and D = diag(deg1, . . . , deg n) ∈ Rn×n be the degree matrix of G. Let ˜A = A + I and ˜D = D + I denote the augmented adjacency and degree matrices with added self-loops on all nodes, respectively. Let ˜Asym = ˜D−1/2 ˜A ˜D−1/2 and ˜Arw = ˜D−1 ˜A denote symmetrically and nonsymmetrically normalized adjacency matrices with self-loops. The task is to learn a hypothesis that predicts yi from xi that generalizes to the unlabeled nodes Vu = V\\Vl. In Section 3.2, we introduce a variant of this setting where only a subset F ⊂ V of the nodes have feature vectors and the rest are missing. 2.1 T H E OV E RS M O OT H IN G PRO BL E M Although GNNs like GCN and GA T achieve state-of-the-art res ults in a variety of graph-based tasks, these models are not very well-understood, especially why t hey work for the SSNC problem where only a small amount of training data is available. The succes s appears to be limited to shallow GNNs, where the performance gradually decreases with the in creasing number of layers. This decrease is often attributed to three contributing factors : (1) overﬁtting due to increasing number of parameters, (2) difﬁculty of training due to vanishing gr adients, and (3) oversmoothing due to many graph convolutions. Among these, perhaps the least understood one is oversmoothing, which indeed lacks a for- mal deﬁnition. In their analysis of GCN’s working mechanism , Li et al. (2018) showed that the graph convolution of GCN is a special form of Laplacian smoot hing. The standard form being (I − γI)X + γ ˜Arw X, the graph convolution lets γ = 1 and uses the symmetrically normalized Laplacian to obtain ˜X = ˜Asym X, where the new features ˜ xof a node is the weighted average of its own and its neighbors’ features. This smoothing allows the n ode representations within the same cluster become more similar, and in turn helps improve SSNC p erformance under the cluster as- sumption (Chapelle et al., 2006). However when GCN goes deep , the performance can suffer from oversmoothing where node representations from different c lusters become mixed up. Let us refer to this issue of node representations becoming too similar as node-wise oversmoothing. Another way of thinking about oversmoothing is as follows. R epeatedly applying Laplacian smooth- ing too many times would drive node features to a stationary p oint, washing away all the information from these features. Let x·j ∈ Rn denote the j-th column of X. Then, for any x·j ∈ Rn: lim k→∞ ˜Ak symx·j = π j and π j ∥π j∥1 = π , (1) where the normalized solution π ∈ Rn satisﬁes π i = √degi∑ i √degi for all i ∈ [n]. Notice that π is independent of the values x·j of the input feature and is only a function of the graph struct ure (i.e., 2Published as a conference paper at ICLR 2020 degree). In other words, (Laplacian) oversmoothing washes away the signal from all the features, making them indistinguishable. W e will refer to this viewpo int as feature-wise oversmoothing. T o this end we propose two measures, row-diff and col-diff, t o quantify these two types of over- smoothing. Let H(k) ∈ Rn×d be the representation matrix after k graph convolutions, i.e. H(k) = ˜Ak symX. Let h(k) i ∈ Rd be the i-th row of H(k) and h(k) ·i ∈ Rn be the i-th column of H(k). Then we deﬁne row-diff( H(k)) and col-diff( H(k)) as follows. row-diff(H(k)) = 1 n2 ∑ i,j∈[n]   h(k) i − h(k) j    2 (2) col-diff(H(k)) = 1 d2 ∑ i,j∈[d]   h(k) ·i / ∥h(k) ·i ∥1 − h(k) ·j / ∥h(k) ·j ∥1    2 (3) The row-diff measure is the average of all pairwise distance s between the node features (i.e., rows of the representation matrix) and quantiﬁes node-wise oversm oothing, whereas col-diff is the average of pairwise distances between ( L1-normalized1) columns of the representation matrix and quantiﬁes feature-wise oversmoothing. 2.2 S T U DY IN G OV E RS M O OT H IN G W IT H SGC Although oversmoothing can be a cause of performance drop wi th increasing number of layers in GCN, adding more layers also leads to more parameters (due to learned linear projections W(k) at each layer k) which magnify the potential of overﬁtting. Furthermore, d eeper models also make the training harder as backpropagation suffers from vanishing gradients. In order to decouple the effect of oversmoothing from these o ther two factors, we study the over- smoothing problem using the SGC model (Wu et al., 2019). (Res ults on other GNNs are presented in §4.) SGC is simpliﬁed from GCN by removing all projection para meters of graph convolution layers and all nonlinear activations between layers. The es timation of SGC is simply written as: ˆY = softmax( ˜AK symX W ) (4) where K is the number of graph convolutions, and W ∈ Rd×c denote the learnable parameters of a logistic regression classiﬁer. Note that SGC has aﬁxed number of parameters that does not depend on the number of gra ph convolutions (i.e. layers). In effect, it is guarded agains t the inﬂuence of overﬁtting and vanishing gradient problem with more layers. This leaves us only with o versmoothing as a possible cause of performance degradation with increasing K. Interestingly, the simplicity of SGC does not seem to be a sacriﬁce; it has been observed that it achieves similar o r better accuracy in various relational classiﬁcation tasks (Wu et al., 2019). 0 20 40 Layers 0.5 1.0 1.5Loss train_loss val_loss test_loss 0 20 40 Layers 0.4 0.6 0.8 1.0Accuracy train_acc val_acc test_acc 0 20 40 Layers 2 4 6Distance row_diff 0 20 40 Layers 0.1 0.2 0.3 0.4Distance col_diff PairNorm Original Figure 1: (best in color) SGC’s performance (dashed lines) w ith increasing graph convolutions ( K) on Cora dataset (train/val/test split is 3%/10%/87%). For each K, we train SGC in 500 epochs, save the model with the best validation accuracy, and report all measures based on the saved model. Measures row-diff and col-diff are computed based on the ﬁna l layer representation of the saved model. (Solid lines depict after applying our method P A IR NO RM , which we discuss in §3.2.) Dashed lines in Figure 1 illustrate the performance of SGC on the Cora dataset as we increase the number of layers ( K). The training (cross-entropy) loss monotonically increa ses with larger K, potentially because graph convolution mixes node represen tations with their neighbors’ and makes them less distinguishable (training becomes harder). On th e other hand, graph convolutions (i.e., smoothing) improve generalization ability, reducing the g ap between training and validation/test loss 1 W e normalize each column j as the Laplacian smoothing stationary point π j is not scale-free. See Eq. (1). 3Published as a conference paper at ICLR 2020 up to K = 4, after which (over)smoothing begins to hurt performance. T he row-diff and col-diff both continue decreasing monotonically with K, providing supporting evidence for oversmoothing. 3 T ACKLING OVERSMOOTH IN G 3.1 P RO P O S E D PA IR NO RM W e start by establishing a connection between graph convolu tion and an optimization problem, that is graph-regularized least squares (GRLS), as shown by NT & M aehara (2019). Let ¯X ∈ Rn×d be a new node representation matrix, with ¯ xi ∈ Rd depicting the i-th row of ¯X. Then the GRLS problem is given as min ¯X ∑ i∈V ∥¯ xi − xi∥2 ˜D + ∑ (i,j)∈E ∥¯ xi − ¯ xj ∥2 2 (5) where ∥zi∥2 ˜D = zT i˜Dzi. The ﬁrst term can be seen as total degree-weighted least squ ares. The second is a graph-regularization term that measures the variation of the new features over the graph structure. The goal of the optimization problem can be state d as estimating new “denoised” features ¯ xi’s that are not too far off of the input features xi’s and are smooth over the graph structure. The GRLS problem has a closed form solution ¯X = (2I − ˜Arw )−1X, for which ˜Arw X is the ﬁrst- order T aylor approximation, that is ˜Arw X ≈ ¯X. By exchanging ˜Arw with ˜Asym we obtain the same form as the graph convolution, i.e., ˜X = ˜Asym X ≈ ¯X. As such, graph convolution can be viewed as an approximate solution of (5), where it minimizes the var iation over the graph structure while keeping the new representations close to the original. The optimization problem in (5) facilitates a closer look tothe oversmoothing problem of graph convolution. Ideally, we want to obtain smoothing over node s within the same cluster, however avoid smoothing over nodes from different clusters. The obj ective in (5) dictates only the ﬁrst goal via the graph-regularization term. It is thus prone to overs moothing when convolutions are applied repeatedly. T o circumvent the issue and fulﬁll both goals si multaneously, we can add a negative term such as the sum of distances between disconnected pairs as follows. min ¯X ∑ i∈V ∥¯ xi − xi∥2 ˜D + ∑ (i,j)∈E ∥¯ xi − ¯ xj ∥2 2 − λ ∑ (i,j)/∈E ∥¯ xi − ¯ xj ∥2 2 (6) where λ is a balancing scalar to account for different volume and imp ortance of the two goals. 2 By deriving the closed-form solution of (6) and approximati ng it with ﬁrst-order T aylor expansion, one can get a revised graph convolution operator with hyperp arameter λ. In this paper, we take a different route. Instead of a completely new graph convolut ion operator, we propose a general and efﬁcient “patch”, called P A IR NO RM , that can be applied to any form of graph convolution having the potential of oversmoothing. Let˜X (the output of graph convolution) and ˙X respectively be the input and output of P A IR NO RM . Observing that the output of graph convolution ˜X = ˜Asym X only achieves the ﬁrst goal, P A IR NO RM serves as a normalization layer that works on ˜X to achieve the second goal of keeping disconnected pair representations farther off. Speciﬁcally, P A IR NO RM normalizes ˜X such that the total pairwise squared distance TPSD ( ˙X) :=∑ i,j∈[n] ∥ ˙ xi − ˙ xj ∥2 2is the same as TPSD (X). That is,∑ (i,j)∈E ∥ ˙ xi − ˙ xj ∥2 2 + ∑ (i,j)/∈E ∥ ˙ xi − ˙ xj ∥2 2 = ∑ (i,j)∈E ∥xi − xj ∥2 2 + ∑ (i,j)/∈E ∥xi − xj ∥2 2 . (7) By keeping the total pairwise squared distance unchanged, the term ∑ (i,j)/∈E ∥ ˙ xi − ˙ xj ∥2 2is guar- anteed to be at least as large as the original value ∑ (i,j)/∈E ∥xi − xj ∥2 2 since the other term∑ (i,j)∈E ∥ ˙ xi − ˙ xj ∥2 2≈ ∑ (i,j)∈E ∥˜ xi − ˜ xj ∥2 2is shrunk through the graph convolution. In practice, instead of always tracking the original value T PSD(X), we can maintain a constant TPSD value C across all layers, where C is a hyperparameter that could be tuned per dataset. T o normalize ˜X to constant TPSD, we need to ﬁrst compute TPSD ( ˜X). Directly computing TPSD involves n2 pairwise distances that is O(n2d), which can be time consuming for large datasets. 2 There exist other variants of (6) that achieve similar goals , and we leave the space for future exploration. 4Published as a conference paper at ICLR 2020 Equivalently, normalization can be done via a two-step appr oach where TPSD is rewritten as 3 TPSD( ˜X) = ∑ i,j∈[n] ∥˜ xi − ˜ xj ∥2 2 = 2 n2 ( 1 n n∑ i=1 ∥˜ xi∥2 2− ∥ 1 n n∑ i=1 ˜ xi∥2 2 ) . (8) The ﬁrst term (ignoring the scale 2n2) in Eq. (8) represents the mean squared length of node representations, and the second term depicts the squared le ngth of the mean of node represen- tations. T o simplify the computation of (8), we subtract the row-wise mean from each ˜ xi, i.e., ˜ xc i = ˜ xi − 1 n ∑ n i˜ xi where ˜ xc idenotes the centered representation. Note that this shifti ng does not affect the TPSD, and furthermore drives the term ∥ 1 n ∑ n i=1 ˜ xi∥2 2to zero, where computing TPSD ( ˜X) boils down to calculating the squared Frobenius norm of ˜Xc and overall takes O(nd). That is, TPSD( ˜X) =TPSD( ˜Xc) = 2n∥ ˜Xc∥2 F . (9) In summary, our proposed P A IR NO RM (with input ˜X and output ˙X) can be written as a two-step, center-and-scale, normalization procedure: ˜ xc i= ˜ xi − 1 n n∑ i=1 ˜ xi (Center) (10) ˙ xi = s · ˜ xc i √ 1 n ∑ n i=1 ∥˜ xc i∥2 2 = s√ n · ˜ xc i √ ∥ ˜Xc∥2 F (Scale) (11) After scaling the data remains centered, that is, ∥ ∑ n i=1 ˙ xi∥2 2= 0. In Eq. (11), s is a hyperparameter that determines C. Speciﬁcally, TPSD( ˙X) = 2n∥ ˙X∥2 F= 2n ∑ i ∥s · ˜ xc i √ 1 n ∑ i ∥˜ xc i∥2 2 ∥2 2= 2n s2 1 n ∑ i ∥˜ xc i∥2 2 ∑ i ∥˜ xc i∥2 2= 2n2s2 (12) Then, ˙X := PA IR NO RM ( ˜X) has row-wise mean 0 (i.e., is centered) and constant total pairwise squared distance C = 2n2s2. An illustration of P A IR NO RM is given in Figure 2. The output of PA IR NO RM is input to the next convolution layer. graph conv center  PairNorm  rescale  X ˜X c˜X ˙X Figure 2: Illustration of P A IR NO RM , comprising centering and rescaling steps. 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GAT train_acc val_acc test_acc Figure 3: (best in color) Performance comparison of the original (dashed) vs. PA IR NO RM -enhanced (solid) GCN and GA T models with increasing layers on Cora. W e also derive a variant of P A IR NO RM by replacing ∑ n i=1 ∥˜ xc i∥2 2 in Eq. (11) with n∥˜ xc i∥2 2, such that the scaling step computes ˙ xi = s · ˜ xc i ∥˜ xc i∥2 . W e call it P A IR NO RM -S I (for Scale Individually), which imposes more re- striction on node representations, such that all have the sameL2-norm s. In practice we found that both P A IR NO RM and P A IR NO RM -S I work well for SGC, whereas P A IR NO RM -S I provides better and more stable results for GCN and GA T . The reason why GCN and GA T require stricter normalization may be because they have more parameters and are more prone to overﬁtting. In Appx. A.6 we p rovide additional measures to demonstrate why P A IR NO RM and P A IR NO RM -S I work. In all experiments, we employ P A IR NO RM for SGC and P A IR NO RM -S I for both GCN and GA T . PA IR NO RM is effective and efﬁcient in solving the oversmoothing prob lem of GNNs. As a general normalization layer, it can be used for any GNN. Solid lines i n Figure 1 present the performance 3 See Appendix A.1 for the detailed derivation. 5Published as a conference paper at ICLR 2020 of SGC on Cora with increasing number of layers, where we employ P A IR NO RM after each graph convolution layer, as compared to ‘vanilla’ versions. Simi larly, Figure 3 is for GCN and GA T (PA IR NO RM is applied after the activation of each graph convolution). Note that the performance decay with P A IR NO RM -at-work is much slower. (See Fig.s 5–6 in Appx. A.3 for other datasets.) While P A IR NO RM enables deeper models that are more robust to oversmoothing , it may seem odd that the overall test accuracy does not improve. In fact, the benchmark graph datasets often used in the literature require no more than 4 layers, after which p erformance decays (even if slowly). In the next section, we present a realistic use case setting for which deeper models are more likely to provide higher performance, where the beneﬁt of P A IR NO RM becomes apparent. 3.2 A C A S E WH E RE DE E P E R GNN S A RE BE N E FICIA L In general, oversmoothing gets increasingly more severe as the number of layers goes up. A task would beneﬁt from employing P A IR NO RM more if it required a large number of layers to achieve its best performance. T o this effect we study the “missing fe ature setting”, where a subset of the nodes lack feature vectors. Let M ⊆ V u be the set where ∀m ∈ M , xm = ∅, i.e., all of their features are missing. W e denote with p = |M|/ |Vu| the missing fraction. W e call this variant of the task as semi-supervised node classiﬁcation with missin g vectors (SSNC-MV). Intuitively, one would require a larger number of propagation steps (hence, a deeper GNN) to be able to “recover” effective feature representations for these nodes. SSNC-MV is a general and realistic problem that ﬁnds severalapplications in the real world. For example, the credit lending problem of identifying low- vs. high-risk customers (nodes) can be modeled as SSNC-MV where a large fraction of nodes do not exhi bit any meaningful features (e.g., due to low-volume activity). In fact, many graph-based clas siﬁcation tasks with the cold-start issue (entity with no history) can be cast into SSNC-MV. T o our know ledge, this is the ﬁrst work to study the SSNC-MV problem using GNN models. Figure 4 presents the performance of SGC, GCN, and GA T modelson Cora with increasing number of layers, where we remove feature vectors from all the unlab eled nodes, i.e. p = 1. The models with P A IR NO RM achieve a higher test accuracy compared to those without, wh ich they typically reach at a larger number of layers. (See Fig. 7 in Appx. A.4 for results on other datasets.) 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-SGC 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy cora-GAT train_acc val_acc test_acc Figure 4: (best in color) Comparison of ‘vanilla’ vs. P A IR NO RM -enhanced SGC, GCN, and GA T performance on Cora for p = 1. Green diamond symbols depict the layer at which validation accuracy peaks. P A IR NO RM boosts overall performance by enabling more robust deep GNN s. 4 E XPERIME NT S In section 3 we have shown the robustness of P A IR NO RM -enhanced models against increasing num- ber of layers in SSNC problem. In this section we design exten sive experiments to evaluate the effectiveness of P A IR NO RM under the SSNC-MV setting, over SGC, GCN and GA T models. 4.1 E X P E RIM E N T SE T U P Datasets. W e use 4 well-known benchmark datasets in GNN domain: Cora, Citeseer, Pubmed (Sen et al., 2008), and CoauthorCS (Shchur et al., 2018). Their statistics are reported in Appx . A.2. For Cora, Citeseer and Pubmed, we use the same dataset splits as Kipf & W elling (2017), wher e all nodes outside train and validation are used as test set. F or CoauthorCS, we randomly split all nodes into train/val/test as 3%/ 10%/ 87%, and keep the same split for all experiments. Models. W e use three different GNN models as our base model: SGC (Wu et al., 2019), GCN (Kipf & W elling, 2017), and GA T (V elickovic et al., 2018). W e compare our P A IR NO RM with residual connection method (He et al., 2016) over base model s (except SGC since there is no “resid- 6Published as a conference paper at ICLR 2020 ual connected” SGC), as we surprisingly ﬁnd it can slow down o versmoothing and beneﬁt SSNC- MV problem. Similar to us, residual connection is a general t echnique that can be applied to any model without changing its architecture. W e focus on the com parison between the base models and PA IR NO RM -enhanced models, rather than achieving the state of the art performance for SSNC and SSNC-MV. There exist a few other work addressing oversmooth ing (Klicpera et al., 2019; Li et al., 2018; Rong et al., 2019; Xu et al., 2018) however they design s pecialized architectures and not sim- ple “patch” procedures like P A IR NO RM that can be applied on top of any GNN. Hyperparameters. W e choose the hyperparameter s of P A IR NO RM from {0. 1, 1, 10, 50, 100} over validation set for SGC, while keeping it ﬁxed at s = 1for both GCN and GA T due to resource limitations. W e set the #hidden units of GCN and GA T (#attent ion heads is set to 1) to 32 and 64 respectively for all datasets. Dropout with rate 0. 6 and L2 regularization with penalty 5 · 10−4 are applied to GCN and GA T . For SGC, we vary number of layers in {1, 2, . . . 10, 15, . . . , 60} and for GCN and GA T in {2, 4, . . . , 12, 15, 20, . . . , 30}. Conﬁgurations. For P A IR NO RM -enhanced models, we apply P A IR NO RM after each graph convo- lution layer (i.e., after activation if any) in the base mode l. For residual-connected models with t skip steps, we connect the output of l-th layer to (l + t)-th, that is, H(l+t) new = H(l+t) + H(l) where H(l) denotes the output of l-th graph convolution (after activation). For the SSNC-MV s etting, we randomly erase p fraction of the feature vectors from nodes in validation and test sets (for which we input vector 0 ∈ Rd), whereas all training (labeled) nodes keep their original features (See 3.2). W e run each experiment within 1000 epochs 5 times and report the average performance. W e mainly use a single GTX-1080ti GPU, with some SGC experiments ran on an Intel i7-8700k CPU. 4.2 E X P E RIM E N T RE S U LT S W e ﬁrst show the global performance gain of applying P A IR NO RM to SGC for SSNC-MV under varying feature missing rates as shown in T able 1. P A IR NO RM -enhanced SGC performs similar or better over 0% missing, while it signiﬁcantly outperforms vanilla SGC for most other settings, especially for larger missing rates. #L denotes the best num ber of layers for the model that yields the largest average validation accuracy (over 5 runs), for w hich we report the average test accuracy (Acc). Notice the larger #L values for SGC-PN compared to van illa SGC, which shows the power of P A IR NO RM for enabling “deep” SGC models by effectively tackling over smoothing. Similar to Wu et al. (2019) who showed that the simple SGC mode l achieves comparable or better performance as other GNNs for various tasks, we found P A IR NO RM -enhanced SGC to follow the same trend when compared with P A IR NO RM -enhanced GCN and GA T , for all SSNC-MV settings. Due to its simplicity and extreme efﬁciency, we believe P A IR NO RM -enhanced SGC sets a strong baseline for the SSNC-MV problem. T able 1: Comparison of ‘vanilla’ vs. PA IR NO RM -enhanced SGC performance in Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with missing rate ranging from 0% to 100%. Showing test accuracy at #L (K in Eq. 4) layers, at which model achieves best validation acc uracy. Missing Percentage 0% 20% 40% 60% 80% 100% Dataset Method Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Cora SGC 0.815 4 0.806 5 0.786 3 0.742 4 0.733 3 0.423 15 SGC-PN 0.811 7 0.799 7 0.797 7 0.783 20 0.780 25 0.745 40 Citeseer SGC 0.689 10 0.684 6 0.668 8 0.657 9 0.565 8 0.290 2 SGC-PN 0.706 3 0.695 3 0.653 4 0.641 5 0.590 50 0.486 50 Pubmed SGC 0.754 1 0.748 1 0.723 4 0.746 2 0.659 3 0.399 35 SGC-PN 0.782 9 0.781 7 0.778 60 0.782 7 0.772 60 0.719 40 CoauthorCS SGC 0.914 1 0.898 2 0.877 2 0.824 2 0.751 4 0.318 2 SGC-PN 0.915 2 0.909 2 0.899 3 0.891 4 0.880 8 0.860 20 W e next employ P A IR NO RM -S I for GCN and GA T under the same setting, comparing it with the residual (skip) connections technique. Results are shown i n T able 2 and T able 3 respectively for GCN and GA T . Due to space and resource limitations, we only sh ow results for 0% and 100% miss- ing rate scenarios. (W e provide results for other missing ra tes ( 70, 80, 90%) over 1 run only in Appx. A.5.) W e observe similar trend for GCN and GA T: (1) vanilla mo del suffers from performance drop under SSNC-MV with increasing missing rate; (2) both residu al connections and P A IR NO RM -S I enable deeper models and improve performance (note the larg er #L and Acc); (3) GCN-PN and 7Published as a conference paper at ICLR 2020 GA T -PN achieve performance that is comparable or better tha n just using skips; (4) performance can be further improved (albeit slightly) by using skips alo ng with P A IR NO RM -S I.4 T able 2: Comparison of ‘vanilla’ and (P A IR NO RM -S I/ residual)-enhanced GCN performance on Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% feature missing rate. t represents the skip-step of residual connection. (See A.5 F ig. 8 for more settings.) Dataset Cora Citeseer Pubmed CoauthorCS Missing(%) 0% 100% 0% 100% 0% 100% 0% 100% Method Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L GCN 0.821 2 0.582 2 0.695 2 0.313 2 0.779 2 0.449 2 0.877 2 0.452 4 GCN-PN 0.790 2 0.731 10 0.660 2 0.498 8 0.780 30 0.745 25 0.910 2 0.846 12 GCN-t1 0.822 2 0.721 15 0.696 2 0.441 12 0.780 2 0.656 25 0.898 2 0.727 12 GCN-t1-PN 0.780 2 0.724 30 0.648 2 0.465 10 0.756 15 0.690 12 0.898 2 0.830 20 GCN-t2 0.820 2 0.722 10 0.691 2 0.432 20 0.779 2 0.645 20 0.882 4 0.630 20 GCN-t2-PN 0.785 4 0.740 30 0.650 2 0.508 12 0.770 15 0.725 30 0.911 2 0.839 20 T able 3: Comparison of ‘vanilla’ and (P A IR NO RM -S I/ residual)-enhanced GA T performance on Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% feature missing rate. t represents the skip-step of residual connection. (See A.5 F ig. 9 for more settings.) Dataset Cora Citeseer Pubmed CoauthorCS Missing(%) 0% 100% 0% 100% 0% 100% 0% 100% Method Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L GA T 0.823 2 0.653 4 0.693 2 0.428 4 0.774 6 0.631 4 0.892 4 0.737 4 GA T -PN 0.787 2 0.718 6 0.670 2 0.483 4 0.774 12 0.714 10 0.916 2 0.843 8 GA T -t1 0.822 2 0.706 8 0.693 2 0.461 6 0.769 4 0.698 8 0.899 4 0.842 10 GA T -t1-PN 0.787 2 0.710 10 0.658 6 0.500 10 0.757 4 0.684 12 0.911 2 0.844 20 GA T -t2 0.820 2 0.691 8 s0.692 2 0.461 6 0.774 8 0.702 8 0.895 4 0.803 6 GA T -t2-PN 0.788 4 0.738 12 0.672 4 0.517 10 0.776 15 0.704 12 0.917 2 0.855 30 5 R ELATED WORK Oversmoothing in GNNs: Li et al. (2018) was the ﬁrst to call attention to the oversmoo thing problem. Xu et al. (2018) introduced Jumping Knowledge Netw orks, which employ skip connec- tions for multi-hop message passing and also enable differe nt neighborhood ranges. Klicpera et al. (2019) proposed a propagation scheme based on personalized Pagerank that ensures locality (via teleports) which in turn prevents oversmoothing. Li et al. ( 2019) built on ideas from ResNet to use residual as well as dense connections to train deep GCNs. Dro pEdge Rong et al. (2019) proposed to alleviate oversmoothing through message passing reduct ion via removing a certain fraction of edges at random from the input graph. These are all specializ ed solutions that introduce additional parameters and/or a different network architecture. Normalization Schemes for Deep-NNs:There exist various normalization schemes proposed for deep neural networks, including batch normalization Ioffe & Szegedy (2015), weight normalization Salimans & Kingma (2016), layer normalization Ba et al. (201 6), and so on. Conceptually these have substantially different goals (e.g., reducing traini ng time), and were not proposed for graph neural networks nor the oversmoothing problem therein. Imp ortant difference to note is that larger depth in regular neural-nets does not translate to more hops of propagation on a graph structure. 6 C ONCLUSIO N W e investigated the oversmoothing problem in GNNs and propo sed P A IR NO RM , a novel normal- ization layer that boosts the robustness of deep GNNs agains t oversmoothing. P A IR NO RM is fast to compute, requires no change in network architecture nor any extra parameters, and can be applied to any GNN. Experiments on real-world classiﬁcation tasks sho wed the effectiveness of P A IR NO RM , where it provides performance gains when the task beneﬁts fr om more layers. Future work will explore other use cases of deeper GNNs that could further sho wcase P A IR NO RM ’s advantages. 4 Notice a slight performance drop when P AIR NORM is applied at 0% rate. For this setting, and the datasets we ha ve, shallow networks are sufﬁcient and smoothing through only a few (2-4) layers i mproves generalization ability for the SSNC problem (recal l Figure 1 solid lines). PAIR NOR M has a small reversing effect in these scenarios, hence the sm all performance drop. 8Published as a conference paper at ICLR 2020 REFERENC ES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer n ormalization. CoRR, abs/1607.06450, 2016. Olivier Chapelle, Bernhard Sch ¨ olkopf, and Alexander Zien . Semi-Supervised Learning . 2006. William L. Hamilton, Zhitao Y ing, and Jure Leskovec. Induct ive representation learning on large graphs. In NIPS, pp. 1024–1034, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep R esidual Learning for Image Recognition. In Proceedings of 2016 IEEE Conference on Computer V ision and P attern Recog- nition, pp. 770–778. IEEE, 2016. Sergey Ioffe and Christian Szegedy. Batch normalization: A ccelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. Thomas N. Kipf and Max W elling. Semi-supervised classiﬁcat ion with graph convolutional net- works. In International Conference on Learning Representations (IC LR). OpenReview .net, 2017. Johannes Klicpera, Aleksandar Bojchevski, and Stephan G ¨ u nnemann. Combining neural networks with personalized pagerank for classiﬁcation on graphs. In International Conference on Learning Representations (ICLR) , 2019. Guohao Li, Matthias M ¨ uller, Ali Thabet, and Bernard Ghanem . Can GCNs go as deep as CNNs? CoRR, abs/1904.03751, 2019. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper Insights int o Graph Convolutional Networks for Semi-Supervised Learning. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intell i- gence, pp. 3538–3545, 2018. Hoang NT and T akanori Maehara. Revisiting graph neural netw orks: All we have is low-pass ﬁlters. CoRR, abs/1905.09550, 2019. Meng Qu, Y oshua Bengio, and Jian T ang. Gmnn: Graph markov neu ral networks. In International Conference on Machine Learning , pp. 5241–5250, 2019. Y u Rong, W enbing Huang, Tingyang Xu, and Junzhou Huang. The t ruly deep graph convolutional networks for node classiﬁcation. CoRR, abs/1907.10903, 2019. Tim Salimans and Durk P Kingma. W eight normalization: A simp le reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems , pp. 901–909, 2016. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoo r, Brian Galligher, and Tina Eliassi-Rad. Collective classiﬁcation in network data. AI magazine , 29(3):93–93, 2008. Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevsk i, and Stephan G ¨ unnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868 , 2018. Petar V elickovic, Guillem Cucurull, Arantxa Casanova, Adr iana Romero, Pietro Li, and Y oshua Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR). OpenReview .net, 2018. Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fif ty, T ao Y u, and Kilian Q. W einberger. Simplifying graph convolutional networks. In ICML, volume 97 of Proceedings of Machine Learning Research , pp. 6861–6871. PMLR, 2019. Keyulu Xu, Chengtao Li, Y onglong Tian, T omohiro Sonobe, Ken -ichi Kawarabayashi, and Stefanie Jegelka. Representation Learning on Graphs with Jumping Kn owledge Networks. In Proceedings of the 35th International Conference on Machine Learning , volume 80, pp. 5453–5462, 2018. 9Published as a conference paper at ICLR 2020 A A PPENDIX A.1 D E RIV AT IO N O F EQ. 8 TPSD( ˜X) = ∑ i,j∈[n] ∥˜ xi − ˜ xj ∥2 2= ∑ i,j∈[n] (˜ xi − ˜ xj )T (˜ xi − ˜ xj ) (13) = ∑ i,j∈[n] (˜ xT i˜ xi + ˜ xT j˜ xj − 2˜ xT i˜ xj ) (14) = 2n ∑ i∈[n] ˜ xT i˜ xi − 2 ∑ i,j∈[n] ˜ xT i˜ xj (15) = 2n ∑ i∈[n] ∥˜ xi∥2 2− 21T ˜X ˜XT 1 (16) = 2n ∑ i∈[n] ∥˜ xi∥2 2− 2∥1T ˜X∥2 2 (17) = 2n2 ( 1 n n∑ i=1 ∥˜ xi∥2 2− ∥ 1 n n∑ i=1 ˜ xi∥2 2 ) . (18) A.2 D ATA S E T STAT IS T ICS T able 4: Dataset statistics. Name #Nodes #Edges #Features #Classes Label Rate Cora 2708 5429 1433 7 0.052 Citeseer 3327 4732 3703 6 0.036 Pubmed 19717 44338 500 3 0.003 CoauthorCS 18333 81894 6805 15 0.030 A.3 A D D IT IO NA L PE RF O RM A N CE PL OT S W IT H IN CRE A S IN G NU M BE R O F LAY E RS 0 20 40 Layers 0.5 1.0Loss train_loss val_loss test_loss 0 20 40 Layers 0.6 0.8 1.0Accuracy train_acc val_acc test_acc 0 20 40 Layers 4 6 8Distance row_diff 0 20 40 Layers 0.2 0.3 0.4Distance col_diff citeseer (random split: 3%/10%/87%) PairNorm Original 0 20 40 Layers 0.4 0.6 0.8 1.0Loss train_loss val_loss test_loss 0 20 40 Layers 0.4 0.6 0.8Accuracy train_acc val_acc test_acc 0 20 40 Layers 0 2 4Distance row_diff 0 20 40 Layers 0.02 0.04Distance col_diff pubmed  (random split: 3%/10%/87%) PairNorm Original 0 20 40 Layers 0.5 1.0 1.5Loss train_loss val_loss test_loss 0 20 40 Layers 0.4 0.6 0.8 1.0Accuracy train_acc val_acc test_acc 0 20 40 Layers 0.0 2.5 5.0 7.5 10.0Distance row_diff 0 20 40 Layers 0.00 0.05 0.10 0.15 0.20Distance col_diff coauthor_CS (random split: 3 %/10%/87%) PairNorm Original Figure 5: Comparison of ‘vanilla’ vs. P A IR NO RM -enhanced SGC, corresponding to Figure 1, for datasets (from top to bottom) Citeseer, Pubmed, and CoauthorCS. P A IR NO RM provides im- proved robustness to performance decay due to oversmoothin g with increasing number of layers. 10Published as a conference paper at ICLR 2020 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GAT train_acc val_acc test_acc 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GAT train_acc val_acc test_acc 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GAT train_acc val_acc test_acc Figure 6: Comparison of ‘vanilla’ (dashed) vs. P A IR NO RM -enhanced (solid) GCN (left) and GA T (right) models, corresponding to Figure 3, for datasets (fr om top to bottom) Citeseer, Pubmed, and CoauthorCS. P A IR NO RM provides improved robustness against performance decay wi th increasing number of layers. 11Published as a conference paper at ICLR 2020 A.4 A D D IT IO NA L PE RF O RM A N CE PL OT S W IT H IN CRE A S IN G NU M BE R O F LAY E RS U N D E R SSNC-MV W IT H p = 1 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-SGC 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy citeseer-GAT train_acc val_acc test_acc 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-SGC 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy pubmed-GAT train_acc val_acc test_acc 0 20 40 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-SGC 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GCN PairNorm(SI) Original 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Accuracy coauthor_CS-GAT train_acc val_acc test_acc Figure 7: Comparison of ‘vanilla’ (dashed) vs. P A IR NO RM -enhanced (solid) (from left to right) SGC, GCN, and GA T model performance under SSNC-MV for p = 1, corresponding to Figure 4, for datasets (from top to bottom) Citeseer, Pubmed, and CoauthorCS. Green diamond symbols depict the layer at which validation accuracy peaks. P A IR NO RM boosts overall performance by enabling more robust deep GNNs. 12Published as a conference paper at ICLR 2020 A.5 A D D IT IO NA L EX P E RIM E N T S U N D E R SSNC-MV W IT H IN CRE A S IN G MIS S IN G FRACT IO N p In this section we report additional experiment results und er the SSNC-MV setting with varying missing fraction, in particular p = {0. 7, 0. 8, 0. 9, 1} and also report the base case where p = 0 for comparison. Figure 8 presents results on all four datasets for GCN vs. PA IR NO RM -enhanced GCN (denoted PN for short). The models without any skip connections are de noted by *-0, with one-hop skip connection by *-1, and with one and two-hop skip connections by *-2. Barcharts on the right report the best layer that each model produced the highest validati on accuracy, and those on the left report the corresponding test accuracy. Figure 9 presents corresp onding results for GA T . W e discuss the take-aways from these ﬁgures on the following page. GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2 0.6 0.7 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc cora GCN GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2 0.4 0.6 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc citeseer GCN GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20.4 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc pubmed GCN GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2GCN-0 PN  -0GCN-1 PN  -1GCN-2 PN  -2 0.4 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc coauthor_CS GCN Figure 8: Supplementary results to T able 2 for GCN on (from to p to bottom) Cora, Citeseer, Pubmed, and CoauthorCS. 13Published as a conference paper at ICLR 2020 W e make the following observations based on Figures 8 and 9: • Performance of ‘vanilla’ GCN and GA T models without skip con nections (i.e., GCN-0 and GA T -0) drop monotonically as we increase missing fraction p. • PA IR NO RM -enhanced ‘vanilla’ models (PN-0, no skips) perform compar ably or better than GCN-0 and GA T -0 in all cases, especially as p increases. In other words, with P A IR NO RM at work, model performance is more robust against missing da ta. • Best number of layers for GCN-0 as we increase p only changes between 2-4. For GA T -0, it changes mostly between 2-6. • PA IR NO RM -enhanced ‘vanilla’ models (PN-0, no skips) can go deeper, i .e., they can lever- age a larger range of #layers (2-12) as we increase p. Speciﬁcally, GCN-PN-0 (GA T -PN-0) uses equal number or more layers than GCN-0 (GA T -0) in almost all cases. • Without any normalization, adding skip connections helps— GCN/GA T -1 and GCN/GA T -2 are better than GCN/GA T -0, especially as we increase p. • With P A IR NO RM but no-skip, performance is comparable or better than just a dding skips. • Adding skips on top of P A IR NO RM does not seem to introduce any notable gains. In summary, simply employing our P A IR NO RM for GCN and GA T provides robustness against oversmoothing that allows them to go deeper and achieve impr oved performance under SSNC-MV. GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc cora GAT GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.4 0.6 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc citeseer GAT GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.5 0.6 0.7 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc pubmed GAT GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -20 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2GAT-0 PN  -0GAT-1 PN  -1GAT-2 PN  -2 0.7 0.8 0.9 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc coauthor_CS GAT Figure 9: Supplementary results to T able 3 for GA T on (from to p to bottom) Cora, Citeseer, Pubmed, and CoauthorCS. 14Published as a conference paper at ICLR 2020 A.6 C A S E S T U DY : A D D IT IO NA L ME A S U RE S F O R PA IR NO RM A N D PA IR NO RM -SI W IT H SGC A N D GCN T o better understand why P A IR NO RM and P A IR NO RM -SI are helpful for training deep GNNs, we report additional measures for (SGC and GCN) with (P A IR NO RM and P A IR NO RM -SI) over the Cora dataset. In the main text, we claim TPSD (total pairwise squa red distances) is constant across layers for SGC with P A IR NO RM (for GCN/GA T this is not guaranteed because of the inﬂuence o f activation function and dropout layer). In this section we e mpirically measure pairwise (squared) distances for both SGC and GCN, with P A IR NO RM and P A IR NO RM -SI. A.6.1 SGC W IT H PA IR NO RM A N D PA IR NO RM -SI T o verify our analysis of P A IR NO RM for SGC, and understand how the variant of P A IR NO RM (PA IR NO RM -SI) works, we measure the average pairwise squared distanc e (APSD) as well as the average pairwise distance (APD) between the representatio ns for two categories of node pairs: (1) connected pairs (nodes that are directly connected in graph ) and (2) random pairs (uniformly ran- domly chosen among the node set). APSD of random pairs reﬂect s the TPSD, and APD of random pairs reﬂects the total pairwise distance (TPD). Under the h omophily assumption of the labels w .r.t. the graph structure, we want APD or APSD of connected pairs to be small while keeping APD or APSD of random pairs relatively large. The results are shown in Figure 10. Without normalization, SGC suffers from fast diminishing APD and APSD of random pairs. As we have proved, P A IR NO RM normalizes APSD to be constant across layers, however it does not normalize APD, which appears to d ecrease linearly with increasing num- ber of layers. Surprisingly, although P A IR NO RM -SI is not theoretically proved to have a constant APSD and APD, empirically it achieves more stable APSD and AP D than P A IR NO RM . W e were not able to prove this phenomenon mathematically, and leave it for further investigation. 0 10 20 30 40 50 0 10 20 30Average squared distance SGC connected pairs random pairs 0 10 20 30 40 50 Layers  0 1 2 3 4 5 6Average distance  0 10 20 30 40 50 0 10 20 30 SGC + PairNorm connected pairs random pairs 0 10 20 30 40 50 Layers  0 1 2 3 4 5 6 0 10 20 30 40 50 0 10 20 30 SGC + PairNorm-SI connected pairs random pairs 0 10 20 30 40 50 Layers  1 2 3 4 5 6 Dataset: cora Figure 10: Measuring average distance (squared and not-squ ared) between representations at each layer for SGC, SGC with P A IR NO RM , and SGC with P A IR NO RM -SI. The setting is the same with Figure 1 and they share the same performance. APD does not capture the full information of the distribution of pairwise distances. T o show how the distribution changes by increasing number of layers, we use T ensorboard to plot the histograms of pairwise distances, as shown in Figure 11. Comparing SGC a nd SGC with P A IR NO RM , adding PA IR NO RM keeps the left shift (shrinkage) of the distribution of rand om pair distances much slower than without normalization, while still sharing similar be havior of the distribution of connected pairwise distances. P A IR NO RM -SI seems to be more powerful in keeping the median and mean of the distribution of random pair distances stable, while “sp reading” the distribution out by increasing the variance. The performance of P A IR NO RM and P A IR NO RM -SI are similar, however it seems that PA IR NO RM -SI is more powerful in stabilizing TPD and TPSD. 15Published as a conference paper at ICLR 2020 distr. of random pair distances  Layers  Distance  Layers  SGC   SGC + PairNorm SGC + PairNorm-SI  distr. of connected pair distances distr. of connected pair di stances  distr. of random pair distances distr. of random pair distances  Distance Distance  Dataset:  Cora  distr. of connected pair distances  Figure 11: Measuring distribution of distances between rep resentations at each layer for SGC, SGC with P A IR NO RM , and SGC with P A IR NO RM -SI. Supplementary results for Figure 10. A.6.2 GCN W IT H PA IR NO RM A N D PA IR NO RM -SI 0 2 4 6 8 10 0.000 0.002 0.004 0.006 0.008 0.010Average squared distance GCN connected pairs random pairs 0 2 4 6 8 10 Layers  0.00 0.02 0.04 0.06 0.08 0.10Average distance  0 2 4 6 8 10 0 20 40 60 80 100 120 GCN + PairNorm connected pairs random pairs 0 2 4 6 8 10 Layers  2 4 6 8 10 0 2 4 6 8 10 0.0 0.2 0.4 0.6 0.8 GCN + PairNorm-SI connected pairs random pairs 0 2 4 6 8 10 Layers  0.2 0.4 0.6 0.8 Dataset: cora Figure 12: Measuring average distance (squared and not-squ ared) between representations at each layer for GCN, GCN with P A IR NO RM , and GCN with P A IR NO RM -SI. W e trained three 12-layer GCNs with #hidden=128 and dropout=0.6 in 1000 epochs. Respe ctive test set accuracies are 31.09%, 77.77%, 75.09%. Note that the scale of distances is n ot comparable across models, since they have learnable parameters that scale these distances d ifferently. The formal analysis for P A IR NO RM and P A IR NO RM -SI is based on SGC. GCN (and other GNNs) has learnable parameters, dropout layers, and activation l ayers, all of which complicate direct math- ematical analyses. Here we perform similar empirical measu rements for pairwise distances to get a rough sense of how P A IR NO RM and P A IR NO RM -SI work with GCN based on the Cora dataset. Figures 12 and 13 demonstrate how P A IR NO RM and P A IR NO RM -SI can help train a relatively deep (12 layers) GCN. Notice that oversmoothing occurs very quickly for GCN without any normalization, where both con- nected and random pair distances reach zero (!). In contrast , GCN with P A IR NO RM or P A IR NO RM - SI is able to keep random pair distances relatively apart whi le allowing connected pair distances to shrink. As also stated in main text, using P A IR NO RM -SI for GCN and GA T is relatively more 16Published as a conference paper at ICLR 2020 stable than using P A IR NO RM in general cases (notice the near-constant random pair dist ances in the rightmost subﬁgures). There are several possible expla nations for why P A IR NO RM -SI is more stable. First, as shown in Figure 10 and Figure 12, P A IR NO RM -SI not only keeps APSD stable but also APD, further, the plots of distributions of pairwise di stances (Figures 11 and 13) also show the power of P A IR NO RM -SI (notice the large gap between smaller connected pairwis e distances and the larger random pairwise distances). Second, we conjecture t hat restricting representations to reside on a sphere can make training stable and faster, which we also observe empirically by studying the training curves. Third, GCN and GA T tend to overﬁt easily for the SSNC problem, due to many learnable parameters across layers and limited labeled inp ut data, therefore it is possible that adding more restriction on these models helps reduce overﬁtting. GCN GCN + PairNorm GCN + PairNorm-SI Dataset:  Cora  distr. of connected pair distances distr. of connected pair di stances distr. of connected pair distances  distr. of random pair distances distr. of random pair distances d istr. of random pair distances  Layers  Distance  Layers  Distance Distance  Figure 13: Measuring distribution of distances between rep resentations at each layer for GCN, GCN with P A IR NO RM , and GCN with P A IR NO RM -SI. Supplementary results for Figure 12. All in all, these empirical measurements as illustrated thr oughout the ﬁgures in this section demon- strates that P A IR NO RM and P A IR NO RM -SI successfully address the oversmoothing problem for deep GNNs. Our work is the ﬁrst to propose a normalization layer speciﬁcally designed for graph neural networks, which we hope will kick-start more work in t his area toward training more robust and effective GNNs. 17",
      "meta_data": {
        "arxiv_id": "1909.12223v2",
        "authors": [
          "Lingxiao Zhao",
          "Leman Akoglu"
        ],
        "published_date": "2019-09-26T16:20:37Z",
        "pdf_url": "https://arxiv.org/pdf/1909.12223v2.pdf"
      }
    },
    {
      "title": "A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks",
      "abstract": "Oversmoothing is a central challenge of building more powerful Graph Neural\nNetworks (GNNs). While previous works have only demonstrated that oversmoothing\nis inevitable when the number of graph convolutions tends to infinity, in this\npaper, we precisely characterize the mechanism behind the phenomenon via a\nnon-asymptotic analysis. Specifically, we distinguish between two different\neffects when applying graph convolutions -- an undesirable mixing effect that\nhomogenizes node representations in different classes, and a desirable\ndenoising effect that homogenizes node representations in the same class. By\nquantifying these two effects on random graphs sampled from the Contextual\nStochastic Block Model (CSBM), we show that oversmoothing happens once the\nmixing effect starts to dominate the denoising effect, and the number of layers\nrequired for this transition is $O(\\log N/\\log (\\log N))$ for sufficiently\ndense graphs with $N$ nodes. We also extend our analysis to study the effects\nof Personalized PageRank (PPR), or equivalently, the effects of initial\nresidual connections on oversmoothing. Our results suggest that while PPR\nmitigates oversmoothing at deeper layers, PPR-based architectures still achieve\ntheir best performance at a shallow depth and are outperformed by the graph\nconvolution approach on certain graphs. Finally, we support our theoretical\nresults with numerical experiments, which further suggest that the\noversmoothing phenomenon observed in practice can be magnified by the\ndifficulty of optimizing deep GNN models.",
      "full_text": "A NON -ASYMPTOTIC ANALYSIS OF OVERSMOOTHING IN GRAPH NEURAL NETWORKS Xinyi Wu1,2, Zhengdao Chen3*, William Wang2, and Ali Jadbabaie1,2 1Institute for Data, Systems, and Society (IDSS), Massachusetts Institute of Technology 2Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology 3Courant Institute of Mathematical Sciences, New York University ABSTRACT Oversmoothing is a central challenge of building more powerful Graph Neural Networks (GNNs). While previous works have only demonstrated that oversmoothing is inevitable when the number of graph convolutions tends to inﬁnity, in this paper, we precisely characterize the mechanism behind the phenomenon via a non-asymptotic analysis. Speciﬁcally, we distinguish between two different effects when applying graph convolutions—an undesirable mixing effect that homogenizes node representations in different classes, and a desirable denoising effect that homogenizes node representations in the same class. By quantifying these two effects on random graphs sampled from the Contextual Stochastic Block Model (CSBM), we show that oversmoothing happens once the mixing effect starts to dominate the denoising effect, and the number of layers required for this transition is O(log N/log(log N)) for sufﬁciently dense graphs with N nodes. We also extend our analysis to study the effects of Personalized PageRank (PPR), or equivalently, the effects of initial residual connections on oversmoothing. Our results suggest that while PPR mitigates oversmoothing at deeper layers, PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs. Finally, we support our theoretical results with numerical experiments, which further suggest that the oversmoothing phenomenon observed in practice can be magniﬁed by the difﬁculty of optimizing deep GNN models. 1 Introduction Graph Neural Networks (GNNs) are a powerful framework for learning with graph-structured data and have shown great promise in diverse domains such as molecular chemistry, physics, and social network analysis [1, 2, 3, 4, 5, 6, 7]. Most GNN models are built by stacking graph convolutions or message-passing layers [8], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes. The most representative and popular example is the Graph Convolutional Network (GCN) [9], which has demonstrated success in node classiﬁcation, a primary graph task which asks for node labels and identiﬁes community structures in real graphs. Despite these achievements, the choice of depth for these GNN models remains an intriguing question. GNNs often achieve optimal classiﬁcation performance when networks are shallow. Many widely used GNNs such as the GCN are no deeper than 4 layers [9, 10], and it has been observed that for deeper GNNs, repeated message-passing makes node representations in different classes indistinguishable and leads to lower node classiﬁcation accuracy—a phenomenon known as oversmoothing [9, 11, 12, 10, 13, 14, 15, 16]. Through the insight that graph convolutions can be regarded as low-pass ﬁlters on graph signals, prior studies have established that oversmoothing is inevitable when the number of layers in a GNN increases to inﬁnity [11, 13]. However, these asymptotic analyses do not fully explain the rapid occurrence of oversmoothing when we increase the network depth, let alone the fact that for some datasets, having no graph convolution is even optimal [17]. These observations motivate the following key questions about oversmoothing in GNNs: Why does oversmoothing happen at a relatively shallow depth? Can we quantitatively model the effect of applying a ﬁnite number of graph convolutions and theoretically predict the “sweet spot” for the choice of depth? In this paper, we propose a non-asymptotic analysis framework to study the effects of graph convolutions and oversmoothing using the Contextual Stochastic Block Model (CSBM) [18]. The CSBM mimics the community structure Correspondence to: xinyiwu@mit.edu *Now at Google. arXiv:2212.10701v2  [cs.LG]  1 Mar 2023A B Figure 1: Illustration of how oversmoothing happens. Stacking GNN layers will increase both the mixing and denoising effects counteracting each other. Depending on the graph characteristics, either the denoising effect dominates the mixing effect, resulting in less difﬁculty classifying nodes ( A), or the mixing effect dominates the denoising effect, resulting in more difﬁculty classifying nodes (B)—this is when oversmoothing starts to happen. of real graphs and enables us to evaluate the performance of linear GNNs through the probabilistic model with ground truth community labels. More importantly, as a generative model, the CSBM gives us full control over the graph structure and allows us to analyze the effect of graph convolutions non-asymptotically. In particular, we distinguish between two counteracting effects of graph convolutions: • mixing effect (undesirable): homogenizing node representations in different classes; • denoising effect (desirable): homogenizing node representations in the same class. Adding graph convolutions will increase both the mixing and denoising effects. As a result, oversmoothing happens not just because the mixing effect keeps accumulating as the depth increases, on which the asymptotic analyses are based [11, 13], but rather because the mixing effect starts to dominate the denoising effect (see Figure 1 for a schematic illustration). By quantifying both effects as a function of the model depth, we show that the turning point of the tradeoff between the two effects is O(log N/log(log N)) for graphs with N nodes sampled from the CSBM in sufﬁciently dense regimes. Besides new theory, this paper also presents numerical results directly comparing theoretical predictions and empirical results. This comparison leads to new insights highlighting the fact that the oversmoothing phenomenon observed in practice is often a mixture of pure oversmoothing and difﬁculty of optimizing weights in deep GNN models. In addition, we apply our framework to analyze the effects of Personalized PageRank (PPR) on oversmoothing. Personalized propagation of neural predictions (PPNP) and its approximate variant (APPNP) make use of PPR and its approximate variant, respectively, and were proposed as a solution to mitigate oversmoothing while retaining the ability to aggregate information from larger neighborhoods in the graph [12]. We show mathematically that PPR makes the model performance more robust to increasing number of layers by reducing the mixing effect at each layer, while it nonetheless reduces the desirable denoising effect at the same time. For graphs with a large size or strong community structure, the reduction of the denoising effect would be greater than the reduction of the mixing effect and thus PPNP and APPNP would perform worse than the baseline GNN on those graphs. Our contributions are summarized as follows: • We show that adding graph convolutions strengthens the denoising effect while exacerbates the mixing effect. Over- smoothing happens because the mixing effect dominates the denoising effect beyond a certain depth. For sufﬁciently dense CSBM graphs with N nodes, the required number of layers for this to happen is O(log N/log(log N)). • We apply our framework to rigorously characterize the effects of PPR on oversmoothing. We show that PPR reduces both the mixing effect and the denoising effect of message-passing and thus does not necessarily improve node classiﬁcation performance. • We verify our theoretical results in experiments. Through comparison between theory and experiments, we ﬁnd that the difﬁculty of optimizing weights in deep GNN architectures often aggravates oversmoothing. 2 Additional Related Work Oversmoothing problem in GNNs Oversmoothing is a well-known issue in deep GNNs, and many techniques have been proposed to relieve it practically [19, 20, 15, 21, 22]. On the theory side, by viewing GNN layers as a form of Laplacian ﬁlter, prior works have shown that as the number of layers goes to inﬁnity, the node representations within each connected component of the graph will converge to the same values [11, 13]. However, oversmoothing can be observed in GNNs with as few as 2 −4 layers [9, 10]. The early onset of oversmoothing renders it an important concern in practice, and it has not been satisfyingly explained by the previous asymptotic studies. Our work addresses this 2gap by quantifying the effects of graph convolutions as a function of model depth and justifying why oversmoothing happens in shallow GNNs. A recent study shared a similar insight of distinguishing between two competing effects of message-passing and showed the existence of an optimal number of layers for node prediction tasks on a latent space random graph model. But the result had no further quantiﬁcation and hence the oversmoothing phenomemon was still only characterized asymptotically [16]. Analysis of GNNs on CSBMs Stochastic block models (SBMs) and their contextual counterparts have been widely used to study node classiﬁcation problems [23, 24]. Recently there have been several works proposing to use CSBMs to theoretically analyze GNNs for the node classiﬁcation task. [ 25] used CSBMs to study the function of nonlinearity on the node classiﬁcation performance, while [26] used CSBMs to study the attention-based GNNs. More relevantly, [27, 28] showed the advantage of applying graph convolutions up to three times for node classiﬁcation on CSBM graphs. Nonetheless, they only focused on the desirable denoising effect of graph convolution instead of its tradeoff with the undesirable mixing effect, and therefore did not explain the occurance of oversmoothing. 3 Problem Setting and Main Results We ﬁrst introduce our theoretical analysis setup using the Contextual Stochastic Block Model (CSBM), a random graph model with planted community structure [18, 27, 28, 29, 25, 26]. We choose the CSBM to study GNNs for the node classiﬁcation task because the main goal of node classiﬁcation is to discover node communities from the data. The CSBM imitates the community structure of real graphs and provides a clear ground truth for us to evaluate the model performance. Moreover, it is a generative model which gives us full control of the data to perform a non-asymptotic analysis. We then present a set of theoretical results establishing bounds for the representation power of GNNs in terms of the best-case node classiﬁcation accuracy. The proofs of all the theorems and additional claims will be provided in the Appendix. 3.1 Notations We represent an undirected graph with N nodes by G= (A,X), where A∈{0,1}N×N is the adjacency matrix and X ∈RN is the node feature vector. For nodes u,v ∈[N], Auv = 1 if and only if uand vare connected with an edge in G, and Xu ∈R represents the node feature of u. We let 1 N denote the all-one vector of length N and D= diag(A1 N) be the degree matrix of G. 3.2 Theoretical Analysis Framework Contextual Stochastic Block Models We will focus on the case where the CSBM consists of two classes C1 and C2 of nodes of equal size, in total with N nodes. For any two nodes in the graph, if they are from the same class, they are connected by an edge independently with probability p, or if they are from different classes, the probability is q. For each node v∈Ci,i ∈{1,2}, the initial feature Xv is sampled independently from a Gaussian distribution N(µi,σ2), where µi ∈R,σ ∈(0,∞). Without loss of generality, we assume that µ1 <µ2. We denote a graph generated from such a CSBM as G(A,X) ∼CSBM(N,p,q,µ 1,µ2,σ2). We further impose the following assumption on the CSBM used in our analysis. Assumption 1. p,q = ω(log N/N) and p>q > 0. The choice p,q = ω(log N/N) ensures that the generated graph Gis connected almost surely [ 23] while being slightly more general than the p,q = ω(log2 N/N) regime considered in some concurrent works [27, 25]. In addition, this regime also guarantees that Ghas a small diameter. Real-world graphs are known to exhibit the “small-world\" phenomenon—even if the number of nodes N is very large, the diameter of graph remains small [ 30, 31]. We will see in the theoretical analysis (Section 3.3) how this small-diameter characteristic contributes to the occurrence of oversmoothing in shallow GNNs. We remark that our results in fact hold for the more general choice of p,q = Ω(log N/N), for which only the concentration bound in Theorem 1 needs to be modiﬁed in the threshold log N/N case where all the constants need a more careful treatment. Further, the choice p > qensures that the graph structure has homophily, meaning that nodes from the same class are more likely to be connected than nodes from different classes. This characteristic is observed in a wide range of real-world graphs [32, 29]. We note that this homophily assumption (p>q ) is not essential to our analysis, though we add it for simplicity since the discussion of homophily versus heterophily (p<q ) is not the focus of our paper. Graph convolution and linear GNN In this paper, our theoretical analysis focuses on the simpliﬁed linear GNN model deﬁned as follows: a graph convolution using the (left-)normalized adjacency matrix takes the operation 3h′= (D−1A)h, where hand h′are the input and output node representations, respectively. A linear GNN layer can then be deﬁned as h′= (D−1A)hW, where W is a learnable weight matrix. As a result, the output of nlinear GNN layers can be written as h(n) ∏n k=1 W(k), where h(n) = (D−1A)nX is the output of ngraph convolutions, and W(k) is the weight matrix of the kth layer. Since this is linear in h(n), it follows that n-layer linear GNNs have the equivalent representation power as linear classiﬁers applied to h(n). In practice, when building GNN models, nonlinear activation functions can be added between consecutive linear GNN layers. For additional results showing that adding certain nonlinearity would not improve the classiﬁcation performance, see Appendix K.1. Bayes error rate and z-score Thanks to the linearity of the model, we see that the representation of node v ∈Ci after ngraph convolutions is distributed as N(µ(n) i ,(σ(n))2), where the variance (σ(n))2 is shared between classes. The optimal node-wise classiﬁer in this case is the Bayes optimal classiﬁer, given by the following lemma. Lemma 1. Suppose the label y is drawn uniformly from {1,2}, and given y, x ∼ N(µ(n) y ,(σ(n))2). Then the Bayes optimal classiﬁer, which minimizes the probability of misclassiﬁcation among all classiﬁers, has decision boundary D= (µ1 + µ2)/2, and predicts y = 1,if x ≤D or y = 2,if x >D. The associated Bayes error rate is 1 −Φ(z(n)), where Φ denotes the cumulative distribution function of the standard Gaussian distribution and z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) is the z-score of Dwith respect to N(µ(n) 1 ,(σ(n))2). Lemma 1 states that we can estimate the optimal performance of an n-layer linear GNN through the z-score z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n). A higher z-score indicates a smaller Bayes error rate, and hence a better expected performance of node classiﬁcation. The z-score serves as a basis for our quantitative analysis of oversmoothing. In the following section, by estimating µ(n) 2 −µ(n) 1 and (σ(n))2, we quantify the two counteracting effects of graph convolutions and obtain bounds on the z-score z(n) as a function of n, which allows us to characterize oversmoothing quantitatively. Speciﬁcally, there are two potential interpretations of oversmoothing based on the z-score: (1) z(n) <z(n⋆), where n⋆ = arg maxn′z(n′); and (2) z(n) <z (0). They correspond to the cases (1) n>n ⋆; and (2) n>n 0, where n0 ≥0 corresponds to the number of layers that yield a z-score on par with z(0). The bounds on the z-score z(n), z(n) lower and z(n) upper, enable us to estimate n⋆ and n0 under different scenarios and provide insights into the optimal choice of depth. 3.3 Main Results We ﬁrst estimate the gap between the means µ(n) 2 −µ(n) 1 with respect to the number of layers n. µ(n) 2 −µ(n) 1 measures how much node representations in different classes have homogenized afternGNN layers, which is the undesirable mixing effect. Lemma 2. For n∈N ∪{0}, assuming D−1A≈E[D]−1E[A], µ(n) 2 −µ(n) 1 = (p−q p+ q )n (µ2 −µ1) . Lemma 2 states that the means µ(n) 1 and µ(n) 2 get closer exponentially fast and as n →∞, both µ(n) 1 and µ(n) 2 will converge to the same value (in this case ( µ(n) 1 + µ(n) 2 ) /2). The rate of change (p−q)/(p+ q) is determined by the intra-community edge density pand the inter-community edge density q. Lemma 2 suggests that graphs with higher inter-community density (q) or lower intra-community density (p) are expected to suffer from a higher mixing effect when we perform message-passing. We provide the following concentration bound for our estimate of µ(n) 2 −µ(n) 1 , which states that the estimate concentrates at a rate of O(1/ √ N(p+ q)). Theorem 1. Fix K ∈N and r> 0. There exists a constant C(r,K) such that with probability at least 1 −O(1/Nr), it holds for all 1 ≤k≤Kthat |(µ(k) 2 −µ(k) 1 ) − (p−q p+ q )k (µ2 −µ1)|≤ C√ N(p+ q) . 4We then study the variance (σ(n))2 with respect to the number of layers n. The variance (σ(n))2 measures how much the node representations in the same class have homogenized, which is the desirable denoising effect. We ﬁrst state that no matter how many layers are applied, there is a nontrivial ﬁxed lower bound for (σ(n))2 for a graph with N nodes. Lemma 3. For all n∈N ∪{0}, 1 Nσ2 ≤(σ(n))2 ≤σ2 . Lemma 3 implies that for a given graph, even as the number of layers ngoes to inﬁnity, the variance (σ(n))2 does not converge to zero, meaning that there is a ﬁxed lower bound for the denoising effect. See Appendix K.2 for the exact theoretical limit for the variance (σ(n))2 as ngoes to inﬁnity. We now establish a set of more precise upper and lower bounds for the variance (σ(n))2 with respect to the number of layers nin the following technical lemma. Lemma 4. Let a= Np/log N. With probability at least 1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 (σ(n))2 ≤min    ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k ,1   σ2 . Lemma 4 holds for all 1 ≤n≤N and directly leads to the following theorem with a clariﬁed upper bound where nis bounded by a constant K. Theorem 2. Let a = Np/log N. Fix K ∈N. There exists a constant C(K) such that with probability at least 1 −O(1/N), it holds for all 1 ≤n≤Kthat max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 ≤min { C min{a,2} 1 (N(p+ q))n,1 } σ2 . Theorem 2 states that the variance (σ(n))2 for each Gaussian distribution decreases more for larger graphs or denser graphs. Moreover, the upper bound implies that the variance (σ(n))2 will initially go down at least at a rate expo- nential in O(1/log N) before reaching the ﬁxed lower bound σ2/N suggested by Lemma 3. This means that after O(log N/log(log N)) layers, the desirable denoising effect homogenizing node representations in the same class will saturate and the undesirable mixing effect will start to dominate. Why does oversmoothing happen at a shallow depth? For each node, message-passing with different-class nodes homogenizes their representations exponentially. The exponential rate depends on the fraction of different-class neighbors among all neighbors (Lemma 2, mixing effect). Meanwhile, message-passing with nodes that have not been encountered before causes the denoising effect, and the magnitude depends on the absolute number of newly encountered neighbors. The diameter of the graph is approximately log N/log(Np) in the p,q = Ω(log N/N) regime [33], and thus is at most log N/log(log N) in our case. After the number of layers surpasses the diameter, for each node, there will be no nodes that have not been encountered before in message-passing and hence the denoising effect will almost vanish (Theorem 2, denoising effect). log N/log(log N) grows very slowly with N; for example, when N = 106,log N/log(log N) ≈8. This is why even in a large graph, the mixing effect will quickly dominate the denoising effect when we increase the number of layers, and so oversmoothing is expected to happen at a shallow depth1. Our theory suggests that the optimal number of layers, n⋆, is at most O(log N/log(log N)). For a more quantitative estimate, we can use Lemma 2 and Theorem 2 to compute bounds z(n) lower and z(n) upper for z= 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) and use them to infer n⋆ and n0, as deﬁned in Section 3.2. See Appendix H for detailed discussion. Next, we investigate the effect of increasing the dimension of the node features X. So far, we have only considered the case with one-dimensional node features. The following proposition states that if features in each dimension are independent, increasing input feature dimension decreases the Bayes error rate for a ﬁxed n. The intuition is that when node features provide more evidence for classiﬁcation, it is easier to classify nodes correctly. Proposition 1. Let the input feature dimension be d, X ∈RN×d. Without loss of generality, suppose for node vin Ci, initial node feature Xv ∼N([µi]d,σ2Id) independently. Then the Bayes error rate is 1 −Φ (√ d 2 (µ(n) 2 −µ(n) 1 ) σ(n) ) = 1 −Φ (√ d 2 z(n) ) , where Φ denotes the cumulative distribution function of the standard Gaussian distribution. Hence the Bayes error rate is decreasing in d, and as d→∞, it converges to 0. 1For mixing and denoising effects in real data, see Appendix K.5. 54 The effects of Personalized PageRank on oversmoothing Our analysis framework in Section 3.3 can also be applied to GNNs with other message-passing schemes. Speciﬁcally, we can analyze the performance of Personalized Propagation of Neural Predictions (PPNP) and its approximate variant, Approximate PPNP (APPNP), which were proposed for alleviating oversmoothing while still making use of multi-hop information in the graph. The main idea is to use Personalized PageRank (PPR) or the approximate Personalized PageRank (APPR) in place of graph convolutions [12]. Mathematically, the output of PPNP can be written as hPPNP = α(IN −(1 −α)(D−1A))−1X, while APPNP computes hAPPNP(n+1) = (1 −α)(D−1A)hAPPNP(n) + αX iteratively in n, where IN is the identity matrix of size N and in both cases αis the teleportation probability. Then for nodes in Ci,i ∈{1,2}, the node representations follow a Gaussian distribution N ( µPPNP i ,(σPPNP)2 ) after applying PPNP, or a Gaussian distributionN ( µAPPNP(n) i ,(σAPPNP(n))2 ) after applying nAPPNP layers. We quantify the effects on the means and variances for PPNP and APPNP in the CSBM case. We can similarly use them to calculate the z-score of (µ1 + µ2)/2 and compare it to the one derived for the baseline GNN in Section 3. The key idea is that the PPR propagation can be written as a weighted average of the standard message-passing, i.e. α(IN −(1 −α)(D−1A))−1 = ∑∞ k=0(1 −α)k(D−1A)k [34]. We ﬁrst state the resulting mixing effect measured by the difference between the two means. Proposition 2. Fix r> 0,K ∈N. For PPNP , with probability at least1−O(1/Nr), there exists a constantC(α,r,K ) such that µPPNP 2 −µPPNP 1 = p+ q p+ 2−α α q(µ2 −µ1) + ϵ. where the error term |ϵ|≤ C/ √ N(p+ q) + (1−α)K+1. Proposition 3. Let r> 0. For APPNP , with probability at least1 −O(1/Nr), µAPPNP(n) 2 −µAPPNP(n) 1 = ( p+ q p+ 2−α α q + (2 −2α)q αp+ (2 −α)q(1 −α)n (p−q p+ q )n) (µ2 −µ1) + ϵ. where the error term ϵis the same as the one deﬁned in Theorem 1 for the case of K = n. Both p+q p+ 2−α α q and (2−2α)q αp+(2−α)q(1 −α) ( p−q p+q ) are monotone increasing in α. Hence from Proposition 2 and 3, we see that with larger α, meaning a higher probability of teleportation back to the root node at each step of message-passing, PPNP and APPNP will indeed make the difference between the means of the two classes larger: while the difference in means for the baseline GNN decays as (p−q p+q )n , the difference for PPNP/APPNP is lower bounded by a constant. This validates the original intuition behind PPNP and APPNP that compared to the baseline GNN, they reduce the mixing effect of message-passing, as staying closer to the root node means aggregating less information from nodes of different classes. This advantage becomes more prominent when the number of layers nis larger, where the model performance is dominated by the mixing effect: as ntends to inﬁnity, while the means converge to the same value for the baseline GNN, their separation is lower-bounded for PPNP/APPNP. However, the problem with the previous intuition is that PPNP and APPNP will also reduce the denoising effect at each layer, as staying closer to the root node also means aggregating less information from new nodes that have not been encountered before. Hence, for an arbitrary graph, the result of the tradeoff after the reduction of both effects is not trivial to analyze. Here, we quantify the resulting denoising effect for CSBM graphs measured by the variances. We denote (σ(n))2 upper as the variance upper bound for depth nin Lemma 4. Proposition 4. For PPNP , with probability at least1 −O(1/N), it holds for all 1 ≤K ≤N that max {α2 min{a,2} 10 , 1 N } σ2 ≤(σPPNP)2 ≤max   α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 ,σ2   . Proposition 5. For APPNP , with probability at least1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) , 1 N } σ2 ≤(σAPPNP(n))2 , (σAPPNP(n))2 ≤min    ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 ,σ2   . 6By comparing the lower bounds in Proposition 4 and 5 with that in Theorem 2, we see that PPR reduces the beneﬁcial denoising effect of message-passing: for large or dense graphs, while the variances for the baseline GNN decay as 1/(Np)n, the variances for PPNP/APPNP are lower bounded by the constant α2 min{a,2}/10. In total, the mixing effect is reduced by a factor of (p−q p+q )n , while the denoising effect is reduced by a factor of 1/(Np)n. Hence PPR would cause greater reduction in the denoising effect than the improvement in the mixing effect for graphs whereN and pare large. This drawback would be especially notable at a shallow depth, where the denoising effect is supposed to dominate the mixing effect. APPNP would perform worse than the baseline GNN on these graphs in terms of the optimal classiﬁcation performance. We remark that in each APPNP layer, another way to interpret the termαX is to regard it as a residual connection to the initial representation X [15]. Thus, our theory also validates the empirical observation that adding initial residual connections allows us to build very deep models without catastrophic oversmoothing. However, our results suggest that initial residual connections do not guarantee an improvement in model performance by themselves. 5 Experiments In this section, we ﬁrst demonstrate our theoretical results in previous sections on synthetic CSBM data. Then we discuss the role of optimizing weights W(k) in GNN layers in the occurrence of oversmoothing through both synthetic data and the three widely used benchmarks: Cora, CiteSeer and PubMed [35]. Our results highlight the fact that the oversmoothing phenomenon observed in practice may be exacerbated by the difﬁculty of optimizing weights in deep GNN models. More details about the experiments are provided in Appendix J. 5.1 The effect of graph topology on oversmoothing We ﬁrst show how graph topology affects the occurrence of oversmoothing and the effects of PPR. We randomly generated synthetic graph data from CSBM( N = 2000 , p, q = 0 .0038, µ1 = 1 , µ2 = 1 .5, σ2 = 1 ). We used 60%/20%/20% random splits and ran GNN and APPNP with α= 0.1. For results in Figure 2, we report averages over 5 graphs and for results in Figure 3, we report averages over 5 runs. In Figure 2, we study how the strength of community structure affects oversmoothing. We can see that when graphs have a stronger community structure in terms of a higher intra-community edge density p, they would beneﬁt more from repeated message-passing. As a result, given the same set of node features, oversmoothing would happen later and a classiﬁer could achieve better classiﬁcation performance. A similar trend can also be observed in Figure 4A. Our theory predicts n⋆ and n0, as deﬁned in Section 3.2, with high accuracy. In Figure 3, we compare APPNP and GNN under different graph topologies. In all three cases, APPNP manifests its advantage of reducing the mixing effect compared to GNN when the number of layers is large, i.e. when the undesirable mixing effect is dominant. However, as Figure 3B,C show, when we have large graphs or graphs with strong community structure, APPNP’s disadvantage of concurrently reducing the denoising effect is more severe, particularly when the number of layers is small. As a result, APPNP’s optimal performance is worse than the baseline GNN. These observations accord well with our theoretical discussions in Section 4. A B C Figure 2: How the strength of community structure affects oversmoothing. When graphs have stronger community structure (i.e. higher a), oversmoothing would happen later. Our theory (gray bar) predicts the optimal number of layers n⋆ in practice (blue) with high accuracy (A). Given the same set of features, a classiﬁer has signiﬁcantly better performance on graphs with higher a(B,C). 7A (base case) B (larger graph) C (stronger community) Figure 3: Comparison of node classiﬁcation performance between the baseline GNN and APPNP. The performance of APPNP is more robust when we increase the model depth. However, compared to the base case (A), APPNP tends to have worse optimal performance than GNN on graphs with larger size ( B) or stronger community structure (C), as predicted by the theory. 5.2 The effect of optimizing weights on oversmoothing We investigate how adding learnable weightsW(k) in each GNN layers affects the node classiﬁcation performance in practice. Consider the case when all the GNN layers used have width one, meaning that the learnable weight matrix W(k) in each layer is a scalar. In theory, the effects of adding such weights on the means and the variances would cancel each other and therefore they would not affect the z-score of our interest and the classiﬁcation performance. Figure 4A shows the the value of n0 predicted by the z-score, the actual n0 without learnable weights in each GNN layer (meaning that we apply pure graph convolutions ﬁrst, and only train weights in the ﬁnal linear classiﬁer) according to the test accuracy and the actual n0 with learnable weights in each GNN layer according to the test accuracy. The results are averages over 5 graphs for each case. We empirically observe that GNNs with weights are much harder to train, and the difﬁculty increases as we increase the number of layers. As a result, n0 is smaller for the model with weights and the gap is larger when n0 is supposed to be larger, possibly due to greater difﬁculty in optimizing deeper architectures [36]. To relieve this potential optimization problem, we increase the width of each GNN layer [37]. Figure 4B,C presents the training and testing accuracies of GNNs with increasing width with respect to the number of layers on a speciﬁc synthetic example. The results are averages over 5 runs. We observe that increasing the width of the network mitigates the difﬁculty of optimizing weights, and the performance after adding weights is able to gradually match the performance without weights. This empirically validates our claim in Section 3.2 that adding learnable weights should not affect the representation power of GNN in terms of node classiﬁcation accuracy on CSBM graphs, besides empirical optimization issues. In practice, as we build deeper GNNs for more complicated tasks on real graph data, the difﬁculty of optimizing weights in deep GNN models persists. We revisit the multi-class node classiﬁcation task on the three widely used benchmark datasets: Cora, CiteSeer and PubMed [35]. We compare the performance of GNN without weights against the performance of GNN with weights in terms of test accuracy. We used 60%/20%/20% random splits, as in [38] and [39] and report averages over 5 runs. Figure 5 shows the same kind of difﬁculty in optimizing deeper models with A B C Figure 4: The effect of optimizing weights on oversmoothing using synthetic CSBM data. Compared to the GNN without weights, oversmoothing happens much sooner after adding learnable weights in each GNN layer, although these two models have the same representation power (A). As we increase the width of each GNN layer, the performance of GNN with weights is able to gradually match that of GNN without weights (B,C). 8Figure 5: The effect of optimizing weights on oversmoothing using real-world benchmark datasets. Adding learnable weights in each GNN layer does not improve node classiﬁcation performance but rather leads to optimization difﬁculty. learnable weights in each GNN layer as we have seen for the synthetic data. Increasing the width of each GNN layer still mitigates the problem for shallower models, but it becomes much more difﬁcult to tackle beyond 10 layers to the point that simply increasing the width could not solve it. As a result, although GNNs with and without weights are on par with each other when both are shallow, the former has much worse performance when the number of layers goes beyond 10. These results suggest that the oversmoothing phenomenon observed in practice is aggravated by the difﬁculty of optimizing deep GNN models. 6 Discussion Designing more powerful GNNs requires deeper understanding of current GNNs—how they work and why they fail. In this paper, we precisely characterize the mechanism of overmoothing via a non-asymptotic analysis and justify why oversmoothing happens at a shallow depth. Our analysis suggests that oversmoothing happens once the undesirable mixing effect homogenizing node representations in different classes starts to dominate the desirable denoising effect homogenizing node representations in the same class. Due to the small diameter characteristic of real graphs, the turning point of the tradeoff will occur after only a few rounds of message-passing, resulting in oversmoothing in shallow GNNs. It is worth noting that oversmoothing becomes an important problem in the literature partly because typical Convolutional Neural Networks (CNNs) used for image processing are much deeper than GNNs [40]. As such, researchers have been trying to use methods that have previously worked for CNNs to make current GNNs deeper [ 20, 15]. However, if we regard images as grids, images and real-world graphs have fundamentally different characteristics. In particular, images are giant grids, meaning that aggregating information between faraway pixels often requires a large number of layers. This contrasts with real-world graphs, which often have small diameters. Hence we believe that building more powerful GNNs will require us to think beyond CNNs and images and take advantage of the structure in real graphs. There are many outlooks to our work and possible directions for further research. First, while our use of the CSBM provided important insights into GNNs, it will be helpful to incorporate other real graph properties such as degree heterogeneity in the analysis. Additionally, further research can focus on the learning perspective of the problem. 7 Acknowledgement This research has been supported by a Vannevar Bush Fellowship from the Ofﬁce of the Secretary of Defense. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the Department of Defense or the U.S. Government. References [1] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IJCNN, 2005. [2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20:61–80, 2009. [3] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. 9[4] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timo- thy D. Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeurIPS, 2015. [5] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In NeurIPS, 2016. [6] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In NeurIPS, 2016. [7] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In ICLR, 2016. [8] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. [9] Thomas Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR, 2017. [10] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32:4–24, 2019. [11] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In AAAI, 2018. [12] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. [13] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In ICLR, 2020. [14] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In AAAI, 2020. [15] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020. [16] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. In NeurIPS, 2022. [17] Meng Liu, Zhengyang Wang, and Shuiwang Ji. Non-local graph neural networks. IEEE transactions on pattern analysis and machine intelligence, 2021. [18] Yash Deshpande, Andrea Montanari, Elchanan Mossel, and Subhabrata Sen. Contextual stochastic block models. In NeurIPS, 2018. [19] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018. [20] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In ICCV, 2019. [21] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864, 2020. [22] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020. [23] Emmanuel Abbe. Community detection and stochastic block models. Foundations and Trends in Communications and Information Theory, 14:1–162, 2018. [24] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural networks. In ICLR, 2019. [25] Rongzhe Wei, Haoteng Yin, J. Jia, Austin R. Benson, and Pan Li. Understanding non-linearity in graph neural networks from the bayesian-inference perspective. ArXiv, abs/2207.11311, 2022. [26] Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath. Graph attention retrospective. ArXiv, abs/2202.13060, 2022. [27] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Graph convolution for semi-supervised classiﬁca- tion: Improved linear separability and out-of-distribution generalization. In ICML, 2021. [28] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of graph convolutions in deep networks. ArXiv, abs/2204.09297, 2022. [29] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? In ICLR, 2022. 10[30] Michelle Girvan and Mark E. J. Newman. Community structure in social and biological networks. Proceedings of the National Academy of Sciences, 99:7821 – 7826, 2002. [31] Fan R. K. Chung. Graph theory in the information age. volume 57, pages 726–732, 2010. [32] David A. Easley and Jon M. Kleinberg. Networks, Crowds, and Markets: Reasoning about a Highly Connected World. 2010. [33] Fan Chung Graham and Linyuan Lu. The diameter of sparse random graphs. Advances in Applied Mathematics, 26:257–279, 2001. [34] Reid Andersen, Fan Chung Graham, and Kevin J. Lang. Local graph partitioning using pagerank vectors. In FOCS, 2006. [35] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016. [36] Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. In COLT, 2019. [37] Simon Shaolei Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In ICML, 2019. [38] Hongwei Wang and Jure Leskovec. Unifying graph convolutional neural networks and label propagation. ArXiv, abs/2002.06755, 2020. [39] Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label propagation and simple models out-performs graph neural networks. In ICLR, 2021. [40] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [41] Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition. In Stochastic Modelling and Applied Probability, 1996. [42] Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey.Internet mathematics, 3(1):79–127, 2006. [43] Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479–2506, 2016. [44] Linyuan Lu and Xing Peng. Spectra of edge-independent random graphs.The Electronic Journal of Combinatorics, 20:27, 2013. [45] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. [47] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. A Proof of Lemma 1 Following the deﬁnition of the Bayes optimal classiﬁer [41], B(x) = arg max i=1,2 P[y= i|x] , we get that the Bayes optimal classiﬁer has a linear decision boundary D= (µ1 + µ2)/2 such that the decision rule is {y= 1 if x≤D y= 2 if x> D. Probability of misclassiﬁcation could be written as P[y= 1,x> D] + P[y= 2,x ≤D] = P[x> D|y= 1]P[y= 1] + P[x≤D|y= 2]P[y= 2] = 1 2(P[x> D|y= 1] + P[x≤D|y= 2]) . 11When D= (µ1 + µ2)/2, the expression is called the Bayes error rate, which is the minimal probability of misclassiﬁca- tion among all classiﬁers. Geometrically, it is easy to see that the Bayes error rate equals 1 2 S, where Sis the overlapping area between the two Gaussian distributions N ( µ(n) 1 ,(σ(n))2 ) and N ( µ(n) 2 ,(σ(n))2 ) . Hence one can use the z-score of (µ1 + µ2)/2 with respect to either of the two Gaussian distributions to directly calculate the Bayes error rate. B Proof of Lemma 2 Under the heuristic assumption D−1A≈E[D]−1E[A], we can write µ(1) 1 = pµ1 + qµ2 p+ q , µ (1) 2 = pµ2 + qµ1 p+ q µ(k) 1 = pµ(k−1) 1 + qµ(k−1) 2 p+ q , µ (k) 2 = pµ(k−1) 2 + qµ(k−1) 1 p+ q , for all k∈N. Writing recursively, we get that µ(n) 1 = (p+ q)n + (p−q)n 2(p+ q)n µ1 + (p+ q)n −(p−q)n 2(p+ q)n µ2 , µ(n) 2 = (p+ q)n + (p−q)n 2(p+ q)n µ2 + (p+ q)n −(p−q)n 2(p+ q)n µ1 . C Proof of Theorem 1 We use ∥·∥2 to denote the spectral norm, ∥A∥2 = maxx:∥x∥=1 ∥Ax∥2. We denote ¯A= E[A], ¯D= E[D], d= A1 N and ¯d= E[d]i. We further deﬁne the following relevant vectors: w1 := 1 N, w 2 := ( 1 N/2 −1 N/2 ) , µ := ( µ11 N/2 µ21 N/2 ) . The quantity of interest is µ(k) 2 −µ(k) 1 = 1 N/2 w⊤ 2 (D−1A)kµ. C.1 Auxiliary results We record some properties of the adjacency matrices: 1. D−1Aand ¯D−1 ¯Ahave an eigenvalue of 1, corresponding to the (right) eigenvector w1. 2. If Jn = 1 n1 ⊤ n, where 1 n is all-one vector of length n, then ¯A:= ( pJN/2 qJN/2 qJN/2 pJN/2 ) . 3. ¯D= N 2 (p+ q)IN. 4. µ= αw1 + βw2, where α= µ1+µ2 2 and β = µ1−µ2 2 . To control the degree matrix D−1, we will use the following standard Chernoff bound [42]: Lemma 5 (Chernoff Bound). Let X1,...,X n be independent, S := ∑n i=1 Xi, and ¯S = E[S]. Then for all ε> 0, P(S ≤¯S−ε) ≤e−ε2/(2 ¯S), P(S ≥¯S+ ε) ≤e−ε2/(2( ¯S+ε/3)). We can thus derive a uniform lower bound on the degree of every vertex: Corollary 1. For every r >0, there is a constant C(r) such that whenever ¯d ≥Clog N, with probability at least 1 −N−r, 1 2 ¯d≤di ≤3 2 ¯d, for all 1 ≤i≤N. Consequently, with probability at least 1 −N−r, ∥D−1 −¯D−1∥2 ≤C/¯dfor some C. 12Proof. By applying Lemma 1 and a union bound, all degrees are within 1/2 ¯dof their expectations, with probability at least 1 −e−¯d/8+log N. Taking C = 8r+ 8 yields the desired lower bound. An analogous proof works for the upper bound. To show the latter part, write ∥D−1 −¯D−1∥2 = max 1≤i≤N |di −¯d| di¯d Using the above bounds, the numerator for each iis at most 1/2 ¯dand the denominator for each iis at least 1/2 ¯d2, with probability at least 1 −N−r. Combining the bounds yields the claim. We will also need a result on concentration of random adjacency matrices, which is a corollary of the sharp bounds derived in [43] Lemma 6 (Concentration of Adjacency Matrix) . For every r >0, there is a constant C(r) such that whenever ¯d≥log N, with probability at least 1 −N−r, ∥A−¯A∥2 <C √ ¯d. Proof. By corollary 3.12 from [43], there is a constant κsuch that P(∥A−¯A∥2 ≥3 √ ¯d+ t) ≤e−t2/κ+log N. Setting t= √ (1 + r) ¯d, C = 3 + √ (1 + r)κsufﬁces to achieve the desired bound. C.2 Sharp concentration of the random walk operator D−1A In this section, we aim to show the following concentration result for the random walk operator D−1A: Theorem 3. Suppose the edge probabilities are ω ( log N N ) , and let ¯dbe the average degree. For anyr, there exists a constant Csuch that for sufﬁciently large N, with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . Proof. We decompose the error E = D−1A−¯D−1 ¯A= D−1(A−¯A) + (D−1 −¯D−1) ¯A= T1 + T2, where T1 = D−1(A−¯A), T 2 = (D−1 −¯D−1) ¯A. We bound the two terms separately. Bounding T1: By Corollary 1, ∥D−1∥2 = max i1/di ≤2/¯d with probability 1 −N−r. Combining this with Lemma 6, we see that with probability at least 1 −2N−r, ∥D−1(A−¯A)∥2 ≤∥D−1∥2∥A−¯A∥2 ≤ C√¯d for some Cdepending only on r. Bounding T2: Similar to [44], we bound T2 by exploiting the low-rank structure of the expected adjacency matrix, ¯A. Recall that ¯Ahas a special block form. The eigendecomposition of ¯Ais thus ¯A= 2∑ j=1 λjw(j), where w(1) = 1√ N1 N,λ1 = N(p+q) 2 ,w(2) = 1√ N ( 1 N/2 −1 N/2 ) ,λ2 = N(p−q) 2 . 13Using the deﬁnition of the spectral norm, we can bound ∥T2∥2 as ∥T2∥2 ≤max ∥x∥=1 ∥(D−1 −¯D−1) ¯Ax∥2 ≤ max α∈R2,∥α∥=1 ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 . Note that when ∥α∥2 = 1, ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 2 = N∑ i=1 (1 di −1 ¯d )2   2∑ j=1 λjαjw(j) i   2 ≤ N∑ i=1 (1 di −1 ¯d )2 2∑ j=1 λ2 j(w(j) i )2 using Cauchy-Schwarz. Since |w(j) i |≤ 1√ N for all i,j, the second summation can be bounded by 1 N ∑2 j=1 λ2 j. Overall, the upper bound is now 1 N N∑ i=1 (di −¯d)2 (di¯d)2 2∑ j=1 λ2 j , Under the event of Corollary 1, di ≥C¯dfor some C <1. Under our setup, we also have λ2 1 = ¯d2, λ2 2 ≤ ¯d2. This means that the upper bound is 1 C2 ¯d2N∥d−¯d1 N∥2 2 , where dis the vector of node degrees. It remains to show that 1 N∥d−¯d1 N∥2 2 = O( ¯d). To do this, we use a form of Talagrand’s concentration inequality, given in [45]. Since the function 1√ N∥d−¯d1 N∥2 = 1√ N∥(A−¯dIN)1 N∥2 is a convex, 1-Lipschitz function of A, Theorem 6.10 from [45] guarantees that for any t> 0, P( 1√ N ∥d−¯d1 N∥2 >E[ 1√ N ∥d−¯d1 N∥2] + t) ≤e−t2/2. Using Jensen’s inequality, E[∥d−¯d1 N∥2] ≤ √ E[∥d−¯d1 N∥2 2] = √ N∑ i=1 Var(di) = √ NVar(d1) ≤ √ N¯d. If ¯d= ω(log N), we can guarantee that 1√ N ∥d−¯d1 N∥2 ≤C √ ¯d with probability at least 1 −e−(C−1)2 ¯d/2 = 1 −O(N−r) for an appropriate constant C. Thus we have shown that with high probability, T2 = O(1/ √¯d), which proves the claim. C.3 Proof of Theorem 1 Fix rand K. We desire to bound 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ. By the ﬁrst property of adjacency matrices in auxiliary results, it sufﬁces to bound β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2. 14where β = µ1−µ2 2 . We will show inductively that there is a Csuch that for every k= 1,...,K , ∥(D−1A)k −( ¯D−1 ¯A)k∥2 ≤C/ √ ¯d. If this is true, then Cauchy-Schwarz gives β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2 ≤β 1 N/2∥w2∥2∥(D−1A)k −( ¯D−1 ¯A)k∥2∥w2∥2 ≤C/ √ ¯d. By Theorem 3, we have that with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . So D−1A= ¯D−1 ¯A+ J where ∥J∥≤ C/ √¯d. Iterating, we have ∥(D−1A)k −( ¯D−1 ¯A)k∥2 = ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 (1) Inductively, (D−1A)k−1 = ( ¯D−1 ¯A)k−1 + H where ∥H∥2 ≤C/ √¯d. Plugging this in (1), we have ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 = ∥(( ¯D−1 ¯A)k−1 + H)( ¯D−1 ¯A+ J) −( ¯D−1 ¯A)k∥2 . Of these terms, ( ¯D−1 ¯A)k−1J has norm at most ∥J∥2, H( ¯D−1 ¯A) has norm at most ∥H∥2, and HJ has norm at most C/¯d.2 Hence the induction step is complete. We have thus shown that there is a constantC(r,K) such that with probability at least 1 −N−r, | 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ|≤ C ¯d . which proves the claim. By simulation one can verify that indeed 1 N/2 w⊤ 2 ( ¯D−1 ¯A)kµ ≈ ( p−q p+q )k (µ2 −µ1). Figure 6 presents µ(n) 1 ,µ(n) 2 calculated from simulation against predicted values from our theoretical results. The simulation results are averaged over 20 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 6: Comparison of the mean estimation in Lemma 2 against simulation results. D Proof of Lemma 3 Fix n and let the element in the ith row and jth column of (D−1A)n be p(n) ij . Consider a ﬁxed node i. The variance of the feature for node iafter nlayers of convolutions is (∑ j(p(n) ij )2)σ2, by the basic property of variance of sum. Since ∑ j|p(n) ij |= 1, it follows that ∑ j(p(n) ij )2 ≤1, which is the second inequality. To show the ﬁrst inequality, consider the following optimization problem: 2More precisely, the C becomes Ck, which is why we restrict the approximation guarantee to constant K. 15min p(n) ij ,1 ≤j ≤N ∑ j (p(n) ij )2 s.t. ∑ j p(n) ij = 1, p(n) ij ≥0, 1 ≤j ≤N This part of proof goes by contradiction. Suppose ∃k,l such that p(n) ik ̸= ∃p(n) il . Fixing all other p(n) ij ,j ̸= k,l, if we average p(n) ik and p(n) il , their sum of squares will strictly decrease while not breaking the constraints: 2 (p(n) ik + p(n) il 2 )2 −((p(n) ik )2 + (p(n) il )2) = −1 2(p(n) ik −p(n) il )2 <0 . So we obtain a contradiction. Thus to minimize ∑ j(p(n) ij )2, p(n) ij = 1 N,1 ≤j ≤N, and the mimimum is 1/N. E Proof of Lemma 4 The proof relies on the following deﬁnition of neighborhood size: in a graph G, we denote by Γk(x) the set of vertices in Gat distance kfrom a vertex x: Γk(x) = {y∈G : d(x,y) = k}. we deﬁne Nk(x) to be the set of vertices within distance kof x: Nk(x) = k⋃ i=0 Γi(x) . To prove the lower bound, we ﬁrst show an intermediate step that 1 |Nn|σ2 ≤(σ(n))2 . The proof is the same as the one for the ﬁrst inequality in Lemma 3, except we add in another constraint that for a ﬁxed i, the row pi·is |Nn(i)|-sparse. This implies that the minimum of ∑ j(p(n) ij )2 becomes 1/|Nn(i)|. The we apply the result on upper bound of neighborhood sizes in Erd˝os-Rényi graph G(N,p) (Lemma 2 [33]), as it also serves as upper bound of neighborhood sizes in SBM(N, p, q). The result implies that with probability at least 1 −O(1/N), we have |Nn|≤ 10 min{a,2}(Np)n,∀1 ≤n≤N. (2) We ignore ifor Nn because of all nodes are identical in CSBM, so the bound applies for every nodes in the graph. The proof of upper bound is combinatorial. Corollary 1 states that whenNis large, the degree of nodeiis approximately the expected degree in G, namely, E[degree] = N 2 (p+ q). Since p(n) ij = ∑ path P={i,v1,...,vn−1,j} 1 deg(i) 1 deg(v1)... 1 deg(vn−1) , (3) using the approximation of degrees, we get that p(n) ij = ( 2 N(p+ q) )n (# of paths P of length n between i and j) . Then we use a tree approximation to calculate the number of paths P of length nbetween iand jby regarding ias the root. Note that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 ∑ j∈Γn−2k (p(n) ij )2 (4) and for j ∈Γn−2k, a deterministic path P′of length n−2kis needed in order to reach j from i. This implies that there are only ksteps deviating from P′. There are (n−2k+ 1)k ways of choosing when to deviate. For each speciﬁc 16way of when to deviate, there are approximately E[degree]k ways of choosing the destinations for deviation. Hence in total, for j ∈Γn−2k, there are (n−2k+ 1)kE[degree]k path of length nbetween iand j. Thus p(n) ij = (n−2k+ 1)k ( 2 N(p+ q) )n−k . (5) Plug in (5) into (4), we get that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 |Γn−2k|(n−2k+ 1)2k ( 2 N(p+ q) )2n−2k (6) ≤ ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k (7) Again, (7) follows from using the upper bound on |Γn−2k|[33] such that with probability at least 1 −O(1/N), |Γn−2k|≤ 9 min{a,2}(Np)n−2k,∀1 ≤k≤⌊n 2 ⌋. Combining with Lemma 3, we obtain the ﬁnal result. Figure 7 presents variance calculated from simulation against predicted upper and lower bounds from our theoretical results. The simulation results are averaged over 1000 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 7: Comparison of the bounds on variance in Theorem 2 against simulation results. F Proof of Theorem 2 When we ﬁx K ∈N, only the upper bound in Theorem 2 will change. Note that now the upper bound in (7) can be written as ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k (p+ q 2p )2k( 2p p+ q )n( 2 N(p+ q) )n ≤ C min{a,2} ( C∑ k=0 (p+ q 2p )2k)( 2 N(p+ q) )n ≤ C min{a,2} ( 2 N(p+ q) )n . G Proof of Proposition 1 Let the node representation vector of node v after n graph convolutions be h(n) v . The Bayes error rate could be written as 1 2 (P[h(n) v > D|v ∈C1] + P[h(n) v ≤D|v ∈C2]). For d ∈N, due to the symmetry of our setup, one can easily see that the optimal linear decision boundary is the hyperplane ∑d j=1 xj = d 2 (µ1 + µ2). Then for v ∈C1, 17∑d j=1 (h(n) v )j ∼N(dµ(n) 1 ,d(σ(n))2) and for v∈C2, ∑d j=1 (h(n) v )j ∼N(dµ(n) 2 ,d(σ(n))2). Thus the Bayes error rate can be written as 1 2(P[ d∑ j=1 (h(n) v )j >D|v∈C1] + P[ d∑ j=1 (h(n) v )j ≤D|v∈C2]) = 1 2 ( 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) )) + 1 2 ( Φ ( d 2 (µ1 + µ2) −dµ(n) 2√ dσ(n) )) = 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) ) . The last equality follows from the fact that d 2 (µ1 + µ2) −dµ(n) 1 = −(d 2 (µ1 + µ2) −dµ(n) 2 ). H How to use the z-score to choose the number of layers The bounds of the z-score with respect to the number of layers, z(n) lower and z(n) upper allow us to calculate bounds for n⋆ and n0 under different scenarios. Speciﬁcally, 1. ∀n∈N,z(n) upper <z(0) = (µ2 −µ1)/σ, then n⋆ = n0 = 0, meaning that no graph convolution should be applied. 2. |{n∈N : z(n) upper ≥z(0)}|>0, and (a) ∀n∈N,z(n) lower < z(0), then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},which means that the number of graph convolutions should not exceed the upper bound of n0, or otherwise one gets worse performance than having no graph convolution. Note that in this case, since n⋆ ≤n0, we can only conclude that 0 ≤n⋆ ≤min{n∈N : z(n) upper ≤z(0)}. (b) |{n∈N : z(n) lower ≥z(0)}|>0, then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},and let arg max n z(n) lower = n⋆ ﬂoor, max { n≤n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } ≤n⋆ ≤min { n≥n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } , meaning that the number of layers one should apply for optimal node classiﬁcation performance is more than the lower bound of n⋆, and less than the upper bound of n⋆. I Proofs of Proposition 2-5 I.1 Proof of Proposition 2 Since the spectral radius of D−1Ais 1, α(Id−(1 −α)(D−1A))−1 = α ∞∑ k=0 (1 −α)k(D−1A)k. Apply Lemma 2, we get that µPPNP 2 −µPPNP 1 ≈ p+q p+ 2−α α q(µ2 −µ1). To bound the approximation error, similar to the proof of the concentration bound in Theorem 1, it sufﬁces to bound µ1 −µ2 N w⊤ 2 ( ∞∑ k=0 α(1 −α)k((D−1A)k −( ¯D−1 ¯A)k))w2 = µ1 −µ2 N w⊤ 2 (TK + TK+1,∞)w2 , where TK = ∑K k=0 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), TK+1,∞= ∑∞ k=K+1 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), and K ∈N up to our own choice. Bounding TK: Apply Theorem 1, ﬁx r> 0, there exists a constantC(r,K,α) such that with probability1−O(N−r), ∥TK∥2 ≤ C√¯d . 18Bounding TK+1,∞: We will show upper bound for(D−1A)k −( ¯D−1 ¯A)k that applies for all k∈N. Note that for every k∈N, (D−1A)k = D−1/2(D−1/2AD−1/2)kD1/2 = D−1/2(VΛkV⊤)D1/2 , where D−1/2AD−1/2 = VΛV⊤is the eigenvalue decomposition. Then ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤∥(D−1A)k∥2 + ∥( ¯D−1 ¯A)k∥2 = ∥(D−1A)k∥2 + 1 ≤∥D−1/2∥2∥(D−1/2AD−1/2)k∥2∥D−1/2∥2 + 1. Since ∥(D−1/2AD−1/2)k∥2 = 1 and by Corollary 1, with probability at least 1 −N−r, ∥D1/2∥2 ≤ √ 3 ¯d/2,∥D−1/2∥2 ≤ √ 2/¯d, the previous inequality becomes ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤ √ 3 + 1. Hence ∥TK+1,∞∥2 ≤(1 −α)K+1 . Combining the two results, we prove the claim. I.2 Proof of Proposition 3 The claim is a direct corollary of Theorem 1. I.3 Proof of Proposition 4 The covariance matrix ΣPPNP of hPPNP could be written as ΣPPNP = α2( ∞∑ k=0 (1 −α)k(D−1A)k)( ∞∑ l=0 (1 −α)l(D−1A)l)⊤σ2 . Note that the variance of node iequals α2 ∑∞ k,l=0(1 −α)k+l(D−1A)k i·((D−1A)l)⊤ i·, where i·refers row iof a matrix. Then by Cauchy-Schwarz Theorem, (D−1A)k i·((D−1A)l)⊤ i· ≤∥(D−1A)k i·∥∥((D−1A)l)i·∥ ≤ √ (σ(k))2(σ(l))2/σ2,for all 1 ≤k,l ≤N. Moreover, by Lemma 3, (σ(k))2 ≤σ2. Due to the identity of each node i, we get that with probability 1 −O(1/N), for all 1 ≤K ≤N, (σPPNP)2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + ∞∑ k=K+1 (1 −α)kσ )2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 . For the lower bound, note that with probability 1 −O(1/N), (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2k 1 Nk + ∞∑ k=N+1 (1 −α)2k 1 N ) σ2 , where Nk is the size of k-hop neighborhood. Then (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2kmin{a,2} 10 1 (Np)k ) σ2 ≥α2 min{a,2} 10 (Np)N+1 −(1 −α)2N+2 (Np)N(Np −(1 −α)2) σ2 ≥α2 min{a,2} 10 σ2 . It is easy to see that Lemma 3 applies to any message-passing scheme which could be regarded as a random walk on the graph. Combining with Lemma 3, we get the ﬁnal result. 19I.4 Proof of Proposition 5 Since hAPPNP(n) = ( α (n−1∑ k=0 (1 −α)k(D−1A)k ) + (1 −α)n(D−1A)n ) X Through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≤ ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 . For the lower bound, through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≥α2 n−1∑ k=0 (1 −α)2k(σ(k))2 + (1 −α)2n(σ(n))2 ≥α2 min{a,2} 10 (n−1∑ k=0 (1 −α)2k 1 (Np)k ) σ2 + min{a,2} 10 (1 −α)2n 1 (Np)nσ2 ≥min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) σ2 . Combining with Lemma 3, we get the ﬁnal result. J Experiments Here we provide more details on the models that we use in Section 5. In all cases we use the Adam optimizer and tune some hyperparameters for better performance. The hyperparameters used are summarized as follows. Data ﬁnal linear classiﬁer weights in GNN layer learning rate (width) iterations (width) synthetic 1 layer no 0.01 8000 yes 0.01(1,4,16)/0.001(64,256) 8000(1,4,16)/10000(64)/50000(256) Cora 3 layer with 32 hidden channels no 0.001 150 yes 0.001 200 CiteSeer 3 layer with 16 hidden channels no 0.001 100 yes 0.001 100 PubMed 3 layer with 32 hidden channels no 0.001 500 yes 0.001 500 We empirically ﬁnd that after adding in weights in each GNN layer, it takes much longer to train the model for one iteration, and the time increases when the depth or the width increases (Figure 8). Since for some combinations, it takes more than 200,000 iterations for the validation accuracy to ﬁnally increase, for each case, we only train for a reasonable amount of iterations. Figure 8: Iterations per second for each model. All models were implemented with PyTorch [46] and PyTorch Geometric [47]. 20K Additional Results K.1 Effect of nonlinearity on classiﬁcation performance In section 3, we consider the case of a simpliﬁed linear GNN. What would happen if we add nonlinearity after linear graph convolutions? Here, we consider the case of a GNN with a ReLU activation function added after nlinear graph convolutions, i.e. h(n)ReLU = ReLU((D−1A)nX). We show that adding such nonlinearity does not improve the classiﬁcation performance. Proposition 6. Applying a ReLU activation function after n linear graph convolutions does not decrease the Bayes error rate, i.e. Bayes error rate based on h(n)ReLU ≥ Bayes error rate based on h(n), and equality holds if µ1 ≥−µ2. Proof. If is known that if xfollows a Gaussian distribution, then ReLU(x) follows a Rectiﬁed Gaussian distribution. Following the deﬁnition of the Bayes optimal classiﬁer, we present a geometric proof in Figure 9 (see next page, top), where the dark blue bar denotes the location of 0 and the red bar denotes the decision boundary Dof the Bayes optimal classiﬁer, and the light blue area denotes the overlapping area S, which is twice the Bayes error rate. Figure 9: A geometric proof of Proposition 6. K.2 Exact limit of variance (σ(n))2 as n→∞ Proposition 7. Given a graph Gwith adjacency matrix A, let its degree vector be d= A1 N, where 1 N is the all-one vector of length N. If Gis connected and non-bipartite, the variance of each node i, denoted as (σi(n))2, converges asymptotically to ∥d∥2 2 ∥d∥2 1 , i.e. (σi (n))2 n→∞ −−−−→∥d∥2 2 ∥d∥2 1 . Then ∥d∥2 ∥d∥2 1 ≥ 1 N, and the equality holds if and only if Gis regular. Proof. Let ei denotes the standard basis unit vector with the ith entry equals 1, and all other entries equal 0. Since Gis connected and non-bipartite, the random walk represented by P = D−1Ais ergodic, meaning that e⊤ i P(n) n→∞ −−−−→π, where πis the stationary distribution of this random walk with πi = di ∥d∥1 . Then since norms are continuous functions, we conclude that (σi (n))2 = ∑ j (p(n) ij )2 = ∥e⊤ i P(n)∥2 2 n→∞ −−−−→∥π∥2 2 = ∥d∥2 2 ∥d∥2 1 . By Lemma 3, it follows that ∥d∥2 2 ∥d∥2 1 ≥ 1 N. The unique minimizer of ∥π∥2 2 subject to ∥π∥1 = 1 is π= 1 N1 N. This means that Gmust be regular to achieve the lower bound asymptotically. 21Under Assumption 1, the graph generated by our CSBM is almost surely connected. Here, we remain to show that with high probability, the graph will also be non-bipartite. Proposition 8. With probability at least 1 −O(1/(Np)3), a graph Ggenerated from CSBM(N, p, q, µ1, µ2, σ2) contains a triangle, which implies that it is non-bipartite. Proof. The proof goes by the classic probabilistic method. Let T∆ = ( N 3 )∑ i 1 τi denotes the number of triangles in G, where 1 τi equals 1 if potential triangle τi exists and 0 otherwise. Then by second moment method, P[T∆ = 0] ≤ Var(T∆) (E[T∆])2 = 1 E[T∆]) + ∑ i̸=jE[1 τi1 τj] −(E[T∆])2 (E[T∆])2 . Since E[T∆] = O(Np), ∑ i̸=jE[1 τi1 τj] = (1 + O(1/N))(E[T∆])2, we get that P[T∆ = 0] ≤O(1/(Np)3) + O(1/N) ≤O(1/(Np)3) . Hence P[Gis non-bipartite] ≥P[T∆ ≥1] ≥1 −O(1/(Np)3). K.3 Symmetric Graph Convolution D−1/2AD−1/2 Proposition 9. When using symmetric message-passing convolution D−1/2AD−1/2 instead, the variance (σ(n))2 is non-increasing with respect to the number of convolutional layers n. i.e. (σ(n+1))2 ≤(σ(n))2,n ∈N ∪{0}. Proof. We want to calculate the diagonal entries of the covariance matrix Σ(n) of (D−1/2AD−1/2)nX, where the covariance matrix of X is σ2IN. Hence Σ(n) = (D−1/2AD−1/2)n( (D−1/2AD−1/2)n)⊤ . Since D−1/2AD−1/2 is symmetric, let its eigendecomposition be VΛV⊤and we could rewrite Σ(n) = (VΛnV⊤)(VΛnV⊤) = VΛ2nV⊤. Notice that the closed form of the diagonal entries is diag(Σ(n)) = N∑ i=1 λ2n i |v|2 . Since for all 1 ≤i≤N, |λi|≤ 1,we obtain monotonicity of each entry of diag(Σ(n)), i.e. variance of each node. Although the proposition does not always hold for random walk message-passing convolution D−1A as one can construct speciﬁc counterexamples (Appendix K.4), in practice, variances are observed to be decreasing with respect with the number of layers. Moreover, we empirically observe that variance goes down more than the variance using symmetric message-passing convolutions. Figure 10 presents visualization of node representations comparing the change of variance with respect to the number of layers using random walk convolution and symmetric message-passing convolution. The data is generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 10: The change of variance with respect to the number of layers using random walk convolution D−1Aand symmetric message-passing convolution D−1/2AD−1/2. 22K.4 Counterexamples Here, we construct a speciﬁc example where the variance (σ(n))2 is not non-increasing with respect to the number of layers n(Figure 11A). We remark that such a non-monotone nature of change in variance is not caused by the bipartiteness of the graph, as a cycle graph with even number of nodes is also bipartite, but does not exhibit such phenomenon (Figure 11B). We conjecture the increase in variance is rather caused by the tree-like structure. A B Figure 11: Counterexamples. K.5 The mixing and denoising effects in practice In this section, we measure the mixing and denoising effects of graph convolutions identiﬁed by our theoretical results in practice, and show that the same tradeoff between the two counteracting effects exists for real-world graphs. For the mixing effect, we measure the pairwise L2 distances between the means of different classes, and for the denoising effect, we measure the within-class variances, both respect to the number of layers. Figure 12 gives a visualization of both metrics for all classes on Cora, CiteSeer and PubMed. We observe that similar to the synthetic CSBM data, adding graph convolutions increases both the mixing effect (homogenizing node representations in different classes, measured by the inter-class distances) and the denoising effect (homogenizing node representations in the same class, measured by the within-class distances). In addition, the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory. Figure 12: The existence of the mixing (top row) and denoising effects (bottom row) of graph convolutions in practice. Adding graph convolutions increases both effects and the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory in Section 3. 23",
      "meta_data": {
        "arxiv_id": "2212.10701v2",
        "authors": [
          "Xinyi Wu",
          "Zhengdao Chen",
          "William Wang",
          "Ali Jadbabaie"
        ],
        "published_date": "2022-12-21T00:33:59Z",
        "pdf_url": "https://arxiv.org/pdf/2212.10701v2.pdf"
      }
    },
    {
      "title": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization",
      "abstract": "Graph neural networks (GNNs), which learn the representation of a node by\naggregating its neighbors, have become an effective computational tool in\ndownstream applications. Over-smoothing is one of the key issues which limit\nthe performance of GNNs as the number of layers increases. It is because the\nstacked aggregators would make node representations converge to\nindistinguishable vectors. Several attempts have been made to tackle the issue\nby bringing linked node pairs close and unlinked pairs distinct. However, they\noften ignore the intrinsic community structures and would result in sub-optimal\nperformance. The representations of nodes within the same community/class need\nbe similar to facilitate the classification, while different classes are\nexpected to be separated in embedding space. To bridge the gap, we introduce\ntwo over-smoothing metrics and a novel technique, i.e., differentiable group\nnormalization (DGN). It normalizes nodes within the same group independently to\nincrease their smoothness, and separates node distributions among different\ngroups to significantly alleviate the over-smoothing issue. Experiments on\nreal-world datasets demonstrate that DGN makes GNN models more robust to\nover-smoothing and achieves better performance with deeper GNNs.",
      "full_text": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization Kaixiong Zhou Texas A&M University zkxiong@tamu.edu Xiao Huang The Hong Kong Polytechnic University xhuang.polyu@gmail.com Yuening Li Texas A&M University liyuening@tamu.edu Daochen Zha Texas A&M University daochen.zha@tamu.edu Rui Chen Samsung Research America rui.chen1@samsung.com Xia Hu Texas A&M University xiahu@tamu.edu Abstract Graph neural networks (GNNs), which learn the representation of a node by aggre- gating its neighbors, have become an effective computational tool in downstream applications. Over-smoothing is one of the key issues which limit the performance of GNNs as the number of layers increases. It is because the stacked aggregators would make node representations converge to indistinguishable vectors. Several attempts have been made to tackle the issue by bringing linked node pairs close and unlinked pairs distinct. However, they often ignore the intrinsic community structures and would result in sub-optimal performance. The representations of nodes within the same community/class need be similar to facilitate the classiﬁca- tion, while different classes are expected to be separated in embedding space. To bridge the gap, we introduce two over-smoothing metrics and a novel technique, i.e., differentiable group normalization (DGN). It normalizes nodes within the same group independently to increase their smoothness, and separates node distributions among different groups to signiﬁcantly alleviate the over-smoothing issue. Exper- iments on real-world datasets demonstrate that DGN makes GNN models more robust to over-smoothing and achieves better performance with deeper GNNs. 1 Introduction Graph neural networks (GNNs) [1, 2, 3] have emerged as a promising tool for analyzing networked data, such as biochemical networks [4, 5], social networks [6, 7], and academic networks [8, 9]. The successful outcomes have led to the development of many advanced GNNs, including graph convolu- tional networks [10], graph attention networks [11], and simple graph convolution networks [12]. Besides the exploration of graph neural network variants in different applications, understanding the mechanism and limitation of GNNs is also a crucial task. The core component of GNNs, i.e., a neighborhood aggregator updating the representation of a node iteratively via mixing itself with its neighbors’ representations [6, 13], is essentially a low-pass smoothing operation [14]. It is in line with graph structures since the linked nodes tend to be similar [15]. It has been reported that, as the number of graph convolutional layers increases, all node representations over a graph will converge to indistinguishable vectors, and GNNs perform poorly in downstream applications [16, 17, 18]. It is recognized as an over-smoothing issue. Such an issue prevents GNN models from going deeper to exploit the multi-hop neighborhood structures and learn better node representations. A lot of efforts have been devoted to alleviating the over-smoothing issue, such as regularizing the node distance [ 19], node/edge dropping [ 20, 21], batch and pair normalizations [ 22, 23, 24]. Preprint. Under review. arXiv:2006.06972v1  [cs.LG]  12 Jun 2020Most of existing studies focused on measuring the over-smoothing based on node pair distances. By using these measurements, representations of linked nodes are forced to be close to each other, while unlinked pairs are separated. Unfortunately, the global graph structures and group/community characteristics are ignored, which leads to sub-optimal performance. For example, to perform node classiﬁcation, an ideal solution is to assign similar vectors to nodes in the same class, instead of only the connected nodes. In the citation network Pubmed [25], 36% of unconnected node pairs belong to the same class. These node pairs should instead have a small distance to facilitate node classiﬁcation. Thus, we are motivated to tackle the over-smoothing issue in GNNs from a group perspective. Given the complicated group structures and characteristics, it remains a challenging task to tackle the over-smoothing issue in GNNs. First, the formation of over-smoothing is complex and related to both local node relations and global graph structures, which makes it hard to measure and quantify. Second, the group information is often not directly available in real-world networks. This prevents existing tools such as group normalization being directly applied to solve our problem [ 26]. For example, while the group of adjacent channels with similar features could be directly accessed in convolutional neural networks [27], it is nontrivial to cluster a network in a suitable way. The node clustering needs to be in line with the embeddings and labels, during the dynamic learning process. To bridge the gap, in this paper, we perform a quantitative study on the over-smoothing in GNNs from a group perspective. We aim to answer two research questions. First, how can we precisely measure the over-smoothing in GNNs? Second, how can we handle over-smoothing in GNNs? Through exploring these questions, we make three signiﬁcant contributions as follows. • Present two metrics to quantify the over-smoothing in GNNs: (1) Group distance ratio, clustering the network and measuring the ratio of inter-group representation distance over intra-group one; (2) Instance information gain, treating node instance independently and measuring the input information loss during the low-pass smoothing. • Propose differentiable group normalization to signiﬁcantly alleviate over-smoothing. It softly clusters nodes and normalizes each group independently, which prevents distinct groups from having close node representations to improve the over-smoothing metrics. • Empirically show that deeper GNNs, when equipped with the proposed differentiable group normalization technique, yield better node classiﬁcation accuracy. 2 Quantitative Analysis of Over-smoothing Issue In this work, we use the semi-supervised node classiﬁcation task as an example and illustrate how to handle the over-smoothing issue. A graph is represented byG= {V,E}, where Vand Erepresent the sets of nodes and edges, respectively. Each node v∈V is associated with a feature vector xv ∈Rd and a class label yv. Given a training set Vl accompanied with labels, the goal is to classify the nodes in the unlabeled set Vu = V\\V l via learning the mapping function based on GNNs. 2.1 Preliminaries Following the message passing strategy [ 28], GNNs update the representation of each node via aggregating itself and its neighbors’ representations. Mathematically, at thek-th layer, we have, N(k) v = AGG({a(k) vv′W(k)h(k−1) v′ : v′∈N(v)}), h (k) v = COM(a(k) vv W(k)h(k−1) v ,N(k) v ). (1) N(k) v and h(k) v denote the aggregated neighbor embedding and embedding of nodev, respectively. We initialize h(0) v = xv. N(v) = {v′|ev,v′ ∈E} represents the set of neighbors for node v, where ev,v′ denotes the edge that connects nodes vand v′. W(k) denotes the trainable matrix used to transform the embedding dimension. a(k) vv′ is the link weight over edge ev,v′, which could be determined based on the graph topology or learned by an attention layer. Symbol AGG denotes the neighborhood aggregator usually implemented by a summation pooling. To update nodev, function COM is applied to combine neighbor information and node embedding from the previous layer. It is observed that the weighted average in Eq. (1) smooths node embedding with its neighbors to make them similar. For a full GNN model with Klayers, the ﬁnal node representation is given by hv = h(K) v , which captures the neighborhood structure information within Khops. 22.2 Measuring Over-smoothing with Group Structures In GNNs, the neighborhood aggregation strategy smooths nodes’ representations over a graph [14]. It will make the representations of nodes converge to similar vectors as the number of layersKincreases. This is called the over-smoothing issue, and would cause the performance of GNNs deteriorates as K increases. To address the issue, the ﬁrst step is to measure and quantify the over-smoothing [19, 21]. Measurements in existing work are mainly based on the distances between node pairs [20, 24]. A small distance means that a pair of nodes generally have undistinguished representation vectors, which might triggers the over-smoothing issue. However, the over-smoothing is also highly related to global graph structures, which have not been taken into consideration. For some unlinked node pairs, we would need their representations to be close if they locate in the same class/community, to facilitate the node classiﬁcation task. Without the speciﬁc group information, the metrics based on pair distances may fail to indicate the over- smoothing. Thus, we propose two novel over-smoothing metrics, i.e., group distance ratio and instance information gain. They quantify the over-smoothing from global (communities/classes/groups) and local (node individuals) views, respectively. Deﬁnition 1 (Group Distance Ratio). Suppose that there areCclasses of node labels. We intuitively cluster nodes of the same class label into a group to formulate the labeled node community. Formally, let Li = {hiv}denote the group of representation vectors, where node vis associated with label i. We have a series of labeled groups{L1,··· ,LC}. Group distance ratio RGroup measures the ratio of inter-group distance over intra-group distance in the Euclidean space. We have: RGroup = 1 (C−1)2 ∑ i̸=j( 1 |Li||Lj| ∑ hiv∈Li ∑ hjv′∈Lj ||hiv −hjv′||2) 1 C ∑ i( 1 |Li|2 ∑ hiv,hiv′∈Li ||hiv −hiv′||2) , (2) where ||·|| 2 denotes the L2 norm of a vector and |·| denotes the set cardinality. The numerator (denominator) represents the average of pairwise representation distances between two different groups (within a group). One would prefer to reduce the intra-group distance to make representations of the same class similar, and increase the inter-group distance to relieve the over-smoothing issue. On the contrary, a small RGroup leads to the over-smoothing issue where all groups are mixed together, and the intra-group distance is maintained to hinder node classiﬁcation. Deﬁnition 2 (Instance Information Gain). In an attributed network, a node’s feature decides its class label to some extent. We treat each node instance independently, and deﬁne instance information gain GIns as how much input feature information is contained in the ﬁnal representation. Let Xand Hdenote the random variables of input feature and representation vector, respectively. We deﬁne their probability distributions with PX and PH, and use PXH to denote their joint distribution. GIns measures the dependency between node feature and representation via their mutual information: GIns = I(X; H) = ∑ xv∈X,hv∈H PXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv). (3) We list the details of variable deﬁnitions and mutual information calculation in the context of GNNs in Appendix. With the intensiﬁcation of the over-smoothing issue, nodes average the neighborhood information and lose their self features, which leads to a small value of GIns. 2.3 Illustration of Proposed Over-smoothing Metrics Based on the two proposed metrics, we take simple graph convolution networks (SGC) as an example, and analyze the over-smoothing issue on Cora dataset [ 25]. SGC simpliﬁes the model through removing all the trainable weights between layers to avoid the potential of overﬁtting [12]. So the over-smoothing issue would be the major cause of performance dropping in SGC. As shown by the red lines in Figure 1, the graph convolutions ﬁrst exploit neighborhood information to improve test accuracy up to K = 5, after which the over-smoothing issue starts to worsen the performance. At the same time, instance information gain GIns and group distance ratio RGroup decrease due to the over-smoothing issue. For the extreme case of K = 120, the input features are ﬁltered out and all groups of nodes converge to the same representation vector, leading to GIns = 0 and RGroup = 1, respectively. Our metrics quantify the smoothness of node representations based on group structures, but also have the similar variation tendency with test accuracy to indicate it well. 30 25 50 75 100 125 Layers 0.4 0.6 0.8Accuracy 0 25 50 75 100 125 Layers 0.06 0.08 0.10 0.12Instance gain 0 25 50 75 100 125 Layers 1.5 2.0 2.5 3.0Group distance None batch pair group Figure 1: The test accuracy, instance information gain, and group distance ratio of SGC on Cora. We compare differentiable group normalization with none, batch and pair normalizations. 3 Differentiable Group Normalization We start with a graph-regularized optimization problem [10, 19]. To optimize the over-smoothing metrics of GIns and RGroup, one traditional approach is to minimize the loss function: L= L0 −GIns −λRGroup. (4) L0 denotes the supervised cross-entropy loss w.r.t. representation probability vectors hv ∈RC×1 and class labels. λis a balancing factor. The goal of optimization problem Eq. (4) is to learn node representations close to the input features and informative for their class labels. Considering the labeled graph communities, it also improves the intra-group similarity and inter-group distance. However, it is non-trivial to optimize this objective function due to the non-derivative of non- parametric statistic GIns [29, 30] and the expensive computation of RGroup. 3.1 Proposed Technique for Addressing Over-smoothing Instead of directly optimizing regularized problem in Eq. (4), we propose the differentiable group normalization (DGN) applied between graph convolutional layers to normalize the node embeddings group by group. The key intuition is to cluster nodes into multiple groups and then normalize them independently. Consider the labeled node groups (or communities) in networked data. The node embeddings within each group are expected to be rescaled with a speciﬁc mean and variance to make them similar. Meanwhile, the embedding distributions from different groups are separated by adjusting their means and variances. We develop an analogue with the group normalization in convolutional neural networks (CNNs) [26], which clusters a set of adjacent channels with similar characteristics into a group and treats it independently. Compared with standard CNNs, the challenge in designing DGN is how to cluster nodes in a suitable way. The clustering needs to be in line with the embedding and labels, during the dynamic learning process. We address this challenge by learning a cluster assignment matrix, which softly maps nodes with close embeddings into a group. Under the supervision of training labels, the nodes close in the embedding space tend to share a common label. To be speciﬁc, we ﬁrst describe how DGN clusters and normalizes nodes in a group-wise fashion given an assignment matrix. After that, we discuss how to learn the assignment matrix to support differentiable node clustering. Group Normalization. Let H(k) = [ h(k) 1 ,··· ,h(k) n ]T ∈Rn×d(k) denote the embedding matrix generated from the k-th graph convolutional layer. TakingH(k) as input, DGN softly assigns nodes into groups and normalizes them independently to output a new embedding matrix for the next layer. Formally, we deﬁne the number of groups as G, and denote the cluster assignment matrix by S(k) ∈Rn×G. Gis a hyperparameter that could be tuned per dataset. The i-th column of S(k), i.e., S(k)[:,i], indicates the assignment probabilities of nodes in a graph to the i-th group. Supposing that S(k) has already been computed, we cluster and normalize nodes in each group as follows: H(k) i = S(k)[:,i] ◦H(k) ∈Rn×d(k) ; ˜H(k) i = γi(H(k) i −µi σi ) + βi ∈Rn×d(k) . (5) Symbol ◦denotes the row-wise multiplication. The left part in the above equation represents the soft node clustering for group i, whose embedding matrix is given by H(k) i . The right part performs the standard normalization operation. In particular, µi and σi denote the vectors of running mean 4and standard deviation of group i, respectively, and γi and βi denote the trainable scale and shift vectors, respectively. Given the input embedding H(k) and the series of normalized embeddings {˜H(k) 1 ,··· , ˜H(k) G }, DGN generates the ﬁnal embedding matrix ˜H(k) for the next layer as follows: ˜H(k) = H(k) + λ G∑ i=1 ˜H(k) i ∈Rn×d(k) . (6) λis a balancing factor as mentioned before. Inspecting the loss function in Eq. (4), DGN utilizes components H(k) and ∑G i=1 ˜H(k) i to improve terms GIns and RGroup, respectively. In particular, we preserve the input embedding H(k) to avoid over-normalization and keep the input feature of each node to some extent. Note that the linear combination of H(k) in DGN is different from the skip connection in GNN models [31, 32], which instead connects the embedding output H(k−1) from the last layer. The technique of skip connection could be included to further boost the model performance. Group normalization ∑G i=1 ˜H(k) i rescales the node embeddings within each group independently to make them similar. Ideally, we assign the close node embeddings with a common label to a group. Node embeddings of the group are then distributed closely around the corresponding running mean. Thus for different groups associate with distinct node labels, we disentangle their running means and separates the node embedding distributions. By applying DGN between the successive graph convolutional layers, we are able to optimize Problem (4) to mitigate the over-smoothing issue. Differentiable Clustering. We apply a linear model to compute the cluster assignment matrix S(k) used in Eq. (5). The mathematical expression is given by: S(k) = softmax(H(k)U(k)). (7) U(k) ∈Rd(k)×G denotes the trainable weights for a DGN module applied after the k-th graph convolutional layer. softmax function is applied in a row-wise way to produce the normalized probability vector w.r.t all the Ggroups for each node. Through the inner product between H(k) and U(k), the nodes with close embeddings are assigned to the same group with a high probability. Here we give a simple and effective way to compute S(k). Advanced neural networks could be applied. Time Complexity Analysis. Suppose that the time complexity of embedding normalization at each group is O(T), where T is a constant depending on embedding dimension d(k) and node number n. The time cost of group normalization ∑G i=1 ˜H(k) i is O(GT). Both the differentiable clustering (in Eq. (5)) and the linear model (in Eq. (7)) have a time cost of O(nd(k)G). Thus the total time complexity of a DGN layer is given by O(nd(k)G+ GT), which linearly increases with G. Comparison with Prior Work. To the best of our knowledge, the existing work mainly focuses on analyzing and improving the node pair distance to relieve the over-smoothing issue [19, 21, 24]. One of the general solutions is to train GNN models regularized by the pair distance [19]. Recently, there are two related studies applying batch normalization [22] or pair normalization [24] to keep the overall pair distance in a graph. Pair normalization is a “slim” realization of batch normalization by removing the trainable scale and shift. However, the metric of pair distance and the resulting techniques ignore global graph structure, and may achieve sub-optimal performance in practice. In this work, we measure over-smoothing of GNN models based on communities/groups and independent node instances. We then formulate the problem in Eq. (4) to optimize the proposed metrics, and propose DGN to solve it in an efﬁcient way, which in turn addresses the over-smoothing issue. 3.2 Evaluating Differentiable Group Normalization on Attributed Graphs We apply DGN to the SGC model to validate its effectiveness in relieving the over-smoothing issue. Furthermore, we compare with the other two available normalization techniques used upon GNNs, i.e., batch normalization and pair normalization. As shown in Figure 1, the test accuracy of DGN remains stable with the increase in the number of layers. By preserving the input embedding and normalizing node groups independently, DGN achieves superior performance in terms of instance information gain as well as group distance ratio. The promising results indicate that our DGN tackles the over-smoothing issue more effectively, compared with none, batch and pair normalizations. It should be noted that, the highest accuracy of 79.7% is achieved with DGN when K = 20. This observation contradicts with the common belief that GNN models work best with a few layers on 50 25 50 75 100 125 Layers 0.2 0.4 0.6 0.8Accuracy SGC 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GCN 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GAT None batch pair group Figure 2: The test accuracies of SGC, GCN, and GAT models on Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. current benchmark datasets [33]. With the integration of advanced techniques, such as DGN, we are able to exploit deeper GNN architectures to unleash the power of deep learning in network analysis. 3.3 Evaluation in Scenario with Missing Features To further illustrate that DGN could enable us to achieve better performance with deeper GNN architectures, we apply it to a more complex scenario. We assume that the attributes of nodes in the test set are missing. It is a common scenario in practice [24]. For example, in social networks, new users are often lack of proﬁles and tags [34]. To perform prediction tasks on new users, we would rely on the node attributes of existing users and their connections to new users. In such a scenario, we would like to apply more layers to exploit the neighborhood structure many hops away to improve node representation learning. Since the over-smoothing issue gets worse with the increasing of layer numbers, the beneﬁt of applying normalization will be more obvious in this scenario. We remove the input features of both validation and test sets in Cora, and replace them with zeros [24]. Figure 2 presents the results on three widely-used models, i.e., SGC, graph convolutional networks (GCN), and graph attention networks (GAT). Due to the over-smoothing issue, GNN models without any normalization fail to distinguish nodes quickly with the increasing number of layers. In contrast, the normalization techniques reach their highest performance at larger layer numbers, after which they drop slowly. We observe that DGN obtains the best performance with50, 20, and 8 layers for SGC, GCN, and GAT, respectively. These layer numbers are signiﬁcantly larger than those of the widely-used shallow models (e.g., two or three layers). 4 Experiments We now empirically evaluate the effectiveness and robustness of DGN on real-world datasets. We aim to answer three questions as follows. Q1: Compared with the state-of-the-art normalization methods, can DGN alleviate the over-smoothing issue in GNNs in a better way? Q2: Can DGN help GNN models achieve better performance by enabling deeper GNNs? Q3: How do the hyperparameters inﬂuence the performance of DGN? 4.1 Experiment Setup Datasets. Joining the practice of previous work, we evaluate GNN models by performing the node classiﬁcation task on four datasets: Cora, Citeseer, Pubmed [ 25], and CoauthorCS [35]. We also create graphs by removing features in validation and test sets. The dataset statistics are in Appendix. Implementations. Following the previous settings, we choose the hyperparameters of GNN models and optimizer as follows. We set the number of hidden units to 16 for GCN and GAT models. The number of attention heads in GAT is 1. Since a larger parameter size in GCN and GAT may lead to overﬁtting and affects the study of over-smoothing issue, we compare normalization methods by varying the number of layersKin {1,2,··· ,10,15,··· ,30}. For SGC, we increase the testing range and vary K in {1,5,10,20,··· ,120}. We train with a maximum of 1000 epochs using the Adam optimizer [36] and early stopping. Weights in GNN models are initialized with Glorot algorithm [37]. We use the following sets of hyperparameters for Citeseer, Cora, CoauthorCS: 0.6 (dropout rate), 5 ·10−4 (L2 regularization), 5 ·10−3 (learning rate), and for Pubmed: 0.6 (dropout rate), 1 ·10−3 (L2 regularization), 1 ·10−2 (learning rate). We run each experiment 5 times and report the average. 6Table 1: Test accuracy in percentage on attributed networks. Layers a/bdenote the layer number ain GCN & GAT and that of bin SGC. #Kdenotes the optimal layer numbers where DGN achieves the highest performance. Dataset Model Layers 2/5 Layers 15/60 Layers 30/120 #KNN BN PN DGN NN BN PN DGN NN BN PN DGN Cora GCN 82.2 73.9 71 .0 82 .0 18.1 70 .3 67 .2 75.2 13.1 67 .2 64 .3 73.2 2 GAT 80.9 77 .8 74 .4 81.1 16.8 33 .1 49 .6 71.8 13.0 25 .0 30 .2 51.3 2 SGC 75.8 76 .3 75 .4 77.9 29.4 72 .1 71 .7 77.8 25.1 51 .2 65 .5 73.7 20 Citeseer GCN 70.6 51.3 60 .5 69 .5 15.2 46 .9 46 .7 53.1 9.4 47 .9 47 .1 52.6 2 GAT 70.2 61.5 62 .0 69 .3 22.6 28 .0 41 .4 52.6 7.7 21 .4 33 .3 45.6 2 SGC 69.6 58.8 64 .8 69 .5 66.3 50.5 65 .0 63 .4 60.8 47 .3 63 .1 64.7 30 Pubmed GCN 79.3 74 .9 71 .1 79.5 22.5 73 .7 70 .6 76.1 18.0 70 .4 70 .4 76.9 2 GAT 77.8 76.2 72 .4 77 .5 37.5 56 .2 68 .8 75.9 18.0 46 .6 58 .2 73.3 5 SGC 71.5 76 .5 75 .8 76.8 34.2 75 .2 77 .1 77.4 23.1 71 .6 76 .7 77.1 10 Coauthors GCN 92.3 86 .0 77 .8 92.3 72.2 78 .5 69 .5 83.7 3.3 84.7 64.5 84 .4 1 GAT 91.5 89 .4 85 .9 91.8 6.0 77 .7 53 .1 84.5 3.3 16 .7 48 .1 75.5 1 SGC 89.9 88 .7 86 .0 90.2 10.2 59 .7 76 .4 81.3 5.8 30 .5 52 .6 60.8 1 Baselines. We compare with none normalization (NN), batch normalization (BN) [22, 23] and pair normalization (PN) [24]. Their technical details are listed in Appendix. DGN Conﬁgurations. The key hyperparameters include group number G and balancing factor λ. Depending on the number of class labels, we apply 5 groups to Pubmed and 10 groups to the others. The criterion is to use more groups to separate representation distributions in networked data accompanied with more class labels. λis tuned on validation sets to ﬁnd a good trade-off between preserving input features and group normalization. We introduce the selection of λin Appendix. 4.2 Experiment Results Studies on alleviating the over-smoothing problem.To answerQ1, Table 1 summarizes the results of applying different normalization techniques to GNN models on all datasets. We report the performance of GCN and GAT with 2/15/30 layers, and SGC with 5/60/120 layers due to space limit. We provide test accuracies, instance information gain and group distance ratio under all depths in Appendix. It can be observed that DGN has signiﬁcantly alleviated the over-smoothing issue. Given the same layers, DGN almost outperforms all other normalization methods for all cases and greatly slows down the performance dropping. It is because the self-preserved component H(k) in Eq. (6) keeps the informative input features and avoids over-normalization to distinguish different nodes. This component is especially crucial for models with a few layers since the over-smoothing issue has not appeared. The other group normalization component in Eq. (6) processes each group of nodes independently. It disentangles the representation similarity between groups, and hence reduces the over-smoothness of nodes over a graph accompanied with graph convolutions. Studies on enabling deeper and better GNNs. To answer Q2, we compare all of the concerned normalization methods over GCN, GAT, and SGC in the scenario with missing features. As we have discussed, normalization techniques will show their power in relieving the over-smoothing issue and exploring deeper architectures especially for this scenario. In Table 2, Acc represents the best test accuracy yielded by model equipped with the optimal layer number #K. We can observe that DGN signiﬁcantly outperforms the other normalization methods on all cases. The average improvements over NN, BN and PN achieved by DGN are 37.8%, 7.1% and 12.8%, respectively. Compared with vanilla GNN models without any normalization layer, the optimal models accompanied with normalization layers (especially for our DGN) usually possess larger values of #K. It demonstrates that DGN enables to explore deeper architectures to exploit neighborhood information with more hops away by tackling the over-smoothing issue. We present the comprehensive analyses in terms of test accuracy, instance information gain and group distance ratio under all depths in Appendix. Hyperparameter studies. We study the impact of hyperparameters, group number Gand balancing factor λ, on DGN in order to answer research question Q3. Over the GCN framework associated with 20 convolutional layers, we evaluate DGN by considering Gand λfrom sets [1,5,10,15,20,30] and [0.001,0.005,0.01,0.03,0.05,0.1], respectively. The left part in Figure 3 presents the test accuracy 7Table 2: The highest accuracy (%) and the accompanied optimal layers in the scenario with missing features. We calculate the average improvement achieved by DGN over each GNN framework. Model Norm Cora Citeseer Pubmed CoauthorCS Improvement%Acc #K Acc #K Acc #K Acc #K GCN NN 57.3 3 44.0 6 36.4 4 67.3 3 42.2 BN 71.8 20 45.1 25 70.4 30 82.7 30 5.2 PN 65.6 20 43.6 25 63.1 30 63.5 4 19.2 DGN 76.3 20 50.2 30 72.0 30 83.7 25 - GAT NN 50.1 2 40.8 4 38.5 4 63.7 3 51.0 BN 72.7 5 48.7 5 60.7 4 80.5 6 9.8 PN 68.8 8 50.3 6 63.2 20 66.6 3 14.7 DGN 75.8 8 54.5 5 72.3 20 83.6 15 - SGC NN 63.4 5 51.2 40 63.7 5 71.0 5 20.1 BN 78.5 20 50.4 20 72.3 50 84.4 20 6.2 PN 73.4 50 58.0 120 75.2 30 80.1 10 4.5 DGN 80.2 50 58.2 90 76.2 90 85.8 20 - 0.2 0.1 0.3 0.4 0.5 0.08 0.6 30 0.7 0.06 25 0.8 20 0.04  15 100.02 5 0 0 Figure 3: Left: Test accuracies of GCN with 20 layers on Cora with missing features, where hyperparameters Gand λare studied. Middle: Node representation visualization for GCN without normalization and with K = 20. Right: Node representation visualization for GCN with DGN layer and K = 20 (node colors represent classes, and black triangles denote the running means of groups). for each hyperparameter combination. We observe that: (i) The model performance is damaged greatly when λis close to zero (e.g.,λ= 0.001). In this case, group normalization contributes slightly in DGN, resulting in over-smoothing in the GCN model. (ii) Model performance is not sensitive to the value of G, and an appropriate λvalue could be tuned to optimize the trade-off between instance gain and group normalization. It is because DGN learns to use the appropriate number of groups by end-to-end training. In particular, some groups might not be used as shown in the right part of Figure 3, at which only 6 out of 10 groups (denoted by black triangles) are adopted. (iii) Even when G= 1, DGN still outperforms BN by utilizing the self-preserved component to achieve an accuracy of 74.7%, where λ= 0.1. Via increasing the group number, the model performance could be further improved, e.g., the accuracy of 76.3% where G= 10 and λ= 0.01. Node representation visualization. We investigate how DGN clusters nodes into different groups to tackle the over-smoothing issue. The middle and right parts of Figure 3 visualize the node representations achieved by GCN models without normalization tool and with the DGN approach, respectively. It is observed that the node representations of different classes mix together when the layer number reaches 20 in the GCN model without normalization. In contrast, our DGN method softly assigns nodes into a series of groups, whose running means at the corresponding normalization modules are highlighted with black triangles. Through normalizing each group independently, the running means are separated to improve inter-group distances and disentangle node representations. In particular, we notice that the running means locate at the borders among different classes (e.g., the upper-right triangle at the border between red and pink classes). That is because the soft assignment may cluster nodes of two or three classes into the same group. Compared with batch or pair normalization, the independent normalization for each group only includes a few classes in DGN. In this way, we relieve the representation noise from other node classes during normalization, and improve the group distance ratio as illustrated in Appendix. 85 Conclusion In this paper, we propose two over-smoothing metrics based on graph structures, i.e., group distance ratio and instance information gain. By inspecting GNN models through the lens of these two metrics, we present a novel normalization layer, DGN, to boost model performance against over- smoothing. It normalizes each group of similar nodes independently to separate node representations of different classes. Experiments on real-world classiﬁcation tasks show that DGN greatly slowed down performance degradation by alleviating the over-smoothing issue. DGN enables us to explore deeper GNNs and achieve higher performance in analyzing attributed networks and the scenario with missing features. Our research will facilitate deep learning models for potential graph applications. Broader Impact The successful outcome of this work will lead to advances in building up deep graph neural networks and dealing with complex graph-structured data. The developed metrics and algorithms have an immediate and strong impact on a number of ﬁelds, including (1) Over-smoothing Quantitative Analysis: GNN models tend to result in the over-smoothing issue with the increase in the number of layers. During the practical development of deeper GNN models, the proposed instance information gain and group distance ratio effectively indicate the over-smoothing issue, in order to push the model exploration toward a good direction. (2) Deep GNN Modeling: The proposed differentiable group normalization tool successfully tackles the over-smoothing issue and enables the modeling of deeper GNN variants. It encourages us to fully unleash the power of deep learning in processing the networked data. (3) Real-world Network Analytics Applications: The proposed research will broadly shed light on utilizing deep GNN models in various applications, such as social network analysis, brain network analysis, and e-commerce network analysis. For such complex graph-structured data, deep GNN models can exploit the multi-hop neighborhood information to boost the task performance. References [1] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008. [2] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. [3] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A comprehensive survey on graph neural networks. arXiv, 2019. [4] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeuIPS, pages 2224–2232, 2015. [5] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [6] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeuIPS, pages 1024–1034, 2017. [7] Xiao Huang, Qingquan Song, Yuening Li, and Xia Hu. Graph recurrent networks with at- tributed random walks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 732–740, 2019. [8] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1416–1424, 2018. [9] Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019. [10] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. ICLR, 2017. [11] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv, 1(2), 2017. 9[12] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Simplifying graph convolutional networks. arXiv preprint arXiv:1902.07153, 2019. [13] Kaixiong Zhou, Qingquan Song, Xiao Huang, Daochen Zha, Na Zou, and Xia Hu. Multi-channel graph neural networks. arXiv preprint arXiv:1912.08306, 2019. [14] Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ﬁlters. arXiv preprint arXiv:1905.09550, 2019. [15] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415–444, 2001. [16] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018. [17] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In International Conference on Learning Representations, 2020. [18] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou. Specae: Spectral autoencoder for anomaly detection in attributed networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2233–2236, 2019. [19] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. arXiv preprint arXiv:1909.03211, 2019. [20] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Representations. https://openreview. net/forum, 2020. [21] Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming- Chang Yang. Measuring and improving the use of graph information in graph neural networks, 2020. [22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. [23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015. [24] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint arXiv:1909.12223, 2019. [25] Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861, 2016. [26] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018. [27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [28] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1263–1272. JMLR. org, 2017. [29] Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017. [30] Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. Entropy, 21(12):1181, 2019. [31] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE International Conference on Computer Vision, pages 9267–9276, 2019. [32] Nezihe Merve Gürel, Hansheng Ren, Yujing Wang, Hui Xue, Yaming Yang, and Ce Zhang. An anatomy of graph neural networks going deep via the lens of mutual information: Exponential decay vs. full preservation. arXiv preprint arXiv:1910.04499, 2019. 10[33] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018. [34] Al Mamunur Rashid, George Karypis, and John Riedl. Learning preferences of new users in recommender systems: an information theoretic approach. Acm Sigkdd Explorations Newsletter, 10(2):90–100, 2008. [35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. [36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, 2014. [37] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedfor- ward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256, 2010. 11A Dataset Statistics For fair comparison with previous work, we perform the node classiﬁcation task on four benchmark datasets, including Cora, Citeseer, Pubmed [ 25], and CoauthorCS [ 35]. They have been widely adopted to study the over-smoothing issue in GNNs [21, 19, 24, 16, 20]. The detailed statistics are listed in Table 3. To further illustrate that the normalization techniques could enable deeper GNNs to achieve better performance, we apply them to a more complex scenario with missing features. For these four benchmark datasets, we create the corresponding scenarios by removing node features in both validation and testing sets. Table 3: Dataset statistics on Cora, Citeseer, Pubmed, and CoauthorCS. Cora Citeseer Pubmed CoauthorCS #Nodes 2708 3327 19717 18333 #Edges 5429 4732 44338 81894 #Features 1433 3703 500 6805 #Classes 7 6 3 15 #Training Nodes 140 120 60 600 #Validation Nodes 500 500 500 2250 #Testing Nodes 1000 1000 1000 15483 B Running Environment All the GNN models and normalization approaches are implemented in PyTorch, and tested on a machine with 24 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GB processors, GeForce GTX-1080 Ti 12 GB GPU, and 128GB memory size. We implement the group normalization in a parallel way. Thus the practical time cost of our DGN is comparable to that of traditional batch normalization. C GNN Models We test over three general GNN models to illustrate the over-smoothing issue, including graph convo- lutional networks (GCN) [10], graph attention networks (GAT) [11] and simple graph convolution (SGC) networks [12]. We list their neighbor aggregation functions in Table 4. Table 4: Neighbor aggregation function at a graph convolutional layer for GCN, GAT and SGC. Model Neighbor aggregation function GCN h(k) v = ReLU(∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) W(k)h(k−1) v′ ) GAT h(k) v = ReLU(∑ v′∈N(v)∪{v}a(k) vv′W(k)h(k−1) v′ ) SGC h(k) v = ∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) h(k−1) v′ Considering the message passing strategy as shown by Eq. (1) in the main manuscript, we explain the key properties of GCN, GAT and SGC as follows. GCN merges the information from node itself and its neighbors weighted by vertices’ degrees, wherea(k) vv′ = 1./ √ (|N(v)|+ 1) ·(|N(v′)|+ 1). Functions AGG and COM are realized by a summation pooling. The activation function of ReLU is then applied to non-linearly transform the latent embedding. Based on GCN, GAT uses an additional attention layer to learn link weight a(k) vv′. GAT aggregates neighbors with the trainable link weights, and achieves signiﬁcant improvements in a variety of applications. SGC is simpliﬁed from GCN by removing all trainable parameters W(k) and nonlinear activations between successive layers. It has been empirically shown that these simpliﬁcations do not negatively impact classiﬁcation accuracy, and even relive the problems of over-ﬁtting and vanishing gradients in deeper models. 12D Normalization Baselines Batch normalization is ﬁrst applied between the successive convolutional layers in CNNs [23]. It is extended to graph neural networks to improve node representation learning and generalization [22]. Taking embedding matrix H(k) as input after each layer, batch normalization scales the node rep- resentations using running mean and variance, and generates a new embedding matrix for the next graph convolutional layer. Formally, we have: ˜H(k) = γ(H(k) −µ σ ) + β ∈Rn×d(k) . µand σdenote the vectors of running mean and standard deviation, respectively; γ and β denote the trainable scale and shift vectors, respectively. Recently, pair normalization has been proposed to tackle the over-smoothing issue in GNNs, targeting at maintaining the average node pair distance over a graph [24]. Pair normalization is a simplifying realization of batch normalization by removing the trainable γ and β. In this work, we augment each graph convolutional layer via appending a normalization module, in order to validate the effectiveness of normalization technique in relieving over-smoothing and enabling deeper GNNs. E Hyperparameter Tuning in DGN The balancing factor, λ, is crucial to determine the trade-off between input feature preservation and group normalization in DGN. It needs to be tuned carefully as GNN models increase the number of layers. To be speciﬁc, we consider the candidate set {5 ·10−4,1 ·10−3,2 ·10−3,3 ·10−3,5 · 10−3,1 ·10−2,2 ·10−2,3 ·10−2,5 ·10−2}. For each speciﬁc model, we use a few epochs to choose the optimal λon the validation set, and then evaluate it on the testing set. We observe that the value of λtends to be larger in the model accompanied with more graph convolutional layers. That is because the over-smoothing issue gets worse with the increase in layer number. The group normalization is much more required to separate the node representations of different classes. F Instance Information Gain In this work, we adopt kernel-density estimators (KDE), one of the common non-parametric ap- proaches, to estimate the mutual information between input feature and representation vector [29, 30]. A key assumption in KDE is that the input feature (or output representation vector) of neural networks is distributed as a mixture of Gaussians. Since a neural network is a deterministic function of the input feature after training, the mutual information would be inﬁnite without such assumption. In the following, we ﬁrst formally deﬁne the Gaussian assumption, input probability distribution and representation probability distribution, and then present how to obtain the instance information gain based on the mutual information metric. Gaussian assumption. In the graph signal processing, it is common to assume that the collected input feature contains both true signal and noise. In other word, we have the input feature as follows: xv = ¯xv + ϵx. ¯xv denotes the true value, and ϵx ∼N(0,σ2I) denotes the added Gaussian noise with variance σ2. Therefore, input feature xv is a Gaussian variable centered on its true value. Input probability distribution. We treat the empirical distribution of input samples as true distri- bution. Given a dataset accompanied with nsamples, we have a series of input features{x1,··· ,xn} for all the samples. Each node feature is sampled with probability 1/|V|following the empirical uniform distribution. Let |V|denotes the number of samples, and let Xdenote the random variable of input features. Based on the above Gaussian assumption, probability PX(xv) of input feature xv is obtained by the product of 1/|V|with Gaussian probability centered on true value ¯xv. Representation probability distribution. Let Hdenote the random variable of node represen- tations. To obtain probability PH(hv) of continuous vector hv, a general approach is to bin and transform Hinto a new discrete variable. However, with the increasing dimensions of hv, it is non-trivial to statistically count the frequencies of all possible discrete values. Considering the task of node classiﬁcation, the index of largest element along vector hv ∈RC×1 is regarded as the label 13of a node. We propose a new binning approach that labels the whole vector hv with the largest index zv. In this way, we only have Cclasses of discrete values to facilitate the frequency counting. To be speciﬁc, let Pc denote the number of representation vectors whose indexes zv = c. The probability of a discrete variable with class cis given by: pc = PH(zv = c) = Pc∑C l=1 Pl . Mutual information calculation. Based on KDE approach, a lower bound of mutual information between input feature and representation vector can be calculated as: GIns = I(X; H) = ∑ xv∈X,hv∈HPXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv) = H(X) −H(X|H) ≥− 1 |V| ∑ ilog 1 |V| ∑ jexp(−1 2 ||xi−xj||2 2 4σ2 ) −∑C c=1 pc[−1 Pc ∑ i,zi=clog 1 Pc ∑ j,zj=cexp(−1 2 ||xi−xj||2 2 4σ2 )]. The sum over i,zi = crepresents a summation over all the input features whose representation vectors are labeled with zi = c. PXH(xv,hv) denotes the joint probability of xv and hv. The effectiveness of GIns in measuring mutual information between input feature and node representation has been demonstrated in the experimental results. As illustrated in Figures 4-7, GIns decreases with the increasing number of graph convolutional layers. This practical observation is in line with the human expert knowledge about neighbor aggregation strategy in GNNs. The neighbor aggregation function as shown in Table 4 is in fact a low-passing smoothing operation, which mixes the input feature of a node with those of its neighbors gradually. At the extreme cases where K = 30 or 120, we ﬁnd that GIns approaches to zero in GNN models without normalization. The loss of informative input feature leads to the dropping of node classiﬁcation accuracy. However, our DGN keeps the input information during graph convolutions and normalization to some extent, resulting in the largest GIns compared with the other normalization approaches. G Performance Comparison on Attributed Graphs In this section, we report the model performances in terms of test accuracy, instance information gain and group distance ratio achieved on all the concerned datasets in Figures 4-7. We make the following observations: • Comparing with other normalization techniques, our DGN generally slows down the dropping of test accuracy with the increase in layer number. Even for GNN models associated with a small number of layers (i.e., G≤5), DGN achieves the competitive performance compared with none normalization. The adoption of DGN module does not damage the model performance, and prevents model from suffering over-smoothing issue when GNN goes deeper. • DGN achieves the larger or comparable instance information gains in all cases, especially for GAT models. That is because DGN keeps embedding matrix H(k) and prevents over-normalization within each group. The preservation of H(k) saves input features to some extent after each layer of graph convolutions and normalization. In an attributed graph, the improved preservation of informative input features in the ﬁnal representations will signiﬁcantly facilitate the downstream node classiﬁcation. Furthermore, such preservation is especially crucial for GNN models with a few layers, since the over-smoothing issue has not appeared. • DGN normalizes each group of node representations independently to generally improve the group distance ratio, especially for models GCN and GAT. A larger value of group distance ratio means that the node representation distributions from all groups are disentangled to address the over-smoothing issue. Although the ratios of DGN are smaller than those of pair normalization in some cases upon SGC framework, we still achieve the largest test accuracy. That may be because the intra-group distance in DGN is much smaller than that of pair normalization. A small value of intra-group distance would facilitate the node classiﬁcation within the same group. We will further compare the intra-group distance in scenarios with missing features in the following experiments. 140 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 0.10 Instance gain None batch pair group 0 10 20 30 1 2 3 4 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0.10 0 10 20 30 1 2 3 4 0 50 100 Layers 0.4 0.6 0.8SGC 0 50 100 Layers 0.050 0.075 0.100 0.125 0 50 100 Layers 2 3 Figure 4: The test accuracy, instance information gain, and group distance ratio in attributed Cora. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 0.000 0.025 0.050 0.075 Instance gain None batch pair group 0 10 20 30 1.0 1.5 2.0 Group ratio 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 0.000 0.025 0.050 0.075 0 10 20 30 1.0 1.5 2.0 0 50 100 Layers 0.5 0.6 0.7SGC 0 50 100 Layers 0.06 0.08 0 50 100 Layers 1.25 1.50 1.75 2.00 Figure 5: The test accuracy, instance information gain, and group distance ratio in attributed Citeseer. We compare differentiable group normalization with none, batch and pair normalizations. 150 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 Instance gain None batch pair group 0 10 20 30 1 2 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0 10 20 30 1.0 1.5 2.0 2.5 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 0.025 0.050 0.075 0.100 0 50 100 Layers 1.5 2.0 Figure 6: The test accuracy, instance information gain, and group distance ratio in attributed Pubmed. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 0.0 0.1 0.2 0.3 Instance gain None batch pair group 0 10 20 30 2 4 6 Group ratio 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 0.0 0.1 0.2 0.3 0 10 20 30 2 4 6 8 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 0.0 0.1 0.2 0.3 0 50 100 Layers 2 4 6 Figure 7: The test accuracy, instance information gain, and group distance ratio in attributed Coau- thorCS. We compare differentiable group normalization with none, batch and pair normalizations. 16H Performance Comparison in Scenarios with Missing Features In this section, we report the model performances in terms of test accuracy, group distance ratio and intra-group distance achieved in scenarios with missing features in Figures 8-11. The intra-group distance is calculated by node pair distance averaged within the same group. Its mathematical expression is given by the denominator of Equation (3) in the main manuscript. We make the following observations: • DGN achieves the largest test accuracy by exploring the deeper neural architecture with a larger number of graph convolutional layers. In the scenarios with missing features, GNN model relies highly on the neighborhood structure to classify nodes. DGN enables the deeper GNN model to exploit neighborhood structure with multiple hops away, and at the same time relieves the over-smoothing issue. • Comparing with other normalization techniques, DGN generally improves the group distance ratio to relieve over-smoothing issue. Although in some cases the ratios are smaller than those of pair normalization upon SGC framework, we still achieve the comparable or even better test accuracy. That is because DGN has a smaller intra-group distance to facilitate node classiﬁcation within the same group, which is analyzed in the followings. • DGN obtains an appropriate intra-group distance to optimize the node classiﬁcation task. While the over-smoothing issue results in an extremely-small distance in the model without normalization, a larger one in pair normalization leads to the inaccurate node classiﬁcation within each group. That is because the pair normalization is designed to maintain the distance between each pair of nodes, no matter whether they locate in the same class group or not. The divergence of node representations in a group prevents a downstream classiﬁer to assign them the same class label. 170 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1 2 3 Group ratio None batch pair group 0 10 20 30 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1 2 3 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 2 3 0 50 100 Layers 0.5 1.0 Figure 8: The test accuracy, group distance ratio and intra-group distance in Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4GCN Test accuracy 0 10 20 30 1.1 1.2 1.3 Group ratio None batch pair group 0 10 20 30 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4GAT 0 10 20 30 1.0 1.2 1.4 0 10 20 30 0.0 0.5 1.0 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.2 1.4 0 50 100 Layers 0.5 1.0 Figure 9: The test accuracy, group distance ratio and intra-group distance in Citeseer with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 180 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1.0 1.2 1.4 1.6 Group ratio None batch pair group 0 10 20 30 0.0 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1.00 1.25 1.50 1.75 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.0 1.5 2.0 0 50 100 Layers 0.0 0.5 1.0 Figure 10: The test accuracy, group distance ratio and intra-group distance in Pubmed with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 2 3 4 Group ratio None batch pair group 0 10 20 30 0.00 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 2 3 4 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 2 4 0 50 100 Layers 0.0 0.5 1.0 Figure 11: The test accuracy, group distance ratio and intra-group distance in CoauthorCS with missing features. We compare differentiable group normalization with none, batch and pair normal- izations. 19",
      "meta_data": {
        "arxiv_id": "2006.06972v1",
        "authors": [
          "Kaixiong Zhou",
          "Xiao Huang",
          "Yuening Li",
          "Daochen Zha",
          "Rui Chen",
          "Xia Hu"
        ],
        "published_date": "2020-06-12T07:18:02Z",
        "pdf_url": "https://arxiv.org/pdf/2006.06972v1.pdf"
      }
    },
    {
      "title": "Node Dependent Local Smoothing for Scalable Graph Learning",
      "abstract": "Recent works reveal that feature or label smoothing lies at the core of Graph\nNeural Networks (GNNs). Concretely, they show feature smoothing combined with\nsimple linear regression achieves comparable performance with the carefully\ndesigned GNNs, and a simple MLP model with label smoothing of its prediction\ncan outperform the vanilla GCN. Though an interesting finding, smoothing has\nnot been well understood, especially regarding how to control the extent of\nsmoothness. Intuitively, too small or too large smoothing iterations may cause\nunder-smoothing or over-smoothing and can lead to sub-optimal performance.\nMoreover, the extent of smoothness is node-specific, depending on its degree\nand local structure. To this end, we propose a novel algorithm called\nnode-dependent local smoothing (NDLS), which aims to control the smoothness of\nevery node by setting a node-specific smoothing iteration. Specifically, NDLS\ncomputes influence scores based on the adjacency matrix and selects the\niteration number by setting a threshold on the scores. Once selected, the\niteration number can be applied to both feature smoothing and label smoothing.\nExperimental results demonstrate that NDLS enjoys high accuracy --\nstate-of-the-art performance on node classifications tasks, flexibility -- can\nbe incorporated with any models, scalability and efficiency -- can support\nlarge scale graphs with fast training.",
      "full_text": "Node Dependent Local Smoothing for Scalable Graph Learning Wentao Zhang1, Mingyu Yang1, Zeang Sheng1, Yang Li1 Wen Ouyang2, Yangyu Tao2, Zhi Yang1,3, Bin Cui1,3,4 1School of CS, Peking University 2Tencent Inc. 3 Key Lab of High Conﬁdence Software Technologies, Peking University 4Institute of Computational Social Science, Peking University (Qingdao), China 1{wentao.zhang, ymyu, shengzeang18, liyang.cs, yangzhi, bin.cui}@pku.edu.cn 2{gdpouyang, brucetao}@tencent.com Abstract Recent works reveal that feature or label smoothing lies at the core of Graph Neural Networks (GNNs). Concretely, they show feature smoothing combined with simple linear regression achieves comparable performance with the carefully designed GNNs, and a simple MLP model with label smoothing of its prediction can outperform the vanilla GCN. Though an interesting ﬁnding, smoothing has not been well understood, especially regarding how to control the extent of smoothness. Intuitively, too small or too large smoothing iterations may cause under-smoothing or over-smoothing and can lead to sub-optimal performance. Moreover, the extent of smoothness is node-speciﬁc, depending on its degree and local structure. To this end, we propose a novel algorithm called node-dependent local smoothing (NDLS), which aims to control the smoothness of every node by setting a node- speciﬁc smoothing iteration. Speciﬁcally, NDLS computes inﬂuence scores based on the adjacency matrix and selects the iteration number by setting a threshold on the scores. Once selected, the iteration number can be applied to both feature smoothing and label smoothing. Experimental results demonstrate that NDLS enjoys high accuracy – state-of-the-art performance on node classiﬁcations tasks, ﬂexibility – can be incorporated with any models, scalability and efﬁciency – can support large scale graphs with fast training. 1 Introduction In recent years, Graph Neural Networks (GNNs) have received a surge of interest with the state-of-the- art performance on many graph-based tasks [2, 41, 12, 39, 33, 34]. Recent works have found that the success of GNNs can be mainly attributed to smoothing, either at feature or label level. For example, SGC [32] shows using smoothed features as input to a simple linear regression model achieves comparable performance with lots of carefully designed and complex GNNs. At the smoothing stage, features of neighbor nodes are aggregated and combined with the current node’s feature to form smoothed features. This process is often iterated multiple times. The smoothing is based on the assumption that labels of nodes that are close to each other are highly correlated, therefore, the features of nodes nearby should help predict the current node’s label. One crucial and interesting parameter of neighborhood feature aggregation is the number of smoothing iterations k, which controls how much information is being gathered. Intuitively, an aggregation process of kiterations (or layers) enables a node to leverage information from nodes that are k-hop away [26, 38]. The choice of k is closely related to the structural properties of graphs and has a 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2110.14377v1  [cs.LG]  27 Oct 2021(a) Two nodes with different local structures /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000026/uni00000027/uni00000029  /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (b) The CDF of LSI in different graphs Figure 1: (Left) The node in dense region has larger smoothed area within two iterations of propagation. (Right) The CDF of LSI in three citation networks. signiﬁcant impact on the model performance. However, most existing GNNs only consider the ﬁxed-length propagation paradigm – a uniform kfor all the nodes. This is problematic since the number of iterations should be node dependent based on its degree and local structures. For example, as shown in Figure 1(a), the two nodes have rather different local structures, with the left red one resides in the center of a dense cluster and the right red one on the periphery with few connections. The number of iterations to reach an optimal level of smoothness are rather different for the two nodes. Ideally, poorly connected nodes (e.g., the red node on the right) needs large iteration numbers to efﬁciently gather information from other nodes while well-connected nodes (e.g., the red node on the left) should keep the iteration number small to avoid over-smoothing. Though some learning-based approaches have proposed to adaptively aggregate information for each node through gate/attention mechanism or reinforcement learning [ 29, 21, 40, 27], the performance gains are at the cost of increased training complexity, hence not suitable for scalable graph learning. In this paper, we propose a simple yet effective solution to this problem. Our approach, called node-dependent local smoothing (NDLS), calculates a node-speciﬁc iteration number for each node, referred to as local smooth iteration (LSI). Once the LSI for a speciﬁc node is computed, the corresponding local smoothing algorithm only aggregates the information from the nodes within a distance less than its LSI as the new feature. The LSI is selected based on inﬂuence scores, which measure how other nodes inﬂuence the current node. NDLS sets the LSI for a speciﬁc node to be the minimum number of iterations so that the inﬂuence score is ϵ-away from the over-smoothing score, deﬁned as the inﬂuence score at inﬁnite iteration. The insight is that each node’s inﬂuence score should be at a reasonable level. Since the nodes with different local structures have different “smoothing speed”, we expect the iteration number to be adaptive. Figure 1(b) illustrates Cumulative Distribution Function (CDF) for the LSI of individual nodes in real-world graphs. The heterogeneous and long-tail property exists in all the datasets, which resembles the characteristics of the degree distribution of nodes in real graphs. Based on NDLS, we propose a new graph learning algorithm with three stages: (1) feature smoothing with NDLS (NDLS-F); (2) model training with smoothed features; (3) label smoothing with NDLS (NDLS-L). Note that in our framework, the graph structure information is only used in pre-processing and post-processing steps, i.e., stages (1) and (3) (See Figure 2). Our NDLS turns a graph learning problem into a vanilla machine learning problem with independent samples. This simplicity enables us to train models on larger-scale graphs. Moreover, our NDLS kernel can act as a drop-in replacement for any other graph kernels and be combined with existing models such as Multilayer Perceptron (MLP), SGC [32], SIGN [28], S2GC [42] and GBP [6]. Extensive evaluations on seven benchmark datasets, including large-scale datasets like ogbn- papers100M [16], demonstrates that NDLS achieves not only the state-of-the-art node classiﬁcation performance but also high training scalability and efﬁciency. Especially, NDLS outperforms APPNP [29] and GAT [30] by a margin of 1.0%-1.9% and 0.9%-2.4% in terms of test accuracy, while achieving up to 39×and 186×training speedups, respectively. 2 Preliminaries In this section, we ﬁrst introduce the semi-supervised node classiﬁcation task and review the prior models, based on which we derive our method in Section 3. Consider a graphG= (V, E) with |V|= n 2nodes and |E|= medges, the adjacency matrix (including self loops) is denoted as ˜A ∈Rn×n and the feature matrix is denoted as X = {x1,x2...,xn}in which xi ∈Rf represents the feature vector of node vi. Besides, Y = {y1,y2...,yl}is the initial label matrix consisting of one-hot label indicator vectors. The goal is to predict the labels for nodes in the unlabeled set Vu with the supervision of labeled set Vl. GCN smooths the representation of each node via aggregating its own representations and the ones of its neighbors’. This process can be deﬁned as X(k+1) = δ ( ˆAX(k)W(k) ) , ˆA = ˜Dr−1 ˜A˜D−r, (1) where ˆA is the normalized adjacency matrix, r∈[0,1] is the convolution coefﬁcient, and ˜D is the diagonal node degree matrix with self loops. Here X(k) and X(k+1) are the smoothed node features of layer k and k+ 1respectively while X(0) is set to X, the original feature matrix. In addition, W(k) is a layer-speciﬁc trainable weight matrix at layer k, and δ(·) is the activation function. By setting r= 0.5, 1 and 0, the convolution matrix ˜Dr−1 ˜A˜D−r represents the symmetric normalization adjacency matrix ˜D−1/2 ˜A˜D−1/2 [20], the transition probability matrix ˜A˜D−1 [37], and the reverse transition probability matrix ˜D−1 ˜A [35], respectively. SGC. For each GCN layer deﬁned in Eq. 1, if the non-linear activation function δ(·) is an identity function and W(k) is an identity matrix, we get the smoothed feature after k-iterations propagation as X(k) = ˆAkX. Recent studies have observed that GNNs primarily derive their beneﬁts from performing feature smoothing over graph neighborhoods rather than learning non-linear hierarchies of features as implied by the analogy to CNNs [ 25, 10, 15]. By hypothesizing that the non-linear transformations between GCN layers are not critical, SGC [32] ﬁrst extracts the smoothed features X(k) then feeds them to a linear model, leading to higher scalability and efﬁciency. Following the design principle of SGC, piles of works have been proposed to further improve the performance of SGC while maintaining high scalability and efﬁciency, such as SIGN [28], S2GC [42] and GBP [6]. Over-Smoothing [22] issue. By continually smoothing the node feature with inﬁnite number of propagation in SGC, the ﬁnal smoothed feature X(∞) is X(∞) = ˆA∞X, ˆA∞ i,j = (di + 1)r(dj + 1)1−r 2m+ n , (2) where ˆA∞is the ﬁnal smoothed adjacency matrix, ˆA∞ i,j is the weight between nodes vi and vj, di and dj are the node degrees for vi and vj, respectively. Eq. (2) shows that as we smooth the node feature with an inﬁnite number of propagations in SGC, the ﬁnal feature is over-smoothed and unable to capture the full graph structure information since it only relates with the node degrees of target nodes and source nodes. For example, if we set r= 0or 1, all nodes will have the same smoothed features because only the degrees of the source or target nodes have been considered. 3 Local Smoothing Iteration (LSI) The features after k iterations of smoothing is X(k) = ˆAkX. Inspired by [ 35], we measure the inﬂuence of node vj on node vi by measuring how much a change in the input feature of vj affects the representation of vi after kiterations. For any node vi, the inﬂuence vector captures the inﬂuences of all other nodes. Considering the hth feature of X, we deﬁne an inﬂuence matrix Ih(k): Ih(k)ij = ∂ˆX(k) ih ∂ˆX(0) jh . (3) I(k) = ˆAk,˜Ii = ˆA∞ (4) Since Ih(k) is independent to h, we replace Ih(k) with I(k), which can be further represented as I(k) = Ih(k), ∀h ∈{1,2,..,f }, where f indicates the number of features of X. We denote I(k)i as the ith row of I(k), and ˜I as I(∞). Given the normalized adjacency matrix ˆA, we can 3have I(k) = ˆAk and ˜I = ˆA∞. According to Eq. (2), ˜I converges to a unique stationary matrix independent of the distance between nodes, resulting in that the aggregated features of nodes are merely relative with their degrees (i.e., over-smoothing). We denote I(k)i as the ith row of I(k), and it means the inﬂuence from the other nodes to the node vi after k iterations of propagation. We introduce a new concept local smoothing iteration (parameterized by ϵ), which measures the minimal number of iterations krequired for the inﬂuence of other nodes on node vi to be within an ϵ-distance to the over-smoothing stationarity ˜Ii. Deﬁnition 3.1. Local-Smoothing Iteration (LSI, parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −I(k)i||2 <ϵ}, (5) where ||·||2 is two-norm, and ϵis an arbitrary small constant with ϵ> 0. Here ϵis a graph-speciﬁc parameter, and a smaller ϵindicates a stronger smoothing effect. The ϵ-distance to the over-smoothing stationarity ˜Ii ensures that the smooth effect on node vi is sufﬁcient and bounded to avoid over-smoothing. As shown in Figure 1(b), we can have that the distribution of LSI owns the heterogeneous and long-tail property, where a large percentage of nodes have much smaller LSI than the rest. Therefore, the required LSI to approach the stationarity is heterogeneous across nodes. Now we discuss the connection between LSI and node local structure, showcasing nodes in the sparse region (e.g., both the degrees of itself and its neighborhood are low) can greatly prolong the iteration to approach over-smoothing stationarity. This heterogeneity property is not fully utilized in the design of current GNNs, leaving the model design in a dilemma between unnecessary iterations for a majority of nodes and insufﬁcient iterations for the rest of nodes. Hence, by adaptively choosing the iteration based on LSI for different nodes, we can signiﬁcantly improve model performance. Theoretical Properties of LSI. We now analyze the factors determining the LSI of a speciﬁc node. To facilitate the analysis, we set the coefﬁcient r = 0for the normalized adjacency matrix ˆA in Eq. (1), thus ˆA = ˜D−1 ˜A. The proofs of following theorems can be found in Appendix A.1. Theorem 3.1. Given feature smoothing X(k) = ˆAkX with ˆA = ˜D−1 ˜A, we have K(i,ϵ) ≤logλ2  ϵ √ ˜di 2m+ n  , (6) where λ2 is the second largest eigenvalue of ˆA, ˜di denotes the degree of node vi plus 1 (i.e., ˜di = di + 1), and m, ndenote the number of edges and nodes respectively. Note that λ2 ≤1. Theorem 3.1 shows that the upper-bound of the LSI is positively correlated with the scale of the graph (m,n), the sparsity of the graph (small λ2 means strong connection and low sparsity, and vice versa), and negatively correlated with the degree of nodevi. Theorem 3.2. For any nodes iin a graph G, K(i,ϵ) ≤max {K(j,ϵ),j ∈N(i)}+ 1, (7) where N(i) is the set of node vi’s neighbours. Theorem 3.2 indicates that the difference between two neighboring nodes’ LSIs is no more than1, therefore the nodes with a super-node as neighbors (or neighbor’s neighbors) may have small LSIs. That is to say, the sparsity of the local area, where a node locates, also affects its LSI positively. Considering Theorems 3.1 and 3.2 together, we can have a union upper-bound of K(i,ϵ) as K(i,ϵ) ≤min   max {K(j,ϵ),j ∈N(i)}+ 1,logλ2  ϵ √ ˜di 2m+ n     . (8) 4 NDLS Pipeline The basic idea of NDLS is to utilize the LSI heterogeneity to perform a node-dependent aggregation over a neighborhood within a distance less than the speciﬁc LSI for each node. Further, we propose 4Adjacent Matrix A Final Prediction �Y ⋯ MLP Soft label �Y Stage 1: preprocessing Stage 2: training Stage 3: postprocessing  Feature X �X Smooth Feature with NDLS-F (replaceable) Adjacent Matrix A B A B1.0 B 0.3 0.3 0.7 … LSI Estimation A A’s feature smoothed  under optimal steps … 1.0 A0.8 0.3 0.2 0.2 0.1 … … LSI(A) = 4 LSI(B) = 2 0.1 0.3 … B’s feature smoothed  under optimal steps Details about NDLS-F B A Smooth Label with NDLS-L �XB�XA Figure 2: Overview of the proposed NDLS method, including (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). NDLS-F and NDLS-L correspond to pre-processing and post-processing steps respectively. a simple pipeline with three main parts (See Figure 2): (1) a node-dependent local smoothing of the feature (NDLS-F) over the graph, (2) a base prediction result with the smoothed feature, (3) a node-dependent local smoothing of the label predictions (NDLS-L) over the graph. Note this pipeline is not trained in an end-to-end way, the stages (1) and (3) in NDLS are only the pre-processing and post-processing steps, respectively. Furthermore, the graph structure is only used in the pre/post- processing NDLS steps, not for the base predictions. Compared with prior GNN models, this key design enables higher scalability and a faster training process. Based on the graph structure, we ﬁrst compute the node-dependent local smoothing iteration that maintains a proper distance to the over-smoothing stationarity. Then the corresponding local smoothing kernel only aggregates the information (feature or prediction) for each node from the nodes within a distance less than its LSI value. The combination of NDLS-F and NDLS-L takes advantage of both label smoothing (which tends to perform fairly well on its own without node features) and the node feature smoothing. We will see that combining these complementary signals yields state-of-the- art predictive accuracy. Moreover, our NDLS-F kernel can act as a drop-in replacement for graph kernels in other scalable GNNs such as SGC, S2GC, GBP, etc. 4.1 Smooth Features with NDLS-F Once the node-dependent LSI K(i,ϵ) for a speciﬁc node iis obtained, we smooth the initial input feature Xi of node iwith node-dependent LSI as: ˜Xi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 X(k) i . (9) To capture sufﬁcient neighborhood information, for each node vi, we average its multi-scale features {X(k) i |k≤K(i,ϵ)}obtained by aggregating information within khops from the node vi. The matrix form of the above equation can be formulated as ˜X(ϵ) = max i K(i,ϵ) ∑ k=0 M(k)X(k), M(k) ij = { 1 K(i,ϵ)+1 , i = j and k ≤K(i,ϵ) 0, otherwise , (10) where M(k) is a set of diagonal matrix. 4.2 Simple Base Prediction With the smoothed feature ˜X according to Eq. 9, we then train a model to minimize the loss –∑ vi∈Vl ℓ ( yi,f( ˜Xi) ) , where ˜Xi denotes the ith row of ˜X, ℓis the cross-entropy loss function, and f( ˜Xi) is the predictive label distribution for node vi. In NDLS, the default f is a MLP model and 5ˆY = f( ˜X) is its soft label predicted (softmax output). Note that, many other models such as Random Forest [24] and XGBoost [7] could also be used in NDLS (See more results in Appendix A.2). 4.3 Smooth Labels with NDLS-L Similar to the feature propagation, we can also propagate the soft label ˆY with ˆY(k) = ˆAk ˆY. Considering the inﬂuence matrix of softmax label Jh(k). Jh(k)ij = ∂ˆY(k) ih ∂ˆY(0) jh . (11) According to the deﬁnition above we have that Jh(k) =Ih(k),∀h∈{1,2,..,f }. (12) Therefore, local smoothing can be further applied to address over-smoothing in label propagation. Concretely, we smooth an initial soft label ˆYi of node vi with NDLS as follows ˜Yi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 ˆY(k) i . (13) Similarly, the matrix form of the above equation can be formulated as ˜Y(ϵ) = max i K(i,ϵ) ∑ k=0 M(k) ˆY(k), (14) where M(k) follows the deﬁnition in Eq. (10). 5 Comparison with Existing Methods Decoupled GNNs. The aggregation and transformation operations in coupled GNNs (i.e., GCN [18], GAT [30] and JK-Net [ 35]) are inherently intertwined in Eq. (1), so the propagation iterations L always equals to the transformation iterations K. Recently, some decoupled GNNs (e.g., PPNP [20], PPRGo [1], APPNP [20], AP-GCN [29] and DAGNN [25]) argue the entanglement of these two operations limits the propagation depth and representation ability of GNNs, so they ﬁrst do the transformation and then smooth and propagate the predictive soft label with higher depth in an end-to-end manner. Especially, AP-GCN and DAGNN both use a learning mechanism to learn propagation adaptively. Unfortunately, all these coupled and decoupled GNNs are hard to scale to large graphs – scalability issue since they need to repeatedly perform an expensive recursive neighborhood expansion in multiple propagations of the features or soft label predicted. NDLS addresses this issue by dividing the training process into multiple stages. Sampling-based GNNs. An intuitive method to tackle the recursive neighborhood expansion problem is sampling. As a node-wise sampling method, GraphSAGE [14] samples the target nodes as a mini-batch and samples a ﬁxed size set of neighbors for computing. VR-GCN [5] analyzes the variance reduction on node-wise sampling, and it can reduce the size of samples with an additional memory cost. In the layer level, Fast-GCN [ 3] samples a ﬁxed number of nodes at each layer, and ASGCN [17] proposes the adaptive layer-wise sampling with better variance control. For the graph-wise sampling, Cluster-GCN [8] clusters the nodes and only samples the nodes in the clusters, and GraphSAINT [37] directly samples a subgraph for mini-batch training. We don’t use sampling in NDLS since the sampling quality highly inﬂuences the classiﬁcation performance. Linear Models. Following SGC [32], some recent methods remove the non-linearity between each layer in the forward propagation. SIGN [28] allows using different local graph operators and proposes to concatenate the different iterations of propagated features. S2GC [42] proposes the simple spectral graph convolution to average the propagated features in different iterations. In addition, GBP [ 6] further improves the combination process by weighted averaging, and all nodes in the same layer share the same weight. In this way, GBP considers the smoothness in a layer perspective way. Similar 6Table 1: Algorithm analysis for existing scalable GNNs. n, m, c, and f are the number of nodes, edges, classes, and feature dimensions, respectively. bis the batch size, and krefers to the number of sampled nodes. Lcorresponds to the number of times we aggregate features, K is the number of layers in MLP classiﬁers. For the coupled GNNs, we always have K = L. Type Method Preprocessing and postprocessingTraining Inference Memory Node-wise samplingGraphSAGE - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Layer-wise samplingFastGCN - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Graph-wise samplingCluster-GCN O(m) O(Lmf+Lnf2) O(Lmf+Lnf2) O(bLf+Lf2) Linear model SGC O(Lmf) O(nf2) O(nf2) O(bf+f2)S2GC O(Lmf) O(nf2) O(nf2) O(bf+f2)SIGN O(Lmf) O(Knf2) O(Knf2) O(bLf+Kf2) GBP O(Lnf+L √mlgnε ) O(Knf2) O(Knf2) O(bf+Kf2)Linear model NDLS O(Lmf+Lmc) O(Knf2) O(Knf2) O(bf+Kf2) Table 2: Overview of datasets and task types (T/I represents Transductive/Inductive). Dataset #Nodes #Features #Edges #Classes #Train/Val/Test Type Description Cora 2,708 1,433 5,429 7 140/500/1,000 T citation network Citeseer 3,327 3,703 4,732 6 120/500/1,000 T citation network Pubmed 19,717 500 44,338 3 60/500/1,000 T citation network Industry 1,000,000 64 1,434,382 253 5K/10K/30K T short-form video network ogbn-papers100M 111,059,956 128 1,615,685,872 172 1,207K/125K/214K T citation network Flickr 89,250 500 899,756 7 44K/22K/22K I image network Reddit 232,965 602 11,606,919 41 155K/23K/54K I social network to these works, we also use a linear model for higher training scalability. The difference lies in that we consider the smoothness from a node-dependent perspective and each node in NDLS has a personalized aggregation iteration with the proposed local smoothing mechanism. Table 1 compares the asymptotic complexity of NDLS with several representative and scalable GNNs. In the stage of the preprocessing, the time cost of clustering in Cluster-GCN is O(m) and the time complexity of most linear models is O(Lmf). Besides, NDLS has an extra time cost O(Lmc) for the postprocessing in label smoothing. GBP conducts this process approximately with a bound of O(Lnf + L √mlg n ε ), where εis a error threshold. Compared with the sampling-based GNNs, the linear models usually have smaller training and inference complexity, i.e., higher efﬁciency. Memory complexity is a crucial factor in large-scale graph learning because it is difﬁcult for memory-intensive algorithms such as GCN and GAT to train large graphs on a single machine. Compared with SIGN, both GBP and NDLS do not need to store smoothed features in different iterations, and the feature storage complexity can be reduced from O(bLf) to O(bf). 6 Experiments In this section, we verify the effectiveness of NDLS on seven real-world graph datasets. We aim to answer the following four questions. Q1: Compared with current SOTA GNNs, can NDLS achieve higher predictive accuracy and why? Q2: Are NDLS-F and NDLS-L better than the current feature and label smoothing mechanisms (e.g., the weighted feature smoothing in GBP and the adaptive label smoothing in DAGNN)? Q3: Can NDLS obtain higher efﬁciency over the considered GNN models? Q4: How does NDLS perform on sparse graphs (i.e., low label/edge rate, missing features)? 6.1 Experimental Setup Datasets. We conduct the experiments on (1) six publicly partitioned datasets, including four citation networks (Citeseer, Cora, PubMed, and ogbn-papers100M) in [ 18, 16] and two social networks (Flickr and Reddit) in [37], and (2) one short-form video recommendation graph (Industry) from our industrial cooperative enterprise. The dataset statistics are shown in Table 2 and more details about these datasets can be found in Appendix A.3. Baselines. In the transductive setting, we compare our method with (1) the coupled GNNs: GCN [18], GAT [ 30] and JK-Net [ 35]; (2) the decoupled GNNs: APPNP [ 20], AP-GCN [ 29], 7Table 3: Results of transductive settings. OOM means “out of memory”. Type Models Cora Citeseer PubMed Industry ogbn-papers100M Coupled GCN 81.8 ±0.5 70.8 ±0.5 79.3 ±0.7 45.9 ±0.4 OOM GAT 83.0 ±0.7 72.5 ±0.7 79.0 ±0.3 46.8 ±0.7 OOM JK-Net 81.8 ±0.5 70.7 ±0.7 78.8 ±0.7 47.2 ±0.3 OOM Decoupled APPNP 83.3 ±0.5 71.8 ±0.5 80.1 ±0.2 46.7 ±0.6 OOM AP-GCN 83.4 ±0.3 71.3 ±0.5 79.7 ±0.3 46.9 ±0.7 OOM PPRGo 82.4 ±0.2 71.3 ±0.5 80.0 ±0.4 46.6 ±0.5 OOM DAGNN (Gate) 84.4±0.5 73.3 ±0.6 80.5 ±0.5 47.1 ±0.6 OOM DAGNN (NDLS-L)∗ 84.4±0.6 73.6 ±0.7 80.9 ±0.5 47.2 ±0.7 OOM Linear MLP 61.1 ±0.6 61.8 ±0.8 72.7 ±0.6 41.3 ±0.8 47.2 ±0.3 SGC 81.0 ±0.2 71.3 ±0.5 78.9 ±0.5 45.2 ±0.3 63.2 ±0.2 SIGN 82.1 ±0.3 72.4 ±0.8 79.5 ±0.5 46.3 ±0.5 64.2 ±0.2 S2GC 82.7 ±0.3 73.0 ±0.2 79.9 ±0.3 46.6 ±0.6 64.7 ±0.3 GBP 83.9 ±0.7 72.9 ±0.5 80.6 ±0.4 46.9 ±0.7 65.2 ±0.3 Linear NDLS-F+MLP∗ 84.1±0.6 73.5 ±0.5 81.1 ±0.6 47.5 ±0.7 65.3 ±0.5 MLP+NDLS-L∗ 83.9±0.6 73.1 ±0.8 81.1 ±0.6 46.9 ±0.7 64.6 ±0.4 SGC+NDLS-L∗ 84.2±0.2 73.4 ±0.5 81.1 ±0.4 47.1 ±0.6 64.9 ±0.3 NDLS∗ 84.6±0.5 73.7 ±0.6 81.4 ±0.4 47.7 ±0.5 65.6 ±0.3 DAGNN (Gate) [25], and PPRGo [1]; (3) the linear-model-based GNNs: MLP, SGC [32], SIGN [28], S2GC [42] and GBP [6]. In the inductive setting, the compared baselines are sampling-based GNNs: GraphSAGE [14], FastGCN [3], ClusterGCN [8] and GraphSAINT [37]. Detailed descriptions of these baselines are provided in Appendix A.4. Implementations. To alleviate the inﬂuence of randomness, we repeat each method ten times and report the mean performance. The hyper-parameters of baselines are tuned by OpenBox [23] or set according to the original paper if available. Please refer to Appendix A.5 for more details. /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000015 /uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048 /uni00000017/uni00000018 /uni00000017/uni00000019 /uni00000017/uni0000001a /uni00000017/uni0000001b/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000036/uni0000002a/uni00000026/uni00000014/uni0000005b /uni0000002a/uni00000026/uni00000031 /uni00000016/uni00000016/uni0000005b /uni00000024/uni00000033/uni00000033/uni00000031/uni00000033 /uni0000001a/uni0000001b/uni0000005b /uni00000031/uni00000027/uni0000002f/uni00000036 /uni00000015/uni0000005b /uni000000362/uni0000002a/uni00000026 /uni00000014/uni0000005b /uni0000002a/uni00000025/uni00000033/uni00000014/uni0000005b /uni0000002a/uni00000024/uni00000037 /uni00000016/uni0000001a/uni00000015/uni0000005b/uni00000024/uni00000033/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000014/uni00000015/uni0000005b /uni0000002d/uni0000002e/uni00000010/uni00000031/uni00000048/uni00000057 /uni00000014/uni00000014/uni00000016/uni0000005b /uni00000035/uni00000048/uni00000056/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000016/uni00000015/uni0000005b /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni00000016/uni0000005b Figure 3: Performance along with training time on the Industry dataset. Table 4: Results of inductive settings. Models Flickr Reddit GraphSAGE 50.1 ±1.3 95.4 ±0.0 FastGCN 50.4 ±0.1 93.7 ±0.0 ClusterGCN 48.1 ±0.5 95.7 ±0.0 GraphSAINT 51.1 ±0.1 96.6 ±0.1 NDLS-F+MLP∗ 51.9±0.2 96.6 ±0.1 GraphSAGE+NDLS-L∗ 51.5±0.4 96.3 ±0.0 NDLS∗ 52.6±0.4 96.8 ±0.1 6.2 Experimental Results. End-to-end comparison. To answerQ1, Table 3 and 4 show the test accuracy of considered methods in transductive and inductive settings. In the inductive setting, NDLS outperforms one of the most competitive baselines – GraphSAINT by a margin of 1.5% and 0.2% on Flickr and Reddit. NDLS exceeds the best GNN model among all considered baselines on each dataset by a margin of 0.2% to 0.8% in the transductive setting. In addition, we observe that with NDLS-L, the model performance of MLP, SGC, NDLS-F+MLP, and GraphSAGE can be further improved by a large margin. For example, the accuracy gain for MLP is 21.8%, 11.3%, 8.4%, and 5.6% on Cora, Citseer, PubMed, and Industry, respectively. To answerQ2, we replace the gate mechanism in the vanilla DAGNN with NDLS-L and refer to this method as DAGNN (NDLS-L). Surprisingly, DAGNN (NDLS-L) achieves at least comparable or (often) higher test accuracy compared with AP-GCN and DAGNN (Gate), and it shows that NDLS-L performs better than the learned mechanism in label smoothing. Furthermore, by replacing the original graph kernels with NDLS-F, NDLS-F+MLP outperforms both S2GC and GBP on all compared datasets. This demonstrates the effectiveness of the proposed NDLS. Training Efﬁciency. To answer Q3, we evaluate the efﬁciency of each method on a real-world industry graph dataset. Here, we pre-compute the smoothed features of each linear-model-based 8/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (a) Feature Sparsity /uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000028/uni00000047/uni0000004a/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (b) Edge Sparsity /uni00000014/uni00000013/uni00000015/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (c) Label Sparsity Figure 4: Test accuracy on PubMed dataset under different levels of feature, edge and label sparsity. /uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000027/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (a) LSI along with the node degree  (b) The visualization of LSI Figure 5: (Left) LSI distribution along with the node degree in three citation networks. (Right) The visualization of LSI in Zachary’s karate club network. Nodes with larger radius have larger LSIs. GNN, and the time for pre-processing is also included in the training time. Figure 3 illustrates the results on the industry dataset across training time. Compared with linear-model-based GNNs, we observe that (1) both the coupled and decoupled GNNs require a signiﬁcantly larger training time; (2) NDLS achieves the best test accuracy while consuming comparable training time with SGC. Performance on Sparse Graphs. To reply Q4, we conduct experiments to test the performance of NDLS on feature, edge, and label sparsity problems. For feature sparsity, we assume that the features of unlabeled nodes are partially missing. In this scenario, it is necessary to calculate a personalized propagation iteration to “recover” each node’s feature representation. To simulate edge sparsity settings, we randomly remove a ﬁxed percentage of edges from the original graph. Besides, we enumerate the number of nodes per class from 1 to 20 in the training set to measure the effectiveness of NDLS given different levels of label sparsity. The results in Figure 4 show that NDLS outperforms all considered baselines by a large margin across different levels of feature, edge, and label sparsity, thus demonstrating that our method is more robust to the graph sparsity problem than the linear-model-based GNNs. Interpretability. As mentioned by Q1, we here answer why NDLS is effective. One theoretical property of LSI is that the value correlates with the node degree negatively. We divide nodes into several groups, and each group consists of nodes with the same degree. And then we calculate the average LSI value for each group in the three citation networks respectively. Figure 5(a) depicts that nodes with a higher degree have a smaller LSI, which is consistent with Theorem 3.1. We also use NetworkX [13] to visualize the LSI in Zachary’s karate club network [36]. Figure 5(b), where the radius of each node corresponds to the value of LSI, shows three interesting observations: (1) nodes with a larger degree have smaller LSIs; (2) nodes in the neighbor area have similar LSIs; (3) nodes adjacent to a super-node have smaller LSIs. The ﬁrst observation is consistent with Theorem 3.1, and the latter two observations show consistency with Theorem 3.2. 7 Conclusion In this paper, we present node-dependent local smoothing (NDLS), a simple and scalable graph learning method based on the local smoothing of features and labels. NDLS theoretically analyzes 9what inﬂuences the smoothness and gives a bound to guide how to control the extent of smoothness for different nodes. By setting a node-speciﬁc smoothing iteration, each node in NDLS can smooth its feature/label to a local-smoothing state and then help to boost the model performance. Extensive experiments on seven real-world graph datasets demonstrate the high accuracy, scalability, efﬁciency, and ﬂexibility of NDLS against the state-of-the-art GNNs. Broader Impact NDLS can be employed in areas where graph modeling is the foremost choice, such as citation networks, social networks, chemical compounds, transaction graphs, road networks, etc. The effectiveness of NDLS when improving the predictive performance in those areas may bring a broad range of societal beneﬁts. For example, accurately predicting the malicious accounts on transaction networks can help identify criminal behaviors such as stealing money and money laundering. Prediction on road networks can help avoid trafﬁc overload and save people’s time. A signiﬁcant beneﬁt of NDLS is that it offers a node-dependent solution. However, NDLS faces the risk of information leakage in the smoothed features or labels. In this regard, we encourage researchers to understand the privacy concerns of NDLS and investigate how to mitigate the possible information leakage. Acknowledgments and Disclosure of Funding This work is supported by NSFC (No. 61832001, 6197200), Beijing Academy of Artiﬁcial Intelligence (BAAI), PKU-Baidu Fund 2019BD006, and PKU-Tencent Joint Research Lab. Zhi Yang and Bin Cui are the corresponding authors. References [1] A. Bojchevski, J. Klicpera, B. Perozzi, A. Kapoor, M. Blais, B. Rózemberczki, M. Lukasik, and S. Günnemann. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2464–2473, 2020. [2] L. Cai and S. Ji. A multi-scale approach for graph link prediction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3308–3315, 2020. [3] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. [4] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In ICLR, 2018. [5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 941–949, 2018. [6] M. Chen, Z. Wei, B. Ding, Y . Li, Y . Yuan, X. Du, and J. Wen. Scalable graph neural networks via bidirectional propagation. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [7] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 785–794, 2016. [8] W. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. InProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 257–266, 2019. 10[9] F. R. Chung and F. C. Graham. Spectral graph theory. Number 92. American Mathematical Soc., 1997. [10] G. Cui, J. Zhou, C. Yang, and Z. Liu. Adaptive graph encoder for attributed graph embedding. In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 976–985, 2020. [11] H. Dihe. An introduction to markov process in random environment [j]. Acta Mathematica Scientia, 5, 2010. [12] Q. Guo, X. Qiu, X. Xue, and Z. Zhang. Syntax-guided text generation via graph neural network. Sci. China Inf. Sci., 64(5), 2021. [13] A. Hagberg, P. Swart, and D. S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008. [14] W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 1024–1034, 2017. [15] X. He, K. Deng, X. Wang, Y . Li, Y . Zhang, and M. Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 639–648, 2020. [16] W. Hu, M. Fey, H. Ren, M. Nakata, Y . Dong, and J. Leskovec. OGB-LSC: A large-scale challenge for machine learning on graphs. CoRR, abs/2103.09430, 2021. [17] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 4563–4572, 2018. [18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [19] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018. [20] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. [21] K.-H. Lai, D. Zha, K. Zhou, and X. Hu. Policy-gnn: Aggregation optimization for graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 461–471, 2020. [22] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. [23] Y . Li, Y . Shen, W. Zhang, Y . Chen, H. Jiang, M. Liu, J. Jiang, J. Gao, W. Wu, Z. Yang, C. Zhang, and B. Cui. Openbox: A generalized black-box optimization service. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 3209–3219, 2021. [24] A. Liaw, M. Wiener, et al. Classiﬁcation and regression by randomforest. R news, 2(3):18–22, 2002. [25] M. Liu, H. Gao, and S. Ji. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 338–348, 2020. 11[26] X. Miao, N. M. Gürel, W. Zhang, Z. Han, B. Li, W. Min, S. X. Rao, H. Ren, Y . Shan, Y . Shao, Y . Wang, F. Wu, H. Xue, Y . Yang, Z. Zhang, Y . Zhao, S. Zhang, Y . Wang, B. Cui, and C. Zhang. Degnn: Improving graph neural networks with graph decomposition. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 1223–1233, 2021. [27] X. Miao, W. Zhang, Y . Shao, B. Cui, L. Chen, C. Zhang, and J. Jiang. Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture. IEEE Transactions on Knowledge and Data Engineering, 2021. [28] E. Rossi, F. Frasca, B. Chamberlain, D. Eynard, M. M. Bronstein, and F. Monti. SIGN: scalable inception graph neural networks. CoRR, abs/2004.11198, 2020. [29] I. Spinelli, S. Scardapane, and A. Uncini. Adaptive propagation graph convolutional network. IEEE Transactions on Neural Networks and Learning Systems, 2020. [30] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] K. Wang, Z. Shen, C. Huang, C.-H. Wu, Y . Dong, and A. Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396–413, 2020. [32] F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 6861–6871, 2019. [33] S. Wu, W. Zhang, F. Sun, and B. Cui. Graph neural networks in recommender systems: A survey. CoRR, abs/2011.02260, 2020. [34] S. Wu, Y . Zhang, C. Gao, K. Bian, and B. Cui. Garg: Anonymous recommendation of point- of-interest in mobile networks by graph convolution network. Data Science and Engineering, 5(4):433–447, 2020. [35] K. Xu, C. Li, Y . Tian, T. Sonobe, K. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 5449–5458, 2018. [36] W. W. Zachary. An information ﬂow model for conﬂict and ﬁssion in small groups. Journal of anthropological research, 33(4):452–473, 1977. [37] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [38] W. Zhang, Y . Jiang, Y . Li, Z. Sheng, Y . Shen, X. Miao, L. Wang, Z. Yang, and B. Cui. ROD: reception-aware online distillation for sparse graphs. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 2232–2242, 2021. [39] W. Zhang, X. Miao, Y . Shao, J. Jiang, L. Chen, O. Ruas, and B. Cui. Reliable data distillation on graph convolutional network. In Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020, pages 1399–1414, 2020. [40] W. Zhang, Z. Sheng, Y . Jiang, Y . Xia, J. Gao, Z. Yang, and B. Cui. Evaluating deep graph neural networks. CoRR, abs/2108.00955, 2021. [41] X. Zhang, H. Liu, Q. Li, and X. Wu. Attributed graph clustering via adaptive graph convolution. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4327–4333, 2019. [42] H. Zhu and P. Koniusz. Simple spectral graph convolution. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. 12A Appendix A.1 Proofs of Theorems We represent the adjacency matrix and the diagonal degree matrix of graphGby Aand Drespectively, represent D+I and A+I by ˜Dand ˜A. Then we denote ˜D−1 ˜Aas a transition matrix P. Suppose P is connected, which means the graph is connected, for any initial distribution π0, let ˜π(π0) = lim k→∞ π0Pk, (15) then according to [11], for any initial distribution π0 ˜π(π0)i = 1 n n∑ j=1 Pji, (16) where ˜πi denotes the ith component of ˜π(π0), and ndenotes the number of nodes in graph. If matrix P is unconnected, we can divide P into connected blocks. Then for each blocks(denoted as Bg), there always be ˜π(π0)i = 1 ng ∑ j∈Bg Pji ∗ ∑ j∈Bg π0j, (17) where ng is the number of nodes in Bg. To make the proof concise, we will assume matrix P is connected, otherwise we can perform the same operation inside each block. Therefore, ˜π is independent to π0, thus we replace ˜π(π0) by ˜π. Deﬁnition A.1 (Local Mixing Time). The local mixing time (parameterized by ϵ) with an initial distribution is deﬁned as T(π0,ϵ) = min{t: ||˜π−π0Pt||2 <ϵ}, (18) where \"||·||2\" symbols two-nor m. In order to consider the impact of each node to the others separately, letπ0 = ei, where ei is a one-hot vector with the ith component equal to 1, and the other components equal to 0. According to [ 9] we have lemma A.1. Lemma A.1. |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (19) where λ2 is the second large eigenvalue of P and ˜di denotes the degree of node vi plus 1 (to include itself). ˜di = di + 1, ˜dj = dj + 1, Theorem A.2. T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n), (20) where mand ndenote the number of edges and nodes in graph Gseparately. ˜di = di + 1, Proof. [9] shows that when π0 = ei, |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (21) where (eiPt)j symbols the jth element of eiPt. We denote eiPt as πi(t), then ||˜π−πi(t)||2 2 = n∑ j=1 (˜πj −πi(t)j)2 ≤ n∑ j=1 ˜dj ˜di λ2t 2 = 2m+ n ˜di λ2t 2 , (22) 13which means ||˜π−πi(t)||2 ≤ √ 2m+ n ˜di λt 2. (23) Now let ϵ= √ 2m+ n ˜di λt 2, there exists T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). Next consider the real situation in SGC with n×m-dimension matrix X(0) as input, where nis the number of nodes, mis the number of features. We apply P as the normalized adjacent matrix.(The deﬁnition of P is the same as ˜A in main text). In feature propagation we have X(t) =PtX(0), Now consider the hth feature of X, we deﬁne an n×ninﬂuence matrix Ihij(t) = ∂X(t)ih ∂X(0)jh , (24) Because Ih(k) is independent to h, we replace Ih(k) by I(k), which can be formulated as I(k) =Ih(k), ∀h∈{1,2,..,f }, (25) where f symbols the number of features of X. Deﬁnition A.2 (Local Smoothing Iteration). The Local Smoothing Iteration (parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −Ii(k)||2 <ϵ}. (26) According to Theorem A.2, there exists Theorem A.3 (Theorem 3.1 in main text). When the normalized adjacent matrix is P, K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). (27) Proof. From equation (9) we can derive that ||eiP∞−eiPk||2 ≤ √ 2m+ n ˜di λk 2. Because Ii(k) =Pk i = eiPk Ii(∞) =P∞ i = eiP∞, we have ||Ii(∞) −Ii(k)||2 ≤ √ 2m+ n ˜di λk 2. Now let ϵ= √ 2m+ n ˜di λk 2, there exists K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). 14Therefore, we expand Theorem A.3 to the propagation in SGC or our method. What is remarkable, Theorem A.3 requires P, which is equal to ˜D−1 ˜Aas the normalized adjacent matrix. From Theorem A.3 we can conclude that the node which has a lager degree may need more steps to propagate. At the same time, we have another bond of local mixing time as following. Theorem A.4. For each node vi in graph G, there always exits T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. (28) where N(i) is the set of node vi’s neighbours. Proof. ∥|˜π−eiPt+1||2 = 1 |N(i)| ∑ j∈N(i) ||˜π−ejPt||2 ≤ max j∈N(i) ||˜π−ejPt||2. (29) Therefore, when max j∈N(i) ||˜π−ejPt||2 ≤ϵ, there exists ||˜π−eiPt+1||2 ≤ϵ. Thus we can derive that T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. As we extend Theorem A.2 to Theorem A.3, according to Theorem A.4, there always be Theorem A.5 (Theorem 3.2 in main text). For each node vi in graph G, there always exits K(i,ϵ) ≤max{K(j,ϵ),j ∈N(i)}+ 1. (30) A.2 Results with More Base Models Our proposed NDLS consists of three stages: (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). In stage (2), the default option of the base model is a Multilayer Perceptron (MLP). Besides MLP, many other models can also be used in stage (2) to generate soft labels. To verify it, here we replace the MLP in stage (2) with popular machine learning models Random Forest [24] and XGBoost [7], and measure their node classiﬁcation performance on PubMed dataset. The experiment results are shown in Table 3 where Random Forest and XGBoost are abbreviated as RF and XGB respectively. Compared to the vanilla model, both Random Forest and XGBoost achieve signiﬁcant performance gain with the addition of our NDLS. With the help of NDLS, Random Forest and XGBoost outperforms their base models by 6.1% and 7.5% respectively. From Table 3, we can observe that both NDLS-F and NDLS-L can contribute great performance boost to the base model, where the gains are at least 5%. When all equipped with both NDLS-F and NDLS-L, XGBoost beat the default MLP, achieving a test accuracy of81.6%. Although Random Forest – 80.5% – cannot outperform the other two models, it is still a competitive model. The above experiment demonstrates that the base model selection in stage (2) is rather ﬂexible in our NDLS. Both traditional machine learning methods and neural networks are promising candidates in the proposed method. A.3 Dataset Description Cora, Citeseer, and Pubmed1 are three popular citation network datasets, and we follow the public training/validation/test split in GCN [18]. In these three networks, papers from different topics are 1https://github.com/tkipf/gcn/tree/master/gcn/data 15Table 5: Results of different base models on PubMed. Base Models Models Accuracy Gain MLP Base 72.7 ±0.6 - + NDLS-F 81.1 ±0.6 + 8.4 + NDLS-L 81.1 ±0.6 + 8.4 + NDLS (both) 81.4 ±0.4 + 8.7 RF Base 74.4 ±0.2 - + NDLS-F 80.3 ±0.1 + 5.9 + NDLS-L 80.0 ±0.2 + 5.6 + NDLS (both) 80.5 ±0.4 + 6.1 XGB Base 74.1 ±0.2 - + NDLS-F 81.0 ±0.3 + 6.9 + NDLS-L 79.8 ±0.2 + 5.7 + NDLS (both) 81.6 ±0.3 + 7.5 considered as nodes, and the edges are citations among the papers. The node attributes are binary word vectors, and class labels are the topics papers belong to. Reddit is a social network dataset derived from the community structure of numerous Reddit posts. It is a well-known inductive training dataset, and the training/validation/test split in our experiment is the same as the one in GraphSAGE [14]. Flickr originates from NUS-wide 2 and contains different types of images based on the descriptions and common properties of online images. The public version of Reddit and Flickr provided by GraphSAINT3 is used in our paper. Industry is a short-form video graph, collected from a real-world mobile application from our industrial cooperative enterprise. We sampled 1,000,000 users and videos from the app, and treat these items as nodes. The edges in the generated bipartite graph represent that the user clicks the short-form videos. Each user has 64 features and the target is to category these short-form videos into 253 different classes. ogbn-papers100M is a directed citation graph of 111 million papers indexed by MAG [31]. Among its node set, approximately 1.5 million of them are arXiv papers, each of which is manually labeled with one of arXiv’s subject areas. Currently, this dataset is much larger than any existing public node classiﬁcation datasets. A.4 Compared Baselines The main characteristic of all baselines are listed below: • GCN [18]: GCN is a novel and efﬁcient method for semi-supervised classiﬁcation on graph-structured data. • GAT [30]: GAT leverages masked self-attention layers to specify different weights to different nodes in a neighborhood, thus better represent graph information. • JK-Net [35]: JK-Net is a ﬂexible network embedding method that could gather different neighborhood ranges to enable better structure-aware representation. • APPNP [19]: APPNP uses the relationship between graph convolution networks (GCN) and PageRank to derive improved node representations. • AP-GCN [29]: AP-GCN uses a halting unit to decide a receptive range of a given node. 2http://lms.comp.nus.edu.sg/research/NUS-WIDE.html 3https://github.com/GraphSAINT/GraphSAINT 16Table 6: URLs of baseline codes. Type Baselines URLs Coupled GCN https://github.com/rusty1s/pytorch_geometric GAT https://github.com/rusty1s/pytorch_geometric Decoupled APPNP https://github.com/rusty1s/pytorch_geometric PPRGo https://github.com/TUM-DAML/pprgo_pytorch AP-GCN https://github.com/spindro/AP-GCN DAGNN https://github.com/divelab/DeeperGNN Sampling GraphSAGE https://github.com/williamleif/GraphSAGE GraphSAINT https://github.com/GraphSAINT/GraphSAINT FastGCN https://github.com/matenure/FastGCN Cluster-GCN https://github.com/benedekrozemberczki/ClusterGCN Linear SGC https://github.com/Tiiiger/SGC SIGN https://github.com/twitter-research/sign S2GC https://github.com/allenhaozhu/SSGC GBP https://github.com/chennnM/GBP NDLS https://github.com/zwt233/NDLS • DAGNN [25]: DAGNN proposes to decouple the representation transformation and propagation, and show that deep graph neural networks without this entanglement can leverage large receptive ﬁelds without suffering from performance deterioration. • PPRGo [1]: utilizes an efﬁcient approximation of information diffusion in GNNs resulting in signiﬁcant speed gains while maintaining state-of-the-art prediction performance. • GraphSAGE [14]: GraphSAGE is an inductive framework that leverages node attribute information to efﬁciently generate representations on previously unseen data. • FastGCN [4]: FastGCN interprets graph convolutions as integral transforms of embedding functions under probability measures. • Cluster-GCN [8]: Cluster-GCN is a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. • GraphSAINT [37]: GraphSAINT constructs mini-batches by sampling the training graph, rather than the nodes or edges across GCN layers. • SGC [32]: SGC simpliﬁes GCN by removing nonlinearities and collapsing weight matrices between consecutive layers. • SIGN [28]: SIGN is an efﬁcient and scalable graph embedding method that sidesteps graph sampling in GCN and uses different local graph operators to support different tasks. • S2GC [42]: S2GC uses a modiﬁed Markov Diffusion Kernel to derive a variant of GCN, and it can be used as a trade-off of low-pass and high-pass ﬁlter which captures the global and local contexts of each node. • GBP [6]: GBP utilizes a localized bidirectional propagation process from both the feature vectors and the training/testing nodes Table 6 summarizes the github URLs of the compared baselines. Following the original paper, we implement JK-Net by ourself since there is no ofﬁcial version available. A.5 Implementation Details Hyperparameter details. In stage (1), when computing the Local Smoothing Iteration, the maximal value of k in equation (12) is set to 200 and the optimal ϵ value is get by means of a grid search from {0.01, 0.03, 0.05}. In stage (2), we use a simple two-layer MLP to get the base prediction. The hidden size is set to 64 in small datasets – Cora, Citeseer and Pubmed. While in larger datasets – Flicker, Reddit, Industry and ogbn-papers100M, the hidden size is set to 256. As for the dropout percentage and the learning rate, we use a grid search from {0.2, 0.4, 0.6, 0.8} and {0.1, 17Table 7: Performance comparison between C&S and NDLS-L Methods Cora Citeseer PubMed ogbn-papers100M MLP+C&S 87.2 76.6 88.3 63.9 MLP+NDLS-L 88.1 78.3 88.5 64.6 Table 8: Performance comparison under varied label rate on the Cora dataset. Methods 2% 5% 10% 20% 40% 60% MLP+S 63.1 77.8 82.6 84.2 85.4 86.4 MLP+C&S 62.8 76.7 82.8 84.9 86.4 87.2 MLP+NDLS-L 77.4 83.9 85.3 86.5 87.6 88.1 Table 9: Performance comparison after combining the node-dependent idea with C&S. Methods Cora Citeseer PubMed MLP+C&S 76.7 70.8 76.5 MLP+C&S+nd 79.9 71.1 78.4 0.01, 0.001} respectively. In stage (3), during the computation of the Local Smoothing Iteration, the maximal value of kis set to 40. The optimal value of ϵis obtained through the same process in stage (1). Implementation environment. The experiments are conducted on a machine with Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz, and a single NVIDIA TITAN RTX GPU with 24GB memory. The operating system of the machine is Ubuntu 16.04. As for software versions, we use Python 3.6, Pytorch 1.7.1 and CUDA 10.1. A.6 Comparison and Combination with Correct&Smooth Similar to our NDLS-L, Correct and Smooth (C&S) also applies post-processing on the model prediction. Therefore, we compare NDLS-L with C&S below. Adaptivity to node. C&S adopts a propagation scheme based on Personalized PageRank (PPR), which always maintains certain input information to slow down the occurrence of over-smoothing. The expected number of smoothing iterations is controlled by the restart probability, which is a constant for all nodes. Therefore, C&S still falls into the routine of ﬁxed smoothing iteration. Instead, NDLS-L employs node-speciﬁc smoothing iterations. We compare each method’s performance (test accuracy, %) under the same data split as in the C&S paper (60%/20%/20% on three citation networks, ofﬁcial split on ogbn-papers100M), and the experimental results in Table 7 show that NDLS-L outperforms C&S in different datasets. Sensitivity to label rate. During the “Correct” stage, C&S propagates uncertainties from the training data across the graph to correct the base predictions. However, the uncertainties might not be accurate when the number of training nodes is relatively small, thus even degrading the performance. To conﬁrm the above assumption, we conduct experiments on the Cora dataset under different label rates, and the experimental results are provided in Table 8. As illustrated, the result of C&S drops much faster than NDLS-L’s when the label rate decreases. What’s more, MLP+S (removing the “Correct” stage) outperforms MLP+C&S when the label rate is low as expected. Compared with C&S, NDLS is more general in terms of smoothing types. C&S can only smooth label predictions. Instead, NDLS can smooth both node features and label predictions and combine them to boost the model performance further. 18Table 10: Efﬁciency comparison on the PubMed dataset. SGC S2GC GBP NDLS SIGN JK-Net DAGNN GCN ResGCN APPNP GAT Time 1.00 1.19 1.20 1.50 1.59 11.42 14.39 20.43 20.49 28.88 33.23 Accuracy 78.9 79.9 80.6 81.4 79.5 78.8 80.5 79.3 78.6 80.1 79.0 Table 11: Performance comparison on the ogbn-arxiv dataset. MLP MLP+C&S GCN SGC SIGN DAGNN JK-Net S2GC GBP NDLS GAT Accuracy55.50 71.58 71.74 71.72 71.95 72.09 72.19 72.21 72.45 73.04 73.56 Node Adaptive C&S. The node-dependent mechanism in our NDLS can easily be combined with C&S. The two stages of C&S both contain a smoothing process using the personalized PageRank matrix, where a coefﬁcient controls the remaining percentage of the original node feature. Here, we can precompute the smoothed node features after the same smoothing step yet under different values like 0.1, 0.2, ..., 0.9. After that, we adopt the same strategy in our NDLS: for each node, we choose the ﬁrst in the ascending order that the distance from the smoothed node feature to the stationarity is less than a tuned hyperparameter. By this means, the smoothing process in C&S can be carried out in a node-dependent way. We also evaluate the performance of C&S combined with the node-dependent idea (represented as C&S+nd) on the three citation networks under ofﬁcial splits, and the experimental results in Table 9 show that C&S combined with NDLS consistently outperforms the original version of C&S. A.7 Training Efﬁciency Study we measure the training efﬁciency of the compared baselines on the widely used PubMed dataset. Using the training time of SGC as the baseline, the relative training time and the corresponding test accuracy of NDLS and the baseline methods are shown in Table 10. Compared with other baselines, NDLS can get the highest test accuracy while maintaining competitive training efﬁciency. A.8 Experiments on ogbn-arxiv We also conduct experiments on the ogbn-arxiv dataset. The experiment results (test accuracy, %) are provided in Table 11. Although GAT outperforms NDLS on ogbn-arxiv dataset, it is hard to scale to large graphs like ogbn-papers100M dataset. Note that MLP+C&S on the OGB leaderboard makes use of not only the original node feature but also diffusion embeddings and spectral embeddings. Here we remove the latter two embeddings for fairness, and the authentic MLP+C&S achieves 71.58% on the ogbn-arxiv dataset. 19",
      "meta_data": {
        "arxiv_id": "2110.14377v1",
        "authors": [
          "Wentao Zhang",
          "Mingyu Yang",
          "Zeang Sheng",
          "Yang Li",
          "Wen Ouyang",
          "Yangyu Tao",
          "Zhi Yang",
          "Bin Cui"
        ],
        "published_date": "2021-10-27T12:24:41Z",
        "venue": "NeurIPS 2021",
        "pdf_url": "https://arxiv.org/pdf/2110.14377v1.pdf"
      }
    },
    {
      "title": "Node Dependent Local Smoothing for Scalable Graph Learning",
      "abstract": "Recent works reveal that feature or label smoothing lies at the core of Graph\nNeural Networks (GNNs). Concretely, they show feature smoothing combined with\nsimple linear regression achieves comparable performance with the carefully\ndesigned GNNs, and a simple MLP model with label smoothing of its prediction\ncan outperform the vanilla GCN. Though an interesting finding, smoothing has\nnot been well understood, especially regarding how to control the extent of\nsmoothness. Intuitively, too small or too large smoothing iterations may cause\nunder-smoothing or over-smoothing and can lead to sub-optimal performance.\nMoreover, the extent of smoothness is node-specific, depending on its degree\nand local structure. To this end, we propose a novel algorithm called\nnode-dependent local smoothing (NDLS), which aims to control the smoothness of\nevery node by setting a node-specific smoothing iteration. Specifically, NDLS\ncomputes influence scores based on the adjacency matrix and selects the\niteration number by setting a threshold on the scores. Once selected, the\niteration number can be applied to both feature smoothing and label smoothing.\nExperimental results demonstrate that NDLS enjoys high accuracy --\nstate-of-the-art performance on node classifications tasks, flexibility -- can\nbe incorporated with any models, scalability and efficiency -- can support\nlarge scale graphs with fast training.",
      "full_text": "Node Dependent Local Smoothing for Scalable Graph Learning Wentao Zhang1, Mingyu Yang1, Zeang Sheng1, Yang Li1 Wen Ouyang2, Yangyu Tao2, Zhi Yang1,3, Bin Cui1,3,4 1School of CS, Peking University 2Tencent Inc. 3 Key Lab of High Conﬁdence Software Technologies, Peking University 4Institute of Computational Social Science, Peking University (Qingdao), China 1{wentao.zhang, ymyu, shengzeang18, liyang.cs, yangzhi, bin.cui}@pku.edu.cn 2{gdpouyang, brucetao}@tencent.com Abstract Recent works reveal that feature or label smoothing lies at the core of Graph Neural Networks (GNNs). Concretely, they show feature smoothing combined with simple linear regression achieves comparable performance with the carefully designed GNNs, and a simple MLP model with label smoothing of its prediction can outperform the vanilla GCN. Though an interesting ﬁnding, smoothing has not been well understood, especially regarding how to control the extent of smoothness. Intuitively, too small or too large smoothing iterations may cause under-smoothing or over-smoothing and can lead to sub-optimal performance. Moreover, the extent of smoothness is node-speciﬁc, depending on its degree and local structure. To this end, we propose a novel algorithm called node-dependent local smoothing (NDLS), which aims to control the smoothness of every node by setting a node- speciﬁc smoothing iteration. Speciﬁcally, NDLS computes inﬂuence scores based on the adjacency matrix and selects the iteration number by setting a threshold on the scores. Once selected, the iteration number can be applied to both feature smoothing and label smoothing. Experimental results demonstrate that NDLS enjoys high accuracy – state-of-the-art performance on node classiﬁcations tasks, ﬂexibility – can be incorporated with any models, scalability and efﬁciency – can support large scale graphs with fast training. 1 Introduction In recent years, Graph Neural Networks (GNNs) have received a surge of interest with the state-of-the- art performance on many graph-based tasks [2, 41, 12, 39, 33, 34]. Recent works have found that the success of GNNs can be mainly attributed to smoothing, either at feature or label level. For example, SGC [32] shows using smoothed features as input to a simple linear regression model achieves comparable performance with lots of carefully designed and complex GNNs. At the smoothing stage, features of neighbor nodes are aggregated and combined with the current node’s feature to form smoothed features. This process is often iterated multiple times. The smoothing is based on the assumption that labels of nodes that are close to each other are highly correlated, therefore, the features of nodes nearby should help predict the current node’s label. One crucial and interesting parameter of neighborhood feature aggregation is the number of smoothing iterations k, which controls how much information is being gathered. Intuitively, an aggregation process of kiterations (or layers) enables a node to leverage information from nodes that are k-hop away [26, 38]. The choice of k is closely related to the structural properties of graphs and has a 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2110.14377v1  [cs.LG]  27 Oct 2021(a) Two nodes with different local structures /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000026/uni00000027/uni00000029  /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (b) The CDF of LSI in different graphs Figure 1: (Left) The node in dense region has larger smoothed area within two iterations of propagation. (Right) The CDF of LSI in three citation networks. signiﬁcant impact on the model performance. However, most existing GNNs only consider the ﬁxed-length propagation paradigm – a uniform kfor all the nodes. This is problematic since the number of iterations should be node dependent based on its degree and local structures. For example, as shown in Figure 1(a), the two nodes have rather different local structures, with the left red one resides in the center of a dense cluster and the right red one on the periphery with few connections. The number of iterations to reach an optimal level of smoothness are rather different for the two nodes. Ideally, poorly connected nodes (e.g., the red node on the right) needs large iteration numbers to efﬁciently gather information from other nodes while well-connected nodes (e.g., the red node on the left) should keep the iteration number small to avoid over-smoothing. Though some learning-based approaches have proposed to adaptively aggregate information for each node through gate/attention mechanism or reinforcement learning [ 29, 21, 40, 27], the performance gains are at the cost of increased training complexity, hence not suitable for scalable graph learning. In this paper, we propose a simple yet effective solution to this problem. Our approach, called node-dependent local smoothing (NDLS), calculates a node-speciﬁc iteration number for each node, referred to as local smooth iteration (LSI). Once the LSI for a speciﬁc node is computed, the corresponding local smoothing algorithm only aggregates the information from the nodes within a distance less than its LSI as the new feature. The LSI is selected based on inﬂuence scores, which measure how other nodes inﬂuence the current node. NDLS sets the LSI for a speciﬁc node to be the minimum number of iterations so that the inﬂuence score is ϵ-away from the over-smoothing score, deﬁned as the inﬂuence score at inﬁnite iteration. The insight is that each node’s inﬂuence score should be at a reasonable level. Since the nodes with different local structures have different “smoothing speed”, we expect the iteration number to be adaptive. Figure 1(b) illustrates Cumulative Distribution Function (CDF) for the LSI of individual nodes in real-world graphs. The heterogeneous and long-tail property exists in all the datasets, which resembles the characteristics of the degree distribution of nodes in real graphs. Based on NDLS, we propose a new graph learning algorithm with three stages: (1) feature smoothing with NDLS (NDLS-F); (2) model training with smoothed features; (3) label smoothing with NDLS (NDLS-L). Note that in our framework, the graph structure information is only used in pre-processing and post-processing steps, i.e., stages (1) and (3) (See Figure 2). Our NDLS turns a graph learning problem into a vanilla machine learning problem with independent samples. This simplicity enables us to train models on larger-scale graphs. Moreover, our NDLS kernel can act as a drop-in replacement for any other graph kernels and be combined with existing models such as Multilayer Perceptron (MLP), SGC [32], SIGN [28], S2GC [42] and GBP [6]. Extensive evaluations on seven benchmark datasets, including large-scale datasets like ogbn- papers100M [16], demonstrates that NDLS achieves not only the state-of-the-art node classiﬁcation performance but also high training scalability and efﬁciency. Especially, NDLS outperforms APPNP [29] and GAT [30] by a margin of 1.0%-1.9% and 0.9%-2.4% in terms of test accuracy, while achieving up to 39×and 186×training speedups, respectively. 2 Preliminaries In this section, we ﬁrst introduce the semi-supervised node classiﬁcation task and review the prior models, based on which we derive our method in Section 3. Consider a graphG= (V, E) with |V|= n 2nodes and |E|= medges, the adjacency matrix (including self loops) is denoted as ˜A ∈Rn×n and the feature matrix is denoted as X = {x1,x2...,xn}in which xi ∈Rf represents the feature vector of node vi. Besides, Y = {y1,y2...,yl}is the initial label matrix consisting of one-hot label indicator vectors. The goal is to predict the labels for nodes in the unlabeled set Vu with the supervision of labeled set Vl. GCN smooths the representation of each node via aggregating its own representations and the ones of its neighbors’. This process can be deﬁned as X(k+1) = δ ( ˆAX(k)W(k) ) , ˆA = ˜Dr−1 ˜A˜D−r, (1) where ˆA is the normalized adjacency matrix, r∈[0,1] is the convolution coefﬁcient, and ˜D is the diagonal node degree matrix with self loops. Here X(k) and X(k+1) are the smoothed node features of layer k and k+ 1respectively while X(0) is set to X, the original feature matrix. In addition, W(k) is a layer-speciﬁc trainable weight matrix at layer k, and δ(·) is the activation function. By setting r= 0.5, 1 and 0, the convolution matrix ˜Dr−1 ˜A˜D−r represents the symmetric normalization adjacency matrix ˜D−1/2 ˜A˜D−1/2 [20], the transition probability matrix ˜A˜D−1 [37], and the reverse transition probability matrix ˜D−1 ˜A [35], respectively. SGC. For each GCN layer deﬁned in Eq. 1, if the non-linear activation function δ(·) is an identity function and W(k) is an identity matrix, we get the smoothed feature after k-iterations propagation as X(k) = ˆAkX. Recent studies have observed that GNNs primarily derive their beneﬁts from performing feature smoothing over graph neighborhoods rather than learning non-linear hierarchies of features as implied by the analogy to CNNs [ 25, 10, 15]. By hypothesizing that the non-linear transformations between GCN layers are not critical, SGC [32] ﬁrst extracts the smoothed features X(k) then feeds them to a linear model, leading to higher scalability and efﬁciency. Following the design principle of SGC, piles of works have been proposed to further improve the performance of SGC while maintaining high scalability and efﬁciency, such as SIGN [28], S2GC [42] and GBP [6]. Over-Smoothing [22] issue. By continually smoothing the node feature with inﬁnite number of propagation in SGC, the ﬁnal smoothed feature X(∞) is X(∞) = ˆA∞X, ˆA∞ i,j = (di + 1)r(dj + 1)1−r 2m+ n , (2) where ˆA∞is the ﬁnal smoothed adjacency matrix, ˆA∞ i,j is the weight between nodes vi and vj, di and dj are the node degrees for vi and vj, respectively. Eq. (2) shows that as we smooth the node feature with an inﬁnite number of propagations in SGC, the ﬁnal feature is over-smoothed and unable to capture the full graph structure information since it only relates with the node degrees of target nodes and source nodes. For example, if we set r= 0or 1, all nodes will have the same smoothed features because only the degrees of the source or target nodes have been considered. 3 Local Smoothing Iteration (LSI) The features after k iterations of smoothing is X(k) = ˆAkX. Inspired by [ 35], we measure the inﬂuence of node vj on node vi by measuring how much a change in the input feature of vj affects the representation of vi after kiterations. For any node vi, the inﬂuence vector captures the inﬂuences of all other nodes. Considering the hth feature of X, we deﬁne an inﬂuence matrix Ih(k): Ih(k)ij = ∂ˆX(k) ih ∂ˆX(0) jh . (3) I(k) = ˆAk,˜Ii = ˆA∞ (4) Since Ih(k) is independent to h, we replace Ih(k) with I(k), which can be further represented as I(k) = Ih(k), ∀h ∈{1,2,..,f }, where f indicates the number of features of X. We denote I(k)i as the ith row of I(k), and ˜I as I(∞). Given the normalized adjacency matrix ˆA, we can 3have I(k) = ˆAk and ˜I = ˆA∞. According to Eq. (2), ˜I converges to a unique stationary matrix independent of the distance between nodes, resulting in that the aggregated features of nodes are merely relative with their degrees (i.e., over-smoothing). We denote I(k)i as the ith row of I(k), and it means the inﬂuence from the other nodes to the node vi after k iterations of propagation. We introduce a new concept local smoothing iteration (parameterized by ϵ), which measures the minimal number of iterations krequired for the inﬂuence of other nodes on node vi to be within an ϵ-distance to the over-smoothing stationarity ˜Ii. Deﬁnition 3.1. Local-Smoothing Iteration (LSI, parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −I(k)i||2 <ϵ}, (5) where ||·||2 is two-norm, and ϵis an arbitrary small constant with ϵ> 0. Here ϵis a graph-speciﬁc parameter, and a smaller ϵindicates a stronger smoothing effect. The ϵ-distance to the over-smoothing stationarity ˜Ii ensures that the smooth effect on node vi is sufﬁcient and bounded to avoid over-smoothing. As shown in Figure 1(b), we can have that the distribution of LSI owns the heterogeneous and long-tail property, where a large percentage of nodes have much smaller LSI than the rest. Therefore, the required LSI to approach the stationarity is heterogeneous across nodes. Now we discuss the connection between LSI and node local structure, showcasing nodes in the sparse region (e.g., both the degrees of itself and its neighborhood are low) can greatly prolong the iteration to approach over-smoothing stationarity. This heterogeneity property is not fully utilized in the design of current GNNs, leaving the model design in a dilemma between unnecessary iterations for a majority of nodes and insufﬁcient iterations for the rest of nodes. Hence, by adaptively choosing the iteration based on LSI for different nodes, we can signiﬁcantly improve model performance. Theoretical Properties of LSI. We now analyze the factors determining the LSI of a speciﬁc node. To facilitate the analysis, we set the coefﬁcient r = 0for the normalized adjacency matrix ˆA in Eq. (1), thus ˆA = ˜D−1 ˜A. The proofs of following theorems can be found in Appendix A.1. Theorem 3.1. Given feature smoothing X(k) = ˆAkX with ˆA = ˜D−1 ˜A, we have K(i,ϵ) ≤logλ2  ϵ √ ˜di 2m+ n  , (6) where λ2 is the second largest eigenvalue of ˆA, ˜di denotes the degree of node vi plus 1 (i.e., ˜di = di + 1), and m, ndenote the number of edges and nodes respectively. Note that λ2 ≤1. Theorem 3.1 shows that the upper-bound of the LSI is positively correlated with the scale of the graph (m,n), the sparsity of the graph (small λ2 means strong connection and low sparsity, and vice versa), and negatively correlated with the degree of nodevi. Theorem 3.2. For any nodes iin a graph G, K(i,ϵ) ≤max {K(j,ϵ),j ∈N(i)}+ 1, (7) where N(i) is the set of node vi’s neighbours. Theorem 3.2 indicates that the difference between two neighboring nodes’ LSIs is no more than1, therefore the nodes with a super-node as neighbors (or neighbor’s neighbors) may have small LSIs. That is to say, the sparsity of the local area, where a node locates, also affects its LSI positively. Considering Theorems 3.1 and 3.2 together, we can have a union upper-bound of K(i,ϵ) as K(i,ϵ) ≤min   max {K(j,ϵ),j ∈N(i)}+ 1,logλ2  ϵ √ ˜di 2m+ n     . (8) 4 NDLS Pipeline The basic idea of NDLS is to utilize the LSI heterogeneity to perform a node-dependent aggregation over a neighborhood within a distance less than the speciﬁc LSI for each node. Further, we propose 4Adjacent Matrix A Final Prediction �Y ⋯ MLP Soft label �Y Stage 1: preprocessing Stage 2: training Stage 3: postprocessing  Feature X �X Smooth Feature with NDLS-F (replaceable) Adjacent Matrix A B A B1.0 B 0.3 0.3 0.7 … LSI Estimation A A’s feature smoothed  under optimal steps … 1.0 A0.8 0.3 0.2 0.2 0.1 … … LSI(A) = 4 LSI(B) = 2 0.1 0.3 … B’s feature smoothed  under optimal steps Details about NDLS-F B A Smooth Label with NDLS-L �XB�XA Figure 2: Overview of the proposed NDLS method, including (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). NDLS-F and NDLS-L correspond to pre-processing and post-processing steps respectively. a simple pipeline with three main parts (See Figure 2): (1) a node-dependent local smoothing of the feature (NDLS-F) over the graph, (2) a base prediction result with the smoothed feature, (3) a node-dependent local smoothing of the label predictions (NDLS-L) over the graph. Note this pipeline is not trained in an end-to-end way, the stages (1) and (3) in NDLS are only the pre-processing and post-processing steps, respectively. Furthermore, the graph structure is only used in the pre/post- processing NDLS steps, not for the base predictions. Compared with prior GNN models, this key design enables higher scalability and a faster training process. Based on the graph structure, we ﬁrst compute the node-dependent local smoothing iteration that maintains a proper distance to the over-smoothing stationarity. Then the corresponding local smoothing kernel only aggregates the information (feature or prediction) for each node from the nodes within a distance less than its LSI value. The combination of NDLS-F and NDLS-L takes advantage of both label smoothing (which tends to perform fairly well on its own without node features) and the node feature smoothing. We will see that combining these complementary signals yields state-of-the- art predictive accuracy. Moreover, our NDLS-F kernel can act as a drop-in replacement for graph kernels in other scalable GNNs such as SGC, S2GC, GBP, etc. 4.1 Smooth Features with NDLS-F Once the node-dependent LSI K(i,ϵ) for a speciﬁc node iis obtained, we smooth the initial input feature Xi of node iwith node-dependent LSI as: ˜Xi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 X(k) i . (9) To capture sufﬁcient neighborhood information, for each node vi, we average its multi-scale features {X(k) i |k≤K(i,ϵ)}obtained by aggregating information within khops from the node vi. The matrix form of the above equation can be formulated as ˜X(ϵ) = max i K(i,ϵ) ∑ k=0 M(k)X(k), M(k) ij = { 1 K(i,ϵ)+1 , i = j and k ≤K(i,ϵ) 0, otherwise , (10) where M(k) is a set of diagonal matrix. 4.2 Simple Base Prediction With the smoothed feature ˜X according to Eq. 9, we then train a model to minimize the loss –∑ vi∈Vl ℓ ( yi,f( ˜Xi) ) , where ˜Xi denotes the ith row of ˜X, ℓis the cross-entropy loss function, and f( ˜Xi) is the predictive label distribution for node vi. In NDLS, the default f is a MLP model and 5ˆY = f( ˜X) is its soft label predicted (softmax output). Note that, many other models such as Random Forest [24] and XGBoost [7] could also be used in NDLS (See more results in Appendix A.2). 4.3 Smooth Labels with NDLS-L Similar to the feature propagation, we can also propagate the soft label ˆY with ˆY(k) = ˆAk ˆY. Considering the inﬂuence matrix of softmax label Jh(k). Jh(k)ij = ∂ˆY(k) ih ∂ˆY(0) jh . (11) According to the deﬁnition above we have that Jh(k) =Ih(k),∀h∈{1,2,..,f }. (12) Therefore, local smoothing can be further applied to address over-smoothing in label propagation. Concretely, we smooth an initial soft label ˆYi of node vi with NDLS as follows ˜Yi(ϵ) = 1 K(i,ϵ) + 1 K(i,ϵ)∑ k=0 ˆY(k) i . (13) Similarly, the matrix form of the above equation can be formulated as ˜Y(ϵ) = max i K(i,ϵ) ∑ k=0 M(k) ˆY(k), (14) where M(k) follows the deﬁnition in Eq. (10). 5 Comparison with Existing Methods Decoupled GNNs. The aggregation and transformation operations in coupled GNNs (i.e., GCN [18], GAT [30] and JK-Net [ 35]) are inherently intertwined in Eq. (1), so the propagation iterations L always equals to the transformation iterations K. Recently, some decoupled GNNs (e.g., PPNP [20], PPRGo [1], APPNP [20], AP-GCN [29] and DAGNN [25]) argue the entanglement of these two operations limits the propagation depth and representation ability of GNNs, so they ﬁrst do the transformation and then smooth and propagate the predictive soft label with higher depth in an end-to-end manner. Especially, AP-GCN and DAGNN both use a learning mechanism to learn propagation adaptively. Unfortunately, all these coupled and decoupled GNNs are hard to scale to large graphs – scalability issue since they need to repeatedly perform an expensive recursive neighborhood expansion in multiple propagations of the features or soft label predicted. NDLS addresses this issue by dividing the training process into multiple stages. Sampling-based GNNs. An intuitive method to tackle the recursive neighborhood expansion problem is sampling. As a node-wise sampling method, GraphSAGE [14] samples the target nodes as a mini-batch and samples a ﬁxed size set of neighbors for computing. VR-GCN [5] analyzes the variance reduction on node-wise sampling, and it can reduce the size of samples with an additional memory cost. In the layer level, Fast-GCN [ 3] samples a ﬁxed number of nodes at each layer, and ASGCN [17] proposes the adaptive layer-wise sampling with better variance control. For the graph-wise sampling, Cluster-GCN [8] clusters the nodes and only samples the nodes in the clusters, and GraphSAINT [37] directly samples a subgraph for mini-batch training. We don’t use sampling in NDLS since the sampling quality highly inﬂuences the classiﬁcation performance. Linear Models. Following SGC [32], some recent methods remove the non-linearity between each layer in the forward propagation. SIGN [28] allows using different local graph operators and proposes to concatenate the different iterations of propagated features. S2GC [42] proposes the simple spectral graph convolution to average the propagated features in different iterations. In addition, GBP [ 6] further improves the combination process by weighted averaging, and all nodes in the same layer share the same weight. In this way, GBP considers the smoothness in a layer perspective way. Similar 6Table 1: Algorithm analysis for existing scalable GNNs. n, m, c, and f are the number of nodes, edges, classes, and feature dimensions, respectively. bis the batch size, and krefers to the number of sampled nodes. Lcorresponds to the number of times we aggregate features, K is the number of layers in MLP classiﬁers. For the coupled GNNs, we always have K = L. Type Method Preprocessing and postprocessingTraining Inference Memory Node-wise samplingGraphSAGE - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Layer-wise samplingFastGCN - O(kLnf2) O(kLnf2) O(bkLf+Lf2)Graph-wise samplingCluster-GCN O(m) O(Lmf+Lnf2) O(Lmf+Lnf2) O(bLf+Lf2) Linear model SGC O(Lmf) O(nf2) O(nf2) O(bf+f2)S2GC O(Lmf) O(nf2) O(nf2) O(bf+f2)SIGN O(Lmf) O(Knf2) O(Knf2) O(bLf+Kf2) GBP O(Lnf+L √mlgnε ) O(Knf2) O(Knf2) O(bf+Kf2)Linear model NDLS O(Lmf+Lmc) O(Knf2) O(Knf2) O(bf+Kf2) Table 2: Overview of datasets and task types (T/I represents Transductive/Inductive). Dataset #Nodes #Features #Edges #Classes #Train/Val/Test Type Description Cora 2,708 1,433 5,429 7 140/500/1,000 T citation network Citeseer 3,327 3,703 4,732 6 120/500/1,000 T citation network Pubmed 19,717 500 44,338 3 60/500/1,000 T citation network Industry 1,000,000 64 1,434,382 253 5K/10K/30K T short-form video network ogbn-papers100M 111,059,956 128 1,615,685,872 172 1,207K/125K/214K T citation network Flickr 89,250 500 899,756 7 44K/22K/22K I image network Reddit 232,965 602 11,606,919 41 155K/23K/54K I social network to these works, we also use a linear model for higher training scalability. The difference lies in that we consider the smoothness from a node-dependent perspective and each node in NDLS has a personalized aggregation iteration with the proposed local smoothing mechanism. Table 1 compares the asymptotic complexity of NDLS with several representative and scalable GNNs. In the stage of the preprocessing, the time cost of clustering in Cluster-GCN is O(m) and the time complexity of most linear models is O(Lmf). Besides, NDLS has an extra time cost O(Lmc) for the postprocessing in label smoothing. GBP conducts this process approximately with a bound of O(Lnf + L √mlg n ε ), where εis a error threshold. Compared with the sampling-based GNNs, the linear models usually have smaller training and inference complexity, i.e., higher efﬁciency. Memory complexity is a crucial factor in large-scale graph learning because it is difﬁcult for memory-intensive algorithms such as GCN and GAT to train large graphs on a single machine. Compared with SIGN, both GBP and NDLS do not need to store smoothed features in different iterations, and the feature storage complexity can be reduced from O(bLf) to O(bf). 6 Experiments In this section, we verify the effectiveness of NDLS on seven real-world graph datasets. We aim to answer the following four questions. Q1: Compared with current SOTA GNNs, can NDLS achieve higher predictive accuracy and why? Q2: Are NDLS-F and NDLS-L better than the current feature and label smoothing mechanisms (e.g., the weighted feature smoothing in GBP and the adaptive label smoothing in DAGNN)? Q3: Can NDLS obtain higher efﬁciency over the considered GNN models? Q4: How does NDLS perform on sparse graphs (i.e., low label/edge rate, missing features)? 6.1 Experimental Setup Datasets. We conduct the experiments on (1) six publicly partitioned datasets, including four citation networks (Citeseer, Cora, PubMed, and ogbn-papers100M) in [ 18, 16] and two social networks (Flickr and Reddit) in [37], and (2) one short-form video recommendation graph (Industry) from our industrial cooperative enterprise. The dataset statistics are shown in Table 2 and more details about these datasets can be found in Appendix A.3. Baselines. In the transductive setting, we compare our method with (1) the coupled GNNs: GCN [18], GAT [ 30] and JK-Net [ 35]; (2) the decoupled GNNs: APPNP [ 20], AP-GCN [ 29], 7Table 3: Results of transductive settings. OOM means “out of memory”. Type Models Cora Citeseer PubMed Industry ogbn-papers100M Coupled GCN 81.8 ±0.5 70.8 ±0.5 79.3 ±0.7 45.9 ±0.4 OOM GAT 83.0 ±0.7 72.5 ±0.7 79.0 ±0.3 46.8 ±0.7 OOM JK-Net 81.8 ±0.5 70.7 ±0.7 78.8 ±0.7 47.2 ±0.3 OOM Decoupled APPNP 83.3 ±0.5 71.8 ±0.5 80.1 ±0.2 46.7 ±0.6 OOM AP-GCN 83.4 ±0.3 71.3 ±0.5 79.7 ±0.3 46.9 ±0.7 OOM PPRGo 82.4 ±0.2 71.3 ±0.5 80.0 ±0.4 46.6 ±0.5 OOM DAGNN (Gate) 84.4±0.5 73.3 ±0.6 80.5 ±0.5 47.1 ±0.6 OOM DAGNN (NDLS-L)∗ 84.4±0.6 73.6 ±0.7 80.9 ±0.5 47.2 ±0.7 OOM Linear MLP 61.1 ±0.6 61.8 ±0.8 72.7 ±0.6 41.3 ±0.8 47.2 ±0.3 SGC 81.0 ±0.2 71.3 ±0.5 78.9 ±0.5 45.2 ±0.3 63.2 ±0.2 SIGN 82.1 ±0.3 72.4 ±0.8 79.5 ±0.5 46.3 ±0.5 64.2 ±0.2 S2GC 82.7 ±0.3 73.0 ±0.2 79.9 ±0.3 46.6 ±0.6 64.7 ±0.3 GBP 83.9 ±0.7 72.9 ±0.5 80.6 ±0.4 46.9 ±0.7 65.2 ±0.3 Linear NDLS-F+MLP∗ 84.1±0.6 73.5 ±0.5 81.1 ±0.6 47.5 ±0.7 65.3 ±0.5 MLP+NDLS-L∗ 83.9±0.6 73.1 ±0.8 81.1 ±0.6 46.9 ±0.7 64.6 ±0.4 SGC+NDLS-L∗ 84.2±0.2 73.4 ±0.5 81.1 ±0.4 47.1 ±0.6 64.9 ±0.3 NDLS∗ 84.6±0.5 73.7 ±0.6 81.4 ±0.4 47.7 ±0.5 65.6 ±0.3 DAGNN (Gate) [25], and PPRGo [1]; (3) the linear-model-based GNNs: MLP, SGC [32], SIGN [28], S2GC [42] and GBP [6]. In the inductive setting, the compared baselines are sampling-based GNNs: GraphSAGE [14], FastGCN [3], ClusterGCN [8] and GraphSAINT [37]. Detailed descriptions of these baselines are provided in Appendix A.4. Implementations. To alleviate the inﬂuence of randomness, we repeat each method ten times and report the mean performance. The hyper-parameters of baselines are tuned by OpenBox [23] or set according to the original paper if available. Please refer to Appendix A.5 for more details. /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000015 /uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048 /uni00000017/uni00000018 /uni00000017/uni00000019 /uni00000017/uni0000001a /uni00000017/uni0000001b/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000036/uni0000002a/uni00000026/uni00000014/uni0000005b /uni0000002a/uni00000026/uni00000031 /uni00000016/uni00000016/uni0000005b /uni00000024/uni00000033/uni00000033/uni00000031/uni00000033 /uni0000001a/uni0000001b/uni0000005b /uni00000031/uni00000027/uni0000002f/uni00000036 /uni00000015/uni0000005b /uni000000362/uni0000002a/uni00000026 /uni00000014/uni0000005b /uni0000002a/uni00000025/uni00000033/uni00000014/uni0000005b /uni0000002a/uni00000024/uni00000037 /uni00000016/uni0000001a/uni00000015/uni0000005b/uni00000024/uni00000033/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000014/uni00000015/uni0000005b /uni0000002d/uni0000002e/uni00000010/uni00000031/uni00000048/uni00000057 /uni00000014/uni00000014/uni00000016/uni0000005b /uni00000035/uni00000048/uni00000056/uni00000010/uni0000002a/uni00000026/uni00000031 /uni00000014/uni00000016/uni00000015/uni0000005b /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni00000016/uni0000005b Figure 3: Performance along with training time on the Industry dataset. Table 4: Results of inductive settings. Models Flickr Reddit GraphSAGE 50.1 ±1.3 95.4 ±0.0 FastGCN 50.4 ±0.1 93.7 ±0.0 ClusterGCN 48.1 ±0.5 95.7 ±0.0 GraphSAINT 51.1 ±0.1 96.6 ±0.1 NDLS-F+MLP∗ 51.9±0.2 96.6 ±0.1 GraphSAGE+NDLS-L∗ 51.5±0.4 96.3 ±0.0 NDLS∗ 52.6±0.4 96.8 ±0.1 6.2 Experimental Results. End-to-end comparison. To answerQ1, Table 3 and 4 show the test accuracy of considered methods in transductive and inductive settings. In the inductive setting, NDLS outperforms one of the most competitive baselines – GraphSAINT by a margin of 1.5% and 0.2% on Flickr and Reddit. NDLS exceeds the best GNN model among all considered baselines on each dataset by a margin of 0.2% to 0.8% in the transductive setting. In addition, we observe that with NDLS-L, the model performance of MLP, SGC, NDLS-F+MLP, and GraphSAGE can be further improved by a large margin. For example, the accuracy gain for MLP is 21.8%, 11.3%, 8.4%, and 5.6% on Cora, Citseer, PubMed, and Industry, respectively. To answerQ2, we replace the gate mechanism in the vanilla DAGNN with NDLS-L and refer to this method as DAGNN (NDLS-L). Surprisingly, DAGNN (NDLS-L) achieves at least comparable or (often) higher test accuracy compared with AP-GCN and DAGNN (Gate), and it shows that NDLS-L performs better than the learned mechanism in label smoothing. Furthermore, by replacing the original graph kernels with NDLS-F, NDLS-F+MLP outperforms both S2GC and GBP on all compared datasets. This demonstrates the effectiveness of the proposed NDLS. Training Efﬁciency. To answer Q3, we evaluate the efﬁciency of each method on a real-world industry graph dataset. Here, we pre-compute the smoothed features of each linear-model-based 8/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (a) Feature Sparsity /uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000050/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000028/uni00000047/uni0000004a/uni00000048/uni00000056 /uni00000019/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (b) Edge Sparsity /uni00000014/uni00000013/uni00000015/uni00000013 /uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000031/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000036/uni0000002a/uni00000026 /uni00000036/uni0000002c/uni0000002a/uni00000031 /uni000000362/uni0000002a/uni00000026 /uni0000002a/uni00000025/uni00000033 /uni00000031/uni00000027/uni0000002f/uni00000036 (c) Label Sparsity Figure 4: Test accuracy on PubMed dataset under different levels of feature, edge and label sparsity. /uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000027/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni0000002f/uni00000052/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000036/uni00000050/uni00000052/uni00000052/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000026/uni00000052/uni00000055/uni00000044 /uni00000026/uni0000004c/uni00000057/uni00000048/uni00000056/uni00000048/uni00000048/uni00000055 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047 (a) LSI along with the node degree  (b) The visualization of LSI Figure 5: (Left) LSI distribution along with the node degree in three citation networks. (Right) The visualization of LSI in Zachary’s karate club network. Nodes with larger radius have larger LSIs. GNN, and the time for pre-processing is also included in the training time. Figure 3 illustrates the results on the industry dataset across training time. Compared with linear-model-based GNNs, we observe that (1) both the coupled and decoupled GNNs require a signiﬁcantly larger training time; (2) NDLS achieves the best test accuracy while consuming comparable training time with SGC. Performance on Sparse Graphs. To reply Q4, we conduct experiments to test the performance of NDLS on feature, edge, and label sparsity problems. For feature sparsity, we assume that the features of unlabeled nodes are partially missing. In this scenario, it is necessary to calculate a personalized propagation iteration to “recover” each node’s feature representation. To simulate edge sparsity settings, we randomly remove a ﬁxed percentage of edges from the original graph. Besides, we enumerate the number of nodes per class from 1 to 20 in the training set to measure the effectiveness of NDLS given different levels of label sparsity. The results in Figure 4 show that NDLS outperforms all considered baselines by a large margin across different levels of feature, edge, and label sparsity, thus demonstrating that our method is more robust to the graph sparsity problem than the linear-model-based GNNs. Interpretability. As mentioned by Q1, we here answer why NDLS is effective. One theoretical property of LSI is that the value correlates with the node degree negatively. We divide nodes into several groups, and each group consists of nodes with the same degree. And then we calculate the average LSI value for each group in the three citation networks respectively. Figure 5(a) depicts that nodes with a higher degree have a smaller LSI, which is consistent with Theorem 3.1. We also use NetworkX [13] to visualize the LSI in Zachary’s karate club network [36]. Figure 5(b), where the radius of each node corresponds to the value of LSI, shows three interesting observations: (1) nodes with a larger degree have smaller LSIs; (2) nodes in the neighbor area have similar LSIs; (3) nodes adjacent to a super-node have smaller LSIs. The ﬁrst observation is consistent with Theorem 3.1, and the latter two observations show consistency with Theorem 3.2. 7 Conclusion In this paper, we present node-dependent local smoothing (NDLS), a simple and scalable graph learning method based on the local smoothing of features and labels. NDLS theoretically analyzes 9what inﬂuences the smoothness and gives a bound to guide how to control the extent of smoothness for different nodes. By setting a node-speciﬁc smoothing iteration, each node in NDLS can smooth its feature/label to a local-smoothing state and then help to boost the model performance. Extensive experiments on seven real-world graph datasets demonstrate the high accuracy, scalability, efﬁciency, and ﬂexibility of NDLS against the state-of-the-art GNNs. Broader Impact NDLS can be employed in areas where graph modeling is the foremost choice, such as citation networks, social networks, chemical compounds, transaction graphs, road networks, etc. The effectiveness of NDLS when improving the predictive performance in those areas may bring a broad range of societal beneﬁts. For example, accurately predicting the malicious accounts on transaction networks can help identify criminal behaviors such as stealing money and money laundering. Prediction on road networks can help avoid trafﬁc overload and save people’s time. A signiﬁcant beneﬁt of NDLS is that it offers a node-dependent solution. However, NDLS faces the risk of information leakage in the smoothed features or labels. In this regard, we encourage researchers to understand the privacy concerns of NDLS and investigate how to mitigate the possible information leakage. Acknowledgments and Disclosure of Funding This work is supported by NSFC (No. 61832001, 6197200), Beijing Academy of Artiﬁcial Intelligence (BAAI), PKU-Baidu Fund 2019BD006, and PKU-Tencent Joint Research Lab. Zhi Yang and Bin Cui are the corresponding authors. References [1] A. Bojchevski, J. Klicpera, B. Perozzi, A. Kapoor, M. Blais, B. Rózemberczki, M. Lukasik, and S. Günnemann. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2464–2473, 2020. [2] L. Cai and S. Ji. A multi-scale approach for graph link prediction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3308–3315, 2020. [3] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. [4] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In ICLR, 2018. [5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 941–949, 2018. [6] M. Chen, Z. Wei, B. Ding, Y . Li, Y . Yuan, X. Du, and J. Wen. Scalable graph neural networks via bidirectional propagation. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [7] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 785–794, 2016. [8] W. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. InProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 257–266, 2019. 10[9] F. R. Chung and F. C. Graham. Spectral graph theory. Number 92. American Mathematical Soc., 1997. [10] G. Cui, J. Zhou, C. Yang, and Z. Liu. Adaptive graph encoder for attributed graph embedding. In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 976–985, 2020. [11] H. Dihe. An introduction to markov process in random environment [j]. Acta Mathematica Scientia, 5, 2010. [12] Q. Guo, X. Qiu, X. Xue, and Z. Zhang. Syntax-guided text generation via graph neural network. Sci. China Inf. Sci., 64(5), 2021. [13] A. Hagberg, P. Swart, and D. S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008. [14] W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 1024–1034, 2017. [15] X. He, K. Deng, X. Wang, Y . Li, Y . Zhang, and M. Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 639–648, 2020. [16] W. Hu, M. Fey, H. Ren, M. Nakata, Y . Dong, and J. Leskovec. OGB-LSC: A large-scale challenge for machine learning on graphs. CoRR, abs/2103.09430, 2021. [17] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 4563–4572, 2018. [18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [19] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018. [20] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. [21] K.-H. Lai, D. Zha, K. Zhou, and X. Hu. Policy-gnn: Aggregation optimization for graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 461–471, 2020. [22] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. [23] Y . Li, Y . Shen, W. Zhang, Y . Chen, H. Jiang, M. Liu, J. Jiang, J. Gao, W. Wu, Z. Yang, C. Zhang, and B. Cui. Openbox: A generalized black-box optimization service. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 3209–3219, 2021. [24] A. Liaw, M. Wiener, et al. Classiﬁcation and regression by randomforest. R news, 2(3):18–22, 2002. [25] M. Liu, H. Gao, and S. Ji. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 338–348, 2020. 11[26] X. Miao, N. M. Gürel, W. Zhang, Z. Han, B. Li, W. Min, S. X. Rao, H. Ren, Y . Shan, Y . Shao, Y . Wang, F. Wu, H. Xue, Y . Yang, Z. Zhang, Y . Zhao, S. Zhang, Y . Wang, B. Cui, and C. Zhang. Degnn: Improving graph neural networks with graph decomposition. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 1223–1233, 2021. [27] X. Miao, W. Zhang, Y . Shao, B. Cui, L. Chen, C. Zhang, and J. Jiang. Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture. IEEE Transactions on Knowledge and Data Engineering, 2021. [28] E. Rossi, F. Frasca, B. Chamberlain, D. Eynard, M. M. Bronstein, and F. Monti. SIGN: scalable inception graph neural networks. CoRR, abs/2004.11198, 2020. [29] I. Spinelli, S. Scardapane, and A. Uncini. Adaptive propagation graph convolutional network. IEEE Transactions on Neural Networks and Learning Systems, 2020. [30] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [31] K. Wang, Z. Shen, C. Huang, C.-H. Wu, Y . Dong, and A. Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396–413, 2020. [32] F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 6861–6871, 2019. [33] S. Wu, W. Zhang, F. Sun, and B. Cui. Graph neural networks in recommender systems: A survey. CoRR, abs/2011.02260, 2020. [34] S. Wu, Y . Zhang, C. Gao, K. Bian, and B. Cui. Garg: Anonymous recommendation of point- of-interest in mobile networks by graph convolution network. Data Science and Engineering, 5(4):433–447, 2020. [35] K. Xu, C. Li, Y . Tian, T. Sonobe, K. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 5449–5458, 2018. [36] W. W. Zachary. An information ﬂow model for conﬂict and ﬁssion in small groups. Journal of anthropological research, 33(4):452–473, 1977. [37] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [38] W. Zhang, Y . Jiang, Y . Li, Z. Sheng, Y . Shen, X. Miao, L. Wang, Z. Yang, and B. Cui. ROD: reception-aware online distillation for sparse graphs. In KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pages 2232–2242, 2021. [39] W. Zhang, X. Miao, Y . Shao, J. Jiang, L. Chen, O. Ruas, and B. Cui. Reliable data distillation on graph convolutional network. In Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020, pages 1399–1414, 2020. [40] W. Zhang, Z. Sheng, Y . Jiang, Y . Xia, J. Gao, Z. Yang, and B. Cui. Evaluating deep graph neural networks. CoRR, abs/2108.00955, 2021. [41] X. Zhang, H. Liu, Q. Li, and X. Wu. Attributed graph clustering via adaptive graph convolution. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4327–4333, 2019. [42] H. Zhu and P. Koniusz. Simple spectral graph convolution. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. 12A Appendix A.1 Proofs of Theorems We represent the adjacency matrix and the diagonal degree matrix of graphGby Aand Drespectively, represent D+I and A+I by ˜Dand ˜A. Then we denote ˜D−1 ˜Aas a transition matrix P. Suppose P is connected, which means the graph is connected, for any initial distribution π0, let ˜π(π0) = lim k→∞ π0Pk, (15) then according to [11], for any initial distribution π0 ˜π(π0)i = 1 n n∑ j=1 Pji, (16) where ˜πi denotes the ith component of ˜π(π0), and ndenotes the number of nodes in graph. If matrix P is unconnected, we can divide P into connected blocks. Then for each blocks(denoted as Bg), there always be ˜π(π0)i = 1 ng ∑ j∈Bg Pji ∗ ∑ j∈Bg π0j, (17) where ng is the number of nodes in Bg. To make the proof concise, we will assume matrix P is connected, otherwise we can perform the same operation inside each block. Therefore, ˜π is independent to π0, thus we replace ˜π(π0) by ˜π. Deﬁnition A.1 (Local Mixing Time). The local mixing time (parameterized by ϵ) with an initial distribution is deﬁned as T(π0,ϵ) = min{t: ||˜π−π0Pt||2 <ϵ}, (18) where \"||·||2\" symbols two-nor m. In order to consider the impact of each node to the others separately, letπ0 = ei, where ei is a one-hot vector with the ith component equal to 1, and the other components equal to 0. According to [ 9] we have lemma A.1. Lemma A.1. |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (19) where λ2 is the second large eigenvalue of P and ˜di denotes the degree of node vi plus 1 (to include itself). ˜di = di + 1, ˜dj = dj + 1, Theorem A.2. T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n), (20) where mand ndenote the number of edges and nodes in graph Gseparately. ˜di = di + 1, Proof. [9] shows that when π0 = ei, |(eiPt)j −˜πj|≤ √ ˜dj ˜di λt 2, (21) where (eiPt)j symbols the jth element of eiPt. We denote eiPt as πi(t), then ||˜π−πi(t)||2 2 = n∑ j=1 (˜πj −πi(t)j)2 ≤ n∑ j=1 ˜dj ˜di λ2t 2 = 2m+ n ˜di λ2t 2 , (22) 13which means ||˜π−πi(t)||2 ≤ √ 2m+ n ˜di λt 2. (23) Now let ϵ= √ 2m+ n ˜di λt 2, there exists T(ei,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). Next consider the real situation in SGC with n×m-dimension matrix X(0) as input, where nis the number of nodes, mis the number of features. We apply P as the normalized adjacent matrix.(The deﬁnition of P is the same as ˜A in main text). In feature propagation we have X(t) =PtX(0), Now consider the hth feature of X, we deﬁne an n×ninﬂuence matrix Ihij(t) = ∂X(t)ih ∂X(0)jh , (24) Because Ih(k) is independent to h, we replace Ih(k) by I(k), which can be formulated as I(k) =Ih(k), ∀h∈{1,2,..,f }, (25) where f symbols the number of features of X. Deﬁnition A.2 (Local Smoothing Iteration). The Local Smoothing Iteration (parameterized by ϵ) is deﬁned as K(i,ϵ) = min{k: ||˜Ii −Ii(k)||2 <ϵ}. (26) According to Theorem A.2, there exists Theorem A.3 (Theorem 3.1 in main text). When the normalized adjacent matrix is P, K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). (27) Proof. From equation (9) we can derive that ||eiP∞−eiPk||2 ≤ √ 2m+ n ˜di λk 2. Because Ii(k) =Pk i = eiPk Ii(∞) =P∞ i = eiP∞, we have ||Ii(∞) −Ii(k)||2 ≤ √ 2m+ n ˜di λk 2. Now let ϵ= √ 2m+ n ˜di λk 2, there exists K(i,ϵ) ≤logλ2 (ϵ √ ˜di 2m+ n). 14Therefore, we expand Theorem A.3 to the propagation in SGC or our method. What is remarkable, Theorem A.3 requires P, which is equal to ˜D−1 ˜Aas the normalized adjacent matrix. From Theorem A.3 we can conclude that the node which has a lager degree may need more steps to propagate. At the same time, we have another bond of local mixing time as following. Theorem A.4. For each node vi in graph G, there always exits T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. (28) where N(i) is the set of node vi’s neighbours. Proof. ∥|˜π−eiPt+1||2 = 1 |N(i)| ∑ j∈N(i) ||˜π−ejPt||2 ≤ max j∈N(i) ||˜π−ejPt||2. (29) Therefore, when max j∈N(i) ||˜π−ejPt||2 ≤ϵ, there exists ||˜π−eiPt+1||2 ≤ϵ. Thus we can derive that T(ei,ϵ) ≤max{T(ej,ϵ),j ∈N(i)}+ 1. As we extend Theorem A.2 to Theorem A.3, according to Theorem A.4, there always be Theorem A.5 (Theorem 3.2 in main text). For each node vi in graph G, there always exits K(i,ϵ) ≤max{K(j,ϵ),j ∈N(i)}+ 1. (30) A.2 Results with More Base Models Our proposed NDLS consists of three stages: (1) feature smoothing with NDLS (NDLS-F), (2) model training with smoothed features, and (3) label smoothing with NDLS (NDLS-L). In stage (2), the default option of the base model is a Multilayer Perceptron (MLP). Besides MLP, many other models can also be used in stage (2) to generate soft labels. To verify it, here we replace the MLP in stage (2) with popular machine learning models Random Forest [24] and XGBoost [7], and measure their node classiﬁcation performance on PubMed dataset. The experiment results are shown in Table 3 where Random Forest and XGBoost are abbreviated as RF and XGB respectively. Compared to the vanilla model, both Random Forest and XGBoost achieve signiﬁcant performance gain with the addition of our NDLS. With the help of NDLS, Random Forest and XGBoost outperforms their base models by 6.1% and 7.5% respectively. From Table 3, we can observe that both NDLS-F and NDLS-L can contribute great performance boost to the base model, where the gains are at least 5%. When all equipped with both NDLS-F and NDLS-L, XGBoost beat the default MLP, achieving a test accuracy of81.6%. Although Random Forest – 80.5% – cannot outperform the other two models, it is still a competitive model. The above experiment demonstrates that the base model selection in stage (2) is rather ﬂexible in our NDLS. Both traditional machine learning methods and neural networks are promising candidates in the proposed method. A.3 Dataset Description Cora, Citeseer, and Pubmed1 are three popular citation network datasets, and we follow the public training/validation/test split in GCN [18]. In these three networks, papers from different topics are 1https://github.com/tkipf/gcn/tree/master/gcn/data 15Table 5: Results of different base models on PubMed. Base Models Models Accuracy Gain MLP Base 72.7 ±0.6 - + NDLS-F 81.1 ±0.6 + 8.4 + NDLS-L 81.1 ±0.6 + 8.4 + NDLS (both) 81.4 ±0.4 + 8.7 RF Base 74.4 ±0.2 - + NDLS-F 80.3 ±0.1 + 5.9 + NDLS-L 80.0 ±0.2 + 5.6 + NDLS (both) 80.5 ±0.4 + 6.1 XGB Base 74.1 ±0.2 - + NDLS-F 81.0 ±0.3 + 6.9 + NDLS-L 79.8 ±0.2 + 5.7 + NDLS (both) 81.6 ±0.3 + 7.5 considered as nodes, and the edges are citations among the papers. The node attributes are binary word vectors, and class labels are the topics papers belong to. Reddit is a social network dataset derived from the community structure of numerous Reddit posts. It is a well-known inductive training dataset, and the training/validation/test split in our experiment is the same as the one in GraphSAGE [14]. Flickr originates from NUS-wide 2 and contains different types of images based on the descriptions and common properties of online images. The public version of Reddit and Flickr provided by GraphSAINT3 is used in our paper. Industry is a short-form video graph, collected from a real-world mobile application from our industrial cooperative enterprise. We sampled 1,000,000 users and videos from the app, and treat these items as nodes. The edges in the generated bipartite graph represent that the user clicks the short-form videos. Each user has 64 features and the target is to category these short-form videos into 253 different classes. ogbn-papers100M is a directed citation graph of 111 million papers indexed by MAG [31]. Among its node set, approximately 1.5 million of them are arXiv papers, each of which is manually labeled with one of arXiv’s subject areas. Currently, this dataset is much larger than any existing public node classiﬁcation datasets. A.4 Compared Baselines The main characteristic of all baselines are listed below: • GCN [18]: GCN is a novel and efﬁcient method for semi-supervised classiﬁcation on graph-structured data. • GAT [30]: GAT leverages masked self-attention layers to specify different weights to different nodes in a neighborhood, thus better represent graph information. • JK-Net [35]: JK-Net is a ﬂexible network embedding method that could gather different neighborhood ranges to enable better structure-aware representation. • APPNP [19]: APPNP uses the relationship between graph convolution networks (GCN) and PageRank to derive improved node representations. • AP-GCN [29]: AP-GCN uses a halting unit to decide a receptive range of a given node. 2http://lms.comp.nus.edu.sg/research/NUS-WIDE.html 3https://github.com/GraphSAINT/GraphSAINT 16Table 6: URLs of baseline codes. Type Baselines URLs Coupled GCN https://github.com/rusty1s/pytorch_geometric GAT https://github.com/rusty1s/pytorch_geometric Decoupled APPNP https://github.com/rusty1s/pytorch_geometric PPRGo https://github.com/TUM-DAML/pprgo_pytorch AP-GCN https://github.com/spindro/AP-GCN DAGNN https://github.com/divelab/DeeperGNN Sampling GraphSAGE https://github.com/williamleif/GraphSAGE GraphSAINT https://github.com/GraphSAINT/GraphSAINT FastGCN https://github.com/matenure/FastGCN Cluster-GCN https://github.com/benedekrozemberczki/ClusterGCN Linear SGC https://github.com/Tiiiger/SGC SIGN https://github.com/twitter-research/sign S2GC https://github.com/allenhaozhu/SSGC GBP https://github.com/chennnM/GBP NDLS https://github.com/zwt233/NDLS • DAGNN [25]: DAGNN proposes to decouple the representation transformation and propagation, and show that deep graph neural networks without this entanglement can leverage large receptive ﬁelds without suffering from performance deterioration. • PPRGo [1]: utilizes an efﬁcient approximation of information diffusion in GNNs resulting in signiﬁcant speed gains while maintaining state-of-the-art prediction performance. • GraphSAGE [14]: GraphSAGE is an inductive framework that leverages node attribute information to efﬁciently generate representations on previously unseen data. • FastGCN [4]: FastGCN interprets graph convolutions as integral transforms of embedding functions under probability measures. • Cluster-GCN [8]: Cluster-GCN is a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. • GraphSAINT [37]: GraphSAINT constructs mini-batches by sampling the training graph, rather than the nodes or edges across GCN layers. • SGC [32]: SGC simpliﬁes GCN by removing nonlinearities and collapsing weight matrices between consecutive layers. • SIGN [28]: SIGN is an efﬁcient and scalable graph embedding method that sidesteps graph sampling in GCN and uses different local graph operators to support different tasks. • S2GC [42]: S2GC uses a modiﬁed Markov Diffusion Kernel to derive a variant of GCN, and it can be used as a trade-off of low-pass and high-pass ﬁlter which captures the global and local contexts of each node. • GBP [6]: GBP utilizes a localized bidirectional propagation process from both the feature vectors and the training/testing nodes Table 6 summarizes the github URLs of the compared baselines. Following the original paper, we implement JK-Net by ourself since there is no ofﬁcial version available. A.5 Implementation Details Hyperparameter details. In stage (1), when computing the Local Smoothing Iteration, the maximal value of k in equation (12) is set to 200 and the optimal ϵ value is get by means of a grid search from {0.01, 0.03, 0.05}. In stage (2), we use a simple two-layer MLP to get the base prediction. The hidden size is set to 64 in small datasets – Cora, Citeseer and Pubmed. While in larger datasets – Flicker, Reddit, Industry and ogbn-papers100M, the hidden size is set to 256. As for the dropout percentage and the learning rate, we use a grid search from {0.2, 0.4, 0.6, 0.8} and {0.1, 17Table 7: Performance comparison between C&S and NDLS-L Methods Cora Citeseer PubMed ogbn-papers100M MLP+C&S 87.2 76.6 88.3 63.9 MLP+NDLS-L 88.1 78.3 88.5 64.6 Table 8: Performance comparison under varied label rate on the Cora dataset. Methods 2% 5% 10% 20% 40% 60% MLP+S 63.1 77.8 82.6 84.2 85.4 86.4 MLP+C&S 62.8 76.7 82.8 84.9 86.4 87.2 MLP+NDLS-L 77.4 83.9 85.3 86.5 87.6 88.1 Table 9: Performance comparison after combining the node-dependent idea with C&S. Methods Cora Citeseer PubMed MLP+C&S 76.7 70.8 76.5 MLP+C&S+nd 79.9 71.1 78.4 0.01, 0.001} respectively. In stage (3), during the computation of the Local Smoothing Iteration, the maximal value of kis set to 40. The optimal value of ϵis obtained through the same process in stage (1). Implementation environment. The experiments are conducted on a machine with Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz, and a single NVIDIA TITAN RTX GPU with 24GB memory. The operating system of the machine is Ubuntu 16.04. As for software versions, we use Python 3.6, Pytorch 1.7.1 and CUDA 10.1. A.6 Comparison and Combination with Correct&Smooth Similar to our NDLS-L, Correct and Smooth (C&S) also applies post-processing on the model prediction. Therefore, we compare NDLS-L with C&S below. Adaptivity to node. C&S adopts a propagation scheme based on Personalized PageRank (PPR), which always maintains certain input information to slow down the occurrence of over-smoothing. The expected number of smoothing iterations is controlled by the restart probability, which is a constant for all nodes. Therefore, C&S still falls into the routine of ﬁxed smoothing iteration. Instead, NDLS-L employs node-speciﬁc smoothing iterations. We compare each method’s performance (test accuracy, %) under the same data split as in the C&S paper (60%/20%/20% on three citation networks, ofﬁcial split on ogbn-papers100M), and the experimental results in Table 7 show that NDLS-L outperforms C&S in different datasets. Sensitivity to label rate. During the “Correct” stage, C&S propagates uncertainties from the training data across the graph to correct the base predictions. However, the uncertainties might not be accurate when the number of training nodes is relatively small, thus even degrading the performance. To conﬁrm the above assumption, we conduct experiments on the Cora dataset under different label rates, and the experimental results are provided in Table 8. As illustrated, the result of C&S drops much faster than NDLS-L’s when the label rate decreases. What’s more, MLP+S (removing the “Correct” stage) outperforms MLP+C&S when the label rate is low as expected. Compared with C&S, NDLS is more general in terms of smoothing types. C&S can only smooth label predictions. Instead, NDLS can smooth both node features and label predictions and combine them to boost the model performance further. 18Table 10: Efﬁciency comparison on the PubMed dataset. SGC S2GC GBP NDLS SIGN JK-Net DAGNN GCN ResGCN APPNP GAT Time 1.00 1.19 1.20 1.50 1.59 11.42 14.39 20.43 20.49 28.88 33.23 Accuracy 78.9 79.9 80.6 81.4 79.5 78.8 80.5 79.3 78.6 80.1 79.0 Table 11: Performance comparison on the ogbn-arxiv dataset. MLP MLP+C&S GCN SGC SIGN DAGNN JK-Net S2GC GBP NDLS GAT Accuracy55.50 71.58 71.74 71.72 71.95 72.09 72.19 72.21 72.45 73.04 73.56 Node Adaptive C&S. The node-dependent mechanism in our NDLS can easily be combined with C&S. The two stages of C&S both contain a smoothing process using the personalized PageRank matrix, where a coefﬁcient controls the remaining percentage of the original node feature. Here, we can precompute the smoothed node features after the same smoothing step yet under different values like 0.1, 0.2, ..., 0.9. After that, we adopt the same strategy in our NDLS: for each node, we choose the ﬁrst in the ascending order that the distance from the smoothed node feature to the stationarity is less than a tuned hyperparameter. By this means, the smoothing process in C&S can be carried out in a node-dependent way. We also evaluate the performance of C&S combined with the node-dependent idea (represented as C&S+nd) on the three citation networks under ofﬁcial splits, and the experimental results in Table 9 show that C&S combined with NDLS consistently outperforms the original version of C&S. A.7 Training Efﬁciency Study we measure the training efﬁciency of the compared baselines on the widely used PubMed dataset. Using the training time of SGC as the baseline, the relative training time and the corresponding test accuracy of NDLS and the baseline methods are shown in Table 10. Compared with other baselines, NDLS can get the highest test accuracy while maintaining competitive training efﬁciency. A.8 Experiments on ogbn-arxiv We also conduct experiments on the ogbn-arxiv dataset. The experiment results (test accuracy, %) are provided in Table 11. Although GAT outperforms NDLS on ogbn-arxiv dataset, it is hard to scale to large graphs like ogbn-papers100M dataset. Note that MLP+C&S on the OGB leaderboard makes use of not only the original node feature but also diffusion embeddings and spectral embeddings. Here we remove the latter two embeddings for fairness, and the authentic MLP+C&S achieves 71.58% on the ogbn-arxiv dataset. 19",
      "meta_data": {
        "arxiv_id": "2110.14377v1",
        "authors": [
          "Wentao Zhang",
          "Mingyu Yang",
          "Zeang Sheng",
          "Yang Li",
          "Wen Ouyang",
          "Yangyu Tao",
          "Zhi Yang",
          "Bin Cui"
        ],
        "published_date": "2021-10-27T12:24:41Z",
        "venue": "NeurIPS 2021",
        "pdf_url": "https://arxiv.org/pdf/2110.14377v1.pdf"
      }
    },
    {
      "title": "Old can be Gold: Better Gradient Flow can Make Vanilla-GCNs Great Again",
      "abstract": "Despite the enormous success of Graph Convolutional Networks (GCNs) in\nmodeling graph-structured data, most of the current GCNs are shallow due to the\nnotoriously challenging problems of over-smoothening and information squashing\nalong with conventional difficulty caused by vanishing gradients and\nover-fitting. Previous works have been primarily focused on the study of\nover-smoothening and over-squashing phenomena in training deep GCNs.\nSurprisingly, in comparison with CNNs/RNNs, very limited attention has been\ngiven to understanding how healthy gradient flow can benefit the trainability\nof deep GCNs. In this paper, firstly, we provide a new perspective of gradient\nflow to understand the substandard performance of deep GCNs and hypothesize\nthat by facilitating healthy gradient flow, we can significantly improve their\ntrainability, as well as achieve state-of-the-art (SOTA) level performance from\nvanilla-GCNs. Next, we argue that blindly adopting the Glorot initialization\nfor GCNs is not optimal, and derive a topology-aware isometric initialization\nscheme for vanilla-GCNs based on the principles of isometry. Additionally,\ncontrary to ad-hoc addition of skip-connections, we propose to use\ngradient-guided dynamic rewiring of vanilla-GCNs} with skip connections. Our\ndynamic rewiring method uses the gradient flow within each layer during\ntraining to introduce on-demand skip-connections adaptively. We provide\nextensive empirical evidence across multiple datasets that our methods improve\ngradient flow in deep vanilla-GCNs and significantly boost their performance to\ncomfortably compete and outperform many fancy state-of-the-art methods. Codes\nare available at: https://github.com/VITA-Group/GradientGCN.",
      "full_text": "Old can be Gold: Better Gradient Flow can Make Vanilla-GCNs Great Again Ajay Jaiswal∗, Peihao Wang∗, Tianlong Chen, Justin F. Rousseau, Ying Ding, Zhangyang Wang University of Texas at Austin {ajayjaiswal, peihaowang, tianlong.chen, atlaswang}@utexas.edu {justin.rousseau, ying.ding}@austin.utexas.edu Abstract Despite the enormous success of Graph Convolutional Networks (GCNs) in mod- eling graph-structured data, most of the current GCNs are shallow due to the notoriously challenging problems of over-smoothening and information squashing along with conventional difﬁculty caused by vanishing gradients and over-ﬁtting. Previous works have been primarily focused on the study of over-smoothening and over-squashing phenomena in training deep GCNs. Surprisingly, in comparison with CNNs/RNNs, very limited attention has been given to understanding how healthy gradient ﬂow can beneﬁt the trainability of deep GCNs. In this paper, ﬁrstly, we provide a new perspective of gradient ﬂow to understand the substandard per- formance of deep GCNs and hypothesize that by facilitating healthy gradient ﬂow, we can signiﬁcantly improve their trainability, as well as achieve state-of-the-art (SOTA) level performance from vanilla-GCNs [1]. Next, we argue that blindly adopting the Glorot initialization for GCNs is not optimal, and derive a topology- aware isometric initialization scheme for vanilla-GCNs based on the principles of isometry. Additionally, contrary to ad-hoc addition of skip-connections, we propose to use gradient-guided dynamic rewiring of vanilla-GCNs with skip connections. Our dynamic rewiring method uses the gradient ﬂow within each layer during training to introduce on-demand skip-connections adaptively. We provide extensive empirical evidence across multiple datasets that our methods improve gradient ﬂow in deep vanilla-GCNs and signiﬁcantly boost their performance to comfortably compete and outperform many fancy state-of-the-art methods. Codes are available at: https://github.com/VITA-Group/GradientGCN. 1 Introduction Graphs are omnipresent and Graphs convolutional networks (GCNs)[1] and their variants[2, 3, 4, 5, 6, 7, 8, 9, 10] are a powerful family of neural networks which can learn from graph-structured data. GCNs have been enormously successful in numerous real-world applications such as recommenda- tion systems [11, 12], social and academic networks[13, 11, 5, 4, 14], modeling proteins for drug discovery[15, 16, 12], computer vision[17, 18], etc. Despite their popularity, training deep GCNs are notoriously hard and many classical GCNs[1, 3, 19] achieve their best performance with shallow depth and completely bilk trainability with increasing stacks of layers and non-linearity[20, 21]. Previous works have focused on studying the roadblocks of training deep GCNs from the perspective of over-smoothening [20, 21] and information bottleneck [22] and many approaches broadly catego- rized as architectural [23, 24, 20, 25], and regularization & normalization[26, 27, 28, 29] tweaks has been proposed for mitigation. In recent work, [30] theoretically validated that the deeper GCN model is at least as expressive as the shallow GCN model, as long as deeper GCNs are trained properly. ∗Equal Contribution. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2210.08122v1  [cs.LG]  14 Oct 2022Furthermore, [31] studied the training difﬁculty of GCNs from the perspective of graph signal energy loss, and proposed modifying the GCN operator, from the energy perspective but it failed to recover the full potential of vanilla-GCNs. Surprisingly, unlike traditional neural networks (CNNs and RNNs), limited effort has been given to understand hurdles in the trainability of deep GCNs from the signal propagation perspective, i.e. gradient ﬂow. In this paper, we comprehensively show that deep GCNs suffer from poor gradient ﬂow (error signals) under back-propagation across multiple datasets and architectures, which signiﬁcantly hurt their trainability and lead to substandard performance. Further, we are motivated to explore an orthogonal direction: Can we make vanilla-GCNs [1] go deeperand comparable/better than SOTA, by improvising healthy gradient ﬂow during training? In this work, weﬁrstly look into theeffect of initialization (surprisingly overlooked) in GCNs, and ﬁnd that blindly adopting the Glorot initialization [32] from CNNs has critical impacts on the gradient ﬂow of GCN training, especially for deep GCNs. When the initial weights are not chosen appropriately, the propagation of the input signal into the layers of random weights can result in poor error signals under back-propagation [33]. Inspired from the signal propagation perspective, we leverage the principle of isometry [34, 35] to derive a theoretical initialization scheme for vanilla-GCNs, by incorporating the graph topological information - which yields a remarkably easy-to-compute form. Skip-connections have been very resourceful to train deep convolutional networks [ 36, 37] by improving the gradient ﬂow, and recently some works [23, 24, 38, 20] have identiﬁed their beneﬁts for deep GCNs. Although their beneﬁts are known, most of the existing works attempt to add skip- connections in an ad-hoc and non-optimal fashion in deep GCNs which creates additional overhead of storing multiple activations during training along with substandard performance. Secondly, we study layerwise gradient ﬂow during the training of deep vanilla-GCNs and identify that there exist some layers which receive almost zero error signal (i.e. gradients) during backpropagation and lose trainability and expressiveness. We use computationally efﬁcient Gradient Flow (p-norm of gradients within a layer) as a metric to dynamically identify such layers during training and propose a principled way of injecting skip-connections on-demand to facilitate healthy gradient ﬂow in deep GCNs. Our main contributions can be summarized as: • Deep GCNs suffer from poor gradient ﬂow during training. We rigorously investigate the gradient ﬂow during the training of GCNs and observe that GCNs suffer from poor gradient ﬂow which worsens with the increase in depth. We hypothesize that by improving the gradient ﬂow in deep vanilla-GCNs, along with effectively improving their trainability, we can achieve SOTA-level performance from vanilla-GCNs [1]. • Topology-Aware Isometric Initialization of GCNs. Unlike blind adoption of Glorot ini- tialization [32] for GCNs, we derive a theoretical initialization scheme for vanilla-GCNs [1] based on the principle of isometry, which relates benign initial parameters with graph topol- ogy. We experimentally validate that our new initialization strategy signiﬁcantly improves the gradient ﬂow in deep GCNs and provide huge performance beneﬁts. • Gradient-Guided Dynamic Rewiring of GCNs. Contrary to ad-hoc addition of skip- connections to improve GCNs performance, in this paper, we leverage Gradient Flow to introduce dynamic rewiring strategy of vanilla-GCNs with skip-connections. Our new rewiring improves gradient ﬂow, reduces the overhead of storing multiple intermediate activations (dense, initial, residual, jumping skips), as well as outperforms many ad-hoc skip connection mechanisms. 2 Methodology In this section, we aim to discuss the trainability of deep GCNs from the signal propagation per- spective, i.e. gradient ﬂow. We will provide a detailed introduction and theoretical derivation of our topology-aware isometric initialization. Furthermore, we will present a novel way to incorporate skip-connections using Gradient Flow to improve the trainability of deep GCNs. 2.1 Preliminaries We begin by formulating Graph Convolutional Networks (GCN) along the way introducing our notation. Let G= (V,E) be a simple and undirected graph with vertex set Vand edge set E. Let 20 20 40 60 80 100 120 Epoch 0.0 0.1 0.2 0.3 0.4 0.5Gradient Flow Final Acc = 26.3% Final Acc = 77.9% Glorot Initialization Our initialization 0 20 40 60 80 100 120 Epoch 0.0 0.1 0.2 0.3 0.4 0.5Gradient Flow Final Acc = 26.3% Final Acc  = 80.2% Vanilla-GCN Gradient-Guided Rewired GCN Figure 1: (a) Comparison of Gradient ﬂow in 10-layer vanilla-GCN with default (Glorot initialization) and our Topology-Aware initialization. (b) Comparison of Gradient ﬂow in 10-layer GCN and GCNs with our new Gradient Flow Guided Rewiring. A ∈RN×N be the associated adjacency matrix, such that Aij = 1 if (i,j) ∈E , and Aij = 0 otherwise. N = |V|is the number of nodes. We also deﬁne di = ∑ j Aij as the node degree of the i-th vertex. Let X ∈RC×N be the node feature matrix, whose i-th column represents a d-dimension feature vector for the i-th node. GCNs aim to learn an embedding for each node via learnable graph convolutional ﬁlters [2, 1]. A graph convolutional layer can be formulated as 2: f(G,X) = WX (A + I), (1) where adding I to adjacency matrix is known as the self-loop trick [1], W ∈RC′×C is the weight matrix of this layer, and C′is the dimension of output channel. An L-layer GCN cascades Llayer of graph convolutional layers followed by non-linear activations: Y = f(L) ◦σ◦f(L−1) ◦··· f(2) ◦σ◦f(1)(G,X), (2) where Y ∈RC′ is the ﬁnal output of the GCN, and σ(·) denotes a non-linear activation, which is typically chosen as ReLU. Although there exists lots of message-passing based Graph Neural Networks (GNNs), which are incorporated with highly sophisticated mechanisms, such as attention [3] and gating [39, 40], the focus of our work only lies on the most classical GCN model (Eq. 2 or [1]) that exactly follows the rigorous deﬁnition of graph convolutions [41]. As we will show in our experiments, our proposal will bring this theoretically clean GCN back to the SOTA performance. 2.2 Understanding Gradient Flow in Deep GCNs 0 20 40 60 80 100 120 Epoch 0.01 0.02 0.03 0.04 0.05 0.06 0.07Gradient FlowFinal Acc = 81.2% Final Acc = 26.3% Final Acc = 26.3% Layer 2 Layer 10 Layer 16 Figure 2: Gradient ﬂow of vanilla-GCNs (layer 4,10,16) trained on Cora. Gradient ﬂow and its associated problems such as ex- ploding and vanishing gradients have been extensively explored in the dynamics of learning of neural networks (CNNs/RNNs) [32, 33, 35]. GCNs [ 1] are a special category of neural networks, which uses “graph convo- lution” operation (linear transformation of all neighbors of a node followed by non-linear activation) to learn the graph representation. Despite being powerful in learning high-quality node representations, GCNs have limited ability to extract information from high-order neighbors and signiﬁcantly lose their trainability and performance. Recently, many works [ 20, 21, 22, 24] have been proposed to address this issue from the perspective of over-smoothing and information bot- tleneck. However, the signal propagation perspective to understand the trainability and performance of GCNs has been highly overlooked compared to the attention it has received for CNNs/RNNs. Gradient ﬂow is used to study the optimization dynamics of neural networks and it has been widely known that healthy gradient ﬂow facilitates better optimization [32, 33, 35, 42]. Consider an L-layer GCN (see Eq. 2), we denote the weight parameter for the l-th layer f(l) as Wl. Suppose we have cost function L, we calculate the gradient across each layer and effective gradient ﬂow (GF) as: g1 = ∂L W1 ,··· ,gi = ∂L Wi ,··· ,gL = ∂L WL Gradient Flow: GFp = 1 L L∑ n=1 ∥gn∥p (3) (4) 2Here we consider A is symmetrically normalized by default. 3where for every layer l, ∂L ∂Wl represents gradients of the learnable parameters Wl and we have used p = 2 in our experiments. Figure 2 represents the gradient ﬂow during training of vanilla-GCNs with layer 4, 6, and 10 on the Cora dataset. Figure 3 illustrates the comparison of validation loss and gradient ﬂow in vanilla-GCNs with 2 and 10 layers on Cora, Citeseer, and Pubmed. We consistently observed across each dataset that with increasing depth, the gradient ﬂow across layers decreases signiﬁcantly along with the performance. In this paper, we hypothesize that by improving the gradient ﬂow in deep vanilla-GCNs, along with effectively improving their trainability, we can achieve SOTA- level performance from vanilla-GCNs. We propose a new topology-aware initialization strategy based on the principles of isometry and dynamic architecture rewiring for vanilla-GCNs to improvise the gradient ﬂow and subsequently performance (Figure 1(a),(b)). 0 20 40 60 80 100 120 Epoch 1.0 1.2 1.4 1.6 1.8 2.0Validation Loss 0.01 0.02 0.03 0.04 0.05 0.06 0.07 Gradient Flow Cora 0 20 40 60 80 100 120 Epoch 1.2 1.3 1.4 1.5 1.6 1.7 1.8Validation Loss 0.01 0.02 0.03 0.04 0.05 0.06 Gradient Flow Citeseer 0 20 40 60 80 100 120 Epoch 0.6 0.7 0.8 0.9 1.0 1.1Validation Loss Val Loss Layer 2 Layer 10 0.01 0.02 0.03 0.04 0.05 0.06 Gradient Flow Pubmed Grad Flow Layer 2  Layer 10 Figure 3: Comparison of validation loss and gradient ﬂow in vanilla-GCNs with 2 and 10 layers. 2.3 Topology-Aware Isometric Initialization Recent work of [43] has revealed that initializing network parameters nearly isometric can train very deep CNNs without any normalization. We are inspired to properly initialize each GCN layer to make it an isometric mapping. To begin with, we ﬁrst give the formal deﬁnition of isometry below: Deﬁnition 1. [43] A map f : X→Y between two inner-product spaces is called an isometry if ⟨f(x),f(x′)⟩= ⟨x,x′⟩ (5) for all x,x′ ∈X. Existing isometric initialization techniques are not free lunch for GCNs. Qi et al. [43] showed that a convolutional operator is isometric if and only if their cross-channel weights are orthogonal to each other. The straightforward implication is that delta kernel can be served as the isometric initialization. However, computing delta kernel in graph signal processing needs to involve complex numbers and eigen-decomposition of the adjacency matrix [44], which is infeasible for a common deep learning framework. To this end, we re-establish theoretical results for GCNs from Deﬁnition 1. Consider one GCN layer (Eq. 1), our goal is to seek a weight matrix such that pair-wise angles between node features are invariant after transforming by the layer. First of all, we denote the output feature of the i-th node as yi = WXa i where ai is the i-th column of (A + I). The inner product of two node features i,j ∈[N] can be computed as: ⟨yi,yj⟩= [( a⊤ i ⊗W ) vec(X) ]⊤[( a⊤ j ⊗W ) vec(X) ] = Tr ([( aia⊤ j ) ⊗ ( W⊤W )] vec(X) vec(X)⊤ ) , (6) (7) where vec(·) denotes vectorization of a matrix, Eq. 6 follows from the equality vec(ABC) = (C⊤⊗A) vecB [45]. On the other hand, xi = ICXei where ei is the i-th canonical basis, we can obtain a similar form of ⟨xi,xj⟩: ⟨xi,xj⟩= Tr ([( eie⊤ j ) ⊗IC ] vec(X) vec(X)⊤) (8) To achieve isometry, we hope to solve a W such that Eq. 8 equals to Eq. 7. However, a simple closed-form solution does not exist in general. Instead, we seek a W to minimize the difference between Eq. 8 and Eq. 7. W∗= argmin W∈RC′×C ∑ i,j∈[N] ⏐⏐⟨yi,yj⟩−⟨xi,xj⟩ ⏐⏐2 = argmin W∈RC′×C ∑ i,j∈[N] Tr2 ([( aia⊤ j ) ⊗ ( W⊤W ) − ( eie⊤ j ) ⊗IC ] vec(X) vec(X)⊤ ) . (9) (10) 4However, minimizing objective Eq. 10 will involve data matrix into the solution, which limits computational efﬁciency and generalization. Hence, we optimize a looser upper bound of Eq. 10 by minimizing the difference between (aia⊤ j ) ⊗(W⊤W and (eie⊤ j ) ⊗IC. The intuition is that once this difference reaches zero, Eq. 10 will be minimized to zero as well. W∗= argmin W∈RC′×C ∑ i,j∈[N]  ( aia⊤ j ) ⊗ ( W⊤W ) − ( eie⊤ j ) ⊗IC  2 F . (11) At the ﬁrst glance, Eq. 19 is non-convex and Kronecker product will produce a computational prohibitive matrix, which makes this problem intractable. However, it has not escaped our notice that the structure of aiaj is highly sparse. Hence, the entire norm minimization can be decomposed into block-wise minimization problems. We defer the detailed derivation into Appendix A. As the consequence, the optimal solution to Eq. 19 should satisfy: ∥wk∥2 2 = N2 ∑ i,j∈[N](di + 1)(dj + 1) ∀k∈[C], w⊤ k wl = 0 ∀k,l ∈[C],k ̸= l, (12) (13) where wk denotes the k-th column of optimal weight matrix W∗. Eq. 13 suggests that isometry requires weights for different channels to be orthogonal, which conforms with the general results of [43]. But different from [ 43], our results indicate that the magnitude of channel-wise weights should be constrained by a degree-related constant (Eq. 12), which binds the initial weights with topological information. To randomly initialize W in practice, one can draw each column ofW from independent distributions (e.g., white Gaussian) with varianceΣ2 = N2/ [ C′∑ i,j(di + 1)(dj + 1) ] and to initialize weights with bounded values, we suggest employ the following uniform distribution: W ∼U  − √ 3N2 C′ (∑ i,j∈[N](di + 1)(dj + 1) ), √ 3N2 C′ (∑ i,j∈[N](di + 1)(dj + 1) )   (14) 2.4 Gradient-Guided Dynamic Rewiring Graph Convolutions can be considered as a type of Laplacian smoothing, and repeatedly ap- plying Laplacian smoothing many times in the case of multi-layer GCNs, leads the representa- tions of the nodes in GCN to converge to a certain value and thus become indistinguishable, i.e. lose expressiveness and trainability [20, 46]. Despite numerous efforts from architectural, regular- ization, and normalization perspectives [23, 26, 27, 24, 20, 25, 28, 29], there still exists a wide gap in fully understanding the trainability issue of deep GCNs which can aid in developing techniques which can prevent performance deterioration with increasing depth. In this work, we look for the effect of increasing depth on the error signal propagation during training and found that deep GCNs suffer from poor gradient ﬂow and it worsens dramatically with increasing depth, thereby completely blocking the gradient propagation and potentially leading to a catastrophic failure to update during training. More speciﬁcally, we performed gradient ﬂow analysis during training of deep vanilla-GCNs (Figure 4 (a)) and found that with the progress in training, many deep GCN layers receive almost zero error signal during backpropagation (i.e. gradients) and unable to train. Motivated by this ﬁnding, we use computationally efﬁcient Gradient Flow as a metric to identify layers that block healthy error signal back-propagation during training and propose to dynamically rewire the vanilla-GCNs using skip-connections (widely known to improve gradient ﬂow), which we call on-demand dynamic rewiring. Our dynamic rewiring technique improves gradient ﬂow, mitigate sudden feature collapse and signiﬁcantly helps in training deep vanilla-GCNs with high performance (Figure 1(b) and 4(c)). Mathematically, our dynamic rewiring can be written as: ˜X l t = Wl−1X(l−1) t (A + I) Xl t = 1 [GF(X(l) t ) <p ·GF(X(l) 0 )]αX(l−1) t + ˜X l t (15) (16) where, 1 [·] is an indicator function, the subscript t denotes the training epoch, the superscript l denotes the layer index, GF denotes gradient ﬂow, αdenotes skip information ratio, and pdenotes gradient ﬂow drop threshold. 5Precisely, we observe the Gradient Flow of each vanilla-GCN layer during training, and if the ﬂow drop by p% (hyperparameter) of its initial value at the start of training, we assist the layer with a skip-connection. We use a modiﬁed version of initial residual connection [47, 24], which we deﬁne as the output feature matrix of ﬁrst layer of vanilla-GCN (gives better performance than [47, 24]), to supplement the layers that quickly loses energy. Once the layers which are more prone to lose expressiveness are identiﬁed and rewired using the skip-connections, the vanilla-GCN training can proceed as normal. In comparison with previous work [23, 24, 38, 20] which blindly inherits skip-connection techniques from CNNs/RNNs, our work provides a principled approach to perform architectural rewiringof vanilla-GCNs. Static skips introduced in [23, 24, 38, 20], by default incorporate skips for training shallow GCNs with 2-3 layers (which do not require it), and tend to hurt the ﬁnal performance. For example, in Table 3, it can be observed that training a 2-layered GCN with initial, jumping, or residual skips suffers -2.1%, -0.12%, -6.37% performance drop compared to a plain vanilla-GCN (81.10%) on Cora. Our method only introduces skip-connections when it is required by tracking the gradient ﬂow of the layer, and thereby overcomes the issue of adding skips in shallow GCNs, and for layers that receive healthy gradient ﬂow. Our extensive experiments on multiple datasets reveal that our approach of guided rewiring not only brings beneﬁts on the efﬁciency front (memory overhead to store intermediate activations) but also comfortably outperforms all SOTA ad-hoc skip-connection. 3 Experiment In this section, we ﬁrst provide experimental evidence to augment our signal propagation hypothesis and show that our newly proposed methods facilitate healthy gradient ﬂow during the training of deep-vanilla GCNs. Next, we extensively evaluate our methods against state-of-the-art graph neural network models and techniques to improve vanilla-GCNs on on a wide variety of open graph datasets. Settings Cora Citeseer Pubmed OGBN-ArXiv {Learning rate, Weight Decay, Hidden dimesnion} {0.005, 5e − 4, 64} {0.005, 5e − 4, 64} {0.01, 5e − 4, 64} {0.005, 0, 256} Table 1: Hyperparameter conﬁguration for our proposed method on representative datasets. 3.1 Dataset and Experimental Setup We use three standard citation network datasets Cora, Citeseer, and Pubmed [48] in GNN domain for evaluating our proposed methods against state-of-the-art GNN models and techniques. We have used the basic vanilla-GCN implementation in PyTorch provided by the authors of [1] to incorporate our proposed techniques and show their effectiveness in making traditional GCN comparable/better with SOTA. For our evaluation on Cora, Citeseer, Pubmed, and OGBN-ArXiv, we have closely followed the data split settings and metrics reported by the recent benchmark [49]. See details in Appendix C. For comparison with SOTA models, we have used JKNet [50], InceptionGCN [51], SGC [52], GAT [3], GCNII [24], and DAGNN [53]. We use Adam optimizer for our experiments and performed a grid search to tune hyperparameters for our proposed methods and reported our settings in Table 1. For all our experiments, we have trained our modiﬁed GCNs for 1500 epochs and 100 independent repetitions following [49] and reported average performances with the standard deviations of the node classiﬁcation accuracies. All experiments on large graph datasets, e.g., OGBN-ArXiv, are conducted on single 48G Quadro RTX 8000 GPU, while small graph experiments are completed using a single 16G RTX 5000 GPU. 3.2 Gradient Flow and our proposed methods Despite signiﬁcant efforts to improve deep neural networks (CNNs/RNNs) training from the signal propagation perspective, in-depth analysis of deep GCNs training with a focus on the gradient ﬂow during back-propagation is highly overlooked. We use Equation 4 to study gradient ﬂow during the training of GCNs and Figure 2 indicates that with an increase in depth, gradient ﬂow in the network drop signiﬁcantly hurting the trainability of GCNs. Figure 3 indicates the relation between gradient ﬂow and drop in validation loss (performance) for 2-layer and 10-layer GCNs for Cora, Citeseer, and Pubmed. It can be observed that across all datasets, 10-layer GCN has poor gradient ﬂow compared to 2-layer GCN and we observed a negligible drop in validation loss during training. 6Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Vanilla-GCN 0.0 0.5 1.0 Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Vanilla-GCN + Our Init 0.0 0.5 1.0 Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Vanilla-GCN + Gradient Rewiring 0.0 0.5 1.0 Figure 4: Visualization of the gradient ﬂow of layers during the training of 10-layers vanilla GCN, vanilla-GCN with our new topology-aware initialization and dynamic DE-based rewiring for 500 epochs on Cora. Our methods promote uniform gradient ﬂow across all layers of vanilla-GCNs. To better understand the behavior of each hidden layer in deep GCN training, we estimated the gradient ﬂow per layer independently of a 10-layer GCN. To our surprise, we found that many layers receive zero error signals (gradients)under backpropagation, potentially leading to a catastrophic failure to update during training. Figure 4(a) illustrate layer-wise analysis of gradient ﬂow in 10-layer vanilla-GCN on Cora. We observed that within a few epochs of training, the gradient ﬂow saturates, and then onwards, many layers of GCN receive zero error signal during the backpropagation and they fail to update leading to no drop in validation loss. Dataset Settings Layer Number 2 3 4 5 6 7 8 9 10 11 12 Cora GCN 81.1 81.9 80.4 79.9 77.5 77.1 69.5 28.2 27.4 30.1 25.4 GCN + Our Init 83.0 83.4 80.6 80.0 81.2 80.6 80.4 79.9 80.1 79.2 78.5 CiteSeer GCN 71.4 67.6 64.6 63.7 63.9 24.1 20.8 22.3 23.2 22.9 21.6 GCN + Our Init 71.7 78.2 68.3 66.1 66.2 63.3 62.8 63.2 62.9 59.8 61.9 PubMed GCN 79.0 78.9 76.5 76.7 77.1 76.5 61.2 41.8 40.7 43.2 41.0 GCN + Our Init 79.3 80.0 78.5 77.6 77.6 76.7 75.8 76.9 76.2 76.3 75.9 Table 2: Performance Comparison (test accuracy %) of of vanilla-GCNs [1] with and without our newly proposed topology-aware isometric initialization. Experiments are conducted on Cora, Citeseer, and PubMed using vanilla-GCNs with layer l∈{2,3,..., 12}. Initialization of neural networks is tightly coupled with the propagation of error signals [33] and effect their trainability. Our newly proposed topology-aware isometric initialization and gradient-guided rewiring signiﬁcantly improve gradient ﬂow (Figure 1(a) and (b)) as well as mitigate the issue of feature collapse of layers in deep vanilla-GCNs. In Figure 4(b) and (c), the detailed layer-wise analysis of vanilla-GCN with our newly proposed methods clearly illustrate how our methods are able to uniformly restore healthy gradients across all layers leading to improved trainability and signiﬁcant performance gains. Category Settings Cora Citeseer PubMed 2 16 32 2 16 32 2 16 32 Vanilla-GCN - 81.10 21.49 21.22 71.46 19.59 20.29 79.76 39.14 38.77 Skip Residual 74.73 20.05 19.57 66.83 20.77 20.90 75.27 38.84 38.74 Connection Initial 79.00 78.61 78.74 70.15 68.41 68.36 77.92 77.52 78.18 Jumping 80.98 76.04 75.57 69.33 58.38 55.03 77.83 75.62 75.36 Dense 77.86 69.61 67.26 66.18 49.33 41.48 72.53 69.91 62.99 Normalization BatchNorm 69.91 61.20 29.05 46.27 26.25 21.82 67.15 58.00 55.98 PairNorm 74.43 55.75 17.67 63.26 27.45 20.67 75.67 71.30 61.54 NodeNorm 79.87 21.46 21.48 68.96 18.81 19.03 78.14 40.92 40.93 CombNorm 80.00 55.64 21.44 68.59 18.90 18.53 78.11 40.93 40.90 Random Dropping DropNode 77.10 27.61 27.65 69.38 21.83 22.18 77.39 40.31 40.38 DropEdge 79.16 28.00 27.87 70.26 22.92 22.92 78.58 40.61 40.50 LADIES 77.12 28.07 27.54 68.87 22.52 22.60 78.31 40.07 40.11 Identity Mapping - 82.98 67.23 40.57 68.25 56.39 35.28 79.09 79.55 73.74 Gradient-Guided Rewiring (Ours) 82.79 80.19 80.01 71.06 68.54 68.49 78.90 78.32 78.51 ±0.16 ±0.32 ±0.15 ±0.21 ±0.11 ±0.31 ±0.20 ±0.19 ±0.27 Table 3: Performance Comparison of Gradient-guided Rewiring with respect to various proposed fancy techniques to improve the GCN training. Note that our results are generated using vanilla-GCN [1]. Experiments are conducted on Cora, Citeseer, and PubMed with 2/16/32 layers GCN. 73.3 Vanilla-GCNs and our proposed methods In this section, we conduct a systematic study to understand the performance gain by improving gradient ﬂow using our proposed methods: topology-aware isometric initialization and Gradient Guided Dynamic Rewiring, by incorporating them into the training process of deep vanilla-GCNs. Table 2 demonstrate the performance comparison of vanilla-GCNs [ 1] with and without our new initialization method with an increase in depth. Our experiments on Cora, Citeseer, and PubMed using vanilla-GCNs with layers l∈{2,3,..., 12}illustrate how our new initialization has been successful in retaining the trainability of vanilla-GCNs with increasing depth along with improving performance at shallow depths. The highlighted (red) box represents the depths at which vanilla-GCNs lose trainability, i.e, validation loss shows no improvement during training. Trainability issue of deep GCNs has been extensively studied from the perspective of over- smoothening [20, 21] and information bottleneck [22] and many approaches broadly categorized as architectural tweaks [23, 24, 20, 25], and regularization & normalization [26, 27, 28, 29] has been proposed for mitigation. We defer their detailed description to Appendix B. Table 3 demonstrates the performance comparison of our gradient-guided dynamic rewiring using skip-connections with re- spect to various state-of-the-art techniques to improve deep GCNs training. It can be clearly observed that our on-demand dynamic rewiring method of introducing new skip-connections signiﬁcantly outperforms all fancy normalization and regularization techniques as well as comfortably beats ad-hoc skip-connections techniques. Method Cora PubMed 2 4 8 16 32 64 2 4 8 16 32 64 Vanilla-GCN [1] 81.1 80.4 69.5 21.5 21.2 21.9 79.0 76.5 61.2 39.1 38.7 35.3 GAT [3] 81.9 80.3 31.3 30.5 27.1 27.9 78.4 77.4 29.1 26.3 28.7 25.0 JKNet [50] 79.1 79.2 75.0 72.9 73.2 71.5 77.8 68.7 67.7 69.8 68.2 63.4 SGC [52] 79.3 79.0 77.2 75.9 68.5 65.3 78.0 73.1 70.9 69.8 66.6 63.2 InceptionGCN [51] 79.2 77.6 76.5 81.7 81.7 80.0 78.5 77.7 77.9 74.9 74.1 74.3 GCNII [24] 82.2 82.6 84.2 84.6 85.4 85.5 78.2 78.8 79.3 80.2 79.8 79.7 Ours 83.1 82.8 82.4 82.3 82.1 80.4 80.2 79.6 79.8 79.5 79.9 79.7 (std: ±) 0.25 0.23 0.31 0.24 0.30 0.22 0.16 0.29 0.11 0.08 0.17 0.24 Table 4: Performance Comparison (test accuracy %) of our proposed method (topology-aware initial- ization and gradient-guided rewiring) with other previous SOTA frameworks on Cora & Pubmed. 3.4 Comparison with state-of-the-art methods To further validate the effectiveness improving gradient ﬂow in vanilla-GCNs, we perform compar- isons with previous state-of-the-art frameworks, including SGC[52], GAT[3], JKNet[50], APPNP[47], InceptionGCN[51], GPRGNN[54], and GCNII[24]. We ﬁrst observe that Gradient guided rewiring and Topology-aware isometric initialization brings orthogonal beneﬁts and combining them helps vanilla-GCNs to achieve better performance. Table 4 reports the mean classiﬁcation accuracy and the standard deviation on the test set of Cora and Pubmed of our methods using vanilla-GCNs with layer l∈{2,4,8,16,32,64}. One key beneﬁt to note is that with the help of our methods, we can train vanilla-GCNs as deep as 64 layers. It can be observed that vanilla-GCNs with our methods outperform all SOTA methods on Cora (layers 2 and 4) and Pubmed (layers 2, 4, 8, and 64), while holding the second position for all other layers. To evaluate on a large graph, we have chosen OGBN-AxRiv dataset, and Table 5 represents the performance comparison of vanilla-GCNs with our method against SOTA baselines. It can be observed that with the help of our methods, vanilla-GCNs can be efﬁciently trained deeper without any signiﬁcant loss in performance. Very similar to Cora and Pubmed, our methods beats all SOTA methods while training 2-layer vanilla-GCNs and holds second position for all other layers. It is interesting to note that for OGBN-AxRiv dataset, vanilla-GCNs with our proposed method consistently have performance improvement with an increase in depth possibly because of being able to capture long-term information effectively, which is in contrast to most of the SOTA methods whose performance suffers with increased in depth. Note that we are NOT trying to beat SOTA, but instead, our objective is to reveal the correct initialization and training scheme for vanilla-GCNs, and scale deep efﬁciently. The last sanity check is whether our proposed methods can make deep vanilla-GCNs effective across multiple different graph datasets. Specially, we evaluate it on seven other open-source graph datasets: (i) one Co-author datasets [55] (CS), (ii) two Amazon datasets [55] (Computers and Photo), (iii) three 8Layer GCN[1] SGC[52] DAGNN [25] GCNII[24] JKNet[50] APPNP[47] GPRGNN[54] Ours 2 69.46±0.22 61.98 ±0.08 67.65 ±0.52 71.24±0.17 63.73±0.38 65.31 ±0.23 69.31 ±0.09 71.59±0.08 16 67.96±0.38 41.58 ±0.27 71.82±0.28 72.61 ±0.29 66.41±0.56 66.95 ±0.24 70.30 ±0.15 71.76 ±0.03 32 45.46±4.50 34.22 ±0.04 71.46 ±0.27 72.60±0.25 66.31±0.63 66.94 ±0.26 70.18 ±0.16 72.03±0.55 64 38.40±7.63 36.17 ±2.11 70.22 ±1.54 72.13±0.79 62.97±3.81 65.54 ±1.74 70.59 ±2.60 72.28±0.92 Table 5: Performance Comparison (test accuracy %) of our proposed method (topology-aware initialization and Gradient-guided rewiring) with previous SOTA frameworks using OGBN-AxRiv. WebKB datasets [56] (Texas, Wisconsin, Cornell), and (iv) the Actor dataset [56]. Table 6 reports the performance comparison of deep vanilla-GCN with 32-layers with state-of-the-art methods having the same depth. Our methods universally encourage signiﬁcant trainability beneﬁts to deep vanilla-GCNs across all datasets achieving state-of-the-art performance on Computers dataset while performing in top-2 for all remaining datasets. Category CS [55] Computers [55] Photo [55] Texas [56] Winconsin [56] Cornell [56] Actors [56] GCN [1] 24.01 ±3.42 58.72 ±4.97 58.64 ±8.40 60.12 ±4.22 52.94 ±3.99 54.05 ±7.11 25.46 ±1.43 GAT [3] 11.04 ±0.94 9.42 ±0.54 17.11 ±1.02 11.54 ±0.72 14.01 ±0.88 19.78 ±1.42 6.42 ±0.39 SGC [52] 70.52 ±3.96 37.53 ±0.20 26.60 ±4.64 56.41 ±4.25 51.29 ±6.44 58.57 ±3.44 26.17 ±1.15 GCNII [24] 71.67 ±2.68 37.56 ±0.43 62.95 ±9.41 69.19±6.56 70.31 ±4.75 74.16 ±6.48 34.28 ±1.12 JKNet [50] 81.82 ±3.32 67.99±5.07 78.42 ±6.95 61.08±6.23 52.76 ±5.69 57.30 ±4.95 28.80 ±0.97 APPNP [47] 91.61±0.49 43.02±10.16 59.62 ±23.27 60.68 ±4.50 54.24 ±5.94 58.43 ±3.74 28.65 ±1.28 Ours 89.33 ±2.10 77.18 ±1.72 72.77 ±2.27 64.28 ±2.93 59.19 ±9.07 58.51 ±1.66 30.95 ±1.04 Table 6: Transfer studies of our proposed method (topology-aware initialization and gradient-guided rewiring) with deep vanilla-GCNs (32-layers). Comparisons are conducted on seven open-source widely adopted datasets with other previous state-of-the-art frameworks. 3.5 Dirichlet Energy-based analysis of our proposed methods In this section, we use Dirichlet Energy to illustrate how our proposed techniques help in mitigating the issue of losing expressiveness, enable deep GNNs to leverage the high-order neighbors. Node pair distance has been widely adopted to quantify the embedding similarities, and Dirichlet energy is simple and expressive metric to estimate the expressiveness of node embeddings learned by GCNs, in a topology-aware fashion [46, 57]. Following [46], we deﬁne Dirichlet Energy as: Deﬁnition 2. [46] Dirichlet energy E(x) of a scalar function x ∈RN on the graph Gis deﬁned as: E(x) = xT Lx = 1 2 ∑ (i,j)∈E ( xi (1 + di)1/2 − xj (1 + dj)1/2 )2 (17) For the vector ﬁeld X = [x1 ··· xN ] ∈RC×N , where xi ∈RC, Dirichlet energy is deﬁned as: E(X) = Tr(XLXT ) = 1 2 ∑ (i,j)∈E  xi (1 + di)1/2 − xj (1 + dj)1/2  2 2 (18) Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Cora + Vanilla-GCN 10 0 Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Citeseer + Vanilla-GCN 10 0 Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Pubmed + Vanilla-GCN 10 0 0 9 18 27 36 45 54 63 72 81 90 99 108 117 126 135 144 153 162 171 180 189 198 Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Cora + Our proposed Methods 10 0 0 9 18 27 36 45 54 63 72 81 90 99 108 117 126 135 144 153 162 171 180 189 198 Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Citeseer + Our proposed Methods 10 0 0 9 18 27 36 45 54 63 72 81 90 99 108 117 126 135 144 153 162 171 180 189 198 Epoch Layer 1 Layer 3 Layer 5 Layer 7 Layer 9 Pubmed + Our proposed Methods 10 0 Figure 5: Visualization of Dirichlet Energy of layers during the training of 10-layers vanilla GCN (row 1) and with our proposed methods on Cora, Pubmed, and Citeseer. Plotted using the log scale for better visualization. Our methods prevent Dirichlet Energy to drop to zero during training and maintain the expressiveness of feature embeddings. 9We track the Dirichlet energy of feature matrix w.r.t. the (augmented) normalized Laplacian at different layers of the vanilla-GCN during training and observed that during training of a deep vanilla-GCN, the Dirichlet energy of some layers decreases exponentially and becomes close to zero within a few epochs of training, i.e lose expressiveness (Figure 5 row 1). Our newly proposed methods: topology-aware isometric initialization and gradient-guided rewiring signiﬁcantly improve gradient ﬂow, as well as mitigate the issue of exponentially dropping Dirichlet energy across all the layers of deep GCNs, thereby restoring the expressiveness of node embeddings (Figure 5 row 2). 4 Other Related Works GNNs have established state-of-the-art performance in numerous real-world applications [ 11, 12, 13, 11, 5, 4, 14, 15, 16, 12, 17, 18]. While deep architectures improve the representational power of neural networks [ 58, 59], not every useful GNN has to be deep. Many real-world graph are “small-world”[60], where a node can reach any other node in a few hops, hence a few layers would sufﬁce to provide global coverage. However, when the graph data has no small-world property, or the related task requires long-range information, then deeper GNNs become very necessary. Many works [20, 21] revealed that when we start stacking spatial aggregations recursively in GNNs, the node representations will collapse to indistinguishable vectors and it will hamper the training of deep GNNs. More generally, this phenomenon happens for any message-passing mechanism via stochastic matrices including attention [61]. Recently, there has been a series of techniques developed to handle the over-smoothing issue, which can be broadly categorized under skip connection, graph normalization, random dropping. Skip-connections [ 23, 24, 38, 20] have been applied to GNNs to exploit node embeddings from the preceding layers, to relieve the over-smoothing issue. Graph normalization techniques [ 28, 62, 63, 64, 65] re-scale node embeddings over an input graph to constrain pairwise node distance and thus alleviate over-smoothing. Dropout methods [66, 26, 27, 67] can be regarded as data augmentations, which help relieve both the over-ﬁtting and over-smoothing issues in training very deep GNNs. 5 Conclusion This paper makes an important step towards understanding the substandard performance of deep vanilla-GCNs from the signal propagation perspective and hypothesizes that by facilitating healthy gradient ﬂow, we can signiﬁcantly improve their trainability, as well as achieve state-of-the-art (SOTA) level performance or close using merely vanilla-GCNs. This paper derives a topology-aware isometric initialization scheme for vanilla-GCNs based on the principles of isometry. Additionally, this paper proposes to use Gradient Flow for the dynamic rewiring of vanilla-GCNs with skip-connections. Extensive experiments across multiple datasets illustrate that these methods improvise gradient ﬂow in deep vanilla-GCNs and signiﬁcantly boost their performance. An interesting direction for future work includes building theoretical relations between our proposed methods and gradient ﬂow. References [1] Thomas Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. ArXiv, abs/1609.02907, 2017. [2] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. Advances in neural information processing systems, 29, 2016. [3] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [4] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. L2-gcn: Layer-wise and learned efﬁcient training of graph convolutional networks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2124–2132, 2020. [5] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018. 10[6] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [7] Wenqing Zheng, Edward W Huang, Nikhil Rao, Sumeet Katariya, Zhangyang Wang, and Karthik Subbian. Cold brew: Distilling graph node representations with incomplete or missing neighborhoods. arXiv preprint arXiv:2111.04840, 2021. [8] Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [9] Keyu Duan, Zirui Liu, Peihao Wang, Wenqing Zheng, Kaixiong Zhou, Tianlong Chen, Xia Hu, and Zhangyang Wang. A comprehensive study on large-scale graph training: Benchmarking and rethinking. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. [10] Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018. [11] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 974–983, 2018. [12] Junyuan Shang, Cao Xiao, Tengfei Ma, Hongyan Li, and Jimeng Sun. Gamenet: Graph aug- mented memory networks for recommending medication combination. ArXiv, abs/1809.01852, 2019. [13] Lei Tang and Huan Liu. Relational learning via latent social dimensions. In KDD, 2009. [14] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help graph convolutional networks? Proceedings of machine learning research, 119:10871–10880, 2020. [15] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph convolutional networks. In NIPS, 2017. [16] Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue networks. Bioinformatics, 33:i190 – i198, 2017. [17] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris N. Metaxas. Semantic graph convolutional networks for 3d human pose regression.2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3420–3430, 2019. [18] Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan Loddon Yuille, and Daguang Xu. When radiology report generation meets knowledge graph. In AAAI, 2020. [19] Zhiwen Fan, Lingjie Zhu, Honghua Li, Xiaohao Chen, Siyu Zhu, and Ping Tan. Floorplancad: a large-scale cad drawing dataset for panoptic symbol spotting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10128–10137, 2021. [20] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI conference on artiﬁcial intelligence, 2018. [21] Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ﬁlters. ArXiv, abs/1905.09550, 2019. [22] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. ArXiv, abs/2006.05205, 2021. [23] Guohao Li, Matthias Müller, Ali K. Thabet, and Bernard Ghanem. Can gcns go as deep as cnns? ArXiv, abs/1904.03751, 2019. [24] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. ArXiv, abs/2007.02133, 2020. [25] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. Proceed- ings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. [26] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In ICLR, 2020. 11[27] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over- smoothing for general graph convolutional networks. ArXiv, abs/2008.09864, 2020. [28] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. ArXiv, abs/1909.12223, 2020. [29] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in deep graph convolutional networks. Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021. [30] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable beneﬁts of depth in training graph convolutional networks. Advances in Neural Information Processing Systems, 34:9936–9949, 2021. [31] Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Training matters: Unlocking potentials of deeper graph convolutional neural networks. arXiv preprint arXiv:2008.08838, 2020. [32] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In AISTATS, 2010. [33] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026–1034, 2015. [34] Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In NIPS, 2017. [35] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep infor- mation propagation. ArXiv, abs/1611.01232, 2017. [36] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 770–778, 2016. [37] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, 2017. [38] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. ArXiv, abs/1806.03536, 2018. [39] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. [40] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017. [41] Aliaksei Sandryhaila and José MF Moura. Discrete signal processing on graphs. IEEE transactions on signal processing, 61(7):1644–1656, 2013. [42] Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, and Zhangyang Wang. Spending your winning lottery better after drawing it. arXiv preprint arXiv:2101.03255, 2021. [43] Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for visual recognition. In International Conference on Machine Learning, pages 7824–7835. PMLR, 2020. [44] Antonio Ortega, Pascal Frossard, Jelena Kovaˇcevi´c, José MF Moura, and Pierre Vandergheynst. Graph signal processing: Overview, challenges, and applications. Proceedings of the IEEE, 106(5):808–828, 2018. [45] Kathrin Schacke. On the kronecker product. Master’s thesis, University of Waterloo, 2004. [46] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318, 2020. [47] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. 12[48] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi- Rad. Collective classiﬁcation in network data articles. AI Magazine, 29:93–106, 09 2008. [49] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. arXiv preprint arXiv:2108.10521, 2021. [50] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, pages 5453–5462. PMLR, 2018. [51] Anees Kazi, Shayan Shekarforoush, S Arvind Krishna, Hendrik Burwinkel, Gerome Vivar, Karsten Kortüm, Seyed-Ahmad Ahmadi, Shadi Albarqouni, and Nassir Navab. Inceptiongcn: receptive ﬁeld aware graph convolutional network for disease prediction. In International Conference on Information Processing in Medical Imaging, pages 73–85. Springer, 2019. [52] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861–6871. PMLR, 2019. [53] Liqi Yang, Linhan Luo, Lifeng Xin, Xiaofeng Zhang, and Xinni Zhang. Dagnn: Demand-aware graph neural networks for session-based recommendation. arXiv preprint arXiv:2105.14428, 2021. [54] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. arXiv preprint arXiv:2006.07988, 2020. [55] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. [56] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020. [57] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi, and Xia Hu. Dirichlet energy constrained learning for deep graph neural networks. Advances in Neural Information Processing Systems, 34:21834–21846, 2021. [58] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [59] Peihao Wang, Yuehao Wang, Hua Lin, and Jianbo Shi. Sogcn: Second-order graph convolutional networks. arXiv preprint arXiv:2110.07141, 2021. [60] Pablo Barceló, Egor V . Kostylev, Mikaël Monet, Jorge Pérez, Juan L. Reutter, and Juan Pablo Silva. The logical expressiveness of graph neural networks. In ICLR, 2020. [61] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. arXiv preprint arXiv:2203.05962, 2022. [62] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. [63] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. Advances in Neural Information Processing Systems, 33:4917–4928, 2020. [64] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. [65] Chaoqi Yang, Ruijie Wang, Shuochao Yao, Shengzhong Liu, and Tarek Abdelzaher. Revisiting over-smoothing in deep gcns. arXiv preprint arXiv:2003.13663, 2020. [66] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res., 15:1929–1958, 2014. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In NeurIPS, 2019. 13A Derivation of Eq. 12 and 13 Recall that, in Section 2.3 our goal is to make initialized GCN isometric. Instead of exactly compute such initialization by solving an optimization, we propose the following objective, which has a closed-form solution: W∗= argmin W∈RC′×C ∑ i,j∈[N]  ( aia⊤ j ) ⊗ ( W⊤W ) − ( eie⊤ j ) ⊗IC  2 F . (19) In this section, we give the detailed derivation to reach the closed-form solution. Notice that both aia⊤ j and eie⊤ j are sparsely structured. Let A(i,j) = aia⊤ j and E(i,j) = eie⊤ j , then we have: A(i,j) m,n = { 1 ( i,m) ∈E and (j,n) ∈E 0 Otherwise , E(i,j) m,n = { 1 m= iand n= j 0 Otherwise , (20) where A(i,j) m,n , E(i,j) m,n denotes the m-th row and the n-th column entry of A(i,j), E(i,j), respectively. Hence, for m,n ∈[N] such that (i,m) ∈E and (j,n) ∈E but m ̸= ior n ̸= j, the block-wise difference at location (m,n) remains W⊤W. While for m = iand n = j, the block difference is W⊤W −IC as A contains self-loop (i.e., (i,i) ∈E and (j,j) ∈E). Then we can simply our objective Eq. 19 as below: W∗= argmin W∈RC′×C ∑ i,j∈[N]   ∑ m,n∈[N] (m,n)̸=(i,j) A(i,j) m,n W⊤W  2 F + W⊤W −IC  2 F   = argmin W∈RC′×C ∑ i,j∈[N] (di + dj + didj) W⊤W  2 F + N2 W⊤W −IC  2 F . (21) (22) To solve Eq. 22, we decompose the diagonal and off-diagonal components from W⊤W. For off-diagonal terms, we can minimize them to zeros, thus we have w⊤ k wl = 0 , where we note that wk denotes the k-th column of W. For on-diagonal terms, it is equivalent to minimizing∑ i,j∈[N](di + dj + didj)∥wk∥4 2 + N2(∥wk∥2 2 −1)2 for every k ∈[C]. Then combining both arguments above, the optimal solution should satisfy: ∥wk∥2 2 = N2 ∑ i,j∈[N](di + 1)(dj + 1) ∀k∈[C], w⊤ k wl = 0 ∀k,l ∈[C],k ̸= l, (23) (24) which reaches our ﬁnal conclusion in Eq. 14. To compute this magnitude given a graph, one needs to ﬁrst compute the degree of each node, and plug them into our Eq. 12. The complexity to compute this value is as small as O(N2), which is signiﬁcantly lower than directly optimizing Eq. 10. B Description of Methods in Table 3 Table 3 presents a comparison of our Gradient-Guided Rewiring dynamic rewiring with respect to various fancy methods to improve the trainability of deep vanilla-GCNs [1] such as skip-connections, regularization. Our formal description of these methods are: B.1 Skip Connections Skip-connections [23, 24, 38, 20] helps in alleviating the problem of over-smoothing and signiﬁcantly improve the accuracy and training stability of deep GCNs. For a Llayer GCN, we can apply various type of skip-connections after certain graph convolutional layers with the current and preceding embeddings Xl, 0 ≤l≤L. We have compared our method with the following four representative types of skip connections: 1. Residual Connection: Xl = (1 −α) ·Xl + α·Xl−1 142. Initial Connection: Xl = (1 −α) ·Xl + α·X0 3. Dense Connection: Xl = COM({Xk,0 ≤k≤l}) 4. Jumping Connection: XL = COM({Xk,0 ≤k≤L}) where αis residual and initial connections is a hyperparameter to weight the contribution of a node features from the current layer land previous layers. Jumping connection is a simpliﬁed case of dense connection and it is only applied at the end of the whole forward propagation process to combine the node features from all previous layers. B.2 Graph Normalization Graph normalization [28, 62, 63, 64, 65] re-scale node embeddings over an input graph to constrain pairwise node distance and thus alleviate over-smoothing. Our investigated normalization mechanisms are formally depicted as follows: 1. BatchNorm: x:,j = γ·x:,j−E(x:,j) std(x:,j) + β 2. PairNorm: ˜xi = xi −1 n ∑n i=1 xi, PairNorm(xi; s) = s·˜xi ( 1 n ∑n i=1 ∥˜xi∥2 2)1/2 3. NodeNorm: NodeNorm(xi; p) = xi std(xi)1/p where x:,j ∈RN denotes the j-th row of X (the j-th channel), xi ∈RC denotes the i-th column of X (the i-th node features), sin PairNorm is a hperparameter controlling the average pair-wise variance and pin NodeNorm denotes the normalization order. C Dataset Details Table 7 provided provides the detailed properties and download links for all adopted datasets. We adopt the following benchmark datasets since i) they are widely applied to develop and evaluate GNN models, especially for deep GNNs studied in this paper; ii) they contain diverse graphs from small- scale to large-scale or from homogeneous to heterogeneous; iii) they are collected from different applications including citation network, social network, etc. Dataset Nodes Edges Features Classes Download Links Cora 2,708 5,429 1,433 7 https://github.com/kimiyoung/planetoid/raw/master/data Citeseer 3,327 4,732 3,703 6 https://github.com/kimiyoung/planetoid/raw/master/data PubMed 19,717 44,338 500 3 https://github.com/kimiyoung/planetoid/raw/master/data OGBN-ArXiv 169,343 1,166,243 128 40 https://ogb.stanford.edu/ CoauthorCS 18,333 81,894 6805 15 https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ Computers 13,381 245,778 767 10 https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ Photo 7,487 119,043 745 8 https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ Texas 183 309 1,703 5 https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master Wisconsin 183 499 1,703 5 https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master Cornell 183 295 1,703 5 https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master Actor 7,600 33,544 931 5 https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master Table 7: Graph datasets statistics and download links. 15",
      "meta_data": {
        "arxiv_id": "2210.08122v1",
        "authors": [
          "Ajay Jaiswal",
          "Peihao Wang",
          "Tianlong Chen",
          "Justin F. Rousseau",
          "Ying Ding",
          "Zhangyang Wang"
        ],
        "published_date": "2022-10-14T21:30:25Z",
        "pdf_url": "https://arxiv.org/pdf/2210.08122v1.pdf"
      }
    },
    {
      "title": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth",
      "abstract": "Graph Neural Networks (GNNs) have been studied through the lens of expressive\npower and generalization. However, their optimization properties are less well\nunderstood. We take the first step towards analyzing GNN training by studying\nthe gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that\ndespite the non-convexity of training, convergence to a global minimum at a\nlinear rate is guaranteed under mild assumptions that we validate on real-world\ngraphs. Second, we study what may affect the GNNs' training speed. Our results\nshow that the training of GNNs is implicitly accelerated by skip connections,\nmore depth, and/or a good label distribution. Empirical results confirm that\nour theoretical results for linearized GNNs align with the training behavior of\nnonlinear GNNs. Our results provide the first theoretical support for the\nsuccess of GNNs with skip connections in terms of optimization, and suggest\nthat deep GNNs with skip connections would be promising in practice.",
      "full_text": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Keyulu Xu * 1 Mozhi Zhang 2 Stefanie Jegelka 1 Kenji Kawaguchi * 3 Abstract Graph Neural Networks (GNNs) have been stud- ied through the lens of expressive power and gen- eralization. However, their optimization proper- ties are less well understood. We take the ﬁrst step towards analyzing GNN training by study- ing the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed under mild assumptions that we validate on real- world graphs. Second, we study what may affect the GNNs’ training speed. Our results show that the training of GNNs is implicitly accelerated by skip connections, more depth, and/or a good label distribution. Empirical results conﬁrm that our theoretical results for linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the ﬁrst theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest that deep GNNs with skip connections would be promising in practice. 1. Introduction Graph Neural Networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009) are an effective framework for learning with graphs. GNNs learn node representations on a graph by extracting high-level features not only from a node itself but also from a node’s surrounding subgraph. Speciﬁ- cally, the node representations are recursively aggregated and updated using neighbor representations (Merkwirth & Lengauer, 2005; Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Gilmer et al., 2017; Hamilton et al., 2017; Velickovic et al., 2018; Liao et al., 2020). Recently, there has been a surge of interest in studying the *Equal contribution 1Massachusetts Institute of Technology (MIT) 2The University of Maryland 3Harvard University. Corre- spondence to: Keyulu Xu <keyulu@mit.edu>, Kenji Kawaguchi <kkawaguchi@fas.harvard.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). theoretical aspects of GNNs to understand their success and limitations. Existing works have studied GNNs’ expressive power (Keriven & Peyr´e, 2019; Maron et al., 2019; Chen et al., 2019; Xu et al., 2019; Sato et al., 2019; Loukas, 2020), generalization capability (Scarselli et al., 2018; Du et al., 2019b; Xu et al., 2020; Garg et al., 2020), and extrapolation properties (Xu et al., 2021). However, the understanding of the optimization properties of GNNs has remained lim- ited. For example, researchers working on the fundamental problem of designing more expressive GNNs hope and of- ten empirically observe that more powerful GNNs better ﬁt the training set (Xu et al., 2019; Sato et al., 2020; Vignac et al., 2020). Theoretically, given the non-convexity of GNN training, it is still an open question whether better represen- tational power always translates into smaller training loss. This motivates the more general questions: Can gradient descent ﬁnd a global minimum for GNNs? What affects the speed of convergence in training? In this work, we take an initial step towards answering the questions above by analyzing the trajectory of gradient de- scent, i.e., gradient dynamics or optimization dynamics. A complete understanding of the dynamics of GNNs, and deep learning in general, is challenging. Following prior works on gradient dynamics (Saxe et al., 2014; Arora et al., 2019a; Bartlett et al., 2019), we consider the linearized regime, i.e., GNNs with linear activation. Despite the linearity, key prop- erties of nonlinear GNNs are present: The objective function is non-convex and the dynamics are nonlinear (Saxe et al., 2014; Kawaguchi, 2016). Moreover, we observe the learn- ing curves of linear GNNs and ReLU GNNs are surprisingly similar, both converging to nearly zero training loss at the same linear rate (Figure 1). Similarly, prior works report comparable performance in node classiﬁcation benchmarks even if we remove the non-linearities (Thekumparampil et al., 2018; Wu et al., 2019). Hence, understanding the dynamics of linearized GNNs is a valuable step towards understanding the general GNNs. Our analysis leads to an afﬁrmative answer to the ﬁrst ques- tion. We establish that gradient descent training of a lin- earized GNN with squared loss converges to a global mini- mum at a linear rate. Experiments conﬁrm that the assump- arXiv:2105.04550v2  [cs.LG]  26 May 2021Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 3000 7000 10000 Iteration 10 3 10 2 10 1 100 Training Loss linear GIN ReLU GIN 0 3000 7000 10000 Iteration 10 2 10 1 100 Training Loss linear GCN ReLU GCN Figure 1.Training curves of linearized GNNs vs. ReLU GNNs on the Cora node classiﬁcation dataset. tions of our theoretical results for global convergence hold on real-world datasets. The most signiﬁcant contribution of our convergence analysis is on multiscale GNNs, i.e., GNN architectures that use skip connections to combine graph features at various scales (Xu et al., 2018; Li et al., 2019; Abu-El-Haija et al., 2020; Chen et al., 2020; Li et al., 2020). The skip connections introduce complex interactions among layers, and thus the resulting dynamics are more intricate. To our knowledge, our results are the ﬁrst convergence re- sults for GNNs with more than one hidden layer, with or without skip connections. We then study what may affect the training speed of GNNs. First, for any ﬁxed depth, GNNs with skip connections train faster. Second, increasing the depth further accelerates the training of GNNs. Third, faster training is obtained when the labels are more correlated with the graph features, i.e., labels contain “signal” instead of “noise”. Overall, experiments for nonlinear GNNs agree with the prediction of our theory for linearized GNNs. Our results provide the ﬁrst theoretical justiﬁcation for the empirical success of multiscale GNNs in terms of optimiza- tion, and suggest that deeper GNNs with skip connections may be promising in practice. In the GNN literature, skip connections are initially motivated by the “over-smoothing” problem (Xu et al., 2018): via the recursive neighbor aggre- gation, node representations of a deep GNN on expander- like subgraphs would be mixing features from almost the entire graph, and may thereby “wash out” relevant local in- formation. In this case, shallow GNNs may perform better. Multiscale GNNs with skip connections can combine and adapt to the graph features at various scales, i.e., the out- put of intermediate GNN layers, and such architectures are shown to help with this over-smoothing problem (Xu et al., 2018; Li et al., 2019; 2020; Abu-El-Haija et al., 2020; Chen et al., 2020). However, the properties of multiscale GNNs have mostly been understood at a conceptual level. Xu et al. (2018) relate the learned representations to random walk distributions and Oono & Suzuki (2020) take a boosting view, but they do not consider the optimization dynamics. We give an explanation from the lens of optimization. The training losses of deeper GNNs may be worse due to over- smoothing. In contrast, multiscale GNNs can express any shallower GNNs and fully exploit the power by converging to a global minimum. Hence, our results suggest that deeper GNNs with skip connections are guaranteed to train faster with smaller training losses. We present our results on global convergence in Section 3, after introducing relevant background (Section 2). In Sec- tion 4, we compare the training speed of GNNs as a function of skip connections, depth, and the label distribution. All proofs are deferred to the Appendix. 2. Preliminaries 2.1. Notation and Background We begin by introducing our notation. Let G= (V,E) be a graph with nvertices V = {v1,v2,··· ,vn}. Its adjacency matrix A ∈Rn×n has entries Aij = 1 if (vi,vj) ∈E and 0 otherwise. The degree matrix associated with Ais D = diag (d1,d2,...,d n) with di = ∑ n j=1 Aij. For any matrix M ∈Rm×m′ , we denote its j-th column vector by M∗j ∈Rm, its i-th row vector by Mi∗ ∈Rm′ , and its largest and smallest (i.e., min(m,m′)-th largest) singular values by σmax(M) and σmin(M), respectively. The data matrix X ∈Rmx×n has columns X∗j corresponding to the feature vector of node vj, with input dimension mx. The task of interest is node classiﬁcation or regression. Each node vi ∈V has an associated label yi ∈Rmy . In the transductive (semi-supervised) setting, we have access to training labels for only a subset I⊆ [n] of nodes on G, and the goal is to predict the labels for the other nodes in [n] \\I. Our problem formulation easily extends to the inductive setting by letting I= [n], and we can use the trained model for prediction on unseen graphs. Hence, we have access to ¯n= |I|≤ ntraining labels Y = [yi]i∈I∈Rmy×¯n, and we train the GNN using X,Y,G . Additionally, for any M ∈ Rm×m′ , Imay index sub-matrices M∗I = [ M∗i]i∈I ∈ Rm×¯n (when m′ ≥n) and MI∗ = [ Mi∗]i∈I ∈R¯n×m (when m≥n). Graph Neural Networks (GNNs) use the graph structure and node features to learn representations of nodes (Scarselli et al., 2009). GNNs maintain hidden representations hv (l) ∈ Rml for each node v, where ml is the hidden dimension on the l-th layer. We let X(l) = [ h1 (l),h2 (l),··· ,hn (l) ] ∈ Rml×n, and set X(0) as the input features X. The node hidden representations X(l) are updated by aggregating and transforming the neighbor representations: X(l) = σ ( B(l)X(l−1)S ) ∈Rml×n, (1) where σis a nonlinearity such as ReLU, B(l) ∈Rml×ml−1 is the weight matrix, and S ∈Rn×n is the GNN aggrega- tion matrix, whose formula depends on the exact variant of GNN. In Graph Isomorphism Networks (GIN) (Xu et al.,Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Cora CiteSeer 10 20 10 13 10 5 103 GCN GIN (a) Graph σ2 min(X(SH)∗I) 0 1500 3000 Iteration (T) 5.07 5.08T 1e 3  (b) Time-dependent λ(H) T 10 6  10 5  10 4  10 3  10 2 T  (c) limT→∞λ(H) T across training settings Figure 2.Empirical validation of assumptions for global convergence of linear GNNs. Left panel conﬁrms the graph condition σ2 min(X(SH)∗I) >0 for datasets Cora and Citeseer, and for models GCN and GIN. Middle panel shows the time-dependent λ(H) T for one training setting (linear GCN on Cora). Each point in right panel is λ(H) T >0 at the last iteration for different training settings. 2019), S = A+ In is the adjacency matrix of Gwith self- loop, where In ∈Rn×n is an identity matrix. In Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), S = ˆD−1 2 (A+In) ˆD−1 2 is the normalized adjacency matrix, where ˆDis the degree matrix of A+ In. 2.2. Problem Setup We ﬁrst formally deﬁne linearized GNNs. Deﬁnition 1. (Linear GNN). Given data matrix X ∈ Rmx×n, aggregation matrix S ∈ Rn×n, weight matri- ces W ∈Rmy×mH , B(l) ∈Rml×ml−1, and their collec- tion B = (B(1),...,B (H)), a linear GNN with H layers f(X,W,B ) ∈Rmy×n is deﬁned as f(X,W,B ) = WX(H), X (l) = B(l)X(l−1)S. (2) Throughout this paper, we refer multiscale GNNs to the commonly used Jumping Knowledge Network (JK-Net) (Xu et al., 2018), which connects the output of all intermediate GNN layers to the ﬁnal layer with skip connections: Deﬁnition 2. (Multiscale linear GNN). Given data X ∈ Rmx×n, aggregation matrix S ∈ Rn×n, weight matri- ces W(l) ∈ Rmy×ml, B(l) ∈ Rml×ml−1 with W = (W(0),W(1),...,W (H)), a multiscale linear GNN with H layers f(X,W,B ) ∈Rmy×n is deﬁned as f(X,W,B ) = H∑ l=0 W(l)X(l), (3) X(l) = B(l)X(l−1)S. (4) Given a GNN f(·) and a loss function ℓ(·,Y ), we can train the GNN by minimizing the training loss L(W,B): L(W,B) = ℓ ( f(X,W,B )∗I,Y ) , (5) where f(X,W,B )∗Icorresponds to the GNN’s predictions on nodes that have training labels and thus incur training losses. The pair (W,B) represents the trainable weights: L(W,B) = L(W(1),...,W (H),B(1),...,B (H)) For completeness, we deﬁne the global minimum of GNNs. Deﬁnition 3. (Global minimum). For any H ∈N0, L∗ H is the global minimum value of the H-layer linear GNN f: L∗ H = inf W,B ℓ ( f(X,W,B )∗I,Y ) . (6) Similarly, we deﬁne L∗ 1:H as the global minimum value of the multiscale linear GNN f with H layers. We are ready to present our main results on global conver- gence for linear GNNs and multiscale linear GNNs. 3. Convergence Analysis In this section, we show that gradient descent training a linear GNN with squared loss, with or without skip connec- tions, converges linearly to a global minimum. Our condi- tions for global convergence hold on real-world datasets and provably hold under assumptions, e.g., initialization. In linearized GNNs, the loss L(W,B) is non-convex (and non-invex) despite the linearity. The graph aggregation S creates interaction among the data and poses additional chal- lenges in the analysis. We show a ﬁne-grained analysis of the GNN’s gradient dynamics can overcome these chal- lenges. Following previous works on gradient dynamics (Saxe et al., 2014; Huang & Yau, 2020; Ji & Telgarsky, 2020; Kawaguchi, 2021), we analyze the GNN learning process via the gradient ﬂow, i.e., gradient descent with inﬁnitesimal steps: ∀t≥0,the network weights evolve as d dtWt = −∂L ∂W(Wt,Bt), d dtBt = −∂L ∂B(Wt,Bt), (7) where (Wt,Bt) represents the trainable parameters at time twith initialization (W0,B0).Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 3.1. Linearized GNNs Theorem 1 states our result on global convergence for lin- earized GNNs without skip connections. Theorem 1. Let f be an H-layer linear GNN and ℓ(q,Y ) = ∥q −Y∥2 F where q,Y ∈Rmy×¯n. Then, for any T >0, L(WT,BT) −L∗ H (8) ≤(L(W0,B0) −L∗ H)e−4λ(H) T σ2 min(X(SH)∗I)T, where λ(H) T is the smallest eigenvalue λ(H) T := inft∈[0,T] λmin(( ¯B(1:H) t )⊤¯B(1:H) t ) and ¯B(1:l) := B(l)B(l−1) ···B(1) for any l ∈ {0,...,H } with ¯B(1:0) := I. Proof. (Sketch) We decompose the gradient dynamics into three components: the graph interaction, non-convex fac- tors, and convex factors. We then bound the effects of the graph interaction and non-convex factors through σ2 min(X(SH)∗I) and λmin(( ¯B(1:H) t )⊤¯B(1:H) t ) respectively. The complete proof is in Appendix A.1. Theorem 1 implies that convergence to a global minimum at a linear rate is guaranteed if σ2 min(X(SH)∗I) > 0 and λT > 0. The ﬁrst condition on the product of X and SH indexed by Ionly depends on the node features X and the GNN aggregation matrix S. It is satisﬁed if rank(X(SH)∗I) = min(mx,¯n), because σmin(X(SH)∗I) is the min(mx,¯n)-th largest singular value of X(SH)∗I∈ Rmx×¯n. The second condition λ(H) T >0 is time-dependent and requires a more careful treatment. Linear convergence is implied as long as λmin(( ¯B(1:H) t )⊤¯B(1:H) t ) ≥ϵ> 0 for all times tbefore stopping. Empirical validation of conditions. We verify both the graph condition σ2 min(X(SH)∗I) > 0 and the time- dependent condition λ(H) T > 0 for (discretized) T > 0. First, on the popular graph datasets, Cora and Cite- seer (Sen et al., 2008), and the GNN models, GCN (Kipf & Welling, 2017) and GIN (Xu et al., 2019), we have σ2 min(X(SH)∗I) > 0 (Figure 2a). Second, we train lin- ear GCN and GIN on Cora and Citeseer to plot an exam- ple of how the λ(H) T = inf t∈[0,T] λmin(( ¯B(1:H) t )⊤¯B(1:H) t ) changes with respect to time T (Figure 2b). We further con- ﬁrm that λ(H) T > 0 until convergence, limT→∞λ(H) T > 0 across different settings, e.g., datasets, depths, models (Fig- ure 2c). Our experiments use the squared loss, random initialization, learning rate 1e-4, and set the hidden dimen- sion to the input dimension (note that Theorem 1 assumes the hidden dimension is at least the input dimension). Fur- ther experimental details are in Appendix C. Along with Theorem 1, we conclude that linear GNNs converge linearly to a global minimum. Empirically, we indeed see both linear and ReLU GNNs converging at the same linear rate to nearly zero training loss in node classiﬁcation tasks (Figure 1). Guarantee via initialization. Besides the empirical ver- iﬁcation, we theoretically show that a good initialization guarantees the time-dependent condition λT > 0 for any T >0. Indeed, like other neural networks, GNNs do not converge to a global optimum with certain initializations: e.g., initializing all weights to zero leads to zero gradients and λ(H) T = 0 for all T, and hence no learning. We intro- duce a notion of singular margin and say an initialization is good if it has a positive singular margin. Intuitively, a good initialization starts with an already small loss. Deﬁnition 4. (Singular margin). The initialization (W0,B0) is said to have singular margin γ >0 with respect to a layer l∈{1,...,H }if σmin(B(l)B(l−1) ···B(1)) ≥γ for all (W,B) such that L(W,B) ≤L(W0,B0). Proposition 1 then states that an initialization with positive singular margin γguarantees λ(H) T ≥γ2 >0 for all T: Proposition 1. Let f be a linear GNN with H layers and ℓ(q,Y ) = ∥q −Y∥2 F. If the initialization (W0,B0) has singular margin γ > 0 with respect to the layer H and mH ≥mx, then λ(H) T ≥γ2 for all T ∈[0,∞). Proposition 1 follows since L(Wt,Bt) is non-increasing with respect to time t(proof in Appendix A.2). Relating to previous works, our singular margin is a general- ized variant of the deﬁciency margin of linear feedforward networks (Arora et al., 2019a, Deﬁnition 2 and Theorem 1): Proposition 2. (Informal) If initialization (W0,B0) has deﬁciency margin c> 0, then it has singular margin γ >0. The formal version of Proposition 2 is in Appendix A.3. To summarize, Theorem 1 along with Proposition 1 implies that we have a prior guarantee of linear convergence to a global minimum for any graph with rank(X(SH)∗I) = min(mx,¯n) and initialization (W0,B0) with singular mar- gin γ > 0: i.e., for any desired ϵ > 0, we have that L(WT,BT) −L∗ H ≤ϵfor any T such that T ≥ 1 4γ2σ2 min(X(SH)∗I) log L(A0,B0) −L∗ H ϵ . (9) While the margin condition theoretically guarantees linear convergence, empirically, we have already seen that the convergence conditions of across different training settings for widely used random initialization. Theorem 1 suggests that the convergence rate depends on a combination of data features X, the GNN architecture and graph structure via Sand H, the label distribution and initialization via λT. For example, GIN has better such con- stants than GCN on the Cora dataset with everything elseOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth held equal (Figure 2a). Indeed, in practice, GIN converges faster than GCN on Cora (Figure 1). In general, the com- putation and comparison of the rates given by Theorem 1 requires computation such as those in Figure 2. In Section 4, we will study an alternative way of comparing the speed of training by directly comparing the gradient dynamics. 3.2. Multiscale Linear GNNs Without skip connections, the GNNs under linearization still behave like linear feedforward networks with augmented graph features. With skip connections, the dynamics and analysis become much more intricate. The expressive power of multiscale linear GNNs changes signiﬁcantly as depth increases. Moreover, the skip connections create complex interactions among different layers and graph structures of various scales in the optimization dynamics. Theorem 2 states our convergence results for multiscale linear GNNs in three cases: (i) a general form; (ii) a weaker condition for boundary cases that uses λH′ T instead of λ1:H T ; (iii) a faster rate if we have monotonic expressive power as depth increases. Theorem 2. Let f be a multiscale linear GNN with H layers and ℓ(q,Y ) = ∥q−Y∥2 F where q,Y ∈Rmy×¯n. Let λ(1:H) T := min0≤l≤H λ(l) T . For any T >0, the following hold: (i) (General). Let GH := [X⊤,(XS)⊤,..., (XSH)⊤]⊤ ∈R(H+1)mx×n. Then L(WT,BT) −L∗ 1:H (10) ≤(L(W0,B0) −L∗ 1:H)e−4λ(1:H) T σ2 min((GH)∗I)T. (ii) (Boundary cases). For any H′∈{0,1,...,H }, L(WT,BT) −L∗ H′ (11) ≤(L(W0,B0) −L∗ H′)e−4λ(H′) T σ2 min(X(SH′ )∗I)T. (iii) (Monotonic expressive power). If there exist l,l′ ∈ {0,...,H }with l <l′such that L∗ l ≥L∗ l+1 ≥···≥ L∗ l′ or L∗ l ≤L∗ l+1 ≤···≤ L∗ l′, then L(WT,BT) −L∗ l′′ (12) ≤(L(W0,B0) −L∗ l′′)e−4 ∑l′ k=l λ(k) T σ2 min(X(Sk)∗I)T, where l′′= lif L∗ l ≥L∗ l+1 ≥···≥ L∗ l′, and l′′= l′if L∗ l ≤L∗ l+1 ≤···≤ L∗ l′. Proof. (Sketch) A key observation in our proof is that the interactions of different scales cancel out to point towards a speciﬁc direction in the gradient dynamics induced in a space of the loss value. The complete proof is in Ap- pendix A.4. Similar to Theorem 1 for linear GNNs, the most general form (i) of Theorem 2 implies that convergence to the global minimum value of the entire multiscale linear GNN L∗ 1:H at linear rate is guaranteed when σ2 min((GH)∗I) > 0 and λ(1:H) T >0. The graph condition σ2 min((GH)∗I) >0 is sat- isﬁed if rank((GH)∗I) = min(mx(H+ 1),¯n). The time- dependent condition λ(1:H) T >0 is guaranteed if the initial- ization (W0,B0) has singular margin γ >0 with respect to every layer (Proposition 3 is proved in Appendix A.5): Proposition 3. Let f be a multiscale linear GNN and ℓ(q,Y ) = ∥q−Y∥2 F. If the initialization (W0,B0) has sin- gular margin γ >0 with respect to every layer l∈[H] and ml ≥mx for l∈[H], then λ(1:H) T ≥γ2 for all T ∈[0,∞). We demonstrate that the conditions of Theorem 2 (i) hold for real-world datasets, suggesting in practice multiscale linear GNNs converge linearly to a global minimum. Empirical validation of conditions. On datasets Cora and Citeseer and for GNN models GCN and GIN, we conﬁrm that σ2 min((GH)∗I) > 0 (Figure 3a). Moreover, we train multiscale linear GCN and GIN on Cora and Citeseer to plot an example of how the λ(1:H) T changes with respect to time T (Figure 3b), and we conﬁrm that at convergence,λ(1:H) T > 0 across different settings (Figure 3c). Experimental details are in Appendix C. Boundary cases. Because the global minimum value of multiscale linear GNNs L∗ 1:H can be smaller than that of linear GNNs L∗ H, the conditions in Theorem 2(i) may some- times be stricter than those of Theorem 1. For example, in Theorem 2(i), we require λ(1:H) T := min0≤l≤H λ(l) T rather than λ(H) T to be positive. If λ(l) T = 0 for some l, then Theo- rem 2(i) will not guarantee convergence to L∗ 1:H. Although the boundary cases above did not occur on the tested real-world graphs (Figure 3), for theoretical interest, Theorem 2(ii) guarantees that in such cases, multiscale lin- ear GNNs still converge to a value no worse than the global minimum value of non-multiscale linear GNNs. For any intermediate layer H′, assuming σ2 min(X(SH′ )∗I) >0 and λ(H′) T >0, Theorem 2(ii) bounds the loss of the multiscale linear GNN L(WT,BT) at convergence by the global mini- mum value L∗ H′ of the corresponding linear GNN with H′ layers. Faster rate under monotonic expressive power.Theorem 2(iii) considers a special case that is likely in real graphs: the global minimum value of the non-multiscale linear GNN L∗ H′ is monotonic as H′increases. Then (iii) gives a faster rate than (ii) and linear GNNs. For example, if the globally optimal value decreases as linear GNNs get deeper. i.e., L∗ 0 ≥L∗ 1 ≥···≥ L∗ H, or vice versa, L∗ 0 ≤L∗ 1 ≤···≤Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Cora CiteSeer 10 20 10 13 10 5 103 GCN GIN (a) Graph σ2 min((GH)∗I) 0 1500 3000 Iteration (T) 4.9 5.0 5.1T 1e 3  (b) Time-dependent λ(1:H) T 10 6  10 5  10 4  10 3  10 2 T  (c) limT→∞λ(1:H) T across training settings Figure 3.Empirical validation of assumptions for global convergence of multiscale linear GNNs. Left panel conﬁrms the graph condition σ2 min((GH)∗I) > 0 for Cora and Citeseer, and for GCN and GIN. Middle panel shows the time-dependent λ(1:H) T for one training setting (multiscale linear GCN on Cora). Each point in right panel is λ(1:H) T >0 at the last iteration for different training settings. L∗ H, then Theorem 2 (i) implies that L(WT,BT) −L∗ l (13) ≤(L(W0,B0) −L∗ l)e−4 ∑H k=0 λ(k) T σ2 min(X(Sk)∗I)T, where l = 0 if L∗ 0 ≥L∗ 1 ≥ ··· ≥L∗ H, and l = H if L∗ 0 ≤L∗ 1 ≤···≤ L∗ H. Moreover, if the globally optimal value does not change with respect to the depth as L∗ 1:H = L∗ 1 = L∗ 2 = ··· = L∗ H, then we have L(WT,BT) −L∗ 1:H (14) ≤(L(W0,B0) −L∗ 1:H)e−4 ∑H k=0 λ(k) T σ2 min(X(Sk)∗I)T. We obtain a faster rate for multiscale linear GNNs than for linear GNNs, as e−4 ∑H k=0 λ(k) T σ2 min(X(Sk)∗I)T ≤ e−4λ(H) T σ2 min(X(SH)∗I)T. Interestingly, unlike linear GNNs, multiscale linear GNNs in this case do not require any condition on initialization to obtain a prior guarantee on global convergence since e−4 ∑H k=0 λ(k) T σ2 min(X(Sk)∗I)T ≤ e−4λ(0) T σ2 min(X(S0)∗I)T with λ(0) T = 1 and X(S0)∗I= X∗I. To summarize, we prove global convergence rates for multi- scale linear GNNs (Thm. 2(i)) and experimentally validate the conditions. Part (ii) addresses boundary cases where the conditions of Part (i) do not hold. Part (iii) gives faster rates assuming monotonic expressive power with respect to depth. So far, we have shown multiscale linear GNNs converge faster than linear GNNs in the case of (iii). Next, we compare the training speed for more general cases. 4. Implicit Acceleration In this section, we study how the skip connections, depth of GNN, and label distribution may affect the speed of training for GNNs. Similar to previous works (Arora et al., 2018), we compare the training speed by comparing the per step loss reduction d dtL(Wt,Bt) for arbitrary differentiable loss functions ℓ(·,Y ) : Rmy →R. Smaller d dtL(Wt,Bt) im- plies faster training. Loss reduction offers a complementary view to the convergence rates in Section 3, since it is instant and not an upper bound. We present an analytical form of the loss reduction d dtL(Wt,Bt) for linear GNNs and multiscale linear GNNs. The comparison of training speed then follows from our for- mula for d dtL(Wt,Bt). For better exposition, we ﬁrst intro- duce several notations. We let ¯B(l′:l) = B(l)B(l−1) ···B(l′) for all l′and lwhere ¯B(l′:l) = I if l′>l. We also deﬁne J(i,l),t := [ ¯B(1:i−1) t ⊗(W(l),t ¯B(i+1:l) t )⊤], F(l),t := [( ¯B(1:l) t )⊤¯B(1:l) t ⊗Imy ] ⪰0, Vt := ∂L(Wt,Bt) ∂ˆYt , where ˆYt := f(X,Wt,Bt)∗I. For any vector v ∈ Rm and positive semideﬁnite matrix M ∈ Rm×m, we use ∥v∥2 M := v⊤Mv.1 Intuitively, Vt represents the deriva- tive of the loss L(Wt,Bt) with respect to the model output ˆY = f(X,Wt,Bt)∗I. J(i,l),t and F(l),t represent matri- ces that describe how the errors are propagated through the weights of the networks. Theorem 3, proved in Appendix A.6, gives an analytical formula of loss reduction for linear GNNs and multiscale linear GNNs. Theorem 3. For any differentiable loss function q ↦→ ℓ(q,Y ), the following hold for any H ≥0 and t≥0: (i) (Non-multiscale) For f as in Deﬁnition 1: d dtL1(Wt,Bt) = − vec[Vt(X(SH)∗I)⊤]2 F(H),t (15) − H∑ i=1 J(i,H),tvec[Vt(X(SH)∗I)⊤]2 2 . 1We use this Mahalanobis norm notation for conciseness with- out assuming it to be a norm, since M may be low rank.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (a) Multiscale vs. non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (b) Depth. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (c) Signal vs. noise. Figure 4.Comparison of the training speed of GNNs. Left: Multiscale GNNs train faster than non-multiscale GNNs. Middle: Deeper GNNs train faster. Right: GNNs train faster when the labels have signals instead of random noise. The patterns above hold for both ReLU and linear GNNs. Additional results are in Appendix B. (ii) (Multiscale) For f as in Deﬁnition 2: d dtL2(Wt,Bt) = − H∑ l=0 vec[Vt(X(Sl)∗I)⊤]2 F(l),t (16) − H∑ i=1  H∑ l=i J(i,l),tvec[Vt(X(Sl)∗I)⊤]  2 2 . In what follows, we apply Theorem 3 to predict how differ- ent factors affect the training speed of GNNs. 4.1. Acceleration with Skip Connections We ﬁrst show that multiscale linear GNNs tend to achieve faster loss reduction d dtL2(Wt,Bt) compared to the corresponding linear GNN without skip connections, d dtL1(Wt,Bt). It follows from Theorem 3 that d dtL2(Wt,Bt) − d dtL1(Wt,Bt) (17) ≤− H−1∑ l=0 vec [ Vt(X(Sl)∗I)⊤]2 F(l),t , if ∑ H i=1(∥ai∥2 2 + 2 b⊤ i ai) ≥ 0, where ai =∑ H−1 l=i J(i,l),tvec[Vt(X(Sl)∗I)⊤], and bi = J(i,H),tvec[ Vt(X(SH)∗I)⊤]. The assumption of ∑ H i=1(∥ai∥2 2 +2b⊤ i ai) ≥0 is satisﬁed in various ways: for example, it is satisﬁed if the last layer’s term bi and the other layers’ terms ai are aligned as b⊤ i ai ≥0, or if the last layer’s term bi is dominated by the other layers’ termsai as 2∥bi∥2 ≤∥ai∥2. Then equation (17) shows that the multiscale linear GNN decreases the loss value with strictly many more negative terms, suggesting faster training. Empirically, we indeed observe that multiscale GNNs train faster (Figure 4a), both for (nonlinear) ReLU and linear GNNs. We verify this by training multiscale and non- multiscale, ReLU and linear GCNs on the Cora and Citeseer datasets with cross-entropy loss, learning rate 5e-5, and hidden dimension 32. Results are in Appendix B. 4.2. Acceleration with More Depth Our second ﬁnding is that deeper GNNs, with or without skip connections, train faster. For any differentiable loss function q↦→ℓ(q,Y ), Theorem 3 states that the loss of the multiscale linear GNN decreases as d dtL(Wt,Bt) = − H∑ l=0 vec[Vt(X(Sl)∗I)⊤]2 F(l),t    ≥0    further improvement as depthHincreases (18) − H∑ i=1  H∑ l=i J(i,l),tvec[Vt(X(Sl)∗I)⊤]  2 2 .    ≥0    further improvement as depthHincreases In equation (18), we can see that the multiscale linear GNN achieves faster loss reduction as depth H increases. A simi- lar argument applies to non-multiscale linear GNNs. Empirically too, deeper GNNs train faster (Figure 4b). Again, the acceleration applies to both (nonlinear) ReLU GNNs and linear GNNs. We verify this by training mul- tiscale and non-multiscale, ReLU and linear GCNs with 2, 4, and 6 layers on the Cora and Citeseer datasets with learning rate 5e-5, hidden dimension 32, and cross-entropy loss. Results are in Appendix B. 4.3. Label Distribution: Signal vs. Noise Finally, we study how the labels affect the training speed. For the loss reduction (15) and (16), we argue that the norm of Vt(X(Sl)∗I)⊤tends to be larger for labels Y that are more correlated with the graph features X(Sl)∗I, e.g., la- bels are signals instead of “noise”. Without loss of generality, we assume Y is normalized, e.g., one-hot labels. Here, Vt = ∂L(At,Bt) ∂ˆYt is the derivative of the loss with respect to the model output, e.g., Vt = 2( ˆYt −Y) for squared loss. If the rows of Y are random noise vectors,Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 1000 Iteration 0.00 0.01 0.02 signal first signal second noise first noise second Figure 5.The scale of the ﬁrst term dominates the second term of the loss reduction d dt L(Wt,Bt) for linear GNNs trained with the original labels vs. random labels on Cora. then so are the rows ofVt, and they are expected to get more orthogonal to the columns of (X(Sl)∗I)⊤as nincreases. In contrast, if the labels Y are highly correlated with the graph features (X(Sl)∗I)⊤, i.e., the labels have signal, then the norm of Vt(X(Sl)∗I)⊤will be larger, implying faster training. Our argument above focuses on the ﬁrst term of the loss reduction, ∥Vt(X(Sl)∗I)⊤∥2 F. We empiri- cally demonstrate that the scale of the second term,∑ H l=iJ(i,l),tvec [ Vt(X(Sl)∗I)⊤] 2 2 , is dominated by that of the ﬁrst term (Figure 5). Thus, we can expect GNNs to train faster with signals than noise. We train GNNs with the original labels of the dataset and random labels (i.e., selecting a class with uniform probabil- ity), respectively. The prediction of our theoretical analysis aligns with practice: training is much slower for random labels (Figure 4c). We verify this for mutliscale and non- multiscale, ReLU and linear GCNs on the Cora and Citseer datasets with learning rate 1e-4, hidden dimension 32, and cross-entropy loss. Results are in Appendix B. 5. Related Work Theoretical analysis of linearized networks. The theoret- ical study of neural networks with some linearized com- ponents has recently drawn much attention. Tremendous efforts have been made to understand linearfeedforward net- works, in terms of their loss landscape (Kawaguchi, 2016; Hardt & Ma, 2017; Laurent & Brecht, 2018) and optimiza- tion dynamics (Saxe et al., 2014; Arora et al., 2019a; Bartlett et al., 2019; Du & Hu, 2019; Zou et al., 2020). Recent works prove global convergence rates for deep linear networks un- der certain conditions (Bartlett et al., 2019; Du & Hu, 2019; Arora et al., 2019a; Zou et al., 2020). For example, Arora et al. (2019a) assume the data to be whitened. Zou et al. (2020) ﬁx the weights of certain layers during training. Our work is inspired by these works but differs in that our anal- ysis applies to all learnable weights and does not require these speciﬁc assumptions, and we study the more complex GNN architecture with skip connections. GNNs consider the interaction of graph structures via the recursive message passing, but such structured, locally varying interaction is not present in feedforward networks. Furthermore, linear feedforward networks, even with skip connections, have the same expressive power as shallow linear models, a crucial condition in previous proofs (Bartlett et al., 2019; Du & Hu, 2019; Arora et al., 2019a; Zou et al., 2020). In contrast, the expressive power of multiscale linear GNNs can change signiﬁcantly as depth increases. Accordingly, our proofs signiﬁcantly differ from previous studies. Another line of works studies the gradient dynamics of neu- ral networks in the neural tangent kernel (NTK) regime (Ja- cot et al., 2018; Li & Liang, 2018; Allen-Zhu et al., 2019; Arora et al., 2019b; Chizat et al., 2019; Du et al., 2019a;c; Kawaguchi & Huang, 2019; Nitanda & Suzuki, 2021). With over-parameterization, the NTK remains almost constant during training. Hence, the corresponding neural network is implicitly linearized with respect to random features of the NTK at initialization (Lee et al., 2019; Yehudai & Shamir, 2019; Liu et al., 2020). On the other hand, our work needs to address nonlinear dynamics and changing expressive power. Learning dynamics and optimization of GNNs. Closely related to our work, Du et al. (2019b); Xu et al. (2021) study the gradient dynamics of GNNs via the Graph NTK but focus on GNNs’ generalization and extrapolation properties. We instead analyze optimization. Only Zhang et al. (2020) also prove global convergence for GNNs, but for the one- hidden-layer case, and they assume a specialized tensor initialization and training algorithms. In contrast, our results work for any ﬁnite depth with no assumptions on specialized training. Other works aim to accelerate and stabilize the training of GNNs through normalization techniques (Cai et al., 2020) and importance sampling (Chen et al., 2018a;b; Huang et al., 2018; Chiang et al., 2019; Zou et al., 2019). Our work complements these practical works with a better theoretical understanding of GNN training. 6. Conclusion This work studies the training properties of GNNs through the lens of optimization dynamics. For linearized GNNs with or without skip connections, despite the non-convex objective, we show that gradient descent training is guar- anteed to converge to a global minimum at a linear rate. The conditions for global convergence are validated on real- world graphs. We further ﬁnd out that skip connections, more depth, and/or a good label distribution implicitly ac- celerate the training of GNNs. Our results suggest deeper GNNs with skip connections may be promising in practice, and serve as a ﬁrst foundational step for understanding the optimization of general GNNs.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Acknowledgements KX and SJ were supported by NSF CAREER award 1553284 and NSF III 1900933. MZ was supported by ODNI, IARPA, via the BETTER Program contract 2019- 19051600005. The research of KK was partially supported by the Center of Mathematical Sciences and Applications at Harvard University. The views, opinions, and/or ﬁnd- ings contained in this article are those of the author and should not be interpreted as representing the ofﬁcial views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency, the Department of Defense, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. References Abu-El-Haija, S., Kapoor, A., Perozzi, B., and Lee, J. N-gcn: Multi-scale graph convolution for semi-supervised node classiﬁcation. In Uncertainty in Artiﬁcial Intelligence, pp. 841–851. PMLR, 2020. Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. In International Conference on Machine Learning, pp. 242–252, 2019. Arora, S., Cohen, N., and Hazan, E. On the optimization of deep networks: Implicit acceleration by overparameteri- zation. In International Conference on Machine Learning, pp. 244–253. PMLR, 2018. Arora, S., Cohen, N., Golowich, N., and Hu, W. A conver- gence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Rep- resentations, 2019a. Arora, S., Du, S., Hu, W., Li, Z., and Wang, R. Fine-grained analysis of optimization and generalization for overpa- rameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332, 2019b. Bartlett, P. L., Helmbold, D. P., and Long, P. M. Gradi- ent descent with identity initialization efﬁciently learns positive-deﬁnite linear transformations by deep residual networks. Neural computation, 31(3):477–502, 2019. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-y., and Wang, L. Graphnorm: A principled approach to acceler- ating graph neural network training. arXiv preprint arXiv:2009.03294, 2020. Chen, J., Ma, T., and Xiao, C. FastGCN: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations, 2018a. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. In Inter- national Conference on Machine Learning, pp. 942–950, 2018b. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Sim- ple and deep graph convolutional networks. In Interna- tional Conference on Machine Learning, pp. 1725–1735. PMLR, 2020. Chen, Z., Villar, S., Chen, L., and Bruna, J. On the equiv- alence between graph isomorphism testing and function approximation with gnns. In Advances in Neural Infor- mation Processing Systems, pp. 15894–15902, 2019. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019. Chizat, L., Oyallon, E., and Bach, F. On lazy training in differentiable programming. In Advances in Neural Information Processing Systems, pp. 2937–2947, 2019. Defferrard, M., Bresson, X., and Vandergheynst, P. Con- volutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016. Du, S. and Hu, W. Width provably matters in optimiza- tion for deep linear neural networks. In International Conference on Machine Learning, pp. 1655–1664, 2019. Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent ﬁnds global minima of deep neural networks. In International Conference on Machine Learning , pp. 1675–1685, 2019a. Du, S. S., Hou, K., Salakhutdinov, R. R., Poczos, B., Wang, R., and Xu, K. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances in Neural Information Processing Systems, pp. 5724–5734, 2019b. Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Rep- resentations, 2019c. Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P. Con- volutional networks on graphs for learning molecular ﬁn- gerprints. In Advances in neural information processing systems, pp. 2224–2232, 2015.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Fey, M. and Lenssen, J. E. Fast graph representation learning with pytorch geometric.arXiv preprint arXiv:1903.02428, 2019. Garg, V . K., Jegelka, S., and Jaakkola, T. Generalization and representational limits of graph neural networks. In International Conference on Machine Learning, 2020. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In International Conference on Machine Learning, pp. 1273–1272, 2017. Gori, M., Monfardini, G., and Scarselli, F. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729–734. IEEE, 2005. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1025–1035, 2017. Hardt, M. and Ma, T. Identity matters in deep learning. In International Conference on Learning Representations, 2017. Huang, J. and Yau, H.-T. Dynamics of deep neural networks and neural tangent hierarchy. In International conference on machine learning, 2020. Huang, W., Zhang, T., Rong, Y ., and Huang, J. Adap- tive sampling towards fast graph representation learning. Advances in neural information processing systems, 31: 4558–4567, 2018. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571–8580, 2018. Ji, Z. and Telgarsky, M. Directional convergence and align- ment in deep learning. arXiv preprint arXiv:2006.06657, 2020. Kawaguchi, K. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586–594, 2016. Kawaguchi, K. On the theory of implicit deep learning: Global convergence with implicit layers. In International Conference on Learning Representations (ICLR), 2021. Kawaguchi, K. and Huang, J. Gradient descent ﬁnds global minima for generalizable deep neural networks of prac- tical sizes. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 92–99. IEEE, 2019. Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: moving beyond ﬁngerprints. Journal of computer-aided molecular design, 30(8):595–608, 2016. Keriven, N. and Peyr´e, G. Universal invariant and equiv- ariant graph neural networks. In Advances in Neural Information Processing Systems, pp. 7092–7101, 2019. Kipf, T. N. and Welling, M. Semi-supervised classiﬁca- tion with graph convolutional networks. In International Conference on Learning Representations, 2017. Laurent, T. and Brecht, J. Deep linear networks with arbi- trary loss: All local minima are global. In International conference on machine learning, pp. 2902–2907. PMLR, 2018. Lee, J., Xiao, L., Schoenholz, S., Bahri, Y ., Novak, R., Sohl- Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in neural information processing systems , pp. 8572–8583, 2019. Li, G., Muller, M., Thabet, A., and Ghanem, B. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE International Conference on Computer Vision, pp. 9267–9276, 2019. Li, G., Xiong, C., Thabet, A., and Ghanem, B. Deep- ergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739, 2020. Li, Y . and Liang, Y . Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157–8166, 2018. Liao, P., Zhao, H., Xu, K., Jaakkola, T., Gordon, G., Jegelka, S., and Salakhutdinov, R. Graph adversarial networks: Protecting information against adversarial attacks. arXiv preprint arXiv:2009.13504, 2020. Liu, C., Zhu, L., and Belkin, M. On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33, 2020. Loukas, A. How hard is to distinguish graphs with graph neural networks? In Advances in neural information processing systems, 2020. Maron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y . Provably powerful graph networks. In Advances in Neural Information Processing Systems, pp. 2156–2167, 2019.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Merkwirth, C. and Lengauer, T. Automatic generation of complementary descriptors with molecular graph net- works. J. Chem. Inf. Model., 45(5):1159–1168, 2005. Nitanda, A. and Suzuki, T. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime. In International Conference on Learning Repre- sentations, 2021. Oono, K. and Suzuki, T. Optimization and generalization analysis of transduction through gradient boosting and ap- plication to multi-scale graph neural networks. Advances in Neural Information Processing Systems, 33, 2020. Sato, R., Yamada, M., and Kashima, H. Approximation ra- tios of graph neural networks for combinatorial problems. In Advances in Neural Information Processing Systems, pp. 4083–4092, 2019. Sato, R., Yamada, M., and Kashima, H. Random fea- tures strengthen graph neural networks. arXiv preprint arXiv:2002.03155, 2020. Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009. Scarselli, F., Tsoi, A. C., and Hagenbuchner, M. The vapnik– chervonenkis dimension of graph and recursive neural networks. Neural Networks, 108:248–259, 2018. Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T. Collective classiﬁcation in network data. AI magazine, 29(3):93, 2008. Thekumparampil, K. K., Wang, C., Oh, S., and Li, L.- J. Attention-based graph neural network for semi- supervised learning. arXiv preprint arXiv:1803.03735, 2018. Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2018. Vignac, C., Loukas, A., and Frossard, P. Building power- ful and equivariant graph neural networks with message- passing. Advances in neural information processing sys- tems, 2020. Wu, F., Souza, A., Fifty, C., Yu, T., and Weinberger, K. Sim- plifying graph convolutional networks. In International Conference on Machine Learning, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, pp. 5453–5462, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. What can neural networks reason about? In International Conference on Learning Representations, 2020. Xu, K., Zhang, M., Li, J., Du, S. S., Kawarabayashi, K.-I., and Jegelka, S. How neural networks extrapolate: From feedforward to graph neural networks. In International Conference on Learning Representations, 2021. Yehudai, G. and Shamir, O. On the power and limitations of random features for understanding neural networks. In Advances in Neural Information Processing Systems, pp. 6598–6608, 2019. Zhang, S., Wang, M., Liu, S., Chen, P.-Y ., and Xiong, J. Fast learning of graph neural networks with guaranteed gener- alizability: One-hidden-layer case. In International Con- ference on Machine Learning, pp. 11268–11277, 2020. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems , pp. 11249– 11259, 2019. Zou, D., Long, P. M., and Gu, Q. On the global conver- gence of training deep linear resnets. In International Conference on Learning Representations, 2020.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A. Proofs In this section, we complete the proofs of our theoretical results. We show the proofs of Theorem 1 in Appendix A.1, Proposition 1 in Appendix A.2, Proposition 2 in Appendix A.3, Theorem 2 in Appendix A.4, Proposition 3 in Appendix A.5, and Theorem 3 in Appendix A.6. Before starting our proofs, we ﬁrst introduce additional notation used in the proofs. We deﬁne the corner cases on the products of Bas: B(H)B(H−1) ···B(l+1) := Iml if H = l (19) B(H)B(H−1) ...B (1) := Imx if H = 0 (20) B(l−1)B(l−2) ...B (1) := Imx if l= 1 (21) Similarly, for any matrices M(l), we deﬁne M(l)M(l−1) ···M(k) := Im if l < k,and M(l)M(l−1) ···M(k) := M(k) = M(l) if l= k.Given a scalar-valued variable a∈R and a matrix M ∈Rd×d′ , we deﬁne ∂a ∂M =   ∂a ∂M11 ··· ∂a ∂M1d′ ... ... ... ∂a ∂Md1 ··· ∂a ∂Mdd′  ∈Rd×d′ , (22) where Mij represents the (i,j)-th entry of the matrix M. Given a vector-valued variable a ∈Rd and a column vector b∈Rd′ , we let ∂a ∂b =   ∂a1 ∂b1 ··· ∂a1 ∂bd′ ... ... ... ∂ad ∂b1 ··· ∂ad ∂bd′  ∈Rd×d′ , (23) where bi represents the i-th entry of the column vector b. Similarly, given a vector-valued variable a∈Rd and a row vector b∈R1×d′ , we write ∂a ∂b =   ∂a1 ∂b11 ··· ∂a1 ∂b1d′ ... ... ... ∂ad ∂b11 ··· ∂ad ∂b1d′  ∈Rd×d′ , (24) where b1i represents the i-th entry of the row vector b. Finally, we recall the deﬁnition of the Kronecker product product of two matrices: for matrices M ∈RdM×d′ M and ¯M ∈Rd¯M×d′ ¯M , M ⊗ ¯M =   M11 ¯M ··· M1d′ M ¯M ... ... ... MdM1 ¯M ··· MdMd′ M ¯M  ∈RdMd¯M×d′ Md′ ¯M . (25) A.1. Proof of Theorem 1 We begin with a proof overview of Theorem 1. We ﬁrst relate the gradients ∇W(H)Land ∇B(l)Lto the gradient ∇(H)L, which is deﬁned by ∇(H)L(W,B) := ∂L(W,B) ∂ˆY (X(SH)∗I)⊤∈Rmy×mx. Using the proven relation of (∇W(H)L,∇B(l)L) and ∇(H)L, we ﬁrst analyze the dynamics induced in the space of W(l)B(l)B(l−1) ···B(1) in Appendix A.1.1, and then the dynamics induced int the space of loss valueL(W,B) in Appendix A.1.2. Finally, we complete the proof by using the assumption of employing the square loss in Appendix A.1.3. Let W(H) = W (during the proof of Theorem 1). We ﬁrst prove the relationship of the gradients ∇W(H)L, ∇B(l)Land ∇(H)Lin the following lemma:Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Lemma 1. Let f be an H-layer linear GNN and ℓ(q,Y ) = ∥q−Y∥2 F where q,Y ∈Rmy×¯n. Then, for any (W,B), ∇W(H)L(W,B) = ∇(H)L(W,B)(B(H)B(H−1) ...B (1))⊤∈Rmy×ml, (26) and ∇B(l)L(W,B) = (W(H)B(H)B(H−1) ···B(l+1))⊤∇(H)L(W,B)(B(l−1)B(l−2) ...B (1))⊤∈Rml×ml−1, (27) Proof of Lemma 1. From Deﬁnition 1, we have ˆY = f(X,W,B )∗I= W(H)(X(H))∗Iwhere X(l) = B(l)X(l−1)S. Using this deﬁnition, we can derive the formula of ∂vec[ˆY] ∂vec[W(H)] ∈Rmyn×mym¯H as: ∂vec[ˆY] ∂vec[W(H)] = ∂ ∂vec[W(H)] vec[W(H)(X(H))∗I] = ∂ ∂vec[W(H)][((X(H))∗I)⊤⊗Imy ] vec[W(H)] = [((X(H))∗I)⊤⊗Imy ] ∈Rmyn×mym¯H (28) We will now derive the formula of ∂vec[ˆY] ∂vec[B(l)] ∈Rmyn×mlml−1: ∂vec[ˆY] ∂vec[B(l)] = ∂ ∂vec[B(l)] vec[W(H)(X(H))∗I] = ∂ ∂vec[B(l)][In ⊗W(H)] vec[(X(H))∗I] = [In ⊗W(H)]∂vec[(X(H))∗I] ∂vec[B(l)] = [In ⊗W(H)]∂vec[(X(H))∗I] ∂vec[X(l)] ∂vec[X(l)] ∂vec[B(l)] = [In ⊗W(H)]∂vec[(X(H))∗I] ∂vec[X(l)] ∂vec[B(l)X(l−1)S] ∂vec[B(l)] = [In ⊗W(H)]∂vec[(X(H))∗I] ∂vec[X(l)] ∂[(X(l−1)S)⊤⊗Iml] vec[B(l)] ∂vec[B(l)] = [In ⊗W(H)]∂vec[(X(H))∗I] ∂vec[X(l)] [(X(l−1)S)⊤⊗Iml] (29) Here, we have that vec[(X(H))∗I] = vec[B(H)X(H−1)S∗I] = vec[(S⊤)I∗⊗B(H)] vec[X(H−1)]. (30) and vec[X(H)] = vec[B(H)X(H−1)S∗I] = vec[S⊗B(H)] vec[X(H−1)]. (31) By recursively applying (31), we have that vec[(X(H))∗I] = vec[(S⊤)I∗⊗B(H)] vec[S⊤⊗B(H−1)] ···vec[S⊤⊗B(l+1)] vec[X(l)] = vec[((SH−l)⊤)I∗⊗B(H)B(H−1) ···B(l+1)] vec[X(l)], where B(H)B(H−1) ···B(l+1) := Iml if H = l. Therefore,Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth ∂vec[(X(H))∗I] ∂vec[X(l)] = vec[((SH−l)⊤)I∗⊗B(H)B(H−1) ···B(l+1)]. (32) Combining (29) and (32) yields ∂vec[ˆY] ∂vec[B(l)] = [In ⊗W(H)]∂vec[(X(H))∗I] ∂vec[X(l)] [(X(l−1)S)⊤⊗Iml] = [In ⊗W(H)] vec[((SH−l)⊤)I∗⊗B(H)B(H−1) ···B(l+1)][(X(l−1)S)⊤⊗Iml] = [(X(l−1)(SH−l+1)∗I)⊤⊗W(H)B(H)B(H−1) ···B(l+1)] ∈Rmyn×mlml−1. (33) Using (28), we will now derive the formula of ∇W(H)L(W,B) ∈Rmy×mH : ∂L(W,B) ∂vec[W(H)] = ∂L(W,B) ∂vec[ˆY] ∂vec[ˆY] ∂vec[W(H)] = ∂L(W,B) ∂vec[ˆY] [(X(H))⊤ ∗I⊗Imy ] Thus, with ∂L(W,B) ∂ˆY ∈Rmy×n, ∇vec[W(H)]L(W,B) = ( ∂L(W,B) ∂vec[W(H)] )⊤ = [(X(H))∗I⊗Imy ] ( ∂L(W,B) ∂vec[ˆY] )⊤ = [(X(H))∗I⊗Imy ] vec [∂L(W,B) ∂ˆY ] = vec [∂L(W,B) ∂ˆY (X(H))⊤ ∗I ] ∈RmymH . Therefore, ∇W(H)L(W,B) = ∂L(W,B) ∂ˆY (X(H))⊤ ∗I∈Rmy×mH . (34) Using (33), we will now derive the formula of ∇B(l)L(W,B) ∈Rml×ml−1: ∂L(W,B) ∂vec[B(l)] = ∂L(W,B) ∂vec[ˆY] ∂vec[ˆY] ∂vec[B(l)] = ∂L(W,B) ∂vec[ˆY] [(X(l−1)(SH−l+1)∗I)⊤⊗W(H)B(H)B(H−1) ···B(l+1)]. Thus, with ∂L(W,B) ∂ˆY ∈Rmy×n, ∇vec[B(l)]L(W,B) = (∂L(W,B) ∂vec[B(l)] )⊤ = [X(l−1)(SH−l+1)∗I⊗(W(H)B(H)B(H−1) ···B(l+1))⊤] ( ∂L(W,B) ∂vec[ˆY] )⊤ = [X(l−1)(SH−l+1)∗I⊗(W(H)B(H)B(H−1) ···B(l+1))⊤] vec [∂L(W,B) ∂ˆY ] = vec [ (W(H)B(H)B(H−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (X(l−1)(SH−l+1)∗I)⊤ ] ∈Rmlml−1.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Therefore, ∇B(l)L(W,B) = (W(H)B(H)B(H−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (X(l−1)(SH−l+1)∗I)⊤∈Rml×ml−1. (35) With (34) and (35), we are now ready to prove the statement of this lemma by introducing the following notation: ∇(l)L(W,B) := ∂L(W,B) ∂ˆY (X(Sl)∗I)⊤∈Rmy×mx. Using this notation along with (34) ∇W(H)L(W,B) = ∂L(W,B) ∂ˆY (X(H))⊤ ∗I = ∂L(W,B) ∂ˆY (B(H)X(H−1)(S)∗I)⊤ = ∂L(W,B) ∂ˆY (B(H)B(H−1) ...B (1)X(SH)∗I)⊤ = ∇(H)L(W,B)(B(H)B(H−1) ...B (1))⊤, Similarly, using (35), ∇B(l)L(W,B) = (W(H)B(H)B(H−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (X(l−1)(SH−l+1)∗I)⊤ = (W(H)B(H)B(H−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (B(l−1)B(l−2) ...B (1)X(Sl−1SH−l+1)∗I)⊤ = (W(H)B(H)B(H−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (B(l−1)B(l−2) ...B (1)X(SH)∗I)⊤ = (W(H)B(H)B(H−1) ···B(l+1))⊤∇(H)L(W,B)(B(l−1)B(l−2) ...B (1))⊤ where B(l−1)B(l−2) ...B (1) := Imx if l= 1. By using Lemma 1, we complete the proof of Theorem 1 in the following. A.1.1. D YNAMICS INDUCED IN THE SPACE OF W(l)B(l)B(l−1) ···B(1) We now consider the dynamics induced in the space of W(l)B(l)B(l−1) ···B(1). We ﬁrst consider the following discrete version of the dynamics: W′ (H) = W(H) −α∇W(H)L(W,B) B′ (l) = B(l) −α∇B(l)L(W,B). This dynamics induces the following dynamics: W′ (H)B′ (H)B′ (H−1) ···B′ (1) = (W(H) −α∇W(H)L(W,B))(B(H) −α∇B(H)L(W,B)) ···(B(1) −α∇B(1)L(W,B)). Deﬁne Z(H) := W(H)B(H)B(H−1) ···B(1), and Z′ (H) := W′ (H)B′ (H)B′ (H−1) ···B′ (1). Then, we can rewrite Z′ (H) = (W(H) −α∇W(H)L(W,B))(B(H) −α∇B(H)L(W,B)) ···(B(1) −α∇B(1)L(W,B)).Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth By expanding the multiplications, this can be written as: Z′ (H) = Z(H) −α∇W(H)L(W,B)B(H) ···B(1) −α H∑ i=1 W(H)B(H) ···B(i+1)∇B(i)L(W,B)B(i−1) ···B(1) + O(α2). By vectorizing both sides, vec[Z′ (H)] −vec[Z(H)] = −αvec[∇W(H)L(W,B)B(H) ···B(1)] −α H∑ i=1 vec[W(H)B(H) ···B(i+1)∇B(i)L(W,B)B(i−1) ···B(1)] + O(α2). Here, using the formula of ∇W(H)L(W,B) and ∇B(H)L(W,B), we have that vec[∇W(H)L(W,B)B(H) ···B(1)] = vec[∇(H)L(W,B)(B(H) ...B (1))⊤B(H) ···B(1)] = [(B(H) ...B (1))⊤B(H) ···B(1) ⊗Imy ] vec[∇(H)L(W,B)], and H∑ i=1 vec[W(H)B(H) ···B(i+1)∇B(i)L(W,B)B(i−1) ···B(1)] = H∑ i=1 vec [ W(H)B(H) ···B(i+1)(W(H)B(H) ···B(i+1))⊤∇(H)L(W,B)(B(i−1) ...B (1))⊤B(i−1) ···B(1) ] = H∑ i=1 [(B(i−1) ...B (1))⊤B(i−1) ···B(1) ⊗W(H)B(H) ···B(i+1)(W(H)B(H) ···B(i+1))⊤] vec [ ∇(H)L(W,B) ] . Summarizing above, vec[Z′ (H)] −vec[Z(H)] = −α[(B(H) ...B (1))⊤B(H) ···B(1) ⊗Imy ] vec[∇(H)L(W,B)] −α H∑ i=1 [(B(i−1) ...B (1))⊤B(i−1) ···B(1) ⊗W(H)B(H) ···B(i+1)(W(H)B(H) ···B(i+1))⊤] vec [ ∇(H)L(W,B) ] + O(α2) Therefore, the induced continuous dynamics of Z(H) = W(H)B(H)B(H−1) ···B(1) is d dtvec[Z(H)] = −F(H) vec[∇(H)L(W,B)] − (H∑ i=1 J⊤ (i,H)J(i,H) ) vec [ ∇(H)L(W,B) ] , where F(H) = [(B(H) ...B (1))⊤B(H) ···B(1) ⊗Imy ], and J(i,H) = [B(i−1) ...B (1) ⊗(W(H)B(H) ···B(i+1))⊤]. This is because J⊤ (i,H)J(i,H) = [(B(i−1) ...B (1))⊤⊗W(H)B(H) ···B(i+1)][B(i−1) ...B (1) ⊗(W(H)B(H) ···B(i+1))⊤] = [(B(i−1) ...B (1))⊤B(i−1) ...B (1) ⊗W(H)B(H) ···B(i+1)(W(H)B(H) ···B(i+1))⊤].Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A.1.2. D YNAMICS INDUCED INT THE SPACE OF LOSS VALUE L(W,B) We now analyze the dynamics induced int the space of loss valueL(W,B). Using chain rule, d dtL(W,B) = d dtL0(Z(H)) = ∂L0(Z(H)) ∂vec[Z(H)] dvec[Z(H)] dt , where L0(Z(H)) = ℓ(f0(X,Z(H))∗I,Y ), f0(X,Z(H)) = Z(H)XSH, and Z(H) = W(H)B(H)B(H−1) ···B(1). Since f0(X,Z(H)) = f(X,W,B ) = ˆY and L0(Z(H)) = L(W,B), we have that (∂L0(Z(H)) ∂vec[Z(H)] )⊤ = ( ∂L(W,B) ∂vec[ˆY] ∂vec[ˆY] ∂vec[Z(H)] )⊤ = ( ∂L(W,B) ∂vec[ˆY] ( ∂ ∂vec[Z(H)][(X(SH)∗I)⊤⊗Imy ] vec[Z(H)] ))⊤ = [X(SH)∗I⊗Imy ] vec [∂L(W,B) ∂ˆY ] = vec [∂L(W,B) ∂ˆY (X(SH)∗I)⊤ ] = vec[∇(H)L(W,B)] Combining these, d dtL(W,B) = vec[∇(H)L(W,B)]⊤dvec[Z(H)] dt = −vec[∇(H)L(W,B)]⊤F(H) vec[∇(H)L(W,B)] − H∑ i=1 vec[∇(H)L(W,B)]⊤J⊤ (i,H)J(i,H) vec [ ∇(H)L(W,B) ] = −vec[∇(H)L(W,B)]⊤F(H) vec[∇(H)L(W,B)] − H∑ i=1 ∥J(i,H) vec [ ∇(H)L(W,B) ] ∥2 2 Therefore, d dtL(W,B) = −vec[∇(H)L(W,B)]⊤F(H) vec[∇(H)L(W,B)] − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 (36) Since F(H) is real symmetric and positive semideﬁnite, d dtL(W,B) ≤−λmin(F(H))∥vec[∇(H)L(W,B)]∥2 2 − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 . With λW,B = λmin(F(H)), d dtL(W,B) ≤−λW,B∥vec[∇(H)L(W,B)]∥2 2 − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 (37)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A.1.3. C OMPLETING THE PROOF BY USING THE ASSUMPTION OF THE SQUARE LOSS Using the assumption that L(W,B) = ℓ(f(X,W,B )∗I,Y ) = ∥f(X,W,B )∗I−Y∥2 F with ˆY = f(X,W,B )∗I, we have ∂L(W,B) ∂ˆY = ∂ ∂ˆY ∥ˆY −Y∥2 F = 2( ˆY −Y) ∈Rmy×n, and vec[∇(H)L(W,B)] = vec [∂L(W,B) ∂ˆY (X(SH)∗I)⊤ ] = 2 vec [ ( ˆY −Y)(X(SH)∗I)⊤ ] = 2[X(SH)∗I⊗Imy ] vec[ˆY −Y]. Therefore, ∥vec[∇(H)L(W,B)]∥2 2 = 4 vec[ˆY −Y]⊤[(X(SH)∗I)⊤X(SH)∗I⊗Imy ] vec[ˆY −Y] (38) Using (37) and (38), d dtL(W,B) ≤−λW,B∥vec[∇(H)L(W,B)]∥2 2 − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 ≤−4λW,Bvec[ˆY −Y]⊤[(X(SH)∗I)⊤X(SH)∗I⊗Imy ] vec[ˆY −Y] − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 = −4λW,Bvec[ˆY −Y]⊤ [ ˜G⊤ H ˜GH ⊗Imy ] vec[ˆY −Y] − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 where the last line follows from the following deﬁnition: ˜GH := X(SH)∗I. Decompose vec[ˆY−Y] as vec[ˆY−Y] = v+v⊥, where v= P˜G⊤ H⊗Imy vec[ˆY−Y], v⊥= (Imyn−P˜G⊤ H⊗Imy ) vec[ˆY−Y], and P˜G⊤ H⊗Imy ∈Rmyn×myn represents the orthogonal projection onto the column space of ˜G⊤ H ⊗Imy ∈Rmyn×mymx. Then, vec[ˆY −Y]⊤ [ ˜G⊤ H ˜GH ⊗Imy ] vec[ˆY −Y] = (v+ v⊥)⊤ [ ˜G⊤ H ⊗Imy ][ ˜GH ⊗Imy ] (v+ v⊥) = v⊤ [ ˜G⊤ H ⊗Imy ][ ˜GH ⊗Imy ] v ≥σ2 min( ˜GH)∥P˜G⊤ H⊗Imy vec[ˆY −Y]∥2 2 = σ2 min( ˜GH)∥P˜G⊤ H⊗Imy vec[ˆY] −P˜G⊤ H⊗Imy vec[Y]∥2 2 = σ2 min( ˜GH)∥vec[ˆY] −P˜G⊤ H⊗Imy vec[Y] ±vec[Y]∥2 2 = σ2 min( ˜GH)∥vec[ˆY] −vec[Y] + (Imyn −P˜G⊤ H⊗Imy ) vec[Y]∥2 2 ≥σ2 min( ˜GH)(∥vec[ˆY −Y]∥2 −∥(Imyn −P˜G⊤ H⊗Imy ) vec[Y]∥2)2 ≥σ2 min( ˜GH)(∥vec[ˆY −Y]∥2 2 −∥(Imyn −P˜G⊤ H⊗Imy ) vec[Y]∥2 2, where we used the fact that the singular values of [ ˜G⊤ H ⊗Imy ] are products of singular values of ˜GH and Imy . By noticing that L(W,B) = ∥vec[ˆY −Y]∥2 2 and L∗ H = ∥(Imyn −P˜G⊤ H⊗Imy ) vec[Y]∥2 2 , vec[ˆY −Y]⊤ [ ˜G⊤ H ˜GH ⊗Imy ] vec[ˆY −Y] ≥σ2 min( ˜GH)(L(W,B) −L∗ H).Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Therefore, d dtL(W,B) ≤−4λW,Bvec[ˆY −Y]⊤ [ ˜G⊤ H ˜GH ⊗Imy ] vec[ˆY −Y] − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 ≤−4λW,Bσ2 min( ˜GH)(L(W,B) −L∗ H) − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 Since d dtL∗ H = 0, d dt(L(W,B) −L∗ H) ≤−4λW,Bσ2 min( ˜GH)(L(W,B) −L∗ H) − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 By deﬁning L = L(W,B) −L∗ H, dL dt ≤−4λW,Bσ2 min( ˜GH)L − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 (39) Since d dtL ≤0 and L ≥0, if L = 0 at some time ¯t, then L = 0 for any time t≥¯t. Therefore, if L = 0 at some time ¯t, then we have the desired statement of this theorem for any time t≥¯t. Thus, we can focus on the time interval [0,¯t] such that L >0 for any time t∈[0,¯t] (here, it is allowed to have ¯t= ∞). Thus, focusing on the time interval with L >0 , equation (39) implies that 1 L dL dt ≤−4λW,Bσ2 min( ˜GH) −1 L H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 By taking integral over time ∫ T 0 1 L dL dtdt≤− ∫ T 0 4λW,Bσ2 min( ˜GH)dt− ∫ T 0 1 L H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 dt By using the substitution rule for integrals, ∫T 0 1 L dL dtdt= ∫LT L0 1 LdL = log(LT) −log(L0), where L0 = L(W0,B0) −L∗ and LT = L(WT,BT) −L∗ H. Thus, log(LT) −log(L0) ≤−4σ2 min( ˜GH) ∫ T 0 λW,Bdt− ∫ T 0 1 L H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 dt which implies that LT ≤elog(L0)−4σ2 min( ˜GH) ∫T 0 λW,Bdt− ∫T 0 1 L ∑H i=1∥J(i,H) vec[∇(H)L(W,B)]∥ 2 2dt = L0e−4σ2 min( ˜GH) ∫T 0 λW,Bdt− ∫T 0 1 L ∑H i=1∥J(i,H) vec[∇(H)L(W,B)]∥ 2 2dt By recalling the deﬁnition of L = L(W,B) −L∗ H and that d dtL ≤0, we have that if L(WT,BT) −L∗ H > 0, then L(Wt,Bt) −L∗ H >0 for all t∈[0,T], and L(WT,BT) −L∗ H ≤(L(W0,B0) −L∗ H)e −4σ2 min( ˜GH) ∫T 0 λWt,Btdt− ∫T 0 1 L(Wt,Bt)−L∗ H ∑H i=1∥J(i,H) vec[∇(H)L(Wt,Bt)]∥ 2 2dt . (40) Using the property of Kronecker product, λmin([(B(H),t...B (1),t)⊤B(H),t···B(1),t ⊗Imy ]) = λmin((B(H),t...B (1),t)⊤B(H),t···B(1),t),Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth which implies that λ(H) T = inf t∈[0,T] λWt,Bt. Thus, by noticing that∫T 0 1 L(Wt,Bt)−L∗ H ∑H i=1 J(i,H) vec[∇(H)L(Wt,Bt)] 2 2 dt≥0, equation (40) implies that L(WT,BT) −L∗ H ≤(L(W0,B0) −L∗ H)e −4λ(H) T σ2 min( ˜GH)T− ∫T 0 1 L(Wt,Bt)−L∗ H ∑H i=1∥J(i,H) vec[∇(H)L(Wt,Bt)]∥ 2 2dt ≤(L(W0,B0) −L∗ H)e−4λ(H) T σ2 min( ˜GH)T = (L(W0,B0) −L∗ H)e−4λ(H) T σ2 min(X(SH)∗I)T A.2. Proof of Proposition 1 From Deﬁnition 4, we have that σmin( ¯B(1:H)) = σmin(B(H)B(H−1) ···B(1)) ≥γ for all (W,B) such that L(W,B) ≤ L(W0,B0). From equation (37) in the proof of Theorem 1, it holds that d dtL(Wt,Bt) ≤0 for all t. Thus, we have that L(Wt,Bt) ≤L(W0,B0) and hence σmin( ¯B(1:H) t ) ≥γfor all t. Under this problem setting (mH ≥mx), this implies that λmin(( ¯B(1:H) t )⊤¯B(1:H) t ) ≥γ2 for all tand thus λ(H) T ≥γ2. A.3. Proof of Proposition 2 We ﬁrst give the complete version of Proposition 2. Proposition 4 is the formal version of Proposition 2 and shows that our singular margin generalizes deﬁciency margin proposed in Arora et al. (2019a). Using the deﬁciency margin assumption, Arora et al. (2019a) analyzed the following optimization problem: minimize ˜W ˜L( ˜W(1),..., ˜W(H+1)) : = 1 2∥˜W(H+1) ˜W(H) ··· ˜W(1) −˜Φ∥2 F (41) = 1 2∥˜W⊤ (1) ˜W⊤ (2) ··· ˜W⊤ (H+1) −˜Φ⊤∥2 F, (42) where ˜Φ ∈R˜my×˜mx is a target matrix and the last equality follows from ∥M∥F = ∥M⊤∥F for any matrix M by the deﬁnition of the Frobenius norm. Therefore, this optimization problem (41) from the previous work is equivalent to the following optimization problem in our notation: minimize W,B L(W,B) := 1 2∥WB(H)B(H−1) ···B(1) −Φ∥2 F, (43) where WB(H)B(H−1) ···B(1) = ˜W(H+1) ˜W(H) ··· ˜W(1) (i.e., W = ˜W(H+1) with B(l) = ˜W(l)) and Φ = ˜Φ if ˜my ≥˜mx, and WB(H)B(H−1) ···B(1) = ˜W⊤ (1) ˜W⊤ (2) ··· ˜W⊤ (H+1) (i.e., W = ˜W⊤ (1) with B(l) = ˜W⊤ (H+2−l)) and Φ = ˜Φ⊤if ˜my < ˜mx. That is, we have Φ ∈Rmy×mx where my = ˜my with mx = ˜mx if ˜my ≥˜mx, and my = ˜mx with mx = ˜my if ˜my < ˜mx. Therefore, our general problem framework with graph structures can be reduced and applicable to the previous optimization problem without graph structures by setting 1 nXX⊤= I, S = I, I= [n], f(X,W,B ) = WB(H)B(H−1) ···B(1), and ℓ(q,Φ) = 1 2 ∥q−Φ∥2 F where Φ ∈Rmy×mx is a target matrix with my ≥mx without loss of generality. An initialization (W0,B0) is said to have deﬁciency margin c >0 if the end-to-end matrix W0 ¯B(1:H) 0 of the initialization (W0,B0) has deﬁciency margin c> 0 with respect to the target Φ (Arora et al., 2019a, Deﬁnition 2): i.e., Arora et al. (2019a) assumed that the initialization (W0,B0) has deﬁciency margin c> 0 (as it is also invariant to the transpose of ˜W(H+1) ˜W(H) ··· ˜W(1) −˜Φ). Proposition 4. Consider the optimization problem in (Arora et al., 2019a) by setting 1 nXX⊤ = I, S = I, I = [ n], f(X,W,B ) = WB(H)B(H−1) ···B(1), and ℓ(q,Φ) = 1 2 ∥q−Φ∥2 F where Φ ∈Rmy×mx is a target matrix with my ≥mx without loss of generality (since the transpose of these two dimensions leads to the equivalent optimization problem under this setting: see above). Then, if an initialization (W0,B0) has deﬁciency margin c> 0, it has singular margin γ >0. Proof of Proposition 4. By the deﬁnition of the deﬁciency margin (Arora et al., 2019a, Deﬁnition 2) and its consequence (Arora et al., 2019a, Claim 1), if an initialization (W0,B0) has deﬁciency margin c> 0, then any pair (W,B) for which L(W,B) ≤L(W0,B0) satisﬁes σmin(WB(H)B(H−1) ···B(1)) ≥c> 0. Since the number of nonzero singular values isOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth equal to the matrix rank, this implies that rank(WB(H)B(H−1) ···B(1)) ≥min(my,mx) for any pair (W,B) for which L(W,B) ≤L(W0,B0). Since rank(MM′) ≤min(rank(M),rank(M′)), this implies that mH ≥min(my,mx) = mx, (44) (as well as ml ≥min(my,mx) for all l), and that for any pair (W,B) for which L(W,B) ≤L(W0,B0), mx = min(my,mx) ≤rank(WB(H)B(H−1) ···B(1)) ≤min(rank(W),rank(B(H)B(H−1) ···B(1))) (45) ≤rank(B(H)B(H−1) ···B(1)) ≤mx. (46) This shows that rank(B(H)B(H−1) ···B(1)) = mx for any pair (W,B) for which L(W,B) ≤ L(W0,B0). Since mH ≥ mx from (44) and the number of nonzero singular values is equal to the matrix rank, this implies that σmin(B(H)B(H−1) ···B(1)) ≥γ for some γ >0 for any pair (W,B) for which L(W,B) ≤L(W0,B0). Thus, if an initialization (W0,B0) has deﬁciency margin c> 0, then it has singular margin γ >0. A.4. Proof of Theorem 2 This section completes the proof of Theorem 2. We compute the derivatives of the output of multiscale linear GNN with respect to the parameters W(l) and B(l) in Appendix A.4.1. Then using these derivatives, we compute the gradient of the loss with respect to W(l) in Appendix A.4.2 and B(l) in Appendix A.4.3. We then rearrange the formula of the gradients such that they are related to the formula of ∇(l)L(W,B) in Appendices A.4.4. Using the proven relation, we ﬁrst analyze the dynamics induced in the space of W(l)B(l)B(l−1) ···B(1) in Appendix A.4.5, and then the dynamics induced int the space of loss value L(W,B) in Appendix A.4.6. Finally, we complete the proof by using the assumption of using the square loss in Appendices A.4.7–A.4.10. In the following, we ﬁrst prove the statement for the case of I= [n] for the simplicity of notation and then prove the statement for the general case afterwards. A.4.1. D ERIVATION OF FORMULA FOR ∂vec[ˆY] ∂vec[W(l)] ∈Rmyn×myml AND ∂vec[ˆY] ∂vec[B(l)] ∈Rmyn×mlml−1 We can easily compute ∂vec[ˆY] ∂vec[W(l)] by using the property of the Kronecker product as follows: ∂vec[ˆY] ∂vec[W(l)] = ∂ ∂vec[W(l)] H∑ k=0 vec[W(k)X(k)] = ∂ ∂vec[W(l)] H∑ k=0 [X⊤ (k) ⊗Imy ] vec[W(k)] = [X⊤ (l) ⊗Imy ] ∈Rmyn×myml (47) We now compute ∂vec[ˆY] ∂vec[B(l)] by using the chain rule and the property of the Kronecker product as follows: ∂vec[ˆY] ∂vec[B(l)] = ∂ ∂vec[B(l)] H∑ k=0 vec[W(k)X(k)] = ∂ ∂vec[B(l)] H∑ k=0 [In ⊗W(k)] vec[X(k)] = H∑ k=0 [In ⊗W(k)]∂vec[X(k)] ∂vec[B(l)] = H∑ k=l [In ⊗W(k)]∂vec[X(k)] ∂vec[X(l)] ∂vec[X(l)] ∂vec[B(l)] = H∑ k=l [In ⊗W(k)]∂vec[X(k)] ∂vec[X(l)] ∂vec[B(l)X(l−1)S] ∂vec[B(l)]Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = H∑ k=l [In ⊗W(k)]∂vec[X(k)] ∂vec[X(l)] ∂[(X(l−1)S)⊤⊗Iml] vec[B(l)] ∂vec[B(l)] = H∑ k=l [In ⊗W(k)]∂vec[X(k)] ∂vec[X(l)] [(X(l−1)S)⊤⊗Iml] Here, for any k≥1, vec[X(k)] = vec[B(k)X(k−1)S] = vec[S⊤⊗B(k)] vec[X(k−1)]. By recursively applying this, we have that for any k≥l, vec[X(k)] = vec[S⊤⊗B(k)] vec[S⊤⊗B(k−1)] ···vec[S⊤⊗B(l+1)] vec[X(l)] = vec[(Sk−l)⊤⊗B(k)B(k−1) ···B(l+1)] vec[X(l)], where S0 := In and B(k)B(k−1) ···B(l+1) := Iml if k= l. Therefore, ∂vec[X(k)] ∂vec[X(l)] = vec[(Sk−l)⊤⊗B(k)B(k−1) ···B(l+1)]. Combining the above equations yields ∂vec[ˆY] ∂vec[B(l)] = H∑ k=l [In ⊗W(k)]∂vec[X(k)] ∂vec[X(l)] [(X(l−1)S)⊤⊗Iml] = H∑ k=l [In ⊗W(k)] vec[(Sk−l)⊤⊗B(k)B(k−1) ···B(l+1)][(X(l−1)S)⊤⊗Iml] = H∑ k=l [(X(l−1)Sk−l+1)⊤⊗W(k)B(k)B(k−1) ···B(l+1)] ∈Rmyn×mlml−1. (48) A.4.2. D ERIVATION OF A FORMULA OF ∇W(l)L(W,B) ∈Rmy×ml Using the chain rule and (47), we have that ∂L(W,B) ∂vec[W(l)] = ∂L(W,B) ∂vec[ˆY] ∂vec[ˆY] ∂vec[W(l)] = ∂L(W,B) ∂vec[ˆY] [X⊤ (l) ⊗Imy ]. Thus, with ∂L(W,B) ∂ˆY ∈Rmy×n, by using ∇vec[W(l)]L(W,B) = (∂L(W,B) ∂vec[W(l)] )⊤ = [X(l) ⊗Imy ] ( ∂L(W,B) ∂vec[ˆY] )⊤ = [X(l) ⊗Imy ] vec [∂L(W,B) ∂ˆY ] = vec [∂L(W,B) ∂ˆY X⊤ (l) ] ∈Rmyml. Therefore, ∇W(l)L(W,B) = ∂L(W,B) ∂ˆY X⊤ (l) ∈Rmy×ml. (49)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A.4.3. D ERIVATION OF A FORMULA OF ∇B(l)L(W,B) ∈Rml×ml−1 Using the chain rule and (48), we have that ∂L(W,B) ∂vec[B(l)] = ∂L(W,B) ∂vec[ˆY] ∂vec[ˆY] ∂vec[B(l)] = ∂L(W,B) ∂vec[ˆY] H∑ k=l [(X(l−1)Sk−l+1)⊤⊗W(k)B(k)B(k−1) ···B(l+1)]. Thus, with ∂L(W,B) ∂ˆY ∈Rmy×n, ∇vec[B(l)]L(W,B) = (∂L(W,B) ∂vec[B(l)] )⊤ = H∑ k=l [X(l−1)Sk−l+1 ⊗(W(k)B(k)B(k−1) ···B(l+1))⊤] ( ∂L(W,B) ∂vec[ˆY] )⊤ = H∑ k=l [X(l−1)Sk−l+1 ⊗(W(k)B(k)B(k−1) ···B(l+1))⊤] vec [∂L(W,B) ∂ˆY ] = H∑ k=l vec [ (W(k)B(k)B(k−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (X(l−1)Sk−l+1)⊤ ] ∈Rmlml−1. Therefore, ∇B(l)L(W,B) = H∑ k=l (W(k)B(k)B(k−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (X(l−1)Sk−l+1)⊤∈Rml×ml−1. (50) A.4.4. R ELATING GRADIENTS TO ∇(l)L We now relate the gradients of the loss to ∇(l)L, which is deﬁned by ∇(l)L(W,B) := ∂L(W,B) ∂ˆY (XSl)⊤∈Rmy×mx. By using this deﬁnition and (49), we have that ∇W(l)L(W,B) = ∂L(W,B) ∂ˆY X⊤ (l) = ∂L(W,B) ∂ˆY (B(l)X(l−1)S)⊤ = ∂L(W,B) ∂ˆY (B(l)B(l−1) ...B (1)XSl)⊤ = ∇(l)L(W,B)(B(l)B(l−1) ...B (1))⊤, where B(l)B(l−1) ...B (1) := Imx if l= 0. Similarly, by using the deﬁnition and (50), ∇B(l)L(W,B) = H∑ k=l (W(k)B(k)B(k−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (X(l−1)Sk−l+1)⊤ = H∑ k=l (W(k)B(k)B(k−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (B(l−1)B(l−2) ...B (1)XSl−1Sk−l+1)⊤ = H∑ k=l (W(k)B(k)B(k−1) ···B(l+1))⊤∂L(W,B) ∂ˆY (B(l−1)B(l−2) ...B (1)XSk)⊤Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = H∑ k=l (W(k)B(k)B(k−1) ···B(l+1))⊤∇(k)L(W,B)(B(l−1)B(l−2) ...B (1))⊤ where B(l−1)B(l−2) ...B (1) := Imx if l= 1. In summary thus far, we have that ∇W(l)L(W,B) = ∇(l)L(W,B)(B(l)B(l−1) ...B (1))⊤∈Rmy×ml, (51) and ∇B(l)L(W,B) = H∑ k=l (W(k)B(k)B(k−1) ···B(l+1))⊤∇(k)L(W,B)(B(l−1)B(l−2) ...B (1))⊤∈Rml×ml−1, (52) where ∇(l)L(W,B) := ∂L(W,B) ∂ˆY (XSl)⊤∈Rmy×mx, B(k)B(k−1) ···B(l+1) := Iml if k= l, B(l)B(l−1) ...B (1) := Imx if l= 0, and B(l−1)B(l−2) ...B (1) := Imx if l= 1. A.4.5. D YNAMICS INDUCED IN THE SPACE OF W(l)B(l)B(l−1) ···B(1) We now consider the Dynamics induced in the space ofW(l)B(l)B(l−1) ···B(1). We ﬁrst consider the following discrete version of the dynamics: W′ (l) = W(l) −α∇W(l)L(W,B) B′ (l) = B(l) −α∇B(l)L(W,B). This dynamics induces the following dynamics: W′ (l)B′ (l)B′ (l−1) ···B′ (1) = (W(l) −α∇W(l)L(W,B))(B(l) −α∇B(l)L(W,B)) ···(B(1) −α∇B(1)L(W,B)). Deﬁne Z(l) := W(l)B(l)B(l−1) ···B(1) and Z′ (l) := W′ (l)B′ (l)B′ (l−1) ···B′ (1). Then, we can rewrite Z′ (l) = (W(l) −α∇W(l)L(W,B))(B(l) −α∇B(l)L(W,B)) ···(B(1) −α∇B(1)L(W,B)). By expanding the multiplications, this can be written as: Z′ (l) = Z(l) −α∇W(l)L(W,B)B(l) ···B(1) −α l∑ i=1 W(l)B(l) ···B(i+1)∇B(i)L(W,B)B(i−1) ···B(1) + O(α2) By vectorizing both sides, vec[Z′ (l)] −vec[Z(l)] = −αvec[∇W(l)L(W,B)B(l) ···B(1)] −α l∑ i=1 vec[W(l)B(l) ···B(i+1)∇B(i)L(W,B)B(i−1) ···B(1)] + O(α2) Here, using the formula of ∇W(l)L(W,B) and ∇B(l)L(W,B), we have that vec[∇W(l)L(W,B)B(l) ···B(1)] = vec[∇(l)L(W,B)(B(l) ...B (1))⊤B(l) ···B(1)] = [(B(l) ...B (1))⊤B(l) ···B(1) ⊗Imy ] vec[∇(l)L(W,B)], and l∑ i=1 vec[W(l)B(l) ···B(i+1)∇B(i)L(W,B)B(i−1) ···B(1)]Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = l∑ i=1 vec [ W(l)B(l) ···B(i+1) H∑ k=i (W(k)B(k) ···B(i+1))⊤∇(k)L(W,B)(B(i−1) ...B (1))⊤B(i−1) ···B(1) ] = l∑ i=1 H∑ k=i vec [ W(l)B(l) ···B(i+1)(W(k)B(k) ···B(i+1))⊤∇(k)L(W,B)(B(i−1) ...B (1))⊤B(i−1) ···B(1) ] = l∑ i=1 H∑ k=i [(B(i−1) ...B (1))⊤B(i−1) ···B(1) ⊗W(l)B(l) ···B(i+1)(W(k)B(k) ···B(i+1))⊤] vec [ ∇(k)L(W,B) ] . Summarizing above, vec[Z′ (l)] −vec[Z(l)] = −α[(B(l) ...B (1))⊤B(l) ···B(1) ⊗Imy ] vec[∇(l)L(W,B)] −α l∑ i=1 H∑ k=i [(B(i−1) ...B (1))⊤B(i−1) ···B(1) ⊗W(l)B(l) ···B(i+1)(W(k)B(k) ···B(i+1))⊤] vec [ ∇(k)L(W,B) ] + O(α2) Therefore, the induced continuous dynamics of Z(l) = W(l)B(l)B(l−1) ···B(1) is d dtvec[Z(l)] = −F(l) vec[∇(l)L(W,B)] − l∑ i=1 H∑ k=i J⊤ (i,l)J(i,k) vec [ ∇(k)L(W,B) ] where F(l) = [(B(l) ...B (1))⊤B(l) ···B(1) ⊗Imy ], and J(i,l) = [B(i−1) ...B (1) ⊗(W(l)B(l) ···B(i+1))⊤]. This is because J⊤ (i,k)J(i,k) = [(B(i−1) ...B (1))⊤⊗W(l)B(l) ···B(i+1)][B(i−1) ...B (1) ⊗(W(k)B(k) ···B(i+1))⊤] = [(B(i−1) ...B (1))⊤B(i−1) ...B (1) ⊗W(l)B(l) ···B(i+1)(W(k)B(k) ···B(i+1))⊤]. A.4.6. D YNAMICS INDUCED INT THE SPACE OF LOSS VALUE L(W,B) We now analyze the dynamics induced int the space of loss valueL(W,B). Deﬁne L(W,B) := ℓ(f(X,W,B ),Y ), where ℓis chosen later. Using chain rule, d dtL(W,B) = d dtL0(Z(H),...,Z (0)) = H∑ l=0 ∂L0(Z(l),...,Z (0)) ∂vec[Z(l)] dvec[Z(l)] dt , where L0(Z(H),...,Z (0)) = ℓ(f0(X,Z),Y ), f0(X,Z) = H∑ l=0 Z(l)XSl, and Z(l) = W(l)B(l)B(l−1) ···B(1). Since f0(X,Z) = f(X,W,B ) = ˆY and L0(Z(H),...,Z (0)) = L(W,B), (∂L0(Z(l),...,Z (0)) ∂vec[Z(l)] )⊤ = ( ∂L(W,B) ∂vec[ˆY] ∂vec[ˆY] ∂vec[Z(l)] )⊤Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = ( ∂L(W,B) ∂vec[ˆY] ( ∂ ∂vec[Z(l)] H∑ k=0 [(XSk)⊤⊗Imy ] vec[Z(k)] ))⊤ = [XSl ⊗Imy ] vec [∂L(W,B) ∂ˆY ] = vec [∂L(W,B) ∂ˆY (XSl)⊤ ] = vec[∇(l)L(W,B)] Therefore, d dtL(W,B) = H∑ l=0 vec[∇(l)L(W,B)]⊤dvec[Z(l)] dt = − H∑ l=0 vec[∇(l)L(W,B)]⊤F(l) vec[∇(l)L(W,B)] − H∑ l=1 l∑ i=1 H∑ k=i vec[∇(l)L(W,B)]⊤J⊤ (i,l)J(i,k) vec [ ∇(k)L(W,B) ] To simplify the second term, deﬁne M(l,i) = ∑H k=ivec[∇(l)L(W,B)]⊤J⊤ (i,l)J(i,k) vec [ ∇(k)L(W,B) ] and note that we can expand the double sums and regroup terms as follows: H∑ l=1 l∑ i=1 M(l,i) = H∑ l=1 M(l,1) + H∑ l=2 M(l,2) + ··· + H∑ l=H M(l,H) = H∑ i=1 H∑ l=i M(l,i). Moreover, for each i∈{1,...,H }, H∑ l=i M(l,i) = H∑ l=i H∑ k=i vec[∇(l)L(W,B)]⊤J⊤ (i,l)J(i,k) vec [ ∇(k)L(W,B) ] = (H∑ l=i J(i,l) vec[∇(l)L(W,B)] )⊤(H∑ k=i J(i,k) vec [ ∇(k)L(W,B) ] ) =  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 Using these facts, the second term can be simpliﬁed as H∑ l=1 l∑ i=1 H∑ k=i vec[∇(l)L(W,B)]⊤J⊤ (i,l)J(i,k) vec [ ∇(k)L(W,B) ] = H∑ l=1 l∑ i=1 M(l,i) = H∑ i=1 H∑ l=i M(l,i) = H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 Combining these, d dtL(W,B) = − H∑ l=0 vec[∇(l)L(W,B)]⊤F(l) vec[∇(l)L(W,B)] − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 (53)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Since F(l) is real symmetric and positive semideﬁnite, d dtL(W,B) ≤− H∑ l=0 λmin(F(l))∥vec[∇(l)L(W,B)]∥2 2 − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 (54) A.4.7. C OMPLETING THE PROOF BY USING THE ASSUMPTION OF THE SQUARE LOSS Using the assumption that L(W,B) = ℓ(f(X,W,B ),Y ) = ∥f(X,W,B ) −Y∥2 F with ˆY = f(X,W,B ), we have ∂L(W,B) ∂ˆY = ∂ ∂ˆY ∥ˆY −Y∥2 F = 2( ˆY −Y) ∈Rmy×n, and hence vec[∇(l)L(W,B)] = vec [∂L(W,B) ∂ˆY (XSl)⊤ ] = 2 vec [ ( ˆY −Y)(XSl)⊤ ] = 2[XSl ⊗Imy ] vec[ˆY −Y]. Therefore, ∥vec[∇(l)L(W,B)]∥2 2 = 4 vec[ˆY −Y]⊤[(XSl)⊤XSl ⊗Imy ] vec[ˆY −Y]. (55) We are now ready to complete the proof of Theorem 2 for each cases (i), (ii) and (iii). A.4.8. C ASE (I): C OMPLETING THE PROOF OF THEOREM 2 (I) Using equation (54) and (55) with λW,B = min0≤l≤H λmin(F(l)), we have that d dtL(W,B) ≤−λW,B H∑ l=0 ∥vec[∇(l)L(W,B)]∥2 2 − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 ≤−4λW,B H∑ l=0 vec[ˆY −Y]⊤[(XSl)⊤XSl ⊗Imy ] vec[ˆY −Y] − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 ≤−4λW,Bvec[ˆY −Y]⊤ [(H∑ l=0 (XSl)⊤XSl ) ⊗Imy ] vec[ˆY −Y] − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 = −4λW,Bvec[ˆY −Y]⊤[ G⊤ HGH ⊗Imy ] vec[ˆY −Y] − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 where the last line follows from the following fact: G⊤ HGH =   X XS ... XSH   ⊤  X XS ... XSH  = H∑ l=0 (XSl)⊤XSl. Decompose vec[ˆY−Y] as vec[ˆY−Y] = v+v⊥, where v= PG⊤ H⊗Imy vec[ˆY−Y], v⊥= (Imyn−PG⊤ H⊗Imy ) vec[ˆY−Y], and PG⊤ H⊗Imy ∈Rmyn×mynrepresents the orthogonal projection onto the column space ofG⊤ H⊗Imy ∈Rmyn×(H+1)mymx. Then, vec[ˆY −Y]⊤[ G⊤ HGH ⊗Imy ] vec[ˆY −Y] = (v+ v⊥)⊤[ G⊤ H ⊗Imy ][ GH ⊗Imy ] (v+ v⊥) = v⊤[ G⊤ H ⊗Imy ][ GH ⊗Imy ] v ≥σ2 min(GH)∥PG⊤ H⊗Imy vec[ˆY −Y]∥2 2Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = σ2 min(GH)∥PG⊤ H⊗Imy vec[ˆY] −PG⊤ H⊗Imy vec[Y]∥2 2 = σ2 min(GH)∥vec[ˆY] −PG⊤ H⊗Imy vec[Y] ±vec[Y]∥2 2 = σ2 min(GH)∥vec[ˆY] −vec[Y] + (Imyn −PG⊤ H⊗Imy ) vec[Y]∥2 2 ≥σ2 min(GH)(∥vec[ˆY −Y]∥2 −∥(Imyn −PG⊤ H⊗Imy ) vec[Y]∥2)2 ≥σ2 min(GH)(∥vec[ˆY −Y]∥2 2 −∥(Imyn −PG⊤ H⊗Imy ) vec[Y]∥2 2, where we used the fact that the singular values of [ G⊤ H ⊗Imy ] are products of singular values of GH and Imy . By noticing that L(W,B) = ∥vec[ˆY −Y]∥2 2 and L∗ 1:H = ∥(Imyn −PG⊤ H⊗Imy ) vec[Y]∥2 2 , vec[ˆY −Y]⊤[ G⊤ HGH ⊗Imy ] vec[ˆY −Y] ≥σ2 min(GH)(L(W,B) −L∗ 1:H). Therefore, d dtL(W,B) ≤−4λW,Bvec[ˆY −Y]⊤[ G⊤ HGH ⊗Imy ] vec[ˆY −Y] − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 ≤−4λW,Bσ2 min(GH)(L(W,B) −L∗ 1:H) − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 Since d dtL∗ 1:H = 0, d dt(L(W,B) −L∗ 1:H) ≤−4λW,Bσ2 min(GH)(L(W,B) −L∗ 1:H) − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 By deﬁning L = L(W,B) −L∗ 1:H, dL dt ≤−4λW,Bσ2 min(GH)L − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 (56) Since d dtL ≤0 and L ≥0, if L = 0 at some time ¯t, then L = 0 for any time t≥¯t. Therefore, if L = 0 at some time ¯t, then we have the desired statement of this theorem for any time t≥¯t. Thus, we can focus on the time interval [0,¯t] such that L >0 for any time t∈[0,¯t] (here, it is allowed to have ¯t= ∞). Thus, focusing on the time interval with L >0 , equation (56) implies that 1 L dL dt ≤−4λW,Bσ2 min(GH) −1 L H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 By taking integral over time ∫ T 0 1 L dL dtdt≤− ∫ T 0 4λW,Bσ2 min(GH)dt− ∫ T 0 1 L H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 dt By using the substitution rule for integrals, ∫T 0 1 L dL dtdt= ∫LT L0 1 LdL = log(LT) −log(L0), where L0 = L(W0,B0) −L∗ 1:H and LT = L(WT,BT) −L∗ 1:H. Thus, log(LT) −log(L0) ≤−4σ2 min(GH) ∫ T 0 λW,Bdt− ∫ T 0 1 L H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 dtOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth which implies that LT ≤elog(L0)−4σ2 min(GH) ∫T 0 λW,Bdt− ∫T 0 1 L ∑H i=1∥ ∑H l=i J(i,l) vec[∇(l)L(W,B)]∥ 2 2dt = L0e−4σ2 min(GH) ∫T 0 λW,Bdt− ∫T 0 1 L ∑H i=1∥ ∑H l=i J(i,l) vec[∇(l)L(W,B)]∥ 2 2dt By recalling the deﬁnition of L = L(W,B) −L∗ 1:H and that d dtL ≤0, we have that if L(WT,BT) −L∗ 1:H > 0, then L(Wt,Bt) −L∗ 1:H >0 for all t∈[0,T], and L(WT,BT) −L∗ 1:H ≤(L(W0,B0) −L∗ 1:H)e−4σ2 min(GH) ∫T 0 λWt,Btdt− ∫T 0 1 L(Wt,Bt)−L∗ ∑H i=1∥ ∑H l=i J(i,l) vec[∇(l)L(Wt,Bt)]∥ 2 2dt. By noticing that λ(1:H) T = inf t∈[0,T] λWt,Bt and that ∫T 0 1 L(Wt,Bt)−L∗ ∑H i=1 ∑H l=iJ(i,l) vec[∇(l)L(Wt,Bt)]  2 2 dt ≥0, this implies that L(WT,BT) −L∗ 1:H ≤(L(W0,B0) −L∗ 1:H)e−4λ(1:H) T σ2 min(GH)T− ∫T 0 1 L(Wt,Bt)−L∗ ∑H i=1∥ ∑H l=i J(i,l) vec[∇(l)L(Wt,Bt)]∥ 2 2dt ≤(L(W0,B0) −L∗ 1:H)e−4λ(1:H) T σ2 min(GH)T. This completes the proof of Theorem 2 (i) for the case of I= [n]. Since every step in this proof is valid when we replace f(X,W,B ) by f(X,W,B )∗Iand XSl by X(Sl)∗Iwithout using any assumption on Sor the relation between Sl−1 and S, our proof also yields for the general case of Ithat L(WT,BT) −L∗ 1:H ≤(L(W0,B0) −L∗ 1:H)e−4λ(1:H) T σ2 min((GH)∗I)T. A.4.9. C ASE (II): C OMPLETING THE PROOF OF THEOREM 2 (II) Using equation (54) and (55) , we have that for any H′∈{0,1,...,H }, d dtL(W,B) ≤−λmin(F(H′))∥vec[∇(H′)L(W,B)]∥2 2 ≤−4λmin(F(H′)) vec[ˆY −Y]⊤[(XSH′ )⊤XSH′ ⊗Imy ] vec[ˆY −Y] = −4λW,Bvec[ˆY −Y]⊤ [ ˜G⊤ H′˜GH′⊗Imy ] vec[ˆY −Y], where λW,B := λmin(F(H′)), and ˜GH′ := XSH′ . Decompose vec[ˆY−Y] as vec[ˆY−Y] = v+v⊥, where v= P˜G⊤ H′⊗Imy vec[ˆY−Y], v⊥= (Imyn−P˜G⊤ H′⊗Imy ) vec[ˆY−Y], and P˜G⊤ H′⊗Imy ∈Rmyn×myn represents the orthogonal projection onto the column space of ˜G⊤ H′⊗Imy ∈Rmyn×mymx. Then, vec[ˆY −Y]⊤ [ ˜G⊤ H′˜GH′⊗Imy ] vec[ˆY −Y] = (v+ v⊥)⊤ [ ˜G⊤ H′⊗Imy ][ ˜GH′⊗Imy ] (v+ v⊥) = v⊤ [ ˜G⊤ H′⊗Imy ][ ˜GH′⊗Imy ] v ≥σ2 min( ˜GH′)∥P˜G⊤ H′⊗Imy vec[ˆY −Y]∥2 2 = σ2 min( ˜GH′)∥P˜G⊤ H′⊗Imy vec[ˆY] −P˜G⊤ H′⊗Imy vec[Y]∥2 2 = σ2 min( ˜GH′)∥vec[ˆY] −P˜G⊤ H′⊗Imy vec[Y] ±vec[Y]∥2 2 = σ2 min( ˜GH′)∥vec[ˆY] −vec[Y] + (Imyn −P˜G⊤ H′⊗Imy ) vec[Y]∥2 2Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth ≥σ2 min( ˜GH′)(∥vec[ˆY −Y]∥2 −∥(Imyn −P˜G⊤ H′⊗Imy ) vec[Y]∥2)2 ≥σ2 min( ˜GH′)(∥vec[ˆY −Y]∥2 2 −∥(Imyn −P˜G⊤ H′⊗Imy ) vec[Y]∥2 2, where we used the fact that the singular values of [ ˜G⊤ H′⊗Imy ] are products of singular values of ˜GH′ and Imy . By noticing that L(W,B) = ∥vec[ˆY −Y]∥2 2 and L∗ H′ = ∥(Imyn −P˜G⊤ H′⊗Imy ) vec[Y]∥2 2 , we have that for any H′ ∈ {0,1,...,H }, vec[ˆY −Y]⊤ [ ˜G⊤ H′˜GH′⊗Imy ] vec[ˆY −Y] ≥σ2 min( ˜GH′)(L(W,B) −L∗ H′). (57) Therefore, d dtL(W,B) ≤−4λW,Bvec[ˆY −Y]⊤ [ ˜G⊤ H′˜GH′⊗Imy ] vec[ˆY −Y] ≤−4λW,Bσ2 min( ˜GH′)(L(W,B) −L∗ H′) Since d dtL∗ H′ = 0, d dt(L(W,B) −L∗ H′) ≤−4λW,Bσ2 min( ˜GH′)(L(W,B) −L∗ H′) By deﬁning L = L(W,B) −L∗ H′, dL dt ≤−4λW,Bσ2 min( ˜GH′)L (58) Since d dtL ≤0 and L ≥0, if L = 0 at some time ¯t, then L = 0 for any time t≥¯t. Therefore, if L = 0 at some time ¯t, then we have the desired statement of this theorem for any time t≥¯t. Thus, we can focus on the time interval [0,¯t] such that L >0 for any time t∈[0,¯t] (here, it is allowed to have ¯t= ∞). Thus, focusing on the time interval with L >0 , equation (58) implies that 1 L dL dt ≤−4λW,Bσ2 min( ˜GH′) By taking integral over time ∫ T 0 1 L dL dtdt≤− ∫ T 0 4λW,Bσ2 min( ˜GH′)dt By using the substitution rule for integrals, ∫T 0 1 L dL dtdt= ∫LT L0 1 LdL = log(LT) −log(L0), where L0 = L(W0,B0) −L∗ and LT = L(WT,BT) −L∗ H′. Thus, log(LT) −log(L0) ≤−4σ2 min( ˜GH′) ∫ T 0 λW,Bdt which implies that LT ≤elog(L0)−4σ2 min( ˜GH′) ∫T 0 λW,Bdt = L0e−4σ2 min( ˜GH′) ∫T 0 λW,Bdt By recalling the deﬁnition of L = L(W,B) −L∗ H′ and that d dtL ≤0, we have that if L(WT,BT) −L∗ H′ > 0, then L(Wt,Bt) −L∗ H′ >0 for all t∈[0,T], and L(WT,BT) −L∗ H′ ≤(L(W0,B0) −L∗ H′)e−4σ2 min( ˜GH′) ∫T 0 λWt,Btdt. By noticing that λ(H′) T = inft∈[0,T] λWt,Bt, this implies that for any H′∈{0,1,...,H }, L(WT,BT) −L∗ H′ ≤(L(W0,B0) −L∗ H′)e−4λ(H′) T σ2 min( ˜GH′)TOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = (L(W0,B0) −L∗ H′)e−4λ(H) T σ2 min(XSH′ )T This completes the proof of Theorem 2 (ii) for the case of I= [n]. Since every step in this proof is valid when we replace f(X,W,B ) by f(X,W,B )∗Iand XSl by X(Sl)∗Iwithout using any assumption on Sor the relation between Sl−1 and S, our proof also yields for the general case of Ithat L(WT,BT) −L∗ H′ ≤(L(W0,B0) −L∗ H′)e−4λ(H) T σ2 min(X(SH′ )∗I)T A.4.10. C ASE (III ): C OMPLETING THE PROOF OF THEOREM 2 (III ) In this case, we have the following assumption: there exist l,l′∈{0,...,H }with l<l ′such that L∗ l ≥L∗ l+1 ≥···≥ L∗ l′ or L∗ l ≤L∗ l+1 ≤···≤ L∗ l′. Using equation (54) and (55) with ˜Gl = XSl, we have that d dtL(W,B) ≤− H∑ l=0 λmin(F(l))∥vec[∇(l)L(W,B)]∥2 2 ≤−4 H∑ l=0 λmin(F(l)) vec[ˆY −Y]⊤[(XSl)⊤XSl ⊗Imy ] vec[ˆY −Y] = −4 H∑ l=0 λmin(F(l)) vec[ˆY −Y]⊤[ ˜G⊤ l ˜Gl ⊗Imy ] vec[ˆY −Y] Using (57), since vec[ˆY −Y]⊤ [ ˜G⊤ l ˜Gl ⊗Imy ] vec[ˆY −Y] ≥σ2 min( ˜Gl)(L(W,B) −L∗ l) for any l∈{0,1,...,H }, d dtL(W,B) ≤−4 H∑ l=0 λmin(F(l))σ2 min( ˜Gl)(L(W,B) −L∗ l). (59) Let l′′ = lif L∗ l ≥L∗ l+1 ≥···≥ L∗ l′, and l′′ = l′if L∗ l ≤L∗ l+1 ≤···≤ L∗ l′. Then, using (59) and the assumption of L∗ l ≥L∗ l+1 ≥···≥ L∗ l′ or L∗ l ≤L∗ l+1 ≤···≤ L∗ l′ for some l,l′∈{0,...,H }, we have that d dtL(W,B) ≤−4(L(W,B) −L∗ l′′) l′ ∑ k=l λmin(F(k))σ2 min( ˜Gk). (60) Since d dtL∗ l′′ = 0, d dt(L(W,B) −L∗ l′′) ≤−4(L(W,B) −L∗ l′′) l′ ∑ k=l λmin(F(k))σ2 min( ˜Gk). By taking integral over time in the same way as that in the proof for the case of (i) and (ii), we have that L(WT,BT) −L∗ l′′ ≤(L(W0,B0) −L∗ l′′)e−4 ∑l′ k=l σ2 min( ˜Gk) ∫T 0 λmin(F(k),t)dt (61) Using the property of Kronecker product, λmin(F(l),t) = λmin([(B(l),t...B (1),t)⊤B(l),t···B(1),t ⊗Imy ]) = λmin((B(l),t...B (1),t)⊤B(l),t···B(1),t), which implies that λ(k) T = inft∈[0,T] λmin(F(k),t). Therefore, equation (61) with λ(k) T = inft∈[0,T] λmin(F(k),t) yields that L(WT,BT) −L∗ l′′ ≤(L(W0,B0) −L∗ l′′)e−4 ∑l′ k=l λ(k) T σ2 min( ˜Gk)T = (L(W0,B0) −L∗ l′′)e−4 ∑l′ k=l λ(k) T σ2 min(XSk)T (62)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth This completes the proof of Theorem 2 (iii) for the case of I= [n]. Since every step in this proof is valid when we replace f(X,W,B ) by f(X,W,B )∗Iand XSl by X(Sl)∗Iwithout using any assumption on Sor the relation between Sl−1 and S, our proof also yields for the general case of Ithat L(WT,BT) −L∗ l′′ ≤(L(W0,B0) −L∗ l′′)e−4 ∑l′ k=l λ(k) T σ2 min(X(Sk)∗I)T. A.5. Proof of Proposition 3 From Deﬁnition 4, for any l∈{1,2,...,H }, we have that σmin( ¯B(1:l)) = σmin(B(l)B(l−1) ···B(1)) ≥γfor all (W,B) such that L(W,B) ≤L(W0,B0). From equation (54) in the proof of Theorem 2, it holds that d dtL(Wt,Bt) ≤0 for all t. Thus, we have that L(Wt,Bt) ≤L(W0,B0) and hence σmin( ¯B(1:l) t ) ≥γfor all t. Under this problem setting (ml ≥mx), this implies that λmin(( ¯B(1:l) t )⊤¯B(1:l) t ) ≥γ2 for all tand thus λ(1:H) T ≥γ2. A.6. Proof of Theorem 3 The proof of Theorem 3 follows from the intermediate results of the proofs of Theorem 1 and Theorem 2 as we show in the following. For the non-multiscale case, from equation (36) in the proof of Theorem 1, we have that d dtL1(W,B) = −∥vec[∇(H)L(W,B)]∥2 F(H) − H∑ i=1 J(i,H) vec[∇(H)L(W,B)] 2 2 where ∥vec[∇(H)L(W,B)]∥2 F(H) := vec[∇(H)L(W,B)]⊤F(H) vec[∇(H)L(W,B)]. Since equation (36) in the proof of Theorem 2 is derived without the assumption on the square loss, this holds for any differentiable loss ℓ. By noticing that ∇(H)L(W,B) = V(X(SH)∗I)⊤, we have that d dtL1(W,B) = −∥vec[V(X(SH)∗I)⊤]∥2 F(H) − H∑ i=1 J(i,H) vec[V(X(SH)∗I)⊤] 2 2 . This proves the statement of Theorem 3 (i). For the multiscale case, from equation (53) in the proof of Theorem 2, we have that d dtL2(W,B) = − H∑ l=0 ∥vec[∇(l)L(W,B)]∥2 F(l) − H∑ i=1  H∑ l=i J(i,l) vec[∇(l)L(W,B)]  2 2 (63) where ∥vec[∇(l)L(W,B)]∥2 F(l) := vec[∇(l)L(W,B)]⊤F(l) vec[∇(l)L(W,B)]. Since equation (53) in the proof of Theorem 2 is derived without the assumption on the square loss, this holds for any differentiable loss ℓ. Since every step to derive equation (53) is valid when we replace f(X,W,B ) by f(X,W,B )∗Iand XSl by X(Sl)∗Iwithout using any assumption on Sor the relation between Sl−1 and S, the steps to derive equation (53) also yields this for the general case of I: i.e., ∇(l)L(W,B) = V(X(Sl)∗I)⊤. Thus, we have that d dtL1(W,B) = − H∑ l=0 ∥vec[V(X(Sl)∗I)⊤]∥2 F(l) − H∑ i=1  H∑ l=i J(i,l) vec[V(X(Sl)∗I)⊤]  2 2 This completes the proof of Theorem 3 (ii). B. Additional Experimental Results In this section, we present additional experimental results.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (a) Linear and Cora. 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (b) ReLU and Cora. 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (c) Linear and Citeseer. 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (d) ReLU and Citeseer. Figure 6.Multiscale skip connection accelerates GNN training. We plot the training curves of GNNs with ReLU and linear activation on the Cora and Citeseer dataset. We use the GCN model with learning rate 5e− 5, six layers, and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (d) ReLU and multiscale. Figure 7.Depth accelerates GNN training . We plot the training curves of GNNs with ReLU and linear activation, multiscale and non-multiscale on the Cora dataset. We use the GCN model with learning rate 5e− 5 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (d) ReLU and multiscale. Figure 8.Depth accelerates GNN training . We plot the training curves of GNNs with ReLU and linear activation, multiscale and non-multiscale on the Citeseer dataset. We use the GCN model with learning rate 5e− 5 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (d) ReLU and multiscale. Figure 9.GNNs train faster when the labels have signal instead of random noise . We plot the training curves of multiscale and non-multiscale GNNs with ReLU and linear activation, on the Cora dataset. We use the two-layer GCN model with learning rate 1e− 4 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (d) ReLU and multiscale. Figure 10.GNNs train faster when the labels have signal instead of random noise . We plot the training curves of multiscale and non-multiscale GNNs with ReLU and linear activation, on the Citeseer dataset. We use the two-layer GCN model with learning rate 1e− 4 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 3000 7000 10000 Iteration 10 3 10 2 10 1 100 Training Loss linear GIN ReLU GIN (a) Linear GIN vs. ReLU GIN. 0 3000 7000 10000 Iteration 10 2 10 1 100 Training Loss linear GCN ReLU GCN (b) Linear GCN vs. ReLU GCN. Figure 11.Linear GNNs vs. ReLU GNNs. We plot the training curves of GCN and GIN with ReLU and linear activation on the Cora dataset. The training curves of linear GNNs and ReLU GNNs are similar, both converging to nearly zero training loss with the same linear rate. Moreover, GIN trains faster than GCN, which agrees with our bound in Theorem 1. We use the learning rate 1e− 4, two layers, and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth C. Experimental Setup In this section, we describe the experimental setup for reproducing our experiments. Dataset. We perform all experiments on the Cora and Citeseer datasets (Sen et al., 2008). Cora and Citeer are citation networks and the goal is to classify academic documents into different subjects. The dataset contains bag-of-words features for each document (node) and citation links (edges) between documents. The tasks are semi-supervised node classiﬁcation. Only a subset of nodes have training labels. In our experiments, we use the default dataset split, i.e., which nodes have training labels, and minimize the training loss accordingly. Tabel 1 shows an overview of the dataset statistics. Dataset Nodes Edges Classes Features Citeseer 3,327 4,732 6 3,703 Cora 2,708 5,429 7 1,433 Table 1.Dataset statistics Training details. We describe the training settings for our experiments. Let us ﬁrst describe some common hyperparame- ters and settings, and then for each experiment or ﬁgure we describe the other hyperparameters. For our experiments, to more closely align with the common practice in GNN training, we use the Adam optimizer and keep optimizer-speciﬁc hyperparameters except initial learning rate default. We set weight decay to zero. Next, we describe the settings for each experiment respectively. For the experiment in Figure 1, i.e., the training curves of linear vs. ReLU GNNs, we train the GCN and GIN with two layers on Cora with cross-entropy loss and learning rate 1e-4. We set the hidden dimension to 32. For the experiment in Figure 2a, i.e., computing the graph condition for linear GNNs, we use the linear GCN and GIN model with three layers on Cora and Citeseer. For linear GIN, we set ϵto zero and MLP layer to one. For the experiment in Figure 2b, i.e., computing and plotting the time-dependent condition for linear GNNs, we train a linear GCN with two layers on Cora with squared loss and learning rate 1e-4. We set the hidden dimension the input dimension for both Cora and for CiteSeer, because the global convergence theorem requires the hidden dimension to be at least the same as input dimension. Note that this requirement is standard in previous works as well, such as Arora et al. (2019a). We use the default random initialization of PyTorch. The formula for computing the time-dependent λT is given in the main paper. For the experiment in Figure 2c, i.e., computing and plotting the time-dependent condition for linear GNNs across multiple training settings, we consider the following settings: 1. Dataset: Cora and Citeseer. 2. Model: GCN and GIN. 3. Depth: Two and four layers. 4. Activation: Linear and ReLU. We train the GNN with the settings above with squared loss and learning rate 1e-4. We set the hidden dimension to input dimension for Cora and CiteSeer. We use the default random initialization of PyTorch. The formula for computing the time-dependent λT is given in the main paper. For each point, we report the λT at last epoch. For the experiment in Figure 3a, i.e., computing the graph condition for multiscale linear GNNs, we use the linear GCN and GIN model with three layers on Cora and Citeseer. For linear GIN, we set ϵto zero and MLP layer to one. For the experiment in Figure 3b, i.e., computing and plotting the time-dependent condition for multiscale linear GNNs, we train a linear GCN with two layers on Cora with squared loss and learning rate 1e-4. We set the hidden dimension to 2000 for Cora and 4000 for CiteSeer. We use the default random initialization of PyTorch. The formula for computing the time-dependent λT is given in the main paper. For the experiment in Figure 3c, i.e., computing and plotting the time-dependent condition for multiscale linear GNNs across multiple training settings, we consider the following settings:Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 1. Dataset: Cora and Citeseer. 2. Model: Multiscale GCN and GIN. 3. Depth: Two and four layers. 4. Activation: Linear and ReLU. We train the multiscale GNN with the settings above with squared loss and learning rate 1e-4. We set the hidden dimension to 2000 for Cora and 4000 for CiteSeer. We use the default random initialization of PyTorch. The formula for computing the time-dependent λT is given in the main paper. For each point, we report the λT at last epoch. For the experiment in Figure 4a, i.e., multiscale vs. non-multiscale, we train the GCN with six layers and ReLU activation on Cora with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. We perform more extensive experiments to verify the conclusion for multiscale vs. non-multiscale in Figure 11. There, we train the GCN with six layers with both ReLU and linear activation on both Cora and Citeseer with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. For the experiment in Figure 4b, i.e., acceleration with depth, we train the non-multiscale GCN with two, four, six layers and ReLU activation on Cora with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. We perform more extensive experiments to verify the conclusion for acceleration with depth in Figure 7 and Figure 8. There, we train both multiscale and non-multiscale GCN with 2, 4, 6 layers with both ReLU and linear activation on both Cora and Citeseer with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. For the experiment in Figure 4c, i.e., signal vs. noise, we train the non-multiscale GCN with two layers and ReLU activation on Cora with cross-entropy loss and learning rate 1e-4. We set the hidden dimension to 32. For signal, we use the default labels of Cora. For noise, we randomly choose a class as the label. We perform more extensive experiments to verify the conclusion for signal vs. noise in Figure 9 and Figure 10. There, we train both multiscale and non-multiscale GCN with two layers with both ReLU and linear activation on both Cora and Citeseer with cross-entropy loss and learning rate 1e-4. We set the hidden dimension to 32. For the experiment in Figure 5, i.e., ﬁrst term vs. second term, we use the same setting as in Figure 4c. We use the formula of our Theorem in the main paper. Computing resources. The computing hardware is based on the CPU and the NVIDIA GeForce RTX 1080 Ti GPU. The software implementation is based on PyTorch and PyTorch Geometric (Fey & Lenssen, 2019). For all experiments, we train the GNNs with CPU and compute the eigenvalues with GPU.",
      "meta_data": {
        "arxiv_id": "2105.04550v2",
        "authors": [
          "Keyulu Xu",
          "Mozhi Zhang",
          "Stefanie Jegelka",
          "Kenji Kawaguchi"
        ],
        "published_date": "2021-05-10T17:59:01Z",
        "pdf_url": "https://arxiv.org/pdf/2105.04550v2.pdf"
      }
    },
    {
      "title": "Improving Breadth-Wise Backpropagation in Graph Neural Networks Helps Learning Long-Range Dependencies."
    },
    {
      "title": "DRew: Dynamically Rewired Message Passing with Delay",
      "abstract": "Message passing neural networks (MPNNs) have been shown to suffer from the\nphenomenon of over-squashing that causes poor performance for tasks relying on\nlong-range interactions. This can be largely attributed to message passing only\noccurring locally, over a node's immediate neighbours. Rewiring approaches\nattempting to make graphs 'more connected', and supposedly better suited to\nlong-range tasks, often lose the inductive bias provided by distance on the\ngraph since they make distant nodes communicate instantly at every layer. In\nthis paper we propose a framework, applicable to any MPNN architecture, that\nperforms a layer-dependent rewiring to ensure gradual densification of the\ngraph. We also propose a delay mechanism that permits skip connections between\nnodes depending on the layer and their mutual distance. We validate our\napproach on several long-range tasks and show that it outperforms graph\nTransformers and multi-hop MPNNs.",
      "full_text": "DRew: Dynamically Rewired Message Passing with Delay Benjamin Gutteridge 1 Xiaowen Dong 1 Michael Bronstein 2 Francesco Di Giovanni 3 4 Abstract Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node’s immediate neigh- bours. Rewiring approaches attempting to make graphs ‘more connected’, and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at ev- ery layer. In this paper we propose a framework, applicable to any MPNN architecture, that per- forms a layer-dependent rewiring to ensure grad- ual densiﬁcation of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on sev- eral long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs. 1. Introduction Graph Neural Networks (GNNs) (Sperduti, 1993; Gori et al., 2005; Scarselli et al., 2008; Bruna et al., 2014), deep learn- ing architectures that operate on graph-structured data, are signiﬁcantly represented by the message-passing paradigm (Gilmer et al., 2017), in which layers consisting of a local neighbourhood aggregation are stacked to form Message Passing Neural Networks (MPNNs). The most commonly used MPNNs (henceforth referred to as ‘classical’), perform only local aggregation, with information being shared at each layer only between nodes that are immediate neigh- bours (i.e., directly connected by an edge). Accordingly, 1Department of Engineering Science, University of Ox- ford 2Department of Computer Science, University of Oxford 3Department of Computer Science and Technology, University of Cambridge 4Faculty of Informatics, University of Lugano. Corre- spondence to: Benjamin Gutteridge <beng@robots.ox.ac.uk>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). for nodes that are distant from one another to share infor- mation, that information must ‘ﬂow’ through the graph at a rate of one edge per layer, necessitating appropriately deep networks when such ‘long-range interactions’ are re- quired for solving the task at hand (Barcel ´o et al., 2019). Unfortunately, this often leads to poor model performance, as deep MPNNs are particularly prone to the phenomena of over-squashing (Alon & Yahav, 2021; Topping et al., 2022; Di Giovanni et al., 2023) and over-smoothing (Nt & Maehara, 2019; Oono & Suzuki, 2020). In this paper, we introduceDynamically Rewired Message Passing (DRew), a novel framework for layer-dependent, multi-hop message passing that takes a principled approach to information ﬂow, is robust to over-squashing, and can be applied to any MPNN for deep learning on graphs. Contributions. First, we formalize DRew, a new frame- work of aggregating information over distant nodes that goes beyond the limitations of classical MPNNs, but re- spects the inductive bias provided by the graph: nodes that are closer should interact earlier in the architecture. Sec- ond, we introduce the concept of delay for message passing, controlled by a tunable parameter ν, and generalize DRew to account for delay in order to alleviate issues such as over- smoothing arising from deep MPNN-architectures; we call this framework νDRew. 1 Third, we present a theoretical analysis that proves that our proposed frameworks can miti- gate over-squashing. Lastly, we experimentally evaluate our framework on both synthetic and real-world datasets.2 Our experiments demonstrate the robustness of DRew and the effectiveness of delayed propagation when applied to deep MPNN architectures or long-range tasks. 2. Message Passing Neural Networks In this section we introduce the class of Message Passing Neural Networks and discuss some of its main limitations. We ﬁrst review some important notions concerning graphs. 2.1. Preliminaries Let G= (V,E) be a graph consisting of nodes V and edges E. We assume that Gis undirected and connected. The 1Pronounced ‘Andrew’. 2https://github.com/BenGutteridge/DRew 1 arXiv:2305.08018v2  [cs.LG]  18 May 2023DRew: Dynamically Rewired Message Passing with Delay (a) Classical MPNN  (b) DRew  (c) νDRew Figure 1.Illustration of the graph across three layers ℓ∈{0,1,2}for (a) a classical MPNN, (b) DRew and (c) νDRew. We choose a source node (coloured blue) on which to focus and demonstrate information ﬂow from this node at each layer. We use arrows to denote direction of information transfer and specify hop-connection distance. In the classicical MPNN setting, at every layer information only travels from a node to its immediate neighbours. In DRew, the graph changes based on the layer, with newly added edges connecting nodes at distance rfrom layer r−1 onward. Finally, in νDRew, we also introduce a delay mechanism equivalent to skip-connections between different nodes based on their mutual distance (see Section 3.3). structure of the graph is encoded in the adjacency matrix A ⊂Rn×n, with number of nodes n= |V|. The simplest quantity measuring the connectivity of a node is the degree, which can be computed as di = ∑ jAij, for i ∈V. The notion of ‘locality’ inGis induced by the shortest walk (or geodesic) distance dG : V ×V →R≥0, which assigns the length of the minimal walk connecting any given pair of nodes. If we ﬁx a node i, the distance allows us to partition the graph into level sets of dG(i,·) which we refer to as k-hop (or k-neighbourhood) and denote by Nk(i) :={j ∈V : dG(i,j) =k}. N1(i) is the set of immediate (or 1-hop) neighbours of node i. We stress that in our notations, the k-hop of a node i represents the nodes at distance exactly k— a subset of the nodes that can be reached by a walk of length k. The MPNN class. Consider a graph Gwith node features {hi ∈Rd,i ∈V}and assume we are interested in predict- ing a quantity (or label) yGi. Typically a GNN processes both the topological data G and the feature information H ∈Rn×d via a sequence of layers, before applying a read- out map to output a ﬁnal prediction hG. The most studied GNN paradigm is MPNNs (Gilmer et al., 2017), where the layer update is given by a(ℓ) i = AGG(ℓ) ( {h(ℓ) j : j ∈N1(i)} ) , h(ℓ+1) i = UP(ℓ) ( h(ℓ) i ,a(ℓ) i ) , (1) for learnable update and aggregation maps UP and AGG. While the choice of the maps UP and AGG may change across speciﬁc architectures (Bresson & Laurent, 2017; Hamilton et al., 2017; Kipf & Welling, 2017; Veli ˇckovi´c et al., 2018), in all MPNNs messages travel from one node to its 1-hop neighbours at each layer. Accordingly, for a node ito exchange information with node j ∈Nk(i), we need to stack at least klayers. In Section 3, we discuss how the interaction between two node representations should, in fact, change based on both their mutual distance and their state in time (i.e. the layer of the network). We argue that it is important not simply how two node states interact with each other, but also when that happens. 2.2. Long-range dependencies and network depth A task exhibitslong-range interactions if, to be solved, there exists some node iwhose representation needs to account for the information contained at a node jwith dG(i,j) ≫1 (Dwivedi et al., 2022). MPNNs rely on 1-hop message prop- agation, so to capture such non-local interactions, multiple layers must be stacked; however, this leads to undesirable phenomena with increasing network depth. We focus on one such problem, known as over-squashing, below. Over-squashing. In a graph, the number of nodes in the receptive ﬁeld of a node ioften expands exponentially with hop distance k. Accordingly, for i to exchange informa- tion with its k-hop neighbours, an exponential volume of messages must pass through ﬁxed-size node representations, which may ultimately lead to a loss of information (Alon & Yahav, 2021). This problem is known as over-squashing, and has been characterized via sensitivity analysis (Top- ping et al., 2022). Methods to address the over-squashing problem typically resort to some form of graph rewiring, 2DRew: Dynamically Rewired Message Passing with Delay in the sense that the graph used for message passing is (partly) decoupled from the input one. A ‘local’ form of graph rewiring consists in aggregating over multiple hops at each layer layer (Abu-El-Haija et al., 2019; 2020; Zhang & Li, 2021; Abboud et al., 2022). A ‘global’ form of graph rewiring is taken to the extreme in graph Transformers (Ying et al., 2021; Kreuzer et al., 2021; Ramp ´aˇsek et al., 2022), which replace the input graph with a complete graph where every pair of nodes is connected by an attention-weighted edge. Transformers, however, are computationally expen- sive and tend to throw away information afforded by the graph topology. Since all nodes can interact in a single layer, any notion of locality induced by distance dG is dis- carded and must be rediscovered implicitly via positional and structural encoding. Over-smoothing and other phenomena. The use of deep MPNNs gives rise to other issues beyond over- squashing. A well-known problem is over-smoothing, where, in the limit of many layers, features become indis- tinguishable (Nt & Maehara, 2019; Oono & Suzuki, 2020). While over-smoothing is now fairly understood and has been formally characterized in recent works (Bodnar et al., 2022; Cai & Wang, 2020; Di Giovanni et al., 2022; Rusch et al., 2022), it is unclear whether the often observed degradation in performance with increasing depth is mainly caused by over-smoothing, over-squashing, or more classical vanish- ing gradient problem (Di Giovanni et al., 2023). It is hence generally desirable to propose frameworks that are not just robust to depth, but can actually adapt to the underlying task, either by ‘fast’ exploration of the graph in fewer lay- ers or by ‘slow’ aggregation through multiple layers. We introduce a new framework for message-passing that can accomplish this thanks to two principles: (i) dynamically rewired message passing and (ii) a delay mechanism. 3. Dynamically Rewired MPNNs In this section we introduce our framework to handle the ag- gregation of messages in MPNNs. We discuss how MPNNs present a ‘static’ way of collecting messages at each layer which is ultimately responsible for over-squashing. By re- moving such static inductive bias, we unlock a physics- inspired way for MPNNs to exchange information that is more suited to handle long-range interactions. Information ﬂow in MPNNs. Consider two nodes i,j ∈ V at distance r. In a classic MPNN, these two nodes start interacting at layer r, meaning that min { ℓ: ∂h(ℓ) i ∂h(0) j ̸= 0 } ≥dG(i,j). (2) In fact, since the aggregation at each layer is computed using the same graph G, one can bound such interaction with powers of the adjacency A as used in Topping et al. (2022) ⏐⏐⏐⏐⏐ ∂h(r) i ∂h(0) j ⏐⏐⏐⏐⏐≤c(Ar)ij, (3) with constant cdepending only on the Lipschitz-regularity of the MPNN and independent of the graph topology. We see that communication between iand j must be ﬁltered by intermediate nodes that are traversed along each path connecting ito j. This, in a nutshell, is the reason behind over-squashing; indeed, the bound in Eq. (3) may decay exponentially with the distance r whenever A is degree- normalized. By the same argument, in an MPNN two nodes at distance ralways interact with a latency or delay of ex- actly r, meaning that for any intermediate state ℓ0 we have min { ℓ: ∂h(ℓ) i ∂h(ℓ0) j ̸= 0 } ≥ℓ0 + dG(i,j), (4) and similar Jacobian bounds apply in this case. Accordingly, in a classic MPNN we have two problems: (i) Distant nodes can only communicate by exchanging information with their neighbours. (ii) Distant nodes always interact with a ﬁxed delay given by their distance. Information ﬂow in multi-hop MPNNs. The ﬁrst issue can, in principle, be easily addressed by rewiring the graph via a process where any pair of nodes within a certain thresh- old are connected via an edge, and which can generally be given its own encoding or weight (Abboud et al., 2022; Br¨uel-Gabrielsson et al., 2022; Ramp´aˇsek et al., 2022). In this way, distant nodes can now exchange information di- rectly; this avoids iterating messages through powers of the adjacency and hence mitigates over-squashing by reducing the exponent in Eq. (3). However, this process brings about two phenomena which could lead to undesirable effects: (i) the computational graph is much denser — with implica- tions for efﬁciency — and (ii) most of the inductive bias afforded by the graph distance information is thrown away, given that nodes i,j at distance rare now able to interact at each layer of the architecture, without any form of latency. In particular, this static rewiring, where the computational graph is densely ‘ﬁlled’ from the ﬁrst MPNN layer, prevents messages from being sent ﬁrst among nodes that are closer together in the input graph. 3.1. A new framework: (ν)DRew message passing Dynamic rewiring. We start by addressing the limitation of MPNNs that nodes can only communicate through inter- mediate neighbours. To motivate our framework, take two 3DRew: Dynamically Rewired Message Passing with Delay nodes i,j ∈V at distance r> 1. For classical MPNNs, we must wait for r layers (i.e. time units with respect to the architecture) before iand jcan interact with each other. As argued above, this preserves information about locality and distance induced by the input graph, since nodes that are closer communicate earlier; however, since the two nodes have waited ‘long enough’, we argue that they should inter- act directly without necessarily relaying messages to their neighbours ﬁrst. Accordingly, given update and aggregation functions as per the MPNN paradigm in Eq. (1), we deﬁne the update in a Dynamically Rewired (DRew-)MPNN by: a(ℓ) i,k = AGG(ℓ) k ( {h(ℓ) j : j ∈Nk(i)} ) ,1 ≤k≤ℓ+ 1 h(ℓ+1) i = UP(ℓ) k ( h(ℓ) i ,a(ℓ) i,1,...,a (ℓ) i,ℓ+1 ) . (5) Some important comments: ﬁrst, if AGGk = I for each k >1, this reduces to the classical MPNN setting. Sec- ond, unlike augmented MPNNs, the sets over which we compute aggregation differ depending on the layer , with the hop Nk(i) only being added from the k-th layer on. So, while this framework shares similarities with other multi- hop MPNN architectures like Abboud et al. (2022), it fea- tures a novel mechanism: dynamic rewiring of the graph at each layer ℓto include aggregation from each k-hop within distance ℓ+ 1. For example, at the ﬁrst layer, ℓ = 0, our DRew layer is identical to the base MPNN represented by the choice of UP and AGG, but at each subsequent layer the receptive ﬁeld of node iexpands by 1 hop. This allows distant nodes to exchange information without intermedi- ate steps, hence solving one of the problems of the MPNN paradigm, but also preserving the inductive bias afforded by the topology since the graph is ﬁlled gradually according to distance rather than treating each layer in the same way. DRew-MPNN explores the middle ground between classical MPNNs and methods like graph Transformers that consider all pairwise interactions at once. The delay mechanism. Next, we generalize DRew- MPNN to also account for whether nodes should interact with ﬁxed (if any) delay. Currently, we have two opposing scenarios: in MPNNs, nodes interact with a constant delay given by their distance – leading to the same lag of infor- mation – while in DRew, nodes interact only from a certain depth of the architecture, but without any delay. For DRew, two nodes i,j at distance r communicate directly after r layers, since information has now been able to travel from j to i. But what if we consider the state ofjas it was when the information ‘left’ to ﬂow towardsi? We account for an extra degree of freedom τ representing the delay of messages ex- changed among nodes at distancerin DRew. If τ = 0, then nodes at distance kinteract at the k-th layer without delay, i.e instantly as per Eq. (5), otherwise node i‘sees’ the state of jat the k-th layer but delayed by τ := k−ν, for some ν. We formalize this by introducing τν(k) = max(0,k −ν) and generalize DRew as a(ℓ) i,k = AGG(ℓ) k ( {h(ℓ−τν(k)) j : j ∈Nk(i)} ) ,1 ≤k≤ℓ+ 1 h(ℓ+1) i = UP(ℓ) k ( h(ℓ) i ,a(ℓ) i,1,...,a (ℓ) i,ℓ+1 ) . (6) If there is no delay, i.e. ν = ∞, then we recover Eq. (5). The opposite case is given byν = 1, so that at layerℓand for any jat distance k, node ireceives ‘delayed’ representation h(ℓ−τ1(k)) j , i.e. the state of jas it was k−1 layers ago. From now on, we refer to Eq. (6) as νDRew. We also note that in our experiments we treat νas a tunable hyperparameter. Delay allows for expressive control of information ﬂow. No delay means that messages travel faster, with distant nodes interacting instantly once an edge is added; conversely, the more delay, the slower the information ﬂow, with distant nodes accessing past states when an edge is added. Our framework generalizes any MPNN since it acts on the computational graph (which nodes exchange information and when) and does not govern the architecture (see Sec- tion 3.3). We describe three instances of νDRew below. 3.2. Instances of our framework In this section we provide examples for the νDRew-MPNN template in Eq. (6) for three classical MPNNs: GCN (Kipf & Welling, 2017), GIN (Xu et al., 2019) and GatedGCN (Bresson & Laurent, 2017). We will use these variants for our experiments in Section 5. For νDRew-GCN, we write the layer-update as h(ℓ+1) i = h(ℓ) i + σ   ℓ+1∑ k=1 ∑ j∈Nk(i) W(ℓ) k γk ijh(ℓ−τν(k)) j  , (7) where σ is a pointwise nonlinearity, W(ℓ) k are learnable channel-mixing matrices for the convolution at layer ℓon the k-neighbourhood, and Γk ⊂Rn×n are matrices with elements γk ij = { 1√ didj , if dG(i,j) =k 0, otherwise. (8) We note again that ifj ∈Nk(i), then i,j only communicate from layer konward, while νdetermines the communication delay. The choice of degree normalization for Γk is to provide a consistent normalization for all terms. We deﬁne νDRew-GIN in a similar fashion, as per Eq. (6); the layer update used below is inspired by Brockschmidt 4DRew: Dynamically Rewired Message Passing with Delay (2020) and Abboud et al. (2022): h(ℓ+1) i = (1 +ϵ)MLP(ℓ) s (h(ℓ) i ) + ℓ+1∑ k=1 ∑ j∈Nk(i) MLP(ℓ) k (h(ℓ−τν(k)) j ), (9) where an MLP is one or more linear layers separated by ReLU activation and ϵis a weight parameter. MLP(ℓ) s is the self-loop (or residual) aggregation while MLP(ℓ) k operates on the k-neighbourhood at layer ℓ. Lastly, we deﬁne νDRew-GatedGCN as follows: h(ℓ+1) i = W(ℓ) 1 h(ℓ) i + ℓ+1∑ k=1 ∑ j∈Nk(i) ηk i,j ⊙W(ℓ) 2 h(ℓ−τν(k)) j , ηk i,j = ˆηk i,j∑ j∈Nk(i)(ˆηk i,j) +ϵ, ˆηk i,j = σ ( W(ℓ) 3 h(ℓ) i + W(ℓ) 4 h(ℓ−τν(k)) j ) , (10) where σis the sigmoid function, ⊙is the element-wise prod- uct, ϵis a small ﬁxed constant for numerical stability and Wℓ 1,Wℓ 2,Wℓ 3,Wℓ 4 are learned channel-mixing matrices. We note that in Eq. (10), unlike Eq. (7) and Eq. (9), weight matrices are shared between k-hop neighbourhoods. We do this because k-neighbourhood weight sharing achieves comparably strong results with non-weight-shared DRew- GatedGCN (see Section 5) while maintaining a lower param- eter count, whereas we see performance drops when using weight sharing for DRew-GCN and -GIN. This can be ex- plained by the edge gates ηk i,j serving as a soft-attention mechanism (Dwivedi et al., 2020) not present in GCN and GIN, affording shared weights more ﬂexibility to model relationships between nodes at varying hop distances. 3.3. The graph-rewiring perspective: νDRew as distance-aware skip-connections We conclude this section by providing an alternative expla- nation of νDRew from a graph-rewiring perspective. Given an underlying MPNN, we study how information travels in the graph at each layer. Referring to Figure 1 to illustrate our explanation, we say that messages travelhorizontally when they move inside a layer (slice), and that they move verti- cally when they travel across different layers (slices). In a classical MPNN, the graph adopted at each layer coincides with the input graph G; information can only travel hori- zontally from a node to its 1-hop neighbours. In the DRew setting — which we recall to be the version of νDRew with- out delay (i.e. ν = ∞) — the graph changes depending on the layer: this amounts to a dynamic rewiring where G is replaced with a sequence {Rk(G)}, where at each layer ℓ we add edges between any node i and Nℓ+1(i). Mes- sages only travel horizontally as before, but the graph is progressively ﬁlled with each layer. Finally, in the delayed version of Eq. (6), messages can travel both horizontally and vertically, meaning that we are also ‘rewiring’ the graph along the time (layer) axis. Residual connections can also be thought of as allowing information to move ‘vertically’, though such connections are only made between the same node iat different layers; typically ℓ,ℓ + 1. From this per- spective, the νDRew framework is equivalent to adding geometric skip connections among different nodes based on their distance. This is a powerful mechanism that com- bines skip-connections, a key tool in architecture design, with metric information provided by the geometry of the data; in this case the distances between vertices in a graph G. 4. Why νDRew Improves Information Processing Mitigating over-squashing. In this section we discuss why the νDRew framework mitigates over-squashing and is hence more suited to handle long-range interactions in a graph. We focus on the case of maximal delay ν = 1. We also restrict our discussion to Eq. (7): νDRew-GCN, though the conclusion extends easily to anyνDRew-MPNN. Consider nodes i,j ∈V at distance r. For a traditional MPNN, i,j ﬁrst exchange information at layer r, meaning that Eq. (2) is satisﬁed; however, a crucial difference from the MPNN paradigm is given by the addition of distance- aware skip connections between i,j as in Eq. (6). One can extend the approach from Topping et al. (2022) and derive ⏐⏐⏐⏐⏐ ∂h(r) i ∂h(0) j ⏐⏐⏐⏐⏐≤C ( ∑ k1+···+kℓ=r ( ∏ k1,...,kℓ (γk)ij )) , recalling that matricesΓkare deﬁned in Eq.(8). We see how, differently from the standard MPNN formalism, nodes at distance rcan now interact via products of message-passing matrices containing fewer than rfactors. In fact, the right- hand side also accounts for a direct interaction between i,j via the matrix Γr. Topping et al. (2022) showed that over-squashing arises precisely due to the entries ijof Ar decaying to zero exponentially with the distance, r, for (normalized) message-passing matrices A; on the other hand, using matrices like Γr, which are not powers of the same adjacency matrix, mitigates over-squashing. Interpreting delay as local smoothing. We comment here on a slightly different perspective from which to under- stand the role of delay in νDRew. As usual, we consider nodes i,j at distance r. In our framework, node i starts collecting messages from jstarting from the r-th layer. A larger delay (i.e. smaller value ofν), means that iaggregates 5DRew: Dynamically Rewired Message Passing with Delay Table 1.Classical MPNN benchmarks vs their DRew variants (without positional encoding) across four LRGB tasks: (from left to right) graph classiﬁcation, graph regression, link prediction and node classiﬁcation. All results are for the given metric on test data. Model Peptides-func Peptides-struct PCQM-Contact PascalVOC-SP AP ↑ MAE ↓ MRR ↑ F1 ↑ GCN 0.5930 ±0.0023 0.3496±0.0013 0.3234±0.0006 0.1268±0.0060 +DRew 0.6996±0.0076 0.2781±0.0028 0.3444±0.0017 0.1848±0.0107 GINE 0.5498 ±0.0079 0.3547±0.0045 0.3180±0.0027 0.1265±0.0076 +DRew 0.6940±0.0074 0.2882±0.0025 0.3300±0.0007 0.2719±0.0043 GatedGCN 0.5864 ±0.0077 0.3420±0.0013 0.3218±0.0011 0.2873±0.0219 +DRew 0.6733±0.0094 0.2699±0.0018 0.3293±0.0005 0.3214±0.0021 the features from jbefore they are (signiﬁcantly) ‘smoothed’ by repeated message passing. Conversely, a smaller delay (i.e. larger value of ν), implies that when icommunicates with j, it also leverages the structure around j which has been encoded in the representation of jvia the earlier layers. Therefore, we see how beyond mitigating over-squashing, the delay offers an extra degree of freedom to our frame- work, which can be used to adapt to the underlying task and how quickly the graph topological information needs to be mixed across different regions. Expressivity. As multi-hop aggregations used in νDRew are based on shortest path distances, they are able to distin- guish any pair of graphs distinguished by the shortest path kernel (Abboud et al., 2022; Borgwardt & Kriegel, 2005). Shortest path can distinguish disconnected graphs, a task at which 1-WL (Weisfeiler & Leman, 1968), which bounds the expressiveness of classical MPNNs (Xu et al., 2019), fails. We can therefore state that, at minimum, (ν)DRew is more expressive than 1-WL and, therefore, classical MPNNs. We leave more detailed expressivity analysis for future work. 5. Empirical Analysis In this section we focus on two strengths of our model. First, we validate performance in comparison with benchmark models, including vanilla and multi-hop MPNNs and graph Transformers, over ﬁve real-world tasks spanning graph-, node- and edge-level tasks. Second, we validate the ro- bustness of νDRew for long-range-dependent tasks and increased-depth architectures, using a synthetic task and a real-world molecular dataset. Parameter scaling. We note here that many of theDRew- MPNNs used in our experiments exhibit parameter scal- ing of approximately L2/2 for network depth L, whereas MPNNs scale with L. For fair comparison, a ﬁxed param- eter budget is maintained for all performance experiments across all network depths via suitable adjustment of hidden dimension d, for both MPNNs and Transformers – we re- serve the exploration of optimal sharing of weights for future work. We discuss space-time complexity in Appendix A. 5.1. Long-range graph benchmark The Long Range Graph Benchmark (LRGB; Dwivedi et al. (2022)) is a set of GNN benchmarks involving long-range interactions. We provide experiments for three datasets from this benchmark (two molecular property prediction, one image segmentation) spanning the full range of tasks associ- ated with GNNs: graph regression ( Peptides-func), graph classiﬁcation ( Peptides-struct), link pre- diction ( PCQM-Contact) and node classiﬁcation (PascalVOC-SP). The tasks presented by LRGB are characterised as possessing long-range dependencies according to the criteria of (a) graph size (i.e. having a large number of nodes), (b) requiring a long range of interaction, and (c) output sensitivity to global graph structure. We compare our DRew-MPNN variants from Section 3.2 against classical MPNN benchmarks in Table 1, and against a range of models in Table 2, including classical and DRew MPNN variants, graph Transformers (Dwivedi et al., 2022; Ramp´aˇsek et al., 2022), a multi-hop baseline (MixHop-GCN; Abu-El-Haija et al. (2019)) and a static graph rewiring benchmark (DIGL; Klicpera et al. (2019)). Experimental details. All experiments are averaged over three runs and were allowed to train for 300 epochs or until convergence. Classical MPNN and graph Trans- former results are reproduced from Dwivedi et al. (2022), except GraphGPS which is reproduced from Ramp ´aˇsek et al. (2022). DRew-MPNN, DIGL and MixHopGCN mod- els were trained using similar hyperparameterisations to their classical MPNN counterparts (see Appendix B. Some models include positional encoding (PE), either Laplacian (LapPE; Dwivedi et al. (2020)) or Random Walk (RWSE; Dwivedi et al. (2021)), as this improves performance and is necessary to induce a notion of locality in Transformers. We provide the performance of the best-caseνDRew model with respect to ν ∈{1,∞}and network depth Lfor both 6DRew: Dynamically Rewired Message Passing with Delay Table 2.Performance of various classical, multi-hop and static rewiring MPNN and graph Transformer benchmarks against DRew- MPNNs across four LRGB tasks. The ﬁrst-, second- and third-best results for each task are colour-coded; models whose performance are within a standard deviation of one another are considered equal. Model Peptides-func Peptides-struct PCQM-Contact PascalVOC-SP AP ↑ MAE ↓ MRR ↑ F1 ↑ GCN 0.5930 ±0.0023 0.3496±0.0013 0.3234±0.0006 0.1268±0.0060 GINE 0.5498 ±0.0079 0.3547±0.0045 0.3180±0.0027 0.1265±0.0076 GatedGCN 0.5864 ±0.0077 0.3420±0.0013 0.3218±0.0011 0.2873±0.0219 GatedGCN+PE 0.6069 ±0.0035 0.3357±0.0006 0.3242±0.0008 0.2860±0.0085 DIGL+MPNN 0.6469 ±0.0019 0.3173±0.0007 0.1656±0.0029 0.2824±0.0039 DIGL+MPNN+LapPE 0.6830 ±0.0026 0.2616±0.0018 0.1707±0.0021 0.2921±0.0038 MixHop-GCN 0.6592 ±0.0036 0.2921±0.0023 0.3183±0.0009 0.2506±0.0133 MixHop-GCN+LapPE 0.6843 ±0.0049 0.2614±0.0023 0.3250±0.0010 0.2218±0.0174 Transformer+LapPE 0.6326 ±0.0126 0.2529±0.0016 0.3174±0.0020 0.2694±0.0098 SAN+LapPE 0.6384 ±0.0121 0.2683±0.0043 0.3350±0.0003 0.3230±0.0039 GraphGPS+LapPE 0.6535 ±0.0041 0.2500±0.0005 0.3337±0.0006 0.3748±0.0109 DRew-GCN 0.6996±0.0076 0.2781±0.0028 0.3444±0.0017 0.1848±0.0107 DRew-GCN+LapPE 0.7150±0.0044 0.2536±0.0015 0.3442±0.0006 0.1851±0.0092 DRew-GIN 0.6940±0.0074 0.2799±0.0016 0.3300±0.0007 0.2719±0.0043 DRew-GIN+LapPE 0.7126±0.0045 0.2606±0.0014 0.3403±0.0035 0.2692±0.0059 DRew-GatedGCN 0.6733 ±0.0094 0.2699±0.0018 0.3293±0.0005 0.3214±0.0021 DRew-GatedGCN+LapPE 0.6977±0.0026 0.2539±0.0007 0.3324±0.0014 0.3314±0.0024 the PE and non-PE cases. Hyperparameters and other exper- imental details are available in Appendix B. As in Dwivedi et al. (2022), we use a ﬁxed ∼500k parameter budget. Discussion. As shown in Table 1, νDRew-MPNNs sub- stantially outperform their classical counterparts across all four tasks. We particularly emphasise this result for GINE and GatedGCN, as both models utilise edge features, un- like their DRew counterparts. Furthermore, DRew out- performs the static rewiring and multi-hop benchmarks in all tasks, and at least one DRew-MPNN model matches or beats the best ‘classical’ graph Transformer baseline from Dwivedi et al. (2022) in all four tasks . GraphGPS (Ramp´aˇsek et al., 2022) outperforms the best DRew model in the PascalVOC-SP and and Peptides-struct tasks, but we stress that GraphGPS is a much more so- phisticated architecture that combines dense Transformers with message passing , and therefore supports our claim that pure global attention throws away important induc- tive bias afforded by MPNN approaches. Even so, DRew still surpasses GraphGPS in the Peptides-func and PCQM-Contact tasks. 5.2. QM9 QM9 (Ramakrishnan et al., 2014) is a molecular multi-task graph regression benchmark dataset of ∼130,000 graphs with ∼18 nodes each and a maximum graph diameter of 10. We compare νDRew-GIN against a number of benchmark MPNNs and a GIN-based multi-hop MPNN: shortest path network (SPN; Abboud et al. (2022)), which is similar to our work in that it uses a multi-hop aggregation based on shortest path distances, but differs crucially in the lack of dynamic, per-layer rewiring or delay. Experimental results for all regression targets are given in Table 3. Experimental details. Our experimental setup is based on Brockschmidt (2020) and uses the same ﬁxed data splits. We use the overall-best-performing SPN parameterisation with a ‘max distance’ ofkmax = 10, referring to the k-hop neighbourhood aggregated over at each layer. This allows every node to interact with every other node at each layer when applied to a small-graph dataset like QM9, amounting to a dense static rewiring. DRew-GIN and SPN models use a parameter budget of ∼800,000, use 8 layers and train for 300 epochs; results are averaged over three runs. Neither SPN or DRew-GIN use relational edge features (denoted ‘R-’; Schlichtkrull et al. (2018); Brockschmidt (2020)) as its impact is minimal (see Appendix B). Other results are re- produced from their respective works (Brockschmidt, 2020; Alon & Yahav, 2021); several of these include a ﬁnal fully adjacent layer (+FA) which we include rather than the base models as they afford improved performance overall. Discussion. DRew demonstrates improvement over the classical and multi-hop MPNN benchmarks, beating or 7DRew: Dynamically Rewired Message Passing with Delay Table 3.Performance of νDRew compared with MPNN benchmarks on QM9. Scores reported are test MAE, i.e. lower is better. Property R-GIN+FA R-GAT+FA R-GatedGNN+FA GNN-FiLM SPN DRew-GIN ν1DRew-GIN mu 2.54 ±0.09 2.73±0.07 3.53±0.13 2.38±0.13 2.32±0.28 1.93±0.06 2.00±0.05 alpha 2.28 ±0.04 2.32±0.16 2.72±0.12 3.75±0.11 1.77±0.09 1.63±0.03 1.63±0.05 HOMO 1.26 ±0.02 1.43±0.02 1.45±0.04 1.22±0.07 1.26±0.09 1.16±0.01 1.17±0.02 LUMO 1.34 ±0.04 1.41±0.03 1.63±0.06 1.30±0.05 1.19±0.05 1.13±0.02 1.15±0.02 gap 1.96 ±0.04 2.08±0.05 2.30±0.05 1.96±0.06 1.89±0.11 1.74±0.02 1.74±0.03 R2 12.61 ±0.37 15.76±1.17 14.33±0.47 15.59±1.38 10.66±0.40 9.39±0.13 9.94±0.07 ZPVE 5.03 ±0.36 5.98±0.43 5.24±0.30 11.00±0.74 2.77±0.17 2.73±0.19 2.90±0.30 U0 2.21 ±0.12 2.19±0.25 3.35±1.68 5.43±0.96 1.12±0.13 1.01±0.09 1.00±0.07 U 2.32 ±0.18 2.11±0.10 2.49±0.34 5.95±0.46 1.03±0.09 0.99±0.08 0.97±0.04 H 2.26 ±0.19 2.27±0.29 2.31±0.15 5.59±0.57 1.05±0.04 1.06±0.09 1.02±0.09 G 2.04 ±0.24 2.07±0.07 2.17±0.29 5.17±1.13 0.97±0.06 1.06±0.14 1.01±0.05 Cv 1.86 ±0.03 2.03±0.14 2.25±0.20 3.46±0.21 1.36±0.06 1.24±0.02 1.25±0.03 Omega 0.80 ±0.04 0.73±0.04 0.87±0.09 0.98±0.06 0.57±0.04 0.55±0.01 0.60±0.03 matching the next-best model, SPN, for 12 out of 13 re- gression targets. We note that, overall, the best average performance across targets is achieved by DRew without delay (ν = ∞). This is as we might expect, as L-layer mod- els with ‘slow’ information ﬂow such as classical GCNs and ν1DRew cannot guarantee direct interaction between all node pairs on graphs with maximum diameter >L. 5.3. Validating robustness In this section we demonstrate the robustness properties, rather than raw performance, of νDRew with increasing network depth for long-range tasks. 5.3.1. R ING TRANSFER RingTransfer is a synthetic task for empirically vali- dating the ability of a GNN to capture long-range node dependencies (Bodnar et al., 2021). The dataset consists of N ring graphs (chordless cycles) of length k. Each graph has a single source node and a single target node that are always ⌊k 2 ⌋hops apart. Source node features are one-hot class label vectors of length C; all other nodes features are uniform. The task is for the target node to output the cor- rect class label at the source. We compare (ν)DRew-GCN againt a GCN and SP-GCN, an instance of the SPN frame- work (Abboud et al., 2022). Results are given in Figure 2 where the number of layers L = ⌊k 2 ⌋is the minimum required depth for source-target interaction. Discussion. RingTransfer demonstrates the power of νDRew in mitigating MPNN performance issues brought on by increased depth. While the classical GCN fails after fewer than 10 layers, ν1DRew achieves strong performance for 30 or more. These results also allow us to directly as- sess the impact of delay. The ‘full-delay’ ν1DRew-GCN consistently achieves perfect accuracy for 30+ layers with 20 40 60  0.2  0.4  0.6  0.8  1  Figure 2.Performance on RingTransfer task for models with varying k,L. Accuracy of 0.2 corresponds to a random guess. no drop in performance. We can attribute this to the di- rect interaction between the target and delayed source node. SP-GCN, however, with its static rewiring and dense com- putational graph, improves on the classical GCN, likely due to increased expressivity, but still fails at much shallowerL than νDRew, with or without delay. 5.3.2. L AYERWISE PERFORMANCE ON PEPTIDES -FUNC In this section we strip back our experiments on Peptides-func to demonstrate the robustness of νDRew for increasing network depth L, as well as the impact of ν, the parameter denoting rate of information ﬂow during message passing. For these experiments we ﬁx the hidden dimension to 64. In Figure 3 we plot model performance against Lfor three different parameterisations of νDRew-GCN: the non-delay version ν = ∞(which reduces to DRew), the full-delay version ν = 1, and a midpoint, where we set ν = L/2. Discussion. Figure 3 demonstrates a crucial contribution of our framework: the ability to tune ν to suit the task. It is evident that using νDRew with delay ensures more robust training when using a deeper architecture; in fact, the more delay used (i.e. the lower the value of ν), the better the performance for large L, whereas using less 8DRew: Dynamically Rewired Message Passing with Delay 5 10 15 20  0.5  0.55  0.6  0.65  0.7  Figure 3.Comparing three parameterizations of νDRew plus a classical residual GCN on Peptides-func over varying L. delay (high ν) ensures faster ﬁlling of the computational graph and greater density of connections after fewer layers. This means that, when using lower L, non-delay DRew often performs better, especially when combined with PE. Conversely, more delay ‘slows down’ the densiﬁcation of node connections, yielding stronger long-term performance with L. Figure 3 demonstrates this with a long-range task: ν1DRew consistently improves with more layers. 6. Related Work Various methods have been developed to improve learning on graphs and avoid issues such as over-squashing. Many of these are broadly classiﬁed as ‘graph rewiring’ methods; one such family involves sampling of nodes and/or edges of the graph based on some sort of performance or topological metric. Examples include sparsiﬁcation (Hamilton et al., 2017), node (edge) dropout (Rong et al., 2019; Papp et al., 2021), rewiring based on graph diffusion (Klicpera et al., 2019), Cayley graphs (Deac et al., 2022), commute times, spectral gap or Ricci curvature to combat over-squashing (Arnaiz-Rodr´ıguez et al., 2022; Topping et al., 2022; Black et al., 2023). Most of these methods remove supposedly irrelevant elements from the graph to make it more amenable to analysis, though many methods also add elements to increase connectivity. This might be a global node (Battaglia et al., 2016; Gilmer et al., 2017), or global layer, such as positional/structural encoding (Dwivedi et al., 2020; 2021; Ramp´aˇsek et al., 2022; Wang et al., 2022), or adding a fully adjacent layer after message passing (Alon & Yahav, 2021). It may also take the form of multiple-hop rewiring, in which aggregations occur over nodes at >1-hop distance at each layer; we distinguish these into ‘local’ graph rewiring, also known as multi-hop MPNNs (Abboud et al., 2022; Abu-El-Haija et al., 2019; 2020; Nikolentzos et al., 2020; Zhang & Li, 2021), and ‘global’ methods such as graph Transformers (Dwivedi et al., 2022; Kreuzer et al., 2021; Ramp´aˇsek et al., 2022; Ying et al., 2021; Yun et al., 2019), which fully connect the input graph. Unlike all of these methods, the rewiring used in DRew is layer-dependent, i.e. it is adaptive rather than static. Our method is also unique in its ability to control the rate of information ﬂow by tuning the delay parameter ν. Our use of delay is loosely inspired by delay differential equations (DDEs), which have also inspired architectures which lever- age delay in the wider deep learning space (Anumasa & PK, 2021; Zhu et al., 2021; 2022) based on neural ordi- nary differential equations (Chen et al., 2018), but we know of no DDE-inspired works in the graph machine learning space. The idea of accessing previous node states resonates with Xu et al. (2018) and Strathmann et al. (2021). Faber & Wattenhofer (2022) use a form of delay to create node identiﬁers as a means to allow nodes to ignore irrelevant messages from their neighbours, but all of these works bear little relation to DRew, which treats dynamic rewiring and delay from the perspective of distance on graphs. 7. Conclusion and Future Work We have introduced an adaptive MPNN framework based on a layer-dependent dynamic rewiring that can be adapted to any MPNN. We have also proposed a delay mechanism per- mitting local skip-connections among different nodes based on their mutual distance. Investigating the expressive power of this framework represents a promising future avenue to compare static and dynamic rewiring approaches, as well as the impact of distance-aware skip-connections. Limitations. Our framework is expected to be useful for tasks with long-range interactions or, more generally, when one requires very deep GNN models, as conﬁrmed by our ex- periments. Accordingly, we do not expect our framework to be advantageous when applied to (for example) homophilic node classiﬁcation tasks where shallow GNNs acting as low-pass ﬁlters are sufﬁcient to perform strongly. This may partly explain why DRew is outperformed by GraphGPS for PascalVOC-SP (see Table 2), as this dataset presents an image segmentation task which likely displays a reason- able degree of homophily. As a result, the extent to which long-range interactions are truly present is uncertain. Acknowledgements. We are grateful for anonymous re- viewer feedback. We acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work (Richards, 2015) , as well as the JADE HPC facility. B.G. acknowledges support from the EPSRC Centre for Doctoral Training in AIMS (EP/S024050/1). X.D. acknowledges support from the Oxford-Man Institute of Quantitative Finance and the EPSRC (EP/T023333/1). M.B. is supported in-part by ERC Consolidator Grant No. 274228 (LEMAN) and Intel AI Grant. 9DRew: Dynamically Rewired Message Passing with Delay References Abboud, R., Dimitrov, R., and Ceylan, ˙I. ˙I. Shortest path networks for graph property prediction. arXiv preprint arXiv:2206.01003, 2022. Abu-El-Haija, S., Perozzi, B., Kapoor, A., Alipourfard, N., Lerman, K., Harutyunyan, H., Ver Steeg, G., and Gal- styan, A. Mixhop: Higher-order graph convolutional architectures via sparsiﬁed neighborhood mixing. In in- ternational conference on machine learning, pp. 21–29. PMLR, 2019. Abu-El-Haija, S., Kapoor, A., Perozzi, B., and Lee, J. N- GCN: Multi-scale graph convolution for semi-supervised node classiﬁcation. In uncertainty in artiﬁcial intelli- gence, pp. 841–851. PMLR, 2020. Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., and S¨usstrunk, S. SLIC superpixels compared to state-of-the- art superpixel methods. IEEE transactions on pattern analysis and machine intelligence , 34(11):2274–2282, 2012. Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In In- ternational Conference on Learning Representations , 2021. URL https://openreview.net/forum? id=i80OPhOCVH2. Anumasa, S. and PK, S. Delay differential neural networks. In 2021 6th International Conference on Machine Learn- ing Technologies, pp. 117–121, 2021. Arnaiz-Rodr´ıguez, A., Begga, A., Escolano, F., and Oliver, N. Diffwire: Inductive graph rewiring via the Lov ´asz bound. arXiv preprint arXiv:2206.07369, 2022. Barcel´o, P., Kostylev, E. V ., Monet, M., P´erez, J., Reutter, J., and Silva, J. P. The logical expressiveness of graph neural networks. In ICLR, 2019. Battaglia, P., Pascanu, R., Lai, M., Jimenez Rezende, D., et al. Interaction networks for learning about objects, relations and physics. Advances in neural information processing systems, 29, 2016. Black, M., Nayyeri, A., Wan, Z., and Wang, Y . Understand- ing oversquashing in gnns through the lens of effective resistance. arXiv preprint arXiv:2302.06835, 2023. Bodnar, C., Frasca, F., Otter, N., Wang, Y ., Lio, P., Montufar, G. F., and Bronstein, M. Weisfeiler and Lehman go cellular: CW networks. Advances in Neural Information Processing Systems, 34:2625–2640, 2021. Bodnar, C., Di Giovanni, F., Chamberlain, B. P., Li`o, P., and Bronstein, M. M. Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in GNNs. arXiv preprint arXiv:2202.04579, 2022. Borgwardt, K. M. and Kriegel, H.-P. Shortest-path kernels on graphs. In Fifth IEEE international conference on data mining (ICDM’05), pp. 8–pp. IEEE, 2005. Bresson, X. and Laurent, T. Residual gated graph ConvNets. arXiv preprint arXiv:1711.07553, 2017. Brockschmidt, M. GNN-FiLM: Graph neural networks with feature-wise linear modulation. In International Confer- ence on Machine Learning, pp. 1144–1152. PMLR, 2020. Br¨uel-Gabrielsson, R., Yurochkin, M., and Solomon, J. Rewiring with positional encodings for graph neural net- works. arXiv preprint arXiv:2201.12674, 2022. Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y . Spectral networks and locally connected networks on graphs. In 2nd International Conference on Learning Representa- tions, ICLR 2014, 2014. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318, 2020. Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. arXiv preprint arXiv:2210.02997, 2022. Di Giovanni, F., Rowbottom, J., Chamberlain, B. P., Markovich, T., and Bronstein, M. M. Graph neu- ral networks as gradient ﬂows. arXiv preprint arXiv:2206.10991, 2022. Di Giovanni, F., Giusti, L., Barbero, F., Luise, G., Lio, P., and Bronstein, M. On over-squashing in message passing neural networks: The impact of width, depth, and topology. arXiv preprint arXiv:2302.02941, 2023. Dwivedi, V . P., Joshi, C. K., Laurent, T., Bengio, Y ., and Bresson, X. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. Dwivedi, V . P., Luu, A. T., Laurent, T., Bengio, Y ., and Bresson, X. Graph neural networks with learnable structural and positional representations. arXiv preprint arXiv:2110.07875, 2021. Dwivedi, V . P., Ramp´aˇsek, L., Galkin, M., Parviz, A., Wolf, G., Luu, A. T., and Beaini, D. Long range graph bench- mark. arXiv preprint arXiv:2206.08164, 2022. Faber, L. and Wattenhofer, R. Asynchronous neu- ral networks for learning in graphs. arXiv preprint arXiv:2205.12245, 2022. 10DRew: Dynamically Rewired Message Passing with Delay Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In International Conference on Machine Learning, pp. 1263–1272. PMLR, 2017. Gori, M., Monfardini, G., and Scarselli, F. A new model for learning in graph domains. In Proc. 2005 IEEE In- ternational Joint Conference on Neural Networks, 2005., volume 2, pp. 729–734. IEEE, 2005. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Conference on Neural Information Processing Systems (NeurIPS), pp. 1025–1035, 2017. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020. Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y ., and Leskovec, J. OGB-LSC: A large-scale chal- lenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021. Kingma, D. P. and Ba, J. Adam: A method for stochas- tic optimization. Proc. of Int. Conference on Learning Representations (ICLR), San Diego, CA, USA, May 2015. Kipf, T. N. and Welling, M. Semi-Supervised Classiﬁ- cation with Graph Convolutional Networks. In Pro- ceedings of the 5th International Conference on Learn- ing Representations, ICLR ’17, 2017. URL https: //openreview.net/forum?id=SJU4ayYgl. Klicpera, J., Weissenberger, S., and G ¨unnemann, S. Dif- fusion improves graph learning. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2019. Kreuzer, D., Beaini, D., Hamilton, W., L´etourneau, V ., and Tossou, P. Rethinking graph Transformers with spectral attention. Advances in Neural Information Processing Systems, 34:21618–21629, 2021. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization. arXiv preprint arXiv:1711.05101, 2017. Nikolentzos, G., Dasoulas, G., and Vazirgiannis, M. k-hop graph neural networks. Neural Networks, 130:195–205, 2020. Nt, H. and Maehara, T. Revisiting graph neural net- works: All we have is low-pass ﬁlters. arXiv preprint arXiv:1905.09550, 2019. Oono, K. and Suzuki, T. Graph neural networks exponen- tially lose expressive power for node classiﬁcation. In International Conference on Learning Representations, 2020. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. DropGNN: Random dropouts increase the expressiveness of graph neural networks. Advances in Neural Informa- tion Processing Systems, 34:21997–22009, 2021. Ramakrishnan, R., Dral, P., Rupp, M., and V on Lilienfeld, O. Quantum chemistry structures and properties of 134 kilo molecules. Scientiﬁc data, 1(1):1–7, 2014. Ramp´aˇsek, L., Galkin, M., Dwivedi, V . P., Luu, A. T., Wolf, G., and Beaini, D. Recipe for a general, powerful, scal- able graph transformer. arXiv preprint arXiv:2205.12454, 2022. Richards, A. University of Oxford Advanced Re- search Computing. http://dx.doi.org/10. 5281/zenodo.22558, 2015. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas- siﬁcation. arXiv preprint arXiv:1907.10903, 2019. Rusch, T. K., Chamberlain, B. P., Mahoney, M. W., Bron- stein, M. M., and Mishra, S. Gradient gating for deep multi-rate learning on graphs. arXiv preprint arXiv:2210.00513, 2022. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008. Schlichtkrull, M., Kipf, T. N., Bloem, P., Berg, R. v. d., Titov, I., and Welling, M. Modeling relational data with graph convolutional networks. In European semantic web conference, pp. 593–607. Springer, 2018. Sperduti, A. Encoding labeled graphs by labeling RAAM. Advances in Neural Information Processing Systems, 6, 1993. Strathmann, H., Barekatain, M., Blundell, C., and Veliˇckovi´c, P. Persistent message passing. arXiv preprint arXiv:2103.01043, 2021. Topping, J., Di Giovanni, F., Chamberlain, B. P., Dong, X., and Bronstein, M. M. Understanding over-squashing and bottlenecks on graphs via curvature. International Conference on Learning Representations, 2022. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=rJXMpikCZ. 11DRew: Dynamically Rewired Message Passing with Delay Wang, H., Yin, H., Zhang, M., and Li, P. Equivariant and stable positional encoding for more powerful graph neural networks. arXiv preprint arXiv:2203.00199, 2022. Weisfeiler, B. and Leman, A. The reduction of a graph to canonical form and the algebra which appears therein. NTI, Series, 2(9):12–16, 1968. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.- i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Con- ference on Machine Learning , pp. 5453–5462. PMLR, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How pow- erful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum? id=ryGs6iA5Km. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y ., and Liu, T.-Y . Do Transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34:28877–28888, 2021. Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J. Graph transformer networks, 2019. Conference on Neural Infor- mation Processing Systems (NeurIPS), 2019. Zhang, M. and Li, P. Nested graph neural networks. Ad- vances in Neural Information Processing Systems , 34: 15734–15747, 2021. Zhu, Q., Guo, Y ., and Lin, W. Neural delay differential equations. arXiv preprint arXiv:2102.10801, 2021. Zhu, Q., Shen, Y ., Li, D., and Lin, W. Neural piecewise- constant delay differential equations. arXiv preprint arXiv:2201.00960, 2022. 12DRew: Dynamically Rewired Message Passing with Delay A. Time and Space Complexity Time complexity. DRew relies on the shortest path and therefore requires up-to k-hop adjacency information for layer k. This can be pre-computed in worst-case time O(|V||E|) using the same breadth-ﬁrst search method of Abboud et al. (2022), but once computed it can be re-used for all runs. In the worst case — when ℓis greater than the max diameter of the graph — DRew performs aggregation over O(V2) elements, i.e. all node pairs. However, each k-neighbourhood aggregation can be computed in parallel, so this is not a serious concern in practice, and as DRew gradually builds the computational graph at each layer, it is faster than comparable multi-hop MPNNs or Transformers that aggregate over the entire graph, or a ﬁxed hop distance, at every layer. Space complexity. As we use delayed representations of nodes, we must store ℓrepresentations of the node features at layer ℓfor a given mini-batch; i.e. linear scaling in network depth L. This has not proved to be a bottleneck in practice, and could be addressed if need be by reducing batch size. B. Further Experimental Details In this section we provide further details about our experiments, as well as further results on Peptides-func. B.1. Hardware All experiments were run on server nodes using a single GPU. A mixture of P100, V100, A100, Titan V and RTX GPUs were used, as well as a mixture of Broadwell, Haswell and Cacade Lake CPUs. B.2. Long range graph benchmark All results in Tables 1 and 2 use similar experimental setups and identical data train–val–test splits to their classical MPNN counterparts in Dwivedi et al. (2022). We use a ﬁxed parameter budget of ∼500,000, which is controlled for by appropriate tuning of hidden dimension when using different network depths L. Signiﬁcant hyperparameter differences for experiments are given in Table 4; other experimental details are itemised below: • For all experiments we train for 300 epochs or until convergence and average results over three seeds. • All results use the AdamW optimizer (Kingma & Ba; Loshchilov & Hutter, 2017) with base learning rate lr=0.001 (except PascalVOC-SP, which uses 0.0005 ), lr decay=0.1 , min lr=1e-5 , momentum=0.9 , and a reduce- on-plateau scheduler with reduce factor=0.5 , schedule patience=10 ( 20 for Peptides). • All Peptides, PCQM-Contact and PascalVOC-SP experiments use batch sizes of 128, 256 and 32 respectively. • All experiments use batch normalisation with eps=1e-5 , momentum=0.1 and post-layer L2 normalisation. • Peptides and PCQM-Contact use the ‘Atom’ node encoder (Hu et al., 2020; 2021), whereasPascalVOC-SP uses a node encoder which is a single linear layer with a ﬁxed output dimension of 14. • None of the experiments in Table 2 use edge encoding or edge features. • Superpixel nodes in PascalVOC-SP are extracted using a SLIC compactness of 30 for the SLIC algorithm (Achanta et al., 2012). • We do not use dropout. • All Laplacian PE uses hidden dimension of 16 and 2 layers. • For PCQM-Contact, all experiments (except for DIGL) use convex combination with equal weights for aggregation; i.e. each of M channel-mixing matrices per k-neighbourhood aggregation is multiplied by 1/M. Other tasks do not use any matrix weights. • Experiments for MixHop-GCN, a a multi-hop MPNN, are parameterised by max P, where integer powers of the adja- cency matrix are aggregated up to max P, with equal-size weight matrices per adjacency power. The hyperparameters in Table 4 were determiend by best performance after hyperparameter search over max P and network depth L. 13DRew: Dynamically Rewired Message Passing with Delay • We perform DIGL rewiring using the default settings from the Graph Diffusion Convolution transform from torch geometric.transforms using PPR diffusion with α = 0.2 and threshold sparsiﬁcation with average degree davg given in Table 4. • All DIGL+MPNN runs use GCN as the base MPNN except for PascalVOC-SP which uses GatedGCN instead for fair comparison, as other classical MPNNs perform poorly on this task • Peptides-func results in Figures 3, 4 and 5 use the same experimental setup as described above. • The reported results for GatedGCN+PE in Table 2 use LapPE for PascalVOC-SPand RWSE for all other tasks. Table 4.Parameter counts (#Ps) and signiﬁcant hyperparameters (HPs) for for all DIGL, MixHop-GCN and(ν)DRew-MPNN experiments in Table 2. Hyperparameterisation details for reproduced results are available in their respective works. Model Peptides-func Peptides-struct PCQM-Contact PascalVOC-SP #Ps HPs #Ps HPs #Ps HPs #Ps HPs DIGL+MPNN 499k davg = 6 L= 13 496k davg = 6 L= 5 497k davg = 2 L= 5 502k davg = 14 L= 8 DIGL+MPNLapPE 493k davg = 6 L= 5 496k davg = 6 L= 7 495k davg = 2 L= 5 502k davg = 14 L= 8 MixHop-GCN 513k max P = 5 L= 17 510k max P = 7 L= 17 523k max P = 3 L= 5 511k max P = 5 L= 8 MixHop-GCN+LapPE 518k max P = 7 L= 14 490k max P = 7 L= 11 521k max P = 3 L= 5 512k max P = 5 L= 8 DRew-GCN 518k ν = 1 L= 23 498k ν = ∞ L= 13 515k ν = ∞ L= 20 498k ν = 1 L= 8 DRew-GCN+LapPE 502k ν = ∞ L= 7 495k ν = ∞ L= 5 498k ν = ∞ L= 10 498k ν = 1 L= 8 DRew-GIN 514k ν = 1 L= 17 505k ν = ∞ L= 15 507k ν = ∞ L= 20 506k ν = 1 L= 8 DRew-GIN+LapPE 502k ν = 1 L= 15 497k ν = ∞ L= 5 506k ν = ∞ L= 10 506k ν = 1 L= 8 DRew-GatedGCN 495k ν = 1 L= 17 497k ν = ∞ L= 13 506k ν = ∞ L= 20 502k ν = 1 L= 8 DRew-GatedGCN+LapPE 495k ν = ∞ L= 7 494k ν = ∞ L= 5 494k ν = ∞ L= 10 502k ν = 1 L= 8 B.3. QM9 Performance experiments use a ﬁxed parameter budget of 800k, controlled for by appropriate tuning of the hidden dimension when using different network depth L. We mostly follow the experimental setup of (Abboud et al., 2022), using a ﬁxed learning rate of 0.001, mean pooling and no dropout. We use batch normalization, train for 300 epochs averaging over 3 runs, and use data splits from (Brockschmidt, 2020). Many of the benchmarks we compare against in Table 3 are Relational MPNNs (R-MPNN; Schlichtkrull et al. (2018); Brockschmidt (2020)) which incorporate edge labels by assigning separate weights for each edge type in the 1-hop neighbourhood, and aggregating over each type separately. For our SPN and DRew-GIN experiments, however, we do not incorporate edge features, as (a) DRew demonstrates strong performance even without this information, and (b) we expect the improvement obtained through using R-GIN to be slight given that over 92% of all graph edges in QM9 are of only one type. 14DRew: Dynamically Rewired Message Passing with Delay B.4. RingTransfer For synthetic RingTransfer (Bodnar et al., 2021) experiments we use a dataset of size N = 2000with C = 5classes and a corresponding node feature dimension. GCN and SP-GCN runs use a hidden dimension of 256, and for fair comparison DRew-GCN runs use a varying hidden dimension to ensure the same parameter count as GCN/SP-GCN for each ring length k(and therefore model depth L). All runs use batch normalization, no dropout, and Adam optimization with learning rate 0.01. We train for 50 epochs and average over three experiments, using the accuracy of predicting the source node label from the readout of the source node representation as a metric. We use an 80:10:10 split for train, test and validation. SP-GCN We deﬁne SP-GCN as an instance of the SP-MPNN framework (Abboud et al., 2022): h(ℓ+1) i = h(ℓ) i + σ   kmax∑ k=1 ∑ j∈Nk α(ℓ) k W(ℓ)γk ijh(ℓ) j  , (11) where kmax is the max distance parameter that determines the number of hops to aggregate over at each layer andα(ℓ) ⊂Rkmax are learned weight vectors, ∑kmax k α(ℓ) k = 1. γk ij is as in Eq. (8). B.5. Ablation on Peptides-func In this section we provide additional experimental results onPeptides-func using our 500k parameter budget setup from Section 5.1. We train a number of varying-depth GCN, residual GCN andνDRew-GCN models with three parameterizations of ν: 1,L/2 and ∞. We provide separate results with and without Laplacian PE (Dwivedi et al., 2020) in Figures 5 and 4 respectively. For reference, on both ﬁgures we also mark the best-performing Transformer, SAN with random walk encoding (Kreuzer et al., 2021; Dwivedi et al., 2021), reproduced from Dwivedi et al. (2022) and denoted with a dashed black line. Discussion From Figures 4 and 5 we can see that more delay leads to stronger performance at greater network depths, even as the hidden dimension decreases severely. We see that lowL, low/no delay νDRew and high L, high delay νDRew outperform GCNs and the best-performing Transformer, with or without positional encoding. We note that the poor performance of low-delay νDRew at high Land vice-versa is as expected. As Peptides-func is a long-range task, strong performance requires interaction between distant nodes. Though it uses more powerful multi-hop aggregations, ν1DRew maintains the same ‘rate’ of information ﬂow as classical MPNNs, i.e.r-distant nodes are able to interact from the rth layer; therefore small Ldoes not give ν1DRew sufﬁcient layers to enable long-range interactions, and performance is comparable to classical MPNNs. As our framework is more robust, however, ν1DRew continues to increase in performance as Lincreases — as we would hope — while the classical MPNNs GCN and ResGCN succumb to the usual issues that affect MPNNs, and degrade. Conversely, the low delay parameterizations, {νL/2,ν∞}DRew-GCN, perform strongly for low L and worsen as the network becomes very deep. Again, this is expected: low delay affords fast (or instantaneous) communication between distant nodes after sufﬁcient layers, and therefore has a rate of information ﬂow that is faster than ν1DRew or classical MPNNs (though still slower than multi-hop MPNNs or Transformers). This means that, for a long-range task such as Peptides-func, performance is stronger for fewer layers, once long-range interactions have been facilitated but before the computational graph becomes too dense, causing performance drop. These experiments further demonstrate the impact and usefulness of the delay parameter as a means of tuning the model to suit the task. Referring to Figure 5, we note that the addition of PE exacerbates the behaviours described above, and indeed accelerates the rate of information ﬂow by preceding the message-passing with a global layer. Positional encoding. As a ﬁnal point, we consider the overall impact of PE, particularly Laplacian PE, on this task. We posit that, for Peptides, the characterization of Transformers as the strongest long-range models (Dwivedi et al., 2022) is due primarily to PE, rather than the Transformer architecture. As evidence we point to the performance of vanilla GCN, the simplest MPNN, on func when LapPE is used; it outperforms SAN with only ﬁve layers. We reiterate that νDRew outperforms MPNN and Transformer+PE benchmarks with or without using PE itself. 15DRew: Dynamically Rewired Message Passing with Delay 5 10 15 20  0.58  0.6  0.62  0.64  0.66  0.68  0.7  0.72  Figure 4.Peptides-func experiments over varying Lwith ﬁxed 500k parameter count using no positional encoding. 5 10 15 20  0.58  0.6  0.62  0.64  0.66  0.68  0.7  0.72  Figure 5.Peptides-func experiments over varying Lwith ﬁxed 500k parameter count using Laplacian positional encoding. 16",
      "meta_data": {
        "arxiv_id": "2305.08018v2",
        "authors": [
          "Benjamin Gutteridge",
          "Xiaowen Dong",
          "Michael Bronstein",
          "Francesco Di Giovanni"
        ],
        "published_date": "2023-05-13T22:47:40Z",
        "pdf_url": "https://arxiv.org/pdf/2305.08018v2.pdf"
      }
    },
    {
      "title": "Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptative Residual Module",
      "abstract": "Graph Neural Networks (GNNs), a type of neural network that can learn from\ngraph-structured data through neighborhood information aggregation, have shown\nsuperior performance in various downstream tasks. However, as the number of\nlayers increases, node representations become indistinguishable, which is known\nas over-smoothing. To address this issue, many residual methods have emerged.\nIn this paper, we focus on the over-smoothing issue and related residual\nmethods. Firstly, we revisit over-smoothing from the perspective of overlapping\nneighborhood subgraphs, and based on this, we explain how residual methods can\nalleviate over-smoothing by integrating multiple orders neighborhood subgraphs\nto avoid the indistinguishability of the single high-order neighborhood\nsubgraphs. Additionally, we reveal the drawbacks of previous residual methods,\nsuch as the lack of node adaptability and severe loss of high-order\nneighborhood subgraph information, and propose a\n\\textbf{Posterior-Sampling-based, Node-Adaptive Residual module (PSNR)}. We\ntheoretically demonstrate that PSNR can alleviate the drawbacks of previous\nresidual methods. Furthermore, extensive experiments verify the superiority of\nthe PSNR module in fully observed node classification and missing feature\nscenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN.",
      "full_text": "Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptive Residual Module Jingbo Zhou1,2∗, Yixuan Du2,3∗, Ruqiong Zhang2,3∗, Jun Xia1, Zhizhi Yu3, Zelin Zang1, Di Jin3, Carl Yang4, Rui Zhang2†, Stan Z. Li1† 1Westlake University,2Jilin University, 3Tianjin University,4Emory University {zhoujingbo, stan.zq.li}@westlake.edu.cn Abstract Graph Neural Networks (GNNs), a type of neural network that can learn from graph-structured data through neighborhood information aggregation, have shown superior performance in various downstream tasks. However, as the number of layers increases, node representations become indistinguishable, which is known as over-smoothing. To address this issue, many residual methods have emerged. In this paper, we focus on the over-smoothing issue and related residual methods. Firstly, we revisit over-smoothing from the perspective of overlapping neighbor- hood subgraphs, and based on this, we explain how residual methods can alleviate over-smoothing by integrating multiple orders neighborhood subgraphs to avoid the indistinguishability of the single high-order neighborhood subgraphs. Additionally, we reveal the drawbacks of previous residual methods, such as the lack of node adaptability and severe loss of high-order neighborhood subgraph information, and propose a Posterior-Sampling-based, Node-Adaptive Residual module (PSNR). We theoretically demonstrate that PSNR can alleviate the drawbacks of previous residual methods. Furthermore, extensive experiments verify the superiority of the PSNR module in fully observed node classification and missing feature scenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN. 1 Introduction GNNs have emerged in recent years as the most powerful model for processing graph-structured data and have demonstrated exceptional performance across various fields, such as social networks [22], recommender systems [6], and drug discovery [4]. Through the message-passing mechanism that propagates and aggregates information of neighboring nodes, GNNs provide a general framework for learning information on graph structure. Despite the remarkable success, according to previous studies [17, 32], GNNs show significant performance degradation as the number of layers increase. One of the main reasons for this situation is over-smoothing [17, 21, 32, 15]. Over-smoothing refers to the phenomenon in which node representations become increasingly similar to each other as GNNs recursively aggregate more neighborhood information. This indistinguishability will inevitably degrade the performance of deep GNNs, restricting their ability to effectively model long-range dependencies among multi-hop neighbors. Several methods have recently been proposed to alleviate over-smoothing in deep GNNs. According to [26], these methods fall into three categories: Normalization and Regularization [34, 33], Change of GNN dynamics [5], and Residual connections [32, 16]. Among all of them, the residual-based ∗Equal contribution. †Corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2305.05368v3  [cs.LG]  31 Oct 2024method is inspired by the success of residual neural networks (ResNets) [9] in computer vision. This type of method introduces a residual connection to the GNNs architecture. For example, JKNet [32] learns node representations by aggregating the outputs of all previous layers at the last layer. DenseGCN [16] concatenates the results of the current layer and all previous layers as the node representations of this layer. APPNP [ 15] uses the initial residual connection to retain the initial feature information with probability α, and utilizes information aggregated at the current layer with probability 1 − α. GCNII [3] shares a similar framework with APPNP, and it further introduces an identical mapping to avoid overfitting. In this paper, we study the over-smoothing issue in GNNs, with a particular emphasis on residual methods. First, we revisit the over-smoothing phenomenon of GNNs from the new perspective of overlapping neighborhood subgraphs and explain the essential reason why the residual method can alleviate the over-smoothing. In essence, these methods mainly use multiple neighborhood subgraph aggregations to alleviate the indistinguishability of the single neighborhood subgraph aggregation, thereby improving the performance of the model. On this basis, we find that these residual methods often lack node adaptivity in utilizing multi-order neighborhood subgraph information, and at the same time, they still struggle to mitigate information loss when dealing with high-order neighborhood subgraphs, which hinders further improvement in the performance of deep GNNs. Although some residual methods, such as DenseGNN, can avoid these drawbacks, they tend to introduce more parameters at deeper layers. This can lead to significant memory consumption and is prone to gradient explosion, limiting the scalability of the methods. Considering these limitations, we propose a Posteriori-Sampling-based Node-Adaptative Residual Module (PSNR). More specifically, this module introduces a graph encoder to learn the posterior distribution of the required residual coefficients for each node in different layers with minor overhead. And then, we can obtain the specific fine-grained node-adaptive residual coefficients by sampling from the distribution. The contributions of this paper are as follows: • Perspective: We revisit the over-smoothing issue from a novel perspective of high-order neighborhood subgraph coincidences and explain why the residual methods can alleviate it. Through this lens, we reveal several significant drawbacks of prior residual methods that limit the performance and scalability of GNNs. • Method: We propose PSNR, a lightweight and model-agnostic module to mitigate the draw- backs of previous residual methods and provide theoretical justification for its advantages. • Experiments: Extensive experiments verify that the PSNR module can effectively mitigate oversmoothing and further improve the performance of GNNs, especially in the case of missing feature that require deep GNNs. 2 Related Work 2.1 Notations A connected undirected graph is represented byG = (V, E), where V = {v1, v2, . . . , vN } is the set of N nodes and E ⊆ V × Vis the edge set. The node features are represented in the matrix H ∈ RN×d, where d represents the length of the feature. Let A ∈ {0, 1}N×N denotes the adjacency matrix and Aij = 1 only if an edge exists between nodes vi and vj. D ∈ RN×N is the diagonal degree matrix, where each element di represents the number of edges connected to node vi. ˜A = A+ I, ˜D = D+ I represent the adjacency matrix and degree matrix with self-loop, respectively. 2.2 Graph Neural Networks A GNN layer updates the representation of each node via aggregating itself and its neighbors’ representations. Specifically, a layer’s output H′ consists of new representations h′ of each node computed as: h′ i = fθ (hi, AGGREGATE ({hj | vj ∈ V, (vi, vj) ∈ E})) , (1) where h′ i indicates the new representation of node vi and fθ(·) denotes the update function. The difference between different GNNs lies in the update function fθ(·) and the AGGREGATE(·) function, which are also key to the performance of GNNs. Graph Convolutional Network (GCN) [14] 2Table 1: Common residual connections for GNNs. Residual Connection Corresponding GCN Formula Res ResGCN [16] Hk =Hk−1 +σ \u0010˜D−1 2 ˜A˜D−1 2Hk−1Wk−1 \u0011 InitialRes APPNP [15] Hk = (1−α) ˜D−1 2 ˜A˜D−1 2Hk−1 +αH Dense DenseGCN [16] Hk = AGG(H,H1, . . . ,Hk−1) JK JKNet [32] Houtput= AGG(H1, . . . ,Hk−1) is a classical massage-passing GNNs follows layer-wise propagation rule: Hk+1 = σ \u0010 ˜D−1 2 ˜A ˜D−1 2 HkWk \u0011 , (2) where Hk is the feature matrix of the k-th layer, Wk is a layer-specific learnable weight matrix, σ(·) denotes an activation function. 2.3 Residual connection Several works have used residual connection to alleviate the over-smoothing issue. Common residual connections for GNNs are summarized in Table 1, where Hk represents the output of the k-th layer, Wk is a learnable weight matrix for thek-th layer, α serves as a hyperparameter denoting the residual coefficient, and σ(·) denotes an activation function. Additionally, in DenseGNN, AGG represents a function with the concatenation of outputs from all previous layers as the input to the current layer, while in JKNet, AGG refers to the aggregation of all previous representations through concatenation, max-pooling, or LSTM-attention only at the final layer. Details can be found in Appendix B. 3 Why does the residual method alleviate over-smoothing? In this section, we revisit over-smoothing from the perspective of overlapping high-order neigh- borhood subgraphs. Based on this, we elucidate the role of various residual methods in alleviating over-smoothing and identify their shortcomings. 3.1 Revisit over-smoothing from the perspective of neighborhood subgraphs overlapping 20 21 22 23 24 25 26 Degree 0.0 0.1 0.2 0.3SMV Cora 20 21 22 23 24 25 Degree 0.15 0.20 0.25 0.30 0.35 Citeseer 20 21 22 23 24 25 26 27 Degree 0.0 0.1 0.2 0.3 Pubmed GAT-2 GAT-4 GAT-8 GAT-16 GAT-32 GAT-64 Figure 1: SMV for node groups of different degrees. More results are shown in Appendix C. For message-passing GNNs without the residual connection, the information domain of each node after k-layer aggregation is a corresponding k-order neighborhood subgraph. Intuitively, the size of its k-order neighborhood subgraph grows exponentially as k increases, leading to a significant increase in the overlap between the k-order neighborhood subgraphs of different nodes. As a result, the aggregation result of different nodes on their respective k-order neighborhood subgraphs becomes indistinguishable. This explanation can be partially validated from the perspective of node degrees. Considering nodes with high degrees tend to have larger neighborhood subgraph overlap, the correlation between neighborhood subgraph overlap and oversmoothiong can be validated if nodes with higher degree exhibit more pronounced over-smoothing. To verify this point, we conduct experiments on three graph datasets: Cora, Citeseer, and Pubmed. Initially, nodes are grouped based on their degrees, with nodes having degrees falling within the range of [2i, 2i+1) assigned to the 3i-th group. Subsequently, we perform aggregation with different layers of GCN and GAT and then calculate the degree of smoothing of the node representations within each group separately. The metric proposed in [19] is used to measure the smoothness of the node representations within each group, namely smoothness metric value (SMV), which calculates the average distances between the nodes within the group: SMV(X) = 1 N(N − 1) X i̸=j D (Xi,:, Xj,:) , (3) where D(·, ·) denotes the normalized Euclidean distance between two vectors: D(x, y) = 1 2 \r\r\r\r x ∥x∥ − y ∥y∥ \r\r\r\r 2 . (4) From the definition, a smaller value of SMV indicates a greater similarity in node representations. We show the result of GAT in Figure 1. More results can be found in Appendix C. It can be observed that the groups of nodes with higher degree tend to be more similar to each other within the group in different layers. This finding supports our claim. 3.2 The role of residual method in alleviating over-smoothing After verifying the conclusion that neighborhood subgraph overlap leads to over-smoothing, a natural idea is to alleviate the overlap of the single neighborhood subgraph by utilizing multi- order neighborhood subgraph aggregations. In the following section, we show that the previous k-layer residual-based GNNs essentially represent different forms of utilizing neighborhood subgraph aggregations from 0 to k orders. Table 2: Utilization of neighborhood subgraphs by various residual methods. Model Closed/Iterative form formulas ResGCN Hk =Pk j=0 \u0000j k \u0001NjH APPNP Hk = (1−α)kNkH+α k−1P j=0 jP i=0 (−1)j−i(1−α)iNiH JKNet Hk = AGG(NH, . . . ,Nk−1H) DenseGCN Hk = AGG(Hk−1, ...,H1,H0) In the rest of this paper, we take GCN, a classical residual-free message-passing GNN, as an example. Assuming that H is non-negative, the ELU function and the weight matrix can be ignored for simplicity. Combined with the formula of GCN in Eq. 2, the k-order neighborhood subgraph aggregation can be formulated as NkH, where N = ˜D−1 2 ˜A ˜D−1 2 . To show more intuitively how different residual models utilize multi-order neighborhood subgraph aggregation NkH, we rewrite their formula in Table 2. Details of the derivation of the closed-form formula in this part are given in Appendix D. As can be observed, the output of GCN’s residual-based variants contains multi-order matrix products that represent different order neighborhood subgraph aggregations from 0 to k. There are two main ways to exploit them: (1) Summation, such as ResGCN and APPNP. Such methods employ linear summation over the aggregation of different order neighborhood subgraphs; (2) Aggregation functions, such as DenseGNN and JKNet. Such methods make direct and explicit exploitation of different order neighborhood subgraph aggregations through operations such as concatenation. However, the utilization of multi-order neighborhood subgraph aggregations in these methods presents the following issues: Firstly, the summation methods all use a fixed coefficient to sum the neighborhood subgraph aggregations. Consequently, these methods inherently presume that the information from the neighborhood subgraph of the same order is equally important for different nodes, which lacks node adaptivity. Secondly, for ResGNN, APPNP, and JKNet, when the number of layers increases, the output of these methods still involves many high-order matrix products that are over-smoothed. This leads to severe information loss when aggregating high-order neighborhood subgraphs, which in turn degrades model performance at deeper layers. Although DenseGNN seems to alleviate the above issues to some extent, the recursive use of all previous neighborhood subgraph aggregation would introduce more parameters as the model deepens. This increases memory consumption and raises the risk of gradient explosion at deeper layers. 44 The Proposed Method PSNR 4.1 Methodology To solve the above issues, we propose a node-adaptive and lightweight residual module named PSNR. The motivation is to learn the adaptive residual coefficients for each node, thereby achiev- ing fine-grained and node-level neighborhood subgraph aggregation to improve the performance of GNNs. However, directly learning these coefficients through backpropagation presents signif- icant challenges. The primary challenge lies in the lack of transferability of learned coefficients. Specifically, in tasks such as semi-supervised node classification, nodes in the test and validation sets often cannot propagate information to the training nodes through multiple message-passing steps. Therefore, we cannot learn effective coefficients for these nodes during the training phase. … 𝐻1 𝐻′𝑘−1 𝐻𝑘 PSNR  Module Encoder𝐻𝑘−1 Residual Connection GNN  Layer Sampling Posterior  Encoding Figure 2: The framework of PSNR Module. As a remedy, we regard node-level residual coefficients as hidden parameters. Our strategy involves estimating their posterior distribution P(ηk|A, Hk, k). In most cases, since the training, validation, and test sets orig- inate from the same distribution, the posterior distri- bution learned from the training data possesses trans- ferability to the validation and testing nodes. We can assume the posterior distribution to be Gaussian: ηk ∼ N(µ(A, Hk, k), σ2(A, Hk, k)). A graph encoder can be used to model this distribu- tion. This encoder leverages graph topology and node information as inputs. Subsequently, to enable back- propagation, we employ the reparameterization trick. This technique enables us to represent the sampling pro- cess from the aforementioned distribution as follows: ηk = µ(A, Hk, k) + ζ · σ(A, Hk, k), ζ∼ N(0, 1). Furthermore, we parameterize µ(A, Hk, k) and σ(A, Hk, k) as an arbitrary GNN layer. While the posterior distributions of residual coefficients vary across different layers, we do not parameterize a specific encoder for each layer. Instead, we employ the same graph encoder and use the positional embedding generated from the layer number k to differentiate the posterior distributions of residual coefficients for various layers. Consequently, the PSNR module can be formulated as follows: H′ k−1 = GraphConv (Hk−1) Hk = H1 + ϕ(diag(ηk−1)) \u0000 H1 − H′ k−1 \u0001 , ηk−1 ∼ N(µk−1, σk−1 2) µk, σk = GraphEncoderµ,σ(H1 − H′ k + γ LayerEmb(k)), (5) where GraphConv(·) is the k-th layer of any backbone GNN, Hk denotes the node representation matrix of the k-th layer, and H′ k represents the output matrix of the k-th layer. The first equation corresponds to the aggregation operation of the backbone GNN at the k-th layer. ηk represents the node-level residual coefficient at the k-th layer, where the element η(i) k corresponds to the residual coefficient at the i-th node. In addition, ηk follows a high-dimensional Gaussian distribution: N(µk, σk2), and each time before each residual calculation, the distribution is first sampled to obtain the exact residual coefficients. µk and σk represent the mean and standard deviation of the distribution, respectively. diag(ηk) represents a diagonal matrix transformed from ηk, where the i-th diagonal element is precisely the i-th element of ηk. ϕ(·) represents the sigmoid function, which constrains the residual coefficient to (0, 1). GraphEncoderµ,σ(·) is a posterior encoder, which can be any graph convolution layer that is independent of the backbone GNN. LayerEmb(k) represents the positional embedding [29] with layer number k as input. γ is a learnable coefficient serving as the layer embedding factor. The analysis of the residual coefficients can be found in Appendix H. Also, it is noteworthy that PSNR introduces randomness by sampling residual coefficients during both training and testing phases, thereby adding learnable random perturbations. This is different to other methods including DropEdge [24], GRAND [8] and DropMessage [7] that only incorporate randomness during the training phase and primarily introduce perturbations to node features and graph structure. We provide the theoretical analysis for this design in Section 4.2. Additionally, we clarify the difference between PSNR and other subgraph-based methods in Appendix F. 54.2 Theoretical justifications on the advantages of the PSNR module In this section, we theoretically show that the PSNR module achieves finer-grained and node-adaptive neighborhood subgraph aggregations while avoiding the loss of high-order subgraph information. Firstly, combining with Equation 5, the matrix form of the iterative formula for PSNR-GCN is: Hk = H1 + Λk−1 \u0010 H1 − ˜D−1/2 ˜A ˜D−1/2Hk−1 \u0011 . For simplicity, we use Λk to denote diag(ηk). Subsequently, based on this recursive form of formula, we derive the closed-form expression of PSNR-GCN as: Hk = k−1X i=2 k−1Y j=i ˜Nj(Mi − Mi−1) + k−1Y i=1 ˜Ni (H1 + M1) − Mk−1, where ˜Ni = −Λk−1N, and Mk = −(ΛkN + I)−1 (I + Λk) H1. The detailed derivation of this formula can be found in Appendix E. The first two terms consist of cumulative product terms of different orders, similar to the form of NkH, thus approximating a new version of neighborhood subgraph aggregation. Additionally, the formula involves the aggregation of all neighborhood subgraphs from 1 to k orders. This ensures that our method, like other residual methods, can comprehensively utilize multi-order neighborhood subgraph aggregations to enhance performance. Furthermore, since Λk is a diagonal matrix computed by a learnable posterior encoder with graph structure, node feature and layer number as input, the neighborhood subgraph aggregation of PSNR- GCN is fine-grained and node-adaptive. This sets it apart from methods like ResGCN and APPNP. Beyond that, we can also demonstrate that the PSNR module can reduce the information loss of high-order subgraph aggregation, thereby further improving the performance of GNNs at deeper layers. Specifically, we aim to prove that as the orderk increases, the smoothing rate of the cumulative product terms in PSNR is slower than that of the matrix power terms in other methods. Since we need to analyse the smoothing rate, which involves analyzing the asymptotic behavior of cumulative product terms or matrix power terms as the order increases, we can generalize the problem setup. Therefore, we only need to prove the following proposition to support our claim. Proposition 1 Let S = {Sj = Λ jN|j ∈ N0}, where Λj represents a random diagonal matrix with each diagonal element Λj,ii satisfying 0 < Λj,ii < 1, and let X be any feature matrix. Then, as the order k increases, the product of elements in the set S and the matrix X, denoted as X(k) = Qk i=1 SiX, converges to a rank-one matrix with identical rows slower than X(k) GCN = NkX. Since the diagonal elements of the diagonal matrix Λ are all results of the sigmoid function, we can assume that they have a lower bound ϵ >0. Therefore, in this setting, each element Si in the set S is a row-stochastic matrix satisfying the following property. Property 1 For a row-stochastic matrixSk, there exists an ϵ >0 satisfying the following conditions: 1. ϵ ≤ Sk,ij ≤ 1, if (i, j) ∈ E, 2. Sk,ij = 0, if (i, j) /∈ E. We can refer to the conclusion in [31]. [31] describes the attention matrix of GAT as a row-stochastic matrix satisfying Property 1. Leveraging mathematical tools such as joint spectral radius from the perspective of dynamical systems, this work proves that the convergence rate of GAT is bounded below by GCN. Due to all the matrix Si from S also satisfies Property 1, this conclusion can be borrowed to prove Proposition 1, thereby demonstrating that the PSNR module can alleviate the loss of high-order neighborhood subgraph information. Furthermore, this conclusion also explains the introduction of randomness during both the training and testing phases. This random perturbation further reduces the smoothing rate, thereby enhancing the model’s performance. 4.3 Complexity analysis We take PSNR-GCN as an example to provide a complexity analysis of the PSNR module. The time complexity of a vanilla GCN layer mainly comes from the matrix multiplication of N and H, hence its complexity is O(n2d). And the main computational parts of PSNR module are the computation of mean and standard deviation, sampling of p(i) k , scalar multiplication and matrix 6addition, which correspond to a complexity of O(n2d), O(n), O(nd), and O(nd), respectively. Thus the time complexity of the PSNR module isO(n2d) and the time complexity of a GCN layer equipped the PSNR module is O(n2d). As for space complexity, PSNR module needs to store the computed mean and variance for each node, i.e., O(2n), which can be approximately considered as O(n). Section 5.6 compares memory consumption with other residual methods on large graphs. 5 Experiment In this section, we assess the performance of the PSNR module in comparison to other methods and answer the following research questions ( RQ): RQ1. How well does the PSNR alleviate oversmoothing? RQ2. How does PSNR perform compared to other baseline when used with different backbone? RQ3. Can PSNR enable deeper networks to perform better under the missing feature scenario? RQ4. How scalable is the PSNR on large graph datasets? 5.1 Datasets We conducted experiments on ten real-world datasets, including three citation network datasets, i.e., Cora, Citeseer, Pubmed [ 27], two web network datasets, i.e., Chameleon and Squirrel [ 25], co-author/co-purchase network datasets, i.e., Coauthor-CS [28], Amazon-Photo [28] and three larger datasets, i.e., Flickr [20], Coauthor-Physics [28] and Ogbn-arxiv [10]. More details of these datasets and data-splitting procedures can be found in Appendix G. 5.2 Baselines and experimental settings We consider two fundamental GNNs, GCN [14] and GAT [30]. For GCN, we test the performance of PSNR-GCN and its residual variant models, including ResGCN [16], DenseGCN [16], GCNII [3] and JKNet [32]. For GAT, we directly equip it with the following residual module: Res-GAT, InitialRes-GAT, Dense-GAT, JK-GAT and PSNR-GAT. And we adopt the GraphSAGE layer as the GraphEncoder of the PSNR module. The impact of different graph encoders on the experiments can be found in Appendix I. In addition, we compare three recent representative methods belonging to different categories aimed at alleviating oversmoothing issues: DropMessage [ 7] for the drop category, DeProp [18] for the norm category, and Half-Hop [1] for graph data processing. For the missing feature setting, we also conduct comparisons with several classical oversmoothing mitigation techniques, including BatchNorm [11], PairNorm [33], DGN [34], Decorr [12], DropEdge [24]. For all baselines, the linear layers in the models are initialized with a standard normal distribution, and the convolutional layers are initialized with Xavier initialization. The Adam optimizer [13] is used for training. Experimental results are obtained from the server with four core Intel(R) Xeon(R) Platinum 8358 CPUs @ 2.60GHZ, one NVIDIA A100 GPU (80G), and models and datasets used in this paper are implemented using the Deep Graph Library (DGL) and Pytorch Geometric (PyG). Further details on the specific parameter settings can be found in Appendix G. 5.3 Effectiveness in mitigating over-smoothing (RQ1) In this section, we aim to assess whether the PSNR module can mitigate the oversmoothing phe- nomenon in deep GNNs. We select representative datasets Cora, Amazon-Photo, and Chameleon. Using GCN as the backbone network, we compare our method against several residual methods: ResGCN, GCNII, JKNet, and DenseGNN. We set the number of layers to 2, 4, 8, 16, 32, 64 and test on ten random splits, with the average accuracy serving as the final result. The experimental results are depicted in Figure 3. Consistent with the analysis in the main text, most methods can alleviate over-smoothing, but at deeper layers, such as 64 layer, over-smoothing still occurs. In contrast, compared to other residual methods, PSNR maintains stable performance even at deeper layers, demonstrating remarkable effectiveness in mitigating over-smoothing in deep GNNs. This is attributed to PSNR module can effectively reduce the loss of high-order neighborhood subgraph information. It is noteworthy that among the various residual methods, another initial residual method GCNII also alleviate over-smoothing well. However, the subsequent experiment will reveal that although the performance of GCNII remains relatively stable with varying layers, it leads to a decrease in overall performance. 721 22 23 24 25 26 Layers 30 40 50 60 70 80Accuracy (%) Cora 21 22 23 24 25 26 Layers 30 40 50 60 70 Amazon Photo 21 22 23 24 25 26 Layers 50 60 70 80 90 Chameleon Vanilla Res Init-Res Dense JK PSNR Figure 3: Different residual methods’ effectiveness in mitigating over-smoothing. Table 3: Summary of classification accuracy (%) results on real-world datasets. The best results are in bold, and the second best results are underlined. Method Cora Citeseer CS Photo Chameleon Squirrel Avg. Rank GCN 78.56 ±1.57 66.00±2.37 90.19±0.83 91.22±0.73 67.70±0.17 47.43±1.31 6.67ResGCN 80.11 ±0.98 66.40±2.29 90.86±0.99 91.31±0.85 70.70±1.54 52.43±1.72 3.67GCNII 77.40 ±1.43 66.69±1.95 90.34±0.78 91.46±0.84 69.69±1.36 46.13±1.59 5.00DenseGCN 74.94±2.46 62.54±2.56 90.28±1.06 90.38±1.02 67.82±1.27 49.57±1.58 7.17JKNet 79.42 ±1.48 65.49±2.32 90.94±0.89 90.85±1.19 68.26±1.15 49.87±1.44 5.17Half-Hop-GCN 80.74±1.80 67.76±2.19 90.30±1.83 89.32±1.17 60.07±2.34 42.78±1.01 6.33DropMessage-GCN 80.00±2.22 67.66±1.84 90.64±2.34 91.40±0.90 65.91±2.43 45.71±1.76 4.83DeProp-GCN 79.52±2.63 67.24±2.02 90.98±2.04 91.38±1.33 61.51±1.40 44.07±1.07 5.17PSNR-GCN 81.01±1.63 68.06±2.12 91.23±1.00 91.51±0.69 71.51±1.90 54.95±1.73 1.00 GAT 79.21 ±1.31 67.12±1.59 89.43±1.62 89.64±0.84 68.04±1.36 47.93±1.99 6.00Res-GAT 79.72 ±1.94 67.19±1.91 89.92±0.86 91.40±0.74 72.66±0.94 55.98±2.12 2.83Init-Res-GAT 79.67±1.71 66.84±2.52 89.61±1.02 91.53±1.06 69.89±1.81 51.29±1.42 4.17JK-GAT 80.04 ±1.61 65.83±2.21 90.10±0.94 90.82±1.24 67.98±1.71 50.43±1.45 4.50Dense-GAT 73.39±1.52 61.23±2.53 89.72±0.86 88.92±1.66 67.36±1.95 50.25±0.88 7.00DropMessage-GAT 80.36±1.92 67.82±2.04 90.67±1.68 90.98±0.90 63.23±2.23 45.23±1.35 4.33DeProp-GAT 76.00±2.08 61.16±3.50 87.34±1.42 89.76±1.52 64.23±3.22 46.29±3.24 8.00Half-Hop-GAT 77.24±1.69 66.74±1.98 89.66±1.45 89.92±0.77 62.86±2.04 47.84±4.36 6.83PSNR-GAT 80.47±1.62 68.01±2.14 90.38±1.21 91.64±0.61 72.24±1.69 60.85±1.61 1.33 5.4 Fully observed feature setting (RQ2) The PSNR module can effectively address the performance degradation of GNNs at deeper layers, but can it further enhance the overall performance of the model? In this section, we comprehensively evaluate the PSNR module across a wider range of datasets. Under the fully observed feature setting, we set the number of layers to 2, 4, 8, 16, 32, 64, and conduct experiments on ten random splits for each dataset, taking the average accuracy as the final result for each layer. The results of all layers of the model can be found in Appendix A. To evaluate the overall performance, we record the best results of each model in all layers for each dataset in Table 3. As can be observed from the Table 3, PSNR outperforms all baselines in most cases. For example, compared to the vanilla model, PSNR improves the test accuracy on the Squirrel dataset by 7.52% and 12.92%, respectively. Compared with the vanilla GCN and GAT, the proposed PSNR can significantly improve the performance under the fully observed feature setting. 5.5 Missing feature setting (RQ3) When do we need the deeper GNN? Real-world data often contain missing features. In that scenario, previous research [33] has shown that deep GNNs can help improve performance. For the nodes with missing features, due to the lack of information, they need a deeper network to gather more neighborhood information, thereby achieving better node representations. However, deep GNNs face the issue of performance degradation. In this section, we examine if PSNR module can improve the performance of GNNs in the context of missing feature. Consistent with [33, 18, 12], we evaluate the performance of GNNs on three datasets, Cora, Citeseer, and Pubmed, and remove their node features from validation and test sets. Under this setting, the test nodes need more propagation layers to reach the training nodes. We reuse the metrics that already reported in [12] for None, BatchNorm [11], PairNorm [33], DGN [34], DeCorr [12], and DropEdge [24]. For all residual-based models, the results are obtained by varying the number of layers in 8Table 4: Test accuracy (%) on missing feature setting. The best results are in bold and the second best results are underlined. GCN GAT Module Cora Citeseer Pubmed Cora Citeseer Pubmed Acc #K Acc #K Acc #K Acc #K Acc #K Acc #K None 57.3 3 44.0 6 36.4 4 50.1 2 40.8 4 38.5 4 BatchNorm 71.8 20 45.1 25 70.4 30 72.7 5 48.7 5 60.7 4 PairNorm 65.6 20 43.6 25 63.1 30 68.8 8 50.3 6 63.2 20 DGN 76.3 20 50.2 30 72.0 30 75.8 8 54.5 5 72.3 20 DeCorr 73.8 20 49.1 30 73.3 15 72.8 15 46.5 6 72.4 15 DropEdge 67.0 6 44.2 8 69.3 6 67.2 6 48.2 6 67.2 6 Res 76.8 8 60.4 10 76.6 6 76.5 8 60.5 6 76.9 8 Init-Res 65.1 6 50.7 15 70.4 10 77.1 8 60.6 8 76.8 6 Dense 66.2 4 51.5 2 74.1 8 68.5 10 52.7 2 75.1 10 JK 75.5 30 60.4 8 76.9 6 77.0 10 60.3 4 76.8 6 DropMessage 75.5 10 61.0 6 74.6 6 76.5 6 61.1 8 76.6 6 DeProp 71.4 6 59.4 2 76.1 4 68.04 2 48.3 2 75.8 4 Half-Hop 73.7 8 59.48 6 76.5 6 76.0 20 59.6 4 76.9 6 PSNR 77.3 20 61.1 15 77.0 30 77.9 8 61.9 15 77.3 10 {2, 4, 6, 8, 10, 15, 20, 30} and running five times for each number of layers. We select the layer #K that achieves the best performance and report its average accuracy. The results are reported in Table 4. By examining the results in Table 4, under the missing feature setting, the optimal number of layers to achieve the best performance is significantly higher than in the fully observed feature setting, demonstrating the importance of deep GNNs. And PSNR outperform other baselines in all cases through alleviating over-smoothing more effectively. Specially, on the Pubmed dataset, PSNR boost the accuracy of GCN and GAT by 40.6% and 38.8%, respectively. 5.6 Performance on large graphs (RQ4) To validate the scalability of PSNR, we conducted additional experiments on three larger graph datasets i.e., Coauthor-Physics, Flickr and Ogbn-arxiv, to further validate the effectiveness and scalability of our method. Specifically, we selected the GCN backbone for our experiment. We report the performance of GCN and various residual methods on three datasets and the memory consumption on the largest dataset, Ogbn-arxiv. The experimental results are presented in Table 5, from which we observe that PSNR-GCN scales well and achieves the best results across all three large datasets. Meanwhile, in terms of memory consumption, PSNR is slightly higher than GCN and ResGCN, comparable to GCNII with the same initial residuals, and significantly lower than JKNet and DenseGCN. Regarding training time, PSNR is roughly at the same level as JKNet, and its time is shorter than that of DenseNet. Table 5: Comparison of different methods across various datasets and memory consumption (MB) and training time (ms / epoch) on Ogbn-arxiv. The best performance for each dataset is in bold, while the second best is underlined. Method Phy Flickr Ogbn-arxiv Memory Time GCN 95.32 ± 0.11 51.40 ± 0.33 64.37 ± 0.41 2421 30.10 ResGCN 95.61 ± 0.18 51.90 ± 0.16 66.32 ± 0.59 2463 36.06 GCNII 95.90 ± 0.14 46.18± 0.21 61.43 ± 1.63 2525 33.33 JKNet 95.88 ± 0.15 51.65 ± 0.31 60.46 ± 1.21 2921 40.05 DenseGCN 95.50 ± 0.12 52.18 ± 0.25 62.46± 1.58 3131 52.24 PSNR-GCN 95.92± 0.17 52.47 ± 0.16 67.81 ± 0.57 2539 42.93 6 Conclusion and Future Work In this paper, we addressed the oversmoothing in Graph Neural Networks (GNNs) with a focus on residual methods. We revisit the oversmoothing from the perspective of overlapping neighborhood subgraphs, explaining why residual methods can alleviate it. Our analysis revealed that current residual methods often lack node adaptivity and struggle with information loss in high-order neigh- borhoods subgraphs. To overcome these limitations, we introduce the Posteriori-Sampling-based Node-Adaptive Residual Module (PSNR). This innovative module uses a graph encoder to learn the posterior distribution of residual coefficients for each node at different layers, enabling fine-grained, 9node-adaptive neighborhood subgraph aggregation with minimal overhead. Extensive experiments confirmed that the PSNR module can effectively mitigate oversmoothing and improve performance, particularly in scenarios requiring deep networks. Despite the significant progress made by PSNR, training a deeper GNN remains challenging, prompting the need for further research in the future. 7 Acknowledgements This work was supported by National Science and Technology Major Project (No. 2022ZD0115101), National Natural Science Foundation of China Project (No. 623B2086, No. U21A20427), Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University and Integrated Bioengineering of Westlake University and Project (No. WU2023C019) from the Westlake University Industries of the Future Research Funding. Carl Yang was not supported by any funds from China. References [1] Mehdi Azabou, Venkataramana Ganesh, Shantanu Thakoor, Chi-Heng Lin, Lakshmi Sathidevi, Ran Liu, Michal Valko, Petar Velickovic, and Eva L. Dyer. Half-hop: A graph upsampling approach for slowing down message passing. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors,International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1341–1360. PMLR, 2023. [2] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [3] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In Proceedings of ICML, volume 119, pages 1725–1735, 2020. [4] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of NeurIPS, pages 2224–2232, 2015. [5] Moshe Eliasof, Eldad Haber, and Eran Treister. PDE-GCN: novel architectures for graph neural networks motivated by partial differential equations. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors,Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages 3836–3849, 2021. [6] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In Proceedings of WWW, pages 417–426, 2019. [7] Taoran Fang, Zhiqing Xiao, Chunping Wang, Jiarong Xu, Xuan Yang, and Yang Yang. Dropmes- sage: Unifying random dropping for graph neural networks. In Brian Williams, Yiling Chen, and Jennifer Neville, editors, Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thir- teenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 4267–4275. AAAI Press, 2023. [8] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of CVPR, pages 770–778, 2016. [10] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. CoRR, abs/2005.00687, 2020. 10[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of ICML, volume 37, pages 448–456, 2015. [12] Wei Jin, Xiaorui Liu, Yao Ma, Charu C. Aggarwal, and Jiliang Tang. Feature overcorrelation in deep graph neural networks: A new perspective. In Proceedings of SIGKDD, pages 709–719, 2022. [13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. InProceedings of ICLR, 2015. [14] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of ICLR, 2017. [15] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In Proceedings of ICLR, 2019. [16] Guohao Li, Matthias Müller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In Proceedings of ICCV, pages 9266–9275, 2019. [17] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of AAAI, pages 3538–3545, 2018. [18] Hua Liu, Haoyu Han, Wei Jin, Xiaorui Liu, and Hui Liu. Enhancing graph representations learn- ing with decorrelated propagation. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors,Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 1466–1476. ACM, 2023. [19] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors,KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 338–348. ACM, 2020. [20] Julian McAuley and Jure Leskovec. Image labeling on a network: using social-network metadata for image classification. In Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part IV 12 , pages 828–841. Springer, 2012. [21] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In Proceedings of ICLR, 2020. [22] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of SIGKDD, pages 701–710, 2014. [23] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. In NeurIPS, 2022. [24] Y . Rong, Wen-bing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. Proceedings of ICLR, 2019. [25] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal of Complex Networks, 9(2), 2021. [26] T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph neural networks. CoRR, abs/2303.10993, 2023. [27] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi- Rad. Collective classification in network data. AI Magazine, 29(3):93–106, 2008. [28] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [30] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, P. Lio’, and Yoshua Bengio. Graph attention networks. Proceedings of ICLR, 2017. [31] Xinyi Wu, Amir Ajorlou, Zihui Wu, and Ali Jadbabaie. Demystifying oversmoothing in attention-based graph neural networks. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing 11Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [32] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of ICML, pages 5449–5458, 2018. [33] Lingxiao Zhao and L. Akoglu. Pairnorm: Tackling oversmoothing in gnns. Proceedings of ICLR, 2019. [34] Kaixiong Zhou, Xiao Huang, Yuening Li, D. Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. Proceedings of NeurIPS, 2020. 12A Fully Observed Node Classification Table 6: Node classification accuracy (%) on GCN backbone. The best results across different layers are highlighted. Datasets Model Layer 2 Layer 4 Layer 8 Layer 16 Layer 32 Layer 64 Cora GCN 78.56±1.57 78.52±2.02 77.27±2.29 72.11±6.30 46.09±12.12 30.76±1.30 ResGCN 79.39 ±1.12 79.80±1.53 80.10±1.46 80.11±0.98 71.27±3.65 36.48±7.53 GCNII 77.40±1.43 76.47±1.96 76.39±1.88 76.15±1.70 75.76±1.83 76.27±1.46 DenseGCN 74.94±2.46 67.06±2.02 67.58±2.66 65.08±3.71 59.98±2.30 34.37±5.69 JKNet 78.44 ±2.10 79.42±1.48 79.36±2.23 77.18±2.17 77.33±2.88 51.58±7.29 DropMessage-GCN80.00±2.22 79.88±1.81 78.14±1.65 76.40±1.22 52.18±1.06 30.34±0.63 DeProp-GCN 79.52±2.63 78.44±2.39 33.02±5.64 31.28±3.24 30.20±0.01 30.20±0.01 Half-Hop-GCN 78.92±2.08 80.74±1.80 80.24±1.61 46.66±13.37 43.68±7.14 31.03±7.39 PSNR-GCN 80.59 ±1.57 81.01±1.63 80.55±1.57 78.26±1.36 76.34±2.18 77.75±2.27 Citeseer GCN 66.00±2.37 64.13±2.31 64.13±2.10 58.52±3.31 27.21±3.98 27.52±5.04 Res-GCN 66.11 ±1.65 66.40±2.29 65.97±1.93 65.46±2.02 48.70±3.77 33.74±4.79 GCNII 66.69±1.95 66.18±1.74 66.50±1.77 66.31±2.08 66.27±1.92 66.50±2.59 Dense-GCN 62.54±2.56 58.99±3.74 54.35±5.42 50.10±5.24 49.09±4.03 31.43±5.54 JKNet 65.49±2.32 64.40±2.28 64.18±3.07 63.77±1.87 60.88±3.53 29.66±7.51 DropMessage-GCN67.66±1.84 67.04±2.20 63.84±2.66 59.82±2.75 22.68±3.77 21.38±1.72 Half-Hop-GCN 67.76±2.19 67.46±1.99 67.02±2.53 54.48±3.93 38.40±8.33 24.14±3.48 DeProp-GCN 67.24±2.02 64.86±2.93 29.08±10.03 21.72±0.91 21.94±1.18 20.98±1.13 PSNR-GCN 68.06±2.12 66.03±1.93 65.46±1.59 65.81±2.39 65.52±1.76 65.85±2.30 CS GCN 90.19±0.83 88.81±0.62 86.93±0.83 84.47±0.95 71.40±4.93 36.74±4.57 Res-GCN 90.86±0.99 90.63±0.96 89.97±0.90 88.40±0.90 85.01±1.53 64.39±3.86 GCNII 90.34±0.78 90.02±0.81 90.07±0.77 90.09±0.94 90.17±0.52 89.93±0.70 Dense-GCN 89.01 ±1.09 89.99±1.27 90.28±1.06 89.33±1.73 88.92±1.08 88.40±1.37 JKNet 90.81 ±1.35 90.94±0.89 90.53±1.41 89.52±1.11 88.81±1.14 88.44±1.44 DropMessage-GCN90.64±2.34 89.18±1.76 89.38±1.52 85.62±7.01 87.94±6.18 84.44±1.13 Half-Hop-GCN 90.30±1.83 89.30±2.23 88.98±1.81 82.98±1.88 54.60±4.55 25.36±10.27 DeProp-GCN 90.98±2.04 89.10±2.23 71.24±4.18 72.34±2.26 52.60±0.03 22.60±0.01 PSNR-GCN 91.23±1.00 90.70±1.49 90.26±1.17 90.26±0.98 90.52±1.02 90.30±0.88 Photo GCN 91.22±0.73 90.49±0.76 88.10±1.02 80.05±4.25 54.80±8.00 46.57±9.64 Res-GCN 91.31±0.85 90.97±0.78 90.71±0.78 85.98±2.36 65.42±6.75 56.15±10.43 GCNII 91.02 ±0.93 90.98±0.92 91.02±0.70 90.99±0.76 91.20±0.81 91.46±0.84 Dense-GCN 90.38±1.02 90.07±1.76 89.34±1.40 87.77±2.00 86.01±1.91 68.22±17.58 JKNet 87.26 ±1.77 87.96±1.91 84.39±2.76 90.85±1.19 90.10±1.20 87.93±2.66 DropMessage-GCN91.40±0.90 90.22±1.19 87.57±2.74 87.82±1.21 86.12±1.05 80.40±1.07 Half-Hop-GCN 52.00±17.28 67.48±24.16 89.32±1.17 83.66±3.02 64.40±4.30 39.00±9.42 DeProp-GCN 91.38±1.33 89.50±5.31 78.12±3.66 81.86±9.36 87.74±4.19 84.43±2.31 PSNR-GCN 91.44 ±0.82 91.20±1.03 91.39±0.76 91.11±0.68 91.51±0.69 91.49±0.88 Chameleon GCN 67.70±0.17 56.30±2.28 48.37±1.56 46.81±2.06 39.62±2.13 33.27±1.86 Res-GCN 70.28 ±1.14 70.70±1.54 69.18±0.78 51.64±2.11 42.61±3.60 31.42±2.73 GCNII 69.45 ±1.63 69.36±1.13 69.62±1.61 69.23±1.21 68.87±2.16 69.69±1.36 Dense-GCN 67.82±1.27 64.59±1.29 65.36±1.50 64.39±1.37 30.63±3.98 28.72±4.66 JKNet 68.26±1.15 67.97±1.59 66.61±1.76 67.78±1.60 66.46±2.25 52.98±2.79 DropMessage-GCN65.91±2.43 60.50±2.84 47.29±2.00 42.66±1.60 34.56±1.61 31.74±1.56 Half-Hop-GCN 60.07±2.34 55.72±3.12 52.64±1.99 43.19±2.55 36.26±2.12 29.34±2.20 DeProp-GCN 61.51±1.40 58.25±2.22 46.46±2.18 31.88±2.52 30.81±2.07 33.36±0.96 PSNR-GCN 71.51±1.90 70.74±2.24 71.40±1.83 66.00±1.97 65.82±2.02 69.67±4.59 Squirrel GCN 47.43±1.31 41.68±1.80 37.43±1.35 33.17±1.22 31.99±1.14 29.83±1.98 Res-GCN 51.72 ±1.63 52.43±1.72 50.38±1.90 38.35±1.56 26.30±2.06 22.66±1.01 GCNII 46.13±1.59 45.28±1.59 45.71±2.08 45.37±1.72 45.06±1.50 45.32±1.85 Dense-GCN 49.57±1.58 49.01±0.98 49.51±1.30 49.08±1.36 49.29±1.25 49.04±1.30 JKNet 49.87±1.44 49.11±2.27 46.62±1.44 24.79±3.88 45.73±1.80 41.75±1.96 DropMessage-GCN45.71±1.76 38.68±2.02 26.67±3.06 25.71±1.15 23.77±1.18 20.06±0.23 Half-Hop-GCN 42.78±1.01 41.87±1.36 37.84±1.68 31.60±2.12 26.36±1.66 21.07±1.53 DeProp-GCN 44.07±1.07 40.98±1.29 33.38±1.70 28.33±0.96 28.42±0.54 29.98±2.78 PSNR-GCN 54.95±1.73 54.13±1.41 50.68±1.45 50.22±6.32 50.06±0.97 50.24±1.58 13Table 7: Node classification accuracy (%) on GAT backbone. The best results across different layers are highlighted. Datasets Model Layer 2 Layer 4 Layer 8 Layer 16 Layer 32 Layer 64 Cora GAT 79.21±1.31 78.83±1.78 41.48±2.84 30.20±0.00 30.20±0.00 30.20±0.00 Res-GAT 79.42 ±1.46 79.72±1.94 78.88±1.69 78.71±2.13 77.80±2.25 30.22±0.12 InitRes-GAT 79.32 ±1.65 79.39±1.45 79.00±1.97 79.16±1.68 79.67±1.71 79.42±1.97 Dense-GAT 73.39±1.52 69.07±2.76 58.85±3.02 60.34±2.12 59.44±3.03 55.80±3.12 JK-GAT 79.32 ±1.06 80.04±1.61 77.28±1.62 77.58±1.51 77.53±1.29 77.58±2.91 DropMessage-GAT 80.06±2.50 80.36±1.92 79.20±0.22 76.54±2.49 52.54±1.02 30.17±0.06 DeProp-GAT 76.00±2.08 69.06±3.94 30.22±0.06 31.20±0.02 30.20±0.01 30.20±0.01 Half-Hop-GAT 77.20±1.86 76.98±1.42 77.24±1.69 75.84±1.68 70.50±4.69 30.20±0.10 PSNR-GAT 80.47±1.62 80.22±0.98 79.96±1.69 80.02±1.64 79.43±1.78 79.69±1.29 Citeseer GAT 67.12±1.59 65.14±3.02 21.90±2.40 21.10±0.00 21.10±0.00 21.10±0.00 Res-GAT 67.19±1.91 65.65±2.21 63.26±2.31 63.47±1.18 62.82±2.92 22.66±1.31 InitRes-GAT 66.84±2.52 64.15±2.44 65.80±1.89 64.89±2.65 64.69±2.49 65.11±2.65 Dense-GAT 61.63±2.53 54.84±2.34 50.69±3.79 49.21±3.48 47.24±3.08 46.34±3.62 JK-GAT 65.83±2.21 64.79±2.46 63.44±2.51 65.00±1.43 63.57±2.27 62.08±2.26 DropMessage-GAT67.82±2.04 66.64±2.51 65.10±3.32 58.20±2.55 21.90±1.64 20.90±2.04 DeProp-GAT 61.16±3.50 51.52±11.25 23.20±2.77 21.12±1.76 21.00±1.67 20.50±0.92 Half-Hop-GAT 66.74±1.98 66.70±2.58 63.12±3.49 60.00±3.30 45.98±4.63 24.32±4.46 PSNR-GAT 68.01±2.14 65.61±2.05 66.46±2.14 65.48±2.59 66.36±2.25 65.50±2.35 CS GAT 89.43±1.62 77.23±10.14 72.63±5.30 38.81±6.52 22.60±0.00 22.60±0.00 Res-GAT 89.48 ±1.25 83.48±3.39 89.92±0.86 64.30±19.93 23.10±1.45 22.46±2.22 InitRes-GAT 89.61±1.02 77.82±5.90 89.43±1.23 88.50±1.17 88.20±1.22 86.90±2.30 Dense-GAT 89.72±0.86 88.81±0.89 88.28±0.72 87.01±1.52 86.47±1.13 87.23±1.78 JK-GAT 89.77 ±1.02 90.10±0.94 90.03±1.26 89.68±1.41 89.66±1.04 89.86±1.12 DropMessage-GAT90.67±1.68 89.20±1.58 87.58±1.46 85.68±1.29 84.58±1.40 81.43±1.13 DeProp-GAT 87.34±1.42 84.18±1.99 72.26±2.77 72.36±2.34 52.60±0.02 22.60±0.01 Half-Hop-GAT 89.66±1.45 89.30±1.84 84.12±2.27 84.75±2.96 56.20±5.96 29.61±9.70 PSNR-GAT 90.38±1.12 86.31±2.68 84.48±1.24 83.84±1.38 81.85±3.55 79.85±5.24 Photo GAT 89.64±0.84 42.4±18.86 51.13±10.00 27.97±7.71 25.40±0.00 25.40±0.00 Res-GAT 89.03 ±1.33 89.30±4.68 91.40±0.74 88.60±1.79 25.61±4.23 26.16±1.88 InitRes-GAT 88.92 ±2.22 31.74±3.12 91.23±1.22 90.72±1.40 91.53±1.06 91.11±1.21 Dense-GAT 88.92±1.66 88.10±1.50 87.03±2.22 86.79±1.97 86.24±2.65 85.63±2.15 JK-GAT 87.55 ±1.39 90.82±1.24 90.35±1.61 90.30±1.37 90.29±1.49 90.07±1.48 DropMessage-GAT90.98±0.90 90.29±0.97 82.15±2.82 84.96±1.56 85.10±0.90 82.15±1.32 DeProp-GAT 89.76±1.52 87.98±1.05 82.54±1.01 81.25±2.88 82.26±2.17 80.00±2.11 Half-Hop-GAT 89.92±0.77 85.48±1.89 85.16±2.77 72.12±1.54 67.84±1.08 45.51±0.65 PSNR-GAT 90.93 ±1.42 91.48±0.94 91.33±1.05 91.05±0.95 91.64±0.61 91.37±0.96 Chameleon GAT 68.04±1.36 48.37±3.71 26.86±6.17 22.86±0.00 22.86±0.00 22.86±0.00 Res-GAT 70.90 ±1.34 67.87±3.43 72.66±0.94 66.68±1.13 27.54±3.63 27.27±3.44 InitRes-GAT 69.87 ±1.65 61.63±5.02 69.89±1.81 69.34±1.34 68.62±1.32 68.53±1.23 Dense-GAT 67.36±1.95 66.87±2.07 66.21±2.03 61.95±2.14 62.32±2.10 61.71±1.69 JK-GAT 67.98±1.71 65.71±1.40 66.75±1.88 66.99±2.23 66.24±1.42 66.31±1.56 DropMessage-GAT63.23±2.23 56.98±2.54 48.53±2.95 47.00±1.93 40.19±1.82 33.75±2.23 DeProp-GAT 64.23±3.22 53.31±4.19 42.29±2.77 41.19±2.17 32.29±2.77 32.26±0.85 Half-Hop-GAT 62.86±2.04 60.34±2.01 50.58±7.19 43.05±1.74 33.75±2.37 32.27±1.42 PSNR-GAT 71.29 ±1.64 72.04±1.82 71.58±1.99 71.78±1.51 71.47±2.54 72.24±1.69 Squirrel GAT 47.93±1.99 32.96±1.85 20.57±1.02 20.00±0.00 20.00±0.00 20.00±0.00 Res-GAT 52.87 ±1.68 49.87±4.74 55.98±2.12 50.31±1.68 23.07±1.98 22.01±1.26 InitRes-GAT 51.29±1.42 43.92±5.77 49.83±1.55 50.34±1.17 50.30±1.81 50.46±1.75 Dense-GAT 50.25±0.88 49.57±1.80 46.71±1.69 47.83±1.70 47.54±1.58 46.87±1.65 JK-GAT 50.43±1.45 44.41±2.33 49.08±0.79 49.31±1.99 48.87±1.74 49.56±1.31 DropMessage-GAT45.23±1.35 40.31±1.25 30.69±1.84 29.44±1.34 28.35±2.50 25.47±1.33 DeProp-GAT 45.76 ±1.32 46.29±3.24 29.99±0.28 28.97±0.56 28.91±0.23 20.00±0.04 Half-Hop-GAT 43.46±1.57 47.84±4.36 42.53±1.53 29.19±1.50 24.37±1.54 23.79±0.65 PSNR-GAT 57.81 ±2.08 60.85±1.61 59.58±2.09 60.43±2.20 60.00±2.20 60.20±1.53 14B Residual Connection Defination Common residual connection for GNNs and their corresponding GNNs are described below. Res. Res is composed of multiple residual blocks containing few stacked layers. Taking the initial input of the n-th residual block as Xn, and the stacked nonlinear layers within the residual block as F(X): Xn+1 = F(Xn) + Xn, where residual mapping and identity mapping refer to F(X) and X on the right side of the above equation, respectively. Inspired by Res, Guohao Li & Matthias Müller(2019) proposed a residual connection learning framework for GCN and called this model ResGCN which can be simply described as follows: Hk = σ \u0010 ˜D−1 2 ˜A ˜D−1 2 Hk−1Wk−1 \u0011 + Hk−1. InitialRes. InitialRes is proposed for the first time in APPNP, unlike Res that carries information from the previous layers, it constructs a connection to the initial representation X0 at each layer: Xn+1 = (1 − α)H(Xn) + αX0, where H(X) denotes the aggregation operation within one layer. InitialRes ensures that each node’s representation retains at least an α-sized portion of the initial feature information. Correspondingly, APPNP can be formulated as: Hk = (1 − α) ˜D−1 2 ˜A ˜D−1 2 Hk−1 + αH. Based on APPNP, GCNII introduces identity mapping from Res to make up for the deficiency in APPNP. Dense. Dense proposes a more efficient way to reuse features between layers. The input is the outputs of all previous layers of the network and at each layer Dense concats them together: Xn+1 = H([X0, X1, . . . ,Xn]), where [·] denotes the concatenation of the feature map for the output of layers 0 to n. Inspired by Dense, DenseGCN applies a similar idea to GCN, i.e., let the output of the k-th layer contains transformations from all previous GCN layers to exploit the information from different GCN layers: Hk = AGGdense(H, H1, . . . ,Hk−1). JK. At the last layer, JK sifts from all previous representations [X1, . . . ,XN ] and combines them: Xoutput = AGG(X1, . . . ,XN ). The AGG operation includes concatenation, Maxpooling and LSTM-attention. When it is introduced to GNN, i.e., JKNet, can be formulated as: Houtput = AGGjk(H1, . . . ,Hk−1). 15C SMV for Node Groups of Different Degrees 20 21 22 23 24 25 26 Degree 0.1 0.2 0.3 0.4SMV Cora 20 21 22 23 24 25 Degree 0.1 0.2 0.3 Citeseer 20 21 22 23 24 25 26 27 Degree 0.1 0.2 0.3 0.4 Pubmed GCN-2 GCN-4 GCN-8 GCN-16 GCN-32 GCN-64 Result of GCN. 20 21 22 23 24 25 26 Degree 0.0 0.1 0.2 0.3SMV Cora 20 21 22 23 24 25 Degree 0.15 0.20 0.25 0.30 0.35 Citeseer 20 21 22 23 24 25 26 27 Degree 0.0 0.1 0.2 0.3 Pubmed GAT-2 GAT-4 GAT-8 GAT-16 GAT-32 GAT-64 Result of GAT. D Derivation of the closed-form formulas in the table ResGCN: We can write the recursive formula for ResGCN in the following form: Hk = (I + N)Hk−1. (6) In turn, the following form can be obtained by recursion: Hk = (I + N)kH. (7) Using the binomial theorem, we can obtain the closed-form formula for ResGCN as follows: Hk = kX j=0 Cj kNjH. (8) APPNP: According to the recurrence formula of APPNP: Hk = αH + (1 − α)NHk−1. (9) To obtain the closed-form formula, we can add a term T to both sides of the equation: Hk + T = (1 − α) NHk−1 + αH + T. (10) We aim to transform the equation into the following form: Hk + T = (1 − α) N (Hk−1 + T) . (11) Then we need to make sure that there exists a very T that satisfies the following equation: (1 − α) NT = αH + T, (12) which can be transformed into the following form: ((1 − α) N − I) T = αH. (13) We can proof the following lemma: 16Lemma 1 Given that α ∈ (0, 1), (1 − α) N − I is invertible. Proof 1 Proving that (1 − α) N − I is invertible is equivalent to demonstrating that it does not possess an eigenvalue of 0. Consider the Rayleigh quotient of (1 − α) N − I: XT ((1 − α) N − I) X XT X = (1 − α) XT \u0010 ˜D−1 2 ˜A ˜D−1 2 \u0011 X XT X − 1. (14) From spectral graph theory, we can know the following equation holds: XT \u0010 ˜D−1 2 L ˜D−1 2 \u0011 X = X (vi,vj )∈E   Xi√di + 1 − Xjp dj + 1 !2 > 0. (15) We can decompose L into ˜D − ˜A, then we have: XT \u0010 ˜D−1 2 ˜D ˜D−1 2 \u0011 X XT X − XT \u0010 ˜D−1 2 ˜A ˜D−1 2 \u0011 X XT X > 0, (16) which is equivalent to: XT \u0010 ˜D−1 2 ˜A ˜D−1 2 \u0011 X XT X < XT IX XT X = 1. (17) Combining Eq. 14 and Ineq. 17, we can obtain: XT ((1 − α) N − I) X XT X < (1 − α) − 1 = −α <0. (18) Hence, 0 can’t be the eigenvalue of (1 − α) N − I . Therefore, (1 − α) N − I is invertible. Since Lemma 1 holds, we can derive the concrete form of T: T = α ((1 − α) N − I)−1 H. (19) Thus we can keep recurring from Eq. 11 and obtain the following equation: Hk + T = ((1 − α) N)k (H + T) , (20) which also can be written as: Hk = ((1 − α) N)k H + ((1 − α) N)k T − T. (21) For the second and third terms in Eq. 21, we write (1 − α) N as (1 − α) N − I + I. We can use the binomial theorem to write ((1 − α) N)k as kP j=0 ((1 − α) N − I)j , then Eq. 21 can be written as : Hk = ((1 − α) N)k H + kX j=1 ((1 − α) N − I)j T. (22) Bring in the specific form of T and further derive the closed-form formula of APPNP: Hk = ((1 − α) N)k H + α k−1X j=0 ((1 − α) N − I)j H (23) = (1 − α)k NkH + α k−1X j=0 jX i=0 (−1)j−i (1 − α)i NiH. (24) 17E Derivation of the closed-form formula of PSNR-GCN For each diagonal element Λk,ii of Λk, it is trivial to obtain: 0 < Λk,ii < 1. To derive the closed-form formula of PSNR-GCN, we need proof the following lemma first. Lemma 2 Set all the diagonal elements of Λ to satisfy 0 < Λii < 1, then (ΛN + I) is invertible. Proof 2 Proving that ΛN + I is invertible is equivalent to demonstrating that its determinant is not equal to 0. Because all the diagonal elements of Λ satisfy 0 < Λii < 1, then Λ is invertible. And due to |ΛN + I| = |Λ||N + Λ−1|. (25) Therefore, proving that its determinant is not equal to 0 is equivalent to demonstrating that|N+Λ−1| is not equal to 0, and further equivalent to demonstrating that N + Λ−1 does not have an eigenvalue of 0. Consider the Rayleigh quotient of N + Λ−1: R1 = XT \u0000 N + Λ−1\u0001 X XT X . (26) Split Eq. 26, and we can derive: R1 = XT NX XT X + XT Λ−1X XT X . (27) The second term of Eq. 27 can be easily written as follows: XT Λ−1X XT X = PN i=1 Λii −1x2 iPN i=1 x2 i . Since 0 < Λii < 1, therefore Λii −1 > 1, then XT Λ−1X XT X > 1. (28) For the first item, we write its specific form as follows: XT NX XT X = XT \u0010 ˜D−1 2 ˜A ˜D−1 2 \u0011 X XT X . (29) From spectral graph theory, we know that the following formula holds: XT \u0010 ˜D−1 2 (A + D) ˜D−1 2 \u0011 X = X (vi,vj )∈E   Xi√di + 1 + Xjp dj + 1 !2 > 0. (30) Further mathematically transforming this formula, we can get the following form: XT \u0010 ˜D−1 2 (A + D) ˜D−1 2 \u0011 X XT X = XT \u0010 ˜D−1 2 \u0010 ˜A + ˜D − 2I \u0011 ˜D−1 2 \u0011 X XT X (31) = XT \u0010 ˜D−1 2 ˜A ˜D−1 2 \u0011 X XT X + XT \u0010 ˜D−1 2 ˜D ˜D−1 2 \u0011 X XT X − 2XT ˜D−1X XT X (32) = XT \u0010 ˜D−1 2 ˜A ˜D−1 2 \u0011 X XT X + 1 − 2XT ˜D−1X XT X > 0. (33) 18Further, we get the following result: XT \u0010 ˜D−1 2 ˜A ˜D−1 2 \u0011 X XT X > 2XT ˜D−1X XT X − 1. (34) It is trivial to obtain: 2XT ˜D−1X XT X = 2 PN i=1 (di + 1)−1 x2 iPN i=1 x2 i > 0. (35) Combining Eq. 27, Ineq. 28, Ineq. 34 and Ineq. 35, we can get the following inequality: XT \u0010 ˜D−1 2 ˜A ˜D−1 2 + Λ−1 \u0011 X XT X > 0. (36) It can be obtained that the eigenvalue of ˜D−1 2 ˜A ˜D−1 2 + Λ−1 is greater than 0, so 0 is not an eigenvalue of it. Further, ΛN + I is invertible. Now, we derive the closed form of the formula. Given the following recursive formula: Hk = H1 + Λk−1 \u0010 H1 − ˜D−1/2 ˜A ˜D−1/2Hk−1 \u0011 , (37) where H1 = ˜D−1/2 ˜A ˜D−1/2H , Λk = diag{λ(1) k , ...λ(n) k }, λ(i) k ∼ Sigmoid(N(α(i) k , β(i) k 2 )). After mathematical transformation, Eq. 37 can be written as: Hk = (I + Λk−1) H1 − Λk−1 ˜D−1/2 ˜A ˜D−1/2Hk−1. (38) Set N = ˜D−1/2 ˜A ˜D−1/2, then Eq. 38 can be abbreviated as: Hk = (I + Λk−1) H1 − Λk−1NHk−1. (39) We try to modify Eq. 39 to the form that is more suitable for obtaining the closed form of the formula: Hk + Mk−1 = −Λk−1N (Hk−1 + Mk−1) . (40) To verify whether there exists such M that satisfies the equation, we need to solve the following equation: −Λk−1NMk−1 = (I + Λk−1) H1 + Mk−1, (41) which is equivalent to the following form: −(Λk−1N + I) Mk−1 = (I + Λk−1) H1. (42) Based on the definition, all the diagonal elements ofΛk satisfy 0 < λ(i) k < 1, so according to Lemma 2, (Λk−1N + I) is invertible. Then Mk−1 = −(Λk−1N + I)−1 (I + Λk−1) H1, which means such Mk−1 that we require exists. First, we perform the following mathematical transformation on Eq. 40: Hk + Mk−1 = −Λk−1N (Hk−1 + Mk−2 + Mk−1 − Mk−2) , (43) which can be split into the following form: Hk + Mk−1 = −Λk−1N (Hk−1 + Mk−2) + (−Λk−1N) (Mk−1 − Mk−2) . (44) Let ˜Nk−1 denotes −Λk−1N, so the formula can be simply written as: Hk + Mk−1 = ˜Nk−1 (Hk−1 + Mk−2) + ˜Nk−1 (Mk−1 − Mk−2) . (45) We first use Eq. 40 to recurse once, then derive the following formula: Hk + Mk−1 = ˜Nk−1 ˜Nk−2 (Hk−2 + Mk−3) + ˜Nk−1 (Mk−1 − Mk−2) . (46) By analogy, continuing to split and iterate, we can get the closed form of the output of the k-th layer: Hk = k−1X i=2 k−1Y j=i ˜Nj (Mi − Mi−1) + k−1Y i=1 ˜Ni (H1 + M1) − Mk−1. (47) 19F Comparing with other subgraph-based methods It is worth noting that a lot of previous work mentions \"subgraph\". However, our approach is fundamentally different from these approaches. Relation with other subgraph-based methods. While there are existing works [2, 23] related to subgraphs, they primarily focus on graph classification tasks, aiming to learn representations of entire graphs. Given the limited capacity of GNNs to effectively represent the entire graphs, these subgraph-based approaches employ various strategies to leverage information from multiple subgraph structures within the overall graph to improve the representation. In contrast, our focus is on node-level tasks, specifically, enhancing the representations of individual nodes. In our context, we naturally refer to neighborhood subgraphs as different orders of ego- networks centered on a node (which can be regarded as subgraphs of the entire graph). Our work uncovers the relationship between these subgraphs and over-smoothing, as well as how to utilize them to enhance node representations. This distinction fundamentally sets our approach apart from other subgraph methods. G Experiment Setup G.1 Details of Datasets The dataset statistics are shown in Table 8, and details on dataset splits are summarized as follows: Experiment 5.3 & 5.4. For Cora, Citeseer, Coauthor-CS, Amazon-Photo, we randomly select 20 nodes per class for training set, 500 nodes for validation and 1000 nodes for testing. For Chameleon and Squirrel, we randomly divide each class’s nodes into 60%, 20%, and 20% as the train, validation, and test sets, respectively. Experiment 5.5. We follow the widely used semi-supervised setting in [14]. Experiment 5.6. For larger datasets, We randomly divide each class’s nodes into 20%, 20%, and 60% as the train, validation, and test sets, respectively. Table 8: Dataset statistics of real-world datasets. Cora Citeseer Pubmed Amazon-Photo Ogbn-arxiv Chameleon Squirrel Coauthor-CS Coauthor-Phy Flickr #Nodes 2708 3327 19717 7650 169343 2277 5201 18333 34493 89250#Edges 5429 4732 119081 126842 1166243 36101 217073 81894 247962 899756#Features 1433 3703 500 745 128 2325 2089 6805 8415 500#Classes 7 6 3 8 39 5 5 5 10 7 G.2 Parameter Settings We summarized the hyperparameters used in different experiments in Table 9. Table 9: Hyperparameters for experiments Experiment Backbone Learning Rate Dropout Weight Decay Hidden State Attention Head Max Epoch Early Stopping Epoch Experiment 5.3 & 5.4GCN {0.01, 0.001} 0.5 0.0005 128 - 500 100GAT {0.01, 0.001} 0.5 0.0005 64 3 500 100 Experiment 5.5GCN {0.01, 0.001} 0.5 0.0005 128 - 1000 1000GAT {0.01, 0.001} 0.5 0.0005 32 1 1000 1000 Experiment 5.6 GCN {0.01, 0.001} 0.5 0.0005 128 - 500 100 H. The Analysis of PSNR Residual Coefficients We conducted an empirical study using an 8-layer PSNR-GCN trained on the Cora dataset to obtain the best-performing model. We saved the mean and standard deviation of the learned residual coefficient distribution for each layer. Nodes were evenly divided into four groups based on their degree, with each group containing a similar number of nodes, and the average mean and standard 20deviation for each group across different layers are reported in Table 10. The following observations can be obtained from the table: the mean residual coefficient increases with the number of layers, suggesting that PSNR effectively retains high-order subgraph information. In certain layers, the variance of the residual coefficients rises, indicating that added randomness helps mitigate information loss in higher-order subgraphs. In the shallow layers, the mean values show no significant differences across node degrees; however, in deeper layers, nodes with higher degrees tend to exhibit lower mean values, indicating that these nodes retain more initial information due to significant subgraph overlap. All of these observations align with our expectations, illustrating how residual coefficients adapt based on node degree and layer depth. Table 10: Summary of means and standard deviations for different layers LayerMean Group 1Mean Group 2Mean Group 3Mean Group 4Std Group 1Std Group 2Std Group 3Std Group 4Layer 0 0.0043 0.0049 0.0063 0.0099 0.0002 0.0001 0.0002 0.0001Layer 1 0.0009 0.0017 0.0013 0.0016 0.3102 0.3120 0.3130 0.2410Layer 2 0.0000 0.0000 0.0003 0.0008 0.0480 0.0431 0.0446 0.0170Layer 3 0.5582 0.6017 0.5972 0.4979 0.0084 0.0118 0.0105 0.0080Layer 4 1.4898 1.5238 1.5212 1.3478 8.3354e-055.4541e-05 0.0001 0.0002Layer 5 2.4080 2.3974 2.3981 2.1414 0.0062 0.0081 0.0078 0.0059Layer 6 5.2347 5.3148 5.2070 3.8177 0.0046 0.0089 0.0075 0.0069Layer 7 11.0787 11.1253 10.9247 8.7238 0.1642 0.2089 0.2128 0.0677 I. Comparison of Experimental Results of Different GraphEncoders. In practice, in addition to SAGE, encoders such as GAT and GCN can also be utilized. To provide further insight, we have included results for different encoders on the classical semi-supervised node classification task. The results are summarized in the table blow. Table 11: Different GraphEncoder performance for SSNC task (layer 2) Graph Encoder Cora Citeseer CS Photo Chameleon Squirrel GCN 80.98±1.60 68.46±2.28 90.52 ±0.82 91.56±0.74 72.02 ±1.60 56.14±1.51 GAT 80.89 ±1.63 68.77±1.89 90.61±0.89 91.18 ±0.92 71.97 ±1.28 56.24±1.11 SAGE 80.59 ±1.57 68.06 ±2.12 91.23±1.00 91.44±0.82 71.51 ±1.90 54.95 ±1.73 Table 12: Different GraphEncoder performance for SSNC task (layer 4) Graph Encoder Cora Citeseer CS Photo Chameleon Squirrel GCN 81.65 ±1.70 68.11±1.24 90.66±0.70 91.14 ±0.90 71.58±2.07 56.34±1.48 GAT 82.21±1.41 67.96±1.20 90.57 ±0.89 91.17 ±0.81 71.29 ±1.75 56.50±1.45 SAGE 81.01 ±1.63 66.03 ±1.93 90.70±1.49 91.20 ±1.03 70.74±2.24 54.13 ±1.41 The results indicate that each encoder has its strengths and performs differently across various datasets, demonstrating superior performance compared to the baseline and emphasizing the potential of PSNR. 21NeurIPS Paper Checklist The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: • You should answer [Yes] , [No] , or [NA] . • [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. • Please provide a short (1–2 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: • Delete this instruction block, but keep the section heading “NeurIPS paper checklist\", • Keep the checklist subsection headings, questions/answers and guidelines below. • Do not modify the questions and only use the provided macros for your answers. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: Our paper’s contributions and scope are presented in the abstract and introduc- tion accurately. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] 22Justification: Limitations of our work are discussed in Section 6. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide the proofs of all theorems and the derivations of all formulas in Section 4.2, Appendix D and Appendix E, respectively. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the details of experiments in Appendix G, which can ensure the reproducibility of our experimental result. Guidelines: 23• The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We only used public datasets and the anonymous link to the code is provided in the Abstract. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. 24• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the details of training and test in Appendix G. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The standard deviation of the experimental results is reported in most of the tables. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report the details about the compute resources in Section 5.2 Guidelines: • The answer NA means that the paper does not include experiments. 25• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed the potential positive societal impacts in section 6. It has no negative societal impact. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? 26Answer: [NA] Justification: [TODO] Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the original owners of assets (e.g., code, data, models) used in this paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, we provide the anonymous link to the code. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 2714. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve any crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 28",
      "meta_data": {
        "arxiv_id": "2305.05368v3",
        "authors": [
          "Jingbo Zhou",
          "Yixuan Du",
          "Ruqiong Zhang",
          "Jun Xia",
          "Zhizhi Yu",
          "Zelin Zang",
          "Di Jin",
          "Carl Yang",
          "Rui Zhang",
          "Stan Z. Li"
        ],
        "published_date": "2023-05-09T12:03:42Z",
        "pdf_url": "https://arxiv.org/pdf/2305.05368v3.pdf"
      }
    },
    {
      "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
      "abstract": "\\emph{Over-fitting} and \\emph{over-smoothing} are two main obstacles of\ndeveloping deep Graph Convolutional Networks (GCNs) for node classification. In\nparticular, over-fitting weakens the generalization ability on small dataset,\nwhile over-smoothing impedes model training by isolating output representations\nfrom the input features with the increase in network depth. This paper proposes\nDropEdge, a novel and flexible technique to alleviate both issues. At its core,\nDropEdge randomly removes a certain number of edges from the input graph at\neach training epoch, acting like a data augmenter and also a message passing\nreducer. Furthermore, we theoretically demonstrate that DropEdge either reduces\nthe convergence speed of over-smoothing or relieves the information loss caused\nby it. More importantly, our DropEdge is a general skill that can be equipped\nwith many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for\nenhanced performance. Extensive experiments on several benchmarks verify that\nDropEdge consistently improves the performance on a variety of both shallow and\ndeep GCNs. The effect of DropEdge on preventing over-smoothing is empirically\nvisualized and validated as well. Codes are released\non~\\url{https://github.com/DropEdge/DropEdge}.",
      "full_text": "Published as a conference paper at ICLR 2020 DROP EDGE : T OWARDS DEEP GRAPH CONVOLU - TIONAL NETWORKS ON NODE CLASSIFICATION Yu Rong1, Wenbing Huang2∗, Tingyang Xu 1, Junzhou Huang1 1 Tencent AI Lab 2 Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University yu.rong@hotmail.com, hwenbing@126.com tingyangxu@tencent.com, jzhuang@uta.edu ABSTRACT Over-ﬁtting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classiﬁcation. In particular, over-ﬁtting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and ﬂexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models ( e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on https://github.com/DropEdge/DropEdge. 1 I NTRODUCTION Graph Convolutional Networks (GCNs), which exploit message passing or equivalently certain neigh- borhood aggregation function to extract high-level features from a node as well as its neighborhoods, have boosted the state-of-the-arts for a variety of tasks on graphs, such as node classiﬁcation (Bhagat et al., 2011; Zhang et al., 2018), social recommendation (Freeman, 2000; Perozzi et al., 2014), and link prediction (Liben-Nowell & Kleinberg, 2007) to name some. In other words, GCNs have been becoming one of the most crucial tools for graph representation learning. Yet, when we revisit typical GCNs on node classiﬁcation (Kipf & Welling, 2017), they are usually shallow (e.g. the number of the layers is 21). Inspired from the success of deep CNNs on image classiﬁcation, several attempts have been proposed to explore how to build deep GCNs towards node classiﬁcation (Kipf & Welling, 2017; Li et al., 2018a; Xu et al., 2018a; Li et al., 2019); nevertheless, none of them delivers sufﬁciently expressive architecture. The motivation of this paper is to analyze the very factors that impede deeper GCNs to perform promisingly, and develop method to address them. We begin by investigating two factors: over-ﬁtting and over-smoothing. Over-ﬁtting comes from the case when we utilize an over-parametric model to ﬁt a distribution with limited training data, where the model we learn ﬁts the training data very well but generalizes poorly to the testing data. It does exist if we apply a deep GCN on small graphs (see 4-layer GCN on Cora in Figure 1). Over-smoothing, towards the other extreme, makes training a very deep GCN difﬁcult. As ﬁrst introduced by Li et al. (2018a) and further explained in Wu et al. (2019); Xu et al. (2018a); Klicpera et al. (2019), graph convolutions essentially push representations of adjacent nodes mixed with each ∗Wenbing Huang is the corresponding author. 1When counting the number of layers (or network depth) of GCN, this paper does not involve the input layer. 1 arXiv:1907.10903v4  [cs.LG]  12 Mar 2020Published as a conference paper at ICLR 2020 0 50 100 150 200 250 300 350 400 Epochs 0.25 0.75 1.25 1.75 Training Loss GCN-8 GCN-8+DropEdge GCN-4 GCN-4+DropEdge 0 50 100 150 200 250 300 350 400 Epochs 0.25 0.75 1.25 1.75 Validation Loss GCN-8 GCN-8+DropEdge GCN-4 GCN-4+DropEdge Figure 1: Performance of Multi-layer GCNs on Cora. We implement 4-layer GCN w and w/o DropEdge (in orange), 8-layer GCN w and w/o DropEdge (in blue) 2. GCN-4 gets stuck in the over-ﬁtting issue attaining low training error but high validation error; the training of GCN-8 fails to converge satisfactorily due to over-smoothing. By applying DropEdge, both GCN-4 and GCN-8 work well for both training and validation. other, such that, if extremely we go with an inﬁnite number of layers, all nodes’ representations will converge to a stationary point, making them unrelated to the input features and leading to vanishing gradients. We call this phenomenon as over-smoothing of node features. To illustrate its inﬂuence, we have conducted an example experiment with 8-layer GCN in Figure 1, in which the training of such a deep GCN is observed to converge poorly. Both of the above two issues can be alleviated, using the proposed method, DropEdge. The term “DropEdge” refers to randomly dropping out certain rate of edges of the input graph for each training time. There are several beneﬁts in applying DropEdge for the GCN training (see the experimental improvements by DropEdge in Figure 1). First, DropEdge can be considered as a data augmentation technique. By DropEdge, we are actually generating different random deformed copies of the original graph; as such, we augment the randomness and the diversity of the input data, thus better capable of preventing over-ﬁtting. Second, DropEdge can also be treated as a message passing reducer. In GCNs, the message passing between adjacent nodes is conducted along edge paths. Removing certain edges is making node connections more sparse, and hence avoiding over-smoothing to some extent when GCN goes very deep. Indeed, as we will draw theoretically in this paper, DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. We are also aware that the dense connections employed by JKNet (Xu et al., 2018a) are another kind of tools that can potentially prevent over-smoothing. In its formulation, JKNet densely connects each hidden layer to the top one, hence the feature mappings in lower layers that are hardly affected by over-smoothing are still maintained. Interestingly and promisingly, we ﬁnd that the performance of JKNet can be promoted further if it is utilized along with our DropEdge. Actually, our DropEdge—as a ﬂexible and general technique—is able to enhance the performance of various popular backbone networks on several benchmarks, including GCN (Kipf & Welling, 2017), ResGCN (Li et al., 2019), JKNet (Xu et al., 2018a), and GraphSAGE (Hamilton et al., 2017). We provide detailed evaluations in the experiments. 2 R ELATED WORK GCNs Inspired by the huge success of CNNs in computer vision, a large number of methods come redeﬁning the notion of convolution on graphs under the umbrella of GCNs. The ﬁrst prominent research on GCNs is presented in Bruna et al. (2013), which develops graph convolution based on spectral graph theory. Later, Kipf & Welling (2017); Defferrard et al. (2016); Henaff et al. (2015); Li et al. (2018b); Levie et al. (2017) apply improvements, extensions, and approximations on spectral- based GCNs. To address the scalability issue of spectral-based GCNs on large graphs, spatial-based GCNs have been rapidly developed (Hamilton et al., 2017; Monti et al., 2017; Niepert et al., 2016; 2To check the efﬁcacy of DropEdge more clearly, here we have removed bias in all GCN layers, while for the experiments in § 5, the bias are kept. 2Published as a conference paper at ICLR 2020 Gao et al., 2018). These methods directly perform convolution in the graph domain by aggregating the information from neighbor nodes. Recently, several sampling-based methods have been proposed for fast graph representation learning, including the node-wise sampling methods (Hamilton et al., 2017), the layer-wise approach (Chen et al., 2018) and its layer-dependent variant (Huang et al., 2018). Speciﬁcally, GAT (Velickovic et al., 2018) has discussed applying dropout on edge attentions. While it actually is a post-conducted version of DropEdge before attention computation, the relation to over-smoothing is never explored in Velickovic et al. (2018). In our paper, however, we have formally presented the formulation of DropEdge and provided rigorous theoretical justiﬁcation of its beneﬁt in alleviating over-smoothing. We also carried out extensive experiments by imposing DropEdge on several popular backbones. One additional point is that we further conduct adjacency normalization after dropping edges, which, even simple, is able to make it much easier to converge during training and reduce gradient vanish as the number of layers grows. Deep GCNs Despite the fruitful progress, most previous works only focus on shallow GCNs while the deeper extension is seldom discussed. The attempt for building deep GCNs is dated back to the GCN paper (Kipf & Welling, 2017), where the residual mechanism is applied; unexpectedly, as shown in their experiments, residual GCNs still perform worse when the depth is 3 and beyond. The authors in Li et al. (2018a) ﬁrst point out the main difﬁculty in constructing deep networks lying in over-smoothing, but unfortunately, they never propose any method to address it. The follow-up study (Klicpera et al., 2019) solves over-smoothing by using personalized PageRank that additionally involves the rooted node into the message passing loop; however, the accuracy is still observed to decrease when the depth increases from 2. JKNet (Xu et al., 2018a) employs dense connections for multi-hop message passing which is compatible with DropEdge for formulating deep GCNs. Oono & Suzuki (2019) theoretically prove that the node features of deep GCNs will converge to a subspace and incur information loss. It generalizes the conclusion in Li et al. (2018a) by further considering the ReLu function and convolution ﬁlters. Our interpretations on why DropEdge can impede over-smoothing is based on the concepts proposed by Oono & Suzuki (2019). A recent method (Li et al., 2019) has incorporated residual layers, dense connections and dilated convolutions into GCNs to facilitate the development of deep architectures. Nevertheless, this model is targeted on graph-level classiﬁcation (i.e. point cloud segmentation), where the data points are graphs and naturally disconnected from each other. In our task for node classiﬁcation, the samples are nodes and they all couple with each other, thus the over-smoothing issue is more demanded to be addressed. By leveraging DropEdge, we are able to relieve over-smoothing, and derive more enhanced deep GCNs on node classiﬁcation. 3 N OTATIONS AND PRELIMINARIES Notations. Let G= (V,E) represent the input graph of size N with nodes vi ∈V and edges (vi,vj) ∈E. The node features are denoted as X = {x1,··· ,xN}∈ RN×C, and the adjacency matrix is deﬁned as A ∈RN×N which associates each edge (vi,vj) with its element Aij. The node degrees are given by d = {d1,··· ,dN}where di computes the sum of edge weights connected to node i. We deﬁne D as the degree matrix whose diagonal elements are obtained from d. GCN is originally developed by Kipf & Welling (2017). The feed forward propagation in GCN is recursively conducted as H(l+1) = σ ( ˆAH(l)W(l) ) , (1) where H(l+1) = {h(l+1) 1 ,··· ,h(l+1) N }are the hidden vectors of thel-th layer with h(l) i as the hidden feature for node i; ˆA = ˆD−1/2(A + I) ˆD−1/2 is the re-normalization of the adjacency matrix, and ˆD is the corresponding degree matrix of A + I; σ(·) is a nonlinear function, i.e. the ReLu function; and W(l) ∈RCl×Cl−1 is the ﬁlter matrix in the l-th layer with Cl refers to the size of l-th hidden layer. We denote one-layer GCN computed by Equation 1 as Graph Convolutional Layer (GCL) in what follows. 3Published as a conference paper at ICLR 2020 4 O UR METHOD : D ROP EDGE This section ﬁrst introduces the methodology of the DropEdge technique as well as its layer-wise variant where the adjacency matrix for each GCN layer is perturbed individually. We also explain how the proposed DropEdge can prevent over-ﬁtting and over-smoothing in generic GCNs. Particularly for over-smoothing, we provide its mathematical deﬁnition and theoretical derivations on showing the beneﬁts of DropEdge. 4.1 M ETHODOLOGY At each training epoch, the DropEdge technique drops out a certain rate of edges of the input graph by random. Formally, it randomly enforces Vp non-zero elements of the adjacency matrix A to be zeros, where V is the total number of edges and pis the dropping rate. If we denote the resulting adjacency matrix as Adrop, then its relation with A becomes Adrop = A −A′, (2) where A′is a sparse matrix expanded by a random subset of sizeVp from original edges E. Following the idea of Kipf & Welling (2017), we also perform the re-normalization trick onAdrop, leading to ˆAdrop. We replace ˆA with ˆAdrop in Equation 1 for propagation and training. When validation and testing, DropEdge is not utilized. Preventing over-ﬁtting. DropEdge produces varying perturbations of the graph connections. As a result, it generates different random deformations of the input data and can be regarded as a data augmentation skill for graphs. To explain why this is valid, we provide an intuitive understanding here. The key in GCNs is to aggregate neighbors’ information for each node, which can be understood as a weighted sum of the neighbor features (the weights are associated with the edges). From the perspective of neighbor aggregation, DropEdge enables a random subset aggregation instead of the full aggregation during GNN training. Statistically, DropEdge only changes the expectation of the neighbor aggregation up to a multiplier p, if we drop edges with probability p. This multiplier will be actually removed after weights normalization, which is often the case in practice. Therefore, DropE- dge does not change the expectation of neighbor aggregation and is an unbiased data augmentation technique for GNN training, similar to typical image augmentation skills (e.g. rotation, cropping and ﬂapping) that are capable of hindering over-ﬁtting in training CNNs. We will provide experimental validations in § 5.1. Layer-Wise DropEdge. The above formulation of DropEdge is one-shot with all layers sharing the same perturbed adjacency matrix. Indeed, we can perform DropEdge for each individual layer. Speciﬁcally, we obtain ˆA(l) drop by independently computing Equation 2 for each l-th layer. Different layer could have different adjacency matrix ˆA(l) drop. Such layer-wise version brings in more randomness and deformations of the original data, and we will experimentally compare its performance with the original DropEdge in § 5.2. Over-smoothing is another obstacle of training deep GCNs, and we will detail how DropEdge can address it to some extent in the next section. For simplicity, the following derivations assume all GCLs share the same perturbed adjacency matrix, and we will leave the discussion on layer-wise DropEdge for future exploration. 4.2 T OWARDS PREVENTING OVER -SMOOTHING By its original deﬁnition in Li et al. (2018a), the over-smoothing phenomenon implies that the node features will converge to a ﬁxed point as the network depth increases. This unwanted convergence restricts the output of deep GCNs to be only relevant to the graph topology but independent to the input node features, which as a matter of course incurs detriment of the expressive power of GCNs. Oono & Suzuki (2019) has generalized the idea in Li et al. (2018a) by taking both the non-linearity (i.e. the ReLu function) and the convolution ﬁlters into account; they explain over-smoothing as convergence to a subspace rather than convergence to a ﬁxed point. This paper will use the concept of subspace by Oono & Suzuki (2019) for more generality. We ﬁrst provide several relevant deﬁnitions that facilitate our later presentations. 4Published as a conference paper at ICLR 2020 Deﬁnition 1 (subspace). Let M:= {EC|C ∈RM×C}be an M-dimensional subspace in RN×C, where E ∈RN×M is orthogonal, i.e. ETE = IM, and M ≤N. Deﬁnition 2 (ϵ-smoothing). We call the ϵ-smoothing of node features happens for a GCN, if all its hidden vectors H(l) beyond a certain layer Lhave a distance no larger than ϵ(ϵ> 0) with respect to a subspace Mthat is independent to the input features, namely, dM(H(l)) <ϵ, ∀l≥L, (3) where dM(·) computes the distance between the input matrix and the subspace M.3 Deﬁnition 3 (the ϵ-smoothing layer). Given the subspace Mand ϵ, we call the minimal value of the layers that satisfy Equation 3 as the ϵ-smoothing layer, that is, l∗(M,ϵ) := minl{dM(H(l)) <ϵ}. Since conducting analysis exactly based on the ϵ-smoothing layer is difﬁcult, we instead deﬁne the relaxed ϵ-smoothing layer which is proved to be an upper bound of l∗. Deﬁnition 4 (the relaxed ϵ-smoothing layer). Given the subspace Mand ϵ, we call ˆl(M,ϵ) = ⌈log(ϵ/dM(X)) log sλ ⌉as the relaxed smoothing layer, where, ⌈·⌉computes the ceil of the input, sis the supremum of the ﬁlters’ singular values over all layers, andλis the second largest eigenvalue of ˆA. Besides, we have ˆl≥l∗4. According to the conclusions by the authors in Oono & Suzuki (2019), a sufﬁciently deep GCN will certainly suffer from the ϵ-smoothing issue for any small value of ϵunder some mild conditions (the details are included in the supplementary material). Note that they only prove the existence of ϵ-smoothing in deep GCN without developing any method to address it. Here, we will demonstrate that adopting DropEdge alleviates the ϵ-smoothing issue in two aspects: 1. By reducing node connections, DropEdge is proved to slow down the convergence of over-smoothing; in other words, the value of the relaxed ϵ-smoothing layer will only increase if using DropEdge. 2. The gap between the dimensions of the original space and the converging subspace, i.e. N −M measures the amount of information loss; larger gap means more severe information loss. As shown by our derivations, DropEdge is able to increase the dimension of the converging subspace, thus capable of reducing information loss. We summarize our conclusions as follows. Theorem 1. We denote the original graph as Gand the one after dropping certain edges out as G′. Given a small value of ϵ, we assume Gand G′will encounter the ϵ-smoothing issue with regard to subspaces Mand M′, respectively. Then, either of the following inequalities holds after sufﬁcient edges removed. • The relaxed smoothing layer only increases: ˆl(M,ϵ) ≤ˆl(M′,ϵ); •The information loss is decreased: N −dim(M) >N −dim(M′). The proof of Theorem 1 is based on the derivations in Oono & Suzuki (2019) as well as the concept of mixing time that has been studied in the random walk theory (Lovász et al., 1993). We provide the full details in the supplementary material. Theorem 1 tells that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. In this way, DropEdge enables us to train deep GCNs more effectively. 4.3 DISCUSSIONS This sections contrasts the difference between DropEdge and other related concepts including Dropout, DropNode, and Graph Sparsiﬁcation. DropEdge vs. Dropout The Dropout trick (Hinton et al., 2012) is trying to perturb the feature matrix by randomly setting feature dimensions to be zeros, which may reduce the effect of over-ﬁtting but is of no help to preventing over-smoothing since it does not make any change of the adjacency 3The deﬁnition of dM(·) is provided in the supplementary material. 4 All detailed deﬁnitions and proofs are provided in the appendix. 5Published as a conference paper at ICLR 2020 matrix. As a reference, DropEdge can be regarded as a generation of Dropout from dropping feature dimensions to dropping edges, which mitigates both over-ﬁtting and over-smoothing. In fact, the impacts of Dropout and DropEdge are complementary to each other, and their compatibility will be shown in the experiments. DropEdge vs. DropNode Another related vein belongs to the kind of node sampling based methods, including GraphSAGE (Hamilton et al., 2017), FastGCN (Chen et al., 2018), and AS- GCN (Huang et al., 2018). We name this category of approaches as DropNode. For its original motivation, DropNode samples sub-graphs for mini-batch training, and it can also be treated as a speciﬁc form of dropping edges since the edges connected to the dropping nodes are also removed. However, the effect of DropNode on dropping edges is node-oriented and indirect. By contrast, DropEdge is edge-oriented, and it is possible to preserve all node features for the training (if they can be ﬁtted into the memory at once), exhibiting more ﬂexibility. Further, to maintain desired performance, the sampling strategies in current DropNode methods are usually inefﬁcient, for example, GraphSAGE suffering from the exponentially-growing layer size, and AS-GCN requiring the sampling to be conducted recursively layer by layer. Our DropEdge, however, neither increases the layer size as the depth grows nor demands the recursive progress because the sampling of all edges are parallel. DropEdge vs. Graph-Sparsiﬁcation Graph-Sparsiﬁcation (Eppstein et al., 1997) is an old re- search topic in the graph domain. Its optimization goal is removing unnecessary edges for graph compressing while keeping almost all information of the input graph. This is clearly district to the purpose of DropEdge where no optimization objective is needed. Speciﬁcally, DropEdge will remove the edges of the input graph by random at each training time, whereas Graph-Sparsiﬁcation resorts to a tedious optimization method to determine which edges to be deleted, and once those edges are discarded the output graph keeps unchanged. 5 E XPERIMENTS Datasets Joining the previous works’ practice, we focus on four benchmark datasets varying in graph size and feature type: (1) classifying the research topic of papers in three citation datasets: Cora, Citeseer and Pubmed (Sen et al., 2008); (2) predicting which community different posts belong to in the Reddit social network (Hamilton et al., 2017). Note that the tasks in Cora, Citeseer and Pubmed are transductive underlying all node features are accessible during training, while the task in Reddit is inductive meaning the testing nodes are unseen for training. We apply the full-supervised training fashion used in Huang et al. (2018) and Chen et al. (2018) on all datasets in our experiments. The statics of all datasets are listed in the supplemental materials. 5.1 C AN DROP EDGE GENERALLY IMPROVE THE PERFORMANCE OF DEEP GCN S? In this section, we are interested in if applying DropEdge can promote the performance of current popular GCNs (especially their deep architectures) on node classiﬁcation. Implementations We consider ﬁve backbones: GCN (Kipf & Welling, 2017), ResGCN (He et al., 2016; Li et al., 2019), JKNet (Xu et al., 2018a), IncepGCN5 and GraphSAGE (Hamilton et al., 2017) with varying depth from 2 to 64.6 Since different structure exhibits different training dynamics on different dataset, to enable more robust comparisons, we perform random hyper-parameter search for each model, and report the case giving the best accuracy on validation set of each benchmark. The searching space of hyper-parameters and more details are provided in Table 4 in the supplementary material. Regarding the same architecture w or w/o DropEdge, we apply the same set of hyper- parameters except the drop rate pfor fair evaluation. Overall Results Table 1 summaries the results on all datasets. We only report the performance of the model with 2/8/32 layers here due to the space limit, and provide the accuracy under other different depths in the supplementary material. It’s observed that DropEdge consistently improves the 5The formulation is given in the appendix. 6For Reddit, the maximum depth is 32 considering the memory bottleneck. 6Published as a conference paper at ICLR 2020 Table 1: Testing accuracy (%) comparisons on different backbones w and w/o DropEdge. Dataset Backbone 2 layers 8 layers 32 layers Orignal DropEdge Orignal DropEdge Orignal DropEdge Cora GCN 86.10 86.50 78.70 85.80 71.60 74.60 ResGCN - - 85.40 86.90 85.10 86.80 JKNet - - 86.70 87.80 87.10 87.60 IncepGCN - - 86.70 88.20 87.40 87.70 GraphSAGE 87.80 88.10 84.30 87.10 31.90 32.20 Citeseer GCN 75.90 78.70 74.60 77.20 59.20 61.40 ResGCN - - 77.80 78.80 74.40 77.90 JKNet - - 79.20 80.20 71.70 80.00 IncepGCN - - 79.60 80.50 72.60 80.30 GraphSAGE 78.40 80.00 74.10 77.10 37.00 53.60 Pubmed GCN 90.20 91.20 90.10 90.90 84.60 86.20 ResGCN - - 89.60 90.50 90.20 91.10 JKNet - - 90.60 91.20 89.20 91.30 IncepGCN - - 90.20 91.50 OOM 90.50 GraphSAGE 90.10 90.70 90.20 91.70 41.30 47.90 Reddit GCN 96.11 96.13 96.17 96.48 45.55 50.51 ResGCN - - 96.37 96.46 93.93 94.27 JKNet - - 96.82 97.02 OOM OOM IncepGCN - - 96.43 96.87 OOM OOM GraphSAGE 96.22 96.28 96.38 96.42 96.43 96.47 testing accuracy for all cases. The improvement is more clearly depicted in Figure 2a, where we have computed the average absolute improvement over all backbones by DropEdge on each dataset under different numbers of layers. On Citeseer, for example, DropEdge yields further improvement for deeper architecture; it gains 0.9% average improvement for the model with 2 layers while achieving a remarkable 13.5% increase for the model with 64 layers. In addition, the validation losses of all 4-layer models on Cora are shown in Figure 2b. The curves along the training epoch are dramatically pulled down after applying DropEdge, which also explains the effect of DropEdge on alleviating over-ﬁtting. Another valuable observation in Table 1 is that the 32-layer IncepGCN without DropEdge incurs the Out-Of-Memory (OOM) issue while the model with DropEdge survives, showing the advantage of DropEdge to save memory consuming by making the adjacency matrix sparse. Comparison with SOTAs We select the best performance for each backbone with DropEdge, and contrast them with existing State of the Arts (SOTA), including GCN, FastGCN, AS-GCN and GraphSAGE in Table 2; for the SOTA methods, we reuse the results reported in Huang et al. (2018). We have these ﬁndings: (1) Clearly, our DropEdge obtains signiﬁcant enhancement against SOTAs; particularly on Reddit, the best accuracy by our method is 97.02%, and it is better than the previous best by AS-GCN (96.27%), which is regarded as a remarkable boost considering the challenge on this benchmark. (2) For most models with DropEdge, the best accuracy is obtained under the depth beyond 2, which again veriﬁes the impact of DropEdge on formulating deep networks. (3) As mentioned in § 4.3, FastGCN, AS-GCN and GraphSAGE are considered as the DropNode extensions of GCNs. The DropEdge based approaches outperform the DropNode based variants as shown in Table 2, which somehow conﬁrms the effectiveness of DropEdge. Actually, employing DropEdge upon the DropNode methods further delivers promising enhancement, which can be checked by revisiting the increase by DropEdge for GraphSAGE in Table 1. 5.2 H OW DOES DROP EDGE HELP ? This section continues a more in-depth analysis on DropEdge and attempts to ﬁgure out why it works. Due to the space limit, we only provide the results on Cora, and defer the evaluations on other datasets to the supplementary material. Note that this section mainly focuses on analyzing DropEdge and its variants, without the concern with pushing state-of-the-art results. So, we do not perform delicate hyper-parameter selection. We employ GCN as the backbone in this section. Here, GCN-ndenotes GCN of depth n. The hidden dimension, learning rate and weight decay are ﬁxed to 256, 0.005 and 0.0005, receptively. The 7Published as a conference paper at ICLR 2020 0% 2% 4% 6% 8% 10% 12% 14% 16% Cora Citeseer Pubmed Reddit 2 layers 4 layers 8 layers 16 layers 32 layers 64 layers (a) The average absolute improvement by DropEdge. 0 25 50 75 100 125 150 175 200 Epoch 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Validation Loss Cora ResGCN-4 GCN-4 InceptGCN-4 JKNet-4 ResGCN-4+DropEdge GCN-4+DropEdge InceptGCN-4+DropEdge JKNet-4+DropEdge (b) The validation loss on different backbones w and w/o DropEdge. Figure 2 Table 2: Accuracy (%) comparisons with SOTAs. The number in parenthesis denotes the network depth for the models with DropEdge. Transductive Inductive Cora Citeseer Pubmed Reddit GCN 86.64 79.34 90.22 95.68 FastGCN 85.00 77.60 88.00 93.70 ASGCN 87.44 79.66 90.60 96.27 GraphSAGE 82.20 71.40 87.10 94.32 GCN+DropEdge 87.60(4) 79.20(4) 91.30(4) 96.71(4) ResGCN+DropEdge 87.00(4) 79.40(16) 91.10(32) 96.48(16) JKNet+DropEdge 88.00(16) 80.20(8) 91.60(64) 97.02(8) IncepGCN+DropEdge 88.20(8) 80.50(8) 91.60(4) 96.87(8) GraphSAGE+DropEdge 88.10(4) 80.00(2) 91.70(8) 96.54(4) random seed is ﬁxed. We train all models with 200 epochs. Unless otherwise mentioned, we do not utilize the “withloop” and “withbn” operation (see their deﬁnitions in Table 4 in the appendix). 5.2.1 O N PREVENTING OVER -SMOOTHING As discussed in § 4.2, the over-smoothing issue exists when the top-layer outputs of GCN converge to a subspace and become unrelated to the input features with the increase in depth. Since we are unable to derive the converging subspace explicitly, we measure the degree of over-smoothing by instead computing the difference between the output of the current layer and that of the previous one. We adopt the Euclidean distance for the difference computation. Lower distance means more serious over-smoothing. Experiments are conducted on GCN-8. Figure 3 (a) shows the distances of different intermediate layers (from 2 to 6) under different edge dropping rates (0 and 0.8). Clearly, over-smoothing becomes more serious in GCN as the layer grows, which is consistent with our conjecture. Conversely, the model with DropEdge ( p = 0.8) reveals higher distance and slower convergent speed than that without DropEdge (p= 0), implying the importance of DropEdge to alleviating over-smoothing. We are also interested in how the over- smoothing will act after training. For this purpose, we display the results after 150-epoch training in Figure 3 (b). For GCN without DropEdge, the difference between outputs of the 5-th and 6-th layers is equal to 0, indicating that the hidden features have converged to a certain stationary point. On the contrary, GCN with DropEdge performs promisingly, as the distance does not vanish to zero when 2 3 4 5 6 Layer 10 1 10 0 Distance (a) Before Training GCN(p=0) GCN(p=0.8) 2 3 4 5 6 Layer 10 17 10 14 10 11 10 8 10 5 10 2 10 1 Distance (b) After Training GCN(p=0) GCN(p=0.8) 0 20 40 60 80 100 120 140 Epoch 1.2 1.4 1.6 1.8Loss (c)Training Loss GCN(p=0) GCN(p=0.8) Figure 3: Analysis on over-smoothing. Smaller distance means more serious over-smoothing. 8Published as a conference paper at ICLR 2020 0 25 50 75 100 125 150 175 200 Epoch 0.4 0.6 0.8 1.0 1.2 1.4 1.6Validation Loss Cora GCN-4 (No DropEdge, No Dropout) GCN-4 (No DropEdge, Dropout) GCN-4 (DropEdge, No Dropout) GCN-4 (DropEdge, Dropout) (a) Dropout vs DropEdge on Cora. 0 25 50 75 100 125 150 175 200 Epoch 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6Train & Validation Loss Cora GCN-4+DropEdge:Validation GCN-4+DropEdge:Train GCN-4+DropEdge (LI):Validation GCN-4+DropEdge (LI):Train (b) Comparison between DropEdge and layer-wise (LW) DropEdge. Figure 4 the number of layers grows; it probably has successfully learned meaningful node representations after training, which could also be validated by the training loss in Figure 3 (c). 5.2.2 O N COMPATIBILITY WITH DROPOUT § 4.3 has discussed the difference between DropEdge and Dropout. Hence, we conduct an ablation study on GCN-4, and the validation losses are demonstrated in Figure 4a. It reads that while both Dropout and DropEdge are able to facilitate the training of GCN, the improvement by DropEdge is more signiﬁcant, and if we adopt them concurrently, the loss is decreased further, indicating the compatibility of DropEdge with Dropout. 5.2.3 O N LAYER -WISE DROP EDGE § 4.1 has descried the Layer-Wise (LW) extension of DropEdge. Here, we provide the experimental evaluation on assessing its effect. As observed from Figure 4b, the LW DropEdge achieves lower training loss than the original version, whereas the validation value between two models is comparable. It implies that LW DropEdge can facilitate the training further than original DropEdge. However, we prefer to use DropEdge other than the LW variant so as to not only avoid the risk of over-ﬁtting but also reduces computational complexity since LW DropEdge demands to sample each layer and spends more time. 6 C ONCLUSION We have presented DropEdge, a novel and efﬁcient technique to facilitate the development of deep Graph Convolutional Networks (GCNs). By dropping out a certain rate of edges by random, DropEdge includes more diversity into the input data to prevent over-ﬁtting, and reduces message passing in graph convolution to alleviate over-smoothing. Considerable experiments on Cora, Citeseer, Pubmed and Reddit have veriﬁed that DropEdge can generally and consistently promote the performance of current popular GCNs, such as GCN, ResGCN, JKNet, IncepGCN, and GraphSAGE. It is expected that our research will open up a new venue on a more in-depth exploration of deep GCNs for broader potential applications. 7 A CKNOWLEDGEMENTS This research was funded by National Science and Technology Major Project of the Ministry of Science and Technology of China (No. 2018AAA0102900). Finally, Yu Rong wants to thank, in particular, the invaluable love and support from Yunman Huang over the years. Will you marry me? REFERENCES Smriti Bhagat, Graham Cormode, and S Muthukrishnan. Node classiﬁcation in social networks. In Social network data analytics, pp. 115–148. Springer, 2011. 9Published as a conference paper at ICLR 2020 Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In Proceedings of International Conference on Learning Represen- tations, 2013. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In Proceedings of the 6th International Conference on Learning Representa- tions, 2018. Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016. David Eppstein, Zvi Galil, Giuseppe F Italiano, and Amnon Nissenzweig. Sparsiﬁcation—a technique for speeding up dynamic graph algorithms. Journal of the ACM (JACM), 44(5):669–696, 1997. Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph convolutional networks. In Advances in Neural Information Processing Systems, pp. 6530–6539, 2017. Linton C Freeman. Visualizing social networks. Journal of social structure, 1(1):4, 2000. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1416–1424. ACM, 2018. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1025–1035, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015. Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558–4567, 2018. Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In Proceedings of the International Conference on Learning Representations, 2017. Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In Proceedings of the 7th International Conference on Learning Representations, 2019. Ron Levie, Federico Monti, Xavier Bresson, and Michael M Bronstein. Cayleynets: Graph con- volutional neural networks with complex rational spectral ﬁlters. IEEE Transactions on Signal Processing, 67(1):97–109, 2017. Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018a. 10Published as a conference paper at ICLR 2020 Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Adaptive graph convolutional neural networks. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018b. David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. László Lovász et al. Random walks on graphs: A survey. Combinatorics, Paul erdos is eighty, 2(1): 1–46, 1993. Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115–5124, 2017. Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pp. 2014–2023, 2016. Kenta Oono and Taiji Suzuki. On asymptotic behaviors of graph cnns from dynamical systems perspective. arXiv preprint arXiv:1905.10947, 2019. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa- tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701–710. ACM, 2014. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classiﬁcation in network data. AI magazine, 29(3):93, 2008. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In ICLR, 2018. Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J Smola, and Zheng Zhang. Deep graph library: Towards efﬁcient and scalable deep learning on graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds , 2019. URL https://arxiv.org/abs/1909.01315. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th International Conference on Machine Learning, 2018a. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018b. Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classiﬁcation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. 11Published as a conference paper at ICLR 2020 A A PPENDIX : P ROOF OF THEOREM 1 To prove Theorem 1, we need to borrow the following deﬁnitions and corollaries from Oono & Suzuki (2019). First, we denote the maximum singular value of Wl by sl and set s:= supl∈N+ sl. We assume that Wl of all layers are initialized so that s≤1. Second, we denote the distance that induced as the Frobenius norm from X to Mby dM(X) := infY ∈M||X −Y ||F. Then, we recall Corollary 3 and Proposition 1 in Oono & Suzuki (2019) as Corollary 1 below. Corollary 1. Let λ1 ≤···≤ λN be the eigenvalues of ˆA, sorted in ascending order. Suppose the multiplicity of the largest eigenvalue λN is M(≤N), i.e., λN−M <λN−M+1 = ··· = λN and the second largest eigenvalue is deﬁned as λ:= N−M max n=1 |λn|<|λN|. (4) Let E to be the eigenspace associated with λN−M+1,··· ,λN. Then we have λ<λ N = 1, and dM(H(l)) ≤slλdM(H(l−1)), (5) where M:= {EC|C ∈RM×C}. Besides, slλ <1, implying that the output of the l-th layer of GCN on Gexponentially approaches M. We also need to adopt some concepts from Lovász et al. (1993) in proving Theorem 1. Consider the graph Gas an electrical network, where each edge represents an unit resistance. Then the effective resistance, Rst from node sto node tis deﬁned as the total resistance between node sand t. According to Corollary 3.3 and Theorem 4.1 (i) in Lovász et al. (1993), we can build the connection between λand Rst for each connected component via commute time as the following inequality. λ≥1 − 1 Rst ( 1 ds + 1 dt ). (6) Prior to proving Theorem 1, we ﬁrst derive the lemma below. Lemma 2. The ϵ-smoothing happens whenever the layer number satisﬁes l≥ˆl= ⌈ log ϵ dM(X) log(sλ) ⌉, (7) where ⌈·⌉computes the ceil of the input. It means ˆl≥l∗. Proof. We start our proof from Inequality 5, leading to dM(H(l)) ≤slλdM(H(l−1)) ≤( l∏ i=1 si)λldM(X) ≤slλldM(X) When it reaches ϵ-smoothing, the following inequality should be satisﬁed as dM(H(l)) ≤slλldM(X) <ϵ, ⇒llog sλ< log ϵ dM(X). (8) Since 0 ≤sλ< 1, then log sλ< 0. Therefore, the Inequality 8 becomes l> log ϵ dM(X) log sλ . (9) Clearly, we have ˆl≥l∗since l∗is deﬁned as the minimal layer that satisﬁes ϵ-smoothing. The proof is concluded. Now, we prove Theorem 1. 12Published as a conference paper at ICLR 2020 Proof. Our proof relies basically on the connection between λand Rst in Equation (6). We recall Corollary 4.3 in Lovász et al. (1993) that removing any edge from Gcan only increase any Rst, then according to (6), the lower bound of λonly increases if the removing edge is not connected to either sor t(i.e. the degree ds and dt keep unchanged). Since there must exist a node pair satisfying Rst = ∞after sufﬁcient edges (except self-loops) are removed from one connected component of G, we have the inﬁnite case λ = 1given in Equation (6) that both 1/ds and 1/dt are consistently bounded by a ﬁnite number,i.e. 1. It implies λdoes increase before it reaches λ= 1. As ˆlis positively related to λ(see the right side of Equation (7) where log(sλ) <0), we have proved the ﬁrst part of Theorem 1, i.e., ˆl(M,ϵ) ≤ˆl(M′,ϵ) after removing sufﬁcient edges. When there happens Rst = ∞, the connected component is disconnected into two parts, which leads to the increment of the dimension of Mby 1 and proves the second part of Theorem 1. i.e., the information loss is decreased: N −dim(M) >N −dim(M′). 13Published as a conference paper at ICLR 2020 B A PPENDIX : M ORE DETAILS IN EXPERIMENTS B.1 D ATASETS STATISTICS Datasets The statistics of all datasets are summarized in Table 3. Table 3: Dataset Statistics Datasets Nodes Edges Classes Features Traing/Validation/Testing Type Cora 2,708 5,429 7 1,433 1,208/500/1,000 Transductive Citeseer 3,327 4,732 6 3,703 1,812/500/1,000 Transductive Pubmed 19,717 44,338 3 500 18,217/500/1,000 Transductive Reddit 232,965 11,606,919 41 602 152,410/23,699/55,334 Inductive B.2 M ODELS AND BACKBONES Backbones Other than the multi-layer GCN, we replace the CNN layer with graph convolution layer to implement three popular backbones recasted from image classiﬁcation. They are residual network (ResGCN)(He et al., 2016; Li et al., 2019), inception network (IncepGCN)(Szegedy et al., 2016) and dense network (JKNet) (Huang et al., 2017; Xu et al., 2018b). Figure 5 shows the detailed architectures of four backbones. Furthermore, we employ one input GCL and one output GCL on these four backbones. Therefore, the layers in ResGCN, JKNet and InceptGCN are at least 3 layers. All backbones are implemented in Pytorch (Paszke et al., 2017). For GraphSAGE, we utilize the Pytorch version implemented by DGL(Wang et al., 2019). Aggregation Input GCL GCL GCL GCL GCL GCL Output (d) IncepGCN Aggregation Input GCL GCL GCL Output (c) JKNet Input GCL GCL GCL Output (a) GCN Input GCL GCL GCL Output (b) ResGCN Figure 5: The illustration of four backbones. GCL indicates graph convolutional layer. Self Feature Modeling We also implement a variant of graph convolution layer with self feature modeling (Fout et al., 2017): H(l+1) = σ ( ˆAH(l)W(l) + H(l)W(l) self ) , (10) where W(l) self ∈RCl×Cl−1 . Hyper-parameter Optimization We adopt the Adam optimizer for model training. To ensure the re-productivity of the results, the seeds of the random numbers of all experiments are set to the same. We ﬁx the number of training epoch to 400 for all datasets. All experiments are conducted on a NVIDIA Tesla P40 GPU with 24GB memory. Given a model with n∈{2,4,8,16,32,64}layers, the hidden dimension is 128 and we conduct a random search strategy to optimize the other hyper-parameter for each backbone in § 5.1. The de- cryptions of hyper-parameters are summarized in Table 4. Table 5 depicts the types of the normalized adjacency matrix that are selectable in the “normalization” hyper-parameter. For GraphSAGE, the aggregation type like GCN, MAX, MEAN, or LSTM is a hyper-parameter as well. For each model, we try 200 different hyper-parameter combinations via random search and select the best test accuracy as the result. Table 6 summaries the hyper-parameters of each backbone with the best accuracy on different datasets and their best accuracy are reported in Table 2. 14Published as a conference paper at ICLR 2020 B.3 T HE VALIDATION LOSS ON DIFFERENT BACKBONES W AND W /O DROP EDGE . Figure 6 depicts the additional results of validation loss on different backbones w and w/o DropEdge. 0 25 50 75 100 125 150 175 200 Epoch 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2Validation Loss Cora ResGCN-6 GCN-6 InceptGCN-6 JKNet-6 ResGCN-6+DropEdge GCN-6+DropEdge InceptGCN-6+DropEdge JKNet-6+DropEdge 0 25 50 75 100 125 150 175 200 Epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0Validation Loss Citeseer ResGCN-4 GCN-4 InceptGCN-4 JKNet-4 ResGCN-4+DropEdge GCN-4+DropEdge InceptGCN-4+DropEdge JKNet-4+DropEdge 0 25 50 75 100 125 150 175 200 Epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0Validation Loss Citeseer ResGCN-6 GCN-6 InceptGCN-6 JKNet-6 ResGCN-6+DropEdge GCN-6+DropEdge InceptGCN-6+DropEdge JKNet-6+DropEdge Figure 6: The validation loss on different backbones w and w/o DropEdge. GCN-ndenotes PlainGCN of depth n; similar denotation follows for other backbones. B.4 T HE ABLATION STUDY ON CITESEER Figure 7a shows the ablation study of Dropout vs. DropEdge and Figure 4b depicts a comparison between the proposed DropEdge and the layer-wise DropEdge on Citeseer. 0 25 50 75 100 125 150 175 200 Epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0Validation Loss Citeseer GCN-4 (No DropEdge, No Dropout) GCN-4 (No DropEdge, Dropout) GCN-4 (DropEdge, No Dropout) GCN-4 (DropEdge, Dropout) (a) Ablation study of Dropout vs. DropEdge on Citeseer. 0 25 50 75 100 125 150 175 200 Epoch 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8Train & Validation Loss Citeseer GCN-4+DropEdge:Validation GCN-4+DropEdge:Train GCN-4+DropEdge (LI):Validation GCN-4+DropEdge (LI):Train (b) Performance comparison of layer-wise DropE- dge. Figure 7 Table 4: Hyper-parameter Description Hyper-parameter Description lr learning rate weight-decay L2 regulation weight sampling-percent edge preserving percent (1 − p) dropout dropout rate normalization the propagation models (Kipf & Welling, 2017) withloop using self feature modeling withbn using batch normalization 15Published as a conference paper at ICLR 2020 Table 5: The normalization / propagation models Description Notation A′ First-order GCN FirstOrderGCN I + D−1/2AD−1/2 Augmented Normalized Adjacency AugNormAdj (D + I)−1/2(A + I)(D + I)−1/2 Augmented Normalized Adjacency with Self-loop BingGeNormAdj I + (D + I)−1/2(A + I)(D + I)−1/2 Augmented Random Walk AugRWalk (D + I)−1(A + I) Table 6: The hyper-parameters of best accuracy for each backbone on all datasets. Dataset Backbone nlayers Acc. Hyper-parameters Cora GCN 4 0.876 lr:0.010, weight-decay:5e-3, sampling-percent:0.7, dropout:0.8, nor- malization:FirstOrderGCN ResGCN 4 0.87 lr:0.001, weight-decay:1e-5, sampling-percent:0.1, dropout:0.5, nor- malization:FirstOrderGCN JKNet 16 0.88 lr:0.008, weight-decay:5e-4, sampling-percent:0.2, dropout:0.8, nor- malization:AugNormAdj IncepGCN 8 0.882 lr:0.010, weight-decay:1e-3, sampling-percent:0.05, dropout:0.5, normalization:AugNormAdj GraphSage 4 0.881 lr:0.010, weight-decay:5e-4, sampling-percent:0.4, dropout:0.5, ag- gregator:mean Citeseer GCN 4 0.792 lr:0.009, weight-decay:1e-3, sampling-percent:0.05, dropout:0.8, normalization:BingGeNormAdj, withloop, withbn ResGCN 16 0.794 lr:0.001, weight-decay:5e-3, sampling-percent:0.5, dropout:0.3, nor- malization:BingGeNormAdj, withloop JKNet 8 0.802 lr:0.004, weight-decay:5e-5, sampling-percent:0.6, dropout:0.3, nor- malization:AugNormAdj, withloop IncepGCN 8 0.805 lr:0.002, weight-decay:5e-3, sampling-percent:0.2, dropout:0.5, nor- malization:BingGeNormAdj, withloop GraphSage 2 0.8 lr:0.001, weight-decay:1e-4, sampling-percent:0.1, dropout:0.5, ag- gregator:mean Pubmed GCN 4 0.913 lr:0.010, weight-decay:1e-3, sampling-percent:0.3, dropout:0.5, nor- malization:BingGeNormAdj, withloop, withbn ResGCN 32 0.911 lr:0.003, weight-decay:5e-5, sampling-percent:0.7, dropout:0.8, nor- malization:AugNormAdj, withloop, withbn JKNet 64 0.916 lr:0.005, weight-decay:1e-4, sampling-percent:0.5, dropout:0.8, nor- malization:AugNormAdj, withloop,withbn IncepGCN 4 0.916 lr:0.002, weight-decay:1e-5, sampling-percent:0.5, dropout:0.8, nor- malization:BingGeNormAdj, withloop, withbn GraphSage 8 0.917 lr:0.007, weight-decay:1e-4, sampling-percent:0.8, dropout:0.3, ag- gregator:mean Reddit GCN 4 0.9671 lr:0.005, weight-decay:1e-4, sampling-percent:0.6, dropout:0.5, nor- malization:AugRWalk, withloop ResGCN 16 0.9648 lr:0.009, weight-decay:1e-5, sampling-percent:0.2, dropout:0.5, nor- malization:BingGeNormAdj, withbn JKNet 8 0.9702 lr:0.010, weight-decay:5e-5, sampling-percent:0.6, dropout:0.5, nor- malization:BingGeNormAdj, withloop,withbn IncepGCN 8 0.9687 lr:0.008, weight-decay:1e-4, sampling-percent:0.4, dropout:0.5, nor- malization:FirstOrderGCN, withbn GraphSAGE 4 0.9654 lr:0.005, weight-decay:5e-5, sampling-percent:0.2, dropout:0.3, ag- gregator:mean 16Published as a conference paper at ICLR 2020 Table 7: Accuracy (%) comparisons on different backbones with and without DropEdge Dataset Backbone 2 4 8 16 32 64 Orignal DropEdge Orignal DropEdge Orignal DropEdge Orignal DropEdge Orignal DropEdge Orignal DropEdge Cora GCN 86.10 86.50 85.50 87.60 78.70 85.80 82.10 84.30 71.60 74.60 52.00 53.20 ResGCN - - 86.00 87.00 85.40 86.90 85.30 86.90 85.10 86.80 79.80 84.80 JKNet - - 86.90 87.70 86.70 87.80 86.20 88.00 87.10 87.60 86.30 87.90 IncepGCN - - 85.60 87.90 86.70 88.20 87.10 87.70 87.40 87.70 85.30 88.20 GraphSAGE 87.80 88.10 87.10 88.10 84.30 87.10 84.10 84.50 31.90 32.20 31.90 31.90 Citeseer GCN 75.90 78.70 76.70 79.20 74.60 77.20 65.20 76.80 59.20 61.40 44.60 45.60 ResGCN - - 78.90 78.80 77.80 78.80 78.20 79.40 74.40 77.90 21.20 75.30 JKNet - - 79.10 80.20 79.20 80.20 78.80 80.10 71.70 80.00 76.70 80.00 IncepGCN - - 79.50 79.90 79.60 80.50 78.50 80.20 72.60 80.30 79.00 79.90 GraphSAGE 78.40 80.00 77.30 79.20 74.10 77.10 72.90 74.50 37.00 53.60 16.90 25.10 Pubmed GCN 90.20 91.20 88.70 91.30 90.10 90.90 88.10 90.30 84.60 86.20 79.70 79.00 ResGCN - - 90.70 90.70 89.60 90.50 89.60 91.00 90.20 91.10 87.90 90.20 JKNet - - 90.50 91.30 90.60 91.20 89.90 91.50 89.20 91.30 90.60 91.60 IncepGCN - - 89.90 91.60 90.20 91.50 90.80 91.30 OOM 90.50 OOM 90.00 GraphSAGE 90.10 90.70 89.40 91.20 90.20 91.70 83.50 87.80 41.30 47.90 40.70 62.30 Reddit GCN 96.11 96.13 96.62 96.71 96.17 96.48 67.11 90.54 45.55 50.51 - - ResGCN - - 96.13 96.33 96.37 96.46 96.34 96.48 93.93 94.27 - - JKNet - - 96.54 96.75 96.82 97.02 OOM 96.78 OOM OOM - - IncepGCN - - 96.48 96.77 96.43 96.87 OOM OOM OOM OOM - - GraphSAGE 96.22 96.28 96.45 96.54 96.38 96.42 96.15 96.18 96.43 96.47 - - 17Published as a conference paper at ICLR 2020 18",
      "meta_data": {
        "arxiv_id": "1907.10903v4",
        "authors": [
          "Yu Rong",
          "Wenbing Huang",
          "Tingyang Xu",
          "Junzhou Huang"
        ],
        "published_date": "2019-07-25T08:57:45Z",
        "pdf_url": "https://arxiv.org/pdf/1907.10903v4.pdf"
      }
    },
    {
      "title": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks",
      "abstract": "This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach\nthat aims to overcome the limitations of standard GNN frameworks. In DropGNNs,\nwe execute multiple runs of a GNN on the input graph, with some of the nodes\nrandomly and independently dropped in each of these runs. Then, we combine the\nresults of these runs to obtain the final result. We prove that DropGNNs can\ndistinguish various graph neighborhoods that cannot be separated by message\npassing GNNs. We derive theoretical bounds for the number of runs required to\nensure a reliable distribution of dropouts, and we prove several properties\nregarding the expressive capabilities and limits of DropGNNs. We experimentally\nvalidate our theoretical findings on expressiveness. Furthermore, we show that\nDropGNNs perform competitively on established GNN benchmarks.",
      "full_text": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks Pál András Papp ETH Zurich apapp@ethz.ch Karolis Martinkus ETH Zurich martinkus@ethz.ch Lukas Faber ETH Zurich lfaber@ethz.ch Roger Wattenhofer ETH Zurich wattenhofer@ethz.ch Abstract This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNNs, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these runs to obtain the ﬁnal result. We prove that DropGNNs can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. We derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and we prove several properties regarding the expressive capabilities and limits of DropGNNs. We experimentally validate our theoretical ﬁndings on expressiveness. Furthermore, we show that DropGNNs perform competitively on established GNN benchmarks. 1 Introduction Neural networks have been successful in handling various forms of data. Since some of the world’s most interesting data is represented by graphs, Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various ﬁelds such as quantum chemistry, physics, or social networks [12; 27; 18]. On the other hand, GNNs are also known to have severe limitations and are sometimes unable to recognize even simple graph structures. In this paper, we present a new approach to increase the expressiveness of GNNs, called Dropout Graph Neural Networks (DropGNNs). Our main idea is to execute not one but multiple different runs of the GNN. We then aggregate the results from these different runs into a ﬁnal result. In each of these runs, we remove (“drop out”) each node in the graph with a small probability p. As such, the different runs of an episode will allow us to not only observe the actual extended neighborhood of a node for some number of layers d, but rather to observe various slightly perturbed versions of this d-hop neighborhood. We emphasize that this notion of dropouts is very different from the popular dropout regularization method; in particular, DropGNNs remove nodes during both training and testing, since their goal is to observe a similar distribution of dropout patterns during training and testing. This dropout technique increases the expressive power of our GNNs dramatically: even when two distinct d-hop neighborhoods cannot be distinguished by a standard GNN, their dropout variants (with a few nodes removed) are already separable by GNNs in most cases. Thus by learning to identify the dropout patterns where the two d-hop neighborhoods differ, DropGNNs can also distinguish a wide variety of cases that are beyond the theoretical limits of standard GNNs. Our contributions. We begin by showing several example graphs that are not distinguishable in the regular GNN setting but can be easily separated by DropGNNs. We then analyze the theoretical properties of DropGNNs in detail. We ﬁrst show that executing ˜O(γ) different runs is often already 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2111.06283v1  [cs.LG]  11 Nov 2021sufﬁcient to ensure that we observe a reasonable distribution of dropouts in a neighborhood of size γ. We then discuss the theoretical capabilities and limitations of DropGNNs in general, as well as the limits of the dropout approach when combined with speciﬁc aggregation methods. We validate our theoretical ﬁndings on established problems that are impossible to solve for standard GNNs. We ﬁnd that DropGNNs clearly outperform the competition on these datasets. We further show that DropGNNs have a competitive performance on several established graph benchmarks, and they provide particularly impressive results in applications where the graph structure is really a crucial factor. 2 Related Work GNNs apply deep learning to graph-structured data [30]. In GNNs, every node has an embedding that is shared over multiple iterations with its neighbors. This way nodes can gather their neighbors’ features. In recent years, many different models have been proposed to realize how the information between nodes is shared [35]. Some approaches take inspiration from convolution [23; 8; 13], others from graph spectra [18; 5], others from attention [33], and others extend previous ideas of established concepts such as skip connections [36]. Principally, GNNs are limited in their expressiveness by the Weisfeiler-Lehman test (WL-test) [37], a heuristic to the graph isomorphism problem. The work of [37] proposes a new architecture, Graph Isomoprhism Networks (GIN), that is proven to be exactly as powerful as the WL-test. However, even GINs cannot distinguish certain different graphs, namely those that the WL-test cannot distinguish. This ﬁnding [11] motivated more expressive GNN architectures. These improvements follow two main paths. The ﬁrst approach augments the features of nodes or edges by additional information to make nodes with similar neighborhoods distinguishable. Several kinds of information have been used: inspired from distributed computing are port numbers on edges [28], unique IDs for nodes [20], or random features on nodes [29; 1]. Another idea is to use angles between edges [19] from chemistry (where edges correspond to electron bonds). However, all of these approaches have some shortcomings. For ports and angles, there are some simple example graphs that still cannot be distinguished with these extensions [11]. Adding IDs or random features helps during training, but the learned models do not generalize: GNNs often tend to overﬁt to the speciﬁc random values in the training set, and as such, they produce weaker results on unseen test graphs that received different random values. In contrast to this, DropGNNs observe a similar distribution of embeddings during training and testing, and hence they also generalize well to test set graphs. The second approach exploits the fact that running the WL-test on tuples, triples, or generally k- tuples keeps increasing its expressiveness. Thus a GNN operating on tuples of nodes has higher expressiveness than a standard GNN [22; 21]. However, the downside of this approach is that even building a second-order graph blows up the graph quadratically. The computational cost quickly becomes a problem that needs to be to addressed, for example with sampling [ 22]. Furthermore, second-order graph creation is a global operation of the graph that destroys the local semantics induced by the edges. In contrast to this, DropGNN can reason about graphs beyond the WL-test with only a small overhead (through run repetition), while also keeping the local graph structure intact. Our work is also somewhat similar to the randomized smoothing approach [7], which has also been extended to GNNs recently [ 4]. This approach also conducts multiple runs on slightly perturbed variants of the data. However, in randomized smoothing, the different embeddings are combined in a smoothing operation (e.g. majority voting), which speciﬁcally aims to get rid of the atypical perturbed variants in order to increase robustness. In contrast to this, the main idea of DropGNNs is exactly to ﬁnd and identify these perturbed special cases which are notably different from the original neighborhood, since these allow us to distinguish graphs that otherwise seem identical. Finally, we note that removing nodes is a common tool for regularization in deep neural networks, which has also seen use in GNNs [26; 9]. However, as mentioned before, this is a different dropout concept where nodes are only removed during training to reduce the co-dependence of nodes. 2u u u u u Figure 1: Illustration of 4 possible dropout combinations from an example 2-hop neighborhood around u: a 0-dropout, two different 1-dropouts and a 2-dropout. 3 DropGNN 3.1 About GNNs Almost all GNN architectures [33; 18; 37; 8; 35; 13; 36] follow the message passing framework [12; 3]. Every node starts with an embedding given by its initial features. One round of message passing has three steps. In the ﬁrst MESSAGE step, nodes create a message based on their embedding and send this message to all neighbors. Second, nodes AGGREGATE all messages they receive. Third, every node UPDATE s its embedding based on its old embedding and the aggregated messages. One such round corresponds to one GNN layer. Usually, a GNN performs drounds of message passing for some small constant d. Thus, the node’s embedding in a GNN reﬂects its features and the information within its d-hop neighborhood. Finally, a READOUT method translates these ﬁnal embeddings into predictions. Usually, MESSAGE , AGGREGATE , UPDATE and READOUT are functions with learnable parameters, for instance linear layers with activation functions. This GNN paradigm is closely related to the WL-test for a pair of graphs, which is an iterative color reﬁnement procedure. In rounds 1,...,d , each node looks at its own color and the multiset of colors of its direct neighbors, and uses a hash function to select a new color based on this information. As such, if the WL-test cannot distinguish two graphs, then a standard GNN cannot distinguish them either: intuitively, the nodes in these graphs receive the same messages and create the same embedding in each round, and thus they always arrive at the same ﬁnal result. 3.2 Idea and motivation The main idea of DropGNNs is to execute multiple independent runs of the GNN during both training and testing. In each run, every node of the GNN is removed with probability p, independently from all other nodes. If a node vis removed during a run, then vdoes not send or receive any messages to/from its neighbors and does not affect the remaining nodes in any way. Essentially, the GNN behaves as if v (and its incident edges) were not present in the graph in the speciﬁc run, and no embedding is computed for vin this run (see Figure 1 for an illustration). Over the course of multiple runs, dropouts allow us to not only observe the d-hop neighborhood around any node u, but also several slightly perturbed variants of this d-hop neighborhood. In the different runs, the embedding computed forumight also slightly vary, depending on which node(s) are missing from its d-hop neighborhood in a speciﬁc run. This increases the expressive power of GNNs signiﬁcantly: even when two different d-hop neighborhoods cannot be distinguished by standard GNNs, the neighborhood variants observed when removing some of the nodes are usually still remarkably different. In Section 3.4, we discuss multiple examples for this improved expressiveness. Our randomized approach means that in different runs, we will have different nodes dropping out of the GNN. As such, the GNN is only guaranteed to produce the same node embeddings in two runs if we have exactly the same subset of nodes dropping out. Given the d-hop neighborhood of a node u, we will refer to a speciﬁc subset of nodes dropping out as a dropout combination, or more concretely as a k-dropout in case the subset has size k. In order to analyze the d-hop neighborhood of u, the reasonable strategy is to use a relatively small dropout probability p: this ensures that in each run, only a few nodes are removed (or none at all), and 3thus the GNN will operate on a d-hop neighborhood that is similar to the original neighborhood of u. As a result, 1-dropouts will be frequent, while for a larger k, observing a k-dropout will be unlikely. To reduce the effect of randomization on the ﬁnal outcome, we have to execute multiple independent runs of our GNN; we denote this number of runs by r. For a successful application of the dropout idea, we have to select rlarge enough to ensure that the set of observed dropout combinations is already reasonably close to the actual probability distribution of dropouts. In practice, this will not be feasible for k-dropouts with large kthat occur very rarely, but we can already ensure for a reasonably small rthat e.g. the frequency of each 1-dropout is relatively close to its expected value. 3.3 Run aggregation Recall that standard GNNs ﬁrst compute a ﬁnal embedding for each node through dlayers, and then they use a READOUT method to transform this into a prediction. In DropGNNs, we also need to introduce an extra phase between these two steps, called run aggregation. In particular, we execute rindependent runs of the d-layer GNN (with different dropouts), which altogether produces rdistinct ﬁnal embeddings for a node u. Hence we also need an extra step to merge these rdistinct embeddings into a single ﬁnal embedding of u, which then acts as the input for the READOUT function. This run aggregation method has to transform a multiset of embeddings into a single embedding; furthermore, it has to be a permutation-invariant function (similarly to neighborhood aggregation), since the ordering of different runs carries no meaning. We note that simply applying a popular permutation-invariant function for run aggregation, such as sum or max, is often not expressive enough to extract sufﬁcient information from the distribution of runs. Instead, one natural solution is to ﬁrst apply a transformation on each node embedding, and only execute sum aggregation afterward. For example, a simple transformation x →σ(Wx + b), where σdenotes a basic non-linearity such as a sigmoid or step function, is already sufﬁcient for almost all of our examples and theoretical results in the paper. 3.4 Motivational examples We discuss several examples to demonstrate how DropGNNs are more expressive than standard GNNs. We only outline the intuitive ideas behind the behavior of the DropGNNs here; however, in Appendix A, we also describe the concrete functions that can separate each pair of graphs. Example 1. Figure 2a shows a fundamental example of two different graphs that cannot be distin- guished by the 1-WL test, consisting of cycles of different length. This example is known to be hard for extended GNNs variants: the two cases cannot even be distinguished if we also use port numbers or angles between the edges [11]. The simplest solution here is to consider a GNN with d = 2 layers; this already provides a very different distribution of dropouts in the two graphs. For example, the8-cycle has 2 distinct 1-dropouts where uretains both of its direct neighbors, but it only has 1 neighbor at distance 2; such a situation is not possible in the 4-cycle at all. Alternatively, the4-cycle has a 1-dropout case with probability p·(1 −p)2 where uhas 2 direct neighbors, but no distance 2 neighbors at all; this only happens for a 2-dropout in the 8-cycle, i.e. with a probability of only p2 ·(1 −p)2. With appropriate weights, a GNN can learn to recognize these situations, and thus distinguish the two cases. Example 2. Figure 2b shows another example of two graphs that cannot be separated by a WL test; note that node features simply correspond to the degrees of the nodes. From an algorithmic perspective, it is not hard to distinguish the two graphs from speciﬁc 1-dropout cases. Let uand vdenote the two gray nodes in the graphs, and consider the process from u’s perspective. In both graphs, ucan recognize if vis removed in a run since udoes not receive a “gray” message in the ﬁrst round. However, the dropout of vhas a different effect in the two graphs later in the process: in the right-hand graph, it means that there is no gray neighbor at a 3-hop distance from u, while in the left-hand graph, uwill still see a gray node (itself) in a 3-hop distance. Thus by identifying the 1-dropout of v, an algorithm can distinguish the two graphs: if we observe runs where ureceives no gray message in the ﬁrst round, but it receives an (aggregated) gray message 4(a) (b) u u (c) Figure 2: Several example graphs which show that DropGNNs are more expressive than standard GNNs in various cases. Different node colors correspond to different node features. in the third round, then uhas the left-hand neighborhood. This also means that a sufﬁciently powerful GNN which is equivalent to the 1-WL test can also separate the two cases. Example 3. Note that using a sum function for neighborhood aggregation is often considered a superior choice to mean, since ucannot separate e.g. the two cases shown in Figure 2c with mean aggregation [37]. However, the mean aggregation of neighbors also has some advantages over sum; most notably, it the computed values do not increase with the degree of the node. We show that dropouts also increase the expressive power of GNNs with mean aggregation, thus possibly making mean aggregation a better choice in some applications. In particular, a DropGNN with mean aggregation is still able to separate the two cases on Figure 2c. Assume that the two colors in the ﬁgure correspond to feature values of 1 and −1, and let p= 1 4 . In the left-hand graph, there is a 1-dropout where uends up with a single neighbor of value 1; hence mean aggregation yields a value of 1 with probability 1 4 ·3 4 ≈0.19 in each run. However, in the right-hand graph, the only way to obtain a mean of 1 is through a 2-dropout or some 3-dropouts; one can calculate that the total probability of these is only 0.06 (see Appendix A). If we ﬁrst transform all other values to 0 (e.g. with σ(x−0.5), where σis a step function), then run aggregation with mean or sum can easily separate these cases. Note that if we apply a more complex transformation at run aggregation, then separation is even much easier, since e.g. the mean value of 0.33 can only appear in the right-hand graph. 4 Theoretical analysis 4.1 Required number of runs We analyze DropGNNs with respect to the neighborhood of interest around a node u, denoted by Γ. That is, we select a speciﬁc region around u, and we want to ensure that the distribution of dropout combinations in this region is reasonably close to the actual probabilities. This choice of Γ then determines the ideal choice of pand rin our DropGNN. One natural choice is to select Γ as the entire d-hop neighborhood of u, since a GNN will always compute its ﬁnal values based on this region of the graph. Note that even for this largest possible Γ, the size of this neighborhood γ := |Γ|does not necessarily scale with the entire graph. That is, input graphs in practice are often sparse, and we can e.g. assume that their node degrees are upper bounded by a constant; this is indeed realistic in many biological or chemical applications, and also a frequent assumptions in previous works [28]. In this case, having d= O(1) layers implies that γis also essentially a constant, regardless of the size of the graph. However, we point out that Γ can be freely chosen as a neighborhood of any speciﬁc size. That is, even if a GNN aggregates information within a distance of d = 5 layers, we can still select Γ to denote, for example, only the 2-hop neighborhood of u. The resulting DropGNN will still compute a ﬁnal node embedding based on the entire 5-hop neighborhood of u; however, our DropGNN will now only ensure that we observe a reasonable distribution of dropout combinations in the 2-hop neighborhood of u. In this sense, the size γ is essentially a trade-off hyperparameter: while a smaller γ will require a smaller number of runs runtil the distribution of dropout combinations stabilizes, a larger γallows us to observe more variations of the region around u. 51-complete dropouts. From a strictly theoretical perspective, choosing a sufﬁciently largeralways allows us to observe every possible dropout combination. However, since the number of combinations is exponential in γ, this approach is not viable in practice (see Appendix B for more details). To reasonably limit the number of necessary runs, we focus on the so-called 1-complete case: we want to have enough runs to ensure that at least every 1-dropout is observed a few times. Indeed, if we can observe each variant of Γ where a single node is removed, then this might already allow a sophisticated algorithm to reconstruct a range of useful properties of Γ. Note that in all of our examples, a speciﬁc 1-dropout was already sufﬁcient to distinguish the two cases. For any speciﬁc node v∈Γ, the probability of a 1-dropout for vis p·(1 −p)γ in a run (including the probability that uis not dropped out). We apply the pvalue that maximizes the probability of such a 1-dropout; a simple differentiation shows that this maximum is obtained at p∗ = 1 1+γ. This choice of palso implies that the probability of observing a speciﬁc 1-dropout in a run is 1 1 + γ · ( γ 1 + γ )γ ≥ 1 1 + γ ·1 e. Hence if we execute r≥e·(γ+ 1) = Ω(γ) runs, then the expected number of times we observe a speciﬁc 1-dropout (let us denote this by E1) is at least E1 ≥r·1 e · 1 1+γ ≥1. Moreover, one can use a Chernoff bound to show that afterΩ(γlog γ) runs, the frequency of each 1-dropout is sharply concentrated around E1. This also implies that we indeed observe each1-dropout at least once with high probability. For a more formal statement, let us consider a constantδ∈[0,1] and an error probability 1 t <1. Also, given a node v ∈Γ (or subset S ⊆Γ), let Xv (or XS) denote the number of times this 1-dropout (|S|-dropout) occurs during our runs. Theorem 1 If r ≥Ω (γlog γt), then with a probability of 1 −1 t, it holds that for each v ∈Γ, we have Xv ∈[ (1−δ) ·E1 , (1+δ) ·E1 ]. With slightly more runs, we can even ensure that each k-dropout for k≥2 happens less frequently than 1-dropouts. In this case, it already becomes possible to distinguish 1-dropouts from multiple- dropout cases based on their frequency. Theorem 2 If r≥Ω ( γ2 + γlog γt ) , then with a probability of 1 −1 t it holds that • for each v∈Γ, we have Xv ∈[ (1−δ) ·E1 , (1+δ) ·E1 ], • for each S ⊆Γ with |S|≥ 2, we have XS <(1−δ) ·E1. Since the number of all dropout combinations is in the magnitude of2γ, proving this bound is slightly more technical. We discuss the proofs of these theorems in Appendix B. Note that in sparse graphs, whereγis essentially a constant, the number of runs described in Theorems 1 and 2 is also essentially a constant; as such, DropGNNs only impose a relatively small (constant factor) overhead in this case. Finally, note that these theorems only consider the dropout distribution around a speciﬁc node u. To ensure the same properties for all nnodes in the graph simultaneously, we need to add a further factor of nwithin the logarithm to the number of necessary runs in Theorems 1 and 2. However, while this is only a logarithmic dependence on n, it might still be undesired in practice. 4.2 Expressive power of DropGNNs In Section 3.4, we have seen that DropGNNs often succeed when a WL-test fails. It is natural to wonder about the capabilities and limits of the dropout approach in general; we study this question for multiple neighborhood aggregation methods separately. We consider neighborhood aggregation with sum and mean in more detail; the proofs of the cor- responding claims are discussed in Appendices C and D, respectively. Appendix D also discusses brieﬂy why max aggregation does not combine well with the dropout approach in practice. 6u u 1–dropouts u Figure 3: Example of two graphs not separable by 1-dropouts (left side). In both of the graphs, for any of the 1-dropouts, uobserves the same tree structure for d= 2, shown on the right side. Aggregation with sum. Previous work has already shown that sum neighborhood aggregation allows for an injective GNN design, which computes a different embedding for any two neighborhoods whenever they are not equivalent for the WL-test [ 37]. Intuitively speaking, this means that sum aggregation has the same expressive power as a general-purpose d-hop distributed algorithm in the corresponding model, i.e. without IDs or port numbers. Hence to understand the expressiveness of DropGNNs in this case, one needs to analyze which embeddings can be computed by such a distributed algorithm from a speciﬁc (observed) distribution of dropout combinations. It is already non-trivial to ﬁnd two distinct neighborhoods that cannot be distinguished in the 1- complete case. However, such an example exists, even if we also consider2-dropouts. That is, one can construct a pair of d-hop neighborhoods that are non-isomorphic, and yet they produce the exact same distribution of 1- and 2-dropout neighborhoods in a d-layer DropGNN. Theorem 3 There exists a pair of neighborhoods that cannot be distinguished by 1- and 2-dropouts. We illustrate a simpler example for only 1-dropouts in Figure 3. For a construction that also covers the case of 2-dropouts, the analysis is more technical; we defer this to Appendix C. We note that even these more difﬁcult examples can be distinguished with our dropout approach, based on their k-dropouts for larger kvalues. However, this requires an even higher number of runs: we need to ensure that we can observe a reliable distribution even for these many-node dropouts. On the other hand, our dropout approach becomes even more powerful if we combine it e.g. with the extension by port numbers introduced in [28]. Intuitively speaking, port numbers allow an algorithm to determine all paths to the removed node in a 1-dropout, which in turn allows us to reconstruct the entire d-hop neighborhood of u. As such, in this case, 1-complete dropouts already allow us to distinguish any two neighborhoods. Theorem 4 In the setting of Theorem 1, a DropGNN with port numbers can distinguish any two non-isomorphic d-hop neighborhoods. Finally, we note that the expressive power of DropGNNs in the1-complete case is closely related to the graph reconstruction problem, which is a major open problem in theoretical computer science since the 1940s [14]. We discuss the differences between the two settings in Appendix C. Aggregation with mean. We have seen in Section 3.4 that even withmean aggregation, DropGNNs can sometimes distinguish 1-hop neighborhoods (that is, multisets S1 and S2 of features) which look identical to a standard GNN. One can also prove in general that a similar separation is possible in various cases, e.g. whenever the two multisets have the same size. Lemma 1 Let S1 ̸= S2 be two multisets of feature vectors with |S1|= |S2|. Then S1 and S2 can be distinguished by a DropGNN with mean neighborhood aggregation. However, in the general case,mean aggregation does not allow us to separate any two multisets based on 1-dropouts. In particular, in Appendix C, we also describe an example of multisets S1 ∩S2 = ∅ where the distribution of means obtained from 0- and 1-dropouts is essentially identical in S1 and S2. This implies that if we want to distinguish these multisets S1 and S2, then the best we can hope for is a more complex approach based on multiple-node dropouts. 75 Experiments In all cases we extend the base GNN model to a DropGNN by running the GNN rtimes in parallel, doing mean aggregation over the resulting rnode embedding copies before the graph readout step and then applying the base GNN’s graph readout. Additionally, an auxiliary readout head is added to produce predictions based on each individual run. These predictions are used for an auxiliary loss term which comprises 1 3 of the ﬁnal loss. Unless stated otherwise, we set the number of runs to m and choose the dropout probability to be p= 1 m, where mis the mean number of nodes in the graphs in the dataset. This is based on the assumption, that in the datasets we use the GNN will usually have the receptive ﬁeld which covers the whole graph. We implement random node dropout by, in each run, setting all features of randomly selected nodes to 0. See Appendix E for more details about the experimental setup and dataset statistics. The code is publicly available1. 5.1 Datasets beyond WL GIN +Ports +IDs +Random feat. +Dropout Dataset Train Test Train Test Train Test Train Test Train Test LIMITS1 [11] 0.50±0.00 0.50±0.00 0.50±0.00 0.50±0.00 1.00±0.00 0.59±0.19 0.66±0.19 0.66±0.22 1.00±0.001.00±0.00LIMITS2 [11] 0.50±0.00 0.50±0.00 0.50±0.00 0.50±0.00 1.00±0.00 0.61±0.26 0.72±0.17 0.64±0.19 1.00±0.001.00±0.004-CYCLES[20] 0.50 ±0.00 0.50±0.00 1.00±0.01 0.84±0.07 1.00±0.00 0.58±0.07 0.75±0.05 0.77±0.05 0.99±0.031.00±0.01LCC [29] 0.41 ±0.09 0.38±0.08 1.0±0.00 0.39±0.09 1.00±0.00 0.42±0.08 0.45±0.16 0.46±0.08 1.00±0.000.99±0.02TRIANGLES[29] 0.53±0.15 0.52±0.15 1.0±0.00 0.54±0.11 1.00±0.00 0.63±0.08 0.57±0.08 0.67±0.05 0.93±0.120.93±0.13SKIP-CIRCLES[6] 0.10±0.00 0.10±0.00 1.00±0.00 0.14±0.08 1.00±0.00 0.10±0.09 0.16±0.11 0.16±0.05 0.81±0.280.82±0.28 Table 1: Evaluation of techniques that increase GNN expressiveness on challenging synthetic datasets. We highlight the best test scores in bold. Compared to other augmentation techniques DropGNN (GIN +Dropout) achieves high training accuracy but also generalizes well to the test set. To see the capabilities of DropGNN in practice we test on existing synthetic datasets, which are known to require expressiveness beyond the WL-test. We use the datasets from Sato et al.[29] that are based on 3−regular graphs. Nodes have to predict whether they are part of a triangle (TRIANGLES ) or have to predict their local clustering coefﬁcient ( LCC ). We test on the two counterexamples LIMITS 1 (Figure 2a) and LIMITS 2 from Garg et al. [11] where we compare two smaller structures versus one larger structure. We employ the dataset by Loukas[20] to classify graphs on containing a cycle of length 4 ( 4-CYCLES ). We increase the regularity in this dataset by ensuring that each node has a degree of 2. Finally we experiment on circular graphs with skip links (SKIP -CIRCLES ) by Chen et al. [6], where the model needs to classify if a given circular graph has skip links of length {2,3,4,5,6,9,11,12,13,16}. For comparison, we try several other GNN modiﬁcations which increase expressiveness. For control, we run a vanilla GNN on these datasets. We then extend this base GNN with (i) ports [28] (randomly assigned), (ii) node IDs [ 20] (randomly permuted), and (iii) a random feature from the standard normal distribution [29]. The architecture for all GNNs is a 4-layer GIN with sum as aggregation and ε= 0. For DropGNN r= 50 runs are performed. For the SKIP -CIRCLES dataset we use a 9-layer GIN instead, as the skip links can form cycles of up to 17 hops. We train all methods for1,000 epochs and then evaluate the accuracy on the training set. We then test on a new graph (with new features). We report training and testing averaged across10 initializations in Table 1. We can see that DropGNN outperforms the competition. 5.2 Sensitivity analysis We investigate the impact of the number of independent runs on the overall accuracy. Generally, we expect an increasing number of runs to more reliably produce informative dropouts. We train with a sufﬁciently high number of runs (50) with the same setting as before. Now, we reevaluate DropGNN but limit the runs to a smaller number. We measure the average accuracy over10 seeds with 10 tests each and plot this average in Figure 4 on three datasets: LIMITS 1 (Figure 4a), 4-CYCLES (Figure 4b), and TRIANGLES (Figure 4c). In all three datasets, more runs directly translate to higher accuracy. Next, we investigate the impact of the dropout probability p. We use the same setting as before, but instead of varying the number of runs in the reevaluation, we train and test with different probabilities 1https://github.com/KarolisMart/DropGNN 80 10 20 30 40 50 Number of Runs 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (a) LIMITS 1 0 10 20 30 40 50 Number of Runs 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (b) 4−CYCLES 0 10 20 30 40 50 Number of Runs 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (c) TRIANGLES Figure 4: Investigating the impact of the number of runs (x−axis) versus the classiﬁcation accuracy (y−axis). In all three plots, having more runs allows for more stable dropout observations, increasing accuracy. The tradeoff is higher runtime since the model computes more runs. 0.0 0.2 0.4 0.6 0.8 Dropout Probability 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (a) LIMITS 1 0.0 0.2 0.4 0.6 0.8 Dropout Probability 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (b) 4−CYCLES 0.0 0.2 0.4 0.6 0.8 Dropout Probability 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (c) TRIANGLES Figure 5: Investigating the impact of the dropout probability ( x−axis) versus the classiﬁcation accuracy (y−axis). DropGNN is robust to the choice of pfor decently small p. Choosing p≈γ−1 is a decent default that is shown by vertical black lines. pon an exponential scale from 0.01 to 0.64. We also try 0 (no dropout) and 0.95 (almost everything is dropped). Figure 5 shows the accuracy for each dropout probability, again averaged over10 seeds with 10 tests each. Generally, DropGNN is robust to different values of puntil pbecomes very large. 5.3 Graph classiﬁcation Model Complexity MUTAG PTC PROTEINS IMDB-B IMDB-M WL subtree [37; 31]O(n) 90.4±5.7 59.9 ±4.3 75.0 ±3.1 73.8 ±3.9 50.9 ±3.8 DCNN [2] O(n) - - 61.3 ±1.6 49.1 ±1.4 33.5 ±1.4 PatchySan [23] O(n) 89.0±4.4 62.3 ±5.7 75.0 ±2.5 71.0 ±2.3 45.2 ±2.8 DGCNN [39] O(n) 85.8±1.7 58.6 ±2.5 75.5 ±0.9 70.0 ±0.9 47.8 ±0.9 GIN [37] O(n) 89.4±5.6 64.6 ±7.0 76.2 ±2.8 75.1 ±5.1 52.3±2.8 DropGIN (ours) O(rn),r≈20 90.4±7.0 66.3 ±8.6 76.3 ±6.1 75.7 ±4.2 51.4±2.8 1-2-3 GNN [22] O(n4) 86.1 60.9 75.5 74.2 49.5 PPGN [21]* O(n3) 90.6±8.7 66.2 ±6.5 77.2 ±4.7 73±5.8 50.5±3.6 Table 2: Graph classiﬁcation accuracy (%). The best performing model in each complexity class is highlighted in bold. *We report the best result achieved by either of the three versions of their model. We evaluate and compare our modiﬁed GIN model (DropGIN) with the original GIN model and other GNN models of various expressiveness levels on real-world graph classiﬁcation datasets. We use three bioinformatics datasets (MUTAG, PTC, PROTEINS) and two social networks (IMDB-BINARY and IMDB-MULTI) [38]. Following [37] node degree is used as the sole input feature for the IMDB datasets, while for the bioinformatics datasets the original categorical node features are used. We follow the evaluation and model selection protocol described in [ 37] and report the 10-fold cross-validation accuracies [38]. We extend the original 4-layer GIN model described in [37] and use the same hyper-parameter selection as [37]. From Figure 5 we can see that it is usually safer to use a slightly larger pthan a slightly smaller one. Due to this, we set the node dropout probability to p= 2 m, where mis the mean number of nodes in the graphs in the dataset. 9Our method successfully improves the results achieved by the original GIN model on the bioinfor- matics datasets (Table 2) and is, in general, competitive with the more complex and computationally expensive expressive GNNs. Namely, the 1-2-3 GNN [ 22] which has expressive power close to that of 3-WL and O(n4) time complexity, and the Provably Powerful Graph Network (PPGN) [21] which has 3-WL expressive power and O(n3) time complexity. Compared to that, our method has only O(rn) time complexity. However, we do observe, that our approach slightly underperforms the original GIN model on the IMDB-M dataset. Since the other expressive GNNs also underperform when compared to the original GIN model, it is possible that classifying graphs in this dataset rarely requires higher expressiveness. In such cases, our model can lose accuracy compared to the base GNN as many runs are required to achieve a fully stable dropout distribution. 5.4 Graph property regression Property Unit MPNN [34] 1-GNN [22] 1-2-3 GNN [22] PPGN [21] DropMPNN Drop-1-GNN µ Debye 0.358 0.493 0.473 0.0934 0.059* 0.453* α Bohr3 0.89 0.78 0.27 0.318 0.173* 0.767* ϵHOMO Hartree 0.00541 0.00321 0.00337 0.00174 0.00193* 0.00306* ϵLUMO Hartree 0.00623 0.00350 0.00351 0.0021 0.00177* 0.00306* ∆ϵ Hartree 0.0066 0.0049 0.0048 0.0029 0.00282* 0.0046* ⟨R2⟩ Bohr2 28.5 34.1 22.9 3.78 0.392* 30.83* ZPVE Hartree 0.00216 0.00124 0.00019 0.000399 0.000112* 0.000895* U0 Hartree 2.05 2.32 0.0427 0.022 0.0409* 1.80* U Hartree 2.0 2.08 0.111 0.0504 0.0536* 1.86* H Hartree 2.02 2.23 0.0419 0.0294 0.0481* 2.00* G Hartree 2.02 1.94 0.0469 0.24 0.0508* 2.12 Cv cal/(mol K) 0.42 0.27 0.0944 0.0144 0.0596* 0.259* Table 3: Mean absolute errors on QM9 dataset [25]. Best performing model is in bold and DropGNN versions that improve over the corresponding base GNN are marked with a *. We investigate how our dropout technique performs using different base GNN models on a different, graph regression, task. We use the QM9 dataset [ 25], which consists of 134k organic molecules. The task is to predict 12 real-valued physical quantities for each molecule. In all cases, a separate model is trained to predict each quantity. We choose two GNN models to augment: MPNN [12] and 1-GNN [22]. We set the DropGNN run count and node dropout probability the same way as done for graph classiﬁcation. Following previous work [22; 21] the data is split into 80% training, 10% validation, and 10% test sets. Both DropGNN model versions are trained for 300 epochs. From Table 3 we can see that Drop-1-GNN improves upon 1-GNN in most of the cases. In some of them, it even outperforms the much more computationally expensive 1-2-3-GNN, which uses higher- order graphs and has three times more parameters [22]. Meanwhile, DropMPNN always substantially improves on MPNN, often outperforming the Provably Powerful Graph Network (PPGN), which as you may recall scales as O(n3). This highlights the fact that while the DropGNN usually improves upon the base model, the ﬁnal performance is highly dependent on the base model itself. For example, 1-GNN does not use skip connections, which might make retaining detailed information about the node’s extended neighborhood much harder and this information is crucial for our dropout technique. 6 Conclusion We have introduced a theoretically motivated DropGNN framework, which allows us to easily increase the expressive power of existing message passing GNNs, both in theory and practice. DropGNNs are also competitive with more complex GNN architectures which are specially designed to have high expressive power but have high computational complexity. In contrast, our framework allows for an arbitrary trade-off between expressiveness and computational complexity by choosing the number of rounds raccordingly. Societal Impact. In summary, we proposed a model-agnostic architecture improvement for GNNs. We do not strive to solve a particular problem but to enhance the GNN toolbox. Therefore, we do not see an immediate impact on society. We found in our experiments that DropGNN works best on graphs with smaller degrees, such as molecular graphs. Therefore, we imagine that using DropGNN in these scenarios is interesting to explore further. 10References [1] R. Abboud, ˙I. ˙I. Ceylan, M. Grohe, and T. Lukasiewicz. The surprising power of graph neural networks with random node initialization. arXiv preprint arXiv:2010.01179, 2020. [2] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in neural information processing systems, pages 1993–2001, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] A. Bojchevski, J. Klicpera, and S. Günnemann. Efﬁcient robustness certiﬁcates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In Proceedings of the 37th International Conference on Machine Learning, pages 1003–1013. PMLR, 2020. [5] J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun. Spectral networks and locally connected networks on graphs. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. [6] Z. Chen, L. Chen, S. Villar, and J. Bruna. On the equivalence between graph isomorphism testing and function approximation with gnns. Advances in neural information processing systems, 2019. [7] J. Cohen, E. Rosenfeld, and Z. Kolter. Certiﬁed adversarial robustness via randomized smooth- ing. In Proceedings of the 36th International Conference on Machine Learning, pages 1310– 1320. PMLR, 2019. [8] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in neural information processing systems, 2016. [9] W. Feng, J. Zhang, Y . Dong, Y . Han, H. Luan, Q. Xu, Q. Yang, E. Kharlamov, and J. Tang. Graph random neural networks for semi-supervised learning on graphs. Advances in Neural Information Processing Systems, 33, 2020. [10] M. Fey and J. E. Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019. [11] V . Garg, S. Jegelka, and T. Jaakkola. Generalization and representational limits of graph neural networks. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3419–3430. PMLR, 13–18 Jul 2020. [12] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), Sydney, Australia, Aug. 2017. [13] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in neural information processing systems, 2017. [14] F. Harary. A survey of the reconstruction conjecture. In R. A. Bari and F. Harary, editors, Graphs and Combinatorics, pages 18–28, Berlin, Heidelberg, 1974. Springer Berlin Heidelberg. ISBN 978-3-540-37809-9. [15] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V . Pande, and J. Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019. [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning , pages 448–456. PMLR, 2015. [17] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. [18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations ICLR, Toulon, France, Apr. 2017. 11[19] J. Klicpera, J. Groß, and S. Günnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations, 2020. [20] A. Loukas. What graph neural networks cannot learn: depth vs width. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [21] H. Maron, H. Ben-Hamu, H. Serviansky, and Y . Lipman. Provably powerful graph networks. arXiv preprint arXiv:1905.11136, 2019. [22] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4602–4609, 2019. [23] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pages 2014–2023. PMLR, 2016. [24] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. [25] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. V on Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientiﬁc data, 1(1):1–7, 2014. [26] Y . Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [27] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pages 8459–8468. PMLR, 2020. [28] R. Sato, M. Yamada, and H. Kashima. Approximation ratios of graph neural networks for combinatorial problems. In Neural Information Processing Systems (NeurIPS), 2019. [29] R. Sato, M. Yamada, and H. Kashima. Random features strengthen graph neural networks. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM) , pages 333–341. SIAM, 2021. [30] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 2008. [31] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011. [32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine learning research, 15 (1):1929–1958, 2014. [33] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), Vancouver, BC, Canada, May 2018. [34] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and V . Pande. Moleculenet: a benchmark for molecular machine learning.Chemical science, 9(2): 513–530, 2018. [35] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2020. [36] K. Xu, C. Li, Y . Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. InInternational Conference on Machine Learning (ICML), Stockholm, Sweden, July 2018. 12[37] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [38] P. Yanardag and S. Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365–1374, 2015. [39] M. Zhang, Z. Cui, M. Neumann, and Y . Chen. An end-to-end deep learning architecture for graph classiﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 32, 2018. 13A Concrete GNN representations for the examples In this section, we revisit the example graphs from Section 3.4, and we provide a concrete GNN implementation for each of them which is able to distinguish the two cases. Example 1. Let us assume for simplicity that each node starts with the integer1 as its single feature. Also, assume that neighborhood aggregation happens with a simple summation, with no non-linearity afterwards, and that this sum is then combined with the node’s own feature again through a simple addition. Now consider this GNN with d= 2 layers. Note that in this case, a node uin the left-hand graph is able to gather information from the whole cycle, while a node uin the right-hand graph will behave as if it was the middle node in a simple path of 5 nodes. In both cases, if no dropouts happen, then u will have a value of 3 after the ﬁrst round, and a value of 9 after the second round. However, the 1-dropouts are already signiﬁcantly different: in the left-hand graph, they will produce a result of 5, 5 and 7, while in the right-hand graph, they result in a ﬁnal value of 5, 5, 8 and 8. One can similarly compute the k-dropouts for k≥2, which will also produce a range of other values (but at most 7 in any case). If we apply a more sophisticated transformation on these embeddings before run aggregation, then it is straightforward to separate these two distributions. For example, we can use an MLP to only obtain a positive value in case if the embedding is 8 (we discuss this technique in more detail at Example 2); this will happen regularly for the right-hand graph, but never for the left-hand graph. After this, a simple sum run aggregation already distinguishes the cases. However, if one prefers a simpler transformation, then a choice of σ(x−8) also sufﬁces (with σ denoting the Heaviside step function). With this transformation, a run aggregation with sum simply counts the number of cases when the ﬁnal embedding was a 9. Since the probability of the 0-dropout is different in the two graphs, the expected value of this count will also differ by at leastΩ(p·r) after rruns, which makes them straightforward to distinguish. Example 2. For an elegant representation of Example 2, the most convenient method is to apply a slightly more complex non-linearity for neighborhood aggregation; this allows a very simple representation for everything else in the GNN. In particular, let us again assume that each node simply starts with an integer 1 as a feature (i.e. not even aware of its degree initially). Furthermore, assume that neighborhood aggregation happens with a simple sum operator; however, after this, we use a more sophisticated non-linearity ˆσwhich ensures ˆσ(2) = 1 , and ˆσ(x) = 0 for all other integers x. One can easily implement this function with a 2-layer MLP: we can use x1 = σ(x−1) and x2 = σ(−x+ 3) as two nodes in the ﬁrst layer, and then combine them with a single node σ(x1 + x2 −1) as the second layer. Finally, for the UPDATE function which merges the aggregated neighborhood xN(u) with the node’s own embeddingxu, let us select σ(xN(u) + xu −2). The resulting GNN can be described rather easily. Each node begins with a feature of 1, and has an embedding of either 0 or 1 in any subsequent round. The update rule for the embedding is also simple: if u’s own value is1 and uhas exactly 2 neighbors with a value of 1, then the embedding of uwill remain 1; in any other case, u’s embedding is set to0, and it will remain 0 forever. In case of dropouts, this GNN will behave differently in the two graphs of Example2. Note that in both cases, whenever the connected component containing node uis not a cycle after the dropouts, then in at most d= 3 rounds, the embedding of uis set to 0. On the other hand, if the component containing uis a cycle, then the embedding of uwill remain 1 after any number of rounds. Now let udenote one of the nodes with degree 3 in both graphs. In the left-hand graph, there is a 1-dropout (of the other gray node) that puts uin a cycle, so uwill produce a ﬁnal embedding of 1 relatively frequently. Besides this, there are also 2 distinct 2-dropouts and a 3-dropout that removes the other gray node but keeps the triangle containinguintact; these will all result in a ﬁnal embedding of 1 for u. On the other hand, in the right-hand graph, there are only 2 distinct 2-dropouts which result in a single cycle containing u. 14This means that the probability of getting a ﬁnal value of 1 is signiﬁcantly higher in the left graph. In particular, after rruns, the difference in the expected frequency of getting a 1 is at least Ω(p·r), so we can easily separate the two cases by executing run aggregation with sum or mean. Example 3. The base idea of this separation has already been outlined in Section 3.4: assume that the middle node uuses a simple mean aggregation of its neighbors, and the dropout probability is p= 1 4 . Since we are now interested in the behavior of a speciﬁc step of mean aggregation, we only study the GNN for d= 1 rounds. With p= 1 4 , the left-hand graph provides the following distribution of means in a DropGNN: Pr(0) = (3 4 )2 and Pr (1) = Pr(−1) = 1 4 ·3 4 . As such, the probability of obtaining a 1 is about 0.19. Note that we disregarded the case when all neighbors of uare removed, but we could assume for convenience that e.g. themean function also returns 0 in this case. Furthermore, we only considered the cases when uis not removed, since these are the only runs when ucomputes an embedding at all. On the other hand, in the right-hand graph, uobtains the following distribution: Pr(0) = (3 4 )4 + 4 · (1 4 )2 · (3 4 )2 , Pr (1 3 ) = Pr ( −1 3 ) = 2 ·1 4 · (3 4 )3 and Pr (1) = Pr(−1) = (1 4 )2 · (3 4 )2 + 2 · (1 4 )3 ·3 4 . This gives a probability of about 0.06 for the value 1. If we apply e.g. the transformation x→σ(x−0.5) on these values, then the embedding 1 is indeed signiﬁcantly more frequent in the left-hand graph. Using either mean or sum for run aggregation allows us to separate the two cases: the ﬁnal embeddings in the two graphs will converge to 0.19 and 0.06 (both multiplied by rin case of sum). Alternative dropout methods. Throughout the paper, we consider a natural and straightforward version of the dropout idea: some nodes of the graph (and their incident edges) are removed for an entire run. However, we note that there are also several alternative ways to implement this dropout approach. For example, one could remove edges instead of nodes, or one could remove nodes in an asymmetrical manner (e.g., they still receive, but do not send messages). We point out that all these examples from Section 3.4. could also be distinguished under these alternative models. B Required number of runs We now discuss the proofs of Theorems 1 and 2. Note that for any speciﬁc subset Sof size k, the probability of this k-dropout happening in a speciﬁc run is pk ·(1 −p)γ+1−k = ( 1 1+γ )k · ( γ 1+γ )γ+1−k . To obtain the expected frequency EXS of this k-dropout after rruns, we simply have to multiply this expression by r. Furthermore, given a constant δ∈[0,1], a Chernoff bound shows that the probability of signiﬁcantly deviating from this expected value is Pr ( XS /∈[ (1−δ) ·EXS, (1+δ) ·EXS] ) ≤2 ·e−δ2·EXS 3 . Let us consider the case of Theorem 1 ﬁrst. Since we have γdifferent 1-dropouts, we can use a union bound over these dropouts to upper bound the probability of the event that any of the nodes v∈Γ will have Xv /∈[ (1−δ) ·E1, (1+δ) ·E1 ]; the probability of this event is at most 2 ·γ·e−δ2·E1 3 . 15If we ensure that this probability is at most 1 t, then the desired property follows. Note that after taking a (natural) logarithm of both sides, this is equivalent to log(2 ·γ) −δ2 ·E1 3 ≤ −log t, and thus E1 ≥ 3 δ2 ·log(2 ·γ·t) . Recall that for E1 we have E1 = 1 1 + γ · ( γ 1 + γ )γ ·r≥ 1 1 + γ ·1 e ·r. Due to this lower bound, it is sufﬁcient to ensure 1 1 + γ ·1 e ·r ≥ 3 δ2 ·log(2 ·γ·t) , that is, r ≥ 3e δ2 ·(γ+ 1) ·log(2 ·γ·t) = Ω(γ·log γt) . This completes the proof of Theorem 1. For Theorem 2, we also need to upper bound the probability of each dropout combination of multiple nodes. Consider k-dropouts for a speciﬁc k. In this case, we have EXS = ( 1 1 + γ )k · ( γ 1 + γ )γ+1−k ·r= 1 γk−1 ·E1 . This implies that in order to ensure XS < (1 −δ) ·E1 in Theorem 2, it is sufﬁcient to have XS < (1 −δ) ·γk−1 ·EXS. If we want to express this as (1 + ϵ) ·EXS for some ϵ, then we get ϵ = (1 −δ) ·γk−1 −1, and thus ϵ = Θ ( γk−1) for appropriately chosen constants. Applying a Chernoff bound (in this case, a different variant that also allows ϵ> 1) then gives Pr ( XS ≥(1+ϵ) ·EXS) ≤e−ϵ2·EXS 2+ϵ . Since ϵ= Θ ( γk−1) and EXS = γ−(k−1) ·E1, this is in fact e−Θ(1)·γk−1·γ−(k−1)·E1 = e−Θ(1)·E1 . Note that the number of differentk-dropouts is (γ k ) ≤2γ, so with a union bound, we can establish this property for each k-dropout simultaneously; for this, we need to multiply this error probability by 2γ. Finally, since we want to ensure this for all k≥2, we can take a union bound over k∈{2,3,...,γ }, getting another multiplier ofγ. Thus to obtain the second condition in Theorem 2 with error probability 1 t, we need γ·2γ ·e−Θ(1)·E1 ≤ 1 t . After taking a logarithm and reorganization, we get E1 ≥Θ(1) ·log(2γ ·γ·t) . With our lower bound for E1 and a reorganization of the right side, we can reduce this to r ≥Θ(1) ·(γ+ 1) ·γ·log(2 ·γ·t) = Ω ( γ2 + logγt ) . Another union bound shows that the two conditions of Theorem 2 also hold simultaneously when ris in this magnitude, thus completing the proof of Theorem 2. Note that if we want to ensure this property for the neighborhood of all the nnodes in the graph simultaneously, then we also have to take a union bound over all thennodes, which results in a factor of nwithin the logarithm in our ﬁnal bounds on r. 16Asymptotic analysis. Finally, let us note that from a strictly theoretical perspective, if we consider γto be a constant, and pto be some function of γ, then the probability of any speciﬁc k-dropout is pk ·(1 −p)γ+1−k, i.e. a constant value. As such, a Chernoff bound shows that if we select rto be a sufﬁciently large constant, then every possible dropout combination is observed, and their frequencies are reasonable close to the expected values. However, this approach is clearly not realistic in practice: e.g. for our choice of p ≈γ−1, the probability of a speciﬁc k-dropout is less than pk ≈γ−k. This means that we need r≥γk runs even to observe this k-dropout at least once in expectation. While this γk is, asymptotically speaking, only a constant value, it still induces a very large overhead in practice, even for relatively smallkand γ values. Different γand pvalues. Note that our choice of γwas deﬁned for an arbitrary node of the graph; however, the dropout probability p, chosen as a function of γ, is a global parameter of DropGNNs. As such, our choice of pfrom the analysis only works well if we assume that the graph is relatively homogeneous, i.e. γis similar for every node. In practice, one can simply apply the average or the maximum of these different γvalues; a slightly smaller/larger than optimal ponly means that we observe some dropouts with slightly lower probabil- ity, or we execute slightly more runs than necessary. The ablation studies in Figures 4 and 5 also show that our approach is generally robust to different number of runs and different dropout probabilities. We note, however, that if e.g. the graph consists of several different but separately homogeneous regions, then a more sophisticated approach could apply a different pvalue in each of these regions. C Expressiveness with sum aggregation We now discuss our claims on DropGNNs with sum neighborhood aggregation. Recall that with this aggregation method, a GNN with injective functions (such as GIN) has the same expressive power as the WL-test. Note that in this setting, we understand a d-hop neighborhood around uto refer to the part of the graph that ucan observe in drounds of message passing. In particular, this contains (i) all nodes that are at most dhops away from u, and (ii) all the edges induced by these nodes, except for the edges where both endpoints are exactly at distance dfrom u. C.1 Proof of Theorem 3 To prove Theorem 3, we show two different d-neighborhoods around a node u(for d = 2) that are non-isomorphic, but they generate the exact same distribution of observations for uif we only consider the case of k-dropouts for k≤2. Note that the example graphs on Figure 3 already provide an example where the 0-dropout and the 1-dropouts are identical. One can easily check this from the ﬁgure: in case of no dropouts, uobserves the same tree representation in d= 2 steps, and in case of any of the 6 possible 1-dropouts (in either of the graphs), uobserves the tree structure shown on the right side of the ﬁgure. To also extend this example to the case of 2-dropouts, we need to slightly change it. Note that the example graph is essentially constructed in the following way: we take two independent cycles of length 3 in one case, and a single cycle of length 6 in the other case, and in both graphs, we connect all these nodes to an extra node u. This construction is easy to generalize to larger cycle lengths. In particular, let us consider an integer ℓ≥3, and create the following two graphs: in one of them, we take two independent cycles of length ℓ, and connect each node to an extra node u, while in the other one, we take a single cycle of length 2 ·ℓ, and connect each node to an extra node u. We claim that with a choice of ℓ= 5, this construction sufﬁces for Theorem 3. As before, one can easily verify that uobserves the same 2-hop neighborhood in case of no dropouts, and also identical 2-hop neighborhoods for any of the 10 possible 1-dropouts in both graphs. The latter essentially has the same structure as the right-hand tree in Figure 3, except for the fact that the number of degree-3 branches (i.e. the ones on the left side of uin the ﬁgure) is now 7 instead of 3. 17It only remains to analyze the distribution of 2-dropouts. For this, note that the only information that ucan gather in d= 2 rounds is the multiset of degrees of its neighbors. In practice, this will depend on the distance of the two removed nodes in the cycles; in particular, we can have the following cases: 1. If the two nodes are neighbors in (one of) the cycle(s), then due to the dropouts, uwill have two neighbors of degree 2, and six neighbors of degree 3. There are 2 ·ℓ= 10 possible cases to have this dropout combination in both graphs. 2. If the two nodes are at distance 2 in (one of) the cycle(s), then u will have a single neighbor of degree 1, two neighbors of degree 2 and ﬁve neighbors of degree 3. This can again happen in 2 ·ℓ= 10 different ways in both graphs. 3. If the nodes have distance at least 3 within the same cycle, or they are in different cycles, then the dropout creates four neighbors of degree 2, and four neighbors of degree 3. In the 2 ·ℓcycle, this can happen in 2·ℓ·(2·ℓ−5) 2 = 2 ·ℓ2 −5 ·ℓ= 25 different ways. In case of the two distinct ℓ-cycles, this cannot happen in a single cycle at all (i.e. for general ℓ, it can happen in ℓ·(ℓ−5) 2 ways, but this equals to 0 for ℓ= 5); however, it can still happen if the two dropouts happen in different cycles, in ℓ·ℓ= 25 different ways. Hence the distribution of observed neighborhoods is also identical in case of 2-dropouts. C.2 Proof of Theorem 4 The setting of Theorem 4 considers GNNs with port numbers (such as CPNGNN) where the neigh- borhood aggregation function is not permutation invariant, i.e. it can produce a different result for a different ordering of the inputs (neighbors) [28]. Our proof of the theorem already builds on the fact that one can extend the idea of injective GNNs (such as GIN in [37]) to this setting with port numbers. To show that port numbers can be combined with the injective property, one can e.g. apply the same proof approach as in [37], using the fact that the possible combinations of embeddings and port numbers is still a countable set. Given such an injective GNN with port numbers, the expressiveness of this GNN is once again identi- cal to that of a general distributed algorithm in the message passing model with port numbers [28]. As such, it sufﬁces to show that a distributed algorithm in this model can separate any two different d-hop neighborhoods. Let us assume the 1-complete setting of Theorem 1, i.e. that we have sufﬁciently many runs to ensure that each 1-dropout is observed at least once in the d-hop neighborhood of u. We show that the set of neighborhoods observed this way is sufﬁcient to separate any two neighborhoods, regardless of the frequency of multi-dropout cases. The general idea of the proof is that 1-dropouts are already sufﬁcient to recognize when two nodes in the tree representation of u’s neighborhood are actually corresponding to the same node. Consider three nodes v1, v2 and v3, and assume that edges (v1,v3) and (v2,v3) are both within the d-hop neighborhood of u. More speciﬁcally, assume that v1’s port number b1 leads to v3, and v2’s port number b2 also leads to v3; then we can observe that the nodes at the endpoints of these two edges are always missing from the graph at the same time. That is, since we are guaranteed to observe every 1-dropout at least once, if neighbor b1 of v1 and neighbor b2 of v2 are distinct nodes, then we must observe at least one neighborhood variant where only one of these two neighbors are missing; in this case, we know that the b1th neighbor of v1 and the b2th neighbor of v2 are not identical. On the other hand, if the two neighbors are always absent simultaneously, then the two edges lead to the same node. The proof of the theorem happens in an inductive fashion. Note that from the 0-dropout, we can already identify the degree of uin the graph, and the port leading to each of its neighbors; this is exactly the 1-hop neighborhood of u. Now let us assume that we have already reconstructed the (i−1)-hop neighborhood of u; in this case, we can identify each outgoing edge from this neighborhood by a combination of a boundary node (a node at distance (i−1) from u) and a port number at this node. We can then extend our graph into the i-hop neighborhood of u(for i≤d) with the following two steps: 181. First, we reconstruct the edges going from distance (i−1) nodes to distance inodes. Let us refer to nodes at distance ias outer nodes. Note that all the outer neighbors of the boundary nodes can be identiﬁed by the speciﬁc outgoing edges from the boundary nodes; we only have to ﬁnd out which of these outer nodes are actually the same. This can be done with the general idea outlined before: if two boundary nodes v1 and v2 have a neighbor at ports b1 and b2, respectively, and we do not observe a graph variant where only one of these neighbors is missing, then the two edges lead to the same outer node. 2. We also need to reconstruct the adjacencies between the boundary nodes; this is part of the i-hop neighborhood of uby deﬁnition, but not part of the (i−1)-hop neighborhood. This happens with the same general idea as before: assume that v2 and v3 are both nodes at distance (i−1), and v1 is a node at distance (i−2) that is adjacent to v3. Then we can check whether v3 disappears simultaneously from the respective ports b1 and b2 of nodes v1 and v2; if it does, then we know that edge b2 of node v2 leads to this other boundary node v3. After dsteps, this process allows us to reconstruct the entire d-hop neighborhood of u, thus proving the theorem. Let us also brieﬂy comment on the GNN interpretation of this graph algorithm. An injective GNN construction ensures that we map differentd-hop neighborhoods to a different real number embedding. Note that the algorithm can separate any two neighborhoods without using the frequency of the speciﬁc neighborhoods variants; this implies that the set of real numbers obtained is different for any two neighborhoods, i.e. there must exist a number z∈R that is present in one of the distributions, but not in the other. One can then develop an MLP that essentially acts as an indicator for this value z, only outputting 1 if the input is z; this allows us to separate the two neighborhoods. Finally, note that our main objective throughout the paper was to compute a different embedding for two different neighborhoods. However, in this setting of Theorem 4, it is also possible to encounter the opposite problem: if two d-hop neighborhoods are actually isomorphic, but they have a different assignment of port numbers, then they might produce a different embedding in the end. We point out that with more sophisticated run aggregation, it is also possible to solve this problem, i.e. to recognize the same neighborhood regardless of the chosen port numbering. In particular, we have seen that in the 1-complete case, the multiset of ﬁnal embeddings already determines the entire neighborhood around u, and thus also its isomorphism class. This means that there is a well-deﬁned function from the embedding vectors in Rr that we can obtain in rruns to the possible isomorphism classes of u’s neighborhood (assuming for convenience that the neighborhood size is bounded). Due to the universal approximation theorem, a sufﬁciently complex MLP can indeed implement this function; as such, determining the isomorphism class of u’s neighborhood is indeed within the expressive capabilities of DropGNNs in this setting. However, while such a solution exists in theory, we note that this graph isomorphism problem is known to be rather challenging in practice. C.3 Brieﬂy on the graph reconstruction problem The graph reconstruction problem is a well-known open question dating back to the 1940s. Assume that there is a hidden graph Gon n≥3 nodes that we are unaware of; instead, what we receive as an input is ndifferent modiﬁed variants of G, each obtained by removing a different node (and its incident edges) from G. This input multiset of graphs is often called the deck of G. Note that the graphs in the deck are only provided up to an isomorphism class, i.e. for a speciﬁc node of the deck graph, we do not know which original node of Git corresponds to. The goal is to identify Gfrom its deck; this problem is solvable exactly if there are no two non-isomorphic graphs with the same deck. This assumption is known as the graph reconstruction conjecture [14]. This problem is clearly close to our task of reconstructing a neighborhood from its1-dropout variants; however, there are also two key differences between the settings. Firstly, in our DropGNNs, we do not observe a graph, but rather a tree-representation of its neighborhood where some nodes may appear multiple times. In this sense, our GNN setting is much more challenging than the reconstruction problem, since it is highly non-trivial to decide whether two nodes in this tree representation correspond to the same original node. On the other hand, the DropGNN setting has the advantage that we can also observe the 0-dropout; this does not happen in the reconstruction problem, since it would correspond to directly receiving the solution besides the deck. 19D Dropouts with mean or max aggregation In this section, we discuss the expressiveness of the dropout technique with mean and max neighbor- hood aggregation. In particular, we prove that separation is always possible withmean aggregation when |S1|= |S2|, we construct a pair of neighborhoods that provide a very similar distribution of mean values, and we brieﬂy discuss the limits of max aggregation in practice. D.1 Proof of Lemma 1 We begin with the proof of Lemma 1. More speciﬁcally, we show that if |S1|= |S2|, then there always exists a choice of pand integers a,b such that after applying an activation function σ(ax+ b) on S1 and S2, a mean neighborhood aggregation allows us to distinguish the two sets. In our proof, we assume that S1 and S2 are both multisets of integers (instead of vectors), i.e. that node features are only 1-dimensional. With multi-dimensional feature vectors, we can apply the same proof to each dimension of the vectors individually; since S1 ̸= S2, we will always have a dimension that allows us to separate the two multisets with the same method. Let s1 denote the mean of S1 and s2 denote the mean of S2. We ﬁrst discuss the simpler case when s1 ̸= s2; if this holds, we can distinguish any two sets S1 and S2, so we make this proof for the general case, without the assumption that |S1|= |S2|. After this, we discuss the case when s1 = s2 and |S1|= |S2|; this completes the proof of Lemma 1. The main idea of the proofs is to ﬁnd a threshold τ such that in S1, we have mean values larger than τ much more frequently than in S2 (or vice versa). We can then use an activation function ˆσ(x) := σ(x−τ) (with σdenoting the Heaviside step function) to ensure that σ(x) = 1 if x≥τ, and σ(x) = 0 otherwise. This means that a run aggregation with sum will simply count the cases when the mean is larger than τ, and thus with high probability, we get a signiﬁcantly different sum in case of S1 and S2. Note that even though the proof is described with a Heaviside activation function for ease of presentation, one could also use the logistic function (a more popular choice in practice), since the logistic function provides an arbitrary close approximation of the step function with the appropriate parameters. When the means are different. First we consider the case when s1 ̸= s2. In this setting, ﬁnding an appropriate τ is relatively straightforward. Assume w.l.o.g. thats1 <s2, and let us choose an arbitrary τ such that s1 < τ <s2. This implies that whenever no node is removed, then the mean in S1 will produce a 0, while the mean in S2 will produce a 1. It only remains to ensure that 0-dropouts are frequent enough to distinguish these two cases. For this, let γ = max(|S1|,|S2|), and let us select p= 1 2γ. For both S1 and S2, this gives a probability of at least (1 −p)γ = (2γ−1 2γ )γ for 0-dropouts. When γ ≥2, this probability is strictly larger than 0.55. With a Chernoff bound, one can also show that the number of 0-dropouts is strictly concentrated around this value: with δ= 0.05 and rruns, the probability of the number of 0-dropouts being below (1 −δ) ·0.55 ≈0.52 is upper bounded by e−1 3 ·δ2·0.55·r. To ensure that this is below 1 t, we only need Θ(1) ·r ≥log t, and hence r ≥Ω(log t). This already ensures that in case of S2, we have at least 0.52 ·rruns that produce a 1, while in S1, we have at least 0.52 ·rruns that produce a 0 (i.e. at most 0.48 ·rruns that produce a 1). Hence with high probability, a sum run aggregation gives a sum below 0.48 ·rand above 0.52 ·rfor S1 and S2 respectively, so the two cases are indeed separable. When the means are the same. Now consider the case when s1 = s2, and we have |S1|= |S2|. In this setting, let γ = |S1|= |S2|. Since the multisets are not identical, there must be an index i∈{1,...,γ }such that in the sorted version of the multisets, the ith element of S1 is different from the ith element of S2. Let us consider the smallest such index i, and assume w.l.o.g. that the ith 20element of S1 (let us call it x1,i) is larger than the ith element of S2 (denoted by x2,i). Furthermore, Let s1,−i and s2,−i denote the mean of S1 and S2, respectively, after removing the ith element. Note that if we only had 1-dropouts and 0-dropouts in our GNNs, then ﬁnding this index iwould already allow a separation in a relatively straightforward way. Since x1,i > x2,i, we must have s1,−i <s2,−i. The idea is again to select a threshold value τ such that s1,−i < τ <s2,−i. This ensures that in S1, at least i of the 1-dropouts produce a 0, whereas in S2, at most i−1 of the 1-dropouts produce a 0. If the frequency of all 1-dropouts is concentrated around its expectation, then this shows that the occurrences of 1 will be signiﬁcantly higher in S2. What makes this argument slightly more technical is the presence of k-dropouts for k≥2. In order to reduce the relevance of these cases, we select a smaller pvalue. In particular, let p= 1 2γ2 . In this case, the probability of a k-dropout is only pk ·(1 −p)γ−k ≤pk = 1 2k ·γ2k , and the probability of having any multiple-dropout case in a speciﬁc run is at most γ∑ k=2 (γ k ) · 1 2k ·γ2k ≤ γ∑ k=2 γk 2 · 1 2k ·γ2k ≤ γ∑ k=2 1 2k+1 · 1 γk ≤ 1 4 ·γ2 , using the fact that (γ k ) ≤1 2 ·γk for k≥2 and the fact that 1 8 + 1 16 + ...≤1 4 . On the other hand, the probability of a 1-dropout is p·(1 −p)γ−1 = 1 2γ2 · (2γ2 −1 2γ2 )γ−1 , where one can observe that the second factor is at least 7 8 for any positive integer γ. As such, the probability of a 1-dropout is lower bounded by 7 16 · 1 γ2 , i.e. it is notably larger than the cumulative probability of multiple-dropout cases. This means that our previous choice of s1,−i < τ <s2,−i also sufﬁces for this general case. In particular, even if all the multiple-dropouts in S1 produce a mean that is larger than τ, and all the multiple-dropouts in S2 produce a mean that is smaller thanτ, we will still end up with a considerably larger probability of obtaining a value of1 in case of S2, due to the 1-dropout of the ith element. More speciﬁcally, the difference between the two probabilities will be at least 3 16 · 1 γ2 ; using a Chernoff bound in a similar fashion to before, one can conclude that Ω(γ4 ·log t) runs are already sufﬁcient to separate the two case with error probability at most 1 t. D.2 Construction for similar mean distribution Let us now comment on the general case when we have s1 = s2 but |S1|̸= |S2|. We present an example for two different sets S1 and S2 where the distribution of mean values obtained from 0- and 1-dropouts is essentially identical, thus showing the limits of any general approach that uses mean aggregation, but does not execute a deeper analysis of k-dropouts for k≥2. Consider an even integer ℓ, and consider the following two subsets. LetS1 consist of ℓ 2 distinct copies of the number −(ℓ−1), and ℓ 2 distinct copies of the number (ℓ−1). Let S2 consist of ℓ 2 distinct copies of the number −ℓ, and ℓ 2 distinct copies of the number ℓ, and a single instance of 0. These sets provide |S1|= ℓand |S2|= ℓ+ 1, and also s1 = s2 = 0. For a concrete example of ℓ= 4, we get the multisets S1 = {−3,−3,3,3}and S2 = {−4,−4,0,4,4}. The mean values obtained for 1-dropouts is also easy to compute in these examples. In S1, we have ℓ 2 distinct 1-dropouts with a mean of 1, and ℓ 2 distinct 1-dropouts with a mean of −1. In S2, we have ℓ 2 distinct 1-dropouts with a mean of 1, and ℓ 2 distinct 1-dropouts with a mean of −1, and a single 1-dropout with a mean of 0. Note that if we only consider these 0 and 1-dropouts, then the probability of getting a 0 is exactly the same in both settings. In S1, this comes from the probability of the 0-dropout only, so it is 21(1 −p)ℓ. In S2, we have to add up the probability of the 0-dropout and a single 1-dropout: this is (1 −p)ℓ+1 + p·(1 −p)ℓ = (1 −p)ℓ. The set of means obtained from 1-dropouts is also identical in the two neighborhoods, it is only their probability that is slightly different. InS1, both −1 and 1 are obtained with probability ℓ 2 ·p·(1−p)ℓ−1, while in S2, they are both obtained with probability ℓ 2 ·p·(1 −p)ℓ. Hence the difference between the two probabilities is only ℓ 2 ·p· ( (1 −p)ℓ−1 −(1 −p)ℓ) = ℓ 2 ·p2 ·(1 −p)ℓ−1 . Recall that we have Θ(ℓ2) distinct 2-dropouts, each with a probability of p2 ·(1 −p)ℓ−1, so these 2-dropouts are together easily able to bridge this difference of frequency of the 1-dropouts between S1 and S2. This shows that we cannot conveniently ignore multiple-node dropouts as in case of |S1|= |S2|before: the only possible 1-dropout-based approach to separate the two sets (i.e. to use the slightly different frequency of the values −1 and 1) is not viable without a deeper analysis of the distributions of 2-dropouts. It is beyond the scope of this paper to analyze this distribution in detail, or to come up with more sophisticated separation methods based on multiple-node dropouts. D.3 Aggregation with max Another well-known permutation-invariant function (and thus a natural candidate for neighborhood aggregation) is max; however, this method does not combine well with the dropout approach in practice. In particular, if the multisets S1 and S2 only differ in their smallest element, then max aggregation can only distinguish them from a speciﬁc (γ−1)-dropout when all other neighbors of uare removed. This dropout combination only has a probability of pγ−1 ·(1 −p)2; thus for a reasonably small p (e.g. for p≈γ−1), we need a very high number of runs to observe this case with a decent probability. E Details of the experimental setup In all of our experiments, we use Adam optimizer [17]. For synthetic benchmarks and graph clas- siﬁcation, we use a learning rate of 0.01, for graph property regression we use a learning rate of 0.001. For graph classiﬁcation benchmarks we decay the learning rate by 0.5 every 50 steps [ 37] and for the graph regression benchmark we decay the learning rate by a factor of 0.7 on plateau [22]. The GIN model always uses 2-layer multilayer perceptrons and batch normalization [16] after each level [37]. For our dropout technique, during preliminary experiments we tested three different node dropout implementation options: i) completely removing the dropped nodes and their edges from the graph; ii) replacing dropped node features by 0s before and after each graph convolution; iii) replacing the initial dropped node features by 0s. These preliminary experiments showed that all of these options performed similarly in practice, but the last option resulted in a more stable training. Since it is also the simplest dropout version to implement we chose to use it in all of our experiments. To ensure that the base model is well trained, when our technique is used we apply an auxiliary loss on each run individually. This auxiliary loss comprises 1 3 of the ﬁnal loss. While our model can have O(n) memory consumption if we execute the runs in sequence, we implement it in a paralleled manner, which reduces the compute time, as all rruns are performed in parallel, but increases memory consumption. For the synthetic benchmarks (LIMITS 1, LIMITS 2, 4-C YCLES , LCC, TRIANGLES , SKIP -CIRCLES ) we use a GIN model with 4 convolutional layers (+ 1 input layer), sum as aggregation, ε= 0 and for simplicity do not use dropout on the ﬁnal READOUT layer, while the ﬁnal layer dropout is treated as a hyper-parameter in the original model. For synthetic node classiﬁcation tasks (LIMITS 1, LIMITS 2, LCC , and TRIANGLES ) we use the same readout head as the original GIN model but skip the graph aggregation step. In all cases, except the SKIP CIRCLES dataset, 16 hidden units are used for synthetic tasks. For the SKIP CIRCLES dataset we use a GIN model with 9 convolutional layers (+ 1 input layer) with 32 hidden units as this dataset has cycles of up to 17 hops and requires long-range information propagation to solve the task. For the DropGIN variant, mean aggregation is used to aggregate node representations from different runs. When the GIN model is augmented with ports, which introduce edge features, we use modiﬁed GIN convolutions that include edge features [15]. In synthetic benchmarks, we always generate the same number of graphs for training and test sets 22(generate a new copy of the dataset for testing) and for each random seed, we re-generate the datasets. We always feed in the whole dataset as one batch. LIMITS 1, LIMITS 2 and SKIP -CIRCLES datasets are always comprised of graphs with the same structure, just with permuted node IDs for each dataset initialization, the remaining datasets have random graph structure, which changes when the datasets are regenerated. You can see the synthetic dataset structure type and statistics in Table 4. All nodes in these datasets have the same degree. Dataset Number of graphs Number of nodes Degree Structure Task LIMITS1 [11] 2 16 2 Fixed Node classiﬁcation LIMITS2 [11] 2 16 3 Fixed Node classiﬁcation 4-CYCLES[20] 50 16 2 Random Graph classiﬁcation LCC [29] 6 10 3 Random Node classiﬁcation TRIANGLES[29] 1 60 3 Random Node classiﬁcation SKIP-CIRCLES[6] 10 41 4 Fixed Graph classiﬁcation Table 4: Synthetic dataset statistics and properties. For graph classiﬁcation tasks we use exactly the same GIN model as described originally and apply our dropout technique on top. Namely, with 1 input layer, 4 convolution layers withsum as aggregation and ε= 0 and dropout [32] on the ﬁnal READOUT layer. For the DropGIN variant, mean aggregation is used to pool node representations from different runs. Note, that in our setting sum and mean aggregations are equivalent, up to a constant multiplicative factor, as the number of runs is a constant chosen on a per dataset level. We use exactly the same model training and selection procedure as described by [37]. We decay the learning rate by 0.5 every 50 epochs and tune the number of hidden units ∈{16,32}for bioinformatics datasets while using 64 for the social graphs. The dropout ratio ∈{0,0.5}after the ﬁnal dense layer the batch size ∈{32,128}are also tuned. The epoch with the best cross-validation accuracy over the 10 folds is selected. You can see the statistics of synthetic datasets in Table 5. Number of nodes Degree Dataset Number of graphs Min Max Mean Min Max Mean MUTAG 188 10 28 18 3 4 3.01 PTC 344 2 64 14 1 4 3.18 PROTEINS 1109 4 336 38 3 12 5.78 IMDB-B 996 12 69 19 11 68 18.49 IMDB-M 1498 7 63 13 6 62 11.91 QM9 130 831 3 29 18 2 5 3.97 Table 5: Real-world dataset statistics. For the graph property regression task ( QM9 ) we augment two models: 1-GNN [ 22] and MPNN [12]. For 1-GNN we use the code and the training setup as provided by the original authors 2. For MPNN we use the reference model implementation from PyTorch Geometric 3. We otherwise follow the training and evaluation procedure used by 1-GNN [22]. The models are trained for 300 epochs and the epoch with the best validation score is chosen. We use PyTorch [24] and PyTorch Geometric [10] for the implementation. All models have been trained on Nvidia Titan RTX GPU (24GB RAM). 2https://github.com/chrsmrrs/k-gnn 3https://github.com/rusty1s/pytorch_geometric/blob/master/examples/qm9_nn_conv.py 23",
      "meta_data": {
        "arxiv_id": "2111.06283v1",
        "authors": [
          "Pál András Papp",
          "Karolis Martinkus",
          "Lukas Faber",
          "Roger Wattenhofer"
        ],
        "published_date": "2021-11-11T15:48:59Z",
        "pdf_url": "https://arxiv.org/pdf/2111.06283v1.pdf"
      }
    },
    {
      "title": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks",
      "abstract": "This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach\nthat aims to overcome the limitations of standard GNN frameworks. In DropGNNs,\nwe execute multiple runs of a GNN on the input graph, with some of the nodes\nrandomly and independently dropped in each of these runs. Then, we combine the\nresults of these runs to obtain the final result. We prove that DropGNNs can\ndistinguish various graph neighborhoods that cannot be separated by message\npassing GNNs. We derive theoretical bounds for the number of runs required to\nensure a reliable distribution of dropouts, and we prove several properties\nregarding the expressive capabilities and limits of DropGNNs. We experimentally\nvalidate our theoretical findings on expressiveness. Furthermore, we show that\nDropGNNs perform competitively on established GNN benchmarks.",
      "full_text": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks Pál András Papp ETH Zurich apapp@ethz.ch Karolis Martinkus ETH Zurich martinkus@ethz.ch Lukas Faber ETH Zurich lfaber@ethz.ch Roger Wattenhofer ETH Zurich wattenhofer@ethz.ch Abstract This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNNs, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these runs to obtain the ﬁnal result. We prove that DropGNNs can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. We derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and we prove several properties regarding the expressive capabilities and limits of DropGNNs. We experimentally validate our theoretical ﬁndings on expressiveness. Furthermore, we show that DropGNNs perform competitively on established GNN benchmarks. 1 Introduction Neural networks have been successful in handling various forms of data. Since some of the world’s most interesting data is represented by graphs, Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various ﬁelds such as quantum chemistry, physics, or social networks [12; 27; 18]. On the other hand, GNNs are also known to have severe limitations and are sometimes unable to recognize even simple graph structures. In this paper, we present a new approach to increase the expressiveness of GNNs, called Dropout Graph Neural Networks (DropGNNs). Our main idea is to execute not one but multiple different runs of the GNN. We then aggregate the results from these different runs into a ﬁnal result. In each of these runs, we remove (“drop out”) each node in the graph with a small probability p. As such, the different runs of an episode will allow us to not only observe the actual extended neighborhood of a node for some number of layers d, but rather to observe various slightly perturbed versions of this d-hop neighborhood. We emphasize that this notion of dropouts is very different from the popular dropout regularization method; in particular, DropGNNs remove nodes during both training and testing, since their goal is to observe a similar distribution of dropout patterns during training and testing. This dropout technique increases the expressive power of our GNNs dramatically: even when two distinct d-hop neighborhoods cannot be distinguished by a standard GNN, their dropout variants (with a few nodes removed) are already separable by GNNs in most cases. Thus by learning to identify the dropout patterns where the two d-hop neighborhoods differ, DropGNNs can also distinguish a wide variety of cases that are beyond the theoretical limits of standard GNNs. Our contributions. We begin by showing several example graphs that are not distinguishable in the regular GNN setting but can be easily separated by DropGNNs. We then analyze the theoretical properties of DropGNNs in detail. We ﬁrst show that executing ˜O(γ) different runs is often already 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2111.06283v1  [cs.LG]  11 Nov 2021sufﬁcient to ensure that we observe a reasonable distribution of dropouts in a neighborhood of size γ. We then discuss the theoretical capabilities and limitations of DropGNNs in general, as well as the limits of the dropout approach when combined with speciﬁc aggregation methods. We validate our theoretical ﬁndings on established problems that are impossible to solve for standard GNNs. We ﬁnd that DropGNNs clearly outperform the competition on these datasets. We further show that DropGNNs have a competitive performance on several established graph benchmarks, and they provide particularly impressive results in applications where the graph structure is really a crucial factor. 2 Related Work GNNs apply deep learning to graph-structured data [30]. In GNNs, every node has an embedding that is shared over multiple iterations with its neighbors. This way nodes can gather their neighbors’ features. In recent years, many different models have been proposed to realize how the information between nodes is shared [35]. Some approaches take inspiration from convolution [23; 8; 13], others from graph spectra [18; 5], others from attention [33], and others extend previous ideas of established concepts such as skip connections [36]. Principally, GNNs are limited in their expressiveness by the Weisfeiler-Lehman test (WL-test) [37], a heuristic to the graph isomorphism problem. The work of [37] proposes a new architecture, Graph Isomoprhism Networks (GIN), that is proven to be exactly as powerful as the WL-test. However, even GINs cannot distinguish certain different graphs, namely those that the WL-test cannot distinguish. This ﬁnding [11] motivated more expressive GNN architectures. These improvements follow two main paths. The ﬁrst approach augments the features of nodes or edges by additional information to make nodes with similar neighborhoods distinguishable. Several kinds of information have been used: inspired from distributed computing are port numbers on edges [28], unique IDs for nodes [20], or random features on nodes [29; 1]. Another idea is to use angles between edges [19] from chemistry (where edges correspond to electron bonds). However, all of these approaches have some shortcomings. For ports and angles, there are some simple example graphs that still cannot be distinguished with these extensions [11]. Adding IDs or random features helps during training, but the learned models do not generalize: GNNs often tend to overﬁt to the speciﬁc random values in the training set, and as such, they produce weaker results on unseen test graphs that received different random values. In contrast to this, DropGNNs observe a similar distribution of embeddings during training and testing, and hence they also generalize well to test set graphs. The second approach exploits the fact that running the WL-test on tuples, triples, or generally k- tuples keeps increasing its expressiveness. Thus a GNN operating on tuples of nodes has higher expressiveness than a standard GNN [22; 21]. However, the downside of this approach is that even building a second-order graph blows up the graph quadratically. The computational cost quickly becomes a problem that needs to be to addressed, for example with sampling [ 22]. Furthermore, second-order graph creation is a global operation of the graph that destroys the local semantics induced by the edges. In contrast to this, DropGNN can reason about graphs beyond the WL-test with only a small overhead (through run repetition), while also keeping the local graph structure intact. Our work is also somewhat similar to the randomized smoothing approach [7], which has also been extended to GNNs recently [ 4]. This approach also conducts multiple runs on slightly perturbed variants of the data. However, in randomized smoothing, the different embeddings are combined in a smoothing operation (e.g. majority voting), which speciﬁcally aims to get rid of the atypical perturbed variants in order to increase robustness. In contrast to this, the main idea of DropGNNs is exactly to ﬁnd and identify these perturbed special cases which are notably different from the original neighborhood, since these allow us to distinguish graphs that otherwise seem identical. Finally, we note that removing nodes is a common tool for regularization in deep neural networks, which has also seen use in GNNs [26; 9]. However, as mentioned before, this is a different dropout concept where nodes are only removed during training to reduce the co-dependence of nodes. 2u u u u u Figure 1: Illustration of 4 possible dropout combinations from an example 2-hop neighborhood around u: a 0-dropout, two different 1-dropouts and a 2-dropout. 3 DropGNN 3.1 About GNNs Almost all GNN architectures [33; 18; 37; 8; 35; 13; 36] follow the message passing framework [12; 3]. Every node starts with an embedding given by its initial features. One round of message passing has three steps. In the ﬁrst MESSAGE step, nodes create a message based on their embedding and send this message to all neighbors. Second, nodes AGGREGATE all messages they receive. Third, every node UPDATE s its embedding based on its old embedding and the aggregated messages. One such round corresponds to one GNN layer. Usually, a GNN performs drounds of message passing for some small constant d. Thus, the node’s embedding in a GNN reﬂects its features and the information within its d-hop neighborhood. Finally, a READOUT method translates these ﬁnal embeddings into predictions. Usually, MESSAGE , AGGREGATE , UPDATE and READOUT are functions with learnable parameters, for instance linear layers with activation functions. This GNN paradigm is closely related to the WL-test for a pair of graphs, which is an iterative color reﬁnement procedure. In rounds 1,...,d , each node looks at its own color and the multiset of colors of its direct neighbors, and uses a hash function to select a new color based on this information. As such, if the WL-test cannot distinguish two graphs, then a standard GNN cannot distinguish them either: intuitively, the nodes in these graphs receive the same messages and create the same embedding in each round, and thus they always arrive at the same ﬁnal result. 3.2 Idea and motivation The main idea of DropGNNs is to execute multiple independent runs of the GNN during both training and testing. In each run, every node of the GNN is removed with probability p, independently from all other nodes. If a node vis removed during a run, then vdoes not send or receive any messages to/from its neighbors and does not affect the remaining nodes in any way. Essentially, the GNN behaves as if v (and its incident edges) were not present in the graph in the speciﬁc run, and no embedding is computed for vin this run (see Figure 1 for an illustration). Over the course of multiple runs, dropouts allow us to not only observe the d-hop neighborhood around any node u, but also several slightly perturbed variants of this d-hop neighborhood. In the different runs, the embedding computed forumight also slightly vary, depending on which node(s) are missing from its d-hop neighborhood in a speciﬁc run. This increases the expressive power of GNNs signiﬁcantly: even when two different d-hop neighborhoods cannot be distinguished by standard GNNs, the neighborhood variants observed when removing some of the nodes are usually still remarkably different. In Section 3.4, we discuss multiple examples for this improved expressiveness. Our randomized approach means that in different runs, we will have different nodes dropping out of the GNN. As such, the GNN is only guaranteed to produce the same node embeddings in two runs if we have exactly the same subset of nodes dropping out. Given the d-hop neighborhood of a node u, we will refer to a speciﬁc subset of nodes dropping out as a dropout combination, or more concretely as a k-dropout in case the subset has size k. In order to analyze the d-hop neighborhood of u, the reasonable strategy is to use a relatively small dropout probability p: this ensures that in each run, only a few nodes are removed (or none at all), and 3thus the GNN will operate on a d-hop neighborhood that is similar to the original neighborhood of u. As a result, 1-dropouts will be frequent, while for a larger k, observing a k-dropout will be unlikely. To reduce the effect of randomization on the ﬁnal outcome, we have to execute multiple independent runs of our GNN; we denote this number of runs by r. For a successful application of the dropout idea, we have to select rlarge enough to ensure that the set of observed dropout combinations is already reasonably close to the actual probability distribution of dropouts. In practice, this will not be feasible for k-dropouts with large kthat occur very rarely, but we can already ensure for a reasonably small rthat e.g. the frequency of each 1-dropout is relatively close to its expected value. 3.3 Run aggregation Recall that standard GNNs ﬁrst compute a ﬁnal embedding for each node through dlayers, and then they use a READOUT method to transform this into a prediction. In DropGNNs, we also need to introduce an extra phase between these two steps, called run aggregation. In particular, we execute rindependent runs of the d-layer GNN (with different dropouts), which altogether produces rdistinct ﬁnal embeddings for a node u. Hence we also need an extra step to merge these rdistinct embeddings into a single ﬁnal embedding of u, which then acts as the input for the READOUT function. This run aggregation method has to transform a multiset of embeddings into a single embedding; furthermore, it has to be a permutation-invariant function (similarly to neighborhood aggregation), since the ordering of different runs carries no meaning. We note that simply applying a popular permutation-invariant function for run aggregation, such as sum or max, is often not expressive enough to extract sufﬁcient information from the distribution of runs. Instead, one natural solution is to ﬁrst apply a transformation on each node embedding, and only execute sum aggregation afterward. For example, a simple transformation x →σ(Wx + b), where σdenotes a basic non-linearity such as a sigmoid or step function, is already sufﬁcient for almost all of our examples and theoretical results in the paper. 3.4 Motivational examples We discuss several examples to demonstrate how DropGNNs are more expressive than standard GNNs. We only outline the intuitive ideas behind the behavior of the DropGNNs here; however, in Appendix A, we also describe the concrete functions that can separate each pair of graphs. Example 1. Figure 2a shows a fundamental example of two different graphs that cannot be distin- guished by the 1-WL test, consisting of cycles of different length. This example is known to be hard for extended GNNs variants: the two cases cannot even be distinguished if we also use port numbers or angles between the edges [11]. The simplest solution here is to consider a GNN with d = 2 layers; this already provides a very different distribution of dropouts in the two graphs. For example, the8-cycle has 2 distinct 1-dropouts where uretains both of its direct neighbors, but it only has 1 neighbor at distance 2; such a situation is not possible in the 4-cycle at all. Alternatively, the4-cycle has a 1-dropout case with probability p·(1 −p)2 where uhas 2 direct neighbors, but no distance 2 neighbors at all; this only happens for a 2-dropout in the 8-cycle, i.e. with a probability of only p2 ·(1 −p)2. With appropriate weights, a GNN can learn to recognize these situations, and thus distinguish the two cases. Example 2. Figure 2b shows another example of two graphs that cannot be separated by a WL test; note that node features simply correspond to the degrees of the nodes. From an algorithmic perspective, it is not hard to distinguish the two graphs from speciﬁc 1-dropout cases. Let uand vdenote the two gray nodes in the graphs, and consider the process from u’s perspective. In both graphs, ucan recognize if vis removed in a run since udoes not receive a “gray” message in the ﬁrst round. However, the dropout of vhas a different effect in the two graphs later in the process: in the right-hand graph, it means that there is no gray neighbor at a 3-hop distance from u, while in the left-hand graph, uwill still see a gray node (itself) in a 3-hop distance. Thus by identifying the 1-dropout of v, an algorithm can distinguish the two graphs: if we observe runs where ureceives no gray message in the ﬁrst round, but it receives an (aggregated) gray message 4(a) (b) u u (c) Figure 2: Several example graphs which show that DropGNNs are more expressive than standard GNNs in various cases. Different node colors correspond to different node features. in the third round, then uhas the left-hand neighborhood. This also means that a sufﬁciently powerful GNN which is equivalent to the 1-WL test can also separate the two cases. Example 3. Note that using a sum function for neighborhood aggregation is often considered a superior choice to mean, since ucannot separate e.g. the two cases shown in Figure 2c with mean aggregation [37]. However, the mean aggregation of neighbors also has some advantages over sum; most notably, it the computed values do not increase with the degree of the node. We show that dropouts also increase the expressive power of GNNs with mean aggregation, thus possibly making mean aggregation a better choice in some applications. In particular, a DropGNN with mean aggregation is still able to separate the two cases on Figure 2c. Assume that the two colors in the ﬁgure correspond to feature values of 1 and −1, and let p= 1 4 . In the left-hand graph, there is a 1-dropout where uends up with a single neighbor of value 1; hence mean aggregation yields a value of 1 with probability 1 4 ·3 4 ≈0.19 in each run. However, in the right-hand graph, the only way to obtain a mean of 1 is through a 2-dropout or some 3-dropouts; one can calculate that the total probability of these is only 0.06 (see Appendix A). If we ﬁrst transform all other values to 0 (e.g. with σ(x−0.5), where σis a step function), then run aggregation with mean or sum can easily separate these cases. Note that if we apply a more complex transformation at run aggregation, then separation is even much easier, since e.g. the mean value of 0.33 can only appear in the right-hand graph. 4 Theoretical analysis 4.1 Required number of runs We analyze DropGNNs with respect to the neighborhood of interest around a node u, denoted by Γ. That is, we select a speciﬁc region around u, and we want to ensure that the distribution of dropout combinations in this region is reasonably close to the actual probabilities. This choice of Γ then determines the ideal choice of pand rin our DropGNN. One natural choice is to select Γ as the entire d-hop neighborhood of u, since a GNN will always compute its ﬁnal values based on this region of the graph. Note that even for this largest possible Γ, the size of this neighborhood γ := |Γ|does not necessarily scale with the entire graph. That is, input graphs in practice are often sparse, and we can e.g. assume that their node degrees are upper bounded by a constant; this is indeed realistic in many biological or chemical applications, and also a frequent assumptions in previous works [28]. In this case, having d= O(1) layers implies that γis also essentially a constant, regardless of the size of the graph. However, we point out that Γ can be freely chosen as a neighborhood of any speciﬁc size. That is, even if a GNN aggregates information within a distance of d = 5 layers, we can still select Γ to denote, for example, only the 2-hop neighborhood of u. The resulting DropGNN will still compute a ﬁnal node embedding based on the entire 5-hop neighborhood of u; however, our DropGNN will now only ensure that we observe a reasonable distribution of dropout combinations in the 2-hop neighborhood of u. In this sense, the size γ is essentially a trade-off hyperparameter: while a smaller γ will require a smaller number of runs runtil the distribution of dropout combinations stabilizes, a larger γallows us to observe more variations of the region around u. 51-complete dropouts. From a strictly theoretical perspective, choosing a sufﬁciently largeralways allows us to observe every possible dropout combination. However, since the number of combinations is exponential in γ, this approach is not viable in practice (see Appendix B for more details). To reasonably limit the number of necessary runs, we focus on the so-called 1-complete case: we want to have enough runs to ensure that at least every 1-dropout is observed a few times. Indeed, if we can observe each variant of Γ where a single node is removed, then this might already allow a sophisticated algorithm to reconstruct a range of useful properties of Γ. Note that in all of our examples, a speciﬁc 1-dropout was already sufﬁcient to distinguish the two cases. For any speciﬁc node v∈Γ, the probability of a 1-dropout for vis p·(1 −p)γ in a run (including the probability that uis not dropped out). We apply the pvalue that maximizes the probability of such a 1-dropout; a simple differentiation shows that this maximum is obtained at p∗ = 1 1+γ. This choice of palso implies that the probability of observing a speciﬁc 1-dropout in a run is 1 1 + γ · ( γ 1 + γ )γ ≥ 1 1 + γ ·1 e. Hence if we execute r≥e·(γ+ 1) = Ω(γ) runs, then the expected number of times we observe a speciﬁc 1-dropout (let us denote this by E1) is at least E1 ≥r·1 e · 1 1+γ ≥1. Moreover, one can use a Chernoff bound to show that afterΩ(γlog γ) runs, the frequency of each 1-dropout is sharply concentrated around E1. This also implies that we indeed observe each1-dropout at least once with high probability. For a more formal statement, let us consider a constantδ∈[0,1] and an error probability 1 t <1. Also, given a node v ∈Γ (or subset S ⊆Γ), let Xv (or XS) denote the number of times this 1-dropout (|S|-dropout) occurs during our runs. Theorem 1 If r ≥Ω (γlog γt), then with a probability of 1 −1 t, it holds that for each v ∈Γ, we have Xv ∈[ (1−δ) ·E1 , (1+δ) ·E1 ]. With slightly more runs, we can even ensure that each k-dropout for k≥2 happens less frequently than 1-dropouts. In this case, it already becomes possible to distinguish 1-dropouts from multiple- dropout cases based on their frequency. Theorem 2 If r≥Ω ( γ2 + γlog γt ) , then with a probability of 1 −1 t it holds that • for each v∈Γ, we have Xv ∈[ (1−δ) ·E1 , (1+δ) ·E1 ], • for each S ⊆Γ with |S|≥ 2, we have XS <(1−δ) ·E1. Since the number of all dropout combinations is in the magnitude of2γ, proving this bound is slightly more technical. We discuss the proofs of these theorems in Appendix B. Note that in sparse graphs, whereγis essentially a constant, the number of runs described in Theorems 1 and 2 is also essentially a constant; as such, DropGNNs only impose a relatively small (constant factor) overhead in this case. Finally, note that these theorems only consider the dropout distribution around a speciﬁc node u. To ensure the same properties for all nnodes in the graph simultaneously, we need to add a further factor of nwithin the logarithm to the number of necessary runs in Theorems 1 and 2. However, while this is only a logarithmic dependence on n, it might still be undesired in practice. 4.2 Expressive power of DropGNNs In Section 3.4, we have seen that DropGNNs often succeed when a WL-test fails. It is natural to wonder about the capabilities and limits of the dropout approach in general; we study this question for multiple neighborhood aggregation methods separately. We consider neighborhood aggregation with sum and mean in more detail; the proofs of the cor- responding claims are discussed in Appendices C and D, respectively. Appendix D also discusses brieﬂy why max aggregation does not combine well with the dropout approach in practice. 6u u 1–dropouts u Figure 3: Example of two graphs not separable by 1-dropouts (left side). In both of the graphs, for any of the 1-dropouts, uobserves the same tree structure for d= 2, shown on the right side. Aggregation with sum. Previous work has already shown that sum neighborhood aggregation allows for an injective GNN design, which computes a different embedding for any two neighborhoods whenever they are not equivalent for the WL-test [ 37]. Intuitively speaking, this means that sum aggregation has the same expressive power as a general-purpose d-hop distributed algorithm in the corresponding model, i.e. without IDs or port numbers. Hence to understand the expressiveness of DropGNNs in this case, one needs to analyze which embeddings can be computed by such a distributed algorithm from a speciﬁc (observed) distribution of dropout combinations. It is already non-trivial to ﬁnd two distinct neighborhoods that cannot be distinguished in the 1- complete case. However, such an example exists, even if we also consider2-dropouts. That is, one can construct a pair of d-hop neighborhoods that are non-isomorphic, and yet they produce the exact same distribution of 1- and 2-dropout neighborhoods in a d-layer DropGNN. Theorem 3 There exists a pair of neighborhoods that cannot be distinguished by 1- and 2-dropouts. We illustrate a simpler example for only 1-dropouts in Figure 3. For a construction that also covers the case of 2-dropouts, the analysis is more technical; we defer this to Appendix C. We note that even these more difﬁcult examples can be distinguished with our dropout approach, based on their k-dropouts for larger kvalues. However, this requires an even higher number of runs: we need to ensure that we can observe a reliable distribution even for these many-node dropouts. On the other hand, our dropout approach becomes even more powerful if we combine it e.g. with the extension by port numbers introduced in [28]. Intuitively speaking, port numbers allow an algorithm to determine all paths to the removed node in a 1-dropout, which in turn allows us to reconstruct the entire d-hop neighborhood of u. As such, in this case, 1-complete dropouts already allow us to distinguish any two neighborhoods. Theorem 4 In the setting of Theorem 1, a DropGNN with port numbers can distinguish any two non-isomorphic d-hop neighborhoods. Finally, we note that the expressive power of DropGNNs in the1-complete case is closely related to the graph reconstruction problem, which is a major open problem in theoretical computer science since the 1940s [14]. We discuss the differences between the two settings in Appendix C. Aggregation with mean. We have seen in Section 3.4 that even withmean aggregation, DropGNNs can sometimes distinguish 1-hop neighborhoods (that is, multisets S1 and S2 of features) which look identical to a standard GNN. One can also prove in general that a similar separation is possible in various cases, e.g. whenever the two multisets have the same size. Lemma 1 Let S1 ̸= S2 be two multisets of feature vectors with |S1|= |S2|. Then S1 and S2 can be distinguished by a DropGNN with mean neighborhood aggregation. However, in the general case,mean aggregation does not allow us to separate any two multisets based on 1-dropouts. In particular, in Appendix C, we also describe an example of multisets S1 ∩S2 = ∅ where the distribution of means obtained from 0- and 1-dropouts is essentially identical in S1 and S2. This implies that if we want to distinguish these multisets S1 and S2, then the best we can hope for is a more complex approach based on multiple-node dropouts. 75 Experiments In all cases we extend the base GNN model to a DropGNN by running the GNN rtimes in parallel, doing mean aggregation over the resulting rnode embedding copies before the graph readout step and then applying the base GNN’s graph readout. Additionally, an auxiliary readout head is added to produce predictions based on each individual run. These predictions are used for an auxiliary loss term which comprises 1 3 of the ﬁnal loss. Unless stated otherwise, we set the number of runs to m and choose the dropout probability to be p= 1 m, where mis the mean number of nodes in the graphs in the dataset. This is based on the assumption, that in the datasets we use the GNN will usually have the receptive ﬁeld which covers the whole graph. We implement random node dropout by, in each run, setting all features of randomly selected nodes to 0. See Appendix E for more details about the experimental setup and dataset statistics. The code is publicly available1. 5.1 Datasets beyond WL GIN +Ports +IDs +Random feat. +Dropout Dataset Train Test Train Test Train Test Train Test Train Test LIMITS1 [11] 0.50±0.00 0.50±0.00 0.50±0.00 0.50±0.00 1.00±0.00 0.59±0.19 0.66±0.19 0.66±0.22 1.00±0.001.00±0.00LIMITS2 [11] 0.50±0.00 0.50±0.00 0.50±0.00 0.50±0.00 1.00±0.00 0.61±0.26 0.72±0.17 0.64±0.19 1.00±0.001.00±0.004-CYCLES[20] 0.50 ±0.00 0.50±0.00 1.00±0.01 0.84±0.07 1.00±0.00 0.58±0.07 0.75±0.05 0.77±0.05 0.99±0.031.00±0.01LCC [29] 0.41 ±0.09 0.38±0.08 1.0±0.00 0.39±0.09 1.00±0.00 0.42±0.08 0.45±0.16 0.46±0.08 1.00±0.000.99±0.02TRIANGLES[29] 0.53±0.15 0.52±0.15 1.0±0.00 0.54±0.11 1.00±0.00 0.63±0.08 0.57±0.08 0.67±0.05 0.93±0.120.93±0.13SKIP-CIRCLES[6] 0.10±0.00 0.10±0.00 1.00±0.00 0.14±0.08 1.00±0.00 0.10±0.09 0.16±0.11 0.16±0.05 0.81±0.280.82±0.28 Table 1: Evaluation of techniques that increase GNN expressiveness on challenging synthetic datasets. We highlight the best test scores in bold. Compared to other augmentation techniques DropGNN (GIN +Dropout) achieves high training accuracy but also generalizes well to the test set. To see the capabilities of DropGNN in practice we test on existing synthetic datasets, which are known to require expressiveness beyond the WL-test. We use the datasets from Sato et al.[29] that are based on 3−regular graphs. Nodes have to predict whether they are part of a triangle (TRIANGLES ) or have to predict their local clustering coefﬁcient ( LCC ). We test on the two counterexamples LIMITS 1 (Figure 2a) and LIMITS 2 from Garg et al. [11] where we compare two smaller structures versus one larger structure. We employ the dataset by Loukas[20] to classify graphs on containing a cycle of length 4 ( 4-CYCLES ). We increase the regularity in this dataset by ensuring that each node has a degree of 2. Finally we experiment on circular graphs with skip links (SKIP -CIRCLES ) by Chen et al. [6], where the model needs to classify if a given circular graph has skip links of length {2,3,4,5,6,9,11,12,13,16}. For comparison, we try several other GNN modiﬁcations which increase expressiveness. For control, we run a vanilla GNN on these datasets. We then extend this base GNN with (i) ports [28] (randomly assigned), (ii) node IDs [ 20] (randomly permuted), and (iii) a random feature from the standard normal distribution [29]. The architecture for all GNNs is a 4-layer GIN with sum as aggregation and ε= 0. For DropGNN r= 50 runs are performed. For the SKIP -CIRCLES dataset we use a 9-layer GIN instead, as the skip links can form cycles of up to 17 hops. We train all methods for1,000 epochs and then evaluate the accuracy on the training set. We then test on a new graph (with new features). We report training and testing averaged across10 initializations in Table 1. We can see that DropGNN outperforms the competition. 5.2 Sensitivity analysis We investigate the impact of the number of independent runs on the overall accuracy. Generally, we expect an increasing number of runs to more reliably produce informative dropouts. We train with a sufﬁciently high number of runs (50) with the same setting as before. Now, we reevaluate DropGNN but limit the runs to a smaller number. We measure the average accuracy over10 seeds with 10 tests each and plot this average in Figure 4 on three datasets: LIMITS 1 (Figure 4a), 4-CYCLES (Figure 4b), and TRIANGLES (Figure 4c). In all three datasets, more runs directly translate to higher accuracy. Next, we investigate the impact of the dropout probability p. We use the same setting as before, but instead of varying the number of runs in the reevaluation, we train and test with different probabilities 1https://github.com/KarolisMart/DropGNN 80 10 20 30 40 50 Number of Runs 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (a) LIMITS 1 0 10 20 30 40 50 Number of Runs 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (b) 4−CYCLES 0 10 20 30 40 50 Number of Runs 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (c) TRIANGLES Figure 4: Investigating the impact of the number of runs (x−axis) versus the classiﬁcation accuracy (y−axis). In all three plots, having more runs allows for more stable dropout observations, increasing accuracy. The tradeoff is higher runtime since the model computes more runs. 0.0 0.2 0.4 0.6 0.8 Dropout Probability 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (a) LIMITS 1 0.0 0.2 0.4 0.6 0.8 Dropout Probability 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (b) 4−CYCLES 0.0 0.2 0.4 0.6 0.8 Dropout Probability 0.4 0.5 0.6 0.7 0.8 0.9 1.0Accuracy (c) TRIANGLES Figure 5: Investigating the impact of the dropout probability ( x−axis) versus the classiﬁcation accuracy (y−axis). DropGNN is robust to the choice of pfor decently small p. Choosing p≈γ−1 is a decent default that is shown by vertical black lines. pon an exponential scale from 0.01 to 0.64. We also try 0 (no dropout) and 0.95 (almost everything is dropped). Figure 5 shows the accuracy for each dropout probability, again averaged over10 seeds with 10 tests each. Generally, DropGNN is robust to different values of puntil pbecomes very large. 5.3 Graph classiﬁcation Model Complexity MUTAG PTC PROTEINS IMDB-B IMDB-M WL subtree [37; 31]O(n) 90.4±5.7 59.9 ±4.3 75.0 ±3.1 73.8 ±3.9 50.9 ±3.8 DCNN [2] O(n) - - 61.3 ±1.6 49.1 ±1.4 33.5 ±1.4 PatchySan [23] O(n) 89.0±4.4 62.3 ±5.7 75.0 ±2.5 71.0 ±2.3 45.2 ±2.8 DGCNN [39] O(n) 85.8±1.7 58.6 ±2.5 75.5 ±0.9 70.0 ±0.9 47.8 ±0.9 GIN [37] O(n) 89.4±5.6 64.6 ±7.0 76.2 ±2.8 75.1 ±5.1 52.3±2.8 DropGIN (ours) O(rn),r≈20 90.4±7.0 66.3 ±8.6 76.3 ±6.1 75.7 ±4.2 51.4±2.8 1-2-3 GNN [22] O(n4) 86.1 60.9 75.5 74.2 49.5 PPGN [21]* O(n3) 90.6±8.7 66.2 ±6.5 77.2 ±4.7 73±5.8 50.5±3.6 Table 2: Graph classiﬁcation accuracy (%). The best performing model in each complexity class is highlighted in bold. *We report the best result achieved by either of the three versions of their model. We evaluate and compare our modiﬁed GIN model (DropGIN) with the original GIN model and other GNN models of various expressiveness levels on real-world graph classiﬁcation datasets. We use three bioinformatics datasets (MUTAG, PTC, PROTEINS) and two social networks (IMDB-BINARY and IMDB-MULTI) [38]. Following [37] node degree is used as the sole input feature for the IMDB datasets, while for the bioinformatics datasets the original categorical node features are used. We follow the evaluation and model selection protocol described in [ 37] and report the 10-fold cross-validation accuracies [38]. We extend the original 4-layer GIN model described in [37] and use the same hyper-parameter selection as [37]. From Figure 5 we can see that it is usually safer to use a slightly larger pthan a slightly smaller one. Due to this, we set the node dropout probability to p= 2 m, where mis the mean number of nodes in the graphs in the dataset. 9Our method successfully improves the results achieved by the original GIN model on the bioinfor- matics datasets (Table 2) and is, in general, competitive with the more complex and computationally expensive expressive GNNs. Namely, the 1-2-3 GNN [ 22] which has expressive power close to that of 3-WL and O(n4) time complexity, and the Provably Powerful Graph Network (PPGN) [21] which has 3-WL expressive power and O(n3) time complexity. Compared to that, our method has only O(rn) time complexity. However, we do observe, that our approach slightly underperforms the original GIN model on the IMDB-M dataset. Since the other expressive GNNs also underperform when compared to the original GIN model, it is possible that classifying graphs in this dataset rarely requires higher expressiveness. In such cases, our model can lose accuracy compared to the base GNN as many runs are required to achieve a fully stable dropout distribution. 5.4 Graph property regression Property Unit MPNN [34] 1-GNN [22] 1-2-3 GNN [22] PPGN [21] DropMPNN Drop-1-GNN µ Debye 0.358 0.493 0.473 0.0934 0.059* 0.453* α Bohr3 0.89 0.78 0.27 0.318 0.173* 0.767* ϵHOMO Hartree 0.00541 0.00321 0.00337 0.00174 0.00193* 0.00306* ϵLUMO Hartree 0.00623 0.00350 0.00351 0.0021 0.00177* 0.00306* ∆ϵ Hartree 0.0066 0.0049 0.0048 0.0029 0.00282* 0.0046* ⟨R2⟩ Bohr2 28.5 34.1 22.9 3.78 0.392* 30.83* ZPVE Hartree 0.00216 0.00124 0.00019 0.000399 0.000112* 0.000895* U0 Hartree 2.05 2.32 0.0427 0.022 0.0409* 1.80* U Hartree 2.0 2.08 0.111 0.0504 0.0536* 1.86* H Hartree 2.02 2.23 0.0419 0.0294 0.0481* 2.00* G Hartree 2.02 1.94 0.0469 0.24 0.0508* 2.12 Cv cal/(mol K) 0.42 0.27 0.0944 0.0144 0.0596* 0.259* Table 3: Mean absolute errors on QM9 dataset [25]. Best performing model is in bold and DropGNN versions that improve over the corresponding base GNN are marked with a *. We investigate how our dropout technique performs using different base GNN models on a different, graph regression, task. We use the QM9 dataset [ 25], which consists of 134k organic molecules. The task is to predict 12 real-valued physical quantities for each molecule. In all cases, a separate model is trained to predict each quantity. We choose two GNN models to augment: MPNN [12] and 1-GNN [22]. We set the DropGNN run count and node dropout probability the same way as done for graph classiﬁcation. Following previous work [22; 21] the data is split into 80% training, 10% validation, and 10% test sets. Both DropGNN model versions are trained for 300 epochs. From Table 3 we can see that Drop-1-GNN improves upon 1-GNN in most of the cases. In some of them, it even outperforms the much more computationally expensive 1-2-3-GNN, which uses higher- order graphs and has three times more parameters [22]. Meanwhile, DropMPNN always substantially improves on MPNN, often outperforming the Provably Powerful Graph Network (PPGN), which as you may recall scales as O(n3). This highlights the fact that while the DropGNN usually improves upon the base model, the ﬁnal performance is highly dependent on the base model itself. For example, 1-GNN does not use skip connections, which might make retaining detailed information about the node’s extended neighborhood much harder and this information is crucial for our dropout technique. 6 Conclusion We have introduced a theoretically motivated DropGNN framework, which allows us to easily increase the expressive power of existing message passing GNNs, both in theory and practice. DropGNNs are also competitive with more complex GNN architectures which are specially designed to have high expressive power but have high computational complexity. In contrast, our framework allows for an arbitrary trade-off between expressiveness and computational complexity by choosing the number of rounds raccordingly. Societal Impact. In summary, we proposed a model-agnostic architecture improvement for GNNs. We do not strive to solve a particular problem but to enhance the GNN toolbox. Therefore, we do not see an immediate impact on society. We found in our experiments that DropGNN works best on graphs with smaller degrees, such as molecular graphs. Therefore, we imagine that using DropGNN in these scenarios is interesting to explore further. 10References [1] R. Abboud, ˙I. ˙I. Ceylan, M. Grohe, and T. Lukasiewicz. The surprising power of graph neural networks with random node initialization. arXiv preprint arXiv:2010.01179, 2020. [2] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in neural information processing systems, pages 1993–2001, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] A. Bojchevski, J. Klicpera, and S. Günnemann. Efﬁcient robustness certiﬁcates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In Proceedings of the 37th International Conference on Machine Learning, pages 1003–1013. PMLR, 2020. [5] J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun. Spectral networks and locally connected networks on graphs. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. [6] Z. Chen, L. Chen, S. Villar, and J. Bruna. On the equivalence between graph isomorphism testing and function approximation with gnns. Advances in neural information processing systems, 2019. [7] J. Cohen, E. Rosenfeld, and Z. Kolter. Certiﬁed adversarial robustness via randomized smooth- ing. In Proceedings of the 36th International Conference on Machine Learning, pages 1310– 1320. PMLR, 2019. [8] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in neural information processing systems, 2016. [9] W. Feng, J. Zhang, Y . Dong, Y . Han, H. Luan, Q. Xu, Q. Yang, E. Kharlamov, and J. Tang. Graph random neural networks for semi-supervised learning on graphs. Advances in Neural Information Processing Systems, 33, 2020. [10] M. Fey and J. E. Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019. [11] V . Garg, S. Jegelka, and T. Jaakkola. Generalization and representational limits of graph neural networks. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3419–3430. PMLR, 13–18 Jul 2020. [12] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), Sydney, Australia, Aug. 2017. [13] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in neural information processing systems, 2017. [14] F. Harary. A survey of the reconstruction conjecture. In R. A. Bari and F. Harary, editors, Graphs and Combinatorics, pages 18–28, Berlin, Heidelberg, 1974. Springer Berlin Heidelberg. ISBN 978-3-540-37809-9. [15] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V . Pande, and J. Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019. [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning , pages 448–456. PMLR, 2015. [17] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. [18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations ICLR, Toulon, France, Apr. 2017. 11[19] J. Klicpera, J. Groß, and S. Günnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations, 2020. [20] A. Loukas. What graph neural networks cannot learn: depth vs width. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [21] H. Maron, H. Ben-Hamu, H. Serviansky, and Y . Lipman. Provably powerful graph networks. arXiv preprint arXiv:1905.11136, 2019. [22] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4602–4609, 2019. [23] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pages 2014–2023. PMLR, 2016. [24] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. [25] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. V on Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientiﬁc data, 1(1):1–7, 2014. [26] Y . Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [27] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pages 8459–8468. PMLR, 2020. [28] R. Sato, M. Yamada, and H. Kashima. Approximation ratios of graph neural networks for combinatorial problems. In Neural Information Processing Systems (NeurIPS), 2019. [29] R. Sato, M. Yamada, and H. Kashima. Random features strengthen graph neural networks. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM) , pages 333–341. SIAM, 2021. [30] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 2008. [31] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011. [32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine learning research, 15 (1):1929–1958, 2014. [33] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), Vancouver, BC, Canada, May 2018. [34] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and V . Pande. Moleculenet: a benchmark for molecular machine learning.Chemical science, 9(2): 513–530, 2018. [35] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2020. [36] K. Xu, C. Li, Y . Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. InInternational Conference on Machine Learning (ICML), Stockholm, Sweden, July 2018. 12[37] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [38] P. Yanardag and S. Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365–1374, 2015. [39] M. Zhang, Z. Cui, M. Neumann, and Y . Chen. An end-to-end deep learning architecture for graph classiﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 32, 2018. 13A Concrete GNN representations for the examples In this section, we revisit the example graphs from Section 3.4, and we provide a concrete GNN implementation for each of them which is able to distinguish the two cases. Example 1. Let us assume for simplicity that each node starts with the integer1 as its single feature. Also, assume that neighborhood aggregation happens with a simple summation, with no non-linearity afterwards, and that this sum is then combined with the node’s own feature again through a simple addition. Now consider this GNN with d= 2 layers. Note that in this case, a node uin the left-hand graph is able to gather information from the whole cycle, while a node uin the right-hand graph will behave as if it was the middle node in a simple path of 5 nodes. In both cases, if no dropouts happen, then u will have a value of 3 after the ﬁrst round, and a value of 9 after the second round. However, the 1-dropouts are already signiﬁcantly different: in the left-hand graph, they will produce a result of 5, 5 and 7, while in the right-hand graph, they result in a ﬁnal value of 5, 5, 8 and 8. One can similarly compute the k-dropouts for k≥2, which will also produce a range of other values (but at most 7 in any case). If we apply a more sophisticated transformation on these embeddings before run aggregation, then it is straightforward to separate these two distributions. For example, we can use an MLP to only obtain a positive value in case if the embedding is 8 (we discuss this technique in more detail at Example 2); this will happen regularly for the right-hand graph, but never for the left-hand graph. After this, a simple sum run aggregation already distinguishes the cases. However, if one prefers a simpler transformation, then a choice of σ(x−8) also sufﬁces (with σ denoting the Heaviside step function). With this transformation, a run aggregation with sum simply counts the number of cases when the ﬁnal embedding was a 9. Since the probability of the 0-dropout is different in the two graphs, the expected value of this count will also differ by at leastΩ(p·r) after rruns, which makes them straightforward to distinguish. Example 2. For an elegant representation of Example 2, the most convenient method is to apply a slightly more complex non-linearity for neighborhood aggregation; this allows a very simple representation for everything else in the GNN. In particular, let us again assume that each node simply starts with an integer 1 as a feature (i.e. not even aware of its degree initially). Furthermore, assume that neighborhood aggregation happens with a simple sum operator; however, after this, we use a more sophisticated non-linearity ˆσwhich ensures ˆσ(2) = 1 , and ˆσ(x) = 0 for all other integers x. One can easily implement this function with a 2-layer MLP: we can use x1 = σ(x−1) and x2 = σ(−x+ 3) as two nodes in the ﬁrst layer, and then combine them with a single node σ(x1 + x2 −1) as the second layer. Finally, for the UPDATE function which merges the aggregated neighborhood xN(u) with the node’s own embeddingxu, let us select σ(xN(u) + xu −2). The resulting GNN can be described rather easily. Each node begins with a feature of 1, and has an embedding of either 0 or 1 in any subsequent round. The update rule for the embedding is also simple: if u’s own value is1 and uhas exactly 2 neighbors with a value of 1, then the embedding of uwill remain 1; in any other case, u’s embedding is set to0, and it will remain 0 forever. In case of dropouts, this GNN will behave differently in the two graphs of Example2. Note that in both cases, whenever the connected component containing node uis not a cycle after the dropouts, then in at most d= 3 rounds, the embedding of uis set to 0. On the other hand, if the component containing uis a cycle, then the embedding of uwill remain 1 after any number of rounds. Now let udenote one of the nodes with degree 3 in both graphs. In the left-hand graph, there is a 1-dropout (of the other gray node) that puts uin a cycle, so uwill produce a ﬁnal embedding of 1 relatively frequently. Besides this, there are also 2 distinct 2-dropouts and a 3-dropout that removes the other gray node but keeps the triangle containinguintact; these will all result in a ﬁnal embedding of 1 for u. On the other hand, in the right-hand graph, there are only 2 distinct 2-dropouts which result in a single cycle containing u. 14This means that the probability of getting a ﬁnal value of 1 is signiﬁcantly higher in the left graph. In particular, after rruns, the difference in the expected frequency of getting a 1 is at least Ω(p·r), so we can easily separate the two cases by executing run aggregation with sum or mean. Example 3. The base idea of this separation has already been outlined in Section 3.4: assume that the middle node uuses a simple mean aggregation of its neighbors, and the dropout probability is p= 1 4 . Since we are now interested in the behavior of a speciﬁc step of mean aggregation, we only study the GNN for d= 1 rounds. With p= 1 4 , the left-hand graph provides the following distribution of means in a DropGNN: Pr(0) = (3 4 )2 and Pr (1) = Pr(−1) = 1 4 ·3 4 . As such, the probability of obtaining a 1 is about 0.19. Note that we disregarded the case when all neighbors of uare removed, but we could assume for convenience that e.g. themean function also returns 0 in this case. Furthermore, we only considered the cases when uis not removed, since these are the only runs when ucomputes an embedding at all. On the other hand, in the right-hand graph, uobtains the following distribution: Pr(0) = (3 4 )4 + 4 · (1 4 )2 · (3 4 )2 , Pr (1 3 ) = Pr ( −1 3 ) = 2 ·1 4 · (3 4 )3 and Pr (1) = Pr(−1) = (1 4 )2 · (3 4 )2 + 2 · (1 4 )3 ·3 4 . This gives a probability of about 0.06 for the value 1. If we apply e.g. the transformation x→σ(x−0.5) on these values, then the embedding 1 is indeed signiﬁcantly more frequent in the left-hand graph. Using either mean or sum for run aggregation allows us to separate the two cases: the ﬁnal embeddings in the two graphs will converge to 0.19 and 0.06 (both multiplied by rin case of sum). Alternative dropout methods. Throughout the paper, we consider a natural and straightforward version of the dropout idea: some nodes of the graph (and their incident edges) are removed for an entire run. However, we note that there are also several alternative ways to implement this dropout approach. For example, one could remove edges instead of nodes, or one could remove nodes in an asymmetrical manner (e.g., they still receive, but do not send messages). We point out that all these examples from Section 3.4. could also be distinguished under these alternative models. B Required number of runs We now discuss the proofs of Theorems 1 and 2. Note that for any speciﬁc subset Sof size k, the probability of this k-dropout happening in a speciﬁc run is pk ·(1 −p)γ+1−k = ( 1 1+γ )k · ( γ 1+γ )γ+1−k . To obtain the expected frequency EXS of this k-dropout after rruns, we simply have to multiply this expression by r. Furthermore, given a constant δ∈[0,1], a Chernoff bound shows that the probability of signiﬁcantly deviating from this expected value is Pr ( XS /∈[ (1−δ) ·EXS, (1+δ) ·EXS] ) ≤2 ·e−δ2·EXS 3 . Let us consider the case of Theorem 1 ﬁrst. Since we have γdifferent 1-dropouts, we can use a union bound over these dropouts to upper bound the probability of the event that any of the nodes v∈Γ will have Xv /∈[ (1−δ) ·E1, (1+δ) ·E1 ]; the probability of this event is at most 2 ·γ·e−δ2·E1 3 . 15If we ensure that this probability is at most 1 t, then the desired property follows. Note that after taking a (natural) logarithm of both sides, this is equivalent to log(2 ·γ) −δ2 ·E1 3 ≤ −log t, and thus E1 ≥ 3 δ2 ·log(2 ·γ·t) . Recall that for E1 we have E1 = 1 1 + γ · ( γ 1 + γ )γ ·r≥ 1 1 + γ ·1 e ·r. Due to this lower bound, it is sufﬁcient to ensure 1 1 + γ ·1 e ·r ≥ 3 δ2 ·log(2 ·γ·t) , that is, r ≥ 3e δ2 ·(γ+ 1) ·log(2 ·γ·t) = Ω(γ·log γt) . This completes the proof of Theorem 1. For Theorem 2, we also need to upper bound the probability of each dropout combination of multiple nodes. Consider k-dropouts for a speciﬁc k. In this case, we have EXS = ( 1 1 + γ )k · ( γ 1 + γ )γ+1−k ·r= 1 γk−1 ·E1 . This implies that in order to ensure XS < (1 −δ) ·E1 in Theorem 2, it is sufﬁcient to have XS < (1 −δ) ·γk−1 ·EXS. If we want to express this as (1 + ϵ) ·EXS for some ϵ, then we get ϵ = (1 −δ) ·γk−1 −1, and thus ϵ = Θ ( γk−1) for appropriately chosen constants. Applying a Chernoff bound (in this case, a different variant that also allows ϵ> 1) then gives Pr ( XS ≥(1+ϵ) ·EXS) ≤e−ϵ2·EXS 2+ϵ . Since ϵ= Θ ( γk−1) and EXS = γ−(k−1) ·E1, this is in fact e−Θ(1)·γk−1·γ−(k−1)·E1 = e−Θ(1)·E1 . Note that the number of differentk-dropouts is (γ k ) ≤2γ, so with a union bound, we can establish this property for each k-dropout simultaneously; for this, we need to multiply this error probability by 2γ. Finally, since we want to ensure this for all k≥2, we can take a union bound over k∈{2,3,...,γ }, getting another multiplier ofγ. Thus to obtain the second condition in Theorem 2 with error probability 1 t, we need γ·2γ ·e−Θ(1)·E1 ≤ 1 t . After taking a logarithm and reorganization, we get E1 ≥Θ(1) ·log(2γ ·γ·t) . With our lower bound for E1 and a reorganization of the right side, we can reduce this to r ≥Θ(1) ·(γ+ 1) ·γ·log(2 ·γ·t) = Ω ( γ2 + logγt ) . Another union bound shows that the two conditions of Theorem 2 also hold simultaneously when ris in this magnitude, thus completing the proof of Theorem 2. Note that if we want to ensure this property for the neighborhood of all the nnodes in the graph simultaneously, then we also have to take a union bound over all thennodes, which results in a factor of nwithin the logarithm in our ﬁnal bounds on r. 16Asymptotic analysis. Finally, let us note that from a strictly theoretical perspective, if we consider γto be a constant, and pto be some function of γ, then the probability of any speciﬁc k-dropout is pk ·(1 −p)γ+1−k, i.e. a constant value. As such, a Chernoff bound shows that if we select rto be a sufﬁciently large constant, then every possible dropout combination is observed, and their frequencies are reasonable close to the expected values. However, this approach is clearly not realistic in practice: e.g. for our choice of p ≈γ−1, the probability of a speciﬁc k-dropout is less than pk ≈γ−k. This means that we need r≥γk runs even to observe this k-dropout at least once in expectation. While this γk is, asymptotically speaking, only a constant value, it still induces a very large overhead in practice, even for relatively smallkand γ values. Different γand pvalues. Note that our choice of γwas deﬁned for an arbitrary node of the graph; however, the dropout probability p, chosen as a function of γ, is a global parameter of DropGNNs. As such, our choice of pfrom the analysis only works well if we assume that the graph is relatively homogeneous, i.e. γis similar for every node. In practice, one can simply apply the average or the maximum of these different γvalues; a slightly smaller/larger than optimal ponly means that we observe some dropouts with slightly lower probabil- ity, or we execute slightly more runs than necessary. The ablation studies in Figures 4 and 5 also show that our approach is generally robust to different number of runs and different dropout probabilities. We note, however, that if e.g. the graph consists of several different but separately homogeneous regions, then a more sophisticated approach could apply a different pvalue in each of these regions. C Expressiveness with sum aggregation We now discuss our claims on DropGNNs with sum neighborhood aggregation. Recall that with this aggregation method, a GNN with injective functions (such as GIN) has the same expressive power as the WL-test. Note that in this setting, we understand a d-hop neighborhood around uto refer to the part of the graph that ucan observe in drounds of message passing. In particular, this contains (i) all nodes that are at most dhops away from u, and (ii) all the edges induced by these nodes, except for the edges where both endpoints are exactly at distance dfrom u. C.1 Proof of Theorem 3 To prove Theorem 3, we show two different d-neighborhoods around a node u(for d = 2) that are non-isomorphic, but they generate the exact same distribution of observations for uif we only consider the case of k-dropouts for k≤2. Note that the example graphs on Figure 3 already provide an example where the 0-dropout and the 1-dropouts are identical. One can easily check this from the ﬁgure: in case of no dropouts, uobserves the same tree representation in d= 2 steps, and in case of any of the 6 possible 1-dropouts (in either of the graphs), uobserves the tree structure shown on the right side of the ﬁgure. To also extend this example to the case of 2-dropouts, we need to slightly change it. Note that the example graph is essentially constructed in the following way: we take two independent cycles of length 3 in one case, and a single cycle of length 6 in the other case, and in both graphs, we connect all these nodes to an extra node u. This construction is easy to generalize to larger cycle lengths. In particular, let us consider an integer ℓ≥3, and create the following two graphs: in one of them, we take two independent cycles of length ℓ, and connect each node to an extra node u, while in the other one, we take a single cycle of length 2 ·ℓ, and connect each node to an extra node u. We claim that with a choice of ℓ= 5, this construction sufﬁces for Theorem 3. As before, one can easily verify that uobserves the same 2-hop neighborhood in case of no dropouts, and also identical 2-hop neighborhoods for any of the 10 possible 1-dropouts in both graphs. The latter essentially has the same structure as the right-hand tree in Figure 3, except for the fact that the number of degree-3 branches (i.e. the ones on the left side of uin the ﬁgure) is now 7 instead of 3. 17It only remains to analyze the distribution of 2-dropouts. For this, note that the only information that ucan gather in d= 2 rounds is the multiset of degrees of its neighbors. In practice, this will depend on the distance of the two removed nodes in the cycles; in particular, we can have the following cases: 1. If the two nodes are neighbors in (one of) the cycle(s), then due to the dropouts, uwill have two neighbors of degree 2, and six neighbors of degree 3. There are 2 ·ℓ= 10 possible cases to have this dropout combination in both graphs. 2. If the two nodes are at distance 2 in (one of) the cycle(s), then u will have a single neighbor of degree 1, two neighbors of degree 2 and ﬁve neighbors of degree 3. This can again happen in 2 ·ℓ= 10 different ways in both graphs. 3. If the nodes have distance at least 3 within the same cycle, or they are in different cycles, then the dropout creates four neighbors of degree 2, and four neighbors of degree 3. In the 2 ·ℓcycle, this can happen in 2·ℓ·(2·ℓ−5) 2 = 2 ·ℓ2 −5 ·ℓ= 25 different ways. In case of the two distinct ℓ-cycles, this cannot happen in a single cycle at all (i.e. for general ℓ, it can happen in ℓ·(ℓ−5) 2 ways, but this equals to 0 for ℓ= 5); however, it can still happen if the two dropouts happen in different cycles, in ℓ·ℓ= 25 different ways. Hence the distribution of observed neighborhoods is also identical in case of 2-dropouts. C.2 Proof of Theorem 4 The setting of Theorem 4 considers GNNs with port numbers (such as CPNGNN) where the neigh- borhood aggregation function is not permutation invariant, i.e. it can produce a different result for a different ordering of the inputs (neighbors) [28]. Our proof of the theorem already builds on the fact that one can extend the idea of injective GNNs (such as GIN in [37]) to this setting with port numbers. To show that port numbers can be combined with the injective property, one can e.g. apply the same proof approach as in [37], using the fact that the possible combinations of embeddings and port numbers is still a countable set. Given such an injective GNN with port numbers, the expressiveness of this GNN is once again identi- cal to that of a general distributed algorithm in the message passing model with port numbers [28]. As such, it sufﬁces to show that a distributed algorithm in this model can separate any two different d-hop neighborhoods. Let us assume the 1-complete setting of Theorem 1, i.e. that we have sufﬁciently many runs to ensure that each 1-dropout is observed at least once in the d-hop neighborhood of u. We show that the set of neighborhoods observed this way is sufﬁcient to separate any two neighborhoods, regardless of the frequency of multi-dropout cases. The general idea of the proof is that 1-dropouts are already sufﬁcient to recognize when two nodes in the tree representation of u’s neighborhood are actually corresponding to the same node. Consider three nodes v1, v2 and v3, and assume that edges (v1,v3) and (v2,v3) are both within the d-hop neighborhood of u. More speciﬁcally, assume that v1’s port number b1 leads to v3, and v2’s port number b2 also leads to v3; then we can observe that the nodes at the endpoints of these two edges are always missing from the graph at the same time. That is, since we are guaranteed to observe every 1-dropout at least once, if neighbor b1 of v1 and neighbor b2 of v2 are distinct nodes, then we must observe at least one neighborhood variant where only one of these two neighbors are missing; in this case, we know that the b1th neighbor of v1 and the b2th neighbor of v2 are not identical. On the other hand, if the two neighbors are always absent simultaneously, then the two edges lead to the same node. The proof of the theorem happens in an inductive fashion. Note that from the 0-dropout, we can already identify the degree of uin the graph, and the port leading to each of its neighbors; this is exactly the 1-hop neighborhood of u. Now let us assume that we have already reconstructed the (i−1)-hop neighborhood of u; in this case, we can identify each outgoing edge from this neighborhood by a combination of a boundary node (a node at distance (i−1) from u) and a port number at this node. We can then extend our graph into the i-hop neighborhood of u(for i≤d) with the following two steps: 181. First, we reconstruct the edges going from distance (i−1) nodes to distance inodes. Let us refer to nodes at distance ias outer nodes. Note that all the outer neighbors of the boundary nodes can be identiﬁed by the speciﬁc outgoing edges from the boundary nodes; we only have to ﬁnd out which of these outer nodes are actually the same. This can be done with the general idea outlined before: if two boundary nodes v1 and v2 have a neighbor at ports b1 and b2, respectively, and we do not observe a graph variant where only one of these neighbors is missing, then the two edges lead to the same outer node. 2. We also need to reconstruct the adjacencies between the boundary nodes; this is part of the i-hop neighborhood of uby deﬁnition, but not part of the (i−1)-hop neighborhood. This happens with the same general idea as before: assume that v2 and v3 are both nodes at distance (i−1), and v1 is a node at distance (i−2) that is adjacent to v3. Then we can check whether v3 disappears simultaneously from the respective ports b1 and b2 of nodes v1 and v2; if it does, then we know that edge b2 of node v2 leads to this other boundary node v3. After dsteps, this process allows us to reconstruct the entire d-hop neighborhood of u, thus proving the theorem. Let us also brieﬂy comment on the GNN interpretation of this graph algorithm. An injective GNN construction ensures that we map differentd-hop neighborhoods to a different real number embedding. Note that the algorithm can separate any two neighborhoods without using the frequency of the speciﬁc neighborhoods variants; this implies that the set of real numbers obtained is different for any two neighborhoods, i.e. there must exist a number z∈R that is present in one of the distributions, but not in the other. One can then develop an MLP that essentially acts as an indicator for this value z, only outputting 1 if the input is z; this allows us to separate the two neighborhoods. Finally, note that our main objective throughout the paper was to compute a different embedding for two different neighborhoods. However, in this setting of Theorem 4, it is also possible to encounter the opposite problem: if two d-hop neighborhoods are actually isomorphic, but they have a different assignment of port numbers, then they might produce a different embedding in the end. We point out that with more sophisticated run aggregation, it is also possible to solve this problem, i.e. to recognize the same neighborhood regardless of the chosen port numbering. In particular, we have seen that in the 1-complete case, the multiset of ﬁnal embeddings already determines the entire neighborhood around u, and thus also its isomorphism class. This means that there is a well-deﬁned function from the embedding vectors in Rr that we can obtain in rruns to the possible isomorphism classes of u’s neighborhood (assuming for convenience that the neighborhood size is bounded). Due to the universal approximation theorem, a sufﬁciently complex MLP can indeed implement this function; as such, determining the isomorphism class of u’s neighborhood is indeed within the expressive capabilities of DropGNNs in this setting. However, while such a solution exists in theory, we note that this graph isomorphism problem is known to be rather challenging in practice. C.3 Brieﬂy on the graph reconstruction problem The graph reconstruction problem is a well-known open question dating back to the 1940s. Assume that there is a hidden graph Gon n≥3 nodes that we are unaware of; instead, what we receive as an input is ndifferent modiﬁed variants of G, each obtained by removing a different node (and its incident edges) from G. This input multiset of graphs is often called the deck of G. Note that the graphs in the deck are only provided up to an isomorphism class, i.e. for a speciﬁc node of the deck graph, we do not know which original node of Git corresponds to. The goal is to identify Gfrom its deck; this problem is solvable exactly if there are no two non-isomorphic graphs with the same deck. This assumption is known as the graph reconstruction conjecture [14]. This problem is clearly close to our task of reconstructing a neighborhood from its1-dropout variants; however, there are also two key differences between the settings. Firstly, in our DropGNNs, we do not observe a graph, but rather a tree-representation of its neighborhood where some nodes may appear multiple times. In this sense, our GNN setting is much more challenging than the reconstruction problem, since it is highly non-trivial to decide whether two nodes in this tree representation correspond to the same original node. On the other hand, the DropGNN setting has the advantage that we can also observe the 0-dropout; this does not happen in the reconstruction problem, since it would correspond to directly receiving the solution besides the deck. 19D Dropouts with mean or max aggregation In this section, we discuss the expressiveness of the dropout technique with mean and max neighbor- hood aggregation. In particular, we prove that separation is always possible withmean aggregation when |S1|= |S2|, we construct a pair of neighborhoods that provide a very similar distribution of mean values, and we brieﬂy discuss the limits of max aggregation in practice. D.1 Proof of Lemma 1 We begin with the proof of Lemma 1. More speciﬁcally, we show that if |S1|= |S2|, then there always exists a choice of pand integers a,b such that after applying an activation function σ(ax+ b) on S1 and S2, a mean neighborhood aggregation allows us to distinguish the two sets. In our proof, we assume that S1 and S2 are both multisets of integers (instead of vectors), i.e. that node features are only 1-dimensional. With multi-dimensional feature vectors, we can apply the same proof to each dimension of the vectors individually; since S1 ̸= S2, we will always have a dimension that allows us to separate the two multisets with the same method. Let s1 denote the mean of S1 and s2 denote the mean of S2. We ﬁrst discuss the simpler case when s1 ̸= s2; if this holds, we can distinguish any two sets S1 and S2, so we make this proof for the general case, without the assumption that |S1|= |S2|. After this, we discuss the case when s1 = s2 and |S1|= |S2|; this completes the proof of Lemma 1. The main idea of the proofs is to ﬁnd a threshold τ such that in S1, we have mean values larger than τ much more frequently than in S2 (or vice versa). We can then use an activation function ˆσ(x) := σ(x−τ) (with σdenoting the Heaviside step function) to ensure that σ(x) = 1 if x≥τ, and σ(x) = 0 otherwise. This means that a run aggregation with sum will simply count the cases when the mean is larger than τ, and thus with high probability, we get a signiﬁcantly different sum in case of S1 and S2. Note that even though the proof is described with a Heaviside activation function for ease of presentation, one could also use the logistic function (a more popular choice in practice), since the logistic function provides an arbitrary close approximation of the step function with the appropriate parameters. When the means are different. First we consider the case when s1 ̸= s2. In this setting, ﬁnding an appropriate τ is relatively straightforward. Assume w.l.o.g. thats1 <s2, and let us choose an arbitrary τ such that s1 < τ <s2. This implies that whenever no node is removed, then the mean in S1 will produce a 0, while the mean in S2 will produce a 1. It only remains to ensure that 0-dropouts are frequent enough to distinguish these two cases. For this, let γ = max(|S1|,|S2|), and let us select p= 1 2γ. For both S1 and S2, this gives a probability of at least (1 −p)γ = (2γ−1 2γ )γ for 0-dropouts. When γ ≥2, this probability is strictly larger than 0.55. With a Chernoff bound, one can also show that the number of 0-dropouts is strictly concentrated around this value: with δ= 0.05 and rruns, the probability of the number of 0-dropouts being below (1 −δ) ·0.55 ≈0.52 is upper bounded by e−1 3 ·δ2·0.55·r. To ensure that this is below 1 t, we only need Θ(1) ·r ≥log t, and hence r ≥Ω(log t). This already ensures that in case of S2, we have at least 0.52 ·rruns that produce a 1, while in S1, we have at least 0.52 ·rruns that produce a 0 (i.e. at most 0.48 ·rruns that produce a 1). Hence with high probability, a sum run aggregation gives a sum below 0.48 ·rand above 0.52 ·rfor S1 and S2 respectively, so the two cases are indeed separable. When the means are the same. Now consider the case when s1 = s2, and we have |S1|= |S2|. In this setting, let γ = |S1|= |S2|. Since the multisets are not identical, there must be an index i∈{1,...,γ }such that in the sorted version of the multisets, the ith element of S1 is different from the ith element of S2. Let us consider the smallest such index i, and assume w.l.o.g. that the ith 20element of S1 (let us call it x1,i) is larger than the ith element of S2 (denoted by x2,i). Furthermore, Let s1,−i and s2,−i denote the mean of S1 and S2, respectively, after removing the ith element. Note that if we only had 1-dropouts and 0-dropouts in our GNNs, then ﬁnding this index iwould already allow a separation in a relatively straightforward way. Since x1,i > x2,i, we must have s1,−i <s2,−i. The idea is again to select a threshold value τ such that s1,−i < τ <s2,−i. This ensures that in S1, at least i of the 1-dropouts produce a 0, whereas in S2, at most i−1 of the 1-dropouts produce a 0. If the frequency of all 1-dropouts is concentrated around its expectation, then this shows that the occurrences of 1 will be signiﬁcantly higher in S2. What makes this argument slightly more technical is the presence of k-dropouts for k≥2. In order to reduce the relevance of these cases, we select a smaller pvalue. In particular, let p= 1 2γ2 . In this case, the probability of a k-dropout is only pk ·(1 −p)γ−k ≤pk = 1 2k ·γ2k , and the probability of having any multiple-dropout case in a speciﬁc run is at most γ∑ k=2 (γ k ) · 1 2k ·γ2k ≤ γ∑ k=2 γk 2 · 1 2k ·γ2k ≤ γ∑ k=2 1 2k+1 · 1 γk ≤ 1 4 ·γ2 , using the fact that (γ k ) ≤1 2 ·γk for k≥2 and the fact that 1 8 + 1 16 + ...≤1 4 . On the other hand, the probability of a 1-dropout is p·(1 −p)γ−1 = 1 2γ2 · (2γ2 −1 2γ2 )γ−1 , where one can observe that the second factor is at least 7 8 for any positive integer γ. As such, the probability of a 1-dropout is lower bounded by 7 16 · 1 γ2 , i.e. it is notably larger than the cumulative probability of multiple-dropout cases. This means that our previous choice of s1,−i < τ <s2,−i also sufﬁces for this general case. In particular, even if all the multiple-dropouts in S1 produce a mean that is larger than τ, and all the multiple-dropouts in S2 produce a mean that is smaller thanτ, we will still end up with a considerably larger probability of obtaining a value of1 in case of S2, due to the 1-dropout of the ith element. More speciﬁcally, the difference between the two probabilities will be at least 3 16 · 1 γ2 ; using a Chernoff bound in a similar fashion to before, one can conclude that Ω(γ4 ·log t) runs are already sufﬁcient to separate the two case with error probability at most 1 t. D.2 Construction for similar mean distribution Let us now comment on the general case when we have s1 = s2 but |S1|̸= |S2|. We present an example for two different sets S1 and S2 where the distribution of mean values obtained from 0- and 1-dropouts is essentially identical, thus showing the limits of any general approach that uses mean aggregation, but does not execute a deeper analysis of k-dropouts for k≥2. Consider an even integer ℓ, and consider the following two subsets. LetS1 consist of ℓ 2 distinct copies of the number −(ℓ−1), and ℓ 2 distinct copies of the number (ℓ−1). Let S2 consist of ℓ 2 distinct copies of the number −ℓ, and ℓ 2 distinct copies of the number ℓ, and a single instance of 0. These sets provide |S1|= ℓand |S2|= ℓ+ 1, and also s1 = s2 = 0. For a concrete example of ℓ= 4, we get the multisets S1 = {−3,−3,3,3}and S2 = {−4,−4,0,4,4}. The mean values obtained for 1-dropouts is also easy to compute in these examples. In S1, we have ℓ 2 distinct 1-dropouts with a mean of 1, and ℓ 2 distinct 1-dropouts with a mean of −1. In S2, we have ℓ 2 distinct 1-dropouts with a mean of 1, and ℓ 2 distinct 1-dropouts with a mean of −1, and a single 1-dropout with a mean of 0. Note that if we only consider these 0 and 1-dropouts, then the probability of getting a 0 is exactly the same in both settings. In S1, this comes from the probability of the 0-dropout only, so it is 21(1 −p)ℓ. In S2, we have to add up the probability of the 0-dropout and a single 1-dropout: this is (1 −p)ℓ+1 + p·(1 −p)ℓ = (1 −p)ℓ. The set of means obtained from 1-dropouts is also identical in the two neighborhoods, it is only their probability that is slightly different. InS1, both −1 and 1 are obtained with probability ℓ 2 ·p·(1−p)ℓ−1, while in S2, they are both obtained with probability ℓ 2 ·p·(1 −p)ℓ. Hence the difference between the two probabilities is only ℓ 2 ·p· ( (1 −p)ℓ−1 −(1 −p)ℓ) = ℓ 2 ·p2 ·(1 −p)ℓ−1 . Recall that we have Θ(ℓ2) distinct 2-dropouts, each with a probability of p2 ·(1 −p)ℓ−1, so these 2-dropouts are together easily able to bridge this difference of frequency of the 1-dropouts between S1 and S2. This shows that we cannot conveniently ignore multiple-node dropouts as in case of |S1|= |S2|before: the only possible 1-dropout-based approach to separate the two sets (i.e. to use the slightly different frequency of the values −1 and 1) is not viable without a deeper analysis of the distributions of 2-dropouts. It is beyond the scope of this paper to analyze this distribution in detail, or to come up with more sophisticated separation methods based on multiple-node dropouts. D.3 Aggregation with max Another well-known permutation-invariant function (and thus a natural candidate for neighborhood aggregation) is max; however, this method does not combine well with the dropout approach in practice. In particular, if the multisets S1 and S2 only differ in their smallest element, then max aggregation can only distinguish them from a speciﬁc (γ−1)-dropout when all other neighbors of uare removed. This dropout combination only has a probability of pγ−1 ·(1 −p)2; thus for a reasonably small p (e.g. for p≈γ−1), we need a very high number of runs to observe this case with a decent probability. E Details of the experimental setup In all of our experiments, we use Adam optimizer [17]. For synthetic benchmarks and graph clas- siﬁcation, we use a learning rate of 0.01, for graph property regression we use a learning rate of 0.001. For graph classiﬁcation benchmarks we decay the learning rate by 0.5 every 50 steps [ 37] and for the graph regression benchmark we decay the learning rate by a factor of 0.7 on plateau [22]. The GIN model always uses 2-layer multilayer perceptrons and batch normalization [16] after each level [37]. For our dropout technique, during preliminary experiments we tested three different node dropout implementation options: i) completely removing the dropped nodes and their edges from the graph; ii) replacing dropped node features by 0s before and after each graph convolution; iii) replacing the initial dropped node features by 0s. These preliminary experiments showed that all of these options performed similarly in practice, but the last option resulted in a more stable training. Since it is also the simplest dropout version to implement we chose to use it in all of our experiments. To ensure that the base model is well trained, when our technique is used we apply an auxiliary loss on each run individually. This auxiliary loss comprises 1 3 of the ﬁnal loss. While our model can have O(n) memory consumption if we execute the runs in sequence, we implement it in a paralleled manner, which reduces the compute time, as all rruns are performed in parallel, but increases memory consumption. For the synthetic benchmarks (LIMITS 1, LIMITS 2, 4-C YCLES , LCC, TRIANGLES , SKIP -CIRCLES ) we use a GIN model with 4 convolutional layers (+ 1 input layer), sum as aggregation, ε= 0 and for simplicity do not use dropout on the ﬁnal READOUT layer, while the ﬁnal layer dropout is treated as a hyper-parameter in the original model. For synthetic node classiﬁcation tasks (LIMITS 1, LIMITS 2, LCC , and TRIANGLES ) we use the same readout head as the original GIN model but skip the graph aggregation step. In all cases, except the SKIP CIRCLES dataset, 16 hidden units are used for synthetic tasks. For the SKIP CIRCLES dataset we use a GIN model with 9 convolutional layers (+ 1 input layer) with 32 hidden units as this dataset has cycles of up to 17 hops and requires long-range information propagation to solve the task. For the DropGIN variant, mean aggregation is used to aggregate node representations from different runs. When the GIN model is augmented with ports, which introduce edge features, we use modiﬁed GIN convolutions that include edge features [15]. In synthetic benchmarks, we always generate the same number of graphs for training and test sets 22(generate a new copy of the dataset for testing) and for each random seed, we re-generate the datasets. We always feed in the whole dataset as one batch. LIMITS 1, LIMITS 2 and SKIP -CIRCLES datasets are always comprised of graphs with the same structure, just with permuted node IDs for each dataset initialization, the remaining datasets have random graph structure, which changes when the datasets are regenerated. You can see the synthetic dataset structure type and statistics in Table 4. All nodes in these datasets have the same degree. Dataset Number of graphs Number of nodes Degree Structure Task LIMITS1 [11] 2 16 2 Fixed Node classiﬁcation LIMITS2 [11] 2 16 3 Fixed Node classiﬁcation 4-CYCLES[20] 50 16 2 Random Graph classiﬁcation LCC [29] 6 10 3 Random Node classiﬁcation TRIANGLES[29] 1 60 3 Random Node classiﬁcation SKIP-CIRCLES[6] 10 41 4 Fixed Graph classiﬁcation Table 4: Synthetic dataset statistics and properties. For graph classiﬁcation tasks we use exactly the same GIN model as described originally and apply our dropout technique on top. Namely, with 1 input layer, 4 convolution layers withsum as aggregation and ε= 0 and dropout [32] on the ﬁnal READOUT layer. For the DropGIN variant, mean aggregation is used to pool node representations from different runs. Note, that in our setting sum and mean aggregations are equivalent, up to a constant multiplicative factor, as the number of runs is a constant chosen on a per dataset level. We use exactly the same model training and selection procedure as described by [37]. We decay the learning rate by 0.5 every 50 epochs and tune the number of hidden units ∈{16,32}for bioinformatics datasets while using 64 for the social graphs. The dropout ratio ∈{0,0.5}after the ﬁnal dense layer the batch size ∈{32,128}are also tuned. The epoch with the best cross-validation accuracy over the 10 folds is selected. You can see the statistics of synthetic datasets in Table 5. Number of nodes Degree Dataset Number of graphs Min Max Mean Min Max Mean MUTAG 188 10 28 18 3 4 3.01 PTC 344 2 64 14 1 4 3.18 PROTEINS 1109 4 336 38 3 12 5.78 IMDB-B 996 12 69 19 11 68 18.49 IMDB-M 1498 7 63 13 6 62 11.91 QM9 130 831 3 29 18 2 5 3.97 Table 5: Real-world dataset statistics. For the graph property regression task ( QM9 ) we augment two models: 1-GNN [ 22] and MPNN [12]. For 1-GNN we use the code and the training setup as provided by the original authors 2. For MPNN we use the reference model implementation from PyTorch Geometric 3. We otherwise follow the training and evaluation procedure used by 1-GNN [22]. The models are trained for 300 epochs and the epoch with the best validation score is chosen. We use PyTorch [24] and PyTorch Geometric [10] for the implementation. All models have been trained on Nvidia Titan RTX GPU (24GB RAM). 2https://github.com/chrsmrrs/k-gnn 3https://github.com/rusty1s/pytorch_geometric/blob/master/examples/qm9_nn_conv.py 23",
      "meta_data": {
        "arxiv_id": "2111.06283v1",
        "authors": [
          "Pál András Papp",
          "Karolis Martinkus",
          "Lukas Faber",
          "Roger Wattenhofer"
        ],
        "published_date": "2021-11-11T15:48:59Z",
        "pdf_url": "https://arxiv.org/pdf/2111.06283v1.pdf"
      }
    },
    {
      "title": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization",
      "abstract": "Graph neural networks (GNNs), which learn the representation of a node by\naggregating its neighbors, have become an effective computational tool in\ndownstream applications. Over-smoothing is one of the key issues which limit\nthe performance of GNNs as the number of layers increases. It is because the\nstacked aggregators would make node representations converge to\nindistinguishable vectors. Several attempts have been made to tackle the issue\nby bringing linked node pairs close and unlinked pairs distinct. However, they\noften ignore the intrinsic community structures and would result in sub-optimal\nperformance. The representations of nodes within the same community/class need\nbe similar to facilitate the classification, while different classes are\nexpected to be separated in embedding space. To bridge the gap, we introduce\ntwo over-smoothing metrics and a novel technique, i.e., differentiable group\nnormalization (DGN). It normalizes nodes within the same group independently to\nincrease their smoothness, and separates node distributions among different\ngroups to significantly alleviate the over-smoothing issue. Experiments on\nreal-world datasets demonstrate that DGN makes GNN models more robust to\nover-smoothing and achieves better performance with deeper GNNs.",
      "full_text": "Towards Deeper Graph Neural Networks with Differentiable Group Normalization Kaixiong Zhou Texas A&M University zkxiong@tamu.edu Xiao Huang The Hong Kong Polytechnic University xhuang.polyu@gmail.com Yuening Li Texas A&M University liyuening@tamu.edu Daochen Zha Texas A&M University daochen.zha@tamu.edu Rui Chen Samsung Research America rui.chen1@samsung.com Xia Hu Texas A&M University xiahu@tamu.edu Abstract Graph neural networks (GNNs), which learn the representation of a node by aggre- gating its neighbors, have become an effective computational tool in downstream applications. Over-smoothing is one of the key issues which limit the performance of GNNs as the number of layers increases. It is because the stacked aggregators would make node representations converge to indistinguishable vectors. Several attempts have been made to tackle the issue by bringing linked node pairs close and unlinked pairs distinct. However, they often ignore the intrinsic community structures and would result in sub-optimal performance. The representations of nodes within the same community/class need be similar to facilitate the classiﬁca- tion, while different classes are expected to be separated in embedding space. To bridge the gap, we introduce two over-smoothing metrics and a novel technique, i.e., differentiable group normalization (DGN). It normalizes nodes within the same group independently to increase their smoothness, and separates node distributions among different groups to signiﬁcantly alleviate the over-smoothing issue. Exper- iments on real-world datasets demonstrate that DGN makes GNN models more robust to over-smoothing and achieves better performance with deeper GNNs. 1 Introduction Graph neural networks (GNNs) [1, 2, 3] have emerged as a promising tool for analyzing networked data, such as biochemical networks [4, 5], social networks [6, 7], and academic networks [8, 9]. The successful outcomes have led to the development of many advanced GNNs, including graph convolu- tional networks [10], graph attention networks [11], and simple graph convolution networks [12]. Besides the exploration of graph neural network variants in different applications, understanding the mechanism and limitation of GNNs is also a crucial task. The core component of GNNs, i.e., a neighborhood aggregator updating the representation of a node iteratively via mixing itself with its neighbors’ representations [6, 13], is essentially a low-pass smoothing operation [14]. It is in line with graph structures since the linked nodes tend to be similar [15]. It has been reported that, as the number of graph convolutional layers increases, all node representations over a graph will converge to indistinguishable vectors, and GNNs perform poorly in downstream applications [16, 17, 18]. It is recognized as an over-smoothing issue. Such an issue prevents GNN models from going deeper to exploit the multi-hop neighborhood structures and learn better node representations. A lot of efforts have been devoted to alleviating the over-smoothing issue, such as regularizing the node distance [ 19], node/edge dropping [ 20, 21], batch and pair normalizations [ 22, 23, 24]. Preprint. Under review. arXiv:2006.06972v1  [cs.LG]  12 Jun 2020Most of existing studies focused on measuring the over-smoothing based on node pair distances. By using these measurements, representations of linked nodes are forced to be close to each other, while unlinked pairs are separated. Unfortunately, the global graph structures and group/community characteristics are ignored, which leads to sub-optimal performance. For example, to perform node classiﬁcation, an ideal solution is to assign similar vectors to nodes in the same class, instead of only the connected nodes. In the citation network Pubmed [25], 36% of unconnected node pairs belong to the same class. These node pairs should instead have a small distance to facilitate node classiﬁcation. Thus, we are motivated to tackle the over-smoothing issue in GNNs from a group perspective. Given the complicated group structures and characteristics, it remains a challenging task to tackle the over-smoothing issue in GNNs. First, the formation of over-smoothing is complex and related to both local node relations and global graph structures, which makes it hard to measure and quantify. Second, the group information is often not directly available in real-world networks. This prevents existing tools such as group normalization being directly applied to solve our problem [ 26]. For example, while the group of adjacent channels with similar features could be directly accessed in convolutional neural networks [27], it is nontrivial to cluster a network in a suitable way. The node clustering needs to be in line with the embeddings and labels, during the dynamic learning process. To bridge the gap, in this paper, we perform a quantitative study on the over-smoothing in GNNs from a group perspective. We aim to answer two research questions. First, how can we precisely measure the over-smoothing in GNNs? Second, how can we handle over-smoothing in GNNs? Through exploring these questions, we make three signiﬁcant contributions as follows. • Present two metrics to quantify the over-smoothing in GNNs: (1) Group distance ratio, clustering the network and measuring the ratio of inter-group representation distance over intra-group one; (2) Instance information gain, treating node instance independently and measuring the input information loss during the low-pass smoothing. • Propose differentiable group normalization to signiﬁcantly alleviate over-smoothing. It softly clusters nodes and normalizes each group independently, which prevents distinct groups from having close node representations to improve the over-smoothing metrics. • Empirically show that deeper GNNs, when equipped with the proposed differentiable group normalization technique, yield better node classiﬁcation accuracy. 2 Quantitative Analysis of Over-smoothing Issue In this work, we use the semi-supervised node classiﬁcation task as an example and illustrate how to handle the over-smoothing issue. A graph is represented byG= {V,E}, where Vand Erepresent the sets of nodes and edges, respectively. Each node v∈V is associated with a feature vector xv ∈Rd and a class label yv. Given a training set Vl accompanied with labels, the goal is to classify the nodes in the unlabeled set Vu = V\\V l via learning the mapping function based on GNNs. 2.1 Preliminaries Following the message passing strategy [ 28], GNNs update the representation of each node via aggregating itself and its neighbors’ representations. Mathematically, at thek-th layer, we have, N(k) v = AGG({a(k) vv′W(k)h(k−1) v′ : v′∈N(v)}), h (k) v = COM(a(k) vv W(k)h(k−1) v ,N(k) v ). (1) N(k) v and h(k) v denote the aggregated neighbor embedding and embedding of nodev, respectively. We initialize h(0) v = xv. N(v) = {v′|ev,v′ ∈E} represents the set of neighbors for node v, where ev,v′ denotes the edge that connects nodes vand v′. W(k) denotes the trainable matrix used to transform the embedding dimension. a(k) vv′ is the link weight over edge ev,v′, which could be determined based on the graph topology or learned by an attention layer. Symbol AGG denotes the neighborhood aggregator usually implemented by a summation pooling. To update nodev, function COM is applied to combine neighbor information and node embedding from the previous layer. It is observed that the weighted average in Eq. (1) smooths node embedding with its neighbors to make them similar. For a full GNN model with Klayers, the ﬁnal node representation is given by hv = h(K) v , which captures the neighborhood structure information within Khops. 22.2 Measuring Over-smoothing with Group Structures In GNNs, the neighborhood aggregation strategy smooths nodes’ representations over a graph [14]. It will make the representations of nodes converge to similar vectors as the number of layersKincreases. This is called the over-smoothing issue, and would cause the performance of GNNs deteriorates as K increases. To address the issue, the ﬁrst step is to measure and quantify the over-smoothing [19, 21]. Measurements in existing work are mainly based on the distances between node pairs [20, 24]. A small distance means that a pair of nodes generally have undistinguished representation vectors, which might triggers the over-smoothing issue. However, the over-smoothing is also highly related to global graph structures, which have not been taken into consideration. For some unlinked node pairs, we would need their representations to be close if they locate in the same class/community, to facilitate the node classiﬁcation task. Without the speciﬁc group information, the metrics based on pair distances may fail to indicate the over- smoothing. Thus, we propose two novel over-smoothing metrics, i.e., group distance ratio and instance information gain. They quantify the over-smoothing from global (communities/classes/groups) and local (node individuals) views, respectively. Deﬁnition 1 (Group Distance Ratio). Suppose that there areCclasses of node labels. We intuitively cluster nodes of the same class label into a group to formulate the labeled node community. Formally, let Li = {hiv}denote the group of representation vectors, where node vis associated with label i. We have a series of labeled groups{L1,··· ,LC}. Group distance ratio RGroup measures the ratio of inter-group distance over intra-group distance in the Euclidean space. We have: RGroup = 1 (C−1)2 ∑ i̸=j( 1 |Li||Lj| ∑ hiv∈Li ∑ hjv′∈Lj ||hiv −hjv′||2) 1 C ∑ i( 1 |Li|2 ∑ hiv,hiv′∈Li ||hiv −hiv′||2) , (2) where ||·|| 2 denotes the L2 norm of a vector and |·| denotes the set cardinality. The numerator (denominator) represents the average of pairwise representation distances between two different groups (within a group). One would prefer to reduce the intra-group distance to make representations of the same class similar, and increase the inter-group distance to relieve the over-smoothing issue. On the contrary, a small RGroup leads to the over-smoothing issue where all groups are mixed together, and the intra-group distance is maintained to hinder node classiﬁcation. Deﬁnition 2 (Instance Information Gain). In an attributed network, a node’s feature decides its class label to some extent. We treat each node instance independently, and deﬁne instance information gain GIns as how much input feature information is contained in the ﬁnal representation. Let Xand Hdenote the random variables of input feature and representation vector, respectively. We deﬁne their probability distributions with PX and PH, and use PXH to denote their joint distribution. GIns measures the dependency between node feature and representation via their mutual information: GIns = I(X; H) = ∑ xv∈X,hv∈H PXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv). (3) We list the details of variable deﬁnitions and mutual information calculation in the context of GNNs in Appendix. With the intensiﬁcation of the over-smoothing issue, nodes average the neighborhood information and lose their self features, which leads to a small value of GIns. 2.3 Illustration of Proposed Over-smoothing Metrics Based on the two proposed metrics, we take simple graph convolution networks (SGC) as an example, and analyze the over-smoothing issue on Cora dataset [ 25]. SGC simpliﬁes the model through removing all the trainable weights between layers to avoid the potential of overﬁtting [12]. So the over-smoothing issue would be the major cause of performance dropping in SGC. As shown by the red lines in Figure 1, the graph convolutions ﬁrst exploit neighborhood information to improve test accuracy up to K = 5, after which the over-smoothing issue starts to worsen the performance. At the same time, instance information gain GIns and group distance ratio RGroup decrease due to the over-smoothing issue. For the extreme case of K = 120, the input features are ﬁltered out and all groups of nodes converge to the same representation vector, leading to GIns = 0 and RGroup = 1, respectively. Our metrics quantify the smoothness of node representations based on group structures, but also have the similar variation tendency with test accuracy to indicate it well. 30 25 50 75 100 125 Layers 0.4 0.6 0.8Accuracy 0 25 50 75 100 125 Layers 0.06 0.08 0.10 0.12Instance gain 0 25 50 75 100 125 Layers 1.5 2.0 2.5 3.0Group distance None batch pair group Figure 1: The test accuracy, instance information gain, and group distance ratio of SGC on Cora. We compare differentiable group normalization with none, batch and pair normalizations. 3 Differentiable Group Normalization We start with a graph-regularized optimization problem [10, 19]. To optimize the over-smoothing metrics of GIns and RGroup, one traditional approach is to minimize the loss function: L= L0 −GIns −λRGroup. (4) L0 denotes the supervised cross-entropy loss w.r.t. representation probability vectors hv ∈RC×1 and class labels. λis a balancing factor. The goal of optimization problem Eq. (4) is to learn node representations close to the input features and informative for their class labels. Considering the labeled graph communities, it also improves the intra-group similarity and inter-group distance. However, it is non-trivial to optimize this objective function due to the non-derivative of non- parametric statistic GIns [29, 30] and the expensive computation of RGroup. 3.1 Proposed Technique for Addressing Over-smoothing Instead of directly optimizing regularized problem in Eq. (4), we propose the differentiable group normalization (DGN) applied between graph convolutional layers to normalize the node embeddings group by group. The key intuition is to cluster nodes into multiple groups and then normalize them independently. Consider the labeled node groups (or communities) in networked data. The node embeddings within each group are expected to be rescaled with a speciﬁc mean and variance to make them similar. Meanwhile, the embedding distributions from different groups are separated by adjusting their means and variances. We develop an analogue with the group normalization in convolutional neural networks (CNNs) [26], which clusters a set of adjacent channels with similar characteristics into a group and treats it independently. Compared with standard CNNs, the challenge in designing DGN is how to cluster nodes in a suitable way. The clustering needs to be in line with the embedding and labels, during the dynamic learning process. We address this challenge by learning a cluster assignment matrix, which softly maps nodes with close embeddings into a group. Under the supervision of training labels, the nodes close in the embedding space tend to share a common label. To be speciﬁc, we ﬁrst describe how DGN clusters and normalizes nodes in a group-wise fashion given an assignment matrix. After that, we discuss how to learn the assignment matrix to support differentiable node clustering. Group Normalization. Let H(k) = [ h(k) 1 ,··· ,h(k) n ]T ∈Rn×d(k) denote the embedding matrix generated from the k-th graph convolutional layer. TakingH(k) as input, DGN softly assigns nodes into groups and normalizes them independently to output a new embedding matrix for the next layer. Formally, we deﬁne the number of groups as G, and denote the cluster assignment matrix by S(k) ∈Rn×G. Gis a hyperparameter that could be tuned per dataset. The i-th column of S(k), i.e., S(k)[:,i], indicates the assignment probabilities of nodes in a graph to the i-th group. Supposing that S(k) has already been computed, we cluster and normalize nodes in each group as follows: H(k) i = S(k)[:,i] ◦H(k) ∈Rn×d(k) ; ˜H(k) i = γi(H(k) i −µi σi ) + βi ∈Rn×d(k) . (5) Symbol ◦denotes the row-wise multiplication. The left part in the above equation represents the soft node clustering for group i, whose embedding matrix is given by H(k) i . The right part performs the standard normalization operation. In particular, µi and σi denote the vectors of running mean 4and standard deviation of group i, respectively, and γi and βi denote the trainable scale and shift vectors, respectively. Given the input embedding H(k) and the series of normalized embeddings {˜H(k) 1 ,··· , ˜H(k) G }, DGN generates the ﬁnal embedding matrix ˜H(k) for the next layer as follows: ˜H(k) = H(k) + λ G∑ i=1 ˜H(k) i ∈Rn×d(k) . (6) λis a balancing factor as mentioned before. Inspecting the loss function in Eq. (4), DGN utilizes components H(k) and ∑G i=1 ˜H(k) i to improve terms GIns and RGroup, respectively. In particular, we preserve the input embedding H(k) to avoid over-normalization and keep the input feature of each node to some extent. Note that the linear combination of H(k) in DGN is different from the skip connection in GNN models [31, 32], which instead connects the embedding output H(k−1) from the last layer. The technique of skip connection could be included to further boost the model performance. Group normalization ∑G i=1 ˜H(k) i rescales the node embeddings within each group independently to make them similar. Ideally, we assign the close node embeddings with a common label to a group. Node embeddings of the group are then distributed closely around the corresponding running mean. Thus for different groups associate with distinct node labels, we disentangle their running means and separates the node embedding distributions. By applying DGN between the successive graph convolutional layers, we are able to optimize Problem (4) to mitigate the over-smoothing issue. Differentiable Clustering. We apply a linear model to compute the cluster assignment matrix S(k) used in Eq. (5). The mathematical expression is given by: S(k) = softmax(H(k)U(k)). (7) U(k) ∈Rd(k)×G denotes the trainable weights for a DGN module applied after the k-th graph convolutional layer. softmax function is applied in a row-wise way to produce the normalized probability vector w.r.t all the Ggroups for each node. Through the inner product between H(k) and U(k), the nodes with close embeddings are assigned to the same group with a high probability. Here we give a simple and effective way to compute S(k). Advanced neural networks could be applied. Time Complexity Analysis. Suppose that the time complexity of embedding normalization at each group is O(T), where T is a constant depending on embedding dimension d(k) and node number n. The time cost of group normalization ∑G i=1 ˜H(k) i is O(GT). Both the differentiable clustering (in Eq. (5)) and the linear model (in Eq. (7)) have a time cost of O(nd(k)G). Thus the total time complexity of a DGN layer is given by O(nd(k)G+ GT), which linearly increases with G. Comparison with Prior Work. To the best of our knowledge, the existing work mainly focuses on analyzing and improving the node pair distance to relieve the over-smoothing issue [19, 21, 24]. One of the general solutions is to train GNN models regularized by the pair distance [19]. Recently, there are two related studies applying batch normalization [22] or pair normalization [24] to keep the overall pair distance in a graph. Pair normalization is a “slim” realization of batch normalization by removing the trainable scale and shift. However, the metric of pair distance and the resulting techniques ignore global graph structure, and may achieve sub-optimal performance in practice. In this work, we measure over-smoothing of GNN models based on communities/groups and independent node instances. We then formulate the problem in Eq. (4) to optimize the proposed metrics, and propose DGN to solve it in an efﬁcient way, which in turn addresses the over-smoothing issue. 3.2 Evaluating Differentiable Group Normalization on Attributed Graphs We apply DGN to the SGC model to validate its effectiveness in relieving the over-smoothing issue. Furthermore, we compare with the other two available normalization techniques used upon GNNs, i.e., batch normalization and pair normalization. As shown in Figure 1, the test accuracy of DGN remains stable with the increase in the number of layers. By preserving the input embedding and normalizing node groups independently, DGN achieves superior performance in terms of instance information gain as well as group distance ratio. The promising results indicate that our DGN tackles the over-smoothing issue more effectively, compared with none, batch and pair normalizations. It should be noted that, the highest accuracy of 79.7% is achieved with DGN when K = 20. This observation contradicts with the common belief that GNN models work best with a few layers on 50 25 50 75 100 125 Layers 0.2 0.4 0.6 0.8Accuracy SGC 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GCN 0 10 20 30 Layers 0.2 0.4 0.6Accuracy GAT None batch pair group Figure 2: The test accuracies of SGC, GCN, and GAT models on Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. current benchmark datasets [33]. With the integration of advanced techniques, such as DGN, we are able to exploit deeper GNN architectures to unleash the power of deep learning in network analysis. 3.3 Evaluation in Scenario with Missing Features To further illustrate that DGN could enable us to achieve better performance with deeper GNN architectures, we apply it to a more complex scenario. We assume that the attributes of nodes in the test set are missing. It is a common scenario in practice [24]. For example, in social networks, new users are often lack of proﬁles and tags [34]. To perform prediction tasks on new users, we would rely on the node attributes of existing users and their connections to new users. In such a scenario, we would like to apply more layers to exploit the neighborhood structure many hops away to improve node representation learning. Since the over-smoothing issue gets worse with the increasing of layer numbers, the beneﬁt of applying normalization will be more obvious in this scenario. We remove the input features of both validation and test sets in Cora, and replace them with zeros [24]. Figure 2 presents the results on three widely-used models, i.e., SGC, graph convolutional networks (GCN), and graph attention networks (GAT). Due to the over-smoothing issue, GNN models without any normalization fail to distinguish nodes quickly with the increasing number of layers. In contrast, the normalization techniques reach their highest performance at larger layer numbers, after which they drop slowly. We observe that DGN obtains the best performance with50, 20, and 8 layers for SGC, GCN, and GAT, respectively. These layer numbers are signiﬁcantly larger than those of the widely-used shallow models (e.g., two or three layers). 4 Experiments We now empirically evaluate the effectiveness and robustness of DGN on real-world datasets. We aim to answer three questions as follows. Q1: Compared with the state-of-the-art normalization methods, can DGN alleviate the over-smoothing issue in GNNs in a better way? Q2: Can DGN help GNN models achieve better performance by enabling deeper GNNs? Q3: How do the hyperparameters inﬂuence the performance of DGN? 4.1 Experiment Setup Datasets. Joining the practice of previous work, we evaluate GNN models by performing the node classiﬁcation task on four datasets: Cora, Citeseer, Pubmed [ 25], and CoauthorCS [35]. We also create graphs by removing features in validation and test sets. The dataset statistics are in Appendix. Implementations. Following the previous settings, we choose the hyperparameters of GNN models and optimizer as follows. We set the number of hidden units to 16 for GCN and GAT models. The number of attention heads in GAT is 1. Since a larger parameter size in GCN and GAT may lead to overﬁtting and affects the study of over-smoothing issue, we compare normalization methods by varying the number of layersKin {1,2,··· ,10,15,··· ,30}. For SGC, we increase the testing range and vary K in {1,5,10,20,··· ,120}. We train with a maximum of 1000 epochs using the Adam optimizer [36] and early stopping. Weights in GNN models are initialized with Glorot algorithm [37]. We use the following sets of hyperparameters for Citeseer, Cora, CoauthorCS: 0.6 (dropout rate), 5 ·10−4 (L2 regularization), 5 ·10−3 (learning rate), and for Pubmed: 0.6 (dropout rate), 1 ·10−3 (L2 regularization), 1 ·10−2 (learning rate). We run each experiment 5 times and report the average. 6Table 1: Test accuracy in percentage on attributed networks. Layers a/bdenote the layer number ain GCN & GAT and that of bin SGC. #Kdenotes the optimal layer numbers where DGN achieves the highest performance. Dataset Model Layers 2/5 Layers 15/60 Layers 30/120 #KNN BN PN DGN NN BN PN DGN NN BN PN DGN Cora GCN 82.2 73.9 71 .0 82 .0 18.1 70 .3 67 .2 75.2 13.1 67 .2 64 .3 73.2 2 GAT 80.9 77 .8 74 .4 81.1 16.8 33 .1 49 .6 71.8 13.0 25 .0 30 .2 51.3 2 SGC 75.8 76 .3 75 .4 77.9 29.4 72 .1 71 .7 77.8 25.1 51 .2 65 .5 73.7 20 Citeseer GCN 70.6 51.3 60 .5 69 .5 15.2 46 .9 46 .7 53.1 9.4 47 .9 47 .1 52.6 2 GAT 70.2 61.5 62 .0 69 .3 22.6 28 .0 41 .4 52.6 7.7 21 .4 33 .3 45.6 2 SGC 69.6 58.8 64 .8 69 .5 66.3 50.5 65 .0 63 .4 60.8 47 .3 63 .1 64.7 30 Pubmed GCN 79.3 74 .9 71 .1 79.5 22.5 73 .7 70 .6 76.1 18.0 70 .4 70 .4 76.9 2 GAT 77.8 76.2 72 .4 77 .5 37.5 56 .2 68 .8 75.9 18.0 46 .6 58 .2 73.3 5 SGC 71.5 76 .5 75 .8 76.8 34.2 75 .2 77 .1 77.4 23.1 71 .6 76 .7 77.1 10 Coauthors GCN 92.3 86 .0 77 .8 92.3 72.2 78 .5 69 .5 83.7 3.3 84.7 64.5 84 .4 1 GAT 91.5 89 .4 85 .9 91.8 6.0 77 .7 53 .1 84.5 3.3 16 .7 48 .1 75.5 1 SGC 89.9 88 .7 86 .0 90.2 10.2 59 .7 76 .4 81.3 5.8 30 .5 52 .6 60.8 1 Baselines. We compare with none normalization (NN), batch normalization (BN) [22, 23] and pair normalization (PN) [24]. Their technical details are listed in Appendix. DGN Conﬁgurations. The key hyperparameters include group number G and balancing factor λ. Depending on the number of class labels, we apply 5 groups to Pubmed and 10 groups to the others. The criterion is to use more groups to separate representation distributions in networked data accompanied with more class labels. λis tuned on validation sets to ﬁnd a good trade-off between preserving input features and group normalization. We introduce the selection of λin Appendix. 4.2 Experiment Results Studies on alleviating the over-smoothing problem.To answerQ1, Table 1 summarizes the results of applying different normalization techniques to GNN models on all datasets. We report the performance of GCN and GAT with 2/15/30 layers, and SGC with 5/60/120 layers due to space limit. We provide test accuracies, instance information gain and group distance ratio under all depths in Appendix. It can be observed that DGN has signiﬁcantly alleviated the over-smoothing issue. Given the same layers, DGN almost outperforms all other normalization methods for all cases and greatly slows down the performance dropping. It is because the self-preserved component H(k) in Eq. (6) keeps the informative input features and avoids over-normalization to distinguish different nodes. This component is especially crucial for models with a few layers since the over-smoothing issue has not appeared. The other group normalization component in Eq. (6) processes each group of nodes independently. It disentangles the representation similarity between groups, and hence reduces the over-smoothness of nodes over a graph accompanied with graph convolutions. Studies on enabling deeper and better GNNs. To answer Q2, we compare all of the concerned normalization methods over GCN, GAT, and SGC in the scenario with missing features. As we have discussed, normalization techniques will show their power in relieving the over-smoothing issue and exploring deeper architectures especially for this scenario. In Table 2, Acc represents the best test accuracy yielded by model equipped with the optimal layer number #K. We can observe that DGN signiﬁcantly outperforms the other normalization methods on all cases. The average improvements over NN, BN and PN achieved by DGN are 37.8%, 7.1% and 12.8%, respectively. Compared with vanilla GNN models without any normalization layer, the optimal models accompanied with normalization layers (especially for our DGN) usually possess larger values of #K. It demonstrates that DGN enables to explore deeper architectures to exploit neighborhood information with more hops away by tackling the over-smoothing issue. We present the comprehensive analyses in terms of test accuracy, instance information gain and group distance ratio under all depths in Appendix. Hyperparameter studies. We study the impact of hyperparameters, group number Gand balancing factor λ, on DGN in order to answer research question Q3. Over the GCN framework associated with 20 convolutional layers, we evaluate DGN by considering Gand λfrom sets [1,5,10,15,20,30] and [0.001,0.005,0.01,0.03,0.05,0.1], respectively. The left part in Figure 3 presents the test accuracy 7Table 2: The highest accuracy (%) and the accompanied optimal layers in the scenario with missing features. We calculate the average improvement achieved by DGN over each GNN framework. Model Norm Cora Citeseer Pubmed CoauthorCS Improvement%Acc #K Acc #K Acc #K Acc #K GCN NN 57.3 3 44.0 6 36.4 4 67.3 3 42.2 BN 71.8 20 45.1 25 70.4 30 82.7 30 5.2 PN 65.6 20 43.6 25 63.1 30 63.5 4 19.2 DGN 76.3 20 50.2 30 72.0 30 83.7 25 - GAT NN 50.1 2 40.8 4 38.5 4 63.7 3 51.0 BN 72.7 5 48.7 5 60.7 4 80.5 6 9.8 PN 68.8 8 50.3 6 63.2 20 66.6 3 14.7 DGN 75.8 8 54.5 5 72.3 20 83.6 15 - SGC NN 63.4 5 51.2 40 63.7 5 71.0 5 20.1 BN 78.5 20 50.4 20 72.3 50 84.4 20 6.2 PN 73.4 50 58.0 120 75.2 30 80.1 10 4.5 DGN 80.2 50 58.2 90 76.2 90 85.8 20 - 0.2 0.1 0.3 0.4 0.5 0.08 0.6 30 0.7 0.06 25 0.8 20 0.04  15 100.02 5 0 0 Figure 3: Left: Test accuracies of GCN with 20 layers on Cora with missing features, where hyperparameters Gand λare studied. Middle: Node representation visualization for GCN without normalization and with K = 20. Right: Node representation visualization for GCN with DGN layer and K = 20 (node colors represent classes, and black triangles denote the running means of groups). for each hyperparameter combination. We observe that: (i) The model performance is damaged greatly when λis close to zero (e.g.,λ= 0.001). In this case, group normalization contributes slightly in DGN, resulting in over-smoothing in the GCN model. (ii) Model performance is not sensitive to the value of G, and an appropriate λvalue could be tuned to optimize the trade-off between instance gain and group normalization. It is because DGN learns to use the appropriate number of groups by end-to-end training. In particular, some groups might not be used as shown in the right part of Figure 3, at which only 6 out of 10 groups (denoted by black triangles) are adopted. (iii) Even when G= 1, DGN still outperforms BN by utilizing the self-preserved component to achieve an accuracy of 74.7%, where λ= 0.1. Via increasing the group number, the model performance could be further improved, e.g., the accuracy of 76.3% where G= 10 and λ= 0.01. Node representation visualization. We investigate how DGN clusters nodes into different groups to tackle the over-smoothing issue. The middle and right parts of Figure 3 visualize the node representations achieved by GCN models without normalization tool and with the DGN approach, respectively. It is observed that the node representations of different classes mix together when the layer number reaches 20 in the GCN model without normalization. In contrast, our DGN method softly assigns nodes into a series of groups, whose running means at the corresponding normalization modules are highlighted with black triangles. Through normalizing each group independently, the running means are separated to improve inter-group distances and disentangle node representations. In particular, we notice that the running means locate at the borders among different classes (e.g., the upper-right triangle at the border between red and pink classes). That is because the soft assignment may cluster nodes of two or three classes into the same group. Compared with batch or pair normalization, the independent normalization for each group only includes a few classes in DGN. In this way, we relieve the representation noise from other node classes during normalization, and improve the group distance ratio as illustrated in Appendix. 85 Conclusion In this paper, we propose two over-smoothing metrics based on graph structures, i.e., group distance ratio and instance information gain. By inspecting GNN models through the lens of these two metrics, we present a novel normalization layer, DGN, to boost model performance against over- smoothing. It normalizes each group of similar nodes independently to separate node representations of different classes. Experiments on real-world classiﬁcation tasks show that DGN greatly slowed down performance degradation by alleviating the over-smoothing issue. DGN enables us to explore deeper GNNs and achieve higher performance in analyzing attributed networks and the scenario with missing features. Our research will facilitate deep learning models for potential graph applications. Broader Impact The successful outcome of this work will lead to advances in building up deep graph neural networks and dealing with complex graph-structured data. The developed metrics and algorithms have an immediate and strong impact on a number of ﬁelds, including (1) Over-smoothing Quantitative Analysis: GNN models tend to result in the over-smoothing issue with the increase in the number of layers. During the practical development of deeper GNN models, the proposed instance information gain and group distance ratio effectively indicate the over-smoothing issue, in order to push the model exploration toward a good direction. (2) Deep GNN Modeling: The proposed differentiable group normalization tool successfully tackles the over-smoothing issue and enables the modeling of deeper GNN variants. It encourages us to fully unleash the power of deep learning in processing the networked data. (3) Real-world Network Analytics Applications: The proposed research will broadly shed light on utilizing deep GNN models in various applications, such as social network analysis, brain network analysis, and e-commerce network analysis. For such complex graph-structured data, deep GNN models can exploit the multi-hop neighborhood information to boost the task performance. References [1] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008. [2] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. [3] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A comprehensive survey on graph neural networks. arXiv, 2019. [4] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeuIPS, pages 2224–2232, 2015. [5] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [6] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeuIPS, pages 1024–1034, 2017. [7] Xiao Huang, Qingquan Song, Yuening Li, and Xia Hu. Graph recurrent networks with at- tributed random walks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 732–740, 2019. [8] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1416–1424, 2018. [9] Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019. [10] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. ICLR, 2017. [11] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv, 1(2), 2017. 9[12] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Simplifying graph convolutional networks. arXiv preprint arXiv:1902.07153, 2019. [13] Kaixiong Zhou, Qingquan Song, Xiao Huang, Daochen Zha, Na Zou, and Xia Hu. Multi-channel graph neural networks. arXiv preprint arXiv:1912.08306, 2019. [14] Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ﬁlters. arXiv preprint arXiv:1905.09550, 2019. [15] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415–444, 2001. [16] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018. [17] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In International Conference on Learning Representations, 2020. [18] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou. Specae: Spectral autoencoder for anomaly detection in attributed networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2233–2236, 2019. [19] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. arXiv preprint arXiv:1909.03211, 2019. [20] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Representations. https://openreview. net/forum, 2020. [21] Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming- Chang Yang. Measuring and improving the use of graph information in graph neural networks, 2020. [22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. [23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015. [24] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint arXiv:1909.12223, 2019. [25] Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861, 2016. [26] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018. [27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [28] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1263–1272. JMLR. org, 2017. [29] Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances. Entropy, 19(7):361, 2017. [30] Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. Entropy, 21(12):1181, 2019. [31] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE International Conference on Computer Vision, pages 9267–9276, 2019. [32] Nezihe Merve Gürel, Hansheng Ren, Yujing Wang, Hui Xue, Yaming Yang, and Ce Zhang. An anatomy of graph neural networks going deep via the lens of mutual information: Exponential decay vs. full preservation. arXiv preprint arXiv:1910.04499, 2019. 10[33] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018. [34] Al Mamunur Rashid, George Karypis, and John Riedl. Learning preferences of new users in recommender systems: an information theoretic approach. Acm Sigkdd Explorations Newsletter, 10(2):90–100, 2008. [35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. [36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, 2014. [37] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedfor- ward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256, 2010. 11A Dataset Statistics For fair comparison with previous work, we perform the node classiﬁcation task on four benchmark datasets, including Cora, Citeseer, Pubmed [ 25], and CoauthorCS [ 35]. They have been widely adopted to study the over-smoothing issue in GNNs [21, 19, 24, 16, 20]. The detailed statistics are listed in Table 3. To further illustrate that the normalization techniques could enable deeper GNNs to achieve better performance, we apply them to a more complex scenario with missing features. For these four benchmark datasets, we create the corresponding scenarios by removing node features in both validation and testing sets. Table 3: Dataset statistics on Cora, Citeseer, Pubmed, and CoauthorCS. Cora Citeseer Pubmed CoauthorCS #Nodes 2708 3327 19717 18333 #Edges 5429 4732 44338 81894 #Features 1433 3703 500 6805 #Classes 7 6 3 15 #Training Nodes 140 120 60 600 #Validation Nodes 500 500 500 2250 #Testing Nodes 1000 1000 1000 15483 B Running Environment All the GNN models and normalization approaches are implemented in PyTorch, and tested on a machine with 24 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GB processors, GeForce GTX-1080 Ti 12 GB GPU, and 128GB memory size. We implement the group normalization in a parallel way. Thus the practical time cost of our DGN is comparable to that of traditional batch normalization. C GNN Models We test over three general GNN models to illustrate the over-smoothing issue, including graph convo- lutional networks (GCN) [10], graph attention networks (GAT) [11] and simple graph convolution (SGC) networks [12]. We list their neighbor aggregation functions in Table 4. Table 4: Neighbor aggregation function at a graph convolutional layer for GCN, GAT and SGC. Model Neighbor aggregation function GCN h(k) v = ReLU(∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) W(k)h(k−1) v′ ) GAT h(k) v = ReLU(∑ v′∈N(v)∪{v}a(k) vv′W(k)h(k−1) v′ ) SGC h(k) v = ∑ v′∈N(v)∪{v} 1√ (|N(v)|+1)·(|N(v′)|+1) h(k−1) v′ Considering the message passing strategy as shown by Eq. (1) in the main manuscript, we explain the key properties of GCN, GAT and SGC as follows. GCN merges the information from node itself and its neighbors weighted by vertices’ degrees, wherea(k) vv′ = 1./ √ (|N(v)|+ 1) ·(|N(v′)|+ 1). Functions AGG and COM are realized by a summation pooling. The activation function of ReLU is then applied to non-linearly transform the latent embedding. Based on GCN, GAT uses an additional attention layer to learn link weight a(k) vv′. GAT aggregates neighbors with the trainable link weights, and achieves signiﬁcant improvements in a variety of applications. SGC is simpliﬁed from GCN by removing all trainable parameters W(k) and nonlinear activations between successive layers. It has been empirically shown that these simpliﬁcations do not negatively impact classiﬁcation accuracy, and even relive the problems of over-ﬁtting and vanishing gradients in deeper models. 12D Normalization Baselines Batch normalization is ﬁrst applied between the successive convolutional layers in CNNs [23]. It is extended to graph neural networks to improve node representation learning and generalization [22]. Taking embedding matrix H(k) as input after each layer, batch normalization scales the node rep- resentations using running mean and variance, and generates a new embedding matrix for the next graph convolutional layer. Formally, we have: ˜H(k) = γ(H(k) −µ σ ) + β ∈Rn×d(k) . µand σdenote the vectors of running mean and standard deviation, respectively; γ and β denote the trainable scale and shift vectors, respectively. Recently, pair normalization has been proposed to tackle the over-smoothing issue in GNNs, targeting at maintaining the average node pair distance over a graph [24]. Pair normalization is a simplifying realization of batch normalization by removing the trainable γ and β. In this work, we augment each graph convolutional layer via appending a normalization module, in order to validate the effectiveness of normalization technique in relieving over-smoothing and enabling deeper GNNs. E Hyperparameter Tuning in DGN The balancing factor, λ, is crucial to determine the trade-off between input feature preservation and group normalization in DGN. It needs to be tuned carefully as GNN models increase the number of layers. To be speciﬁc, we consider the candidate set {5 ·10−4,1 ·10−3,2 ·10−3,3 ·10−3,5 · 10−3,1 ·10−2,2 ·10−2,3 ·10−2,5 ·10−2}. For each speciﬁc model, we use a few epochs to choose the optimal λon the validation set, and then evaluate it on the testing set. We observe that the value of λtends to be larger in the model accompanied with more graph convolutional layers. That is because the over-smoothing issue gets worse with the increase in layer number. The group normalization is much more required to separate the node representations of different classes. F Instance Information Gain In this work, we adopt kernel-density estimators (KDE), one of the common non-parametric ap- proaches, to estimate the mutual information between input feature and representation vector [29, 30]. A key assumption in KDE is that the input feature (or output representation vector) of neural networks is distributed as a mixture of Gaussians. Since a neural network is a deterministic function of the input feature after training, the mutual information would be inﬁnite without such assumption. In the following, we ﬁrst formally deﬁne the Gaussian assumption, input probability distribution and representation probability distribution, and then present how to obtain the instance information gain based on the mutual information metric. Gaussian assumption. In the graph signal processing, it is common to assume that the collected input feature contains both true signal and noise. In other word, we have the input feature as follows: xv = ¯xv + ϵx. ¯xv denotes the true value, and ϵx ∼N(0,σ2I) denotes the added Gaussian noise with variance σ2. Therefore, input feature xv is a Gaussian variable centered on its true value. Input probability distribution. We treat the empirical distribution of input samples as true distri- bution. Given a dataset accompanied with nsamples, we have a series of input features{x1,··· ,xn} for all the samples. Each node feature is sampled with probability 1/|V|following the empirical uniform distribution. Let |V|denotes the number of samples, and let Xdenote the random variable of input features. Based on the above Gaussian assumption, probability PX(xv) of input feature xv is obtained by the product of 1/|V|with Gaussian probability centered on true value ¯xv. Representation probability distribution. Let Hdenote the random variable of node represen- tations. To obtain probability PH(hv) of continuous vector hv, a general approach is to bin and transform Hinto a new discrete variable. However, with the increasing dimensions of hv, it is non-trivial to statistically count the frequencies of all possible discrete values. Considering the task of node classiﬁcation, the index of largest element along vector hv ∈RC×1 is regarded as the label 13of a node. We propose a new binning approach that labels the whole vector hv with the largest index zv. In this way, we only have Cclasses of discrete values to facilitate the frequency counting. To be speciﬁc, let Pc denote the number of representation vectors whose indexes zv = c. The probability of a discrete variable with class cis given by: pc = PH(zv = c) = Pc∑C l=1 Pl . Mutual information calculation. Based on KDE approach, a lower bound of mutual information between input feature and representation vector can be calculated as: GIns = I(X; H) = ∑ xv∈X,hv∈HPXH(xv,hv) log PXH(xv,hv) PX(xv)PH(hv) = H(X) −H(X|H) ≥− 1 |V| ∑ ilog 1 |V| ∑ jexp(−1 2 ||xi−xj||2 2 4σ2 ) −∑C c=1 pc[−1 Pc ∑ i,zi=clog 1 Pc ∑ j,zj=cexp(−1 2 ||xi−xj||2 2 4σ2 )]. The sum over i,zi = crepresents a summation over all the input features whose representation vectors are labeled with zi = c. PXH(xv,hv) denotes the joint probability of xv and hv. The effectiveness of GIns in measuring mutual information between input feature and node representation has been demonstrated in the experimental results. As illustrated in Figures 4-7, GIns decreases with the increasing number of graph convolutional layers. This practical observation is in line with the human expert knowledge about neighbor aggregation strategy in GNNs. The neighbor aggregation function as shown in Table 4 is in fact a low-passing smoothing operation, which mixes the input feature of a node with those of its neighbors gradually. At the extreme cases where K = 30 or 120, we ﬁnd that GIns approaches to zero in GNN models without normalization. The loss of informative input feature leads to the dropping of node classiﬁcation accuracy. However, our DGN keeps the input information during graph convolutions and normalization to some extent, resulting in the largest GIns compared with the other normalization approaches. G Performance Comparison on Attributed Graphs In this section, we report the model performances in terms of test accuracy, instance information gain and group distance ratio achieved on all the concerned datasets in Figures 4-7. We make the following observations: • Comparing with other normalization techniques, our DGN generally slows down the dropping of test accuracy with the increase in layer number. Even for GNN models associated with a small number of layers (i.e., G≤5), DGN achieves the competitive performance compared with none normalization. The adoption of DGN module does not damage the model performance, and prevents model from suffering over-smoothing issue when GNN goes deeper. • DGN achieves the larger or comparable instance information gains in all cases, especially for GAT models. That is because DGN keeps embedding matrix H(k) and prevents over-normalization within each group. The preservation of H(k) saves input features to some extent after each layer of graph convolutions and normalization. In an attributed graph, the improved preservation of informative input features in the ﬁnal representations will signiﬁcantly facilitate the downstream node classiﬁcation. Furthermore, such preservation is especially crucial for GNN models with a few layers, since the over-smoothing issue has not appeared. • DGN normalizes each group of node representations independently to generally improve the group distance ratio, especially for models GCN and GAT. A larger value of group distance ratio means that the node representation distributions from all groups are disentangled to address the over-smoothing issue. Although the ratios of DGN are smaller than those of pair normalization in some cases upon SGC framework, we still achieve the largest test accuracy. That may be because the intra-group distance in DGN is much smaller than that of pair normalization. A small value of intra-group distance would facilitate the node classiﬁcation within the same group. We will further compare the intra-group distance in scenarios with missing features in the following experiments. 140 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 0.10 Instance gain None batch pair group 0 10 20 30 1 2 3 4 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0.10 0 10 20 30 1 2 3 4 0 50 100 Layers 0.4 0.6 0.8SGC 0 50 100 Layers 0.050 0.075 0.100 0.125 0 50 100 Layers 2 3 Figure 4: The test accuracy, instance information gain, and group distance ratio in attributed Cora. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 0.000 0.025 0.050 0.075 Instance gain None batch pair group 0 10 20 30 1.0 1.5 2.0 Group ratio 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 0.000 0.025 0.050 0.075 0 10 20 30 1.0 1.5 2.0 0 50 100 Layers 0.5 0.6 0.7SGC 0 50 100 Layers 0.06 0.08 0 50 100 Layers 1.25 1.50 1.75 2.00 Figure 5: The test accuracy, instance information gain, and group distance ratio in attributed Citeseer. We compare differentiable group normalization with none, batch and pair normalizations. 150 10 20 30 0.2 0.4 0.6 0.8GCN Test accuracy 0 10 20 30 0.00 0.05 Instance gain None batch pair group 0 10 20 30 1 2 Group ratio 0 10 20 30 0.2 0.4 0.6 0.8GAT 0 10 20 30 0.00 0.05 0 10 20 30 1.0 1.5 2.0 2.5 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 0.025 0.050 0.075 0.100 0 50 100 Layers 1.5 2.0 Figure 6: The test accuracy, instance information gain, and group distance ratio in attributed Pubmed. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 0.0 0.1 0.2 0.3 Instance gain None batch pair group 0 10 20 30 2 4 6 Group ratio 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 0.0 0.1 0.2 0.3 0 10 20 30 2 4 6 8 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 0.0 0.1 0.2 0.3 0 50 100 Layers 2 4 6 Figure 7: The test accuracy, instance information gain, and group distance ratio in attributed Coau- thorCS. We compare differentiable group normalization with none, batch and pair normalizations. 16H Performance Comparison in Scenarios with Missing Features In this section, we report the model performances in terms of test accuracy, group distance ratio and intra-group distance achieved in scenarios with missing features in Figures 8-11. The intra-group distance is calculated by node pair distance averaged within the same group. Its mathematical expression is given by the denominator of Equation (3) in the main manuscript. We make the following observations: • DGN achieves the largest test accuracy by exploring the deeper neural architecture with a larger number of graph convolutional layers. In the scenarios with missing features, GNN model relies highly on the neighborhood structure to classify nodes. DGN enables the deeper GNN model to exploit neighborhood structure with multiple hops away, and at the same time relieves the over-smoothing issue. • Comparing with other normalization techniques, DGN generally improves the group distance ratio to relieve over-smoothing issue. Although in some cases the ratios are smaller than those of pair normalization upon SGC framework, we still achieve the comparable or even better test accuracy. That is because DGN has a smaller intra-group distance to facilitate node classiﬁcation within the same group, which is analyzed in the followings. • DGN obtains an appropriate intra-group distance to optimize the node classiﬁcation task. While the over-smoothing issue results in an extremely-small distance in the model without normalization, a larger one in pair normalization leads to the inaccurate node classiﬁcation within each group. That is because the pair normalization is designed to maintain the distance between each pair of nodes, no matter whether they locate in the same class group or not. The divergence of node representations in a group prevents a downstream classiﬁer to assign them the same class label. 170 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1 2 3 Group ratio None batch pair group 0 10 20 30 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1 2 3 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6 0.8SGC 0 50 100 Layers 2 3 0 50 100 Layers 0.5 1.0 Figure 8: The test accuracy, group distance ratio and intra-group distance in Cora with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.2 0.4GCN Test accuracy 0 10 20 30 1.1 1.2 1.3 Group ratio None batch pair group 0 10 20 30 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4GAT 0 10 20 30 1.0 1.2 1.4 0 10 20 30 0.0 0.5 1.0 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.2 1.4 0 50 100 Layers 0.5 1.0 Figure 9: The test accuracy, group distance ratio and intra-group distance in Citeseer with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 180 10 20 30 0.2 0.4 0.6GCN Test accuracy 0 10 20 30 1.0 1.2 1.4 1.6 Group ratio None batch pair group 0 10 20 30 0.0 0.5 1.0 Intra-group distance 0 10 20 30 0.2 0.4 0.6GAT 0 10 20 30 1.00 1.25 1.50 1.75 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.2 0.4 0.6SGC 0 50 100 Layers 1.0 1.5 2.0 0 50 100 Layers 0.0 0.5 1.0 Figure 10: The test accuracy, group distance ratio and intra-group distance in Pubmed with missing features. We compare differentiable group normalization with none, batch and pair normalizations. 0 10 20 30 0.00 0.25 0.50 0.75GCN Test accuracy 0 10 20 30 2 3 4 Group ratio None batch pair group 0 10 20 30 0.00 0.25 0.50 0.75 Intra-group distance 0 10 20 30 0.00 0.25 0.50 0.75GAT 0 10 20 30 2 3 4 0 10 20 30 0.00 0.25 0.50 0.75 0 50 100 Layers 0.25 0.50 0.75SGC 0 50 100 Layers 2 4 0 50 100 Layers 0.0 0.5 1.0 Figure 11: The test accuracy, group distance ratio and intra-group distance in CoauthorCS with missing features. We compare differentiable group normalization with none, batch and pair normal- izations. 19",
      "meta_data": {
        "arxiv_id": "2006.06972v1",
        "authors": [
          "Kaixiong Zhou",
          "Xiao Huang",
          "Yuening Li",
          "Daochen Zha",
          "Rui Chen",
          "Xia Hu"
        ],
        "published_date": "2020-06-12T07:18:02Z",
        "pdf_url": "https://arxiv.org/pdf/2006.06972v1.pdf"
      }
    },
    {
      "title": "A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks",
      "abstract": "Oversmoothing is a central challenge of building more powerful Graph Neural\nNetworks (GNNs). While previous works have only demonstrated that oversmoothing\nis inevitable when the number of graph convolutions tends to infinity, in this\npaper, we precisely characterize the mechanism behind the phenomenon via a\nnon-asymptotic analysis. Specifically, we distinguish between two different\neffects when applying graph convolutions -- an undesirable mixing effect that\nhomogenizes node representations in different classes, and a desirable\ndenoising effect that homogenizes node representations in the same class. By\nquantifying these two effects on random graphs sampled from the Contextual\nStochastic Block Model (CSBM), we show that oversmoothing happens once the\nmixing effect starts to dominate the denoising effect, and the number of layers\nrequired for this transition is $O(\\log N/\\log (\\log N))$ for sufficiently\ndense graphs with $N$ nodes. We also extend our analysis to study the effects\nof Personalized PageRank (PPR), or equivalently, the effects of initial\nresidual connections on oversmoothing. Our results suggest that while PPR\nmitigates oversmoothing at deeper layers, PPR-based architectures still achieve\ntheir best performance at a shallow depth and are outperformed by the graph\nconvolution approach on certain graphs. Finally, we support our theoretical\nresults with numerical experiments, which further suggest that the\noversmoothing phenomenon observed in practice can be magnified by the\ndifficulty of optimizing deep GNN models.",
      "full_text": "A NON -ASYMPTOTIC ANALYSIS OF OVERSMOOTHING IN GRAPH NEURAL NETWORKS Xinyi Wu1,2, Zhengdao Chen3*, William Wang2, and Ali Jadbabaie1,2 1Institute for Data, Systems, and Society (IDSS), Massachusetts Institute of Technology 2Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology 3Courant Institute of Mathematical Sciences, New York University ABSTRACT Oversmoothing is a central challenge of building more powerful Graph Neural Networks (GNNs). While previous works have only demonstrated that oversmoothing is inevitable when the number of graph convolutions tends to inﬁnity, in this paper, we precisely characterize the mechanism behind the phenomenon via a non-asymptotic analysis. Speciﬁcally, we distinguish between two different effects when applying graph convolutions—an undesirable mixing effect that homogenizes node representations in different classes, and a desirable denoising effect that homogenizes node representations in the same class. By quantifying these two effects on random graphs sampled from the Contextual Stochastic Block Model (CSBM), we show that oversmoothing happens once the mixing effect starts to dominate the denoising effect, and the number of layers required for this transition is O(log N/log(log N)) for sufﬁciently dense graphs with N nodes. We also extend our analysis to study the effects of Personalized PageRank (PPR), or equivalently, the effects of initial residual connections on oversmoothing. Our results suggest that while PPR mitigates oversmoothing at deeper layers, PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs. Finally, we support our theoretical results with numerical experiments, which further suggest that the oversmoothing phenomenon observed in practice can be magniﬁed by the difﬁculty of optimizing deep GNN models. 1 Introduction Graph Neural Networks (GNNs) are a powerful framework for learning with graph-structured data and have shown great promise in diverse domains such as molecular chemistry, physics, and social network analysis [1, 2, 3, 4, 5, 6, 7]. Most GNN models are built by stacking graph convolutions or message-passing layers [8], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes. The most representative and popular example is the Graph Convolutional Network (GCN) [9], which has demonstrated success in node classiﬁcation, a primary graph task which asks for node labels and identiﬁes community structures in real graphs. Despite these achievements, the choice of depth for these GNN models remains an intriguing question. GNNs often achieve optimal classiﬁcation performance when networks are shallow. Many widely used GNNs such as the GCN are no deeper than 4 layers [9, 10], and it has been observed that for deeper GNNs, repeated message-passing makes node representations in different classes indistinguishable and leads to lower node classiﬁcation accuracy—a phenomenon known as oversmoothing [9, 11, 12, 10, 13, 14, 15, 16]. Through the insight that graph convolutions can be regarded as low-pass ﬁlters on graph signals, prior studies have established that oversmoothing is inevitable when the number of layers in a GNN increases to inﬁnity [11, 13]. However, these asymptotic analyses do not fully explain the rapid occurrence of oversmoothing when we increase the network depth, let alone the fact that for some datasets, having no graph convolution is even optimal [17]. These observations motivate the following key questions about oversmoothing in GNNs: Why does oversmoothing happen at a relatively shallow depth? Can we quantitatively model the effect of applying a ﬁnite number of graph convolutions and theoretically predict the “sweet spot” for the choice of depth? In this paper, we propose a non-asymptotic analysis framework to study the effects of graph convolutions and oversmoothing using the Contextual Stochastic Block Model (CSBM) [18]. The CSBM mimics the community structure Correspondence to: xinyiwu@mit.edu *Now at Google. arXiv:2212.10701v2  [cs.LG]  1 Mar 2023A B Figure 1: Illustration of how oversmoothing happens. Stacking GNN layers will increase both the mixing and denoising effects counteracting each other. Depending on the graph characteristics, either the denoising effect dominates the mixing effect, resulting in less difﬁculty classifying nodes ( A), or the mixing effect dominates the denoising effect, resulting in more difﬁculty classifying nodes (B)—this is when oversmoothing starts to happen. of real graphs and enables us to evaluate the performance of linear GNNs through the probabilistic model with ground truth community labels. More importantly, as a generative model, the CSBM gives us full control over the graph structure and allows us to analyze the effect of graph convolutions non-asymptotically. In particular, we distinguish between two counteracting effects of graph convolutions: • mixing effect (undesirable): homogenizing node representations in different classes; • denoising effect (desirable): homogenizing node representations in the same class. Adding graph convolutions will increase both the mixing and denoising effects. As a result, oversmoothing happens not just because the mixing effect keeps accumulating as the depth increases, on which the asymptotic analyses are based [11, 13], but rather because the mixing effect starts to dominate the denoising effect (see Figure 1 for a schematic illustration). By quantifying both effects as a function of the model depth, we show that the turning point of the tradeoff between the two effects is O(log N/log(log N)) for graphs with N nodes sampled from the CSBM in sufﬁciently dense regimes. Besides new theory, this paper also presents numerical results directly comparing theoretical predictions and empirical results. This comparison leads to new insights highlighting the fact that the oversmoothing phenomenon observed in practice is often a mixture of pure oversmoothing and difﬁculty of optimizing weights in deep GNN models. In addition, we apply our framework to analyze the effects of Personalized PageRank (PPR) on oversmoothing. Personalized propagation of neural predictions (PPNP) and its approximate variant (APPNP) make use of PPR and its approximate variant, respectively, and were proposed as a solution to mitigate oversmoothing while retaining the ability to aggregate information from larger neighborhoods in the graph [12]. We show mathematically that PPR makes the model performance more robust to increasing number of layers by reducing the mixing effect at each layer, while it nonetheless reduces the desirable denoising effect at the same time. For graphs with a large size or strong community structure, the reduction of the denoising effect would be greater than the reduction of the mixing effect and thus PPNP and APPNP would perform worse than the baseline GNN on those graphs. Our contributions are summarized as follows: • We show that adding graph convolutions strengthens the denoising effect while exacerbates the mixing effect. Over- smoothing happens because the mixing effect dominates the denoising effect beyond a certain depth. For sufﬁciently dense CSBM graphs with N nodes, the required number of layers for this to happen is O(log N/log(log N)). • We apply our framework to rigorously characterize the effects of PPR on oversmoothing. We show that PPR reduces both the mixing effect and the denoising effect of message-passing and thus does not necessarily improve node classiﬁcation performance. • We verify our theoretical results in experiments. Through comparison between theory and experiments, we ﬁnd that the difﬁculty of optimizing weights in deep GNN architectures often aggravates oversmoothing. 2 Additional Related Work Oversmoothing problem in GNNs Oversmoothing is a well-known issue in deep GNNs, and many techniques have been proposed to relieve it practically [19, 20, 15, 21, 22]. On the theory side, by viewing GNN layers as a form of Laplacian ﬁlter, prior works have shown that as the number of layers goes to inﬁnity, the node representations within each connected component of the graph will converge to the same values [11, 13]. However, oversmoothing can be observed in GNNs with as few as 2 −4 layers [9, 10]. The early onset of oversmoothing renders it an important concern in practice, and it has not been satisfyingly explained by the previous asymptotic studies. Our work addresses this 2gap by quantifying the effects of graph convolutions as a function of model depth and justifying why oversmoothing happens in shallow GNNs. A recent study shared a similar insight of distinguishing between two competing effects of message-passing and showed the existence of an optimal number of layers for node prediction tasks on a latent space random graph model. But the result had no further quantiﬁcation and hence the oversmoothing phenomemon was still only characterized asymptotically [16]. Analysis of GNNs on CSBMs Stochastic block models (SBMs) and their contextual counterparts have been widely used to study node classiﬁcation problems [23, 24]. Recently there have been several works proposing to use CSBMs to theoretically analyze GNNs for the node classiﬁcation task. [ 25] used CSBMs to study the function of nonlinearity on the node classiﬁcation performance, while [26] used CSBMs to study the attention-based GNNs. More relevantly, [27, 28] showed the advantage of applying graph convolutions up to three times for node classiﬁcation on CSBM graphs. Nonetheless, they only focused on the desirable denoising effect of graph convolution instead of its tradeoff with the undesirable mixing effect, and therefore did not explain the occurance of oversmoothing. 3 Problem Setting and Main Results We ﬁrst introduce our theoretical analysis setup using the Contextual Stochastic Block Model (CSBM), a random graph model with planted community structure [18, 27, 28, 29, 25, 26]. We choose the CSBM to study GNNs for the node classiﬁcation task because the main goal of node classiﬁcation is to discover node communities from the data. The CSBM imitates the community structure of real graphs and provides a clear ground truth for us to evaluate the model performance. Moreover, it is a generative model which gives us full control of the data to perform a non-asymptotic analysis. We then present a set of theoretical results establishing bounds for the representation power of GNNs in terms of the best-case node classiﬁcation accuracy. The proofs of all the theorems and additional claims will be provided in the Appendix. 3.1 Notations We represent an undirected graph with N nodes by G= (A,X), where A∈{0,1}N×N is the adjacency matrix and X ∈RN is the node feature vector. For nodes u,v ∈[N], Auv = 1 if and only if uand vare connected with an edge in G, and Xu ∈R represents the node feature of u. We let 1 N denote the all-one vector of length N and D= diag(A1 N) be the degree matrix of G. 3.2 Theoretical Analysis Framework Contextual Stochastic Block Models We will focus on the case where the CSBM consists of two classes C1 and C2 of nodes of equal size, in total with N nodes. For any two nodes in the graph, if they are from the same class, they are connected by an edge independently with probability p, or if they are from different classes, the probability is q. For each node v∈Ci,i ∈{1,2}, the initial feature Xv is sampled independently from a Gaussian distribution N(µi,σ2), where µi ∈R,σ ∈(0,∞). Without loss of generality, we assume that µ1 <µ2. We denote a graph generated from such a CSBM as G(A,X) ∼CSBM(N,p,q,µ 1,µ2,σ2). We further impose the following assumption on the CSBM used in our analysis. Assumption 1. p,q = ω(log N/N) and p>q > 0. The choice p,q = ω(log N/N) ensures that the generated graph Gis connected almost surely [ 23] while being slightly more general than the p,q = ω(log2 N/N) regime considered in some concurrent works [27, 25]. In addition, this regime also guarantees that Ghas a small diameter. Real-world graphs are known to exhibit the “small-world\" phenomenon—even if the number of nodes N is very large, the diameter of graph remains small [ 30, 31]. We will see in the theoretical analysis (Section 3.3) how this small-diameter characteristic contributes to the occurrence of oversmoothing in shallow GNNs. We remark that our results in fact hold for the more general choice of p,q = Ω(log N/N), for which only the concentration bound in Theorem 1 needs to be modiﬁed in the threshold log N/N case where all the constants need a more careful treatment. Further, the choice p > qensures that the graph structure has homophily, meaning that nodes from the same class are more likely to be connected than nodes from different classes. This characteristic is observed in a wide range of real-world graphs [32, 29]. We note that this homophily assumption (p>q ) is not essential to our analysis, though we add it for simplicity since the discussion of homophily versus heterophily (p<q ) is not the focus of our paper. Graph convolution and linear GNN In this paper, our theoretical analysis focuses on the simpliﬁed linear GNN model deﬁned as follows: a graph convolution using the (left-)normalized adjacency matrix takes the operation 3h′= (D−1A)h, where hand h′are the input and output node representations, respectively. A linear GNN layer can then be deﬁned as h′= (D−1A)hW, where W is a learnable weight matrix. As a result, the output of nlinear GNN layers can be written as h(n) ∏n k=1 W(k), where h(n) = (D−1A)nX is the output of ngraph convolutions, and W(k) is the weight matrix of the kth layer. Since this is linear in h(n), it follows that n-layer linear GNNs have the equivalent representation power as linear classiﬁers applied to h(n). In practice, when building GNN models, nonlinear activation functions can be added between consecutive linear GNN layers. For additional results showing that adding certain nonlinearity would not improve the classiﬁcation performance, see Appendix K.1. Bayes error rate and z-score Thanks to the linearity of the model, we see that the representation of node v ∈Ci after ngraph convolutions is distributed as N(µ(n) i ,(σ(n))2), where the variance (σ(n))2 is shared between classes. The optimal node-wise classiﬁer in this case is the Bayes optimal classiﬁer, given by the following lemma. Lemma 1. Suppose the label y is drawn uniformly from {1,2}, and given y, x ∼ N(µ(n) y ,(σ(n))2). Then the Bayes optimal classiﬁer, which minimizes the probability of misclassiﬁcation among all classiﬁers, has decision boundary D= (µ1 + µ2)/2, and predicts y = 1,if x ≤D or y = 2,if x >D. The associated Bayes error rate is 1 −Φ(z(n)), where Φ denotes the cumulative distribution function of the standard Gaussian distribution and z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) is the z-score of Dwith respect to N(µ(n) 1 ,(σ(n))2). Lemma 1 states that we can estimate the optimal performance of an n-layer linear GNN through the z-score z(n) = 1 2 (µ(n) 2 −µ(n) 1 )/σ(n). A higher z-score indicates a smaller Bayes error rate, and hence a better expected performance of node classiﬁcation. The z-score serves as a basis for our quantitative analysis of oversmoothing. In the following section, by estimating µ(n) 2 −µ(n) 1 and (σ(n))2, we quantify the two counteracting effects of graph convolutions and obtain bounds on the z-score z(n) as a function of n, which allows us to characterize oversmoothing quantitatively. Speciﬁcally, there are two potential interpretations of oversmoothing based on the z-score: (1) z(n) <z(n⋆), where n⋆ = arg maxn′z(n′); and (2) z(n) <z (0). They correspond to the cases (1) n>n ⋆; and (2) n>n 0, where n0 ≥0 corresponds to the number of layers that yield a z-score on par with z(0). The bounds on the z-score z(n), z(n) lower and z(n) upper, enable us to estimate n⋆ and n0 under different scenarios and provide insights into the optimal choice of depth. 3.3 Main Results We ﬁrst estimate the gap between the means µ(n) 2 −µ(n) 1 with respect to the number of layers n. µ(n) 2 −µ(n) 1 measures how much node representations in different classes have homogenized afternGNN layers, which is the undesirable mixing effect. Lemma 2. For n∈N ∪{0}, assuming D−1A≈E[D]−1E[A], µ(n) 2 −µ(n) 1 = (p−q p+ q )n (µ2 −µ1) . Lemma 2 states that the means µ(n) 1 and µ(n) 2 get closer exponentially fast and as n →∞, both µ(n) 1 and µ(n) 2 will converge to the same value (in this case ( µ(n) 1 + µ(n) 2 ) /2). The rate of change (p−q)/(p+ q) is determined by the intra-community edge density pand the inter-community edge density q. Lemma 2 suggests that graphs with higher inter-community density (q) or lower intra-community density (p) are expected to suffer from a higher mixing effect when we perform message-passing. We provide the following concentration bound for our estimate of µ(n) 2 −µ(n) 1 , which states that the estimate concentrates at a rate of O(1/ √ N(p+ q)). Theorem 1. Fix K ∈N and r> 0. There exists a constant C(r,K) such that with probability at least 1 −O(1/Nr), it holds for all 1 ≤k≤Kthat |(µ(k) 2 −µ(k) 1 ) − (p−q p+ q )k (µ2 −µ1)|≤ C√ N(p+ q) . 4We then study the variance (σ(n))2 with respect to the number of layers n. The variance (σ(n))2 measures how much the node representations in the same class have homogenized, which is the desirable denoising effect. We ﬁrst state that no matter how many layers are applied, there is a nontrivial ﬁxed lower bound for (σ(n))2 for a graph with N nodes. Lemma 3. For all n∈N ∪{0}, 1 Nσ2 ≤(σ(n))2 ≤σ2 . Lemma 3 implies that for a given graph, even as the number of layers ngoes to inﬁnity, the variance (σ(n))2 does not converge to zero, meaning that there is a ﬁxed lower bound for the denoising effect. See Appendix K.2 for the exact theoretical limit for the variance (σ(n))2 as ngoes to inﬁnity. We now establish a set of more precise upper and lower bounds for the variance (σ(n))2 with respect to the number of layers nin the following technical lemma. Lemma 4. Let a= Np/log N. With probability at least 1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 (σ(n))2 ≤min    ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k ,1   σ2 . Lemma 4 holds for all 1 ≤n≤N and directly leads to the following theorem with a clariﬁed upper bound where nis bounded by a constant K. Theorem 2. Let a = Np/log N. Fix K ∈N. There exists a constant C(K) such that with probability at least 1 −O(1/N), it holds for all 1 ≤n≤Kthat max {min{a,2} 10 1 (Np)n, 1 N } σ2 ≤(σ(n))2 ≤min { C min{a,2} 1 (N(p+ q))n,1 } σ2 . Theorem 2 states that the variance (σ(n))2 for each Gaussian distribution decreases more for larger graphs or denser graphs. Moreover, the upper bound implies that the variance (σ(n))2 will initially go down at least at a rate expo- nential in O(1/log N) before reaching the ﬁxed lower bound σ2/N suggested by Lemma 3. This means that after O(log N/log(log N)) layers, the desirable denoising effect homogenizing node representations in the same class will saturate and the undesirable mixing effect will start to dominate. Why does oversmoothing happen at a shallow depth? For each node, message-passing with different-class nodes homogenizes their representations exponentially. The exponential rate depends on the fraction of different-class neighbors among all neighbors (Lemma 2, mixing effect). Meanwhile, message-passing with nodes that have not been encountered before causes the denoising effect, and the magnitude depends on the absolute number of newly encountered neighbors. The diameter of the graph is approximately log N/log(Np) in the p,q = Ω(log N/N) regime [33], and thus is at most log N/log(log N) in our case. After the number of layers surpasses the diameter, for each node, there will be no nodes that have not been encountered before in message-passing and hence the denoising effect will almost vanish (Theorem 2, denoising effect). log N/log(log N) grows very slowly with N; for example, when N = 106,log N/log(log N) ≈8. This is why even in a large graph, the mixing effect will quickly dominate the denoising effect when we increase the number of layers, and so oversmoothing is expected to happen at a shallow depth1. Our theory suggests that the optimal number of layers, n⋆, is at most O(log N/log(log N)). For a more quantitative estimate, we can use Lemma 2 and Theorem 2 to compute bounds z(n) lower and z(n) upper for z= 1 2 (µ(n) 2 −µ(n) 1 )/σ(n) and use them to infer n⋆ and n0, as deﬁned in Section 3.2. See Appendix H for detailed discussion. Next, we investigate the effect of increasing the dimension of the node features X. So far, we have only considered the case with one-dimensional node features. The following proposition states that if features in each dimension are independent, increasing input feature dimension decreases the Bayes error rate for a ﬁxed n. The intuition is that when node features provide more evidence for classiﬁcation, it is easier to classify nodes correctly. Proposition 1. Let the input feature dimension be d, X ∈RN×d. Without loss of generality, suppose for node vin Ci, initial node feature Xv ∼N([µi]d,σ2Id) independently. Then the Bayes error rate is 1 −Φ (√ d 2 (µ(n) 2 −µ(n) 1 ) σ(n) ) = 1 −Φ (√ d 2 z(n) ) , where Φ denotes the cumulative distribution function of the standard Gaussian distribution. Hence the Bayes error rate is decreasing in d, and as d→∞, it converges to 0. 1For mixing and denoising effects in real data, see Appendix K.5. 54 The effects of Personalized PageRank on oversmoothing Our analysis framework in Section 3.3 can also be applied to GNNs with other message-passing schemes. Speciﬁcally, we can analyze the performance of Personalized Propagation of Neural Predictions (PPNP) and its approximate variant, Approximate PPNP (APPNP), which were proposed for alleviating oversmoothing while still making use of multi-hop information in the graph. The main idea is to use Personalized PageRank (PPR) or the approximate Personalized PageRank (APPR) in place of graph convolutions [12]. Mathematically, the output of PPNP can be written as hPPNP = α(IN −(1 −α)(D−1A))−1X, while APPNP computes hAPPNP(n+1) = (1 −α)(D−1A)hAPPNP(n) + αX iteratively in n, where IN is the identity matrix of size N and in both cases αis the teleportation probability. Then for nodes in Ci,i ∈{1,2}, the node representations follow a Gaussian distribution N ( µPPNP i ,(σPPNP)2 ) after applying PPNP, or a Gaussian distributionN ( µAPPNP(n) i ,(σAPPNP(n))2 ) after applying nAPPNP layers. We quantify the effects on the means and variances for PPNP and APPNP in the CSBM case. We can similarly use them to calculate the z-score of (µ1 + µ2)/2 and compare it to the one derived for the baseline GNN in Section 3. The key idea is that the PPR propagation can be written as a weighted average of the standard message-passing, i.e. α(IN −(1 −α)(D−1A))−1 = ∑∞ k=0(1 −α)k(D−1A)k [34]. We ﬁrst state the resulting mixing effect measured by the difference between the two means. Proposition 2. Fix r> 0,K ∈N. For PPNP , with probability at least1−O(1/Nr), there exists a constantC(α,r,K ) such that µPPNP 2 −µPPNP 1 = p+ q p+ 2−α α q(µ2 −µ1) + ϵ. where the error term |ϵ|≤ C/ √ N(p+ q) + (1−α)K+1. Proposition 3. Let r> 0. For APPNP , with probability at least1 −O(1/Nr), µAPPNP(n) 2 −µAPPNP(n) 1 = ( p+ q p+ 2−α α q + (2 −2α)q αp+ (2 −α)q(1 −α)n (p−q p+ q )n) (µ2 −µ1) + ϵ. where the error term ϵis the same as the one deﬁned in Theorem 1 for the case of K = n. Both p+q p+ 2−α α q and (2−2α)q αp+(2−α)q(1 −α) ( p−q p+q ) are monotone increasing in α. Hence from Proposition 2 and 3, we see that with larger α, meaning a higher probability of teleportation back to the root node at each step of message-passing, PPNP and APPNP will indeed make the difference between the means of the two classes larger: while the difference in means for the baseline GNN decays as (p−q p+q )n , the difference for PPNP/APPNP is lower bounded by a constant. This validates the original intuition behind PPNP and APPNP that compared to the baseline GNN, they reduce the mixing effect of message-passing, as staying closer to the root node means aggregating less information from nodes of different classes. This advantage becomes more prominent when the number of layers nis larger, where the model performance is dominated by the mixing effect: as ntends to inﬁnity, while the means converge to the same value for the baseline GNN, their separation is lower-bounded for PPNP/APPNP. However, the problem with the previous intuition is that PPNP and APPNP will also reduce the denoising effect at each layer, as staying closer to the root node also means aggregating less information from new nodes that have not been encountered before. Hence, for an arbitrary graph, the result of the tradeoff after the reduction of both effects is not trivial to analyze. Here, we quantify the resulting denoising effect for CSBM graphs measured by the variances. We denote (σ(n))2 upper as the variance upper bound for depth nin Lemma 4. Proposition 4. For PPNP , with probability at least1 −O(1/N), it holds for all 1 ≤K ≤N that max {α2 min{a,2} 10 , 1 N } σ2 ≤(σPPNP)2 ≤max   α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 ,σ2   . Proposition 5. For APPNP , with probability at least1 −O(1/N), it holds for all 1 ≤n≤N that max {min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) , 1 N } σ2 ≤(σAPPNP(n))2 , (σAPPNP(n))2 ≤min    ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 ,σ2   . 6By comparing the lower bounds in Proposition 4 and 5 with that in Theorem 2, we see that PPR reduces the beneﬁcial denoising effect of message-passing: for large or dense graphs, while the variances for the baseline GNN decay as 1/(Np)n, the variances for PPNP/APPNP are lower bounded by the constant α2 min{a,2}/10. In total, the mixing effect is reduced by a factor of (p−q p+q )n , while the denoising effect is reduced by a factor of 1/(Np)n. Hence PPR would cause greater reduction in the denoising effect than the improvement in the mixing effect for graphs whereN and pare large. This drawback would be especially notable at a shallow depth, where the denoising effect is supposed to dominate the mixing effect. APPNP would perform worse than the baseline GNN on these graphs in terms of the optimal classiﬁcation performance. We remark that in each APPNP layer, another way to interpret the termαX is to regard it as a residual connection to the initial representation X [15]. Thus, our theory also validates the empirical observation that adding initial residual connections allows us to build very deep models without catastrophic oversmoothing. However, our results suggest that initial residual connections do not guarantee an improvement in model performance by themselves. 5 Experiments In this section, we ﬁrst demonstrate our theoretical results in previous sections on synthetic CSBM data. Then we discuss the role of optimizing weights W(k) in GNN layers in the occurrence of oversmoothing through both synthetic data and the three widely used benchmarks: Cora, CiteSeer and PubMed [35]. Our results highlight the fact that the oversmoothing phenomenon observed in practice may be exacerbated by the difﬁculty of optimizing weights in deep GNN models. More details about the experiments are provided in Appendix J. 5.1 The effect of graph topology on oversmoothing We ﬁrst show how graph topology affects the occurrence of oversmoothing and the effects of PPR. We randomly generated synthetic graph data from CSBM( N = 2000 , p, q = 0 .0038, µ1 = 1 , µ2 = 1 .5, σ2 = 1 ). We used 60%/20%/20% random splits and ran GNN and APPNP with α= 0.1. For results in Figure 2, we report averages over 5 graphs and for results in Figure 3, we report averages over 5 runs. In Figure 2, we study how the strength of community structure affects oversmoothing. We can see that when graphs have a stronger community structure in terms of a higher intra-community edge density p, they would beneﬁt more from repeated message-passing. As a result, given the same set of node features, oversmoothing would happen later and a classiﬁer could achieve better classiﬁcation performance. A similar trend can also be observed in Figure 4A. Our theory predicts n⋆ and n0, as deﬁned in Section 3.2, with high accuracy. In Figure 3, we compare APPNP and GNN under different graph topologies. In all three cases, APPNP manifests its advantage of reducing the mixing effect compared to GNN when the number of layers is large, i.e. when the undesirable mixing effect is dominant. However, as Figure 3B,C show, when we have large graphs or graphs with strong community structure, APPNP’s disadvantage of concurrently reducing the denoising effect is more severe, particularly when the number of layers is small. As a result, APPNP’s optimal performance is worse than the baseline GNN. These observations accord well with our theoretical discussions in Section 4. A B C Figure 2: How the strength of community structure affects oversmoothing. When graphs have stronger community structure (i.e. higher a), oversmoothing would happen later. Our theory (gray bar) predicts the optimal number of layers n⋆ in practice (blue) with high accuracy (A). Given the same set of features, a classiﬁer has signiﬁcantly better performance on graphs with higher a(B,C). 7A (base case) B (larger graph) C (stronger community) Figure 3: Comparison of node classiﬁcation performance between the baseline GNN and APPNP. The performance of APPNP is more robust when we increase the model depth. However, compared to the base case (A), APPNP tends to have worse optimal performance than GNN on graphs with larger size ( B) or stronger community structure (C), as predicted by the theory. 5.2 The effect of optimizing weights on oversmoothing We investigate how adding learnable weightsW(k) in each GNN layers affects the node classiﬁcation performance in practice. Consider the case when all the GNN layers used have width one, meaning that the learnable weight matrix W(k) in each layer is a scalar. In theory, the effects of adding such weights on the means and the variances would cancel each other and therefore they would not affect the z-score of our interest and the classiﬁcation performance. Figure 4A shows the the value of n0 predicted by the z-score, the actual n0 without learnable weights in each GNN layer (meaning that we apply pure graph convolutions ﬁrst, and only train weights in the ﬁnal linear classiﬁer) according to the test accuracy and the actual n0 with learnable weights in each GNN layer according to the test accuracy. The results are averages over 5 graphs for each case. We empirically observe that GNNs with weights are much harder to train, and the difﬁculty increases as we increase the number of layers. As a result, n0 is smaller for the model with weights and the gap is larger when n0 is supposed to be larger, possibly due to greater difﬁculty in optimizing deeper architectures [36]. To relieve this potential optimization problem, we increase the width of each GNN layer [37]. Figure 4B,C presents the training and testing accuracies of GNNs with increasing width with respect to the number of layers on a speciﬁc synthetic example. The results are averages over 5 runs. We observe that increasing the width of the network mitigates the difﬁculty of optimizing weights, and the performance after adding weights is able to gradually match the performance without weights. This empirically validates our claim in Section 3.2 that adding learnable weights should not affect the representation power of GNN in terms of node classiﬁcation accuracy on CSBM graphs, besides empirical optimization issues. In practice, as we build deeper GNNs for more complicated tasks on real graph data, the difﬁculty of optimizing weights in deep GNN models persists. We revisit the multi-class node classiﬁcation task on the three widely used benchmark datasets: Cora, CiteSeer and PubMed [35]. We compare the performance of GNN without weights against the performance of GNN with weights in terms of test accuracy. We used 60%/20%/20% random splits, as in [38] and [39] and report averages over 5 runs. Figure 5 shows the same kind of difﬁculty in optimizing deeper models with A B C Figure 4: The effect of optimizing weights on oversmoothing using synthetic CSBM data. Compared to the GNN without weights, oversmoothing happens much sooner after adding learnable weights in each GNN layer, although these two models have the same representation power (A). As we increase the width of each GNN layer, the performance of GNN with weights is able to gradually match that of GNN without weights (B,C). 8Figure 5: The effect of optimizing weights on oversmoothing using real-world benchmark datasets. Adding learnable weights in each GNN layer does not improve node classiﬁcation performance but rather leads to optimization difﬁculty. learnable weights in each GNN layer as we have seen for the synthetic data. Increasing the width of each GNN layer still mitigates the problem for shallower models, but it becomes much more difﬁcult to tackle beyond 10 layers to the point that simply increasing the width could not solve it. As a result, although GNNs with and without weights are on par with each other when both are shallow, the former has much worse performance when the number of layers goes beyond 10. These results suggest that the oversmoothing phenomenon observed in practice is aggravated by the difﬁculty of optimizing deep GNN models. 6 Discussion Designing more powerful GNNs requires deeper understanding of current GNNs—how they work and why they fail. In this paper, we precisely characterize the mechanism of overmoothing via a non-asymptotic analysis and justify why oversmoothing happens at a shallow depth. Our analysis suggests that oversmoothing happens once the undesirable mixing effect homogenizing node representations in different classes starts to dominate the desirable denoising effect homogenizing node representations in the same class. Due to the small diameter characteristic of real graphs, the turning point of the tradeoff will occur after only a few rounds of message-passing, resulting in oversmoothing in shallow GNNs. It is worth noting that oversmoothing becomes an important problem in the literature partly because typical Convolutional Neural Networks (CNNs) used for image processing are much deeper than GNNs [40]. As such, researchers have been trying to use methods that have previously worked for CNNs to make current GNNs deeper [ 20, 15]. However, if we regard images as grids, images and real-world graphs have fundamentally different characteristics. In particular, images are giant grids, meaning that aggregating information between faraway pixels often requires a large number of layers. This contrasts with real-world graphs, which often have small diameters. Hence we believe that building more powerful GNNs will require us to think beyond CNNs and images and take advantage of the structure in real graphs. There are many outlooks to our work and possible directions for further research. First, while our use of the CSBM provided important insights into GNNs, it will be helpful to incorporate other real graph properties such as degree heterogeneity in the analysis. Additionally, further research can focus on the learning perspective of the problem. 7 Acknowledgement This research has been supported by a Vannevar Bush Fellowship from the Ofﬁce of the Secretary of Defense. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the Department of Defense or the U.S. Government. References [1] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IJCNN, 2005. [2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20:61–80, 2009. [3] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. 9[4] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timo- thy D. Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeurIPS, 2015. [5] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In NeurIPS, 2016. [6] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In NeurIPS, 2016. [7] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In ICLR, 2016. [8] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. [9] Thomas Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR, 2017. [10] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32:4–24, 2019. [11] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In AAAI, 2018. [12] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. [13] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. In ICLR, 2020. [14] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In AAAI, 2020. [15] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020. [16] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. In NeurIPS, 2022. [17] Meng Liu, Zhengyang Wang, and Shuiwang Ji. Non-local graph neural networks. IEEE transactions on pattern analysis and machine intelligence, 2021. [18] Yash Deshpande, Andrea Montanari, Elchanan Mossel, and Subhabrata Sen. Contextual stochastic block models. In NeurIPS, 2018. [19] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018. [20] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In ICCV, 2019. [21] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864, 2020. [22] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020. [23] Emmanuel Abbe. Community detection and stochastic block models. Foundations and Trends in Communications and Information Theory, 14:1–162, 2018. [24] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural networks. In ICLR, 2019. [25] Rongzhe Wei, Haoteng Yin, J. Jia, Austin R. Benson, and Pan Li. Understanding non-linearity in graph neural networks from the bayesian-inference perspective. ArXiv, abs/2207.11311, 2022. [26] Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath. Graph attention retrospective. ArXiv, abs/2202.13060, 2022. [27] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Graph convolution for semi-supervised classiﬁca- tion: Improved linear separability and out-of-distribution generalization. In ICML, 2021. [28] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of graph convolutions in deep networks. ArXiv, abs/2204.09297, 2022. [29] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? In ICLR, 2022. 10[30] Michelle Girvan and Mark E. J. Newman. Community structure in social and biological networks. Proceedings of the National Academy of Sciences, 99:7821 – 7826, 2002. [31] Fan R. K. Chung. Graph theory in the information age. volume 57, pages 726–732, 2010. [32] David A. Easley and Jon M. Kleinberg. Networks, Crowds, and Markets: Reasoning about a Highly Connected World. 2010. [33] Fan Chung Graham and Linyuan Lu. The diameter of sparse random graphs. Advances in Applied Mathematics, 26:257–279, 2001. [34] Reid Andersen, Fan Chung Graham, and Kevin J. Lang. Local graph partitioning using pagerank vectors. In FOCS, 2006. [35] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016. [36] Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. In COLT, 2019. [37] Simon Shaolei Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In ICML, 2019. [38] Hongwei Wang and Jure Leskovec. Unifying graph convolutional neural networks and label propagation. ArXiv, abs/2002.06755, 2020. [39] Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label propagation and simple models out-performs graph neural networks. In ICLR, 2021. [40] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [41] Luc Devroye, László Györﬁ, and Gábor Lugosi. A probabilistic theory of pattern recognition. In Stochastic Modelling and Applied Probability, 1996. [42] Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey.Internet mathematics, 3(1):79–127, 2006. [43] Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479–2506, 2016. [44] Linyuan Lu and Xing Peng. Spectra of edge-independent random graphs.The Electronic Journal of Combinatorics, 20:27, 2013. [45] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. [47] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. A Proof of Lemma 1 Following the deﬁnition of the Bayes optimal classiﬁer [41], B(x) = arg max i=1,2 P[y= i|x] , we get that the Bayes optimal classiﬁer has a linear decision boundary D= (µ1 + µ2)/2 such that the decision rule is {y= 1 if x≤D y= 2 if x> D. Probability of misclassiﬁcation could be written as P[y= 1,x> D] + P[y= 2,x ≤D] = P[x> D|y= 1]P[y= 1] + P[x≤D|y= 2]P[y= 2] = 1 2(P[x> D|y= 1] + P[x≤D|y= 2]) . 11When D= (µ1 + µ2)/2, the expression is called the Bayes error rate, which is the minimal probability of misclassiﬁca- tion among all classiﬁers. Geometrically, it is easy to see that the Bayes error rate equals 1 2 S, where Sis the overlapping area between the two Gaussian distributions N ( µ(n) 1 ,(σ(n))2 ) and N ( µ(n) 2 ,(σ(n))2 ) . Hence one can use the z-score of (µ1 + µ2)/2 with respect to either of the two Gaussian distributions to directly calculate the Bayes error rate. B Proof of Lemma 2 Under the heuristic assumption D−1A≈E[D]−1E[A], we can write µ(1) 1 = pµ1 + qµ2 p+ q , µ (1) 2 = pµ2 + qµ1 p+ q µ(k) 1 = pµ(k−1) 1 + qµ(k−1) 2 p+ q , µ (k) 2 = pµ(k−1) 2 + qµ(k−1) 1 p+ q , for all k∈N. Writing recursively, we get that µ(n) 1 = (p+ q)n + (p−q)n 2(p+ q)n µ1 + (p+ q)n −(p−q)n 2(p+ q)n µ2 , µ(n) 2 = (p+ q)n + (p−q)n 2(p+ q)n µ2 + (p+ q)n −(p−q)n 2(p+ q)n µ1 . C Proof of Theorem 1 We use ∥·∥2 to denote the spectral norm, ∥A∥2 = maxx:∥x∥=1 ∥Ax∥2. We denote ¯A= E[A], ¯D= E[D], d= A1 N and ¯d= E[d]i. We further deﬁne the following relevant vectors: w1 := 1 N, w 2 := ( 1 N/2 −1 N/2 ) , µ := ( µ11 N/2 µ21 N/2 ) . The quantity of interest is µ(k) 2 −µ(k) 1 = 1 N/2 w⊤ 2 (D−1A)kµ. C.1 Auxiliary results We record some properties of the adjacency matrices: 1. D−1Aand ¯D−1 ¯Ahave an eigenvalue of 1, corresponding to the (right) eigenvector w1. 2. If Jn = 1 n1 ⊤ n, where 1 n is all-one vector of length n, then ¯A:= ( pJN/2 qJN/2 qJN/2 pJN/2 ) . 3. ¯D= N 2 (p+ q)IN. 4. µ= αw1 + βw2, where α= µ1+µ2 2 and β = µ1−µ2 2 . To control the degree matrix D−1, we will use the following standard Chernoff bound [42]: Lemma 5 (Chernoff Bound). Let X1,...,X n be independent, S := ∑n i=1 Xi, and ¯S = E[S]. Then for all ε> 0, P(S ≤¯S−ε) ≤e−ε2/(2 ¯S), P(S ≥¯S+ ε) ≤e−ε2/(2( ¯S+ε/3)). We can thus derive a uniform lower bound on the degree of every vertex: Corollary 1. For every r >0, there is a constant C(r) such that whenever ¯d ≥Clog N, with probability at least 1 −N−r, 1 2 ¯d≤di ≤3 2 ¯d, for all 1 ≤i≤N. Consequently, with probability at least 1 −N−r, ∥D−1 −¯D−1∥2 ≤C/¯dfor some C. 12Proof. By applying Lemma 1 and a union bound, all degrees are within 1/2 ¯dof their expectations, with probability at least 1 −e−¯d/8+log N. Taking C = 8r+ 8 yields the desired lower bound. An analogous proof works for the upper bound. To show the latter part, write ∥D−1 −¯D−1∥2 = max 1≤i≤N |di −¯d| di¯d Using the above bounds, the numerator for each iis at most 1/2 ¯dand the denominator for each iis at least 1/2 ¯d2, with probability at least 1 −N−r. Combining the bounds yields the claim. We will also need a result on concentration of random adjacency matrices, which is a corollary of the sharp bounds derived in [43] Lemma 6 (Concentration of Adjacency Matrix) . For every r >0, there is a constant C(r) such that whenever ¯d≥log N, with probability at least 1 −N−r, ∥A−¯A∥2 <C √ ¯d. Proof. By corollary 3.12 from [43], there is a constant κsuch that P(∥A−¯A∥2 ≥3 √ ¯d+ t) ≤e−t2/κ+log N. Setting t= √ (1 + r) ¯d, C = 3 + √ (1 + r)κsufﬁces to achieve the desired bound. C.2 Sharp concentration of the random walk operator D−1A In this section, we aim to show the following concentration result for the random walk operator D−1A: Theorem 3. Suppose the edge probabilities are ω ( log N N ) , and let ¯dbe the average degree. For anyr, there exists a constant Csuch that for sufﬁciently large N, with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . Proof. We decompose the error E = D−1A−¯D−1 ¯A= D−1(A−¯A) + (D−1 −¯D−1) ¯A= T1 + T2, where T1 = D−1(A−¯A), T 2 = (D−1 −¯D−1) ¯A. We bound the two terms separately. Bounding T1: By Corollary 1, ∥D−1∥2 = max i1/di ≤2/¯d with probability 1 −N−r. Combining this with Lemma 6, we see that with probability at least 1 −2N−r, ∥D−1(A−¯A)∥2 ≤∥D−1∥2∥A−¯A∥2 ≤ C√¯d for some Cdepending only on r. Bounding T2: Similar to [44], we bound T2 by exploiting the low-rank structure of the expected adjacency matrix, ¯A. Recall that ¯Ahas a special block form. The eigendecomposition of ¯Ais thus ¯A= 2∑ j=1 λjw(j), where w(1) = 1√ N1 N,λ1 = N(p+q) 2 ,w(2) = 1√ N ( 1 N/2 −1 N/2 ) ,λ2 = N(p−q) 2 . 13Using the deﬁnition of the spectral norm, we can bound ∥T2∥2 as ∥T2∥2 ≤max ∥x∥=1 ∥(D−1 −¯D−1) ¯Ax∥2 ≤ max α∈R2,∥α∥=1 ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 . Note that when ∥α∥2 = 1, ∥(D−1 −¯D−1) ¯A(α1w(1) + α2w(2))∥2 2 = N∑ i=1 (1 di −1 ¯d )2   2∑ j=1 λjαjw(j) i   2 ≤ N∑ i=1 (1 di −1 ¯d )2 2∑ j=1 λ2 j(w(j) i )2 using Cauchy-Schwarz. Since |w(j) i |≤ 1√ N for all i,j, the second summation can be bounded by 1 N ∑2 j=1 λ2 j. Overall, the upper bound is now 1 N N∑ i=1 (di −¯d)2 (di¯d)2 2∑ j=1 λ2 j , Under the event of Corollary 1, di ≥C¯dfor some C <1. Under our setup, we also have λ2 1 = ¯d2, λ2 2 ≤ ¯d2. This means that the upper bound is 1 C2 ¯d2N∥d−¯d1 N∥2 2 , where dis the vector of node degrees. It remains to show that 1 N∥d−¯d1 N∥2 2 = O( ¯d). To do this, we use a form of Talagrand’s concentration inequality, given in [45]. Since the function 1√ N∥d−¯d1 N∥2 = 1√ N∥(A−¯dIN)1 N∥2 is a convex, 1-Lipschitz function of A, Theorem 6.10 from [45] guarantees that for any t> 0, P( 1√ N ∥d−¯d1 N∥2 >E[ 1√ N ∥d−¯d1 N∥2] + t) ≤e−t2/2. Using Jensen’s inequality, E[∥d−¯d1 N∥2] ≤ √ E[∥d−¯d1 N∥2 2] = √ N∑ i=1 Var(di) = √ NVar(d1) ≤ √ N¯d. If ¯d= ω(log N), we can guarantee that 1√ N ∥d−¯d1 N∥2 ≤C √ ¯d with probability at least 1 −e−(C−1)2 ¯d/2 = 1 −O(N−r) for an appropriate constant C. Thus we have shown that with high probability, T2 = O(1/ √¯d), which proves the claim. C.3 Proof of Theorem 1 Fix rand K. We desire to bound 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ. By the ﬁrst property of adjacency matrices in auxiliary results, it sufﬁces to bound β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2. 14where β = µ1−µ2 2 . We will show inductively that there is a Csuch that for every k= 1,...,K , ∥(D−1A)k −( ¯D−1 ¯A)k∥2 ≤C/ √ ¯d. If this is true, then Cauchy-Schwarz gives β 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)w2 ≤β 1 N/2∥w2∥2∥(D−1A)k −( ¯D−1 ¯A)k∥2∥w2∥2 ≤C/ √ ¯d. By Theorem 3, we have that with probability at least 1 −O(N−r), ∥D−1A−¯D−1 ¯A∥2 ≤ C√¯d . So D−1A= ¯D−1 ¯A+ J where ∥J∥≤ C/ √¯d. Iterating, we have ∥(D−1A)k −( ¯D−1 ¯A)k∥2 = ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 (1) Inductively, (D−1A)k−1 = ( ¯D−1 ¯A)k−1 + H where ∥H∥2 ≤C/ √¯d. Plugging this in (1), we have ∥(D−1A)k−1D−1A−( ¯D−1 ¯A)k∥2 = ∥(( ¯D−1 ¯A)k−1 + H)( ¯D−1 ¯A+ J) −( ¯D−1 ¯A)k∥2 . Of these terms, ( ¯D−1 ¯A)k−1J has norm at most ∥J∥2, H( ¯D−1 ¯A) has norm at most ∥H∥2, and HJ has norm at most C/¯d.2 Hence the induction step is complete. We have thus shown that there is a constantC(r,K) such that with probability at least 1 −N−r, | 1 N/2w⊤ 2 ((D−1A)k −( ¯D−1 ¯A)k)µ|≤ C ¯d . which proves the claim. By simulation one can verify that indeed 1 N/2 w⊤ 2 ( ¯D−1 ¯A)kµ ≈ ( p−q p+q )k (µ2 −µ1). Figure 6 presents µ(n) 1 ,µ(n) 2 calculated from simulation against predicted values from our theoretical results. The simulation results are averaged over 20 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 6: Comparison of the mean estimation in Lemma 2 against simulation results. D Proof of Lemma 3 Fix n and let the element in the ith row and jth column of (D−1A)n be p(n) ij . Consider a ﬁxed node i. The variance of the feature for node iafter nlayers of convolutions is (∑ j(p(n) ij )2)σ2, by the basic property of variance of sum. Since ∑ j|p(n) ij |= 1, it follows that ∑ j(p(n) ij )2 ≤1, which is the second inequality. To show the ﬁrst inequality, consider the following optimization problem: 2More precisely, the C becomes Ck, which is why we restrict the approximation guarantee to constant K. 15min p(n) ij ,1 ≤j ≤N ∑ j (p(n) ij )2 s.t. ∑ j p(n) ij = 1, p(n) ij ≥0, 1 ≤j ≤N This part of proof goes by contradiction. Suppose ∃k,l such that p(n) ik ̸= ∃p(n) il . Fixing all other p(n) ij ,j ̸= k,l, if we average p(n) ik and p(n) il , their sum of squares will strictly decrease while not breaking the constraints: 2 (p(n) ik + p(n) il 2 )2 −((p(n) ik )2 + (p(n) il )2) = −1 2(p(n) ik −p(n) il )2 <0 . So we obtain a contradiction. Thus to minimize ∑ j(p(n) ij )2, p(n) ij = 1 N,1 ≤j ≤N, and the mimimum is 1/N. E Proof of Lemma 4 The proof relies on the following deﬁnition of neighborhood size: in a graph G, we denote by Γk(x) the set of vertices in Gat distance kfrom a vertex x: Γk(x) = {y∈G : d(x,y) = k}. we deﬁne Nk(x) to be the set of vertices within distance kof x: Nk(x) = k⋃ i=0 Γi(x) . To prove the lower bound, we ﬁrst show an intermediate step that 1 |Nn|σ2 ≤(σ(n))2 . The proof is the same as the one for the ﬁrst inequality in Lemma 3, except we add in another constraint that for a ﬁxed i, the row pi·is |Nn(i)|-sparse. This implies that the minimum of ∑ j(p(n) ij )2 becomes 1/|Nn(i)|. The we apply the result on upper bound of neighborhood sizes in Erd˝os-Rényi graph G(N,p) (Lemma 2 [33]), as it also serves as upper bound of neighborhood sizes in SBM(N, p, q). The result implies that with probability at least 1 −O(1/N), we have |Nn|≤ 10 min{a,2}(Np)n,∀1 ≤n≤N. (2) We ignore ifor Nn because of all nodes are identical in CSBM, so the bound applies for every nodes in the graph. The proof of upper bound is combinatorial. Corollary 1 states that whenNis large, the degree of nodeiis approximately the expected degree in G, namely, E[degree] = N 2 (p+ q). Since p(n) ij = ∑ path P={i,v1,...,vn−1,j} 1 deg(i) 1 deg(v1)... 1 deg(vn−1) , (3) using the approximation of degrees, we get that p(n) ij = ( 2 N(p+ q) )n (# of paths P of length n between i and j) . Then we use a tree approximation to calculate the number of paths P of length nbetween iand jby regarding ias the root. Note that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 ∑ j∈Γn−2k (p(n) ij )2 (4) and for j ∈Γn−2k, a deterministic path P′of length n−2kis needed in order to reach j from i. This implies that there are only ksteps deviating from P′. There are (n−2k+ 1)k ways of choosing when to deviate. For each speciﬁc 16way of when to deviate, there are approximately E[degree]k ways of choosing the destinations for deviation. Hence in total, for j ∈Γn−2k, there are (n−2k+ 1)kE[degree]k path of length nbetween iand j. Thus p(n) ij = (n−2k+ 1)k ( 2 N(p+ q) )n−k . (5) Plug in (5) into (4), we get that ∑ j (p(n) ij )2 = ⌊n 2 ⌋∑ k=0 |Γn−2k|(n−2k+ 1)2k ( 2 N(p+ q) )2n−2k (6) ≤ ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k(Np)n−2k ( 2 N(p+ q) )2n−2k (7) Again, (7) follows from using the upper bound on |Γn−2k|[33] such that with probability at least 1 −O(1/N), |Γn−2k|≤ 9 min{a,2}(Np)n−2k,∀1 ≤k≤⌊n 2 ⌋. Combining with Lemma 3, we obtain the ﬁnal result. Figure 7 presents variance calculated from simulation against predicted upper and lower bounds from our theoretical results. The simulation results are averaged over 1000 instances generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 7: Comparison of the bounds on variance in Theorem 2 against simulation results. F Proof of Theorem 2 When we ﬁx K ∈N, only the upper bound in Theorem 2 will change. Note that now the upper bound in (7) can be written as ⌊n 2 ⌋∑ k=0 9 min{a,2}(n−2k+ 1)2k (p+ q 2p )2k( 2p p+ q )n( 2 N(p+ q) )n ≤ C min{a,2} ( C∑ k=0 (p+ q 2p )2k)( 2 N(p+ q) )n ≤ C min{a,2} ( 2 N(p+ q) )n . G Proof of Proposition 1 Let the node representation vector of node v after n graph convolutions be h(n) v . The Bayes error rate could be written as 1 2 (P[h(n) v > D|v ∈C1] + P[h(n) v ≤D|v ∈C2]). For d ∈N, due to the symmetry of our setup, one can easily see that the optimal linear decision boundary is the hyperplane ∑d j=1 xj = d 2 (µ1 + µ2). Then for v ∈C1, 17∑d j=1 (h(n) v )j ∼N(dµ(n) 1 ,d(σ(n))2) and for v∈C2, ∑d j=1 (h(n) v )j ∼N(dµ(n) 2 ,d(σ(n))2). Thus the Bayes error rate can be written as 1 2(P[ d∑ j=1 (h(n) v )j >D|v∈C1] + P[ d∑ j=1 (h(n) v )j ≤D|v∈C2]) = 1 2 ( 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) )) + 1 2 ( Φ ( d 2 (µ1 + µ2) −dµ(n) 2√ dσ(n) )) = 1 −Φ ( d 2 (µ1 + µ2) −dµ(n) 1√ dσ(n) ) . The last equality follows from the fact that d 2 (µ1 + µ2) −dµ(n) 1 = −(d 2 (µ1 + µ2) −dµ(n) 2 ). H How to use the z-score to choose the number of layers The bounds of the z-score with respect to the number of layers, z(n) lower and z(n) upper allow us to calculate bounds for n⋆ and n0 under different scenarios. Speciﬁcally, 1. ∀n∈N,z(n) upper <z(0) = (µ2 −µ1)/σ, then n⋆ = n0 = 0, meaning that no graph convolution should be applied. 2. |{n∈N : z(n) upper ≥z(0)}|>0, and (a) ∀n∈N,z(n) lower < z(0), then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},which means that the number of graph convolutions should not exceed the upper bound of n0, or otherwise one gets worse performance than having no graph convolution. Note that in this case, since n⋆ ≤n0, we can only conclude that 0 ≤n⋆ ≤min{n∈N : z(n) upper ≤z(0)}. (b) |{n∈N : z(n) lower ≥z(0)}|>0, then 0 ≤n0 ≤min{n∈N : z(n) upper ≤z(0)},and let arg max n z(n) lower = n⋆ ﬂoor, max { n≤n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } ≤n⋆ ≤min { n≥n⋆ ﬂoor : z(n) upper ≤z(n⋆ ﬂoor ) lower } , meaning that the number of layers one should apply for optimal node classiﬁcation performance is more than the lower bound of n⋆, and less than the upper bound of n⋆. I Proofs of Proposition 2-5 I.1 Proof of Proposition 2 Since the spectral radius of D−1Ais 1, α(Id−(1 −α)(D−1A))−1 = α ∞∑ k=0 (1 −α)k(D−1A)k. Apply Lemma 2, we get that µPPNP 2 −µPPNP 1 ≈ p+q p+ 2−α α q(µ2 −µ1). To bound the approximation error, similar to the proof of the concentration bound in Theorem 1, it sufﬁces to bound µ1 −µ2 N w⊤ 2 ( ∞∑ k=0 α(1 −α)k((D−1A)k −( ¯D−1 ¯A)k))w2 = µ1 −µ2 N w⊤ 2 (TK + TK+1,∞)w2 , where TK = ∑K k=0 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), TK+1,∞= ∑∞ k=K+1 α(1 −α)k((D−1A)k−( ¯D−1 ¯A)k), and K ∈N up to our own choice. Bounding TK: Apply Theorem 1, ﬁx r> 0, there exists a constantC(r,K,α) such that with probability1−O(N−r), ∥TK∥2 ≤ C√¯d . 18Bounding TK+1,∞: We will show upper bound for(D−1A)k −( ¯D−1 ¯A)k that applies for all k∈N. Note that for every k∈N, (D−1A)k = D−1/2(D−1/2AD−1/2)kD1/2 = D−1/2(VΛkV⊤)D1/2 , where D−1/2AD−1/2 = VΛV⊤is the eigenvalue decomposition. Then ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤∥(D−1A)k∥2 + ∥( ¯D−1 ¯A)k∥2 = ∥(D−1A)k∥2 + 1 ≤∥D−1/2∥2∥(D−1/2AD−1/2)k∥2∥D−1/2∥2 + 1. Since ∥(D−1/2AD−1/2)k∥2 = 1 and by Corollary 1, with probability at least 1 −N−r, ∥D1/2∥2 ≤ √ 3 ¯d/2,∥D−1/2∥2 ≤ √ 2/¯d, the previous inequality becomes ∥(D−1A)k −( ¯D−1 ¯A)k)∥2 ≤ √ 3 + 1. Hence ∥TK+1,∞∥2 ≤(1 −α)K+1 . Combining the two results, we prove the claim. I.2 Proof of Proposition 3 The claim is a direct corollary of Theorem 1. I.3 Proof of Proposition 4 The covariance matrix ΣPPNP of hPPNP could be written as ΣPPNP = α2( ∞∑ k=0 (1 −α)k(D−1A)k)( ∞∑ l=0 (1 −α)l(D−1A)l)⊤σ2 . Note that the variance of node iequals α2 ∑∞ k,l=0(1 −α)k+l(D−1A)k i·((D−1A)l)⊤ i·, where i·refers row iof a matrix. Then by Cauchy-Schwarz Theorem, (D−1A)k i·((D−1A)l)⊤ i· ≤∥(D−1A)k i·∥∥((D−1A)l)i·∥ ≤ √ (σ(k))2(σ(l))2/σ2,for all 1 ≤k,l ≤N. Moreover, by Lemma 3, (σ(k))2 ≤σ2. Due to the identity of each node i, we get that with probability 1 −O(1/N), for all 1 ≤K ≤N, (σPPNP)2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + ∞∑ k=K+1 (1 −α)kσ )2 ≤α2 (K∑ k=0 (1 −α)k √ (σ(k))2upper + (1 −α)K+1 α σ )2 . For the lower bound, note that with probability 1 −O(1/N), (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2k 1 Nk + ∞∑ k=N+1 (1 −α)2k 1 N ) σ2 , where Nk is the size of k-hop neighborhood. Then (σPPNP)2 ≥α2 (N∑ k=0 (1 −α)2kmin{a,2} 10 1 (Np)k ) σ2 ≥α2 min{a,2} 10 (Np)N+1 −(1 −α)2N+2 (Np)N(Np −(1 −α)2) σ2 ≥α2 min{a,2} 10 σ2 . It is easy to see that Lemma 3 applies to any message-passing scheme which could be regarded as a random walk on the graph. Combining with Lemma 3, we get the ﬁnal result. 19I.4 Proof of Proposition 5 Since hAPPNP(n) = ( α (n−1∑ k=0 (1 −α)k(D−1A)k ) + (1 −α)n(D−1A)n ) X Through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≤ ( α (n−1∑ k=0 (1 −α)k √ (σ(k))2upper ) + (1 −α)n √ (σ(n))2upper )2 . For the lower bound, through the same calculation as for the upper bound in the proof of Proposition 2, we get that with probability 1 −O(1/N), (σAPPNP(n))2 ≥α2 n−1∑ k=0 (1 −α)2k(σ(k))2 + (1 −α)2n(σ(n))2 ≥α2 min{a,2} 10 (n−1∑ k=0 (1 −α)2k 1 (Np)k ) σ2 + min{a,2} 10 (1 −α)2n 1 (Np)nσ2 ≥min{a,2} 10 ( α2 + (1 −α)2n (Np)n ) σ2 . Combining with Lemma 3, we get the ﬁnal result. J Experiments Here we provide more details on the models that we use in Section 5. In all cases we use the Adam optimizer and tune some hyperparameters for better performance. The hyperparameters used are summarized as follows. Data ﬁnal linear classiﬁer weights in GNN layer learning rate (width) iterations (width) synthetic 1 layer no 0.01 8000 yes 0.01(1,4,16)/0.001(64,256) 8000(1,4,16)/10000(64)/50000(256) Cora 3 layer with 32 hidden channels no 0.001 150 yes 0.001 200 CiteSeer 3 layer with 16 hidden channels no 0.001 100 yes 0.001 100 PubMed 3 layer with 32 hidden channels no 0.001 500 yes 0.001 500 We empirically ﬁnd that after adding in weights in each GNN layer, it takes much longer to train the model for one iteration, and the time increases when the depth or the width increases (Figure 8). Since for some combinations, it takes more than 200,000 iterations for the validation accuracy to ﬁnally increase, for each case, we only train for a reasonable amount of iterations. Figure 8: Iterations per second for each model. All models were implemented with PyTorch [46] and PyTorch Geometric [47]. 20K Additional Results K.1 Effect of nonlinearity on classiﬁcation performance In section 3, we consider the case of a simpliﬁed linear GNN. What would happen if we add nonlinearity after linear graph convolutions? Here, we consider the case of a GNN with a ReLU activation function added after nlinear graph convolutions, i.e. h(n)ReLU = ReLU((D−1A)nX). We show that adding such nonlinearity does not improve the classiﬁcation performance. Proposition 6. Applying a ReLU activation function after n linear graph convolutions does not decrease the Bayes error rate, i.e. Bayes error rate based on h(n)ReLU ≥ Bayes error rate based on h(n), and equality holds if µ1 ≥−µ2. Proof. If is known that if xfollows a Gaussian distribution, then ReLU(x) follows a Rectiﬁed Gaussian distribution. Following the deﬁnition of the Bayes optimal classiﬁer, we present a geometric proof in Figure 9 (see next page, top), where the dark blue bar denotes the location of 0 and the red bar denotes the decision boundary Dof the Bayes optimal classiﬁer, and the light blue area denotes the overlapping area S, which is twice the Bayes error rate. Figure 9: A geometric proof of Proposition 6. K.2 Exact limit of variance (σ(n))2 as n→∞ Proposition 7. Given a graph Gwith adjacency matrix A, let its degree vector be d= A1 N, where 1 N is the all-one vector of length N. If Gis connected and non-bipartite, the variance of each node i, denoted as (σi(n))2, converges asymptotically to ∥d∥2 2 ∥d∥2 1 , i.e. (σi (n))2 n→∞ −−−−→∥d∥2 2 ∥d∥2 1 . Then ∥d∥2 ∥d∥2 1 ≥ 1 N, and the equality holds if and only if Gis regular. Proof. Let ei denotes the standard basis unit vector with the ith entry equals 1, and all other entries equal 0. Since Gis connected and non-bipartite, the random walk represented by P = D−1Ais ergodic, meaning that e⊤ i P(n) n→∞ −−−−→π, where πis the stationary distribution of this random walk with πi = di ∥d∥1 . Then since norms are continuous functions, we conclude that (σi (n))2 = ∑ j (p(n) ij )2 = ∥e⊤ i P(n)∥2 2 n→∞ −−−−→∥π∥2 2 = ∥d∥2 2 ∥d∥2 1 . By Lemma 3, it follows that ∥d∥2 2 ∥d∥2 1 ≥ 1 N. The unique minimizer of ∥π∥2 2 subject to ∥π∥1 = 1 is π= 1 N1 N. This means that Gmust be regular to achieve the lower bound asymptotically. 21Under Assumption 1, the graph generated by our CSBM is almost surely connected. Here, we remain to show that with high probability, the graph will also be non-bipartite. Proposition 8. With probability at least 1 −O(1/(Np)3), a graph Ggenerated from CSBM(N, p, q, µ1, µ2, σ2) contains a triangle, which implies that it is non-bipartite. Proof. The proof goes by the classic probabilistic method. Let T∆ = ( N 3 )∑ i 1 τi denotes the number of triangles in G, where 1 τi equals 1 if potential triangle τi exists and 0 otherwise. Then by second moment method, P[T∆ = 0] ≤ Var(T∆) (E[T∆])2 = 1 E[T∆]) + ∑ i̸=jE[1 τi1 τj] −(E[T∆])2 (E[T∆])2 . Since E[T∆] = O(Np), ∑ i̸=jE[1 τi1 τj] = (1 + O(1/N))(E[T∆])2, we get that P[T∆ = 0] ≤O(1/(Np)3) + O(1/N) ≤O(1/(Np)3) . Hence P[Gis non-bipartite] ≥P[T∆ ≥1] ≥1 −O(1/(Np)3). K.3 Symmetric Graph Convolution D−1/2AD−1/2 Proposition 9. When using symmetric message-passing convolution D−1/2AD−1/2 instead, the variance (σ(n))2 is non-increasing with respect to the number of convolutional layers n. i.e. (σ(n+1))2 ≤(σ(n))2,n ∈N ∪{0}. Proof. We want to calculate the diagonal entries of the covariance matrix Σ(n) of (D−1/2AD−1/2)nX, where the covariance matrix of X is σ2IN. Hence Σ(n) = (D−1/2AD−1/2)n( (D−1/2AD−1/2)n)⊤ . Since D−1/2AD−1/2 is symmetric, let its eigendecomposition be VΛV⊤and we could rewrite Σ(n) = (VΛnV⊤)(VΛnV⊤) = VΛ2nV⊤. Notice that the closed form of the diagonal entries is diag(Σ(n)) = N∑ i=1 λ2n i |v|2 . Since for all 1 ≤i≤N, |λi|≤ 1,we obtain monotonicity of each entry of diag(Σ(n)), i.e. variance of each node. Although the proposition does not always hold for random walk message-passing convolution D−1A as one can construct speciﬁc counterexamples (Appendix K.4), in practice, variances are observed to be decreasing with respect with the number of layers. Moreover, we empirically observe that variance goes down more than the variance using symmetric message-passing convolutions. Figure 10 presents visualization of node representations comparing the change of variance with respect to the number of layers using random walk convolution and symmetric message-passing convolution. The data is generated from CSBM(N = 2000,p = 0.0114,q = 0.0038,µ1 = 1,µ2 = 1.5,σ2 = 1). Figure 10: The change of variance with respect to the number of layers using random walk convolution D−1Aand symmetric message-passing convolution D−1/2AD−1/2. 22K.4 Counterexamples Here, we construct a speciﬁc example where the variance (σ(n))2 is not non-increasing with respect to the number of layers n(Figure 11A). We remark that such a non-monotone nature of change in variance is not caused by the bipartiteness of the graph, as a cycle graph with even number of nodes is also bipartite, but does not exhibit such phenomenon (Figure 11B). We conjecture the increase in variance is rather caused by the tree-like structure. A B Figure 11: Counterexamples. K.5 The mixing and denoising effects in practice In this section, we measure the mixing and denoising effects of graph convolutions identiﬁed by our theoretical results in practice, and show that the same tradeoff between the two counteracting effects exists for real-world graphs. For the mixing effect, we measure the pairwise L2 distances between the means of different classes, and for the denoising effect, we measure the within-class variances, both respect to the number of layers. Figure 12 gives a visualization of both metrics for all classes on Cora, CiteSeer and PubMed. We observe that similar to the synthetic CSBM data, adding graph convolutions increases both the mixing effect (homogenizing node representations in different classes, measured by the inter-class distances) and the denoising effect (homogenizing node representations in the same class, measured by the within-class distances). In addition, the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory. Figure 12: The existence of the mixing (top row) and denoising effects (bottom row) of graph convolutions in practice. Adding graph convolutions increases both effects and the beneﬁcial denoising effect clearly reaches saturation just after a small number of layers, as predicted by our theory in Section 3. 23",
      "meta_data": {
        "arxiv_id": "2212.10701v2",
        "authors": [
          "Xinyi Wu",
          "Zhengdao Chen",
          "William Wang",
          "Ali Jadbabaie"
        ],
        "published_date": "2022-12-21T00:33:59Z",
        "pdf_url": "https://arxiv.org/pdf/2212.10701v2.pdf"
      }
    }
  ]
}